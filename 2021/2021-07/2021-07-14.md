## Summary for 2021-07-14, created on 2021-12-19


<details><summary><b>The Benchmark Lottery</b>
<a href="https://arxiv.org/abs/2107.07002">arxiv:2107.07002</a>
&#x1F4C8; 84 <br>
<p>Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, Oriol Vinyals</p></summary>
<p>

**Abstract:** The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of "a benchmark lottery" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.

</p>
</details>

<details><summary><b>Deduplicating Training Data Makes Language Models Better</b>
<a href="https://arxiv.org/abs/2107.06499">arxiv:2107.06499</a>
&#x1F4C8; 68 <br>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini</p></summary>
<p>

**Abstract:** We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.

</p>
</details>

<details><summary><b>Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot</b>
<a href="https://arxiv.org/abs/2107.06857">arxiv:2107.06857</a>
&#x1F4C8; 49 <br>
<p>Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets, John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor Mordatch, Thore Graepel</p></summary>
<p>

**Abstract:** Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.

</p>
</details>

<details><summary><b>Online Learning for Recommendations at Grubhub</b>
<a href="https://arxiv.org/abs/2107.07106">arxiv:2107.07106</a>
&#x1F4C8; 47 <br>
<p>Alex Egg</p></summary>
<p>

**Abstract:** We propose a method to easily modify existing offline Recommender Systems to run online using Transfer Learning. Online Learning for Recommender Systems has two main advantages: quality and scale. Like many Machine Learning algorithms in production if not regularly retrained will suffer from Concept Drift. A policy that is updated frequently online can adapt to drift faster than a batch system. This is especially true for user-interaction systems like recommenders where the underlying distribution can shift drastically to follow user behaviour. As a platform grows rapidly like Grubhub, the cost of running batch training jobs becomes material. A shift from stateless batch learning offline to stateful incremental learning online can recover, for example, at Grubhub, up to a 45x cost savings and a +20% metrics increase. There are a few challenges to overcome with the transition to online stateful learning, namely convergence, non-stationary embeddings and off-policy evaluation, which we explore from our experiences running this system in production.

</p>
</details>

<details><summary><b>HTLM: Hyper-Text Pre-Training and Prompting of Language Models</b>
<a href="https://arxiv.org/abs/2107.06955">arxiv:2107.06955</a>
&#x1F4C8; 36 <br>
<p>Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer</p></summary>
<p>

**Abstract:** We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research.

</p>
</details>

<details><summary><b>Hida-Matérn Kernel</b>
<a href="https://arxiv.org/abs/2107.07098">arxiv:2107.07098</a>
&#x1F4C8; 24 <br>
<p>Matthew Dowling, Piotr Sokół, Il Memming Park</p></summary>
<p>

**Abstract:** We present the class of Hida-Matérn kernels, which is the canonical family of covariance functions over the entire space of stationary Gauss-Markov Processes. It extends upon Matérn kernels, by allowing for flexible construction of priors over processes with oscillatory components. Any stationary kernel, including the widely used squared-exponential and spectral mixture kernels, are either directly within this class or are appropriate asymptotic limits, demonstrating the generality of this class. Taking advantage of its Markovian nature we show how to represent such processes as state space models using only the kernel and its derivatives. In turn this allows us to perform Gaussian Process inference more efficiently and side step the usual computational burdens. We also show how exploiting special properties of the state space representation enables improved numerical stability in addition to further reductions of computational complexity.

</p>
</details>

<details><summary><b>Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines</b>
<a href="https://arxiv.org/abs/2107.06925">arxiv:2107.06925</a>
&#x1F4C8; 20 <br>
<p>Shigang Li, Torsten Hoefler</p></summary>
<p>

**Abstract:** Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchronous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to 50%; benefiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer based language models. For a GPT-2 model with 1.3 billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by 1.16x-2.34x over the state-of-the-art synchronous and asynchronous pipeline approaches.

</p>
</details>

<details><summary><b>DULA: A Differentiable Ergonomics Model for Postural Optimization in Physical HRI</b>
<a href="https://arxiv.org/abs/2107.06875">arxiv:2107.06875</a>
&#x1F4C8; 14 <br>
<p>Amir Yazdani, Roya Sabbagh Novin, Andrew Merryweather, Tucker Hermans</p></summary>
<p>

**Abstract:** Ergonomics and human comfort are essential concerns in physical human-robot interaction applications. Defining an accurate and easy-to-use ergonomic assessment model stands as an important step in providing feedback for postural correction to improve operator health and comfort. In order to enable efficient computation, previously proposed automated ergonomic assessment and correction tools make approximations or simplifications to gold-standard assessment tools used by ergonomists in practice. In order to retain assessment quality, while improving computational considerations, we introduce DULA, a differentiable and continuous ergonomics model learned to replicate the popular and scientifically validated RULA assessment. We show that DULA provides assessment comparable to RULA while providing computational benefits. We highlight DULA's strength in a demonstration of gradient-based postural optimization for a simulated teleoperation task.

</p>
</details>

<details><summary><b>Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition</b>
<a href="https://arxiv.org/abs/2107.07029">arxiv:2107.07029</a>
&#x1F4C8; 9 <br>
<p>Hugo Flores Garcia, Aldo Aguilar, Ethan Manilow, Bryan Pardo</p></summary>
<p>

**Abstract:** Deep learning work on musical instrument recognition has generally focused on instrument classes for which we have abundant data. In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference. We apply a hierarchical loss function to the training of prototypical networks, combined with a method to aggregate prototypes hierarchically, mirroring the structure of a predefined musical instrument hierarchy. These extensions require no changes to the network architecture and new levels can be easily added or removed. Compared to a non-hierarchical few-shot baseline, our method leads to a significant increase in classification accuracy and significant decrease mistake severity on instrument classes unseen in training.

</p>
</details>

<details><summary><b>Recurrent Parameter Generators</b>
<a href="https://arxiv.org/abs/2107.07110">arxiv:2107.07110</a>
&#x1F4C8; 8 <br>
<p>Jiayun Wang, Yubei Chen, Stella X. Yu, Brian Cheung, Yann LeCun</p></summary>
<p>

**Abstract:** We present a generic method for recurrently using the same parameters for many different convolution layers to build a deep network. Specifically, for a network, we create a recurrent parameter generator (RPG), from which the parameters of each convolution layer are generated. Though using recurrent models to build a deep convolutional neural network (CNN) is not entirely new, our method achieves significant performance gain compared to the existing works. We demonstrate how to build a one-layer neural network to achieve similar performance compared to other traditional CNN models on various applications and datasets. Such a method allows us to build an arbitrarily complex neural network with any amount of parameters. For example, we build a ResNet34 with model parameters reduced by more than $400$ times, which still achieves $41.6\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG can be applied at different scales, such as layers, blocks, or even sub-networks. Specifically, we use the RPG to build a ResNet18 network with the number of weights equivalent to one convolutional layer of a conventional ResNet and show this model can achieve $67.2\%$ ImageNet top-1 accuracy. The proposed method can be viewed as an inverse approach to model compression. Rather than removing the unused parameters from a large model, it aims to squeeze more information into a small number of parameters. Extensive experiment results are provided to demonstrate the power of the proposed recurrent parameter generator.

</p>
</details>

<details><summary><b>Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning</b>
<a href="https://arxiv.org/abs/2107.06946">arxiv:2107.06946</a>
&#x1F4C8; 8 <br>
<p>Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, Ahmed Zamzam</p></summary>
<p>

**Abstract:** In recent years, machine learning techniques utilizing large-scale datasets have achieved remarkable performance. Differential privacy, by means of adding noise, provides strong privacy guarantees for such learning algorithms. The cost of differential privacy is often a reduced model accuracy and a lowered convergence speed. This paper investigates the impact of differential privacy on learning algorithms in terms of their carbon footprint due to either longer run-times or failed experiments. Through extensive experiments, further guidance is provided on choosing the noise levels which can strike a balance between desired privacy levels and reduced carbon emissions.

</p>
</details>

<details><summary><b>A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing</b>
<a href="https://arxiv.org/abs/2107.07058">arxiv:2107.07058</a>
&#x1F4C8; 7 <br>
<p>Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang, Michael Ng</p></summary>
<p>

**Abstract:** Image smoothing is a fundamental procedure in applications of both computer vision and graphics. The required smoothing properties can be different or even contradictive among different tasks. Nevertheless, the inherent smoothing nature of one smoothing operator is usually fixed and thus cannot meet the various requirements of different applications. In this paper, we first introduce the truncated Huber penalty function which shows strong flexibility under different parameter settings. A generalized framework is then proposed with the introduced truncated Huber penalty function. When combined with its strong flexibility, our framework is able to achieve diverse smoothing natures where contradictive smoothing behaviors can even be achieved. It can also yield the smoothing behavior that can seldom be achieved by previous methods, and superior performance is thus achieved in challenging cases. These together enable our framework capable of a range of applications and able to outperform the state-of-the-art approaches in several tasks, such as image detail enhancement, clip-art compression artifacts removal, guided depth map restoration, image texture removal, etc. In addition, an efficient numerical solution is provided and its convergence is theoretically guaranteed even the optimization framework is non-convex and non-smooth. A simple yet effective approach is further proposed to reduce the computational cost of our method while maintaining its performance. The effectiveness and superior performance of our approach are validated through comprehensive experiments in a range of applications. Our code is available at https://github.com/wliusjtu/Generalized-Smoothing-Framework.

</p>
</details>

<details><summary><b>A Framework for Machine Learning of Model Error in Dynamical Systems</b>
<a href="https://arxiv.org/abs/2107.06658">arxiv:2107.06658</a>
&#x1F4C8; 7 <br>
<p>Matthew E. Levine, Andrew M. Stuart</p></summary>
<p>

**Abstract:** The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge. We cast the problem in both continuous- and discrete-time, for problems in which the model error is memoryless and in which it has significant memory, and we compare data-driven and hybrid approaches experimentally. Our formulation is agnostic to the chosen machine learning model.
  Using Lorenz '63 and Lorenz '96 Multiscale systems, we find that hybrid methods substantially outperform solely data-driven approaches in terms of data hunger, demands for model complexity, and overall predictive performance. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field cannot be resolved.
  We study model error from the learning theory perspective, defining excess risk and generalization error; for a linear model of the error used to learn about ergodic dynamical systems, both errors are bounded by terms that diminish with the square-root of T. We also illustrate scenarios that benefit from modeling with memory, proving that continuous-time recurrent neural networks (RNNs) can, in principle, learn memory-dependent model error and reconstruct the original system arbitrarily well; numerical results depict challenges in representing memory by this approach. We also connect RNNs to reservoir computing and thereby relate the learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features.

</p>
</details>

<details><summary><b>Large-Scale News Classification using BERT Language Model: Spark NLP Approach</b>
<a href="https://arxiv.org/abs/2107.06785">arxiv:2107.06785</a>
&#x1F4C8; 6 <br>
<p>Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Novanto Yudistira</p></summary>
<p>

**Abstract:** The rise of big data analytics on top of NLP increases the computational burden for text processing at scale. The problems faced in NLP are very high dimensional text, so it takes a high computation resource. The MapReduce allows parallelization of large computations and can improve the efficiency of text processing. This research aims to study the effect of big data processing on NLP tasks based on a deep learning approach. We classify a big text of news topics with fine-tuning BERT used pre-trained models. Five pre-trained models with a different number of parameters were used in this study. To measure the efficiency of this method, we compared the performance of the BERT with the pipelines from Spark NLP. The result shows that BERT without Spark NLP gives higher accuracy compared to BERT with Spark NLP. The accuracy average and training time of all models using BERT is 0.9187 and 35 minutes while using BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will take more computation resources and need a longer time to complete the tasks. However, the accuracy of BERT with Spark NLP only decreased by an average of 5.7%, while the training time was reduced significantly by 62.9% compared to BERT without Spark NLP.

</p>
</details>

<details><summary><b>Safer Reinforcement Learning through Transferable Instinct Networks</b>
<a href="https://arxiv.org/abs/2107.06686">arxiv:2107.06686</a>
&#x1F4C8; 6 <br>
<p>Djordje Grbic, Sebastian Risi</p></summary>
<p>

**Abstract:** Random exploration is one of the main mechanisms through which reinforcement learning (RL) finds well-performing policies. However, it can lead to undesirable or catastrophic outcomes when learning online in safety-critical environments. In fact, safe learning is one of the major obstacles towards real-world agents that can learn during deployment. One way of ensuring that agents respect hard limitations is to explicitly configure boundaries in which they can operate. While this might work in some cases, we do not always have clear a-priori information which states and actions can lead dangerously close to hazardous states. Here, we present an approach where an additional policy can override the main policy and offer a safer alternative action. In our instinct-regulated RL (IR^2L) approach, an "instinctual" network is trained to recognize undesirable situations, while guarding the learning policy against entering them. The instinct network is pre-trained on a single task where it is safe to make mistakes, and transferred to environments in which learning a new task safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain, in which it receives a significantly lower number of safety violations during training than a baseline RL approach while reaching similar task performance.

</p>
</details>

<details><summary><b>Learning Algebraic Recombination for Compositional Generalization</b>
<a href="https://arxiv.org/abs/2107.06516">arxiv:2107.06516</a>
&#x1F4C8; 6 <br>
<p>Chenyao Liu, Shengnan An, Zeqi Lin, Qian Liu, Bei Chen, Jian-Guang Lou, Lijie Wen, Nanning Zheng, Dongmei Zhang</p></summary>
<p>

**Abstract:** Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination. In this paper, we propose LeAR, an end-to-end neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP.

</p>
</details>

<details><summary><b>Few-shot Neural Human Performance Rendering from Sparse RGBD Videos</b>
<a href="https://arxiv.org/abs/2107.06505">arxiv:2107.06505</a>
&#x1F4C8; 6 <br>
<p>Anqi Pang, Xin Chen, Haimin Luo, Minye Wu, Jingyi Yu, Lan Xu</p></summary>
<p>

**Abstract:** Recent neural rendering approaches for human activities achieve remarkable view synthesis results, but still rely on dense input views or dense training with all the capture frames, leading to deployment difficulty and inefficient training overload. However, existing advances will be ill-posed if the input is both spatially and temporally sparse. To fill this gap, in this paper we propose a few-shot neural human rendering approach (FNHR) from only sparse RGBD inputs, which exploits the temporal and spatial redundancy to generate photo-realistic free-view output of human activities. Our FNHR is trained only on the key-frames which expand the motion manifold in the input sequences. We introduce a two-branch neural blending to combine the neural point render and classical graphics texturing pipeline, which integrates reliable observations over sparse key-frames. Furthermore, we adopt a patch-based adversarial training process to make use of the local redundancy and avoids over-fitting to the key-frames, which generates fine-detailed rendering results. Extensive experiments demonstrate the effectiveness of our approach to generate high-quality free view-point results for challenging human performances under the sparse setting.

</p>
</details>

<details><summary><b>FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements</b>
<a href="https://arxiv.org/abs/2107.06943">arxiv:2107.06943</a>
&#x1F4C8; 5 <br>
<p>Szymon Płotka, Tomasz Włodarczyk, Adam Klasa, Michał Lipa, Arkadiusz Sitek, Tomasz Trzciński</p></summary>
<p>

**Abstract:** In this paper, we propose an end-to-end multi-task neural network called FetalNet with an attention mechanism and stacked module for spatio-temporal fetal ultrasound scan video analysis. Fetal biometric measurement is a standard examination during pregnancy used for the fetus growth monitoring and estimation of gestational age and fetal weight. The main goal in fetal ultrasound scan video analysis is to find proper standard planes to measure the fetal head, abdomen and femur. Due to natural high speckle noise and shadows in ultrasound data, medical expertise and sonographic experience are required to find the appropriate acquisition plane and perform accurate measurements of the fetus. In addition, existing computer-aided methods for fetal US biometric measurement address only one single image frame without considering temporal features. To address these shortcomings, we propose an end-to-end multi-task neural network for spatio-temporal ultrasound scan video analysis to simultaneously localize, classify and measure the fetal body parts. We propose a new encoder-decoder segmentation architecture that incorporates a classification branch. Additionally, we employ an attention mechanism with a stacked module to learn salient maps to suppress irrelevant US regions and efficient scan plane localization. We trained on the fetal ultrasound video comes from routine examinations of 700 different patients. Our method called FetalNet outperforms existing state-of-the-art methods in both classification and segmentation in fetal ultrasound video recordings.

</p>
</details>

<details><summary><b>Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More</b>
<a href="https://arxiv.org/abs/2107.06876">arxiv:2107.06876</a>
&#x1F4C8; 5 <br>
<p>Johannes Klicpera, Marten Lienen, Stephan Günnemann</p></summary>
<p>

**Abstract:** The current best practice for computing optimal transport (OT) is via entropy regularization and Sinkhorn iterations. This algorithm runs in quadratic time as it requires the full pairwise cost matrix, which is prohibitively expensive for large sets of objects. In this work we propose two effective log-linear time approximations of the cost matrix: First, a sparse approximation based on locality-sensitive hashing (LSH) and, second, a Nyström approximation with LSH-based sparse corrections, which we call locally corrected Nyström (LCN). These approximations enable general log-linear time algorithms for entropy-regularized OT that perform well even for the complex, high-dimensional spaces common in deep learning. We analyse these approximations theoretically and evaluate them experimentally both directly and end-to-end as a component for real-world applications. Using our approximations for unsupervised word embedding alignment enables us to speed up a state-of-the-art method by a factor of 3 while also improving the accuracy by 3.1 percentage points without any additional model changes. For graph distance regression we propose the graph transport network (GTN), which combines graph neural networks (GNNs) with enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales log-linearly in the number of nodes.

</p>
</details>

<details><summary><b>Artificial Intelligence in PET: an Industry Perspective</b>
<a href="https://arxiv.org/abs/2107.06747">arxiv:2107.06747</a>
&#x1F4C8; 5 <br>
<p>Arkadiusz Sitek, Sangtae Ahn, Evren Asma, Adam Chandler, Alvin Ihsani, Sven Prevrhal, Arman Rahmim, Babak Saboury, Kris Thielemans</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) has significant potential to positively impact and advance medical imaging, including positron emission tomography (PET) imaging applications. AI has the ability to enhance and optimize all aspects of the PET imaging chain from patient scheduling, patient setup, protocoling, data acquisition, detector signal processing, reconstruction, image processing and interpretation. AI poses industry-specific challenges which will need to be addressed and overcome to maximize the future potentials of AI in PET. This paper provides an overview of these industry-specific challenges for the development, standardization, commercialization, and clinical adoption of AI, and explores the potential enhancements to PET imaging brought on by AI in the near future. In particular, the combination of on-demand image reconstruction, AI, and custom designed data processing workflows may open new possibilities for innovation which would positively impact the industry and ultimately patients.

</p>
</details>

<details><summary><b>Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration</b>
<a href="https://arxiv.org/abs/2107.06629">arxiv:2107.06629</a>
&#x1F4C8; 5 <br>
<p>Miroslav Bogdanovic, Majid Khadiv, Ludovic Righetti</p></summary>
<p>

**Abstract:** In this work we present a general, two-stage reinforcement learning approach for going from a single demonstration trajectory to a robust policy that can be deployed on hardware without any additional training. The demonstration is used in the first stage as a starting point to facilitate initial exploration. In the second stage, the relevant task reward is optimized directly and a policy robust to environment uncertainties is computed. We demonstrate and examine in detail performance and robustness of our approach on highly dynamic hopping and bounding tasks on a real quadruped robot.

</p>
</details>

<details><summary><b>Continuous vs. Discrete Optimization of Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2107.06608">arxiv:2107.06608</a>
&#x1F4C8; 5 <br>
<p>Omer Elkabetz, Nadav Cohen</p></summary>
<p>

**Abstract:** Existing analyses of optimization in deep learning are either continuous, focusing on (variants of) gradient flow, or discrete, directly treating (variants of) gradient descent. Gradient flow is amenable to theoretical analysis, but is stylized and disregards computational efficiency. The extent to which it represents gradient descent is an open question in the theory of deep learning. The current paper studies this question. Viewing gradient descent as an approximate numerical solution to the initial value problem of gradient flow, we find that the degree of approximation depends on the curvature around the gradient flow trajectory. We then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows us to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple deep neural networks, gradient descent with conventional step size is indeed close to gradient flow. We hypothesize that the theory of gradient flows will unravel mysteries behind deep learning.

</p>
</details>

<details><summary><b>MESS: Manifold Embedding Motivated Super Sampling</b>
<a href="https://arxiv.org/abs/2107.06566">arxiv:2107.06566</a>
&#x1F4C8; 5 <br>
<p>Erik Thordsen, Erich Schubert</p></summary>
<p>

**Abstract:** Many approaches in the field of machine learning and data analysis rely on the assumption that the observed data lies on lower-dimensional manifolds. This assumption has been verified empirically for many real data sets. To make use of this manifold assumption one generally requires the manifold to be locally sampled to a certain density such that features of the manifold can be observed. However, for increasing intrinsic dimensionality of a data set the required data density introduces the need for very large data sets, resulting in one of the many faces of the curse of dimensionality. To combat the increased requirement for local data density we propose a framework to generate virtual data points that faithful to an approximate embedding function underlying the manifold observable in the data.

</p>
</details>

<details><summary><b>STAR: Sparse Transformer-based Action Recognition</b>
<a href="https://arxiv.org/abs/2107.07089">arxiv:2107.07089</a>
&#x1F4C8; 4 <br>
<p>Feng Shi, Chonghan Lee, Liang Qiu, Yizhou Zhao, Tianyi Shen, Shivran Muralidhar, Tian Han, Song-Chun Zhu, Vijaykrishnan Narayanan</p></summary>
<p>

**Abstract:** The cognitive system for human action and behavior has evolved into a deep learning regime, and especially the advent of Graph Convolution Networks has transformed the field in recent years. However, previous works have mainly focused on over-parameterized and complex models based on dense graph convolution networks, resulting in low efficiency in training and inference. Meanwhile, the Transformer architecture-based model has not yet been well explored for cognitive application in human action and behavior estimation. This work proposes a novel skeleton-based human action recognition model with sparse attention on the spatial dimension and segmented linear attention on the temporal dimension of data. Our model can also process the variable length of video clips grouped as a single batch. Experiments show that our model can achieve comparable performance while utilizing much less trainable parameters and achieve high speed in training and inference. Experiments show that our model achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline models at competitive accuracy.

</p>
</details>

<details><summary><b>Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing</b>
<a href="https://arxiv.org/abs/2107.06990">arxiv:2107.06990</a>
&#x1F4C8; 4 <br>
<p>Tazin Afrin, Elaine Wang, Diane Litman, Lindsay C. Matsumura, Richard Correnti</p></summary>
<p>

**Abstract:** Automated writing evaluation systems can improve students' writing insofar as students attend to the feedback provided and revise their essay drafts in ways aligned with such feedback. Existing research on revision of argumentative writing in such systems, however, has focused on the types of revisions students make (e.g., surface vs. content) rather than the extent to which revisions actually respond to the feedback provided and improve the essay. We introduce an annotation scheme to capture the nature of sentence-level revisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th- and 6th-grade students' argumentative essays. We show that reliable manual annotation can be achieved and that revision annotations correlate with a holistic assessment of essay improvement in line with the feedback provided. Furthermore, we explore the feasibility of automatically classifying revisions according to our scheme.

</p>
</details>

<details><summary><b>Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion</b>
<a href="https://arxiv.org/abs/2107.06916">arxiv:2107.06916</a>
&#x1F4C8; 4 <br>
<p>Mingbao Lin, Rongrong Ji, Bohong Chen, Fei Chao, Jianzhuang Liu, Wei Zeng, Yonghong Tian, Qi Tian</p></summary>
<p>

**Abstract:** The mainstream approach for filter pruning is usually either to force a hard-coded importance estimation upon a computation-heavy pretrained model to select "important" filters, or to impose a hyperparameter-sensitive sparse constraint on the loss objective to regularize the network training. In this paper, we present a novel filter pruning method, dubbed dynamic-coded filter fusion (DCFF), to derive compact CNNs in a computation-economical and regularization-free manner for efficient image classification. Each filter in our DCFF is firstly given an inter-similarity distribution with a temperature parameter as a filter proxy, on top of which, a fresh Kullback-Leibler divergence based dynamic-coded criterion is proposed to evaluate the filter importance. In contrast to simply keeping high-score filters in other methods, we propose the concept of filter fusion, i.e., the weighted averages using the assigned proxies, as our preserved filters. We obtain a one-hot inter-similarity distribution as the temperature parameter approaches infinity. Thus, the relative importance of each filter can vary along with the training of the compact CNN, leading to dynamically changeable fused filters without both the dependency on the pretrained model and the introduction of sparse constraints. Extensive experiments on classification benchmarks demonstrate the superiority of our DCFF over the compared counterparts. For example, our DCFF derives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while reaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained with 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1 accuracy on ILSVRC-2012. Our code, narrower models and training logs are available at https://github.com/lmbxmu/DCFF.

</p>
</details>

<details><summary><b>Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data</b>
<a href="https://arxiv.org/abs/2107.06777">arxiv:2107.06777</a>
&#x1F4C8; 4 <br>
<p>Christian Bartz, Hendrik Rätz, Haojin Yang, Joseph Bethge, Christoph Meinel</p></summary>
<p>

**Abstract:** One of the most pressing problems in the automated analysis of historical documents is the availability of annotated training data. In this paper, we propose a novel method for the synthesis of training data for semantic segmentation of document images. We utilize clusters found in intermediate features of a StyleGAN generator for the synthesis of RGB and label images at the same time. Our model can be applied to any dataset of scanned documents without the need for manual annotation of individual images, as each model is custom-fit to the dataset. In our experiments, we show that models trained on our synthetic data can reach competitive performance on open benchmark datasets for line segmentation.

</p>
</details>

<details><summary><b>Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs</b>
<a href="https://arxiv.org/abs/2107.06618">arxiv:2107.06618</a>
&#x1F4C8; 4 <br>
<p>Shruthi Bannur, Ozan Oktay, Melanie Bernhardt, Anton Schwaighofer, Rajesh Jena, Besmira Nushi, Sharan Wadhwani, Aditya Nori, Kal Natarajan, Shazad Ashraf, Javier Alvarez-Valle, Daniel C. Castro</p></summary>
<p>

**Abstract:** Chest radiography has been a recommended procedure for patient triaging and resource management in intensive care units (ICUs) throughout the COVID-19 pandemic. The machine learning efforts to augment this workflow have been long challenged due to deficiencies in reporting, model evaluation, and failure mode analysis. To address some of those shortcomings, we model radiological features with a human-interpretable class hierarchy that aligns with the radiological decision process. Also, we propose the use of a data-driven error analysis methodology to uncover the blind spots of our model, providing further transparency on its clinical utility. For example, our experiments show that model failures highly correlate with ICU imaging conditions and with the inherent difficulty in distinguishing certain types of radiological features. Also, our hierarchical interpretation and analysis facilitates the comparison with respect to radiologists' findings and inter-variability, which in return helps us to better assess the clinical applicability of models.

</p>
</details>

<details><summary><b>Probabilistic Human Motion Prediction via A Bayesian Neural Network</b>
<a href="https://arxiv.org/abs/2107.06564">arxiv:2107.06564</a>
&#x1F4C8; 4 <br>
<p>Jie Xu, Xingyu Chen, Xuguang Lan, Nanning Zheng</p></summary>
<p>

**Abstract:** Human motion prediction is an important and challenging topic that has promising prospects in efficient and safe human-robot-interaction systems. Currently, the majority of the human motion prediction algorithms are based on deterministic models, which may lead to risky decisions for robots. To solve this problem, we propose a probabilistic model for human motion prediction in this paper. The key idea of our approach is to extend the conventional deterministic motion prediction neural network to a Bayesian one. On one hand, our model could generate several future motions when given an observed motion sequence. On the other hand, by calculating the Epistemic Uncertainty and the Heteroscedastic Aleatoric Uncertainty, our model could tell the robot if the observation has been seen before and also give the optimal result among all possible predictions. We extensively validate our approach on a large scale benchmark dataset Human3.6m. The experiments show that our approach performs better than deterministic methods. We further evaluate our approach in a Human-Robot-Interaction (HRI) scenario. The experimental results show that our approach makes the interaction more efficient and safer.

</p>
</details>

<details><summary><b>Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing</b>
<a href="https://arxiv.org/abs/2107.06552">arxiv:2107.06552</a>
&#x1F4C8; 4 <br>
<p>Young Eun Kim, Seong-Whan Lee</p></summary>
<p>

**Abstract:** Face anti-spoofing (FAS) plays an important role in protecting face recognition systems from face representation attacks. Many recent studies in FAS have approached this problem with domain generalization technique. Domain generalization aims to increase generalization performance to better detect various types of attacks and unseen attacks. However, previous studies in this area have defined each domain simply as an anti-spoofing datasets and focused on developing learning techniques. In this paper, we proposed a method that enables network to judge its domain by itself with the clustered convolutional feature statistics from intermediate layers of the network, without labeling domains as datasets. We obtained pseudo-domain labels by not only using the network extracting features, but also using depth estimators, which were previously used only as an auxiliary task in FAS. In our experiments, we trained with three datasets and evaluated the performance with the remaining one dataset to demonstrate the effectiveness of the proposed method by conducting a total of four sets of experiments.

</p>
</details>

<details><summary><b>ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition</b>
<a href="https://arxiv.org/abs/2107.06546">arxiv:2107.06546</a>
&#x1F4C8; 4 <br>
<p>Afra Alishahi, Grzegorz Chrupała, Alejandrina Cristia, Emmanuel Dupoux, Bertrand Higy, Marvin Lavechin, Okko Räsänen, Chen Yu</p></summary>
<p>

**Abstract:** We present the visually-grounded language modelling track that was introduced in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the new track and discuss participation rules in detail. We also present the two baseline systems that were developed for this track.

</p>
</details>

<details><summary><b>Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2107.06536">arxiv:2107.06536</a>
&#x1F4C8; 4 <br>
<p>Meng Xu, Zhihao Wang, Jiasong Zhu, Xiuping Jia, Sen Jia</p></summary>
<p>

**Abstract:** Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost, thereby providing a feasible way to acquire high-resolution remote sensing images, which are difficult to obtain due to the high cost of acquisition equipment and complex weather. Clearly, image super-resolution is a severe ill-posed problem. Fortunately, with the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. In this paper, we propose a network based on the generative adversarial network (GAN) to generate high resolution remote sensing images, named the multi-attention generative adversarial network (MA-GAN). We first designed a GAN-based framework for the image SR task. The core to accomplishing the SR task is the image generator with post-upsampling that we designed. The main body of the generator contains two blocks; one is the pyramidal convolution in the residual-dense block (PCRDB), and the other is the attention-based upsample (AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB block is a module that combines multi-scale convolution and channel attention to automatically learn and adjust the scaling of the residuals for better results. The AUP block is a module that combines pixel attention (PA) to perform arbitrary multiples of upsampling. These two blocks work together to help generate better quality images. For the loss function, we design a loss function based on pixel loss and introduce both adversarial loss and feature loss to guide the generator learning. We have compared our method with several state-of-the-art methods on a remote sensing scene image dataset, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN.

</p>
</details>

<details><summary><b>Detection of Abnormal Behavior with Self-Supervised Gaze Estimation</b>
<a href="https://arxiv.org/abs/2107.06530">arxiv:2107.06530</a>
&#x1F4C8; 4 <br>
<p> Suneung-Kim, Seong-Whan Lee</p></summary>
<p>

**Abstract:** Due to the recent outbreak of COVID-19, many classes, exams, and meetings have been conducted non-face-to-face. However, the foundation for video conferencing solutions is still insufficient. So this technology has become an important issue. In particular, these technologies are essential for non-face-to-face testing, and technology dissemination is urgent. In this paper, we present a single video conferencing solution using gaze estimation in preparation for these problems. Gaze is an important cue for the tasks such as analysis of human behavior. Hence, numerous studies have been proposed to solve gaze estimation using deep learning, which is one of the most prominent methods up to date. We use these gaze estimation methods to detect abnormal behavior of video conferencing participants. Our contribution is as follows. i) We find and apply the optimal network for the gaze estimation method and apply a self-supervised method to improve accuracy. ii) For anomaly detection, we present a new dataset that aggregates the values of a new gaze, head pose, etc. iii) We train newly created data on Multi Layer Perceptron (MLP) models to detect anomaly behavior based on deep learning. We demonstrate the robustness of our method through experiments.

</p>
</details>

<details><summary><b>AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning</b>
<a href="https://arxiv.org/abs/2107.06501">arxiv:2107.06501</a>
&#x1F4C8; 4 <br>
<p>Yihao Huang, Qing Guo, Felix Juefei-Xu, Lei Ma, Weikai Miao, Yang Liu, Geguang Pu</p></summary>
<p>

**Abstract:** High-level representation-guided pixel denoising and adversarial training are independent solutions to enhance the robustness of CNNs against adversarial attacks by pre-processing input data and re-training models, respectively. Most recently, adversarial training techniques have been widely studied and improved while the pixel denoising-based method is getting less attractive. However, it is still questionable whether there exists a more advanced pixel denoising-based method and whether the combination of the two solutions benefits each other. To this end, we first comprehensively investigate two kinds of pixel denoising methods for adversarial robustness enhancement (i.e., existing additive-based and unexplored filtering-based methods) under the loss functions of image-level and semantic-level, respectively, showing that pixel-wise filtering can obtain much higher image quality (e.g., higher PSNR) as well as higher robustness (e.g., higher accuracy on adversarial examples) than existing pixel-wise additive-based method. However, we also observe that the robustness results of the filtering-based method rely on the perturbation amplitude of adversarial examples used for training. To address this problem, we propose predictive perturbation-aware & pixel-wise filtering}, where dual-perturbation filtering and an uncertainty-aware fusion module are designed and employed to automatically perceive the perturbation amplitude during the training and testing process. The method is termed as AdvFilter. Moreover, we combine adversarial pixel denoising methods with three adversarial training-based methods, hinting that considering data and models jointly is able to achieve more robust CNNs. The experiments conduct on NeurIPS-2017DEV, SVHN and CIFAR10 datasets and show advantages over enhancing CNNs' robustness, high generalization to different models and noise levels.

</p>
</details>

<details><summary><b>Integrating LSTMs and GNNs for COVID-19 Forecasting</b>
<a href="https://arxiv.org/abs/2108.10052">arxiv:2108.10052</a>
&#x1F4C8; 3 <br>
<p>Nathan Sesti, Juan Jose Garau-Luis, Edward Crawley, Bruce Cameron</p></summary>
<p>

**Abstract:** The spread of COVID-19 has coincided with the rise of Graph Neural Networks (GNNs), leading to several studies proposing their use to better forecast the evolution of the pandemic. Many such models also include Long Short Term Memory (LSTM) networks, a common tool for time series forecasting. In this work, we further investigate the integration of these two methods by implementing GNNs within the gates of an LSTM and exploiting spatial information. In addition, we introduce a skip connection which proves critical to jointly capture the spatial and temporal patterns in the data. We validate our daily COVID-19 new cases forecast model on data of 37 European nations for the last 472 days and show superior performance compared to state-of-the-art graph time series models based on mean absolute scaled error (MASE). This area of research has important applications to policy-making and we analyze its potential for pandemic resource control.

</p>
</details>

<details><summary><b>Neural Representation Learning for Scribal Hands of Linear B</b>
<a href="https://arxiv.org/abs/2108.04199">arxiv:2108.04199</a>
&#x1F4C8; 3 <br>
<p>Nikita Srivatsan, Jason Vega, Christina Skelton, Taylor Berg-Kirkpatrick</p></summary>
<p>

**Abstract:** In this work, we present an investigation into the use of neural feature extraction in performing scribal hand analysis of the Linear B writing system. While prior work has demonstrated the usefulness of strategies such as phylogenetic systematics in tracing Linear B's history, these approaches have relied on manually extracted features which can be very time consuming to define by hand. Instead we propose learning features using a fully unsupervised neural network that does not require any human annotation. Specifically our model assigns each glyph written by the same scribal hand a shared vector embedding to represent that author's stylistic patterns, and each glyph representing the same syllabic sign a shared vector embedding to represent the identifying shape of that character. Thus the properties of each image in our dataset are represented as the combination of a scribe embedding and a sign embedding. We train this model using both a reconstructive loss governed by a decoder that seeks to reproduce glyphs from their corresponding embeddings, and a discriminative loss which measures the model's ability to predict whether or not an embedding corresponds to a given image. Among the key contributions of this work we (1) present a new dataset of Linear B glyphs, annotated by scribal hand and sign type, (2) propose a neural model for disentangling properties of scribal hands from glyph shape, and (3) quantitatively evaluate the learned embeddings on findplace prediction and similarity to manually extracted features, showing improvements over simpler baseline methods.

</p>
</details>

<details><summary><b>Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining</b>
<a href="https://arxiv.org/abs/2107.07113">arxiv:2107.07113</a>
&#x1F4C8; 3 <br>
<p>Guowei Xu, Wenbiao Ding, Weiping Fu, Zhongqin Wu, Zitao Liu</p></summary>
<p>

**Abstract:** Many real-world applications involve the use of Optical Character Recognition (OCR) engines to transform handwritten images into transcripts on which downstream Natural Language Processing (NLP) models are applied. In this process, OCR engines may introduce errors and inputs to downstream NLP models become noisy. Despite that pre-trained models achieve state-of-the-art performance in many NLP benchmarks, we prove that they are not robust to noisy texts generated by real OCR engines. This greatly limits the application of NLP models in real-world scenarios. In order to improve model performance on noisy OCR transcripts, it is natural to train the NLP model on labelled noisy texts. However, in most cases there are only labelled clean texts. Since there is no handwritten pictures corresponding to the text, it is impossible to directly use the recognition model to obtain noisy labelled data. Human resources can be employed to copy texts and take pictures, but it is extremely expensive considering the size of data for model training. Consequently, we are interested in making NLP models intrinsically robust to OCR errors in a low resource manner. We propose a novel robust training framework which 1) employs simple but effective methods to directly simulate natural OCR noises from clean texts and 2) iteratively mines the hard examples from a large number of simulated samples for optimal performance. 3) To make our model learn noise-invariant representations, a stability loss is employed. Experiments on three real-world datasets show that the proposed framework boosts the robustness of pre-trained models by a large margin. We believe that this work can greatly promote the application of NLP models in actual scenarios, although the algorithm we use is simple and straightforward. We make our codes and three datasets publicly available\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.

</p>
</details>

<details><summary><b>Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features</b>
<a href="https://arxiv.org/abs/2107.07095">arxiv:2107.07095</a>
&#x1F4C8; 3 <br>
<p>Xiaomeng Ye, Ziwei Zhao, David Leake, Xizi Wang, David Crandall</p></summary>
<p>

**Abstract:** The case difference heuristic (CDH) approach is a knowledge-light method for learning case adaptation knowledge from the case base of a case-based reasoning system. Given a pair of cases, the CDH approach attributes the difference in their solutions to the difference in the problems they solve, and generates adaptation rules to adjust solutions accordingly when a retrieved case and new query have similar problem differences. As an alternative to learning adaptation rules, several researchers have applied neural networks to learn to predict solution differences from problem differences. Previous work on such approaches has assumed that the feature set describing problems is predefined. This paper investigates a two-phase process combining deep learning for feature extraction and neural network based adaptation learning from extracted features. Its performance is demonstrated in a regression task on an image data: predicting age given the image of a face. Results show that the combined process can successfully learn adaptation knowledge applicable to nonsymbolic differences in cases. The CBR system achieves slightly lower performance overall than a baseline deep network regressor, but better performance than the baseline on novel queries.

</p>
</details>

<details><summary><b>Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction</b>
<a href="https://arxiv.org/abs/2107.07056">arxiv:2107.07056</a>
&#x1F4C8; 3 <br>
<p>Zhe Huang, Ruohua Li, Kazuki Shin, Katherine Driggs-Campbell</p></summary>
<p>

**Abstract:** Multi-pedestrian trajectory prediction is an indispensable safety element of autonomous systems that interact with crowds in unstructured environments. Many recent efforts have developed trajectory prediction algorithms with focus on understanding social norms behind pedestrian motions. Yet we observe these works usually hold two assumptions that prevent them from being smoothly applied to robot applications: positions of all pedestrians are consistently tracked; the target agent pays attention to all pedestrians in the scene. The first assumption leads to biased interaction modeling with incomplete pedestrian data, and the second assumption introduces unnecessary disturbances and leads to the freezing robot problem. Thus, we propose Gumbel Social Transformer, in which an Edge Gumbel Selector samples a sparse interaction graph of partially observed pedestrians at each time step. A Node Transformer Encoder and a Masked LSTM encode the pedestrian features with the sampled sparse graphs to predict trajectories. We demonstrate that our model overcomes the potential problems caused by the assumptions, and our approach outperforms the related works in benchmark evaluation.

</p>
</details>

<details><summary><b>Expert Graphs: Synthesizing New Expertise via Collaboration</b>
<a href="https://arxiv.org/abs/2107.07054">arxiv:2107.07054</a>
&#x1F4C8; 3 <br>
<p>Bijan Mazaheri, Siddharth Jain, Jehoshua Bruck</p></summary>
<p>

**Abstract:** Consider multiple experts with overlapping expertise working on a classification problem under uncertain input. What constitutes a consistent set of opinions? How can we predict the opinions of experts on missing sub-domains? In this paper, we define a framework of to analyze this problem, termed "expert graphs." In an expert graph, vertices represent classes and edges represent binary opinions on the topics of their vertices. We derive necessary conditions for expert graph validity and use them to create "synthetic experts" which describe opinions consistent with the observed opinions of other experts. We show this framework to be equivalent to the well-studied linear ordering polytope. We show our conditions are not sufficient for describing all expert graphs on cliques, but are sufficient for cycles.

</p>
</details>

<details><summary><b>Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation</b>
<a href="https://arxiv.org/abs/2107.06941">arxiv:2107.06941</a>
&#x1F4C8; 3 <br>
<p>Lalith Sharan, Gabriele Romano, Sven Koehler, Halvar Kelm, Matthias Karck, Raffaele De Simone, Sandy Engelhardt</p></summary>
<p>

**Abstract:** The CycleGAN framework allows for unsupervised image-to-image translation of unpaired data. In a scenario of surgical training on a physical surgical simulator, this method can be used to transform endoscopic images of phantoms into images which more closely resemble the intra-operative appearance of the same surgical target structure. This can be viewed as a novel augmented reality approach, which we coined Hyperrealism in previous work. In this use case, it is of paramount importance to display objects like needles, sutures or instruments consistent in both domains while altering the style to a more tissue-like appearance. Segmentation of these objects would allow for a direct transfer, however, contouring of these, partly tiny and thin foreground objects is cumbersome and perhaps inaccurate. Instead, we propose to use landmark detection on the points when sutures pass into the tissue. This objective is directly incorporated into a CycleGAN framework by treating the performance of pre-trained detector models as an additional optimization goal. We show that a task defined on these sparse landmark labels improves consistency of synthesis by the generator network in both domains. Comparing a baseline CycleGAN architecture to our proposed extension (DetCycleGAN), mean precision (PPV) improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by +0.4743. Furthermore, it could be shown that by dataset fusion, generated intra-operative images can be leveraged as additional training data for the detection network itself. The data is released within the scope of the AdaptOR MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at https://github.com/Cardio-AI/detcyclegan_pytorch.

</p>
</details>

<details><summary><b>Performance of Bayesian linear regression in a model with mismatch</b>
<a href="https://arxiv.org/abs/2107.06936">arxiv:2107.06936</a>
&#x1F4C8; 3 <br>
<p>Jean Barbier, Wei-Kuo Chen, Dmitry Panchenko, Manuel Sáenz</p></summary>
<p>

**Abstract:** In this paper we analyze, for a model of linear regression with gaussian covariates, the performance of a Bayesian estimator given by the mean of a log-concave posterior distribution with gaussian prior, in the high-dimensional limit where the number of samples and the covariates' dimension are large and proportional. Although the high-dimensional analysis of Bayesian estimators has been previously studied for Bayesian-optimal linear regression where the correct posterior is used for inference, much less is known when there is a mismatch. Here we consider a model in which the responses are corrupted by gaussian noise and are known to be generated as linear combinations of the covariates, but the distributions of the ground-truth regression coefficients and of the noise are unknown. This regression task can be rephrased as a statistical mechanics model known as the Gardner spin glass, an analogy which we exploit. Using a leave-one-out approach we characterize the mean-square error for the regression coefficients. We also derive the log-normalizing constant of the posterior. Similar models have been studied by Shcherbina and Tirozzi and by Talagrand, but our arguments are much more straightforward. An interesting consequence of our analysis is that in the quadratic loss case, the performance of the Bayesian estimator is independent of a global "temperature" hyperparameter and matches the ridge estimator: sampling and optimizing are equally good.

</p>
</details>

<details><summary><b>High-Speed and High-Quality Text-to-Lip Generation</b>
<a href="https://arxiv.org/abs/2107.06831">arxiv:2107.06831</a>
&#x1F4C8; 3 <br>
<p>Jinglin Liu, Zhiying Zhu, Yi Ren, Zhou Zhao</p></summary>
<p>

**Abstract:** As a key component of talking face generation, lip movements generation determines the naturalness and coherence of the generated talking face video. Prior literature mainly focuses on speech-to-lip generation while there is a paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing end-to-end works depend on the attention mechanism and autoregressive (AR) decoding manner. However, the AR decoding manner generates current lip frame conditioned on frames generated previously, which inherently hinders the inference speed, and also has a detrimental effect on the quality of generated lip frames due to error propagation. This encourages the research of parallel T2L generation. In this work, we propose a novel parallel decoding model for high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we predict the duration of the encoded linguistic features and model the target lip frames conditioned on the encoded linguistic features with their duration in a non-autoregressive manner. Furthermore, we incorporate the structural similarity index loss and adversarial learning to improve perceptual quality of generated lip frames and alleviate the blurry prediction problem. Extensive experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L generates lip movements with competitive quality compared with the state-of-the-art AR T2L model DualLip and exceeds the baseline AR model TransformerT2L by a notable margin benefiting from the mitigation of the error propagation problem; and 2) exhibits distinct superiority in inference speed (an average speedup of 19$\times$ than DualLip on TCD-TIMIT).

</p>
</details>

<details><summary><b>RCDNet: An Interpretable Rain Convolutional Dictionary Network for Single Image Deraining</b>
<a href="https://arxiv.org/abs/2107.06808">arxiv:2107.06808</a>
&#x1F4C8; 3 <br>
<p>Hong Wang, Qi Xie, Qian Zhao, Yong Liang, Deyu Meng</p></summary>
<p>

**Abstract:** As a common weather, rain streaks adversely degrade the image quality. Hence, removing rains from an image has become an important issue in the field. To handle such an ill-posed single image deraining task, in this paper, we specifically build a novel deep architecture, called rain convolutional dictionary network (RCDNet), which embeds the intrinsic priors of rain streaks and has clear interpretability. In specific, we first establish a RCD model for representing rain streaks and utilize the proximal gradient descent technique to design an iterative algorithm only containing simple operators for solving the model. By unfolding it, we then build the RCDNet in which every network module has clear physical meanings and corresponds to each operation involved in the algorithm. This good interpretability greatly facilitates an easy visualization and analysis on what happens inside the network and why it works well in inference process. Moreover, taking into account the domain gap issue in real scenarios, we further design a novel dynamic RCDNet, where the rain kernels can be dynamically inferred corresponding to input rainy images and then help shrink the space for rain layer estimation with few rain maps so as to ensure a fine generalization performance in the inconsistent scenarios of rain types between training and testing data. By end-to-end training such an interpretable network, all involved rain kernels and proximal operators can be automatically extracted, faithfully characterizing the features of both rain and clean background layers, and thus naturally lead to better deraining performance. Comprehensive experiments substantiate the superiority of our method, especially on its well generality to diverse testing scenarios and good interpretability for all its modules. Code is available in \emph{\url{https://github.com/hongwang01/DRCDNet}}.

</p>
</details>

<details><summary><b>BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews</b>
<a href="https://arxiv.org/abs/2107.06802">arxiv:2107.06802</a>
&#x1F4C8; 3 <br>
<p>Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Haftittah Wuswilahaken DW, Fitra Abdurrachman Bachtiar, Novanto Yudistira</p></summary>
<p>

**Abstract:** User reviews have an essential role in the success of the developed mobile apps. User reviews in the textual form are unstructured data, creating a very high complexity when processed for sentiment analysis. Previous approaches that have been used often ignore the context of reviews. In addition, the relatively small data makes the model overfitting. A new approach, BERT, has been introduced as a transfer learning model with a pre-trained model that has previously been trained to have a better context representation. This study examines the effectiveness of fine-tuning BERT for sentiment analysis using two different pre-trained models. Besides the multilingual pre-trained model, we use the pre-trained model that only has been trained in Indonesian. The dataset used is Indonesian user reviews of the ten best apps in 2020 in Google Play sites. We also perform hyper-parameter tuning to find the optimum trained model. Two training data labeling approaches were also tested to determine the effectiveness of the model, which is score-based and lexicon-based. The experimental results show that pre-trained models trained in Indonesian have better average accuracy on lexicon-based data. The pre-trained Indonesian model highest accuracy is 84%, with 25 epochs and a training time of 24 minutes. These results are better than all of the machine learning and multilingual pre-trained models.

</p>
</details>

<details><summary><b>Indonesia's Fake News Detection using Transformer Network</b>
<a href="https://arxiv.org/abs/2107.06796">arxiv:2107.06796</a>
&#x1F4C8; 3 <br>
<p>Aisyah Awalina, Jibran Fawaid, Rifky Yunus Krisnabayu, Novanto Yudistira</p></summary>
<p>

**Abstract:** Fake news is a problem faced by society in this era. It is not rare for fake news to cause provocation and problem for the people. Indonesia, as a country with the 4th largest population, has a problem in dealing with fake news. More than 30% of rural and urban population are deceived by this fake news problem. As we have been studying, there is only few literatures on preventing the spread of fake news in Bahasa Indonesia. So, this research is conducted to prevent these problems. The dataset used in this research was obtained from a news portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on this page, we got 1116 data consisting of valid news and fake news. The dataset can be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This dataset will be combined with other available datasets. The methods used are CNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This research shows that the BERT method with Transformer Network has the best results with an accuracy of up to 90%.

</p>
</details>

<details><summary><b>Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities</b>
<a href="https://arxiv.org/abs/2107.06767">arxiv:2107.06767</a>
&#x1F4C8; 3 <br>
<p>Miklos Z. Racz, Anirudh Sridhar</p></summary>
<p>

**Abstract:** We consider the task of learning latent community structure from multiple correlated networks. First, we study the problem of learning the latent vertex correspondence between two edge-correlated stochastic block models, focusing on the regime where the average degree is logarithmic in the number of vertices. We derive the precise information-theoretic threshold for exact recovery: above the threshold there exists an estimator that outputs the true correspondence with probability close to 1, while below it no estimator can recover the true correspondence with probability bounded away from 0. As an application of our results, we show how one can exactly recover the latent communities using multiple correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph.

</p>
</details>

<details><summary><b>M5 Competition Uncertainty: Overdispersion, distributional forecasting, GAMLSS and beyond</b>
<a href="https://arxiv.org/abs/2107.06675">arxiv:2107.06675</a>
&#x1F4C8; 3 <br>
<p>Florian Ziel</p></summary>
<p>

**Abstract:** The M5 competition uncertainty track aims for probabilistic forecasting of sales of thousands of Walmart retail goods. We show that the M5 competition data faces strong overdispersion and sporadic demand, especially zero demand. We discuss resulting modeling issues concerning adequate probabilistic forecasting of such count data processes. Unfortunately, the majority of popular prediction methods used in the M5 competition (e.g. lightgbm and xgboost GBMs) fails to address the data characteristics due to the considered objective functions. The distributional forecasting provides a suitable modeling approach for to the overcome those problems. The GAMLSS framework allows flexible probabilistic forecasting using low dimensional distributions. We illustrate, how the GAMLSS approach can be applied for the M5 competition data by modeling the location and scale parameter of various distributions, e.g. the negative binomial distribution. Finally, we discuss software packages for distributional modeling and their drawback, like the R package gamlss with its package extensions, and (deep) distributional forecasting libraries such as TensorFlow Probability.

</p>
</details>

<details><summary><b>Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks</b>
<a href="https://arxiv.org/abs/2107.06661">arxiv:2107.06661</a>
&#x1F4C8; 3 <br>
<p>Ingmar Schubert, Ozgur S. Oguz, Marc Toussaint</p></summary>
<p>

**Abstract:** In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.

</p>
</details>

<details><summary><b>Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition</b>
<a href="https://arxiv.org/abs/2107.06538">arxiv:2107.06538</a>
&#x1F4C8; 3 <br>
<p>Xinda Liu, Lili Wang, Xiaoguang Han</p></summary>
<p>

**Abstract:** Fine-grained image recognition is challenging because discriminative clues are usually fragmented, whether from a single image or multiple images. Despite their significant improvements, most existing methods still focus on the most discriminative parts from a single image, ignoring informative details in other regions and lacking consideration of clues from other associated images. In this paper, we analyze the difficulties of fine-grained image recognition from a new perspective and propose a transformer architecture with the peak suppression module and knowledge guidance module, which respects the diversification of discriminative features in a single image and the aggregation of discriminative clues among multiple images. Specifically, the peak suppression module first utilizes a linear projection to convert the input image into sequential tokens. It then blocks the token based on the attention response generated by the transformer encoder. This module penalizes the attention to the most discriminative parts in the feature learning process, therefore, enhancing the information exploitation of the neglected regions. The knowledge guidance module compares the image-based representation generated from the peak suppression module with the learnable knowledge embedding set to obtain the knowledge response coefficients. Afterwards, it formalizes the knowledge learning as a classification problem using response coefficients as the classification scores. Knowledge embeddings and image-based representations are updated during training so that the knowledge embedding includes discriminative clues for different images. Finally, we incorporate the acquired knowledge embeddings into the image-based representations as comprehensive representations, leading to significantly higher performance. Extensive evaluations on the six popular datasets demonstrate the advantage of the proposed method.

</p>
</details>

<details><summary><b>Accelerating Distributed K-FAC with Smart Parallelism of Computing and Communication Tasks</b>
<a href="https://arxiv.org/abs/2107.06533">arxiv:2107.06533</a>
&#x1F4C8; 3 <br>
<p>Shaohuai Shi, Lin Zhang, Bo Li</p></summary>
<p>

**Abstract:** Distributed training with synchronous stochastic gradient descent (SGD) on GPU clusters has been widely used to accelerate the training process of deep models. However, SGD only utilizes the first-order gradient in model parameter updates, which may take days or weeks. Recent studies have successfully exploited approximate second-order information to speed up the training process, in which the Kronecker-Factored Approximate Curvature (KFAC) emerges as one of the most efficient approximation algorithms for training deep models. Yet, when leveraging GPU clusters to train models with distributed KFAC (D-KFAC), it incurs extensive computation as well as introduces extra communications during each iteration. In this work, we propose D-KFAC (SPD-KFAC) with smart parallelism of computing and communication tasks to reduce the iteration time. Specifically, 1) we first characterize the performance bottlenecks of D-KFAC, 2) we design and implement a pipelining mechanism for Kronecker factors computation and communication with dynamic tensor fusion, and 3) we develop a load balancing placement for inverting multiple matrices on GPU clusters. We conduct real-world experiments on a 64-GPU cluster with 100Gb/s InfiniBand interconnect. Experimental results show that our proposed SPD-KFAC training scheme can achieve 10%-35% improvement over state-of-the-art algorithms.

</p>
</details>

<details><summary><b>Remote Sensing and Machine Learning for Food Crop Production Data in Africa Post-COVID-19</b>
<a href="https://arxiv.org/abs/2108.10054">arxiv:2108.10054</a>
&#x1F4C8; 2 <br>
<p>Racine Ly, Khadim Dia, Mariam Diallo</p></summary>
<p>

**Abstract:** In the agricultural sector, the COVID-19 threatens to lead to a severe food security crisis in the region, with disruptions in the food supply chain and agricultural production expected to contract between 2.6% and 7%. From the food crop production side, the travel bans and border closures, the late reception and the use of agricultural inputs such as imported seeds, fertilizers, and pesticides could lead to poor food crop production performances. Another layer of disruption introduced by the mobility restriction measures is the scarcity of agricultural workers, mainly seasonal workers. The lockdown measures and border closures limit seasonal workers' availability to get to the farm on time for planting and harvesting activities. Moreover, most of the imported agricultural inputs travel by air, which the pandemic has heavily impacted. Such transportation disruptions can also negatively affect the food crop production system.
  This chapter assesses food crop production levels in 2020 -- before the harvesting period -- in all African regions and four staples such as maize, cassava, rice, and wheat. The production levels are predicted using the combination of biogeophysical remote sensing data retrieved from satellite images and machine learning artificial neural networks (ANNs) technique. The remote sensing products are used as input variables and the ANNs as the predictive modeling framework. The input remote sensing products are the Normalized Difference Vegetation Index (NDVI), the daytime Land Surface Temperature (LST), rainfall data, and agricultural lands' Evapotranspiration (ET). The output maps and data are made publicly available on a web-based platform, AAgWa (Africa Agriculture Watch, www.aagwa.org), to facilitate access to such information to policymakers, deciders, and other stakeholders.

</p>
</details>

<details><summary><b>An Efficient DP-SGD Mechanism for Large Scale NLP Models</b>
<a href="https://arxiv.org/abs/2107.14586">arxiv:2107.14586</a>
&#x1F4C8; 2 <br>
<p>Christophe Dupuy, Radhika Arava, Rahul Gupta, Anna Rumshisky</p></summary>
<p>

**Abstract:** Recent advances in deep learning have drastically improved performance on many Natural Language Understanding (NLU) tasks. However, the data used to train NLU models may contain private information such as addresses or phone numbers, particularly when drawn from human subjects. It is desirable that underlying models do not expose private information contained in the training data. Differentially Private Stochastic Gradient Descent (DP-SGD) has been proposed as a mechanism to build privacy-preserving models. However, DP-SGD can be prohibitively slow to train. In this work, we propose a more efficient DP-SGD for training using a GPU infrastructure and apply it to fine-tuning models based on LSTM and transformer architectures. We report faster training times, alongside accuracy, theoretical privacy guarantees and success of Membership inference attacks for our models and observe that fine-tuning with proposed variant of DP-SGD can yield competitive models without significant degradation in training time and improvement in privacy protection. We also make observations such as looser theoretical $ε, δ$ can translate into significant practical privacy gains.

</p>
</details>

<details><summary><b>Ethical AI for Social Good</b>
<a href="https://arxiv.org/abs/2107.14044">arxiv:2107.14044</a>
&#x1F4C8; 2 <br>
<p>Ramya Akula, Ivan Garibay</p></summary>
<p>

**Abstract:** The concept of AI for Social Good(AI4SG) is gaining momentum in both information societies and the AI community. Through all the advancement of AI-based solutions, it can solve societal issues effectively. To date, however, there is only a rudimentary grasp of what constitutes AI socially beneficial in principle, what constitutes AI4SG in reality, and what are the policies and regulations needed to ensure it. This paper fills the vacuum by addressing the ethical aspects that are critical for future AI4SG efforts. Some of these characteristics are new to AI, while others have greater importance due to its usage.

</p>
</details>

<details><summary><b>Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions</b>
<a href="https://arxiv.org/abs/2107.13394">arxiv:2107.13394</a>
&#x1F4C8; 2 <br>
<p>Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Susan P Fisher-Hoch, Joseph B Mccormic</p></summary>
<p>

**Abstract:** The emergence and progression of multiple chronic conditions (MCC) over time often form a dynamic network that depends on patient's modifiable risk factors and their interaction with non-modifiable risk factors and existing conditions. Continuous time Bayesian networks (CTBNs) are effective methods for modeling the complex network of MCC relationships over time. However, CTBNs are not able to effectively formulate the dynamic impact of patient's modifiable risk factors on the emergence and progression of MCC. Considering a functional CTBN (FCTBN) to represent the underlying structure of the MCC relationships with respect to individuals' risk factors and existing conditions, we propose a nonlinear state-space model based on Extended Kalman filter (EKF) to capture the dynamics of the patients' modifiable risk factors and existing conditions on the MCC evolution over time. We also develop a tensor control chart to dynamically monitor the effect of changes in the modifiable risk factors of individual patients on the risk of new chronic conditions emergence. We validate the proposed approach based on a combination of simulation and real data from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC) over multiple years. The dataset examines the emergence of 5 chronic conditions (Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension) based on 4 modifiable risk factors representing lifestyle behaviors (Diet, Exercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors, including demographic information (Age, Gender, Education). The results demonstrate the effectiveness of the proposed methodology for dynamic prediction and monitoring of the risk of MCC emergence in individual patients.

</p>
</details>

<details><summary><b>Multiclass Permanent Magnets Superstructure for Indoor Localization using Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2107.07425">arxiv:2107.07425</a>
&#x1F4C8; 2 <br>
<p>Amir Ivry, Elad Fisher, Roger Alimi, Idan Mosseri, Kanna Nahir</p></summary>
<p>

**Abstract:** Smartphones have become a popular tool for indoor localization and position estimation of users. Existing solutions mainly employ Wi-Fi, RFID, and magnetic sensing techniques to track movements in crowded venues. These are highly sensitive to magnetic clutters and depend on local ambient magnetic fields, which frequently degrades their performance. Also, these techniques often require pre-known mapping surveys of the area, or the presence of active beacons, which are not always available. We embed small-volume and large-moment magnets in pre-known locations and arrange them in specific geometric constellations that create magnetic superstructure patterns of supervised magnetic signatures. These signatures constitute an unambiguous magnetic environment with respect to the moving sensor carrier. The localization algorithm learns the unique patterns of the scattered magnets during training and detects them from the ongoing streaming of data during localization. Our contribution is twofold. First, we deploy passive permanent magnets that do not require a power supply, in contrast to active magnetic transmitters. Second, we perform localization based on smartphone motion rather than on static positioning of the magnetometer. In our previous study, we considered a single superstructure pattern. Here, we present an extended version of that algorithm for multi-superstructure localization, which covers a broader localization area of the user. Experimental results demonstrate localization accuracy of 95% with a mean localization error of less than 1m using artificial intelligence.

</p>
</details>

<details><summary><b>Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice</b>
<a href="https://arxiv.org/abs/2107.07376">arxiv:2107.07376</a>
&#x1F4C8; 2 <br>
<p>Elaine Pimentel, Enrico Tassi</p></summary>
<p>

**Abstract:** Logical frameworks and meta-languages form a common substrate for representing, implementing and reasoning about a wide variety of deductive systems of interest in logic and computer science. Their design, implementation and their use in reasoning tasks, ranging from the correctness of software to the properties of formal systems, have been the focus of considerable research over the last two decades. This workshop brings together designers, implementors and practitioners to discuss various aspects impinging on the structure and utility of logical frameworks, including the treatment of variable binding, inductive and co-inductive reasoning techniques and the expressiveness and lucidity of the reasoning process.

</p>
</details>

<details><summary><b>Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis</b>
<a href="https://arxiv.org/abs/2107.07116">arxiv:2107.07116</a>
&#x1F4C8; 2 <br>
<p>Feng Shi, Chonghan Lee, Mohammad Khairul Bashar, Nikhil Shukla, Song-Chun Zhu, Vijaykrishnan Narayanan</p></summary>
<p>

**Abstract:** CNF-based SAT and MaxSAT solvers are central to logic synthesis and verification systems. The increasing popularity of these constraint problems in electronic design automation encourages studies on different SAT problems and their properties for further computational efficiency. There has been both theoretical and practical success of modern Conflict-driven clause learning SAT solvers, which allows solving very large industrial instances in a relatively short amount of time. Recently, machine learning approaches provide a new dimension to solving this challenging problem. Neural symbolic models could serve as generic solvers that can be specialized for specific domains based on data without any changes to the structure of the model. In this work, we propose a one-shot model derived from the Transformer architecture to solve the MaxSAT problem, which is the optimization version of SAT where the goal is to satisfy the maximum number of clauses. Our model has a scale-free structure which could process varying size of instances. We use meta-path and self-attention mechanism to capture interactions among homogeneous nodes. We adopt cross-attention mechanisms on the bipartite graph to capture interactions among heterogeneous nodes. We further apply an iterative algorithm to our model to satisfy additional clauses, enabling a solution approaching that of an exact-SAT problem. The attention mechanisms leverage the parallelism for speedup. Our evaluation indicates improved speedup compared to heuristic approaches and improved completion rate compared to machine learning approaches.

</p>
</details>

<details><summary><b>Principal component analysis for Gaussian process posteriors</b>
<a href="https://arxiv.org/abs/2107.07115">arxiv:2107.07115</a>
&#x1F4C8; 2 <br>
<p>Hideaki Ishibashi, Shotaro Akaho</p></summary>
<p>

**Abstract:** This paper proposes an extension of principal component analysis for Gaussian process posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional space of GP posteriors, it can be used for meta-learning, which is a framework for improving the precision of a new task by estimating a structure of a set of tasks. The issue is how to define a structure of a set of GPs with an infinite-dimensional parameter, such as coordinate system and a divergence. In this study, we reduce the infiniteness of GP to the finite-dimensional case under the information geometrical framework by considering a space of GP posteriors that has the same prior. In addition, we propose an approximation method of GP-PCA based on variational inference and demonstrate the effectiveness of GP-PCA as meta-learning through experiments.

</p>
</details>

<details><summary><b>Neural Code Summarization: How Far Are We?</b>
<a href="https://arxiv.org/abs/2107.07112">arxiv:2107.07112</a>
&#x1F4C8; 2 <br>
<p>Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, Hongbin Sun</p></summary>
<p>

**Abstract:** Source code summaries are important for the comprehension and maintenance of programs. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem, in this paper, we conduct a systematic and in-depth analysis of five state-of-the-art neural source code summarization models on three widely used datasets. Our evaluation results suggest that: (1) The BLEU metric, which is widely used by existing work for evaluating the performance of the summarization models, has many variants. Ignoring the differences among the BLEU variants could affect the validity of the claimed results. Furthermore, we discover an important, previously unknown bug about BLEU calculation in a commonly-used software package. (2) Code pre-processing choices can have a large impact on the summarization performance, therefore they should not be ignored. (3) Some important characteristics of datasets (corpus size, data splitting method, and duplication ratio) have a significant impact on model evaluation. Based on the experimental results, we give some actionable guidelines on more systematic ways for evaluating code summarization and choosing the best method in different scenarios. We also suggest possible future research directions. We believe that our results can be of great help for practitioners and researchers in this interesting area.

</p>
</details>

<details><summary><b>Continuous-variable neural-network quantum states and the quantum rotor model</b>
<a href="https://arxiv.org/abs/2107.07105">arxiv:2107.07105</a>
&#x1F4C8; 2 <br>
<p>James Stokes, Saibal De, Shravan Veerapaneni, Giuseppe Carleo</p></summary>
<p>

**Abstract:** We initiate the study of neural-network quantum state algorithms for analyzing continuous-variable lattice quantum systems in first quantization. A simple family of continuous-variable trial wavefunctons is introduced which naturally generalizes the restricted Boltzmann machine (RBM) wavefunction introduced for analyzing quantum spin systems. By virtue of its simplicity, the same variational Monte Carlo training algorithms that have been developed for ground state determination and time evolution of spin systems have natural analogues in the continuum. We offer a proof of principle demonstration in the context of ground state determination of a stoquastic quantum rotor Hamiltonian. Results are compared against those obtained from partial differential equation (PDE) based scalable eigensolvers. This study serves as a benchmark against which future investigation of continuous-variable neural quantum states can be compared, and points to the need to consider deep network architectures and more sophisticated training algorithms.

</p>
</details>

<details><summary><b>DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder</b>
<a href="https://arxiv.org/abs/2107.07064">arxiv:2107.07064</a>
&#x1F4C8; 2 <br>
<p>Dae-Hyeok Lee, Sung-Jin Kim, Seong-Whan Lee</p></summary>
<p>

**Abstract:** Brain-computer interface (BCI) is one of the tools which enables the communication between humans and devices by reflecting intention and status of humans. With the development of artificial intelligence, the interest in communication between humans and drones using electroencephalogram (EEG) is increased. Especially, in the case of controlling drone swarms such as direction or formation, there are many advantages compared with controlling a drone unit. Imagined speech is one of the endogenous BCI paradigms, which can identify intentions of users. When conducting imagined speech, the users imagine the pronunciation as if actually speaking. In contrast, overt speech is a task in which the users directly pronounce the words. When controlling drone swarms using imagined speech, complex commands can be delivered more intuitively, but decoding performance is lower than that of other endogenous BCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of overt speech for imagined speech-based EEG signals classification. To the best of our knowledge, this study is the first attempt to use EEG features of overt speech to decode imagined speech-based EEG signals with an autoencoder. A total of eight subjects participated in the experiment. When classifying four words, the average accuracy of the DAL was 48.41%. In addition, when comparing the performance between w/o and w/ EEG features of overt speech, there was a performance improvement of 7.42% when including EEG features of overt speech. Hence, we demonstrated that EEG features of overt speech could improve the decoding performance of imagined speech.

</p>
</details>

<details><summary><b>Hybrid Bayesian Neural Networks with Functional Probabilistic Layers</b>
<a href="https://arxiv.org/abs/2107.07014">arxiv:2107.07014</a>
&#x1F4C8; 2 <br>
<p>Daniel T. Chang</p></summary>
<p>

**Abstract:** Bayesian neural networks provide a direct and natural way to extend standard deep neural networks to support probabilistic deep learning through the use of probabilistic layers that, traditionally, encode weight (and bias) uncertainty. In particular, hybrid Bayesian neural networks utilize standard deterministic layers together with few probabilistic layers judicially positioned in the networks for uncertainty estimation. A major aspect and benefit of Bayesian inference is that priors, in principle, provide the means to encode prior knowledge for use in inference and prediction. However, it is difficult to specify priors on weights since the weights have no intuitive interpretation. Further, the relationships of priors on weights to the functions computed by networks are difficult to characterize. In contrast, functions are intuitive to interpret and are direct since they map inputs to outputs. Therefore, it is natural to specify priors on functions to encode prior knowledge, and to use them in inference and prediction based on functions. To support this, we propose hybrid Bayesian neural networks with functional probabilistic layers that encode function (and activation) uncertainty. We discuss their foundations in functional Bayesian inference, functional variational inference, sparse Gaussian processes, and sparse variational Gaussian processes. We further perform few proof-of-concept experiments using GPflus, a new library that provides Gaussian process layers and supports their use with deterministic Keras layers to form hybrid neural network and Gaussian process models.

</p>
</details>

<details><summary><b>Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests</b>
<a href="https://arxiv.org/abs/2107.06929">arxiv:2107.06929</a>
&#x1F4C8; 2 <br>
<p>Sean Kulinski, Saurabh Bagchi, David I. Inouye</p></summary>
<p>

**Abstract:** While previous distribution shift detection approaches can identify if a shift has occurred, these approaches cannot localize which specific features have caused a distribution shift -- a critical step in diagnosing or fixing any underlying issue. For example, in military sensor networks, users will want to detect when one or more of the sensors has been compromised, and critically, they will want to know which specific sensors might be compromised. Thus, we first define a formalization of this problem as multiple conditional distribution hypothesis tests and propose both non-parametric and parametric statistical tests. For both efficiency and flexibility, we then propose to use a test statistic based on the density model score function (i.e. gradient with respect to the input) -- which can easily compute test statistics for all dimensions in a single forward and backward pass. Any density model could be used for computing the necessary statistics including deep density models such as normalizing flows or autoregressive models. We additionally develop methods for identifying when and where a shift occurs in multivariate time-series data and show results for multiple scenarios using realistic attack models on both simulated and real world data.

</p>
</details>

<details><summary><b>Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network</b>
<a href="https://arxiv.org/abs/2107.06846">arxiv:2107.06846</a>
&#x1F4C8; 2 <br>
<p>Daniel Salles Civitarese, Daniela Szwarcman, Bianca Zadrozny, Campbell Watson</p></summary>
<p>

**Abstract:** An impact of climate change is the increase in frequency and intensity of extreme precipitation events. However, confidently predicting the likelihood of extreme precipitation at seasonal scales remains an outstanding challenge. Here, we present an approach to forecasting the quantiles of the maximum daily precipitation in each week up to six months ahead using the temporal fusion transformer (TFT) model. Through experiments in two regions, we compare TFT predictions with those of two baselines: climatology and a calibrated ECMWF SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk at six month lead time, the TFT predictions significantly outperform those from S5 and show an overall small improvement compared to climatology. The TFT also responds positively to departures from normal that climatology cannot.

</p>
</details>

<details><summary><b>Meta-Optimization of Deep CNN for Image Denoising Using LSTM</b>
<a href="https://arxiv.org/abs/2107.06845">arxiv:2107.06845</a>
&#x1F4C8; 2 <br>
<p>Basit O. Alawode, Motaz Alfarraj</p></summary>
<p>

**Abstract:** The recent application of deep learning (DL) to various tasks has seen the performance of classical techniques surpassed by their DL-based counterparts. As a result, DL has equally seen application in the removal of noise from images. In particular, the use of deep feed-forward convolutional neural networks (DnCNNs) has been investigated for denoising. It utilizes advances in DL techniques such as deep architecture, residual learning, and batch normalization to achieve better denoising performance when compared with the other classical state-of-the-art denoising algorithms. However, its deep architecture resulted in a huge set of trainable parameters. Meta-optimization is a training approach of enabling algorithms to learn to train themselves by themselves. Training algorithms using meta-optimizers have been shown to enable algorithms to achieve better performance when compared to the classical gradient descent-based training approach. In this work, we investigate the application of the meta-optimization training approach to the DnCNN denoising algorithm to enhance its denoising capability. Our preliminary experiments on simpler algorithms reveal the prospects of utilizing the meta-optimization training approach towards the enhancement of the DnCNN denoising capability.

</p>
</details>

<details><summary><b>Deep Adaptive Multi-Intention Inverse Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.06692">arxiv:2107.06692</a>
&#x1F4C8; 2 <br>
<p>Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, Gijs Dubbelman</p></summary>
<p>

**Abstract:** This paper presents a deep Inverse Reinforcement Learning (IRL) framework that can learn an a priori unknown number of nonlinear reward functions from unlabeled experts' demonstrations. For this purpose, we employ the tools from Dirichlet processes and propose an adaptive approach to simultaneously account for both complex and unknown number of reward functions. Using the conditional maximum entropy principle, we model the experts' multi-intention behaviors as a mixture of latent intention distributions and derive two algorithms to estimate the parameters of the deep reward network along with the number of experts' intentions from unlabeled demonstrations. The proposed algorithms are evaluated on three benchmarks, two of which have been specifically extended in this study for multi-intention IRL, and compared with well-known baselines. We demonstrate through several experiments the advantages of our algorithms over the existing approaches and the benefits of online inferring, rather than fixing beforehand, the number of expert's intentions.

</p>
</details>

<details><summary><b>Oblivious sketching for logistic regression</b>
<a href="https://arxiv.org/abs/2107.06615">arxiv:2107.06615</a>
&#x1F4C8; 2 <br>
<p>Alexander Munteanu, Simon Omlor, David Woodruff</p></summary>
<p>

**Abstract:** What guarantees are possible for solving logistic regression in one pass over a data stream? To answer this question, we present the first data oblivious sketch for logistic regression. Our sketch can be computed in input sparsity time over a turnstile data stream and reduces the size of a $d$-dimensional data set from $n$ to only $\operatorname{poly}(μd\log n)$ weighted points, where $μ$ is a useful parameter which captures the complexity of compressing the data. Solving (weighted) logistic regression on the sketch gives an $O(\log n)$-approximation to the original problem on the full data set. We also show how to obtain an $O(1)$-approximation with slight modifications. Our sketches are fast, simple, easy to implement, and our experiments demonstrate their practicality.

</p>
</details>

<details><summary><b>A Distance Measure for Privacy-preserving Process Mining based on Feature Learning</b>
<a href="https://arxiv.org/abs/2107.06578">arxiv:2107.06578</a>
&#x1F4C8; 2 <br>
<p>Fabian Rösel, Stephan A. Fahrenkrog-Petersen, Han van der Aa, Matthias Weidlich</p></summary>
<p>

**Abstract:** To enable process analysis based on an event log without compromising the privacy of individuals involved in process execution, a log may be anonymized. Such anonymization strives to transform a log so that it satisfies provable privacy guarantees, while largely maintaining its utility for process analysis. Existing techniques perform anonymization using simple, syntactic measures to identify suitable transformation operations. This way, the semantics of the activities referenced by the events in a trace are neglected, potentially leading to transformations in which events of unrelated activities are merged. To avoid this and incorporate the semantics of activities during anonymization, we propose to instead incorporate a distance measure based on feature learning. Specifically, we show how embeddings of events enable the definition of a distance measure for traces to guide event log anonymization. Our experiments with real-world data indicate that anonymization using this measure, compared to a syntactic one, yields logs that are closer to the original log in various dimensions and, hence, have higher utility for process analysis.

</p>
</details>

<details><summary><b>A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer</b>
<a href="https://arxiv.org/abs/2107.06573">arxiv:2107.06573</a>
&#x1F4C8; 2 <br>
<p>Wenqi Zeng, Siqin Cao, Xuhui Huang, Yuan Yao</p></summary>
<p>

**Abstract:** Recurrent neural networks for language models like long short-term memory (LSTM) have been utilized as a tool for modeling and predicting long term dynamics of complex stochastic molecular systems. Recently successful examples on learning slow dynamics by LSTM are given with simulation data of low dimensional reaction coordinate. However, in this report we show that the following three key factors significantly affect the performance of language model learning, namely dimensionality of reaction coordinates, temporal resolution and state partition. When applying recurrent neural networks to molecular dynamics simulation trajectories of high dimensionality, we find that rare events corresponding to the slow dynamics might be obscured by other faster dynamics of the system, and cannot be efficiently learned. Under such conditions, we find that coarse graining the conformational space into metastable states and removing recrossing events when estimating transition probabilities between states could greatly help improve the accuracy of slow dynamics learning in molecular dynamics. Moreover, we also explore other models like Transformer, which do not show superior performance than LSTM in overcoming these issues. Therefore, to learn rare events of slow molecular dynamics by LSTM and Transformer, it is critical to choose proper temporal resolution (i.e., saving intervals of MD simulation trajectories) and state partition in high resolution data, since deep neural network models might not automatically disentangle slow dynamics from fast dynamics when both are present in data influencing each other.

</p>
</details>

<details><summary><b>Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization</b>
<a href="https://arxiv.org/abs/2107.06534">arxiv:2107.06534</a>
&#x1F4C8; 2 <br>
<p>Zeeshan Akhtar, Ketan Rajawat</p></summary>
<p>

**Abstract:** This paper considers stochastic convex optimization problems with two sets of constraints: (a) deterministic constraints on the domain of the optimization variable, which are difficult to project onto; and (b) deterministic or stochastic constraints that admit efficient projection. Problems of this form arise frequently in the context of semidefinite programming as well as when various NP-hard problems are solved approximately via semidefinite relaxation. Since projection onto the first set of constraints is difficult, it becomes necessary to explore projection-free algorithms, such as the stochastic Frank-Wolfe (FW) algorithm. On the other hand, the second set of constraints cannot be handled in the same way, and must be incorporated as an indicator function within the objective function, thereby complicating the application of FW methods. Similar problems have been studied before, and solved using first-order stochastic FW algorithms by applying homotopy and Nesterov's smoothing techniques to the indicator function. This work improves upon these existing results and puts forth momentum-based first-order methods that yield improved convergence rates, at par with the best known rates for problems without the second set of constraints. Zeroth-order variants of the proposed algorithms are also developed and again improve upon the state-of-the-art rate results. The efficacy of the proposed algorithms is tested on relevant applications of sparse matrix estimation, clustering via semidefinite relaxation, and uniform sparsest cut problem.

</p>
</details>

<details><summary><b>RCLC: ROI-based joint conventional and learning video compression</b>
<a href="https://arxiv.org/abs/2107.06492">arxiv:2107.06492</a>
&#x1F4C8; 2 <br>
<p>Trinh Man Hoang, Jinjia Zhou</p></summary>
<p>

**Abstract:** COVID-19 leads to the high demand for remote interactive systems ever seen. One of the key elements of these systems is video streaming, which requires a very high network bandwidth due to its specific real-time demand, especially with high-resolution video. Existing video compression methods are struggling in the trade-off between video quality and the speed requirement. Addressed that the background information rarely changes in most remote meeting cases, we introduce a Region-Of-Interests (ROI) based video compression framework (named RCLC) that leverages the cutting-edge learning-based and conventional technologies. In RCLC, each coming frame is marked as a background-updating (BU) or ROI-updating (RU) frame. By applying the conventional video codec, the BU frame is compressed with low-quality and high-compression, while the ROI from RU-frame is compressed with high-quality and low-compression. The learning-based methods are applied to detect the ROI, blend background-ROI, and enhance video quality. The experimental results show that our RCLC can reduce up to 32.55\% BD-rate for the ROI region compared to H.265 video codec under a similar compression time with 1080p resolution.

</p>
</details>

<details><summary><b>MARC: Mining Association Rules from datasets by using Clustering models</b>
<a href="https://arxiv.org/abs/2107.08814">arxiv:2107.08814</a>
&#x1F4C8; 1 <br>
<p>Shadi Al Shehabi, Abdullatif Baba</p></summary>
<p>

**Abstract:** Association rules are useful to discover relationships, which are mostly hidden, between the different items in large datasets. Symbolic models are the principal tools to extract association rules. This basic technique is time-consuming, and it generates a big number of associated rules. To overcome this drawback, we suggest a new method, called MARC, to extract the more important association rules of two important levels: Type I, and Type II. This approach relies on a multi-topographic unsupervised neural network model as well as clustering quality measures that evaluate the success of a given numerical classification model to behave as a natural symbolic model.

</p>
</details>

<details><summary><b>Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs</b>
<a href="https://arxiv.org/abs/2107.07049">arxiv:2107.07049</a>
&#x1F4C8; 1 <br>
<p>Bharath Keshavamurthy, Nicolo Michelusi</p></summary>
<p>

**Abstract:** A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is proposed, wherein a cognitive radio (CR) learns a time-frequency correlation model underlying spectrum occupancy of licensed users (LUs) in a radio ecosystem; concurrently, it devises an approximately optimal spectrum sensing and access policy under sensing constraints. A Baum-Welch algorithm is proposed to learn a parametric Markov transition model of LU spectrum occupancy based on noisy spectrum measurements. Spectrum sensing and access are cast as a Partially-Observable Markov Decision Process, approximately optimized via randomized point-based value iteration. Fragmentation, Hamming-distance state filters and Monte-Carlo methods are proposed to alleviate the inherent computational complexity, and a weighted reward metric to regulate the trade-off between CR throughput and LU interference. Numerical evaluations demonstrate that LESSA performs within 5 percent of a genie-aided upper bound with foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art algorithms across the entire trade-off region: 71 percent over correlation-based clustering, 26 percent over Neyman-Pearson detection, 6 percent over the Viterbi algorithm, and 9 percent over an adaptive Deep Q-Network. LESSA is then extended to a distributed Multi-Agent setting (MA-LESSA), by proposing novel neighbor discovery and channel access rank allocation. MA-LESSA improves CR throughput by 43 percent over cooperative TD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over non-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is implemented on the DARPA SC2 platform, manifesting superior performance over competitors in a real-world TDWR-UNII WLAN emulation; its implementation feasibility is further validated on a testbed of ESP32 radios, exhibiting 96 percent success probability.

</p>
</details>

<details><summary><b>Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group</b>
<a href="https://arxiv.org/abs/2107.06898">arxiv:2107.06898</a>
&#x1F4C8; 1 <br>
<p>Johanna Erdmenger, Kevin T. Grosvenor, Ro Jefferson</p></summary>
<p>

**Abstract:** We investigate the analogy between the renormalization group (RG) and deep neural networks, wherein subsequent layers of neurons are analogous to successive steps along the RG. In particular, we quantify the flow of information by explicitly computing the relative entropy or Kullback-Leibler divergence in both the one- and two-dimensional Ising models under decimation RG, as well as in a feedforward neural network as a function of depth. We observe qualitatively identical behavior characterized by the monotonic increase to a parameter-dependent asymptotic value. On the quantum field theory side, the monotonic increase confirms the connection between the relative entropy and the c-theorem. For the neural networks, the asymptotic behavior may have implications for various information maximization methods in machine learning, as well as for disentangling compactness and generalizability. Furthermore, while both the two-dimensional Ising model and the random neural networks we consider exhibit non-trivial critical points, the relative entropy appears insensitive to the phase structure of either system. In this sense, more refined probes are required in order to fully elucidate the flow of information in these models.

</p>
</details>

<details><summary><b>MDE4QAI: Towards Model-Driven Engineering for Quantum Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2107.06708">arxiv:2107.06708</a>
&#x1F4C8; 1 <br>
<p>Armin Moin, Moharram Challenger, Atta Badii, Stephan Günnemann</p></summary>
<p>

**Abstract:** Over the past decade, Artificial Intelligence (AI) has provided enormous new possibilities and opportunities, but also new demands and requirements for software systems. In particular, Machine Learning (ML) has proven useful in almost every vertical application domain. Although other sub-disciplines of AI, such as intelligent agents and Multi-Agent Systems (MAS) did not become promoted to the same extent, they still possess the potential to be integrated into the mainstream technology stacks and ecosystems, for example, due to the ongoing prevalence of the Internet of Things (IoT) and smart Cyber-Physical Systems (CPS). However, in the decade ahead, an unprecedented paradigm shift from classical computing towards Quantum Computing (QC) is expected, with perhaps a quantum-classical hybrid model. We expect the Model-Driven Engineering (MDE) paradigm to be an enabler and a facilitator, when it comes to the quantum and the quantum-classical hybrid applications as it has already proven beneficial in the highly complex domains of IoT, smart CPS and AI with inherently heterogeneous hardware and software platforms, and APIs. This includes not only automated code generation, but also automated model checking and verification, as well as model analysis in the early design phases, and model-to-model transformations both at the design-time and at the runtime. In this paper, the vision is focused on MDE for Quantum AI, and a holistic approach integrating all of the above.

</p>
</details>

<details><summary><b>Hybrid Model and Data Driven Algorithm for Online Learning of Any-to-Any Path Loss Maps</b>
<a href="https://arxiv.org/abs/2107.06677">arxiv:2107.06677</a>
&#x1F4C8; 1 <br>
<p>M. A. Gutierrez-Estevez, Martin Kasparick, Renato L. G. Cavalvante, Sławomir Stańczak</p></summary>
<p>

**Abstract:** Learning any-to-any (A2A) path loss maps, where the objective is the reconstruction of path loss between any two given points in a map, might be a key enabler for many applications that rely on device-to-device (D2D) communication. Such applications include machine-type communications (MTC) or vehicle-to-vehicle (V2V) communications. Current approaches for learning A2A maps are either model-based methods, or pure data-driven methods. Model-based methods have the advantage that they can generate reliable estimations with low computational complexity, but they cannot exploit information coming from data. Pure data-driven methods can achieve good performance without assuming any physical model, but their complexity and their lack of robustness is not acceptable for many applications. In this paper, we propose a novel hybrid model and data-driven approach that fuses information obtained from datasets and models in an online fashion. To that end, we leverage the framework of stochastic learning to deal with the sequential arrival of samples and propose an online algorithm that alternatively and sequentially minimizes the original non-convex problem. A proof of convergence is presented, along with experiments based firstly on synthetic data, and secondly on a more realistic dataset for V2X, with both experiments showing promising results.

</p>
</details>

<details><summary><b>Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain</b>
<a href="https://arxiv.org/abs/2107.06676">arxiv:2107.06676</a>
&#x1F4C8; 1 <br>
<p>Martin Svedin, Artur Podobas, Steven W. D. Chien, Stefano Markidis</p></summary>
<p>

**Abstract:** One of the most promising approaches for data analysis and exploration of large data sets is Machine Learning techniques that are inspired by brain models. Such methods use alternative learning rules potentially more efficiently than established learning rules. In this work, we focus on the potential of brain-inspired ML for exploiting High-Performance Computing (HPC) resources to solve ML problems: we discuss the BCPNN and an HPC implementation, called StreamBrain, its computational cost, suitability to HPC systems. As an example, we use StreamBrain to analyze the Higgs Boson dataset from High Energy Physics and discriminate between background and signal classes in collisions of high-energy particle colliders. Overall, we reach up to 69.15% accuracy and 76.4% Area Under the Curve (AUC) performance.

</p>
</details>

<details><summary><b>DeepMutants: Training neural bug detectors with contextual mutations</b>
<a href="https://arxiv.org/abs/2107.06657">arxiv:2107.06657</a>
&#x1F4C8; 1 <br>
<p>Cedric Richter, Heike Wehrheim</p></summary>
<p>

**Abstract:** Learning-based bug detectors promise to find bugs in large code bases by exploiting natural hints such as names of variables and functions or comments. Still, existing techniques tend to underperform when presented with realistic bugs. We believe bug detector learning to currently suffer from a lack of realistic defective training examples. In fact, real world bugs are scarce which has driven existing methods to train on artificially created and mostly unrealistic mutants. In this work, we propose a novel contextual mutation operator which incorporates knowledge about the mutation context to dynamically inject natural and more realistic faults into code. Our approach employs a masked language model to produce a context-dependent distribution over feasible token replacements. The evaluation shows that sampling from a language model does not only produce mutants which more accurately represent real bugs but also lead to better performing bug detectors, both on artificial benchmarks and on real world source code.

</p>
</details>

<details><summary><b>Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures</b>
<a href="https://arxiv.org/abs/2107.06626">arxiv:2107.06626</a>
&#x1F4C8; 1 <br>
<p>Yair Bartal, Ora Nova Fandina, Kasper Green Larsen</p></summary>
<p>

**Abstract:** It is well known that the Johnson-Lindenstrauss dimensionality reduction method is optimal for worst case distortion. While in practice many other methods and heuristics are used, not much is known in terms of bounds on their performance. The question of whether the JL method is optimal for practical measures of distortion was recently raised in \cite{BFN19} (NeurIPS'19). They provided upper bounds on its quality for a wide range of practical measures and showed that indeed these are best possible in many cases. Yet, some of the most important cases, including the fundamental case of average distortion were left open. In particular, they show that the JL transform has $1+ε$ average distortion for embedding into $k$-dimensional Euclidean space, where $k=O(1/\eps^2)$, and for more general $q$-norms of distortion, $k = O(\max\{1/\eps^2,q/\eps\})$, whereas tight lower bounds were established only for large values of $q$ via reduction to the worst case.
  In this paper we prove that these bounds are best possible for any dimensionality reduction method, for any $1 \leq q \leq O(\frac{\log (2\eps^2 n)}{\eps})$ and $ε\geq \frac{1}{\sqrt{n}}$, where $n$ is the size of the subset of Euclidean space.
  Our results imply that the JL method is optimal for various distortion measures commonly used in practice, such as {\it stress, energy} and {\it relative error}. We prove that if any of these measures is bounded by $\eps$ then $k=Ω(1/\eps^2)$, for any $ε\geq \frac{1}{\sqrt{n}}$, matching the upper bounds of \cite{BFN19} and extending their tightness results for the full range moment analysis.
  Our results may indicate that the JL dimensionality reduction method should be considered more often in practical applications, and the bounds we provide for its quality should be served as a measure for comparison when evaluating the performance of other methods and heuristics.

</p>
</details>

<details><summary><b>A Granular Sieving Algorithm for Deterministic Global Optimization</b>
<a href="https://arxiv.org/abs/2107.06581">arxiv:2107.06581</a>
&#x1F4C8; 1 <br>
<p>Tao Qian, Lei Dai, Liming Zhang, Zehua Chen</p></summary>
<p>

**Abstract:** A gradient-free deterministic method is developed to solve global optimization problems for Lipschitz continuous functions defined in arbitrary path-wise connected compact sets in Euclidean spaces. The method can be regarded as granular sieving with synchronous analysis in both the domain and range of the objective function. With straightforward mathematical formulation applicable to both univariate and multivariate objective functions, the global minimum value and all the global minimizers are located through two decreasing sequences of compact sets in, respectively, the domain and range spaces. The algorithm is easy to implement with moderate computational cost. The method is tested against extensive benchmark functions in the literature. The experimental results show remarkable effectiveness and applicability of the algorithm.

</p>
</details>

<details><summary><b>QoS-Aware Scheduling in New Radio Using Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.06570">arxiv:2107.06570</a>
&#x1F4C8; 1 <br>
<p>Jakob Stigenberg, Vidit Saxena, Soma Tayamon, Euhanna Ghadimi</p></summary>
<p>

**Abstract:** Fifth-generation (5G) New Radio (NR) cellular networks support a wide range of new services, many of which require an application-specific quality of service (QoS), e.g. in terms of a guaranteed minimum bit-rate or a maximum tolerable delay. Therefore, scheduling multiple parallel data flows, each serving a unique application instance, is bound to become an even more challenging task compared to the previous generations. Leveraging recent advances in deep reinforcement learning, in this paper, we propose a QoS-Aware Deep Reinforcement learning Agent (QADRA) scheduler for NR networks. In contrast to state-of-the-art scheduling heuristics, the QADRA scheduler explicitly optimizes for the QoS satisfaction rate while simultaneously maximizing the network performance. Moreover, we train our algorithm end-to-end on these objectives. We evaluate QADRA in a full scale, near-product, system level NR simulator and demonstrate a significant boost in network performance. In our particular evaluation scenario, the QADRA scheduler improves network throughput by 30% while simultaneously maintaining the QoS satisfaction rate of VoIP users served by the network, compared to state-of-the-art baselines.

</p>
</details>

<details><summary><b>Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables</b>
<a href="https://arxiv.org/abs/2107.07087">arxiv:2107.07087</a>
&#x1F4C8; 0 <br>
<p>Noam Finkelstein, Beata Zjawin, Elie Wolfe, Ilya Shpitser, Robert W. Spekkens</p></summary>
<p>

**Abstract:** Directed acyclic graphs (DAGs) with hidden variables are often used to characterize causal relations between variables in a system. When some variables are unobserved, DAGs imply a notoriously complicated set of constraints on the distribution of observed variables. In this work, we present entropic inequality constraints that are implied by $e$-separation relations in hidden variable DAGs with discrete observed variables. The constraints can intuitively be understood to follow from the fact that the capacity of variables along a causal pathway to convey information is restricted by their entropy; e.g. at the extreme case, a variable with entropy $0$ can convey no information. We show how these constraints can be used to learn about the true causal model from an observed data distribution. In addition, we propose a measure of causal influence called the minimal mediary entropy, and demonstrate that it can augment traditional measures such as the average causal effect.

</p>
</details>

<details><summary><b>Efficient Approximate Search for Sets of Vectors</b>
<a href="https://arxiv.org/abs/2107.06817">arxiv:2107.06817</a>
&#x1F4C8; 0 <br>
<p>Michael Leybovich, Oded Shmueli</p></summary>
<p>

**Abstract:** We consider a similarity measure between two sets $A$ and $B$ of vectors, that balances the average and maximum cosine distance between pairs of vectors, one from set $A$ and one from set $B$. As a motivation for this measure, we present lineage tracking in a database. To practically realize this measure, we need an approximate search algorithm that given a set of vectors $A$ and sets of vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that maximizes the similarity measure. For the case where all sets are singleton sets, essentially each is a single vector, there are known efficient approximate search algorithms, e.g., approximated versions of tree search algorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and proximity graph algorithms. In this work, we present approximate search algorithms for the general case. The underlying idea in these algorithms is encoding a set of vectors via a "long" single vector. The proposed approximate approach achieves significant performance gains over an optimized, exact search on vector sets.

</p>
</details>

<details><summary><b>Resonant tunnelling diode nano-optoelectronic spiking nodes for neuromorphic information processing</b>
<a href="https://arxiv.org/abs/2107.06721">arxiv:2107.06721</a>
&#x1F4C8; 0 <br>
<p>Matěj Hejda, Juan Arturo Alanis, Ignacio Ortega-Piwonka, João Lourenço, José Figueiredo, Julien Javaloyes, Bruno Romeira, Antonio Hurtado</p></summary>
<p>

**Abstract:** In this work, we introduce an optoelectronic spiking artificial neuron capable of operating at ultrafast rates ($\approx$ 100 ps/optical spike) and with low energy consumption ($<$ pJ/spike). The proposed system combines an excitable resonant tunnelling diode (RTD) element exhibiting negative differential conductance, coupled to a nanoscale light source (forming a master node) or a photodetector (forming a receiver node). We study numerically the spiking dynamical responses and information propagation functionality of an interconnected master-receiver RTD node system. Using the key functionality of pulse thresholding and integration, we utilize a single node to classify sequential pulse patterns and perform convolutional functionality for image feature (edge) recognition. We also demonstrate an optically-interconnected spiking neural network model for processing of spatiotemporal data at over 10 Gbps with high inference accuracy. Finally, we demonstrate an off-chip supervised learning approach utilizing spike-timing dependent plasticity for the RTD-enabled photonic spiking neural network. These results demonstrate the potential and viability of RTD spiking nodes for low footprint, low energy, high-speed optoelectronic realization of neuromorphic hardware.

</p>
</details>

<details><summary><b>TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence</b>
<a href="https://arxiv.org/abs/2107.06543">arxiv:2107.06543</a>
&#x1F4C8; 0 <br>
<p>Davide Bacciu, Siranush Akarmazyan, Eric Armengaud, Manlio Bacco, George Bravos, Calogero Calandra, Emanuele Carlini, Antonio Carta, Pietro Cassara, Massimo Coppola, Charalampos Davalas, Patrizio Dazzi, Maria Carmela Degennaro, Daniele Di Sarli, Jürgen Dobaj, Claudio Gallicchio, Sylvain Girbal, Alberto Gotta, Riccardo Groppo, Vincenzo Lomonaco, Georg Macher, Daniele Mazzei, Gabriele Mencagli, Dimitrios Michail, Alessio Micheli</p></summary>
<p>

**Abstract:** This paper discusses the perspective of the H2020 TEACHING project on the next generation of autonomous applications running in a distributed and highly heterogeneous environment comprising both virtual and physical resources spanning the edge-cloud continuum. TEACHING puts forward a human-centred vision leveraging the physiological, emotional, and cognitive state of the users as a driver for the adaptation and optimization of the autonomous applications. It does so by building a distributed, embedded and federated learning system complemented by methods and tools to enforce its dependability, security and privacy preservation. The paper discusses the main concepts of the TEACHING approach and singles out the main AI-related research challenges associated with it. Further, we provide a discussion of the design choices for the TEACHING system to tackle the aforementioned challenges

</p>
</details>


[Next Page](2021/2021-07/2021-07-13.md)
