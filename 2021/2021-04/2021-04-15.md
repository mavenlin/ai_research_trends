## Summary for 2021-04-15, created on 2021-12-22


<details><summary><b>Image Super-Resolution via Iterative Refinement</b>
<a href="https://arxiv.org/abs/2104.07636">arxiv:2104.07636</a>
&#x1F4C8; 161 <br>
<p>Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi</p></summary>
<p>

**Abstract:** We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.

</p>
</details>

<details><summary><b>How to Train BERT with an Academic Budget</b>
<a href="https://arxiv.org/abs/2104.07705">arxiv:2104.07705</a>
&#x1F4C8; 148 <br>
<p>Peter Izsak, Moshe Berchansky, Omer Levy</p></summary>
<p>

**Abstract:** While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.

</p>
</details>

<details><summary><b>mlf-core: a framework for deterministic machine learning</b>
<a href="https://arxiv.org/abs/2104.07651">arxiv:2104.07651</a>
&#x1F4C8; 73 <br>
<p>Lukas Heumos, Philipp Ehmele, Kevin Menden, Luis Kuhn Cuellar, Edmund Miller, Steffen Lemke, Gisela Gabernet, Sven Nahnsen</p></summary>
<p>

**Abstract:** Machine learning has shown extensive growth in recent years. However, previously existing studies highlighted a reproducibility crisis in machine learning. The reasons for irreproducibility are manifold. Major machine learning libraries default to the usage of non-deterministic algorithms based on atomic operations. Solely fixing all random seeds is not sufficient for deterministic machine learning. To overcome this shortcoming, various machine learning libraries released deterministic counterparts to the non-deterministic algorithms. We evaluated the effect of these algorithms on determinism and runtime. Based on these results, we formulated a set of requirements for reproducible machine learning and developed a new software solution, the mlf-core ecosystem, which aids machine learning projects to meet and keep these requirements. We applied mlf-core to develop fully reproducible models in various biomedical fields including a single cell autoencoder with TensorFlow, a PyTorch-based U-Net model for liver-tumor segmentation in CT scans, and a liver cancer classifier based on gene expression profiles with XGBoost.

</p>
</details>

<details><summary><b>Self-supervised Video Object Segmentation by Motion Grouping</b>
<a href="https://arxiv.org/abs/2104.07658">arxiv:2104.07658</a>
&#x1F4C8; 66 <br>
<p>Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, Weidi Xie</p></summary>
<p>

**Abstract:** Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.

</p>
</details>

<details><summary><b>PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models</b>
<a href="https://arxiv.org/abs/2104.07788">arxiv:2104.07788</a>
&#x1F4C8; 65 <br>
<p>Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzmán López, Nicolas Collignon, Rik Sarkar</p></summary>
<p>

**Abstract:** We present PyTorch Geometric Temporal a deep learning framework combining state-of-the-art machine learning algorithms for neural spatiotemporal signal processing. The main goal of the library is to make temporal geometric deep learning available for researchers and machine learning practitioners in a unified easy-to-use framework. PyTorch Geometric Temporal was created with foundations on existing libraries in the PyTorch eco-system, streamlined neural network layer definitions, temporal snapshot generators for batching, and integrated benchmark datasets. These features are illustrated with a tutorial-like case study. Experiments demonstrate the predictive performance of the models implemented in the library on real world problems such as epidemiological forecasting, ridehail demand prediction and web-traffic management. Our sensitivity analysis of runtime shows that the framework can potentially operate on web-scale datasets with rich temporal features and spatial structure.

</p>
</details>

<details><summary><b>ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning</b>
<a href="https://arxiv.org/abs/2104.07644">arxiv:2104.07644</a>
&#x1F4C8; 46 <br>
<p>Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal</p></summary>
<p>

**Abstract:** Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model's ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be "right for the right reasons". In this work, we present ExplaGraphs, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90%) via multiple rounds of verification and refinement. A significant 79% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task. ExplaGraphs will be publicly available at https://explagraphs.github.io.

</p>
</details>

<details><summary><b>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</b>
<a href="https://arxiv.org/abs/2104.07857">arxiv:2104.07857</a>
&#x1F4C8; 45 <br>
<p>Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He</p></summary>
<p>

**Abstract:** In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.
  In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.

</p>
</details>

<details><summary><b>Auto-Tuned Sim-to-Real Transfer</b>
<a href="https://arxiv.org/abs/2104.07662">arxiv:2104.07662</a>
&#x1F4C8; 32 <br>
<p>Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Deepak Pathak</p></summary>
<p>

**Abstract:** Policies trained in simulation often fail when transferred to the real world due to the `reality gap' where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real-world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization. Project videos and code at https://yuqingd.github.io/autotuned-sim2real/

</p>
</details>

<details><summary><b>Retrieval Augmentation Reduces Hallucination in Conversation</b>
<a href="https://arxiv.org/abs/2104.07567">arxiv:2104.07567</a>
&#x1F4C8; 30 <br>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston</p></summary>
<p>

**Abstract:** Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.

</p>
</details>

<details><summary><b>XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</b>
<a href="https://arxiv.org/abs/2104.07412">arxiv:2104.07412</a>
&#x1F4C8; 27 <br>
<p>Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson</p></summary>
<p>

**Abstract:** Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models. The leaderboard and code for XTREME-R will be made available at https://sites.research.google/xtreme and https://github.com/google-research/xtreme respectively.

</p>
</details>

<details><summary><b>Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?</b>
<a href="https://arxiv.org/abs/2104.07762">arxiv:2104.07762</a>
&#x1F4C8; 26 <br>
<p>Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C. Wallace</p></summary>
<p>

**Abstract:** Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated "attacks" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release

</p>
</details>

<details><summary><b>See through Gradients: Image Batch Recovery via GradInversion</b>
<a href="https://arxiv.org/abs/2104.07586">arxiv:2104.07586</a>
&#x1F4C8; 23 <br>
<p>Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov</p></summary>
<p>

**Abstract:** Training deep neural networks requires gradient estimation from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications. Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions - a single input point, or a network with no non-linearities, or a small 32x32 px input batch. Therefore, averaging gradients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 - 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224x224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image fidelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to find an enhanced reconstruction of original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recovered with high fidelity via GradInversion, even for complex datasets, deep networks, and large batch sizes.

</p>
</details>

<details><summary><b>Spectrogram Inpainting for Interactive Generation of Instrument Sounds</b>
<a href="https://arxiv.org/abs/2104.07519">arxiv:2104.07519</a>
&#x1F4C8; 23 <br>
<p>Théis Bazin, Gaëtan Hadjeres, Philippe Esling, Mikhail Malt</p></summary>
<p>

**Abstract:** Modern approaches to sound synthesis using deep neural networks are hard to control, especially when fine-grained conditioning information is not available, hindering their adoption by musicians.
  In this paper, we cast the generation of individual instrumental notes as an inpainting-based task, introducing novel and unique ways to iteratively shape sounds. To this end, we propose a two-step approach: first, we adapt the VQ-VAE-2 image generation architecture to spectrograms in order to convert real-valued spectrograms into compact discrete codemaps, we then implement token-masked Transformers for the inpainting-based generation of these codemaps.
  We apply the proposed architecture on the NSynth dataset on masked resampling tasks. Most crucially, we open-source an interactive web interface to transform sounds by inpainting, for artists and practitioners alike, opening up to new, creative uses.

</p>
</details>

<details><summary><b>Generating Datasets with Pretrained Language Models</b>
<a href="https://arxiv.org/abs/2104.07540">arxiv:2104.07540</a>
&#x1F4C8; 22 <br>
<p>Timo Schick, Hinrich Schütze</p></summary>
<p>

**Abstract:** To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.

</p>
</details>

<details><summary><b>Cross-Domain Label-Adaptive Stance Detection</b>
<a href="https://arxiv.org/abs/2104.07467">arxiv:2104.07467</a>
&#x1F4C8; 20 <br>
<p>Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein</p></summary>
<p>

**Abstract:** Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance.

</p>
</details>

<details><summary><b>Gradient-based Adversarial Attacks against Text Transformers</b>
<a href="https://arxiv.org/abs/2104.13733">arxiv:2104.13733</a>
&#x1F4C8; 15 <br>
<p>Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, Douwe Kiela</p></summary>
<p>

**Abstract:** We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.

</p>
</details>

<details><summary><b>Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events</b>
<a href="https://arxiv.org/abs/2104.07635">arxiv:2104.07635</a>
&#x1F4C8; 12 <br>
<p>Hossein Rajaby Faghihi, Parisa Kordjamshidi</p></summary>
<p>

**Abstract:** Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model~(TSLM model) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a $3.1\%$ increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.

</p>
</details>

<details><summary><b>Deep Stable Learning for Out-Of-Distribution Generalization</b>
<a href="https://arxiv.org/abs/2104.07876">arxiv:2104.07876</a>
&#x1F4C8; 10 <br>
<p>Xingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, Zheyan Shen</p></summary>
<p>

**Abstract:** Approaches based on deep neural networks have achieved striking performance when testing data and training data share similar distribution, but can significantly fail otherwise. Therefore, eliminating the impact of distribution shifts between training and testing data is crucial for building performance-promising deep models. Conventional methods assume either the known heterogeneity of training data (e.g. domain labels) or the approximately equal capacities of different domains. In this paper, we consider a more challenging case where neither of the above assumptions holds. We propose to address this problem by removing the dependencies between features via learning weights for training samples, which helps deep models get rid of spurious correlations and, in turn, concentrate more on the true connection between discriminative features and labels. Extensive experiments clearly demonstrate the effectiveness of our method on multiple distribution generalization benchmarks compared with state-of-the-art counterparts. Through extensive experiments on distribution generalization benchmarks including PACS, VLCS, MNIST-M, and NICO, we show the effectiveness of our method compared with state-of-the-art counterparts.

</p>
</details>

<details><summary><b>Exploring Visual Engagement Signals for Representation Learning</b>
<a href="https://arxiv.org/abs/2104.07767">arxiv:2104.07767</a>
&#x1F4C8; 10 <br>
<p>Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, Ser-Nam Lim</p></summary>
<p>

**Abstract:** Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such visual engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition.

</p>
</details>

<details><summary><b>Investigations on Output Parameterizations of Neural Networks for Single Shot 6D Object Pose Estimation</b>
<a href="https://arxiv.org/abs/2104.07528">arxiv:2104.07528</a>
&#x1F4C8; 10 <br>
<p>Kilian Kleeberger, Markus Völk, Richard Bormann, Marco F. Huber</p></summary>
<p>

**Abstract:** Single shot approaches have demonstrated tremendous success on various computer vision tasks. Finding good parameterizations for 6D object pose estimation remains an open challenge. In this work, we propose different novel parameterizations for the output of the neural network for single shot 6D object pose estimation. Our learning-based approach achieves state-of-the-art performance on two public benchmark datasets. Furthermore, we demonstrate that the pose estimates can be used for real-world robotic grasping tasks without additional ICP refinement.

</p>
</details>

<details><summary><b>Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution</b>
<a href="https://arxiv.org/abs/2104.07473">arxiv:2104.07473</a>
&#x1F4C8; 10 <br>
<p>Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach, Chenliang Xu</p></summary>
<p>

**Abstract:** In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A naïve method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .

</p>
</details>

<details><summary><b>Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills</b>
<a href="https://arxiv.org/abs/2104.07749">arxiv:2104.07749</a>
&#x1F4C8; 9 <br>
<p>Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, Sergey Levine</p></summary>
<p>

**Abstract:** We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting that is becoming increasingly important for scaling robot learning by reusing past robotic data. In particular, we propose the objective of learning a functional understanding of the environment by learning to reach any goal state in a given dataset. We employ goal-conditioned Q-learning with hindsight relabeling and develop several techniques that enable training in a particularly challenging offline setting. We find that our method can operate on high-dimensional camera images and learn a variety of skills on real robots that generalize to previously unseen scenes and objects. We also show that our method can learn to reach long-horizon goals across multiple episodes through goal chaining, and learn rich representations that can help with downstream tasks through pre-training or auxiliary objectives. The videos of our experiments can be found at https://actionable-models.github.io

</p>
</details>

<details><summary><b>Contrastive Learning with Stronger Augmentations</b>
<a href="https://arxiv.org/abs/2104.07713">arxiv:2104.07713</a>
&#x1F4C8; 9 <br>
<p>Xiao Wang, Guo-Jun Qi</p></summary>
<p>

**Abstract:** Representation learning has significantly been developed with the advance of contrastive learning methods. Most of those methods have benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. However, those carefully designed transformations limited us to further explore the novel patterns exposed by other transformations. Meanwhile, as found in our experiments, the strong augmentations distorted the images' structures, resulting in difficult retrieval. Thus, we propose a general framework called Contrastive Learning with Stronger Augmentations~(CLSA) to complement current contrastive learning approaches. Here, the distribution divergence between the weakly and strongly augmented images over the representation bank is adopted to supervise the retrieval of strongly augmented queries from a pool of instances. Experiments on the ImageNet dataset and downstream datasets showed the information from the strongly augmented images can significantly boost the performance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, which is almost the same level as 76.5% of supervised results. The code and pre-trained models are available in https://github.com/maple-research-lab/CLSA.

</p>
</details>

<details><summary><b>Ensemble of MRR and NDCG models for Visual Dialog</b>
<a href="https://arxiv.org/abs/2104.07511">arxiv:2104.07511</a>
&#x1F4C8; 9 <br>
<p>Idan Schwartz</p></summary>
<p>

**Abstract:** Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as `I don't know. Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.

</p>
</details>

<details><summary><b>ROC: An Ontology for Country Responses towards COVID-19</b>
<a href="https://arxiv.org/abs/2104.07345">arxiv:2104.07345</a>
&#x1F4C8; 9 <br>
<p>Jamal Al Qundus, Ralph Schäfermeier, Naouel Karam, Silvio Peikert, Adrian Paschke</p></summary>
<p>

**Abstract:** The ROC ontology for country responses to COVID-19 provides a model for collecting, linking and sharing data on the COVID-19 pandemic. It follows semantic standardization (W3C standards RDF, OWL, SPARQL) for the representation of concepts and creation of vocabularies. ROC focuses on country measures and enables the integration of data from heterogeneous data sources. The proposed ontology is intended to facilitate statistical analysis to study and evaluate the effectiveness and side effects of government responses to COVID-19 in different countries. The ontology contains data collected by OxCGRT from publicly available information. This data has been compiled from information provided by ECDC for most countries, as well as from various repositories used to collect data on COVID-19.

</p>
</details>

<details><summary><b>Micro-Estimates of Wealth for all Low- and Middle-Income Countries</b>
<a href="https://arxiv.org/abs/2104.07761">arxiv:2104.07761</a>
&#x1F4C8; 8 <br>
<p>Guanghua Chi, Han Fang, Sourav Chatterjee, Joshua E. Blumenstock</p></summary>
<p>

**Abstract:** Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. Yet many poverty maps are out of date or exist only at very coarse levels of granularity. Here we develop the first micro-estimates of wealth and poverty that cover the populated surface of all 135 low and middle-income countries (LMICs) at 2.4km resolution. The estimates are built by applying machine learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, topographic maps, as well as aggregated and de-identified connectivity data from Facebook. We train and calibrate the estimates using nationally-representative household survey data from 56 LMICs, then validate their accuracy using four independent sources of household survey data from 18 countries. We also provide confidence intervals for each micro-estimate to facilitate responsible downstream use. These estimates are provided free for public use in the hope that they enable targeted policy response to the COVID-19 pandemic, provide the foundation for new insights into the causes and consequences of economic development and growth, and promote responsible policymaking in support of the Sustainable Development Goals.

</p>
</details>

<details><summary><b>Street-Map Based Validation of Semantic Segmentation in Autonomous Driving</b>
<a href="https://arxiv.org/abs/2104.07538">arxiv:2104.07538</a>
&#x1F4C8; 8 <br>
<p>Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Nico Piatkowski, Christian Bauckhage</p></summary>
<p>

**Abstract:** Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness, which motivates the thorough validation of learned models. However, current validation approaches mostly require ground truth data and are thus both cost-intensive and limited in their applicability. We propose to overcome these limitations by a model agnostic validation using a-priori knowledge from street maps. In particular, we show how to validate semantic segmentation masks and demonstrate the potential of our approach using OpenStreetMap. We introduce validation metrics that indicate false positive or negative road segments. Besides the validation approach, we present a method to correct the vehicle's GPS position so that a more accurate localization can be used for the street-map based validation. Lastly, we present quantitative results on the Cityscapes dataset indicating that our validation approach can indeed uncover errors in semantic segmentation masks.

</p>
</details>

<details><summary><b>Tracking entities in technical procedures -- a new dataset and baselines</b>
<a href="https://arxiv.org/abs/2104.07378">arxiv:2104.07378</a>
&#x1F4C8; 8 <br>
<p>Saransh Goyal, Pratyush Pandey, Garima Gaur, Subhalingam D, Srikanta Bedathur, Maya Ramanath</p></summary>
<p>

**Abstract:** We introduce TechTrack, a new dataset for tracking entities in technical procedures. The dataset, prepared by annotating open domain articles from WikiHow, consists of 1351 procedures, e.g., "How to connect a printer", identifies more than 1200 unique entities with an average of 4.7 entities per procedure. We evaluate the performance of state-of-the-art models on the entity-tracking task and find that they are well below the human annotation performance. We describe how TechTrack can be used to take forward the research on understanding procedures from temporal texts.

</p>
</details>

<details><summary><b>Rehearsal revealed: The limits and merits of revisiting samples in continual learning</b>
<a href="https://arxiv.org/abs/2104.07446">arxiv:2104.07446</a>
&#x1F4C8; 7 <br>
<p>Eli Verwimp, Matthias De Lange, Tinne Tuytelaars</p></summary>
<p>

**Abstract:** Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning's most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes.

</p>
</details>

<details><summary><b>The Role of Context in Detecting Previously Fact-Checked Claims</b>
<a href="https://arxiv.org/abs/2104.07423">arxiv:2104.07423</a>
&#x1F4C8; 7 <br>
<p>Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov</p></summary>
<p>

**Abstract:** Recent years have seen the proliferation of disinformation and misinformation online, thanks to the freedom of expression on the Internet and to the rise of social media. Two solutions were proposed to address the problem: (i) manual fact-checking, which is accurate and credible, but slow and non-scalable, and (ii) automatic fact-checking, which is fast and scalable, but lacks explainability and credibility. With the accumulation of enough manually fact-checked claims, a middle-ground approach has emerged: checking whether a given claim has previously been fact-checked. This can be made automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. This is a relatively new and understudied research direction, and here we focus on claims made in a political debate, where context really matters. Thus, we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and reasoning over the target text using Transformer-XH. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is more important, and can yield 10+ points of absolute improvement.

</p>
</details>

<details><summary><b>Speaker Attentive Speech Emotion Recognition</b>
<a href="https://arxiv.org/abs/2104.07288">arxiv:2104.07288</a>
&#x1F4C8; 7 <br>
<p>Clément Le Moine, Nicolas Obin, Axel Roebel</p></summary>
<p>

**Abstract:** Speech Emotion Recognition (SER) task has known significant improvements over the last years with the advent of Deep Neural Networks (DNNs). However, even the most successful methods are still rather failing when adaptation to specific speakers and scenarios is needed, inevitably leading to poorer performances when compared to humans. In this paper, we present novel work based on the idea of teaching the emotion recognition network about speaker identity. Our system is a combination of two ACRNN classifiers respectively dedicated to speaker and emotion recognition. The first informs the latter through a Self Speaker Attention (SSA) mechanism that is shown to considerably help to focus on emotional information of the speech signal. Experiments on social attitudes database Att-HACK and IEMOCAP corpus demonstrate the effectiveness of the proposed method and achieve the state-of-the-art performance in terms of unweighted average recall.

</p>
</details>

<details><summary><b>Self-Supervised Exploration via Latent Bayesian Surprise</b>
<a href="https://arxiv.org/abs/2104.07495">arxiv:2104.07495</a>
&#x1F4C8; 6 <br>
<p>Pietro Mazzaglia, Ozan Catal, Tim Verbelen, Bart Dhoedt</p></summary>
<p>

**Abstract:** Training with Reinforcement Learning requires a reward function that is used to guide the agent towards achieving its objective. However, designing smooth and well-behaved rewards is in general not trivial and requires significant human engineering efforts. Generating rewards in self-supervised way, by inspiring the agent with an intrinsic desire to learn and explore the environment, might induce more general behaviours. In this work, we propose a curiosity-based bonus as intrinsic reward for Reinforcement Learning, computed as the Bayesian surprise with respect to a latent state variable, learnt by reconstructing fixed random features. We extensively evaluate our model by measuring the agent's performance in terms of environment exploration, for continuous tasks, and looking at the game scores achieved, for video games. Our model is computationally cheap and empirically shows state-of-the-art performance on several problems. Furthermore, experimenting on an environment with stochastic actions, our approach emerged to be the most resilient to simple stochasticity. Further visualization is available on the project webpage.(https://lbsexploration.github.io/)

</p>
</details>

<details><summary><b>Action Segmentation with Mixed Temporal Domain Adaptation</b>
<a href="https://arxiv.org/abs/2104.07461">arxiv:2104.07461</a>
&#x1F4C8; 6 <br>
<p>Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib</p></summary>
<p>

**Abstract:** The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is time-consuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame- and video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).

</p>
</details>

<details><summary><b>Integration of Pre-trained Networks with Continuous Token Interface for End-to-End Spoken Language Understanding</b>
<a href="https://arxiv.org/abs/2104.07253">arxiv:2104.07253</a>
&#x1F4C8; 6 <br>
<p>Seunghyun Seo, Donghyun Kwak, Bowon Lee</p></summary>
<p>

**Abstract:** Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but still lack the capability to understand the semantics of utterances, crucial for the SLU task. To solve this, recently proposed studies use pre-trained NLU networks. However, it is not trivial to fully utilize both pre-trained networks; many solutions were proposed, such as Knowledge Distillation, cross-modal shared embedding, and network integration with Interface. We propose a simple and robust integration method for the E2E SLU network with novel Interface, Continuous Token Interface (CTI), the junctional representation of the ASR and NLU networks when both networks are pre-trained with the same vocabulary. Because the only difference is the noise level, we directly feed the ASR network's output to the NLU network. Thus, we can train our SLU network in an E2E manner without additional modules, such as Gumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset and achieve state-of-the-art scores on both intent classification and slot filling tasks. We also verify the NLU network, pre-trained with Masked Language Model, can utilize a noisy textual representation of CTI. Moreover, we show our model can be trained with multi-task learning from heterogeneous data even after integration with CTI.

</p>
</details>

<details><summary><b>Machine Learning and Glioblastoma: Treatment Response Monitoring Biomarkers in 2021</b>
<a href="https://arxiv.org/abs/2104.08072">arxiv:2104.08072</a>
&#x1F4C8; 5 <br>
<p>Thomas Booth, Bernice Akpinar, Andrei Roman, Haris Shuaib, Aysha Luis, Alysha Chelliah, Ayisha Al Busaidi, Ayesha Mirchandani, Burcu Alparslan, Nina Mansoor, Keyoumars Ashkan, Sebastien Ourselin, Marc Modat</p></summary>
<p>

**Abstract:** The aim of the systematic review was to assess recently published studies on diagnostic test accuracy of glioblastoma treatment response monitoring biomarkers in adults, developed through machine learning (ML). Articles were searched for using MEDLINE, EMBASE, and the Cochrane Register. Included study participants were adult patients with high grade glioma who had undergone standard treatment (maximal resection, radiotherapy with concomitant and adjuvant temozolomide) and subsequently underwent follow-up imaging to determine treatment response status. Risk of bias and applicability was assessed with QUADAS 2 methodology. Contingency tables were created for hold-out test sets and recall, specificity, precision, F1-score, balanced accuracy calculated. Fifteen studies were included with 1038 patients in training sets and 233 in test sets. To determine whether there was progression or a mimic, the reference standard combination of follow-up imaging and histopathology at re-operation was applied in 67% of studies. The small numbers of patient included in studies, the high risk of bias and concerns of applicability in the study designs (particularly in relation to the reference standard and patient selection due to confounding), and the low level of evidence, suggest that limited conclusions can be drawn from the data. There is likely good diagnostic performance of machine learning models that use MRI features to distinguish between progression and mimics. The diagnostic performance of ML using implicit features did not appear to be superior to ML using explicit features. There are a range of ML-based solutions poised to become treatment response monitoring biomarkers for glioblastoma. To achieve this, the development and validation of ML models require large, well-annotated datasets where the potential for confounding in the study design has been carefully considered.

</p>
</details>

<details><summary><b>Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</b>
<a href="https://arxiv.org/abs/2104.07283">arxiv:2104.07283</a>
&#x1F4C8; 5 <br>
<p>Clément Le Moine Veillon, Nicolas Obin, Axel Roebel</p></summary>
<p>

**Abstract:** This paper presents a end-to-end framework for the F0 transformation in the context of expressive voice conversion. A single neural network is proposed, in which a first module is used to learn F0 representation over different temporal scales and a second adversarial module is used to learn the transformation from one emotion to another. The first module is composed of a convolution layer with wavelet kernels so that the various temporal scales of F0 variations can be efficiently encoded. The single decomposition/transformation network allows to learn in a end-to-end manner the F0 decomposition that are optimal with respect to the transformation, directly from the raw F0 signal.

</p>
</details>

<details><summary><b>Federated Learning for Malware Detection in IoT Devices</b>
<a href="https://arxiv.org/abs/2104.09994">arxiv:2104.09994</a>
&#x1F4C8; 4 <br>
<p>Valerian Rey, Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Gérôme Bovet, Martin Jaggi</p></summary>
<p>

**Abstract:** This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware affecting IoT devices is presented. N-BaIoT, a dataset modeling network traffic of several real IoT devices while affected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware affecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The first one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participant's privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered. The baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to different attacks, even with a single adversary. The performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios. These functions provide a significant improvement against malicious participants, but more efforts are still needed to make federated approaches robust.

</p>
</details>

<details><summary><b>Out-of-Distribution Detection for Dermoscopic Image Classification</b>
<a href="https://arxiv.org/abs/2104.07819">arxiv:2104.07819</a>
&#x1F4C8; 4 <br>
<p>Mohammadreza Mohseni, Jordan Yap, William Yolland, Majid Razmara, M Stella Atkins</p></summary>
<p>

**Abstract:** Medical image diagnosis can be achieved by deep neural networks, provided there is enough varied training data for each disease class. However, a hitherto unknown disease class not encountered during training will inevitably be misclassified, even if predicted with low probability. This problem is especially important for medical image diagnosis, when an image of a hitherto unknown disease is presented for diagnosis, especially when the images come from the same image domain, such as dermoscopic skin images.
  Current out-of-distribution detection algorithms act unfairly when the in-distribution classes are imbalanced, by favouring the most numerous disease in the training sets. This could lead to false diagnoses for rare cases which are often medically important. We developed a novel yet simple method to train neural networks, which enables them to classify in-distribution dermoscopic skin disease images and also detect novel diseases from dermoscopic images at test time. We show that our BinaryHeads model not only does not hurt classification balanced accuracy when the data is imbalanced, but also consistently improves the balanced accuracy. We also introduce an important method to investigate the effectiveness of out-of-distribution detection methods based on presence of varying amounts of out-of-distribution data, which may arise in real-world settings.

</p>
</details>

<details><summary><b>Towards Robust Neural Retrieval Models with Synthetic Pre-Training</b>
<a href="https://arxiv.org/abs/2104.07800">arxiv:2104.07800</a>
&#x1F4C8; 4 <br>
<p>Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz, Vittorio Castelli, Heng Ji, Avirup Sil</p></summary>
<p>

**Abstract:** Recent work has shown that commonly available machine reading comprehension (MRC) datasets can be used to train high-performance neural information retrieval (IR) systems. However, the evaluation of neural IR has so far been limited to standard supervised learning settings, where they have outperformed traditional term matching baselines. We conduct in-domain and out-of-domain evaluations of neural IR, and seek to improve its robustness across different scenarios, including zero-shot settings. We show that synthetic training examples generated using a sequence-to-sequence generator can be effective towards this goal: in our experiments, pre-training with synthetic examples improves retrieval performance in both in-domain and out-of-domain evaluation on five different test sets.

</p>
</details>

<details><summary><b>Rethinking Text Line Recognition Models</b>
<a href="https://arxiv.org/abs/2104.07787">arxiv:2104.07787</a>
&#x1F4C8; 4 <br>
<p>Daniel Hernandez Diaz, Siyang Qin, Reeve Ingle, Yasuhisa Fujii, Alessandro Bissacco</p></summary>
<p>

**Abstract:** In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.

</p>
</details>

<details><summary><b>AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks</b>
<a href="https://arxiv.org/abs/2104.07770">arxiv:2104.07770</a>
&#x1F4C8; 4 <br>
<p>Haojin Yang, Zhen Shen, Yucheng Zhao</p></summary>
<p>

**Abstract:** Deep convolutional neural networks (CNN) have achieved astonishing results in a large variety of applications. However, using these models on mobile or embedded devices is difficult due to the limited memory and computation resources. Recently, the inverted residual block becomes the dominating solution for the architecture design of compact CNNs. In this work, we comprehensively investigated the existing design concepts, rethink the functional characteristics of two pointwise convolutions in the inverted residuals. We propose a novel design, called asymmetrical bottlenecks. Precisely, we adjust the first pointwise convolution dimension, enrich the information flow by feature reuse, and migrate saved computations to the second pointwise convolution. By doing so we can further improve the accuracy without increasing the computation overhead. The asymmetrical bottlenecks can be adopted as a drop-in replacement for the existing CNN blocks. We can thus create AsymmNet by easily stack those blocks according to proper depth and width conditions. Extensive experiments demonstrate that our proposed block design is more beneficial than the original inverted residual bottlenecks for mobile networks, especially useful for those ultralight CNNs within the regime of <220M MAdds. Code is available at https://github.com/Spark001/AsymmNet

</p>
</details>

<details><summary><b>Random Persistence Diagram Generation</b>
<a href="https://arxiv.org/abs/2104.07737">arxiv:2104.07737</a>
&#x1F4C8; 4 <br>
<p>Farzana Nasrin, Theodore Papamarkou, Na Gong, Orlando Rios, Vasileios Maroulas</p></summary>
<p>

**Abstract:** Topological data analysis (TDA) studies the shape patterns of data. Persistent homology (PH) is a widely used method in TDA that summarizes homological features of data at multiple scales and stores them in persistence diagrams (PDs). In this paper, we propose a random persistence diagram generation (RPDG) method that generates a sequence of random PDs from the ones produced by the data. RPDG is underpinned by (i) a model based on pairwise interacting point processes for inference of persistence diagrams, and (ii) by a reversible jump Markov chain Monte Carlo (RJ-MCMC) algorithm for generating samples of PDs. A first example, which is based on a synthetic dataset, demonstrates the efficacy of RPDG and provides a detailed comparison with other existing methods for sampling PDs. A second example demonstrates the utility of RPDG to solve a materials science problem given a real dataset of small sample size.

</p>
</details>

<details><summary><b>Towards A Process Model for Co-Creating AI Experiences</b>
<a href="https://arxiv.org/abs/2104.07595">arxiv:2104.07595</a>
&#x1F4C8; 4 <br>
<p>Hariharan Subramonyam, Colleen Seifert, Eytan Adar</p></summary>
<p>

**Abstract:** Thinking of technology as a design material is appealing. It encourages designers to explore the material's properties to understand its capabilities and limitations, a prerequisite to generative design thinking. However, as a material, AI resists this approach because its properties emerge as part of the design process itself. Therefore, designers and AI engineers must collaborate in new ways to create both the material and its application experience. We investigate the co-creation process through a design study with 10 pairs of designers and engineers. We find that design 'probes' with user data are a useful tool in defining AI materials. Through data probes, designers construct designerly representations of the envisioned AI experience (AIX) to identify desirable AI characteristics. Data probes facilitate divergent thinking, material testing, and design validation. Based on our findings, we propose a process model for co-creating AIX and offer design considerations for incorporating data probes in design tools.

</p>
</details>

<details><summary><b>Embedding Adaptation is Still Needed for Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2104.07255">arxiv:2104.07255</a>
&#x1F4C8; 4 <br>
<p>Sébastien M. R. Arnold, Fei Sha</p></summary>
<p>

**Abstract:** Constructing new and more challenging tasksets is a fruitful methodology to analyse and understand few-shot classification methods. Unfortunately, existing approaches to building those tasksets are somewhat unsatisfactory: they either assume train and test task distributions to be identical -- which leads to overly optimistic evaluations -- or take a "worst-case" philosophy -- which typically requires additional human labor such as obtaining semantic class relationships. We propose ATG, a principled clustering method to defining train and test tasksets without additional human knowledge. ATG models train and test task distributions while requiring them to share a predefined amount of information. We empirically demonstrate the effectiveness of ATG in generating tasksets that are easier, in-between, or harder than existing benchmarks, including those that rely on semantic information. Finally, we leverage our generated tasksets to shed a new light on few-shot classification: gradient-based methods -- previously believed to underperform -- can outperform metric-based ones when transfer is most challenging.

</p>
</details>

<details><summary><b>Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research</b>
<a href="https://arxiv.org/abs/2104.07874">arxiv:2104.07874</a>
&#x1F4C8; 3 <br>
<p>Denis Newman-Griffis, Jill Fain Lehman, Carolyn Rosé, Harry Hochheiser</p></summary>
<p>

**Abstract:** Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.

</p>
</details>

<details><summary><b>On the Importance of Trust in Next-Generation Networked CPS Systems: An AI Perspective</b>
<a href="https://arxiv.org/abs/2104.07853">arxiv:2104.07853</a>
&#x1F4C8; 3 <br>
<p>Anousheh Gholami, Nariman Torkzaban, John S. Baras</p></summary>
<p>

**Abstract:** With the increasing scale, complexity, and heterogeneity of the next generation networked systems, seamless control, management, and security of such systems becomes increasingly challenging. Many diverse applications have driven interest in networked systems, including large-scale distributed learning, multi-agent optimization, 5G service provisioning, and network slicing, etc. In this paper, we propose trust as a measure to evaluate the status of network agents and improve the decision-making process. We interpret trust as a relation among entities that participate in various protocols. Trust relations are based on evidence created by the interactions of entities within a protocol and may be a composite of multiple metrics such as availability, reliability, resilience, etc. depending on application context. We first elaborate on the importance of trust as a metric and then present a mathematical framework for trust computation and aggregation within a network. Then we show in practice, how trust can be integrated into network decision-making processes by presenting two examples. In the first example, we show how utilizing the trust evidence can improve the performance and the security of Federated Learning. Second, we show how a 5G network resource provisioning framework can be improved when augmented with a trust-aware decision-making scheme. We verify the validity of our trust-based approach through simulations. Finally, we explain the challenges associated with aggregating the trust evidence and briefly explain our ideas to tackle them.

</p>
</details>

<details><summary><b>Detect and Classify -- Joint Span Detection and Classification for Health Outcomes</b>
<a href="https://arxiv.org/abs/2104.07789">arxiv:2104.07789</a>
&#x1F4C8; 3 <br>
<p>Michael Abaho, Danushka Bollegala, Paula Williamson, Susanna Dodd</p></summary>
<p>

**Abstract:** A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a pre-defined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.

</p>
</details>

<details><summary><b>Ontology-based Feature Selection: A Survey</b>
<a href="https://arxiv.org/abs/2104.07720">arxiv:2104.07720</a>
&#x1F4C8; 3 <br>
<p>Konstantinos Sikelis, George E Tsekouras, Konstantinos I Kotis</p></summary>
<p>

**Abstract:** The SemanticWeb emerged as an extension to the traditional Web, towards adding meaning to a distributed Web of structured and linked data. At its core, the concept of ontology provides the means to semantically describe and structure information and data and expose it to software and human agents in a machine and human-readable form. For software agents to be realized, it is crucial to develop powerful artificial intelligence and machine learning techniques, able to extract knowledge from information and data sources and represent it in the underlying ontology. This survey aims to provide insight into key aspects of ontology-based knowledge extraction, from various sources such as text, images, databases and human expertise, with emphasis on the task of feature selection. First, some of the most common classification and feature selection algorithms are briefly presented. Then, selected methodologies, which utilize ontologies to represent features and perform feature selection and classification, are described. The presented examples span diverse application domains, e.g., medicine, tourism, mechanical and civil engineering, and demonstrate the feasibility and applicability of such methods.

</p>
</details>

<details><summary><b>Demographic-Guided Attention in Recurrent Neural Networks for Modeling Neuropathophysiological Heterogeneity</b>
<a href="https://arxiv.org/abs/2104.07654">arxiv:2104.07654</a>
&#x1F4C8; 3 <br>
<p>Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, Pamela Ventola, James S. Duncan</p></summary>
<p>

**Abstract:** Heterogeneous presentation of a neurological disorder suggests potential differences in the underlying pathophysiological changes that occur in the brain. We propose to model heterogeneous patterns of functional network differences using a demographic-guided attention (DGA) mechanism for recurrent neural network models for prediction from functional magnetic resonance imaging (fMRI) time-series data. The context computed from the DGA head is used to help focus on the appropriate functional networks based on individual demographic information. We demonstrate improved classification on 3 subsets of the ABIDE I dataset used in published studies that have previously produced state-of-the-art results, evaluating performance under a leave-one-site-out cross-validation framework for better generalizeability to new data. Finally, we provide examples of interpreting functional network differences based on individual demographic variables.

</p>
</details>

<details><summary><b>Robust Optimization for Multilingual Translation with Imbalanced Data</b>
<a href="https://arxiv.org/abs/2104.07639">arxiv:2104.07639</a>
&#x1F4C8; 3 <br>
<p>Xian Li, Hongyu Gong</p></summary>
<p>

**Abstract:** Multilingual models are parameter-efficient and especially effective in improving low-resource languages by leveraging crosslingual transfer. Despite recent advance in massive multilingual translation with ever-growing model and data, how to effectively train multilingual models has not been well understood. In this paper, we show that a common situation in multilingual training, data imbalance among languages, poses optimization tension between high resource and low resource languages where the found multilingual solution is often sub-optimal for low resources. We show that common training method which upsamples low resources can not robustly optimize population loss with risks of either underfitting high resource languages or overfitting low resource ones. Drawing on recent findings on the geometry of loss landscape and its effect on generalization, we propose a principled optimization algorithm, Curvature Aware Task Scaling (CATS), which adaptively rescales gradients from different tasks with a meta objective of guiding multilingual training to low-curvature neighborhoods with uniformly low loss for all languages. We ran experiments on common benchmarks (TED, WMT and OPUS-100) with varying degrees of data imbalance. CATS effectively improved multilingual optimization and as a result demonstrated consistent gains on low resources ($+0.8$ to $+2.2$ BLEU) without hurting high resources. In addition, CATS is robust to overparameterization and large batch size training, making it a promising training method for massive multilingual models that truly improve low resource languages.

</p>
</details>

<details><summary><b>Deep Learning-based Online Alternative Product Recommendations at Scale</b>
<a href="https://arxiv.org/abs/2104.07572">arxiv:2104.07572</a>
&#x1F4C8; 3 <br>
<p>Mingming Guo, Nian Yan, Xiquan Cui, San He Wu, Unaiza Ahsan, Rebecca West, Khalifeh Al Jadda</p></summary>
<p>

**Abstract:** Alternative recommender systems are critical for ecommerce companies. They guide customers to explore a massive product catalog and assist customers to find the right products among an overwhelming number of options. However, it is a non-trivial task to recommend alternative products that fit customer needs. In this paper, we use both textual product information (e.g. product titles and descriptions) and customer behavior data to recommend alternative products. Our results show that the coverage of alternative products is significantly improved in offline evaluations as well as recall and precision. The final A/B test shows that our algorithm increases the conversion rate by 12 percent in a statistically significant way. In order to better capture the semantic meaning of product information, we build a Siamese Network with Bidirectional LSTM to learn product embeddings. In order to learn a similarity space that better matches the preference of real customers, we use co-compared data from historical customer behavior as labels to train the network. In addition, we use NMSLIB to accelerate the computationally expensive kNN computation for millions of products so that the alternative recommendation is able to scale across the entire catalog of a major ecommerce site.

</p>
</details>

<details><summary><b>Effect of Post-processing on Contextualized Word Representations</b>
<a href="https://arxiv.org/abs/2104.07456">arxiv:2104.07456</a>
&#x1F4C8; 3 <br>
<p>Hassan Sajjad, Firoj Alam, Fahim Dalvi, Nadir Durrani</p></summary>
<p>

**Abstract:** Post-processing of static embedding has beenshown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principle components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy)and sequence classification tasks. Our findings raise interesting points in relation to theresearch studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.

</p>
</details>

<details><summary><b>Conditional independence for pretext task selection in Self-supervised speech representation learning</b>
<a href="https://arxiv.org/abs/2104.07388">arxiv:2104.07388</a>
&#x1F4C8; 3 <br>
<p>Salah Zaiem, Titouan Parcollet, Slim Essid</p></summary>
<p>

**Abstract:** Through solving pretext tasks, self-supervised learning (SSL) leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. A common pretext task consists in pretraining a SSL model on pseudo-labels derived from the original signal. This technique is particularly relevant for speech data where various meaningful signal processing features may serve as pseudo-labels. However, the process of selecting pseudo-labels, for speech or other types of data, remains mostly unexplored and currently relies on observing the results on the final downstream task. Nevertheless, this methodology is not sustainable at scale due to substantial computational (hence carbon) costs. Thus, this paper introduces a practical and theoretical framework to select relevant pseudo-labels with respect to a given downstream task. More precisely, we propose a functional estimator of the pseudo-label utility grounded in the conditional independence theory, which does not require any training. The experiments conducted on speaker recognition and automatic speech recognition validate our estimator, showing a significant correlation between the performance observed on the downstream task and the utility estimates obtained with our approach, facilitating the prospection of relevant pseudo-labels for self-supervised speech representation learning.

</p>
</details>

<details><summary><b>Variational Co-embedding Learning for Attributed Network Clustering</b>
<a href="https://arxiv.org/abs/2104.07295">arxiv:2104.07295</a>
&#x1F4C8; 3 <br>
<p>Shuiqiao Yang, Sunny Verma, Borui Cai, Jiaojiao Jiang, Kun Yu, Fang Chen, Shui Yu</p></summary>
<p>

**Abstract:** Recent works for attributed network clustering utilize graph convolution to obtain node embeddings and simultaneously perform clustering assignments on the embedding space. It is effective since graph convolution combines the structural and attributive information for node embedding learning. However, a major limitation of such works is that the graph convolution only incorporates the attribute information from the local neighborhood of nodes but fails to exploit the mutual affinities between nodes and attributes. In this regard, we propose a variational co-embedding learning model for attributed network clustering (VCLANC). VCLANC is composed of dual variational auto-encoders to simultaneously embed nodes and attributes. Relying on this, the mutual affinity information between nodes and attributes could be reconstructed from the embedding space and served as extra self-supervised knowledge for representation learning. At the same time, trainable Gaussian mixture model is used as priors to infer the node clustering assignments. To strengthen the performance of the inferred clusters, we use a mutual distance loss on the centers of the Gaussian priors and a clustering assignment hardening loss on the node embeddings. Experimental results on four real-world attributed network datasets demonstrate the effectiveness of the proposed VCLANC for attributed network clustering.

</p>
</details>

<details><summary><b>Generalising Discrete Action Spaces with Conditional Action Trees</b>
<a href="https://arxiv.org/abs/2104.07294">arxiv:2104.07294</a>
&#x1F4C8; 3 <br>
<p>Christopher Bamford, Alvaro Ovalle</p></summary>
<p>

**Abstract:** There are relatively few conventions followed in reinforcement learning (RL) environments to structure the action spaces. As a consequence the application of RL algorithms to tasks with large action spaces with multiple components require additional effort to adjust to different formats. In this paper we introduce {\em Conditional Action Trees} with two main objectives: (1) as a method of structuring action spaces in RL to generalise across several action space specifications, and (2) to formalise a process to significantly reduce the action space by decomposing it into multiple sub-spaces, favoring a multi-staged decision making approach. We show several proof-of-concept experiments validating our scheme, ranging from environments with basic discrete action spaces to those with large combinatorial action spaces commonly found in RTS-style games.

</p>
</details>

<details><summary><b>Continual Learning for Fake Audio Detection</b>
<a href="https://arxiv.org/abs/2104.07286">arxiv:2104.07286</a>
&#x1F4C8; 3 <br>
<p>Haoxin Ma, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, Chenglong Wang</p></summary>
<p>

**Abstract:** Fake audio attack becomes a major threat to the speaker verification system. Although current detection approaches have achieved promising results on dataset-specific scenarios, they encounter difficulties on unseen spoofing data. Fine-tuning and retraining from scratch have been applied to incorporate new data. However, fine-tuning leads to performance degradation on previous data. Retraining takes a lot of time and computation resources. Besides, previous data are unavailable due to privacy in some situations. To solve the above problems, this paper proposes detecting fake without forgetting, a continual-learning-based method, to make the model learn new spoofing attacks incrementally. A knowledge distillation loss is introduced to loss function to preserve the memory of original model. Supposing the distribution of genuine voice is consistent among different scenarios, an extra embedding similarity loss is used as another constraint to further do a positive sample alignment. Experiments are conducted on the ASVspoof2019 dataset. The results show that our proposed method outperforms fine-tuning by the relative reduction of average equal error rate up to 81.62%.

</p>
</details>

<details><summary><b>A Survey of Recent Abstract Summarization Techniques</b>
<a href="https://arxiv.org/abs/2105.00824">arxiv:2105.00824</a>
&#x1F4C8; 2 <br>
<p>Diyah Puspitaningrum</p></summary>
<p>

**Abstract:** This paper surveys several recent abstract summarization methods: T5, Pegasus, and ProphetNet. We implement the systems in two languages: English and Indonesian languages. We investigate the impact of pre-training models (one T5, three Pegasuses, three ProphetNets) on several Wikipedia datasets in English and Indonesian language and compare the results to the Wikipedia systems' summaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the best summarization. The most significant factors that influence ROUGE performance are coverage, density, and compression. The higher the scores, the better the summary. Other factors that influence the ROUGE scores are the pre-training goal, the dataset's characteristics, the dataset used for testing the pre-trained model, and the cross-lingual function. Several suggestions to improve this paper's limitation are: 1) assure that the dataset used for the pre-training model must sufficiently large, contains adequate instances for handling cross-lingual purpose; 2) Advanced process (finetuning) shall be reasonable. We recommend using the large dataset consists of comprehensive coverage of topics from many languages before implementing advanced processes such as the train-infer-train procedure to the zero-shot translation in the training stage of the pre-training model.

</p>
</details>

<details><summary><b>Predictor-Corrector(PC) Temporal Difference(TD) Learning (PCTD)</b>
<a href="https://arxiv.org/abs/2104.09620">arxiv:2104.09620</a>
&#x1F4C8; 2 <br>
<p>Caleb Bowyer</p></summary>
<p>

**Abstract:** Using insight from numerical approximation of ODEs and the problem formulation and solution methodology of TD learning through a Galerkin relaxation, I propose a new class of TD learning algorithms. After applying the improved numerical methods, the parameter being approximated has a guaranteed order of magnitude reduction in the Taylor Series error of the solution to the ODE for the parameter $θ(t)$ that is used in constructing the linearly parameterized value function. Predictor-Corrector Temporal Difference (PCTD) is what I call the translated discrete time Reinforcement Learning(RL) algorithm from the continuous time ODE using the theory of Stochastic Approximation(SA). Both causal and non-causal implementations of the algorithm are provided, and simulation results are listed for an infinite horizon task to compare the original TD(0) algorithm against both versions of PCTD(0).

</p>
</details>

<details><summary><b>LEx: A Framework for Operationalising Layers of Machine Learning Explanations</b>
<a href="https://arxiv.org/abs/2104.09612">arxiv:2104.09612</a>
&#x1F4C8; 2 <br>
<p>Ronal Singh, Upol Ehsan, Marc Cheong, Mark O. Riedl, Tim Miller</p></summary>
<p>

**Abstract:** Several social factors impact how people respond to AI explanations used to justify AI decisions affecting them personally. In this position paper, we define a framework called the \textit{layers of explanation} (LEx), a lens through which we can assess the appropriateness of different types of explanations. The framework uses the notions of \textit{sensitivity} (emotional responsiveness) of features and the level of \textit{stakes} (decision's consequence) in a domain to determine whether different types of explanations are \textit{appropriate} in a given context. We demonstrate how to use the framework to assess the appropriateness of different types of explanations in different domains.

</p>
</details>

<details><summary><b>AI supported Topic Modeling using KNIME-Workflows</b>
<a href="https://arxiv.org/abs/2104.09428">arxiv:2104.09428</a>
&#x1F4C8; 2 <br>
<p>Jamal Al Qundus, Silvio Peikert, Adrian Paschke</p></summary>
<p>

**Abstract:** Topic modeling algorithms traditionally model topics as list of weighted terms. These topic models can be used effectively to classify texts or to support text mining tasks such as text summarization or fact extraction. The general procedure relies on statistical analysis of term frequencies. The focus of this work is on the implementation of the knowledge-based topic modelling services in a KNIME workflow. A brief description and evaluation of the DBPedia-based enrichment approach and the comparative evaluation of enriched topic models will be outlined based on our previous work. DBpedia-Spotlight is used to identify entities in the input text and information from DBpedia is used to extend these entities. We provide a workflow developed in KNIME implementing this approach and perform a result comparison of topic modeling supported by knowledge base information to traditional LDA. This topic modeling approach allows semantic interpretation both by algorithms and by humans.

</p>
</details>

<details><summary><b>Assessment of deep learning based blood pressure prediction from PPG and rPPG signals</b>
<a href="https://arxiv.org/abs/2104.09313">arxiv:2104.09313</a>
&#x1F4C8; 2 <br>
<p>Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff, Mirco Fuchs</p></summary>
<p>

**Abstract:** Exploiting photoplethysmography signals (PPG) for non-invasive blood pressure (BP) measurement is interesting for various reasons. First, PPG can easily be measured using fingerclip sensors. Second, camera-based approaches allow to derive remote PPG (rPPG) signals similar to PPG and therefore provide the opportunity for non-invasive measurements of BP. Various methods relying on machine learning techniques have recently been published. Performances are often reported as the mean average error (MAE) on the data which is problematic. This work aims to analyze the PPG- and rPPG-based BP prediction error with respect to the underlying data distribution. First, we train established neural network (NN) architectures and derive an appropriate parameterization of input segments drawn from continuous PPG signals. Second, we apply this parameterization to a larger PPG dataset and train NNs to predict BP. The resulting prediction errors increase towards less frequent BP values. Third, we use transfer learning to train the NNs for rPPG based BP prediction. The resulting performances are similar to the PPG-only case. Finally, we apply a personalization technique and retrain our NNs with subject-specific data. This slightly reduces the prediction errors.

</p>
</details>

<details><summary><b>Efficient Ring-topology Decentralized Federated Learning with Deep Generative Models for Industrial Artificial Intelligent</b>
<a href="https://arxiv.org/abs/2104.08100">arxiv:2104.08100</a>
&#x1F4C8; 2 <br>
<p>Zhao Wang, Yifan Hu, Jun Xiao, Chao Wu</p></summary>
<p>

**Abstract:** By leveraging deep learning based technologies, the data-driven based approaches have reached great success with the rapid increase of data generated of Industrial Indernet of Things(IIot). However, security and privacy concerns are obstacles for data providers in many sensitive data-driven industrial scenarios, such as healthcare and auto-driving. Many Federated Learning(FL) approaches have been proposed with DNNs for IIoT applications, these works still suffer from low usability of data due to data incompleteness, low quality, insufficient quantity, sensitivity, etc. Therefore, we propose a ring-topogy based decentralized federated learning(RDFL) scheme for Deep Generative Models(DGMs), where DGMs is a promising solution for solving the aforementioned data usability issues. Compare with existing IIoT FL works, our RDFL schemes provides communication efficiency and maintain training performance to boost DGMs in target IIoT tasks. A novel ring FL topology as well as a map-reduce based synchronizing method are designed in the proposed RDFL to improve decentralized FL performance and bandwidth utilization. In addition, InterPlanetary File System(IPFS) is introduced to further improve communication efficiency and FL security. Extensive experiments have been taken to demonstate the superiority of RDFL with either independent and identically distributed(IID) datasets or non-independent and identically distributed(Non-IID) datasets.

</p>
</details>

<details><summary><b>Multiple feature fusion-based video face tracking for IoT big data</b>
<a href="https://arxiv.org/abs/2104.08096">arxiv:2104.08096</a>
&#x1F4C8; 2 <br>
<p>Tianping Li, Zhifeng Liu, Jianping Qiao</p></summary>
<p>

**Abstract:** With the advancement of IoT and artificial intelligence technologies, and the need for rapid application growth in fields such as security entrance control and financial business trade, facial information processing has become an important means for achieving identity authentication and information security. In this paper, we propose a multi-feature fusion algorithm based on integral histograms and a real-time update tracking particle filtering module. First, edge and colour features are extracted, weighting methods are used to weight the colour histogram and edge features to describe facial features, and fusion of colour and edge features is made adaptive by using fusion coefficients to improve face tracking reliability. Then, the integral histogram is integrated into the particle filtering algorithm to simplify the calculation steps of complex particles. Finally, the tracking window size is adjusted in real time according to the change in the average distance from the particle centre to the edge of the current model and the initial model to reduce the drift problem and achieve stable tracking with significant changes in the target dimension. The results show that the algorithm improves video tracking accuracy, simplifies particle operation complexity, improves the speed, and has good anti-interference ability and robustness.

</p>
</details>

<details><summary><b>NePTuNe: Neural Powered Tucker Network for Knowledge Graph Completion</b>
<a href="https://arxiv.org/abs/2104.07824">arxiv:2104.07824</a>
&#x1F4C8; 2 <br>
<p>Shashank Sonkar, Arzoo Katiyar, Richard G. Baraniuk</p></summary>
<p>

**Abstract:** Knowledge graphs link entities through relations to provide a structured representation of real world facts. However, they are often incomplete, because they are based on only a small fraction of all plausible facts. The task of knowledge graph completion via link prediction aims to overcome this challenge by inferring missing facts represented as links between entities. Current approaches to link prediction leverage tensor factorization and/or deep learning. Factorization methods train and deploy rapidly thanks to their small number of parameters but have limited expressiveness due to their underlying linear methodology. Deep learning methods are more expressive but also computationally expensive and prone to overfitting due to their large number of trainable parameters. We propose Neural Powered Tucker Network (NePTuNe), a new hybrid link prediction model that couples the expressiveness of deep models with the speed and size of linear models. We demonstrate that NePTuNe provides state-of-the-art performance on the FB15K-237 dataset and near state-of-the-art performance on the WN18RR dataset.

</p>
</details>

<details><summary><b>Estimating and Improving Dynamic Treatment Regimes With a Time-Varying Instrumental Variable</b>
<a href="https://arxiv.org/abs/2104.07822">arxiv:2104.07822</a>
&#x1F4C8; 2 <br>
<p>Shuxiao Chen, Bo Zhang</p></summary>
<p>

**Abstract:** Estimating dynamic treatment regimes (DTRs) from retrospective observational data is challenging as some degree of unmeasured confounding is often expected. In this work, we develop a framework of estimating properly defined "optimal" DTRs with a time-varying instrumental variable (IV) when unmeasured covariates confound the treatment and outcome, rendering the potential outcome distributions only partially identified. We derive a novel Bellman equation under partial identification, use it to define a generic class of estimands (termed IV-optimal DTRs), and study the associated estimation problem. We then extend the IV-optimality framework to tackle the policy improvement problem, delivering IV-improved DTRs that are guaranteed to perform no worse and potentially better than a pre-specified baseline DTR. Importantly, our IV-improvement framework opens up the possibility of strictly improving upon DTRs that are optimal under the no unmeasured confounding assumption (NUCA). We demonstrate via extensive simulations the superior performance of IV-optimal and IV-improved DTRs over the DTRs that are optimal only under the NUCA. In a real data example, we embed retrospective observational registry data into a natural, two-stage experiment with noncompliance using a time-varying IV and estimate useful IV-optimal DTRs that assign mothers to high-level or low-level neonatal intensive care units based on their prognostic variables.

</p>
</details>

<details><summary><b>Machine Learning Approaches for Type 2 Diabetes Prediction and Care Management</b>
<a href="https://arxiv.org/abs/2104.07820">arxiv:2104.07820</a>
&#x1F4C8; 2 <br>
<p>Aloysius Lim, Ashish Singh, Jody Chiam, Carly Eckert, Vikas Kumar, Muhammad Aurangzeb Ahmad, Ankur Teredesai</p></summary>
<p>

**Abstract:** Prediction of diabetes and its various complications has been studied in a number of settings, but a comprehensive overview of problem setting for diabetes prediction and care management has not been addressed in the literature. In this document we seek to remedy this omission in literature with an encompassing overview of diabetes complication prediction as well as situating this problem in the context of real world healthcare management. We illustrate various problems encountered in real world clinical scenarios via our own experience with building and deploying such models. In this manuscript we illustrate a Machine Learning (ML) framework for addressing the problem of predicting Type 2 Diabetes Mellitus (T2DM) together with a solution for risk stratification, intervention and management. These ML models align with how physicians think about disease management and mitigation, which comprises these four steps: Identify, Stratify, Engage, Measure.

</p>
</details>

<details><summary><b>A Method to Reveal Speaker Identity in Distributed ASR Training, and How to Counter It</b>
<a href="https://arxiv.org/abs/2104.07815">arxiv:2104.07815</a>
&#x1F4C8; 2 <br>
<p>Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter Chin, Françoise Beaufays</p></summary>
<p>

**Abstract:** End-to-end Automatic Speech Recognition (ASR) models are commonly trained over spoken utterances using optimization methods like Stochastic Gradient Descent (SGD). In distributed settings like Federated Learning, model training requires transmission of gradients over a network. In this work, we design the first method for revealing the identity of the speaker of a training utterance with access only to a gradient. We propose Hessian-Free Gradients Matching, an input reconstruction technique that operates without second derivatives of the loss function (required in prior works), which can be expensive to compute. We show the effectiveness of our method using the DeepSpeech model architecture, demonstrating that it is possible to reveal the speaker's identity with 34% top-1 accuracy (51% top-5 accuracy) on the LibriSpeech dataset. Further, we study the effect of two well-known techniques, Differentially Private SGD and Dropout, on the success of our method. We show that a dropout rate of 0.2 can reduce the speaker identity accuracy to 0% top-1 (0.5% top-5).

</p>
</details>

<details><summary><b>Learning User's confidence for active learning</b>
<a href="https://arxiv.org/abs/2104.07791">arxiv:2104.07791</a>
&#x1F4C8; 2 <br>
<p>Devis Tuia, Jordi Munoz-Mari</p></summary>
<p>

**Abstract:** In this paper, we study the applicability of active learning in operative scenarios: more particularly, we consider the well-known contradiction between the active learning heuristics, which rank the pixels according to their uncertainty, and the user's confidence in labeling, which is related to both the homogeneity of the pixel context and user's knowledge of the scene. We propose a filtering scheme based on a classifier that learns the confidence of the user in labeling, thus minimizing the queries where the user would not be able to provide a class for the pixel. The capacity of a model to learn the user's confidence is studied in detail, also showing the effect of resolution is such a learning task. Experiments on two QuickBird images of different resolutions (with and without pansharpening) and considering committees of users prove the efficiency of the filtering scheme proposed, which maximizes the number of useful queries with respect to traditional active learning.

</p>
</details>

<details><summary><b>Ridge Regression Neural Network for Pediatric Bone Age Assessment</b>
<a href="https://arxiv.org/abs/2104.07785">arxiv:2104.07785</a>
&#x1F4C8; 2 <br>
<p>Ibrahim Salim, A. Ben Hamza</p></summary>
<p>

**Abstract:** Bone age is an important measure for assessing the skeletal and biological maturity of children. Delayed or increased bone age is a serious concern for pediatricians, and needs to be accurately assessed in a bid to determine whether bone maturity is occurring at a rate consistent with chronological age. In this paper, we introduce a unified deep learning framework for bone age assessment using instance segmentation and ridge regression. The proposed approach consists of two integrated stages. In the first stage, we employ an image annotation and segmentation model to annotate and segment the hand from the radiographic image, followed by background removal. In the second stage, we design a regression neural network architecture composed of a pre-trained convolutional neural network for learning salient features from the segmented pediatric hand radiographs and a ridge regression output layer for predicting the bone age. Experimental evaluation on a dataset of hand radiographs demonstrates the competitive performance of our approach in comparison with existing deep learning based methods for bone age assessment.

</p>
</details>

<details><summary><b>Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain</b>
<a href="https://arxiv.org/abs/2104.07782">arxiv:2104.07782</a>
&#x1F4C8; 2 <br>
<p>Ha-Thanh Nguyen, Le-Minh Nguyen</p></summary>
<p>

**Abstract:** Legal English is a sublanguage that is important for everyone but not for everyone to understand. Pretrained models have become best practices among current deep learning approaches for different problems. It would be a waste or even a danger if these models were applied in practice without knowledge of the sublanguage of the law. In this paper, we raise the issue and propose a trivial solution by introducing BERTLaw a legal sublanguage pretrained model. The paper's experiments demonstrate the superior effectiveness of the method compared to the baseline pretrained model

</p>
</details>

<details><summary><b>Comparative Study of Learning Outcomes for Online Learning Platforms</b>
<a href="https://arxiv.org/abs/2104.07763">arxiv:2104.07763</a>
&#x1F4C8; 2 <br>
<p>Francois St-Hilaire, Nathan Burns, Robert Belfer, Muhammad Shayan, Ariella Smofsky, Dung Do Vu, Antoine Frau, Joseph Potochny, Farid Faraji, Vincent Pavero, Neroli Ko, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan, Adela Matajova, Laurent Charlin, Yoshua Bengio, Iulian Vlad Serban, Ekaterina Kochmar</p></summary>
<p>

**Abstract:** Personalization and active learning are key aspects to successful learning. These aspects are important to address in intelligent educational applications, as they help systems to adapt and close the gap between students with varying abilities, which becomes increasingly important in the context of online and distance learning. We run a comparative head-to-head study of learning outcomes for two popular online learning platforms: Platform A, which follows a traditional model delivering content over a series of lecture videos and multiple-choice quizzes, and Platform B, which creates a personalized learning environment and provides problem-solving exercises and personalized feedback. We report on the results of our study using pre- and post-assessment quizzes with participants taking courses on an introductory data science topic on two platforms. We observe a statistically significant increase in the learning outcomes on Platform B, highlighting the impact of well-designed and well-engineered technology supporting active learning and problem-based learning in online education. Moreover, the results of the self-assessment questionnaire, where participants reported on perceived learning gains, suggest that participants using Platform B improve their metacognition.

</p>
</details>

<details><summary><b>Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment</b>
<a href="https://arxiv.org/abs/2104.07719">arxiv:2104.07719</a>
&#x1F4C8; 2 <br>
<p>Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, Shih-Fu Chang</p></summary>
<p>

**Abstract:** Few-shot object detection (FSOD) aims to detect objects using only a few examples. How to adapt state-of-the-art object detectors to the few-shot domain remains challenging. Object proposal is a key ingredient in modern object detectors. However, the quality of proposals generated for few-shot classes using existing methods is far worse than that of many-shot classes, e.g., missing boxes for few-shot classes due to misclassification or inaccurate spatial locations with respect to true objects. To address the noisy proposal problem, we propose a novel meta-learning based FSOD model by jointly optimizing the few-shot proposal generation and fine-grained few-shot proposal classification. To improve proposal generation for few-shot classes, we propose to learn a lightweight metric-learning based prototype matching network, instead of the conventional simple linear object/nonobject classifier, e.g., used in RPN. Our non-linear classifier with the feature fusion network could improve the discriminative prototype matching and the proposal recall for few-shot classes. To improve the fine-grained few-shot proposal classification, we propose a novel attentive feature alignment method to address the spatial misalignment between the noisy proposals and few-shot classes, thus improving the performance of few-shot object detection. Meanwhile we learn a separate Faster R-CNN detection head for many-shot base classes and show strong performance of maintaining base-classes knowledge. Our model achieves state-of-the-art performance on multiple FSOD benchmarks over most of the shots and metrics.

</p>
</details>

<details><summary><b>Quantum Architecture Search via Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2104.07715">arxiv:2104.07715</a>
&#x1F4C8; 2 <br>
<p>En-Jui Kuo, Yao-Lung L. Fang, Samuel Yen-Chi Chen</p></summary>
<p>

**Abstract:** Recent advances in quantum computing have drawn considerable attention to building realistic application for and using quantum computers. However, designing a suitable quantum circuit architecture requires expert knowledge. For example, it is non-trivial to design a quantum gate sequence for generating a particular quantum state with as fewer gates as possible. We propose a quantum architecture search framework with the power of deep reinforcement learning (DRL) to address this challenge. In the proposed framework, the DRL agent can only access the Pauli-$X$, $Y$, $Z$ expectation values and a predefined set of quantum operations for learning the target quantum state, and is optimized by the advantage actor-critic (A2C) and proximal policy optimization (PPO) algorithms. We demonstrate a successful generation of quantum gate sequences for multi-qubit GHZ states without encoding any knowledge of quantum physics in the agent. The design of our framework is rather general and can be employed with other DRL architectures or optimization methods to study gate synthesis and compilation for many quantum states.

</p>
</details>

<details><summary><b>Higgs analysis with quantum classifiers</b>
<a href="https://arxiv.org/abs/2104.07692">arxiv:2104.07692</a>
&#x1F4C8; 2 <br>
<p>Vasileios Belis, Samuel González-Castillo, Christina Reissel, Sofia Vallecorsa, Elías F. Combarro, Günther Dissertori, Florentin Reiter</p></summary>
<p>

**Abstract:** We have developed two quantum classifier models for the $t\bar{t}H(b\bar{b})$ classification problem, both of which fall into the category of hybrid quantum-classical algorithms for Noisy Intermediate Scale Quantum devices (NISQ). Our results, along with other studies, serve as a proof of concept that Quantum Machine Learning (QML) methods can have similar or better performance, in specific cases of low number of training samples, with respect to conventional ML methods even with a limited number of qubits available in current hardware. To utilise algorithms with a low number of qubits -- to accommodate for limitations in both simulation hardware and real quantum hardware -- we investigated different feature reduction methods. Their impact on the performance of both the classical and quantum models was assessed. We addressed different implementations of two QML models, representative of the two main approaches to supervised quantum machine learning today: a Quantum Support Vector Machine (QSVM), a kernel-based method, and a Variational Quantum Circuit (VQC), a variational approach.

</p>
</details>

<details><summary><b>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</b>
<a href="https://arxiv.org/abs/2104.07650">arxiv:2104.07650</a>
&#x1F4C8; 2 <br>
<p>Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen</p></summary>
<p>

**Abstract:** Recently, prompt-tuning has achieved promising results for certain few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exist abundant semantic knowledge among the entities and relations that cannot be ignored. To this end, we focus on incorporating knowledge into prompt-tuning for relation extraction and propose a knowledge-aware prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject entity and relation knowledge into prompt construction with learnable virtual template words as well as answer words and synergistically optimize their representation with knowledge constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach.

</p>
</details>

<details><summary><b>Collective Iterative Learning Control: Exploiting Diversity in Multi-Agent Systems for Reference Tracking Tasks</b>
<a href="https://arxiv.org/abs/2104.07620">arxiv:2104.07620</a>
&#x1F4C8; 2 <br>
<p>Michael Meindl, Fabio Molinari, Dustin Lehmann, Thomas Seel</p></summary>
<p>

**Abstract:** Multi-agent systems (MASs) can autonomously learn to solve previously unknown tasks by means of each agent's individual intelligence as well as by collaborating and exploiting collective intelligence. This article considers a group of autonomous agents learning to track the same given reference trajectory in a possibly small number of trials. We propose a novel collective learning control method that combines iterative learning control (ILC) with a collective update strategy. We derive conditions for desirable convergence properties of such systems. We show that the proposed method allows the collective to combine the advantages of the agents' individual learning strategies and thereby overcomes trade-offs and limitations of single-agent ILC. This benefit is achieved by designing a heterogeneous collective, i.e., a different learning law is assigned to each agent. All theoretical results are confirmed in simulations and experiments with two-wheeled-inverted-pendulum robots (TWIPRs) that jointly learn to perform the desired maneuver.

</p>
</details>

<details><summary><b>Reward Optimization for Neural Machine Translation with Learned Metrics</b>
<a href="https://arxiv.org/abs/2104.07541">arxiv:2104.07541</a>
&#x1F4C8; 2 <br>
<p>Raphael Shu, Kang Min Yoo, Jung-Woo Ha</p></summary>
<p>

**Abstract:** Neural machine translation (NMT) models are conventionally trained with token-level negative log-likelihood (NLL), which does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. Multiple approaches are proposed to train NMT with BLEU as the reward, in order to directly improve the metric. However, it was reported that the gain in BLEU does not translate to real quality improvement, limiting the application in industry. Recently, it became clear to the community that BLEU has a low correlation with human judgment when dealing with state-of-the-art models. This leads to the emerging of model-based evaluation metrics. These new metrics are shown to have a much higher human correlation. In this paper, we investigate whether it is beneficial to optimize NMT models with the state-of-the-art model-based metric, BLEURT. We propose a contrastive-margin loss for fast and stable reward optimization suitable for large NMT models. In experiments, we perform automatic and human evaluations to compare models trained with smoothed BLEU and BLEURT to the baseline models. Results show that the reward optimization with BLEURT is able to increase the metric scores by a large margin, in contrast to limited gain when training with smoothed BLEU. The human evaluation shows that models trained with BLEURT improve adequacy and coverage of translations. Code is available via https://github.com/naver-ai/MetricMT.

</p>
</details>

<details><summary><b>On Energy-Based Models with Overparametrized Shallow Neural Networks</b>
<a href="https://arxiv.org/abs/2104.07531">arxiv:2104.07531</a>
&#x1F4C8; 2 <br>
<p>Carles Domingo-Enrich, Alberto Bietti, Eric Vanden-Eijnden, Joan Bruna</p></summary>
<p>

**Abstract:** Energy-based models (EBMs) are a simple yet powerful framework for generative modeling. They are based on a trainable energy function which defines an associated Gibbs measure, and they can be trained and sampled from via well-established statistical tools, such as MCMC. Neural networks may be used as energy function approximators, providing both a rich class of expressive models as well as a flexible device to incorporate data structure. In this work we focus on shallow neural networks. Building from the incipient theory of overparametrized neural networks, we show that models trained in the so-called "active" regime provide a statistical advantage over their associated "lazy" or kernel regime, leading to improved adaptivity to hidden low-dimensional structure in the data distribution, as already observed in supervised learning. Our study covers both maximum likelihood and Stein Discrepancy estimators, and we validate our theoretical results with numerical experiments on synthetic data.

</p>
</details>

<details><summary><b>A Sample-Based Training Method for Distantly Supervised Relation Extraction with Pre-Trained Transformers</b>
<a href="https://arxiv.org/abs/2104.07512">arxiv:2104.07512</a>
&#x1F4C8; 2 <br>
<p>Mehrdad Nasser, Mohamad Bagher Sajadi, Behrouz Minaei-Bidgoli</p></summary>
<p>

**Abstract:** Multiple instance learning (MIL) has become the standard learning paradigm for distantly supervised relation extraction (DSRE). However, due to relation extraction being performed at bag level, MIL has significant hardware requirements for training when coupled with large sentence encoders such as deep transformer neural networks. In this paper, we propose a novel sampling method for DSRE that relaxes these hardware requirements. In the proposed method, we limit the number of sentences in a batch by randomly sampling sentences from the bags in the batch. However, this comes at the cost of losing valid sentences from bags. To alleviate the issues caused by random sampling, we use an ensemble of trained models for prediction. We demonstrate the effectiveness of our approach by using our proposed learning setting to fine-tuning BERT on the widely NYT dataset. Our approach significantly outperforms previous state-of-the-art methods in terms of AUC and P@N metrics.

</p>
</details>

<details><summary><b>Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models</b>
<a href="https://arxiv.org/abs/2104.07505">arxiv:2104.07505</a>
&#x1F4C8; 2 <br>
<p>Karolina Stańczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan Cotterell, Isabelle Augenstein</p></summary>
<p>

**Abstract:** While the prevalence of large pre-trained language models has led to significant improvements in the performance of NLP systems, recent research has demonstrated that these models inherit societal biases extant in natural language. In this paper, we explore a simple method to probe pre-trained language models for gender bias, which we use to effect a multi-lingual study of gender bias towards politicians. We construct a dataset of 250k politicians from most countries in the world and quantify adjective and verb usage around those politicians' names as a function of their gender. We conduct our study in 7 languages across 6 different language modeling architectures. Our results demonstrate that stance towards politicians in pre-trained language models is highly dependent on the language used. Finally, contrary to previous findings, our study suggests that larger language models do not tend to be significantly more gender-biased than smaller ones.

</p>
</details>

<details><summary><b>Sparse online relative similarity learning</b>
<a href="https://arxiv.org/abs/2104.07501">arxiv:2104.07501</a>
&#x1F4C8; 2 <br>
<p>Dezhong Yao, Peilin Zhao, Chen Yu, Hai Jin, Bin Li</p></summary>
<p>

**Abstract:** For many data mining and machine learning tasks, the quality of a similarity measure is the key for their performance. To automatically find a good similarity measure from datasets, metric learning and similarity learning are proposed and studied extensively. Metric learning will learn a Mahalanobis distance based on positive semi-definite (PSD) matrix, to measure the distances between objectives, while similarity learning aims to directly learn a similarity function without PSD constraint so that it is more attractive. Most of the existing similarity learning algorithms are online similarity learning method, since online learning is more scalable than offline learning. However, most existing online similarity learning algorithms learn a full matrix with d 2 parameters, where d is the dimension of the instances. This is clearly inefficient for high dimensional tasks due to its high memory and computational complexity. To solve this issue, we introduce several Sparse Online Relative Similarity (SORS) learning algorithms, which learn a sparse model during the learning process, so that the memory and computational cost can be significantly reduced. We theoretically analyze the proposed algorithms, and evaluate them on some real-world high dimensional datasets. Encouraging empirical results demonstrate the advantages of our approach in terms of efficiency and efficacy.

</p>
</details>

<details><summary><b>Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching</b>
<a href="https://arxiv.org/abs/2104.07491">arxiv:2104.07491</a>
&#x1F4C8; 2 <br>
<p>Wenxin Hou, Jindong Wang, Xu Tan, Tao Qin, Takahiro Shinozaki</p></summary>
<p>

**Abstract:** End-to-end automatic speech recognition (ASR) can achieve promising performance with large-scale training data. However, it is known that domain mismatch between training and testing data often leads to a degradation of recognition accuracy. In this work, we focus on the unsupervised domain adaptation for ASR and propose CMatch, a Character-level distribution matching method to perform fine-grained adaptation between each character in two domains. First, to obtain labels for the features belonging to each character, we achieve frame-level label assignment using the Connectionist Temporal Classification (CTC) pseudo labels. Then, we match the character-level distributions using Maximum Mean Discrepancy. We train our algorithm using the self-training technique. Experiments on the Libri-Adapt dataset show that our proposed approach achieves 14.39% and 16.50% relative Word Error Rate (WER) reduction on both cross-device and cross-environment ASR. We also comprehensively analyze the different strategies for frame-level label assignment and Transformer adaptations.

</p>
</details>

<details><summary><b>NICE: An Algorithm for Nearest Instance Counterfactual Explanations</b>
<a href="https://arxiv.org/abs/2104.07411">arxiv:2104.07411</a>
&#x1F4C8; 2 <br>
<p>Dieter Brughmans, David Martens</p></summary>
<p>

**Abstract:** In this paper we suggest NICE: a new algorithm to generate counterfactual explanations for heterogeneous tabular data. The design of our algorithm specifically takes into account algorithmic requirements that often emerge in real-life deployments: the ability to provide an explanation for all predictions, being efficient in run-time, and being able to handle any classification model (also non-differentiable ones). More specifically, our approach exploits information from a nearest instance tospeed up the search process. We propose four versions of NICE, where three of them optimize the explanations for one of the following properties: sparsity, proximity or plausibility. An extensive empirical comparison on 10 datasets shows that our algorithm performs better on all properties than the current state-of-the-art. These analyses show a trade-off between on the one hand plausiblity and on the other hand proximity or sparsity, with our different optimization methods offering the choice to select the preferred trade-off. An open-source implementation of NICE can be found at https://github.com/ADMAntwerp/NICE.

</p>
</details>

<details><summary><b>All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2104.07329">arxiv:2104.07329</a>
&#x1F4C8; 2 <br>
<p>Cheng-Wei Huang, Tim-Wei Chen, Juinn-Dar Huang</p></summary>
<p>

**Abstract:** Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors - the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit - are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation - both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\%\sim 0.3\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.

</p>
</details>

<details><summary><b>Consistency Training with Virtual Adversarial Discrete Perturbation</b>
<a href="https://arxiv.org/abs/2104.07284">arxiv:2104.07284</a>
&#x1F4C8; 2 <br>
<p>Jungsoo Park, Gyuwan Kim, Jaewoo Kang</p></summary>
<p>

**Abstract:** We propose an effective consistency training framework that enforces a training model's predictions given original and perturbed inputs to be similar by adding a discrete noise that would incur the highest divergence between predictions. This virtual adversarial discrete noise obtained by replacing a small portion of tokens while keeping original semantics as much as possible efficiently pushes a training model's decision boundary. Moreover, we perform an iterative refinement process to alleviate the degraded fluency of the perturbed sentence due to the conditional independence assumption. Experimental results show that our proposed method outperforms other consistency training baselines with text editing, paraphrasing, or a continuous noise on semi-supervised text classification tasks and a robustness benchmark.

</p>
</details>

<details><summary><b>Rule-Based Reinforcement Learning for Efficient Robot Navigation with Space Reduction</b>
<a href="https://arxiv.org/abs/2104.07282">arxiv:2104.07282</a>
&#x1F4C8; 2 <br>
<p>Yuanyang Zhu, Zhi Wang, Chunlin Chen, Daoyi Dong</p></summary>
<p>

**Abstract:** For real-world deployments, it is critical to allow robots to navigate in complex environments autonomously. Traditional methods usually maintain an internal map of the environment, and then design several simple rules, in conjunction with a localization and planning approach, to navigate through the internal map. These approaches often involve a variety of assumptions and prior knowledge. In contrast, recent reinforcement learning (RL) methods can provide a model-free, self-learning mechanism as the robot interacts with an initially unknown environment, but are expensive to deploy in real-world scenarios due to inefficient exploration. In this paper, we focus on efficient navigation with the RL technique and combine the advantages of these two kinds of methods into a rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of time. First, we use the rule of wall-following to generate a closed-loop trajectory. Second, we employ a reduction rule to shrink the trajectory, which in turn effectively reduces the redundant exploration space. Besides, we give the detailed theoretical guarantee that the optimal navigation path is still in the reduced space. Third, in the reduced space, we utilize the Pledge rule to guide the exploration strategy for accelerating the RL process at the early stage. Experiments conducted on real robot navigation problems in hex-grid environments demonstrate that RuRL can achieve improved navigation performance.

</p>
</details>

<details><summary><b>Regularizing Models via Pointwise Mutual Information for Named Entity Recognition</b>
<a href="https://arxiv.org/abs/2104.07249">arxiv:2104.07249</a>
&#x1F4C8; 2 <br>
<p>Minbyul Jeong, Jaewoo Kang</p></summary>
<p>

**Abstract:** In Named Entity Recognition (NER), pre-trained language models have been overestimated by focusing on dataset biases to solve current benchmark datasets. However, these biases hinder generalizability which is necessary to address real-world situations such as weak name regularity and plenty of unseen mentions. To alleviate the use of dataset biases and make the models fully exploit data, we propose a debiasing method that our bias-only model can be replaced with a Pointwise Mutual Information (PMI) to enhance generalization ability while outperforming an in-domain performance. Our approach enables to debias highly correlated word and labels in the benchmark datasets; reflect informative statistics via subword frequency; alleviates a class imbalance between positive and negative examples. For long-named and complex-structure entities, our method can predict these entities through debiasing on conjunction or special characters. Extensive experiments on both general and biomedical domains demonstrate the effectiveness and generalization capabilities of the PMI.

</p>
</details>

<details><summary><b>Accurate Prediction of Free Solvation Energy of Organic Molecules via Graph Attention Network and Message Passing Neural Network from Pairwise Atomistic Interactions</b>
<a href="https://arxiv.org/abs/2105.02048">arxiv:2105.02048</a>
&#x1F4C8; 1 <br>
<p>Ramin Ansari, Amirata Ghorbani</p></summary>
<p>

**Abstract:** Deep learning based methods have been widely applied to predict various kinds of molecular properties in the pharmaceutical industry with increasingly more success. Solvation free energy is an important index in the field of organic synthesis, medicinal chemistry, drug delivery, and biological processes. However, accurate solvation free energy determination is a time-consuming experimental process. Furthermore, it could be useful to assess solvation free energy in the absence of a physical sample. In this study, we propose two novel models for the problem of free solvation energy predictions, based on the Graph Neural Network (GNN) architectures: Message Passing Neural Network (MPNN) and Graph Attention Network (GAT). GNNs are capable of summarizing the predictive information of a molecule as low-dimensional features directly from its graph structure without relying on an extensive amount of intra-molecular descriptors. As a result, these models are capable of making accurate predictions of the molecular properties without the time consuming process of running an experiment on each molecule. We show that our proposed models outperform all quantum mechanical and molecular dynamics methods in addition to existing alternative machine learning based approaches in the task of solvation free energy prediction. We believe such promising predictive models will be applicable to enhancing the efficiency of the screening of drug molecules and be a useful tool to promote the development of molecular pharmaceutics.

</p>
</details>

<details><summary><b>The mixed deep energy method for resolving concentration features in finite strain hyperelasticity</b>
<a href="https://arxiv.org/abs/2104.09623">arxiv:2104.09623</a>
&#x1F4C8; 1 <br>
<p>Jan N. Fuhg, Nikolaos Bouklas</p></summary>
<p>

**Abstract:** The introduction of Physics-informed Neural Networks (PINNs) has led to an increased interest in deep neural networks as universal approximators of PDEs in the solid mechanics community. Recently, the Deep Energy Method (DEM) has been proposed. DEM is based on energy minimization principles, contrary to PINN which is based on the residual of the PDEs. A significant advantage of DEM, is that it requires the approximation of lower order derivatives compared to formulations that are based on strong form residuals. However both DEM and classical PINN formulations struggle to resolve fine features of the stress and displacement fields, for example concentration features in solid mechanics applications. We propose an extension to the Deep Energy Method (DEM) to resolve these features for finite strain hyperelasticity. The developed framework termed mixed Deep Energy Method (mDEM) introduces stress measures as an additional output of the NN to the recently introduced pure displacement formulation. Using this approach, Neumann boundary conditions are approximated more accurately and the accuracy around spatial features which are typically responsible for high concentrations is increased. In order to make the proposed approach more versatile, we introduce a numerical integration scheme based on Delaunay integration, which enables the mDEM framework to be used for random training point position sets commonly needed for computational domains with stress concentrations. We highlight the advantages of the proposed approach while showing the shortcomings of classical PINN and DEM formulations. The method is offering comparable results to Finite-Element Method (FEM) on the forward calculation of challenging computational experiments involving domains with fine geometric features and concentrated loads.

</p>
</details>

<details><summary><b>Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings</b>
<a href="https://arxiv.org/abs/2104.07814">arxiv:2104.07814</a>
&#x1F4C8; 1 <br>
<p>Zihao He, Negar Mokhberian, Antonio Camara, Andres Abeliuk, Kristina Lerman</p></summary>
<p>

**Abstract:** Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research challenge. To address this gap, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE), a method to automatically detect polarized topics from partisan news sources. Specifically, utilizing a language model that has been finetuned on recognizing partisanship of the news articles, we represent the ideology of a news corpus on a topic by corpus-contextualized topic embedding and measure the polarization using cosine distance. We apply our method to a dataset of news articles about the COVID-19 pandemic. Extensive experiments on different news sources and topics demonstrate the efficacy of our method to capture topical polarization, as indicated by its effectiveness of retrieving the most polarized topics.

</p>
</details>

<details><summary><b>Variational Inference for Category Recommendation in E-Commerce platforms</b>
<a href="https://arxiv.org/abs/2104.07748">arxiv:2104.07748</a>
&#x1F4C8; 1 <br>
<p>Ramasubramanian Balasubramanian, Venugopal Mani, Abhinav Mathur, Sushant Kumar, Kannan Achan</p></summary>
<p>

**Abstract:** Category recommendation for users on an e-Commerce platform is an important task as it dictates the flow of traffic through the website. It is therefore important to surface precise and diverse category recommendations to aid the users' journey through the platform and to help them discover new groups of items. An often understated part in category recommendation is users' proclivity to repeat purchases. The structure of this temporal behavior can be harvested for better category recommendations and in this work, we attempt to harness this through variational inference. Further, to enhance the variational inference based optimization, we initialize the optimizer at better starting points through the well known Metapath2Vec algorithm. We demonstrate our results on two real-world datasets and show that our model outperforms standard baseline methods.

</p>
</details>

<details><summary><b>Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer</b>
<a href="https://arxiv.org/abs/2104.07667">arxiv:2104.07667</a>
&#x1F4C8; 1 <br>
<p>Meng Zhou, Shanglin Mo</p></summary>
<p>

**Abstract:** Shoulder replacement surgery, also called total shoulder replacement, is a common and complex surgery in Orthopedics discipline. It involves replacing a dead shoulder joint with an artificial implant. In the market, there are many artificial implant manufacturers and each of them may produce different implants with different structures compares to other providers. The problem arises in the following situation: a patient has some problems with the shoulder implant accessory and the manufacturer of that implant maybe unknown to either the patient or the doctor, therefore, correctly identification of the manufacturer is the key prior to the treatment. In this paper, we will demonstrate different methods for classifying the manufacturer of a shoulder implant. We will use Vision Transformer approach to this task for the first time ever

</p>
</details>

<details><summary><b>Piecewise-linear modelling with feature selection for Li-ion battery end of life prognosis</b>
<a href="https://arxiv.org/abs/2104.07576">arxiv:2104.07576</a>
&#x1F4C8; 1 <br>
<p>Samuel Greenbank, David A. Howey</p></summary>
<p>

**Abstract:** The complex nature of lithium-ion battery degradation has led to many machine learning based approaches to health forecasting being proposed in literature. However, machine learning can be computationally intensive. Linear approaches are faster but have previously been too inflexible for successful prognosis. For both techniques, the choice and quality of the inputs is a limiting factor of performance. Piecewise-linear models, combined with automated feature selection, offer a fast and flexible alternative without being as computationally intensive as machine learning. Here, a piecewise-linear approach to battery health forecasting was compared to a Gaussian process regression tool and found to perform equally well. The input feature selection process demonstrated the benefit of limiting the correlation between inputs. Further trials found that the piecewise-linear approach was robust to changing input size and availability of training data.

</p>
</details>

<details><summary><b>Measuring the Impact of Blockchain and Smart Contract on Construction Supply Chain Visibility</b>
<a href="https://arxiv.org/abs/2104.07532">arxiv:2104.07532</a>
&#x1F4C8; 1 <br>
<p>Hesam Hamledari, Martin Fischer</p></summary>
<p>

**Abstract:** This work assesses the impact of blockchain and smart contract on the visibility of construction supply chain and in the context of payments (intersection of cash and product flows). It uses comparative empirical experiments (Charrette Test Method) to draw comparisons between the visibility of state-of-practice and blockchain-enabled payment systems in a commercial construction project. Comparisons were drawn across four levels of granularity. The findings are twofold: 1) blockchain improved information completeness and information accuracy respectively by an average 216% and 261% compared with the digital state-of-practice solution. The improvements were significantly more pronounced for inquiries that had higher product, trade, and temporal granularity; 2) blockchain-enabled solution was robust in the face of increased granularity, while the conventional solution experienced 50% and 66.7% decline respectively in completeness and accuracy of information. The paper concludes with a discussion of mechanisms contributing to visibility and technology adoption based on business objectives.

</p>
</details>

<details><summary><b>QuickLoc: Adaptive Deep-Learning for Fast Indoor Localization with Mobile Devices</b>
<a href="https://arxiv.org/abs/2104.07521">arxiv:2104.07521</a>
&#x1F4C8; 1 <br>
<p>Saideep Tiku, Prathmesh Kale, Sudeep Pasricha</p></summary>
<p>

**Abstract:** Indoor localization services are a crucial aspect for the realization of smart cyber-physical systems within cities of the future. Such services are poised to reinvent the process of navigation and tracking of people and assets in a variety of indoor and subterranean environments. The growing ownership of computationally capable smartphones has laid the foundations of portable fingerprinting-based indoor localization through deep learning. However, as the demand for accurate localization increases, the computational complexity of the associated deep learning models increases as well. We present an approach for reducing the computational requirements of a deep learning-based indoor localization framework while maintaining localization accuracy targets. Our proposed methodology is deployed and validated across multiple smartphones and is shown to deliver up to 42% reduction in prediction latency and 45% reduction in prediction energy as compared to the best-known baseline deep learning-based indoor localization model.

</p>
</details>

<details><summary><b>Estimation of atrial fibrillation from lead-I ECGs: Comparison with cardiologists and machine learning model (CurAlive), a clinical validation study</b>
<a href="https://arxiv.org/abs/2104.07427">arxiv:2104.07427</a>
&#x1F4C8; 1 <br>
<p>N. Korucuk, C. Polat, E. S. Gunduz, O. Karaman, V. Tosun, M. Onac, N. Yildirim, Y. Cete, K. Polat</p></summary>
<p>

**Abstract:** Electrocardiogram recognition of cardiac arrhythmias is critical for cardiac abnormality diagnosis. Because of their strong prediction characteristics, artificial neural networks are the preferred method in medical diagnosis systems. This study presents a method to detect atrial fibrillation with lead-I ECGs using artificial intelligence. The aim of the study is to compare the accuracy of the diagnoses estimated by cardiologists and artificial intelligence over lead-I ECGs using 12-lead ECGs as references. To evaluate the performance of the proposed model, dataset were collected from China Physiological Signal Challenge 2018. In the study, diagnoses were examined in three groups as normal sinus rhythm, atrial fibrillation and OTHER. All rhythm and beat types except NSR and AFIB were labeled as OTHER super-class. OTHER contains First-degree atrioventricular blocks, Conduction disturbances, Left bundle branch block, Right bundle branch block, Premature atrial contraction, Premature ventricular contraction, ST-segment depression and ST-segment elevated type ECGs. CurAlive A.I. model which is using DenseNet as a CNN architecture and continuous wavelet transform as feature extraction method, showed a great performance on classifying ECGs from only lead-I compared to cardiologists. The AI model reached the weighted average precision, recall, F1-score and total accuracy 94.1%, 93.6%, 93.7% and 93.6% respectively, and the average of each of the three cardiologists has reached weighted average precision, recall, F1-score and total accuracy 82.2%, 54.6%, 57.5% and 54.6% respectively. This study showed that the proposed CNN model CurAlive, can be used to accurately diagnose AFIB, NSR, and OTHER rhythm using lead-I ECGs to accelerate the early detection of AFIB as a cardiologist assistant. It is also able to identify patients into different risk groups as part of remote patient monitoring systems.

</p>
</details>

<details><summary><b>Robust Backdoor Attacks against Deep Neural Networks in Real Physical World</b>
<a href="https://arxiv.org/abs/2104.07395">arxiv:2104.07395</a>
&#x1F4C8; 1 <br>
<p>Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu</p></summary>
<p>

**Abstract:** Deep neural networks (DNN) have been widely deployed in various applications. However, many researches indicated that DNN is vulnerable to backdoor attacks. The attacker can create a hidden backdoor in target DNN model, and trigger the malicious behaviors by submitting specific backdoor instance. However, almost all the existing backdoor works focused on the digital domain, while few studies investigate the backdoor attacks in real physical world. Restricted to a variety of physical constraints, the performance of backdoor attacks in the real physical world will be severely degraded. In this paper, we propose a robust physical backdoor attack method, PTB (physical transformations for backdoors), to implement the backdoor attacks against deep learning models in the real physical world. Specifically, in the training phase, we perform a series of physical transformations on these injected backdoor instances at each round of model training, so as to simulate various transformations that a backdoor may experience in real world, thus improves its physical robustness. Experimental results on the state-of-the-art face recognition model show that, compared with the backdoor methods that without PTB, the proposed attack method can significantly improve the performance of backdoor attacks in real physical world. Under various complex physical conditions, by injecting only a very small ratio (0.5%) of backdoor instances, the attack success rate of physical backdoor attacks with the PTB method on VGGFace is 82%, while the attack success rate of backdoor attacks without the proposed PTB method is lower than 11%. Meanwhile, the normal performance of the target DNN model has not been affected.

</p>
</details>

<details><summary><b>OneLog: Towards End-to-End Training in Software Log Anomaly Detection</b>
<a href="https://arxiv.org/abs/2104.07324">arxiv:2104.07324</a>
&#x1F4C8; 1 <br>
<p>Shayan Hashemi, Mika Mäntylä</p></summary>
<p>

**Abstract:** In recent years, with the growth of online services and IoT devices, software log anomaly detection has become a significant concern for both academia and industry. However, at the time of writing this paper, almost all contributions to the log anomaly detection task, follow the same traditional architecture based on parsing, vectorizing, and classifying. This paper proposes OneLog, a new approach that uses a large deep model based on instead of multiple small components. OneLog utilizes a character-based convolutional neural network (CNN) originating from traditional NLP tasks. This allows the model to take advantage of multiple datasets at once and take advantage of numbers and punctuations, which were removed in previous architectures. We evaluate OneLog using four open data sets Hadoop Distributed File System (HDFS), BlueGene/L (BGL), Hadoop, and OpenStack. We evaluate our model with single and multi-project datasets. Additionally, we evaluate robustness with synthetically evolved datasets and ahead-of-time anomaly detection test that indicates capabilities to predict anomalies before occurring. To the best of our knowledge, our multi-project model outperforms state-of-the-art methods in HDFS, Hadoop, and BGL datasets, respectively setting getting F1 scores of 99.99, 99.99, and 99.98. However, OneLog's performance on the Openstack is unsatisfying with F1 score of only 21.18. Furthermore, Onelogs performance suffers very little from noise showing F1 scores of 99.95, 99.92, and 99.98 in HDFS, Hadoop, and BGL.

</p>
</details>

<details><summary><b>Internet of quantum blockchains: security modeling and dynamic resource pricing for stable digital currency</b>
<a href="https://arxiv.org/abs/2104.07323">arxiv:2104.07323</a>
&#x1F4C8; 1 <br>
<p>Wanyang Dai</p></summary>
<p>

**Abstract:** Internet of quantum blockchains (IoB) will be the future Internet. In this paper, we make two new contributions to IoB: developing a block based quantum channel networking technology to handle its security modeling in face of the quantum supremacy and establishing IoB based FinTech platform model with dynamic pricing for stable digital currency. The interaction between our new contributions is also addressed. In doing so, we establish a generalized IoB security model by quantum channel networking in terms of both time and space quantum entanglements with quantum key distribution (QKD). Our IoB can interact with general structured things (e.g., supply chain systems) having online trading and payment capability via stable digital currency and can handle vector-valued data streams requiring synchronized services. Thus, within our designed QKD, a generalized random number generator for private and public keys is proposed by a mixed zero-sum and non-zero-sum resource-competition pricing policy. The effectiveness of this policy is justified by diffusion modeling with approximation theory and numerical implementations.

</p>
</details>

<details><summary><b>Vec2GC -- A Graph Based Clustering Method for Text Representations</b>
<a href="https://arxiv.org/abs/2104.09439">arxiv:2104.09439</a>
&#x1F4C8; 0 <br>
<p>Rajesh N Rao, Manojit Chakraborty</p></summary>
<p>

**Abstract:** NLP pipelines with limited or no labeled data, rely on unsupervised methods for document processing. Unsupervised approaches typically depend on clustering of terms or documents. In this paper, we introduce a novel clustering algorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline to cluster terms or documents for any given text corpus. Our method uses community detection on a weighted graph of the terms or documents, created using text representation learning. Vec2GC clustering algorithm is a density based approach, that supports hierarchical clustering as well.

</p>
</details>

<details><summary><b>The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning</b>
<a href="https://arxiv.org/abs/2104.07637">arxiv:2104.07637</a>
&#x1F4C8; 0 <br>
<p>Yuchen Lian, Arianna Bisazza, Tessa Verhoef</p></summary>
<p>

**Abstract:** Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during learning, instead of developing a more efficient or systematic language.

</p>
</details>

<details><summary><b>BAM: A Balanced Attention Mechanism for Single Image Super Resolution</b>
<a href="https://arxiv.org/abs/2104.07566">arxiv:2104.07566</a>
&#x1F4C8; 0 <br>
<p>Fanyi Wang, Haotian Hu, Cheng Shen</p></summary>
<p>

**Abstract:** Recovering texture information from the aliasing regions has always been a major challenge for Single Image Super Resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose a Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large scale feature maps while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently improve the networks performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.

</p>
</details>

<details><summary><b>Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities</b>
<a href="https://arxiv.org/abs/2104.07557">arxiv:2104.07557</a>
&#x1F4C8; 0 <br>
<p>Yuben Qu, Haipeng Dai, Yan Zhuang, Jiafa Chen, Chao Dong, Fan Wu, Song Guo</p></summary>
<p>

**Abstract:** Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support extensive applications in next-generation wireless networks in both civil and military fields. Empowering UAVs networks intelligence by artificial intelligence (AI) especially machine learning (ML) techniques is inevitable and appealing to enable the aforementioned applications. To solve the problems of traditional cloud-centric ML for UAV networks such as privacy concern, unacceptable latency, and resource burden, a distributed ML technique, \textit(i.e.), federated learning (FL), has been recently proposed to enable multiple UAVs to collaboratively train ML model without letting out raw data. However, almost all existing FL paradigms are still centralized, \textit{i.e.}, a central entity is in charge of ML model aggregation and fusion over the whole network, which could result in the issue of a single point of failure and are inappropriate to UAV networks with both unreliable nodes and links. Thus motivated, in this article, we propose a novel architecture called DFL-UN (\underline{D}ecentralized \underline{F}ederated \underline{L}earning for \underline{U}AV \underline{N}etworks), which enables FL within UAV networks without a central entity. We also conduct a preliminary simulation study to validate the feasibility and effectiveness of the DFL-UN architecture. Finally, we discuss the main challenges and potential research directions in the DFL-UN.

</p>
</details>

<details><summary><b>Ransomware Detection Using Deep Learning in the SCADA System of Electric Vehicle Charging Station</b>
<a href="https://arxiv.org/abs/2104.07409">arxiv:2104.07409</a>
&#x1F4C8; 0 <br>
<p>Manoj Basnet, Subash Poudyal, Mohd. Hasan Ali, Dipankar Dasgupta</p></summary>
<p>

**Abstract:** The Supervisory control and data acquisition (SCADA) systems have been continuously leveraging the evolution of network architecture, communication protocols, next-generation communication techniques (5G, 6G, Wi-Fi 6), and the internet of things (IoT). However, SCADA system has become the most profitable and alluring target for ransomware attackers. This paper proposes the deep learning-based novel ransomware detection framework in the SCADA controlled electric vehicle charging station (EVCS) with the performance analysis of three deep learning algorithms, namely deep neural network (DNN), 1D convolution neural network (CNN), and long short-term memory (LSTM) recurrent neural network. All three-deep learning-based simulated frameworks achieve around 97% average accuracy (ACC), more than 98% of the average area under the curve (AUC), and an average F1-score under 10-fold stratified cross-validation with an average false alarm rate (FAR) less than 1.88%. Ransomware driven distributed denial of service (DDoS) attack tends to shift the SOC profile by exceeding the SOC control thresholds. The severity has been found to increase as the attack progress and penetration increases. Also, ransomware driven false data injection (FDI) attack has the potential to damage the entire BES or physical system by manipulating the SOC control thresholds. It's a design choice and optimization issue that a deep learning algorithm can deploy based on the tradeoffs between performance metrics.

</p>
</details>

<details><summary><b>Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction</b>
<a href="https://arxiv.org/abs/2104.07396">arxiv:2104.07396</a>
&#x1F4C8; 0 <br>
<p>Dai Quoc Nguyen, Vinh Tong, Dinh Phung, Dat Quoc Nguyen</p></summary>
<p>

**Abstract:** We introduce a novel embedding model, named NoGE, which aims to integrate co-occurrence among entities and relations into graph neural networks to improve knowledge graph completion (i.e., link prediction). Given a knowledge graph, NoGE constructs a single graph considering entities and relations as individual nodes. NoGE then computes weights for edges among nodes based on the co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector representations for entity and relation nodes. NoGE then adopts a score function to produce the triple scores. Comprehensive experimental results show that NoGE obtains state-of-the-art results on three new and difficult benchmark datasets CoDEx for knowledge graph completion.

</p>
</details>

<details><summary><b>RIANN -- A Robust Neural Network Outperforms Attitude Estimation Filters</b>
<a href="https://arxiv.org/abs/2104.07391">arxiv:2104.07391</a>
&#x1F4C8; 0 <br>
<p>Daniel Weber, Clemens Gühmann, Thomas Seel</p></summary>
<p>

**Abstract:** Inertial-sensor-based attitude estimation is a crucial technology in various applications, from human motion tracking to autonomous aerial and ground vehicles. Application scenarios differ in characteristics of the performed motion, presence of disturbances, and environmental conditions. Since state-of-the-art attitude estimators do not generalize well over these characteristics, their parameters must be tuned for the individual motion characteristics and circumstances. We propose RIANN, a ready-to-use, neural network-based, parameter-free, real-time-capable inertial attitude estimator, which generalizes well across different motion dynamics, environments, and sampling rates, without the need for application-specific adaptations. We gather six publicly available datasets of which we exploit two datasets for the method development and the training, and we use four datasets for evaluation of the trained estimator in three different test scenarios with varying practical relevance. Results show that RIANN outperforms state-of-the-art attitude estimation filters in the sense that it generalizes much better across a variety of motions and conditions in different applications, with different sensor hardware and different sampling frequencies. This is true even if the filters are tuned on each individual test dataset, whereas RIANN was trained on completely separate data and has never seen any of these test datasets. RIANN can be applied directly without adaptations or training and is therefore expected to enable plug-and-play solutions in numerous applications, especially when accuracy is crucial but no ground-truth data is available for tuning or when motion and disturbance characteristics are uncertain. We made RIANN publicly available.

</p>
</details>

<details><summary><b>Do Deep Neural Networks Forget Facial Action Units? -- Exploring the Effects of Transfer Learning in Health Related Facial Expression Recognition</b>
<a href="https://arxiv.org/abs/2104.07389">arxiv:2104.07389</a>
&#x1F4C8; 0 <br>
<p>Pooja Prajod, Dominik Schiller, Tobias Huber, Elisabeth André</p></summary>
<p>

**Abstract:** In this paper, we present a process to investigate the effects of transfer learning for automatic facial expression recognition from emotions to pain. To this end, we first train a VGG16 convolutional neural network to automatically discern between eight categorical emotions. We then fine-tune successively larger parts of this network to learn suitable representations for the task of automatic pain recognition. Subsequently, we apply those fine-tuned representations again to the original task of emotion recognition to further investigate the differences in performance between the models. In the second step, we use Layer-wise Relevance Propagation to analyze predictions of the model that have been predicted correctly previously but are now wrongly classified. Based on this analysis, we rely on the visual inspection of a human observer to generate hypotheses about what has been forgotten by the model. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods. Our results show that the network, which was fully fine-tuned for pain recognition, indeed payed less attention to two action units that are relevant for expression recognition but not for pain recognition.

</p>
</details>

<details><summary><b>D-Cliques: Compensating for Data Heterogeneity with Topology in Decentralized Federated Learning</b>
<a href="https://arxiv.org/abs/2104.07365">arxiv:2104.07365</a>
&#x1F4C8; 0 <br>
<p>Aurélien Bellet, Anne-Marie Kermarrec, Erick Lavoie</p></summary>
<p>

**Abstract:** The convergence speed of machine learning models trained with Federated Learning is significantly affected by heterogeneous data partitions, even more so in a fully decentralized setting without a central server. In this paper, we show that the impact of label distribution skew, an important type of data heterogeneity, can be significantly reduced by carefully designing the underlying communication topology. We present D-Cliques, a novel topology that reduces gradient bias by grouping nodes in sparsely interconnected cliques such that the label distribution in a clique is representative of the global label distribution. We also show how to adapt the updates of decentralized SGD to obtain unbiased gradients and implement an effective momentum with D-Cliques. Our extensive empirical evaluation on MNIST and CIFAR10 demonstrates that our approach provides similar convergence speed as a fully-connected topology, which provides the best convergence in a data heterogeneous setting, with a significant reduction in the number of edges and messages. In a 1000-node topology, D-Cliques require 98% less edges and 96% less total messages, with further possible gains using a small-world topology across cliques.

</p>
</details>

<details><summary><b>COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images</b>
<a href="https://arxiv.org/abs/2104.07279">arxiv:2104.07279</a>
&#x1F4C8; 0 <br>
<p>Mohammad Saber Iraji, Mohammad-Reza Feizi-Derakhshi, Jafar Tanha</p></summary>
<p>

**Abstract:** The new Coronavirus is spreading rapidly, and it has taken the lives of many people so far. The virus has destructive effects on the human lung, and early detection is very important. Deep Convolution neural networks are such powerful tools in classifying images. Therefore, in this paper, a hybrid approach based on a deep network is presented. Feature vectors were extracted by applying a deep convolution neural network on the images, and useful features were selected by the binary differential meta-heuristic algorithm. These optimized features were given to the SVM classifier. A database consisting of three categories of images such as COVID-19, pneumonia, and healthy included in 1092 X-ray samples was considered. The proposed method achieved an accuracy of 99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results demonstrate that the suggested approach is better than recent studies on COVID-19 detection with X-ray images.

</p>
</details>

<details><summary><b>A Novel Neuron Model of Visual Processor</b>
<a href="https://arxiv.org/abs/2104.07257">arxiv:2104.07257</a>
&#x1F4C8; 0 <br>
<p>Jizhao Liu, Jing Lian, J C Sprott, Yide Ma</p></summary>
<p>

**Abstract:** Simulating and imitating the neuronal network of humans or mammals is a popular topic that has been explored for many years in the fields of pattern recognition and computer vision. Inspired by neuronal conduction characteristics in the primary visual cortex of cats, pulse-coupled neural networks (PCNNs) can exhibit synchronous oscillation behavior, which can process digital images without training. However, according to the study of single cells in the cat primary visual cortex, when a neuron is stimulated by an external periodic signal, the interspike-interval (ISI) distributions represent a multimodal distribution. This phenomenon cannot be explained by all PCNN models. By analyzing the working mechanism of the PCNN, we present a novel neuron model of the primary visual cortex consisting of a continuous-coupled neural network (CCNN). Our model inherited the threshold exponential decay and synchronous pulse oscillation property of the original PCNN model, and it can exhibit chaotic behavior consistent with the testing results of cat primary visual cortex neurons. Therefore, our CCNN model is closer to real visual neural networks. For image segmentation tasks, the algorithm based on CCNN model has better performance than the state-of-art of visual cortex neural network model. The strength of our approach is that it helps neurophysiologists further understand how the primary visual cortex works and can be used to quantitatively predict the temporal-spatial behavior of real neural networks. CCNN may also inspire engineers to create brain-inspired deep learning networks for artificial intelligence purposes.

</p>
</details>

<details><summary><b>Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps for Interval Regression</b>
<a href="https://arxiv.org/abs/2104.07245">arxiv:2104.07245</a>
&#x1F4C8; 0 <br>
<p>Shaily Kabir, Christian Wagner, Zack Ellerby</p></summary>
<p>

**Abstract:** Most of statistics and AI draw insights through modelling discord or variance between sources of information (i.e., inter-source uncertainty). Increasingly, however, research is focusing upon uncertainty arising at the level of individual measurements (i.e., within- or intra-source), such as for a given sensor output or human response. Here, adopting intervals rather than numbers as the fundamental data-type provides an efficient, powerful, yet challenging way forward -- offering systematic capture of uncertainty-at-source, increasing informational capacity, and ultimately potential for insight. Following recent progress in the capture of interval-valued data, including from human participants, conducting machine learning directly upon intervals is a crucial next step. This paper focuses on linear regression for interval-valued data as a recent growth area, providing an essential foundation for broader use of intervals in AI. We conduct an in-depth analysis of state-of-the-art methods, elucidating their behaviour, advantages, and pitfalls when applied to datasets with different properties. Specific emphasis is given to the challenge of preserving mathematical coherence -- i.e., ensuring that models maintain fundamental mathematical properties of intervals throughout -- and the paper puts forward extensions to an existing approach to guarantee this. Carefully designed experiments, using both synthetic and real-world data, are conducted -- with findings presented alongside novel visualizations for interval-valued regression outputs, designed to maximise model interpretability. Finally, the paper makes recommendations concerning method suitability for data sets with specific properties and highlights remaining challenges and important next steps for developing AI with the capacity to handle uncertainty-at-source.

</p>
</details>


[Next Page](2021/2021-04/2021-04-14.md)
