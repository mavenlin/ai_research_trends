## Summary for 2021-11-21, created on 2021-12-17


<details><summary><b>ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning</b>
<a href="https://arxiv.org/abs/2111.10952">arxiv:2111.10952</a>
&#x1F4C8; 3290 <br>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, Donald Metzler</p></summary>
<p>

**Abstract:** Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.

</p>
</details>

<details><summary><b>TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning</b>
<a href="https://arxiv.org/abs/2111.10756">arxiv:2111.10756</a>
&#x1F4C8; 9 <br>
<p>Keng Ji Chow, Samson Tan, Min-Yen Kan</p></summary>
<p>

**Abstract:** Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not evaluate the extent to which they represent visual and linguistic concepts in a unified space. Inspired by the crosslingual transfer and psycholinguistics literature, we propose a novel evaluation setting for V+L models: zero-shot cross-modal transfer. Existing V+L benchmarks also often report global accuracy scores on the entire dataset, rendering it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. To address this issue and enable the evaluation of cross-modal transfer, we present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. Each example encodes the scene bimodally such that either modality can be dropped during training/testing with no loss of relevant information. TraVLR's training and testing distributions are also constrained along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. We evaluate four state-of-the-art V+L models and find that although they perform well on the test set from the same modality, all models fail to transfer cross-modally and have limited success accommodating the addition or deletion of one modality. In alignment with prior work, we also find these models to require large amounts of data to learn simple spatial relationships. We release TraVLR as an open challenge for the research community.

</p>
</details>

<details><summary><b>Federated Social Recommendation with Graph Neural Network</b>
<a href="https://arxiv.org/abs/2111.10778">arxiv:2111.10778</a>
&#x1F4C8; 8 <br>
<p>Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, Philip S. Yu</p></summary>
<p>

**Abstract:** Recommender systems have become prosperous nowadays, designed to predict users' potential interests in items by learning embeddings. Recent developments of the Graph Neural Networks~(GNNs) also provide recommender systems with powerful backbones to learn embeddings from a user-item graph. However, only leveraging the user-item interactions suffers from the cold-start issue due to the difficulty in data collection. Hence, current endeavors propose fusing social information with user-item interactions to alleviate it, which is the social recommendation problem. Existing work employs GNNs to aggregate both social links and user-item interactions simultaneously. However, they all require centralized storage of the social links and item interactions of users, which leads to privacy concerns. Additionally, according to strict privacy protection under General Data Protection Regulation, centralized data storage may not be feasible in the future, urging a decentralized framework of social recommendation. To this end, we devise a novel framework \textbf{Fe}drated \textbf{So}cial recommendation with \textbf{G}raph neural network (FeSoG). Firstly, FeSoG adopts relational attention and aggregation to handle heterogeneity. Secondly, FeSoG infers user embeddings using local data to retain personalization. Last but not least, the proposed model employs pseudo-labeling techniques with item sampling to protect the privacy and enhance training. Extensive experiments on three real-world datasets justify the effectiveness of FeSoG in completing social recommendation and privacy protection. We are the first work proposing a federated learning framework for social recommendation to the best of our knowledge.

</p>
</details>

<details><summary><b>Many Heads but One Brain: an Overview of Fusion Brain Challenge on AI Journey 2021</b>
<a href="https://arxiv.org/abs/2111.10974">arxiv:2111.10974</a>
&#x1F4C8; 6 <br>
<p>Daria Bakshandaeva, Denis Dimitrov, Alex Shonenkov, Mark Potanin, Vladimir Arkhipkin, Denis Karachev, Vera Davydova, Anton Voronov, Mikhail Martynov, Natalia Semenova, Mikhail Stepnov, Elena Tutubalina, Andrey Chertok, Aleksandr Petiushko</p></summary>
<p>

**Abstract:** Supporting the current trend in the AI community, we propose the AI Journey 2021 Challenge called Fusion Brain which is targeted to make the universal architecture process different modalities (namely, images, texts, and code) and to solve multiple tasks for vision and language. The Fusion Brain Challenge https://github.com/sberbank-ai/fusion_brain_aij2021 combines the following specific tasks: Code2code Translation, Handwritten Text recognition, Zero-shot Object Detection, and Visual Question Answering. We have created datasets for each task to test the participants' submissions on it. Moreover, we have opened a new handwritten dataset in both Russian and English, which consists of 94,130 pairs of images and texts. The Russian part of the dataset is the largest Russian handwritten dataset in the world. We also propose the baseline solution and corresponding task-specific solutions as well as overall metrics.

</p>
</details>

<details><summary><b>Knowledge Based Multilingual Language Model</b>
<a href="https://arxiv.org/abs/2111.10962">arxiv:2111.10962</a>
&#x1F4C8; 6 <br>
<p>Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq Joty, Luo Si</p></summary>
<p>

**Abstract:** Knowledge enriched language representation learning has shown promising performance across various knowledge-intensive NLP tasks. However, existing knowledge based language models are all trained with monolingual knowledge graph data, which limits their application to more languages. In this work, we present a novel framework to pretrain knowledge based multilingual language models (KMLMs). We first generate a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by us, namely, logic reasoning. Our code and pretrained language models will be made publicly available.

</p>
</details>

<details><summary><b>FreqNet: A Frequency-domain Image Super-Resolution Network with Dicrete Cosine Transform</b>
<a href="https://arxiv.org/abs/2111.10800">arxiv:2111.10800</a>
&#x1F4C8; 6 <br>
<p>Runyuan Cai, Yue Ding, Hongtao Lu</p></summary>
<p>

**Abstract:** Single image super-resolution(SISR) is an ill-posed problem that aims to obtain high-resolution (HR) output from low-resolution (LR) input, during which extra high-frequency information is supposed to be added to improve the perceptual quality. Existing SISR works mainly operate in the spatial domain by minimizing the mean squared reconstruction error. Despite the high peak signal-to-noise ratios(PSNR) results, it is difficult to determine whether the model correctly adds desired high-frequency details. Some residual-based structures are proposed to guide the model to focus on high-frequency features implicitly. However, how to verify the fidelity of those artificial details remains a problem since the interpretation from spatial-domain metrics is limited. In this paper, we propose FreqNet, an intuitive pipeline from the frequency domain perspective, to solve this problem. Inspired by existing frequency-domain works, we convert images into discrete cosine transform (DCT) blocks, then reform them to obtain the DCT feature maps, which serve as the input and target of our model. A specialized pipeline is designed, and we further propose a frequency loss function to fit the nature of our frequency-domain task. Our SISR method in the frequency domain can learn the high-frequency information explicitly, provide fidelity and good perceptual quality for the SR images. We further observe that our model can be merged with other spatial super-resolution models to enhance the quality of their original SR output.

</p>
</details>

<details><summary><b>Inferring User Facial Affect in Work-like Settings</b>
<a href="https://arxiv.org/abs/2111.11862">arxiv:2111.11862</a>
&#x1F4C8; 5 <br>
<p>Chaudhary Muhammad Aqdus Ilyas, Siyang Song, Hatice Gunes</p></summary>
<p>

**Abstract:** Unlike the six basic emotions of happiness, sadness, fear, anger, disgust and surprise, modelling and predicting dimensional affect in terms of valence (positivity - negativity) and arousal (intensity) has proven to be more flexible, applicable and useful for naturalistic and real-world settings. In this paper, we aim to infer user facial affect when the user is engaged in multiple work-like tasks under varying difficulty levels (baseline, easy, hard and stressful conditions), including (i) an office-like setting where they undertake a task that is less physically demanding but requires greater mental strain; (ii) an assembly-line-like setting that requires the usage of fine motor skills; and (iii) an office-like setting representing teleworking and teleconferencing. In line with this aim, we first design a study with different conditions and gather multimodal data from 12 subjects. We then perform several experiments with various machine learning models and find that: (i) the display and prediction of facial affect vary from non-working to working settings; (ii) prediction capability can be boosted by using datasets captured in a work-like context; and (iii) segment-level (spectral representation) information is crucial in improving the facial affect prediction.

</p>
</details>

<details><summary><b>Deep Reinforced Attention Regression for Partial Sketch Based Image Retrieval</b>
<a href="https://arxiv.org/abs/2111.10917">arxiv:2111.10917</a>
&#x1F4C8; 5 <br>
<p>Dingrong Wang, Hitesh Sapkota, Xumin Liu, Qi Yu</p></summary>
<p>

**Abstract:** Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims at finding a specific image from a large gallery given a query sketch. Despite the widespread applicability of FG-SBIR in many critical domains (e.g., crime activity tracking), existing approaches still suffer from a low accuracy while being sensitive to external noises such as unnecessary strokes in the sketch. The retrieval performance will further deteriorate under a more practical on-the-fly setting, where only a partially complete sketch with only a few (noisy) strokes are available to retrieve corresponding images. We propose a novel framework that leverages a uniquely designed deep reinforcement learning model that performs a dual-level exploration to deal with partial sketch training and attention region selection. By enforcing the model's attention on the important regions of the original sketches, it remains robust to unnecessary stroke noises and improve the retrieval accuracy by a large margin. To sufficiently explore partial sketches and locate the important regions to attend, the model performs bootstrapped policy gradient for global exploration while adjusting a standard deviation term that governs a locator network for local exploration. The training process is guided by a hybrid loss that integrates a reinforcement loss and a supervised loss. A dynamic ranking reward is developed to fit the on-the-fly image retrieval process using partial sketches. The extensive experimentation performed on three public datasets shows that our proposed approach achieves the state-of-the-art performance on partial sketch based image retrieval.

</p>
</details>

<details><summary><b>Dynamic imaging using motion-compensated smoothness regularization on manifolds (MoCo-SToRM)</b>
<a href="https://arxiv.org/abs/2111.10887">arxiv:2111.10887</a>
&#x1F4C8; 5 <br>
<p>Qing Zou, Luis A. Torres, Sean B. Fain, Mathews Jacob</p></summary>
<p>

**Abstract:** We introduce an unsupervised deep manifold learning algorithm for motion-compensated dynamic MRI. We assume that the motion fields in a free-breathing lung MRI dataset live on a manifold. The motion field at each time instant is modeled as the output of a deep generative model, driven by low-dimensional time-varying latent vectors that capture the temporal variability. The images at each time instant are modeled as the deformed version of an image template using the above motion fields. The template, the parameters of the deep generator, and the latent vectors are learned from the k-t space data in an unsupervised fashion. The manifold motion model serves as a regularizer, making the joint estimation of the motion fields and images from few radial spokes/frame well-posed. The utility of the algorithm is demonstrated in the context of motion-compensated high-resolution lung MRI.

</p>
</details>

<details><summary><b>Vulcan: Solving the Steiner Tree Problem with Graph Neural Networks and Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2111.10810">arxiv:2111.10810</a>
&#x1F4C8; 5 <br>
<p>Haizhou Du, Zong Yan, Qiao Xiang, Qinqing Zhan</p></summary>
<p>

**Abstract:** Steiner Tree Problem (STP) in graphs aims to find a tree of minimum weight in the graph that connects a given set of vertices. It is a classic NP-hard combinatorial optimization problem and has many real-world applications (e.g., VLSI chip design, transportation network planning and wireless sensor networks). Many exact and approximate algorithms have been developed for STP, but they suffer from high computational complexity and weak worst-case solution guarantees, respectively. Heuristic algorithms are also developed. However, each of them requires application domain knowledge to design and is only suitable for specific scenarios. Motivated by the recently reported observation that instances of the same NP-hard combinatorial problem may maintain the same or similar combinatorial structure but mainly differ in their data, we investigate the feasibility and benefits of applying machine learning techniques to solving STP. To this end, we design a novel model Vulcan based on novel graph neural networks and deep reinforcement learning. The core of Vulcan is a novel, compact graph embedding that transforms highdimensional graph structure data (i.e., path-changed information) into a low-dimensional vector representation. Given an STP instance, Vulcan uses this embedding to encode its pathrelated information and sends the encoded graph to a deep reinforcement learning component based on a double deep Q network (DDQN) to find solutions. In addition to STP, Vulcan can also find solutions to a wide range of NP-hard problems (e.g., SAT, MVC and X3C) by reducing them to STP. We implement a prototype of Vulcan and demonstrate its efficacy and efficiency with extensive experiments using real-world and synthetic datasets.

</p>
</details>

<details><summary><b>COVID-19 Detection through Deep Feature Extraction</b>
<a href="https://arxiv.org/abs/2111.10762">arxiv:2111.10762</a>
&#x1F4C8; 5 <br>
<p>Jash Dalvi, Aziz Bohra</p></summary>
<p>

**Abstract:** The SARS-CoV2 virus has caused a lot of tribulation to the human population. Predictive modeling that can accurately determine whether a person is infected with COVID-19 is imperative. The study proposes a novel approach that utilizes deep feature extraction technique, pre-trained ResNet50 acting as the backbone of the network, combined with Logistic Regression as the head model. The proposed model has been trained on Kaggle COVID-19 Radiography Dataset. The proposed model achieves a cross-validation accuracy of 100% on the COVID-19 and Normal X-Ray image classes. Similarly, when tested on combined three classes, the proposed model achieves 98.84% accuracy.

</p>
</details>

<details><summary><b>A Pseudo-Inverse for Nonlinear Operators</b>
<a href="https://arxiv.org/abs/2111.10755">arxiv:2111.10755</a>
&#x1F4C8; 5 <br>
<p>Eyal Gofer, Guy Gilboa</p></summary>
<p>

**Abstract:** The Moore-Penrose inverse is widely used in physics, statistics and various fields of engineering. Among other characteristics, it captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we define and characterize the fundamental properties of a pseudo-inverse for nonlinear operators.
  The concept is defined broadly. First for general sets, and then a refinement for normed spaces. Our pseudo-inverse for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a pseudo-inverse and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the pseudo-inverse of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding and to regularized loss minimization.

</p>
</details>

<details><summary><b>Real-World Dexterous Object Manipulation based Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.04893">arxiv:2112.04893</a>
&#x1F4C8; 4 <br>
<p>Qingfeng Yao, Jilong Wang, Shuyu Yang</p></summary>
<p>

**Abstract:** Deep reinforcement learning has shown its advantages in real-time decision-making based on the state of the agent. In this stage, we solved the task of using a real robot to manipulate the cube to a given trajectory. The task is broken down into different procedures and we propose a hierarchical structure, the high-level deep reinforcement learning model selects appropriate contact positions and the low-level control module performs the position control under the corresponding trajectory. Our framework reduces the disadvantage of low sample efficiency of deep reinforcement learning and lacking adaptability of traditional robot control methods. Our algorithm is trained in simulation and migrated to reality without fine-tuning. The experimental results show the effectiveness of our method both simulation and reality. Our code and video can be found at https://github.com/42jaylonw/RRC2021ThreeWolves and https://youtu.be/Jr176xsn9wg.

</p>
</details>

<details><summary><b>Customizing an Affective Tutoring System Based on Facial Expression and Head Pose Estimation</b>
<a href="https://arxiv.org/abs/2111.14262">arxiv:2111.14262</a>
&#x1F4C8; 4 <br>
<p>Mahdi Pourmirzaei, Gholam Ali Montazer, Ebrahim Mousavi</p></summary>
<p>

**Abstract:** In recent years, the main problem in e-learning has shifted from analyzing content to personalization of learning environment by Intelligence Tutoring Systems (ITSs). Therefore, by designing personalized teaching models, learners are able to have a successful and satisfying experience in achieving their learning goals. Affective Tutoring Systems (ATSs) are some kinds of ITS that can recognize and respond to affective states of learner. In this study, we designed, implemented, and evaluated a system to personalize the learning environment based on the facial emotions recognition, head pose estimation, and cognitive style of learners. First, a unit called Intelligent Analyzer (AI) created which was responsible for recognizing facial expression and head angles of learners. Next, the ATS was built which mainly made of two units: ITS, IA. Results indicated that with the ATS, participants needed less efforts to pass the tests. In other words, we observed when the IA unit was activated, learners could pass the final tests in fewer attempts than those for whom the IA unit was deactivated. Additionally, they showed an improvement in terms of the mean passing score and academic satisfaction.

</p>
</details>

<details><summary><b>Multi-agent Bayesian Deep Reinforcement Learning for Microgrid Energy Management under Communication Failures</b>
<a href="https://arxiv.org/abs/2111.11868">arxiv:2111.11868</a>
&#x1F4C8; 4 <br>
<p>Hao Zhou, Atakan Aral, Ivona Brandic, Melike Erol-Kantarci</p></summary>
<p>

**Abstract:** Microgrids (MGs) are important players for the future transactive energy systems where a number of intelligent Internet of Things (IoT) devices interact for energy management in the smart grid. Although there have been many works on MG energy management, most studies assume a perfect communication environment, where communication failures are not considered. In this paper, we consider the MG as a multi-agent environment with IoT devices in which AI agents exchange information with their peers for collaboration. However, the collaboration information may be lost due to communication failures or packet loss. Such events may affect the operation of the whole MG. To this end, we propose a multi-agent Bayesian deep reinforcement learning (BA-DRL) method for MG energy management under communication failures. We first define a multi-agent partially observable Markov decision process (MA-POMDP) to describe agents under communication failures, in which each agent can update its beliefs on the actions of its peers. Then, we apply a double deep Q-learning (DDQN) architecture for Q-value estimation in BA-DRL, and propose a belief-based correlated equilibrium for the joint-action selection of multi-agent BA-DRL. Finally, the simulation results show that BA-DRL is robust to both power supply uncertainty and communication failure uncertainty. BA-DRL has 4.1% and 10.3% higher reward than Nash Deep Q-learning (Nash-DQN) and alternating direction method of multipliers (ADMM) respectively under 1% communication failure probability.

</p>
</details>

<details><summary><b>MidNet: An Anchor-and-Angle-Free Detector for Oriented Ship Detection in Aerial Images</b>
<a href="https://arxiv.org/abs/2111.10961">arxiv:2111.10961</a>
&#x1F4C8; 4 <br>
<p>Feng Jie, Yuping Liang, Junpeng Zhang, Xiangrong Zhang, Quanhe Yao, Licheng Jiao</p></summary>
<p>

**Abstract:** Ship detection in aerial images remains an active yet challenging task due to arbitrary object orientation and complex background from a bird's-eye perspective. Most of the existing methods rely on angular prediction or predefined anchor boxes, making these methods highly sensitive to unstable angular regression and excessive hyper-parameter setting. To address these issues, we replace the angular-based object encoding with an anchor-and-angle-free paradigm, and propose a novel detector deploying a center and four midpoints for encoding each oriented object, namely MidNet. MidNet designs a symmetrical deformable convolution customized for enhancing the midpoints of ships, then the center and midpoints for an identical ship are adaptively matched by predicting corresponding centripetal shift and matching radius. Finally, a concise analytical geometry algorithm is proposed to refine the centers and midpoints step-wisely for building precise oriented bounding boxes. On two public ship detection datasets, HRSC2016 and FGSD2021, MidNet outperforms the state-of-the-art detectors by achieving APs of 90.52% and 86.50%. Additionally, MidNet obtains competitive results in the ship detection of DOTA.

</p>
</details>

<details><summary><b>Adaptive Transfer Learning: a simple but effective transfer learning</b>
<a href="https://arxiv.org/abs/2111.10937">arxiv:2111.10937</a>
&#x1F4C8; 4 <br>
<p>Jung H Lee, Henry J Kvinge, Scott Howland, Zachary New, John Buckheit, Lauren A. Phillips, Elliott Skomski, Jessica Hibler, Courtney D. Corley, Nathan O. Hodas</p></summary>
<p>

**Abstract:** Transfer learning (TL) leverages previously obtained knowledge to learn new tasks efficiently and has been used to train deep learning (DL) models with limited amount of data. When TL is applied to DL, pretrained (teacher) models are fine-tuned to build domain specific (student) models. This fine-tuning relies on the fact that DL model can be decomposed to classifiers and feature extractors, and a line of studies showed that the same feature extractors can be used to train classifiers on multiple tasks. Furthermore, recent studies proposed multiple algorithms that can fine-tune teacher models' feature extractors to train student models more efficiently. We note that regardless of the fine-tuning of feature extractors, the classifiers of student models are trained with final outputs of feature extractors (i.e., the outputs of penultimate layers). However, a recent study suggested that feature maps in ResNets across layers could be functionally equivalent, raising the possibility that feature maps inside the feature extractors can also be used to train student models' classifiers. Inspired by this study, we tested if feature maps in the hidden layers of the teacher models can be used to improve the student models' accuracy (i.e., TL's efficiency). Specifically, we developed 'adaptive transfer learning (ATL)', which can choose an optimal set of feature maps for TL, and tested it in the few-shot learning setting. Our empirical evaluations suggest that ATL can help DL models learn more efficiently, especially when available examples are limited.

</p>
</details>

<details><summary><b>Self-supervised Semi-supervised Learning for Data Labeling and Quality Evaluation</b>
<a href="https://arxiv.org/abs/2111.10932">arxiv:2111.10932</a>
&#x1F4C8; 4 <br>
<p>Haoping Bai, Meng Cao, Ping Huang, Jiulong Shan</p></summary>
<p>

**Abstract:** As the adoption of deep learning techniques in industrial applications grows with increasing speed and scale, successful deployment of deep learning models often hinges on the availability, volume, and quality of annotated data. In this paper, we tackle the problems of efficient data labeling and annotation verification under the human-in-the-loop setting. We showcase that the latest advancements in the field of self-supervised visual representation learning can lead to tools and methods that benefit the curation and engineering of natural image datasets, reducing annotation cost and increasing annotation quality. We propose a unifying framework by leveraging self-supervised semi-supervised learning and use it to construct workflows for data labeling and annotation verification tasks. We demonstrate the effectiveness of our workflows over existing methodologies. On active learning task, our method achieves 97.0% Top-1 Accuracy on CIFAR10 with 0.1% annotated data, and 83.9% Top-1 Accuracy on CIFAR100 with 10% annotated data. When learning with 50% of wrong labels, our method achieves 97.4% Top-1 Accuracy on CIFAR10 and 85.5% Top-1 Accuracy on CIFAR100.

</p>
</details>

<details><summary><b>Video Content Swapping Using GAN</b>
<a href="https://arxiv.org/abs/2111.10916">arxiv:2111.10916</a>
&#x1F4C8; 4 <br>
<p>Tingfung Lau, Sailun Xu, Xinze Wang</p></summary>
<p>

**Abstract:** Video generation is an interesting problem in computer vision. It is quite popular for data augmentation, special effect in move, AR/VR and so on. With the advances of deep learning, many deep generative models have been proposed to solve this task. These deep generative models provide away to utilize all the unlabeled images and videos online, since it can learn deep feature representations with unsupervised manner. These models can also generate different kinds of images, which have great value for visual application. However generating a video would be much more challenging since we need to model not only the appearances of objects in the video but also their temporal motion. In this work, we will break down any frame in the video into content and pose. We first extract the pose information from a video using a pre-trained human pose detection and use a generative model to synthesize the video based on the content code and pose code.

</p>
</details>

<details><summary><b>Deep Image Prior using Stein's Unbiased Risk Estimator: SURE-DIP</b>
<a href="https://arxiv.org/abs/2111.10892">arxiv:2111.10892</a>
&#x1F4C8; 4 <br>
<p>Maneesh John, Hemant Kumar Aggarwal, Qing Zou, Mathews Jacob</p></summary>
<p>

**Abstract:** Deep learning algorithms that rely on extensive training data are revolutionizing image recovery from ill-posed measurements. Training data is scarce in many imaging applications, including ultra-high-resolution imaging. The deep image prior (DIP) algorithm was introduced for single-shot image recovery, completely eliminating the need for training data. A challenge with this scheme is the need for early stopping to minimize the overfitting of the CNN parameters to the noise in the measurements. We introduce a generalized Stein's unbiased risk estimate (GSURE) loss metric to minimize the overfitting. Our experiments show that the SURE-DIP approach minimizes the overfitting issues, thus offering significantly improved performance over classical DIP schemes. We also use the SURE-DIP approach with model-based unrolling architectures, which offers improved performance over direct inversion schemes.

</p>
</details>

<details><summary><b>XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2111.10854">arxiv:2111.10854</a>
&#x1F4C8; 4 <br>
<p>Jian Sun, Ali Pourramezan Fard, Mohammad H. Mahoor</p></summary>
<p>

**Abstract:** Although Capsule Networks show great abilities in defining the position relationship between features in deep neural networks for visual recognition tasks, they are computationally expensive and not suitable for running on mobile devices. The bottleneck is in the computational complexity of the Dynamic Routing mechanism used between capsules. On the other hand, neural networks such as XNOR-Net are fast and computationally efficient but have relatively low accuracy because of their information loss in the binarization process. This paper proposes a new class of Fully Connected (FC) Layers by xnorizing the linear projector outside or inside the Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR (Xnorizing Linear Projector Outside Dynamic Routing) and XnIDR (Xnorizing Linear Projector Inside Dynamic Routing). To test their generalization, we insert them into MobileNet V2 and ResNet-50 separately. Experiments on three datasets, MNIST, CIFAR-10, MultiMNIST validate their effectiveness. Our experimental results demonstrate that both XnODR and XnIDR help networks to have high accuracy with lower FLOPs and fewer parameters (e.g., 95.32\% accuracy with 2.99M parameters and 311.22M FLOPs on CIFAR-10).

</p>
</details>

<details><summary><b>Calibrated Diffusion Tensor Estimation</b>
<a href="https://arxiv.org/abs/2111.10847">arxiv:2111.10847</a>
&#x1F4C8; 4 <br>
<p>Davood Karimi, Simon K. Warfield, Ali Gholipour</p></summary>
<p>

**Abstract:** It is highly desirable to know how uncertain a model's predictions are, especially for models that are complex and hard to understand as in deep learning. Although there has been a growing interest in using deep learning methods in diffusion-weighted MRI, prior works have not addressed the issue of model uncertainty. Here, we propose a deep learning method to estimate the diffusion tensor and compute the estimation uncertainty. Data-dependent uncertainty is computed directly by the network and learned via loss attenuation. Model uncertainty is computed using Monte Carlo dropout. We also propose a new method for evaluating the quality of predicted uncertainties. We compare the new method with the standard least-squares tensor estimation and bootstrap-based uncertainty computation techniques. Our experiments show that when the number of measurements is small the deep learning method is more accurate and its uncertainty predictions are better calibrated than the standard methods. We show that the estimation uncertainties computed by the new method can highlight the model's biases, detect domain shift, and reflect the strength of noise in the measurements. Our study shows the importance and practical value of modeling prediction uncertainties in deep learning-based diffusion MRI analysis.

</p>
</details>

<details><summary><b>Is Speech Emotion Recognition Language-Independent? Analysis of English and Bangla Languages using Language-Independent Vocal Features</b>
<a href="https://arxiv.org/abs/2111.10776">arxiv:2111.10776</a>
&#x1F4C8; 4 <br>
<p>Fardin Saad, Hasan Mahmud, Md. Alamin Shaheen, Md. Kamrul Hasan, Paresha Farastu, Mohammad Ridwan Kabir</p></summary>
<p>

**Abstract:** A language agnostic approach to recognizing emotions from speech remains an incomplete and challenging task. In this paper, we used Bangla and English languages to assess whether distinguishing emotions from speech is independent of language. The following emotions were categorized for this study: happiness, anger, neutral, sadness, disgust, and fear. We employed three Emotional Speech Sets, of which the first two were developed by native Bengali speakers from the Islamic University of Technology in Bangla and English languages separately. The third was the Toronto Emotional Speech Set (TESS), which was developed by native English speakers from Canada. We carefully selected language-independent prosodic features, adopted a Support Vector Machine (SVM) model, and conducted three experiments to carry out our proposition. In the first experiment, we measured the performance of the three speech sets individually. This was followed by the second experiment, where we recorded the classification rate by combining the speech sets. Finally, in the third experiment we measured the recognition rate by training and testing the model with different speech sets. Although this study reveals that Speech Emotion Recognition (SER) is mostly language-independent, there is some disparity while recognizing emotional states like disgust and fear in these two languages. Moreover, our investigations inferred that non-native speakers convey emotions through speech, much like expressing themselves in their native tongue.

</p>
</details>

<details><summary><b>Distributed Unsupervised Visual Representation Learning with Fused Features</b>
<a href="https://arxiv.org/abs/2111.10763">arxiv:2111.10763</a>
&#x1F4C8; 4 <br>
<p>Yawen Wu, Zhepeng Wang, Dewen Zeng, Meng Li, Yiyu Shi, Jingtong Hu</p></summary>
<p>

**Abstract:** Federated learning (FL) enables distributed clients to learn a shared model for prediction while keeping the training data local on each client. However, existing FL requires fully-labeled data for training, which is inconvenient or sometimes infeasible to obtain due to the high labeling cost and the requirement of expertise. The lack of labels makes FL impractical in many realistic settings. Self-supervised learning can address this challenge by learning from unlabeled data such that FL can be widely used. Contrastive learning (CL), a self-supervised learning approach, can effectively learn data representations from unlabeled data. However, the distributed data collected on clients are usually not independent and identically distributed (non-IID) among clients, and each client may only have few classes of data, which degrades the performance of CL and learned representations. To tackle this problem, we propose a federated contrastive learning framework consisting of two approaches: feature fusion and neighborhood matching, by which a unified feature space among clients is learned for better data representations. Feature fusion provides remote features as accurate contrastive information to each client for better local learning. Neighborhood matching further aligns each client's local features to the remote features such that well-clustered features among clients can be learned. Extensive experiments show the effectiveness of the proposed framework. It outperforms other methods by 11\% on IID data and matches the performance of centralized learning.

</p>
</details>

<details><summary><b>Adversarial Mask: Real-World Adversarial Attack Against Face Recognition Models</b>
<a href="https://arxiv.org/abs/2111.10759">arxiv:2111.10759</a>
&#x1F4C8; 4 <br>
<p>Alon Zolfi, Shai Avidan, Yuval Elovici, Asaf Shabtai</p></summary>
<p>

**Abstract:** Deep learning-based facial recognition (FR) models have demonstrated state-of-the-art performance in the past few years, even when wearing protective medical face masks became commonplace during the COVID-19 pandemic. Given the outstanding performance of these models, the machine learning research community has shown increasing interest in challenging their robustness. Initially, researchers presented adversarial attacks in the digital domain, and later the attacks were transferred to the physical domain. However, in many cases, attacks in the physical domain are conspicuous, requiring, for example, the placement of a sticker on the face, and thus may raise suspicion in real-world environments (e.g., airports). In this paper, we propose Adversarial Mask, a physical adversarial universal perturbation (UAP) against state-of-the-art FR models that is applied on face masks in the form of a carefully crafted pattern. In our experiments, we examined the transferability of our adversarial mask to a wide range of FR model architectures and datasets. In addition, we validated our adversarial mask effectiveness in real-world experiments by printing the adversarial pattern on a fabric medical face mask, causing the FR system to identify only 3.34% of the participants wearing the mask (compared to a minimum of 83.34% with other evaluated masks).

</p>
</details>

<details><summary><b>Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability</b>
<a href="https://arxiv.org/abs/2111.10752">arxiv:2111.10752</a>
&#x1F4C8; 4 <br>
<p>Yifeng Xiong, Jiadong Lin, Min Zhang, John E. Hopcroft, Kun He</p></summary>
<p>

**Abstract:** The black-box adversarial attack has attracted impressive attention for its practical use in the field of deep learning security, meanwhile, it is very challenging as there is no access to the network architecture or internal weights of the target model. Based on the hypothesis that if an example remains adversarial for multiple models, then it is more likely to transfer the attack capability to other models, the ensemble-based adversarial attack methods are efficient and widely used for black-box attacks. However, ways of ensemble attack are rather less investigated, and existing ensemble attacks simply fuse the outputs of all the models evenly. In this work, we treat the iterative ensemble attack as a stochastic gradient descent optimization process, in which the variance of the gradients on different models may lead to poor local optima. To this end, we propose a novel attack method called the stochastic variance reduced ensemble (SVRE) attack, which could reduce the gradient variance of the ensemble models and take full advantage of the ensemble attack. Empirical results on the standard ImageNet dataset demonstrate that the proposed method could boost the adversarial transferability and outperforms existing ensemble attacks significantly.

</p>
</details>

<details><summary><b>Isomer: Transfer enhanced Dual-Channel Heterogeneous Dependency Attention Network for Aspect-based Sentiment Classification</b>
<a href="https://arxiv.org/abs/2112.03011">arxiv:2112.03011</a>
&#x1F4C8; 3 <br>
<p>Yukun Cao, Yijia Tang, Ziyue Wei, ChengKun Jin, Zeyu Miao, Yixin Fang, Haizhou Du, Feifei Xu</p></summary>
<p>

**Abstract:** Aspect-based sentiment classification aims to predict the sentiment polarity of a specific aspect in a sentence. However, most existing methods attempt to construct dependency relations into a homogeneous dependency graph with the sparsity and ambiguity, which cannot cover the comprehensive contextualized features of short texts or consider any additional node types or semantic relation information. To solve those issues, we present a sentiment analysis model named Isomer, which performs a dual-channel attention on heterogeneous dependency graphs incorporating external knowledge, to effectively integrate other additional information. Specifically, a transfer-enhanced dual-channel heterogeneous dependency attention network is devised in Isomer to model short texts using heterogeneous dependency graphs. These heterogeneous dependency graphs not only consider different types of information but also incorporate external knowledge. Experiments studies show that our model outperforms recent models on benchmark datasets. Furthermore, the results suggest that our method captures the importance of various information features to focus on informative contextual words.

</p>
</details>

<details><summary><b>Operations for Autonomous Spacecraft</b>
<a href="https://arxiv.org/abs/2111.10970">arxiv:2111.10970</a>
&#x1F4C8; 3 <br>
<p>Rebecca Castano, Tiago Vaquero, Federico Rossi, Vandi Verma, Ellen Van Wyk, Dan Allard, Bennett Huffmann, Erin M. Murphy, Nihal Dhamani, Robert A. Hewitt, Scott Davidoff, Rashied Amini, Anthony Barrett, Julie Castillo-Rogez, Steve A. Chien, Mathieu Choukroun, Alain Dadaian, Raymond Francis, Benjamin Gorr, Mark Hofstadter, Mitch Ingham, Cristina Sorice, Iain Tierney</p></summary>
<p>

**Abstract:** Onboard autonomy technologies such as planning and scheduling, identification of scientific targets, and content-based data summarization, will lead to exciting new space science missions. However, the challenge of operating missions with such onboard autonomous capabilities has not been studied to a level of detail sufficient for consideration in mission concepts. These autonomy capabilities will require changes to current operations processes, practices, and tools. We have developed a case study to assess the changes needed to enable operators and scientists to operate an autonomous spacecraft by facilitating a common model between the ground personnel and the onboard algorithms. We assess the new operations tools and workflows necessary to enable operators and scientists to convey their desired intent to the spacecraft, and to be able to reconstruct and explain the decisions made onboard and the state of the spacecraft. Mock-ups of these tools were used in a user study to understand the effectiveness of the processes and tools in enabling a shared framework of understanding, and in the ability of the operators and scientists to effectively achieve mission science objectives.

</p>
</details>

<details><summary><b>Hierarchical Knowledge Distillation for Dialogue Sequence Labeling</b>
<a href="https://arxiv.org/abs/2111.10957">arxiv:2111.10957</a>
&#x1F4C8; 3 <br>
<p>Shota Orihashi, Yoshihiro Yamazaki, Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro Tanaka, Ryo Masumura</p></summary>
<p>

**Abstract:** This paper presents a novel knowledge distillation method for dialogue sequence labeling. Dialogue sequence labeling is a supervised learning task that estimates labels for each utterance in the target dialogue document, and is useful for many applications such as dialogue act estimation. Accurate labeling is often realized by a hierarchically-structured large model consisting of utterance-level and dialogue-level networks that capture the contexts within an utterance and between utterances, respectively. However, due to its large model size, such a model cannot be deployed on resource-constrained devices. To overcome this difficulty, we focus on knowledge distillation which trains a small model by distilling the knowledge of a large and high performance teacher model. Our key idea is to distill the knowledge while keeping the complex contexts captured by the teacher model. To this end, the proposed method, hierarchical knowledge distillation, trains the small model by distilling not only the probability distribution of the label classification, but also the knowledge of utterance-level and dialogue-level contexts trained in the teacher model by training the model to mimic the teacher model's output in each level. Experiments on dialogue act estimation and call scene segmentation demonstrate the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>Renewable energy integration and microgrid energy trading using multi-agent deep reinforcement learning</b>
<a href="https://arxiv.org/abs/2111.10898">arxiv:2111.10898</a>
&#x1F4C8; 3 <br>
<p>Daniel J. B. Harrold, Jun Cao, Zhong Fan</p></summary>
<p>

**Abstract:** In this paper, multi-agent reinforcement learning is used to control a hybrid energy storage system working collaboratively to reduce the energy costs of a microgrid through maximising the value of renewable energy and trading. The agents must learn to control three different types of energy storage system suited for short, medium, and long-term storage under fluctuating demand, dynamic wholesale energy prices, and unpredictable renewable energy generation. Two case studies are considered: the first looking at how the energy storage systems can better integrate renewable energy generation under dynamic pricing, and the second with how those same agents can be used alongside an aggregator agent to sell energy to self-interested external microgrids looking to reduce their own energy bills. This work found that the centralised learning with decentralised execution of the multi-agent deep deterministic policy gradient and its state-of-the-art variants allowed the multi-agent methods to perform significantly better than the control from a single global agent. It was also found that using separate reward functions in the multi-agent approach performed much better than using a single control agent. Being able to trade with the other microgrids, rather than just selling back to the utility grid, also was found to greatly increase the grid's savings.

</p>
</details>

<details><summary><b>Health Monitoring of Industrial machines using Scene-Aware Threshold Selection</b>
<a href="https://arxiv.org/abs/2111.10897">arxiv:2111.10897</a>
&#x1F4C8; 3 <br>
<p>Arshdeep Singh, Raju Arvind, Padmanabhan Rajan</p></summary>
<p>

**Abstract:** This paper presents an autoencoder based unsupervised approach to identify anomaly in an industrial machine using sounds produced by the machine. The proposed framework is trained using log-melspectrogram representations of the sound signal. In classification, our hypothesis is that the reconstruction error computed for an abnormal machine is larger than that of the a normal machine, since only normal machine sounds are being used to train the autoencoder. A threshold is chosen to discriminate between normal and abnormal machines. However, the threshold changes as surrounding conditions vary. To select an appropriate threshold irrespective of the surrounding, we propose a scene classification framework, which can classify the underlying surrounding. Hence, the threshold can be selected adaptively irrespective of the surrounding. The experiment evaluation is performed on MIMII dataset for industrial machines namely fan, pump, valve and slide rail. Our experiment analysis shows that utilizing adaptive threshold, the performance improves significantly as that obtained using the fixed threshold computed for a given surrounding only.

</p>
</details>

<details><summary><b>Joint alignment and reconstruction of multislice dynamic MRI using variational manifold learning</b>
<a href="https://arxiv.org/abs/2111.10889">arxiv:2111.10889</a>
&#x1F4C8; 3 <br>
<p>Qing Zou, Abdul Haseeb Ahmed, Prashant Nagpal, Sarv Priya, Rolf F Schulte, Mathews Jacob</p></summary>
<p>

**Abstract:** Free-breathing cardiac MRI schemes are emerging as competitive alternatives to breath-held cine MRI protocols, enabling applicability to pediatric and other population groups that cannot hold their breath. Because the data from the slices are acquired sequentially, the cardiac/respiratory motion patterns may be different for each slice; current free-breathing approaches perform independent recovery of each slice. In addition to not being able to exploit the inter-slice redundancies, manual intervention or sophisticated post-processing methods are needed to align the images post-recovery for quantification. To overcome these challenges, we propose an unsupervised variational deep manifold learning scheme for the joint alignment and reconstruction of multislice dynamic MRI. The proposed scheme jointly learns the parameters of the deep network as well as the latent vectors for each slice, which capture the motion-induced dynamic variations, from the k-t space data of the specific subject. The variational framework minimizes the non-uniqueness in the representation, thus offering improved alignment and reconstructions.

</p>
</details>

<details><summary><b>Jointly Dynamic Topic Model for Recognition of Lead-lag Relationship in Two Text Corpora</b>
<a href="https://arxiv.org/abs/2111.10846">arxiv:2111.10846</a>
&#x1F4C8; 3 <br>
<p>Yandi Zhu, Xiaoling Lu, Jingya Hong, Feifei Wang</p></summary>
<p>

**Abstract:** Topic evolution modeling has received significant attentions in recent decades. Although various topic evolution models have been proposed, most studies focus on the single document corpus. However in practice, we can easily access data from multiple sources and also observe relationships between them. Then it is of great interest to recognize the relationship between multiple text corpora and further utilize this relationship to improve topic modeling. In this work, we focus on a special type of relationship between two text corpora, which we define as the "lead-lag relationship". This relationship characterizes the phenomenon that one text corpus would influence the topics to be discussed in the other text corpus in the future. To discover the lead-lag relationship, we propose a jointly dynamic topic model and also develop an embedding extension to address the modeling problem of large-scale text corpus. With the recognized lead-lag relationship, the similarities of the two text corpora can be figured out and the quality of topic learning in both corpora can be improved. We numerically investigate the performance of the jointly dynamic topic modeling approach using synthetic data. Finally, we apply the proposed model on two text corpora consisting of statistical papers and the graduation theses. Results show the proposed model can well recognize the lead-lag relationship between the two corpora, and the specific and shared topic patterns in the two corpora are also discovered.

</p>
</details>

<details><summary><b>Automated Controller Calibration by Kalman Filtering</b>
<a href="https://arxiv.org/abs/2111.10832">arxiv:2111.10832</a>
&#x1F4C8; 3 <br>
<p>Marcel Menner, Karl Berntorp, Stefano Di Cairano</p></summary>
<p>

**Abstract:** This paper proposes a method for calibrating control parameters. Examples of such control parameters are gains of PID controllers, weights of a cost function for optimal control, filter coefficients, the sliding surface of a sliding mode controller, or weights of a neural network. Hence, the proposed method can be applied to a wide range of controllers. The method uses a Kalman filter that estimates control parameters rather than the system's state, using data of closed-loop system operation. The control parameter calibration is driven by a training objective, which encompasses specifications on the performance of the dynamical system. The calibration method tunes the parameters online and robustly, is computationally efficient, has low data storage requirements, and is easy to implement making it appealing for many real-time applications. Simulation results show that the method is able to learn control parameters quickly (approximately 24% average decay factor of closed-loop cost), is able to tune the parameters to compensate for disturbances (approximately 29% improvement on tracking precision), and is robust to noise. Further, a simulation study with the high-fidelity vehicle simulator CarSim shows that the method can calibrate controllers of a complex dynamical system online, which indicates its applicability to a real-world system.

</p>
</details>

<details><summary><b>Learning by Active Forgetting for Neural Networks</b>
<a href="https://arxiv.org/abs/2111.10831">arxiv:2111.10831</a>
&#x1F4C8; 3 <br>
<p>Jian Peng, Xian Sun, Min Deng, Chao Tao, Bo Tang, Wenbo Li, Guohua Wu,  QingZhu, Yu Liu, Tao Lin, Haifeng Li</p></summary>
<p>

**Abstract:** Remembering and forgetting mechanisms are two sides of the same coin in a human learning-memory system. Inspired by human brain memory mechanisms, modern machine learning systems have been working to endow machine with lifelong learning capability through better remembering while pushing the forgetting as the antagonist to overcome. Nevertheless, this idea might only see the half picture. Up until very recently, increasing researchers argue that a brain is born to forget, i.e., forgetting is a natural and active process for abstract, rich, and flexible representations. This paper presents a learning model by active forgetting mechanism with artificial neural networks. The active forgetting mechanism (AFM) is introduced to a neural network via a "plug-and-play" forgetting layer (P\&PF), consisting of groups of inhibitory neurons with Internal Regulation Strategy (IRS) to adjust the extinction rate of themselves via lateral inhibition mechanism and External Regulation Strategy (ERS) to adjust the extinction rate of excitatory neurons via inhibition mechanism. Experimental studies have shown that the P\&PF offers surprising benefits: self-adaptive structure, strong generalization, long-term learning and memory, and robustness to data and parameter perturbation. This work sheds light on the importance of forgetting in the learning process and offers new perspectives to understand the underlying mechanisms of neural networks.

</p>
</details>

<details><summary><b>Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning</b>
<a href="https://arxiv.org/abs/2111.10827">arxiv:2111.10827</a>
&#x1F4C8; 3 <br>
<p>Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng</p></summary>
<p>

**Abstract:** Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods.

</p>
</details>

<details><summary><b>Structure-Preserving Graph Kernel for Brain Network Classification</b>
<a href="https://arxiv.org/abs/2111.10803">arxiv:2111.10803</a>
&#x1F4C8; 3 <br>
<p>Zhaomin Kong, Aditya Kendre, Jun Yu, Hao Peng, Carl Yang, Lichao Sun, Alex Leow, Lifang He</p></summary>
<p>

**Abstract:** This paper presents a novel graph-based kernel learning approach for connectome analysis. Specifically, we demonstrate how to leverage the naturally available structure within the graph representation to encode prior knowledge in the kernel. We first proposed a matrix factorization to directly extract structural features from natural symmetric graph representations of connectome data. We then used them to derive a structure-persevering graph kernel to be fed into the support vector machine. The proposed approach has the advantage of being clinically interpretable. Quantitative evaluations on challenging HIV disease classification (DTI- and fMRI-derived connectome data) and emotion recognition (EEG-derived connectome data) tasks demonstrate the superior performance of our proposed methods against the state-of-the-art. Results showed that relevant EEG-connectome information is primarily encoded in the alpha band during the emotion regulation task.

</p>
</details>

<details><summary><b>DuDoTrans: Dual-Domain Transformer Provides More Attention for Sinogram Restoration in Sparse-View CT Reconstruction</b>
<a href="https://arxiv.org/abs/2111.10790">arxiv:2111.10790</a>
&#x1F4C8; 3 <br>
<p>Ce Wang, Kun Shang, Haimiao Zhang, Qian Li, Yuan Hui, S. Kevin Zhou</p></summary>
<p>

**Abstract:** While Computed Tomography (CT) reconstruction from X-ray sinograms is necessary for clinical diagnosis, iodine radiation in the imaging process induces irreversible injury, thereby driving researchers to study sparse-view CT reconstruction, that is, recovering a high-quality CT image from a sparse set of sinogram views. Iterative models are proposed to alleviate the appeared artifacts in sparse-view CT images, but the computation cost is too expensive. Then deep-learning-based methods have gained prevalence due to the excellent performances and lower computation. However, these methods ignore the mismatch between the CNN's \textbf{local} feature extraction capability and the sinogram's \textbf{global} characteristics. To overcome the problem, we propose \textbf{Du}al-\textbf{Do}main \textbf{Trans}former (\textbf{DuDoTrans}) to simultaneously restore informative sinograms via the long-range dependency modeling capability of Transformer and reconstruct CT image with both the enhanced and raw sinograms. With such a novel design, reconstruction performance on the NIH-AAPM dataset and COVID-19 dataset experimentally confirms the effectiveness and generalizability of DuDoTrans with fewer involved parameters. Extensive experiments also demonstrate its robustness with different noise-level scenarios for sparse-view CT reconstruction. The code and models are publicly available at https://github.com/DuDoTrans/CODE

</p>
</details>

<details><summary><b>One-shot Weakly-Supervised Segmentation in Medical Images</b>
<a href="https://arxiv.org/abs/2111.10773">arxiv:2111.10773</a>
&#x1F4C8; 3 <br>
<p>Wenhui Lei, Qi Su, Ran Gu, Na Wang, Xinglong Liu, Guotai Wang, Xiaofan Zhang, Shaoting Zhang</p></summary>
<p>

**Abstract:** Deep neural networks usually require accurate and a large number of annotations to achieve outstanding performance in medical image segmentation. One-shot segmentation and weakly-supervised learning are promising research directions that lower labeling effort by learning a new class from only one annotated image and utilizing coarse labels instead, respectively. Previous works usually fail to leverage the anatomical structure and suffer from class imbalance and low contrast problems. Hence, we present an innovative framework for 3D medical image segmentation with one-shot and weakly-supervised settings. Firstly a propagation-reconstruction network is proposed to project scribbles from annotated volume to unlabeled 3D images based on the assumption that anatomical patterns in different human bodies are similar. Then a dual-level feature denoising module is designed to refine the scribbles based on anatomical- and pixel-level features. After expanding the scribbles to pseudo masks, we could train a segmentation model for the new class with the noisy label training strategy. Experiments on one abdomen and one head-and-neck CT dataset show the proposed method obtains significant improvement over the state-of-the-art methods and performs robustly even under severe class imbalance and low contrast.

</p>
</details>

<details><summary><b>Network representation learning: A macro and micro view</b>
<a href="https://arxiv.org/abs/2111.10772">arxiv:2111.10772</a>
&#x1F4C8; 3 <br>
<p>Xueyi Liu, Jie Tang</p></summary>
<p>

**Abstract:** Graph is a universe data structure that is widely used to organize data in real-world. Various real-word networks like the transportation network, social and academic network can be represented by graphs. Recent years have witnessed the quick development on representing vertices in the network into a low-dimensional vector space, referred to as network representation learning. Representation learning can facilitate the design of new algorithms on the graph data. In this survey, we conduct a comprehensive review of current literature on network representation learning. Existing algorithms can be categorized into three groups: shallow embedding models, heterogeneous network embedding models, graph neural network based models. We review state-of-the-art algorithms for each category and discuss the essential differences between these algorithms. One advantage of the survey is that we systematically study the underlying theoretical foundations underlying the different categories of algorithms, which offers deep insights for better understanding the development of the network representation learning field.

</p>
</details>

<details><summary><b>Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation</b>
<a href="https://arxiv.org/abs/2111.10919">arxiv:2111.10919</a>
&#x1F4C8; 2 <br>
<p>Dylan J. Foster, Akshay Krishnamurthy, David Simchi-Levi, Yunzong Xu</p></summary>
<p>

**Abstract:** We consider the offline reinforcement learning problem, where the aim is to learn a decision making policy from logged data. Offline RL -- particularly when coupled with (value) function approximation to allow for generalization in large or continuous state spaces -- is becoming increasingly relevant in practice, because it avoids costly and time-consuming online data collection and is well suited to safety-critical domains. Existing sample complexity guarantees for offline value function approximation methods typically require both (1) distributional assumptions (i.e., good coverage) and (2) representational assumptions (i.e., ability to represent some or all $Q$-value functions) stronger than what is required for supervised learning. However, the necessity of these conditions and the fundamental limits of offline RL are not well understood in spite of decades of research. This led Chen and Jiang (2019) to conjecture that concentrability (the most standard notion of coverage) and realizability (the weakest representation condition) alone are not sufficient for sample-efficient offline RL. We resolve this conjecture in the positive by proving that in general, even if both concentrability and realizability are satisfied, any algorithm requires sample complexity polynomial in the size of the state space to learn a non-trivial policy.
  Our results show that sample-efficient offline reinforcement learning requires either restrictive coverage conditions or representation conditions that go beyond supervised learning, and highlight a phenomenon called over-coverage which serves as a fundamental barrier for offline value function approximation methods. A consequence of our results for reinforcement learning with linear function approximation is that the separation between online and offline RL can be arbitrarily large, even in constant dimension.

</p>
</details>

<details><summary><b>Johnson Coverage Hypothesis: Inapproximability of k-means and k-median in L_p metrics</b>
<a href="https://arxiv.org/abs/2111.10912">arxiv:2111.10912</a>
&#x1F4C8; 2 <br>
<p>Vincent Cohen-Addad, Karthik C. S., Euiwoong Lee</p></summary>
<p>

**Abstract:** K-median and k-means are the two most popular objectives for clustering algorithms. Despite intensive effort, a good understanding of the approximability of these objectives, particularly in $\ell_p$-metrics, remains a major open problem. In this paper, we significantly improve upon the hardness of approximation factors known in literature for these objectives in $\ell_p$-metrics.
  We introduce a new hypothesis called the Johnson Coverage Hypothesis (JCH), which roughly asserts that the well-studied max k-coverage problem on set systems is hard to approximate to a factor greater than 1-1/e, even when the membership graph of the set system is a subgraph of the Johnson graph. We then show that together with generalizations of the embedding techniques introduced by Cohen-Addad and Karthik (FOCS '19), JCH implies hardness of approximation results for k-median and k-means in $\ell_p$-metrics for factors which are close to the ones obtained for general metrics. In particular, assuming JCH we show that it is hard to approximate the k-means objective:
  $\bullet$ Discrete case: To a factor of 3.94 in the $\ell_1$-metric and to a factor of 1.73 in the $\ell_2$-metric; this improves upon the previous factor of 1.56 and 1.17 respectively, obtained under UGC.
  $\bullet$ Continuous case: To a factor of 2.10 in the $\ell_1$-metric and to a factor of 1.36 in the $\ell_2$-metric; this improves upon the previous factor of 1.07 in the $\ell_2$-metric obtained under UGC.
  We also obtain similar improvements under JCH for the k-median objective. Additionally, we prove a weak version of JCH using the work of Dinur et al. (SICOMP '05) on Hypergraph Vertex Cover, and recover all the results stated above of Cohen-Addad and Karthik (FOCS '19) to (nearly) the same inapproximability factors but now under the standard NP$\neq$P assumption (instead of UGC).

</p>
</details>

<details><summary><b>ARMAS: Active Reconstruction of Missing Audio Segments</b>
<a href="https://arxiv.org/abs/2111.10891">arxiv:2111.10891</a>
&#x1F4C8; 2 <br>
<p>Sachin Pokharel, Muhammad Ali, Zohra Cheddad, Abbas Cheddad</p></summary>
<p>

**Abstract:** Digital audio signal reconstruction of lost or corrupt segment using deep learning algorithms has been explored intensively in the recent years. Nevertheless, prior traditional methods with linear interpolation, phase coding and tone insertion techniques are still in vogue. However, we found no research work on the reconstruction of audio signals with the fusion of dithering, steganography, and machine learning regressors. Therefore, this paper proposes the combination of steganography, halftoning (dithering), and state-of-the-art shallow (RF- Random Forest and SVR- Support Vector Regression) and deep learning (LSTM- Long Short-Term Memory) methods. The results (including comparison to the SPAIN and Autoregressive methods) are evaluated with four different metrics. The observations from the results show that the proposed solution is effective and can enhance the reconstruction of audio signals performed by the side information (noisy-latent representation) steganography provides. This work may trigger interest in the optimization of this approach and/or in transferring it to different domains (i.e., image reconstruction).

</p>
</details>

<details><summary><b>A Software Tool for Evaluating Unmanned Autonomous Systems</b>
<a href="https://arxiv.org/abs/2111.10871">arxiv:2111.10871</a>
&#x1F4C8; 2 <br>
<p>Abdollah Homaifar, Ali Karimoddini, Mike Heiges, Mubbashar A. Khan, Berat A. Erol, Shabnam Nazmi</p></summary>
<p>

**Abstract:** The North Carolina Agriculture and Technical State University (NC A&T) in collaboration with Georgia Tech Research Institute (GTRI) has developed methodologies for creating simulation-based technology tools that are capable of inferring the perceptions and behavioral states of autonomous systems. These methodologies have the potential to provide the Test and Evaluation (T&E) community at the Department of Defense (DoD) with a greater insight into the internal processes of these systems. The methodologies use only external observations and do not require complete knowledge of the internal processing of and/or any modifications to the system under test. This paper presents an example of one such simulation-based technology tool, named as the Data-Driven Intelligent Prediction Tool (DIPT). DIPT was developed for testing a multi-platform Unmanned Aerial Vehicle (UAV) system capable of conducting collaborative search missions. DIPT's Graphical User Interface (GUI) enables the testers to view the aircraft's current operating state, predicts its current target-detection status, and provides reasoning for exhibiting a particular behavior along with an explanation of assigning a particular task to it.

</p>
</details>

<details><summary><b>A Data-Driven Line Search Rule for Support Recovery in High-dimensional Data Analysis</b>
<a href="https://arxiv.org/abs/2111.10806">arxiv:2111.10806</a>
&#x1F4C8; 2 <br>
<p>Peili Li, Yuling Jiao, Xiliang Lu, Lican Kang</p></summary>
<p>

**Abstract:** In this work, we consider the algorithm to the (nonlinear) regression problems with $\ell_0$ penalty. The existing algorithms for $\ell_0$ based optimization problem are often carried out with a fixed step size, and the selection of an appropriate step size depends on the restricted strong convexity and smoothness for the loss function, hence it is difficult to compute in practical calculation. In sprite of the ideas of support detection and root finding \cite{HJK2020}, we proposes a novel and efficient data-driven line search rule to adaptively determine the appropriate step size. We prove the $\ell_2$ error bound to the proposed algorithm without much restrictions for the cost functional. A large number of numerical comparisons with state-of-the-art algorithms in linear and logistic regression problems show the stability, effectiveness and superiority of the proposed algorithms.

</p>
</details>

<details><summary><b>Efficient Softmax Approximation for Deep Neural Networks with Attention Mechanism</b>
<a href="https://arxiv.org/abs/2111.10770">arxiv:2111.10770</a>
&#x1F4C8; 2 <br>
<p>Ihor Vasyltsov, Wooseok Chang</p></summary>
<p>

**Abstract:** There has been a rapid advance of custom hardware (HW) for accelerating the inference speed of deep neural networks (DNNs). Previously, the softmax layer was not a main concern of DNN accelerating HW, because its portion is relatively small in multi-layer perceptron or convolutional neural networks. However, as the attention mechanisms are widely used in various modern DNNs, a cost-efficient implementation of softmax layer is becoming very important. In this paper, we propose two methods to approximate softmax computation, which are based on the usage of LookUp Tables (LUTs). The required size of LUT is quite small (about 700 Bytes) because ranges of numerators and denominators of softmax are stable if normalization is applied to the input. We have validated the proposed technique over different AI tasks (object detection, machine translation, sentiment analysis, and semantic equivalence) and DNN models (DETR, Transformer, BERT) by a variety of benchmarks (COCO17, WMT14, WMT17, GLUE). We showed that 8-bit approximation allows to obtain acceptable accuracy loss below $1.0\%$.

</p>
</details>

<details><summary><b>Modelling Direct Messaging Networks with Multiple Recipients for Cyber Deception</b>
<a href="https://arxiv.org/abs/2111.11932">arxiv:2111.11932</a>
&#x1F4C8; 1 <br>
<p>Kristen Moore, Cody J. Christopher, David Liebowitz, Surya Nepal, Renee Selvey</p></summary>
<p>

**Abstract:** Cyber deception is emerging as a promising approach to defending networks and systems against attackers and data thieves. However, despite being relatively cheap to deploy, the generation of realistic content at scale is very costly, due to the fact that rich, interactive deceptive technologies are largely hand-crafted. With recent improvements in Machine Learning, we now have the opportunity to bring scale and automation to the creation of realistic and enticing simulated content. In this work, we propose a framework to automate the generation of email and instant messaging-style group communications at scale. Such messaging platforms within organisations contain a lot of valuable information inside private communications and document attachments, making them an enticing target for an adversary. We address two key aspects of simulating this type of system: modelling when and with whom participants communicate, and generating topical, multi-party text to populate simulated conversation threads. We present the LogNormMix-Net Temporal Point Process as an approach to the first of these, building upon the intensity-free modeling approach of Shchur et al.~\cite{shchur2019intensity} to create a generative model for unicast and multi-cast communications. We demonstrate the use of fine-tuned, pre-trained language models to generate convincing multi-party conversation threads. A live email server is simulated by uniting our LogNormMix-Net TPP (to generate the communication timestamp, sender and recipients) with the language model, which generates the contents of the multi-party email threads. We evaluate the generated content with respect to a number of realism-based properties, that encourage a model to learn to generate content that will engage the attention of an adversary to achieve a deception outcome.

</p>
</details>

<details><summary><b>How do kernel-based sensor fusion algorithms behave under high dimensional noise?</b>
<a href="https://arxiv.org/abs/2111.10940">arxiv:2111.10940</a>
&#x1F4C8; 1 <br>
<p>Xiucai Ding, Hau-Tieng Wu</p></summary>
<p>

**Abstract:** We study the behavior of two kernel based sensor fusion algorithms, nonparametric canonical correlation analysis (NCCA) and alternating diffusion (AD), under the nonnull setting that the clean datasets collected from two sensors are modeled by a common low dimensional manifold embedded in a high dimensional Euclidean space and the datasets are corrupted by high dimensional noise. We establish the asymptotic limits and convergence rates for the eigenvalues of the associated kernel matrices assuming that the sample dimension and sample size are comparably large, where NCCA and AD are conducted using the Gaussian kernel. It turns out that both the asymptotic limits and convergence rates depend on the signal-to-noise ratio (SNR) of each sensor and selected bandwidths. On one hand, we show that if NCCA and AD are directly applied to the noisy point clouds without any sanity check, it may generate artificial information that misleads scientists' interpretation. On the other hand, we prove that if the bandwidths are selected adequately, both NCCA and AD can be made robust to high dimensional noise when the SNRs are relatively large.

</p>
</details>

<details><summary><b>A hybrid optimization approach for employee rostering: Use cases at Swissgrid and lessons learned</b>
<a href="https://arxiv.org/abs/2111.10845">arxiv:2111.10845</a>
&#x1F4C8; 1 <br>
<p>Jangwon Park, Evangelos Vrettos</p></summary>
<p>

**Abstract:** Employee rostering is a process of assigning available employees to open shifts. Automating it has ubiquitous practical benefits for nearly all industries, such as reducing manual workload and producing flexible, high-quality schedules. In this work, we develop a hybrid methodology which combines Mixed-Integer Linear Programming (MILP) with scatter search, an evolutionary algorithm, having as use case the optimization of employee rostering for Swissgrid, where it is currently a largely manual process. The hybrid methodology guarantees compliance with labor laws, maximizes employees' preference satisfaction, and distributes workload as uniformly as possible among them. Above all, it is shown to be a robust and efficient algorithm, consistently solving realistic problems of varying complexity to near-optimality an order of magnitude faster than an MILP-alone approach using a state-of-the-art commercial solver. Several practical extensions and use cases are presented, which are incorporated into a software tool currently being in pilot use at Swissgrid.

</p>
</details>

<details><summary><b>Design of an Novel Spectrum Sensing Scheme Based on Long Short-Term Memory and Experimental Validation</b>
<a href="https://arxiv.org/abs/2111.10769">arxiv:2111.10769</a>
&#x1F4C8; 1 <br>
<p>Nupur Choudhury, Kandarpa Kumar Sarma, Chinmoy Kalita, Aradhana Misra</p></summary>
<p>

**Abstract:** Spectrum sensing allows cognitive radio systems to detect relevant signals in despite the presence of severe interference. Most of the existing spectrum sensing techniques use a particular signal-noise model with certain assumptions and derive certain detection performance. To deal with this uncertainty, learning based approaches are being adopted and more recently deep learning based tools have become popular. Here, we propose an approach of spectrum sensing which is based on long short term memory (LSTM) which is a critical element of deep learning networks (DLN). Use of LSTM facilitates implicit feature learning from spectrum data. The DLN is trained using several features and the performance of the proposed sensing technique is validated with the help of an empirical testbed setup using Adalm Pluto. The testbed is trained to acquire the primary signal of a real world radio broadcast taking place using FM. Experimental data show that even at low signal to noise ratio, our approach performs well in terms of detection and classification accuracies, as compared to current spectrum sensing methods.

</p>
</details>


[Next Page](2021/2021-11/2021-11-20.md)
