## Summary for 2021-11-15, created on 2021-12-17


<details><summary><b>Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization</b>
<a href="https://arxiv.org/abs/2111.07898">arxiv:2111.07898</a>
&#x1F4C8; 864 <br>
<p>Sushrut Thorat, Giacomo Aldegheri, Tim C. Kietzmann</p></summary>
<p>

**Abstract:** Recurrent neural networks (RNNs) have been shown to perform better than feedforward architectures in visual object categorization tasks, especially in challenging conditions such as cluttered images. However, little is known about the exact computational role of recurrent information flow in these conditions. Here we test RNNs trained for object categorization on the hypothesis that recurrence iteratively aids object categorization via the communication of category-orthogonal auxiliary variables (the location, orientation, and scale of the object). Using diagnostic linear readouts, we find that: (a) information about auxiliary variables increases across time in all network layers, (b) this information is indeed present in the recurrent information flow, and (c) its manipulation significantly affects task performance. These observations confirm the hypothesis that category-orthogonal auxiliary variable information is conveyed through recurrent connectivity and is used to optimize category inference in cluttered environments.

</p>
</details>

<details><summary><b>Solving Linear Algebra by Program Synthesis</b>
<a href="https://arxiv.org/abs/2111.08171">arxiv:2111.08171</a>
&#x1F4C8; 120 <br>
<p>Iddo Drori, Nakul Verma</p></summary>
<p>

**Abstract:** We solve MIT's Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra COMS3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use OpenAI Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all COMS3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level STEM courses by machine.

</p>
</details>

<details><summary><b>AnimeCeleb: Large-Scale Animation CelebFaces Dataset via Controllable 3D Synthetic Models</b>
<a href="https://arxiv.org/abs/2111.07640">arxiv:2111.07640</a>
&#x1F4C8; 83 <br>
<p>Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo Chung, Junsoo Lee, Jaegul Choo</p></summary>
<p>

**Abstract:** Despite remarkable success in deep learning-based face-related models, these models are still limited to the domain of real human faces. On the other hand, the domain of animation faces has been studied less intensively due to the absence of a well-organized dataset. In this paper, we present a large-scale animation celebfaces dataset (AnimeCeleb) via controllable synthetic animation models to boost research on the animation face domain. To facilitate the data generation process, we build a semi-automatic pipeline based on an open 3D software and a developed annotation system. This leads to constructing a large-scale animation face dataset that includes multi-pose and multi-style animation faces with rich annotations. Experiments suggest that our dataset is applicable to various animation-related tasks such as head reenactment and colorization.

</p>
</details>

<details><summary><b>Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation</b>
<a href="https://arxiv.org/abs/2111.07971">arxiv:2111.07971</a>
&#x1F4C8; 55 <br>
<p>David Acuna, Jonah Philion, Sanja Fidler</p></summary>
<p>

**Abstract:** Autonomous driving relies on a huge volume of real-world data to be labeled to high precision. Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the domain gap between the synthetic and real data remains, raising the following important question: What are the best ways to utilize a self-driving simulator for perception tasks? In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective, propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone. Our approach introduces both a principled way to learn neural-invariant representations and a theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator. We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes). Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design, and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique.

</p>
</details>

<details><summary><b>Spectral learning of multivariate extremes</b>
<a href="https://arxiv.org/abs/2111.07799">arxiv:2111.07799</a>
&#x1F4C8; 43 <br>
<p>Marco Avella Medina, Richard A. Davis, Gennady Samorodnitsky</p></summary>
<p>

**Abstract:** We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.

</p>
</details>

<details><summary><b>Joint Unsupervised and Supervised Training for Multilingual ASR</b>
<a href="https://arxiv.org/abs/2111.08137">arxiv:2111.08137</a>
&#x1F4C8; 42 <br>
<p>Junwen Bai, Bo Li, Yu Zhang, Ankur Bapna, Nikhil Siddhartha, Khe Chai Sim, Tara N. Sainath</p></summary>
<p>

**Abstract:** Self-supervised training has shown promising gains in pretraining models and facilitating the downstream finetuning for speech recognition, like multilingual ASR. Most existing methods adopt a 2-stage scheme where the self-supervised loss is optimized in the first pretraining stage, and the standard supervised finetuning resumes in the second stage. In this paper, we propose an end-to-end (E2E) Joint Unsupervised and Supervised Training (JUST) method to combine the supervised RNN-T loss and the self-supervised contrastive and masked language modeling (MLM) losses. We validate its performance on the public dataset Multilingual LibriSpeech (MLS), which includes 8 languages and is extremely imbalanced. On MLS, we explore (1) JUST trained from scratch, and (2) JUST finetuned from a pretrained checkpoint. Experiments show that JUST can consistently outperform other existing state-of-the-art methods, and beat the monolingual baseline by a significant margin, demonstrating JUST's capability of handling low-resource languages in multilingual ASR. Our average WER of all languages outperforms average monolingual baseline by 33.3%, and the state-of-the-art 2-stage XLSR by 32%. On low-resource languages like Polish, our WER is less than half of the monolingual baseline and even beats the supervised transfer learning method which uses external supervision.

</p>
</details>

<details><summary><b>Solving Inverse Problems in Medical Imaging with Score-Based Generative Models</b>
<a href="https://arxiv.org/abs/2111.08005">arxiv:2111.08005</a>
&#x1F4C8; 27 <br>
<p>Yang Song, Liyue Shen, Lei Xing, Stefano Ermon</p></summary>
<p>

**Abstract:** Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.

</p>
</details>

<details><summary><b>LiT: Zero-Shot Transfer with Locked-image Text Tuning</b>
<a href="https://arxiv.org/abs/2111.07991">arxiv:2111.07991</a>
&#x1F4C8; 27 <br>
<p>Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer</p></summary>
<p>

**Abstract:** This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning "Locked-image Text tuning" (LiT-tuning), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT-tuned model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT-tuning is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT-tuned model achieves 84.5% zero-shot transfer accuracy on the ImageNet test set, and 81.1% on the challenging out-of-distribution ObjectNet test set.

</p>
</details>

<details><summary><b>Scaling Law for Recommendation Models: Towards General-purpose User Representations</b>
<a href="https://arxiv.org/abs/2111.11294">arxiv:2111.11294</a>
&#x1F4C8; 21 <br>
<p>Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Su Young Kim, Max Nihlen Ramstrom, Jisu Jeong</p></summary>
<p>

**Abstract:** A recent trend shows that a general class of models, e.g., BERT, GPT-3, CLIP, trained on broad data at scale have shown a great variety of functionalities with a single learning architecture. In this work, we explore the possibility of general-purpose user representation learning by training a universal user encoder at large scales. We demonstrate that the scaling law holds in the user modeling areas, where the training error scales as a power-law with the amount of compute. Our Contrastive Learning User Encoder (CLUE), optimizes task-agnostic objectives, and the resulting user embeddings stretches our expectation of what is possible to do in various downstream tasks. CLUE also shows great transferability to other domains and systems, as performances on an online experiment shows significant improvements in online Click-Through-Rate (CTR). Furthermore, we also investigate how the performance changes according to the scale-up factors, i.e., model capacity, sequence length and batch size. Finally, we discuss the broader impacts of CLUE in general.

</p>
</details>

<details><summary><b>Machine Learning for Genomic Data</b>
<a href="https://arxiv.org/abs/2111.08507">arxiv:2111.08507</a>
&#x1F4C8; 10 <br>
<p>Akankshita Dash</p></summary>
<p>

**Abstract:** This report explores the application of machine learning techniques on short timeseries gene expression data. Although standard machine learning algorithms work well on longer time-series', they often fail to find meaningful insights from fewer timepoints. In this report, we explore model-based clustering techniques. We combine popular unsupervised learning techniques like K-Means, Gaussian Mixture Models, Bayesian Networks, Hidden Markov Models with the well-known Expectation Maximization algorithm. K-Means and Gaussian Mixture Models are fairly standard, while Hidden Markov Model and Bayesian Networks clustering are more novel ideas that suit time-series gene expression data.

</p>
</details>

<details><summary><b>Coarse-to-fine Animal Pose and Shape Estimation</b>
<a href="https://arxiv.org/abs/2111.08176">arxiv:2111.08176</a>
&#x1F4C8; 10 <br>
<p>Chen Li, Gim Hee Lee</p></summary>
<p>

**Abstract:** Most existing animal pose and shape estimation approaches reconstruct animal meshes with a parametric SMAL model. This is because the low-dimensional pose and shape parameters of the SMAL model makes it easier for deep networks to learn the high-dimensional animal meshes. However, the SMAL model is learned from scans of toy animals with limited pose and shape variations, and thus may not be able to represent highly varying real animals well. This may result in poor fittings of the estimated meshes to the 2D evidences, e.g. 2D keypoints or silhouettes. To mitigate this problem, we propose a coarse-to-fine approach to reconstruct 3D animal mesh from a single image. The coarse estimation stage first estimates the pose, shape and translation parameters of the SMAL model. The estimated meshes are then used as a starting point by a graph convolutional network (GCN) to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations benefits from both parametric and non-parametric representations. We design our mesh refinement GCN (MRGCN) as an encoder-decoder structure with hierarchical feature representations to overcome the limited receptive field of traditional GCNs. Moreover, we observe that the global image feature used by existing animal mesh reconstruction works is unable to capture detailed shape information for mesh refinement. We thus introduce a local feature extractor to retrieve a vertex-level feature and use it together with the global feature as the input of the MRGCN. We test our approach on the StanfordExtra dataset and achieve state-of-the-art results. Furthermore, we test the generalization capacity of our approach on the Animal Pose and BADJA datasets. Our code is available at the project website.

</p>
</details>

<details><summary><b>Adding more data does not always help: A study in medical conversation summarization with PEGASUS</b>
<a href="https://arxiv.org/abs/2111.07564">arxiv:2111.07564</a>
&#x1F4C8; 10 <br>
<p>Varun Nair, Namit Katariya, Xavier Amatriain, Ilya Valmianski, Anitha Kannan</p></summary>
<p>

**Abstract:** Medical conversation summarization is integral in capturing information gathered during interactions between patients and physicians. Summarized conversations are used to facilitate patient hand-offs between physicians, and as part of providing care in the future. Summaries, however, can be time-consuming to produce and require domain expertise. Modern pre-trained NLP models such as PEGASUS have emerged as capable alternatives to human summarization, reaching state-of-the-art performance on many summarization benchmarks. However, many downstream tasks still require at least moderately sized datasets to achieve satisfactory performance. In this work we (1) explore the effect of dataset size on transfer learning medical conversation summarization using PEGASUS and (2) evaluate various iterative labeling strategies in the low-data regime, following their success in the classification setting. We find that model performance saturates with increase in dataset size and that the various active-learning strategies evaluated all show equivalent performance consistent with simple dataset size increase. We also find that naive iterative pseudo-labeling is on-par or slightly worse than no pseudo-labeling. Our work sheds light on the successes and challenges of translating low-data regime techniques in classification to medical conversation summarization and helps guides future work in this space. Relevant code available at \url{https://github.com/curai/curai-research/tree/main/medical-summarization-ML4H-2021}.

</p>
</details>

<details><summary><b>Continual Learning via Local Module Composition</b>
<a href="https://arxiv.org/abs/2111.07736">arxiv:2111.07736</a>
&#x1F4C8; 9 <br>
<p>Oleksiy Ostapenko, Pau Rodriguez, Massimo Caccia, Laurent Charlin</p></summary>
<p>

**Abstract:** Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module's relevance to the input. Dynamic module composition is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-specific as opposed to the task- and/or model-specific as in previous works, making LMC applicable to more CL settings compared to previous works. In addition, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the first set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any fine-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however, it reaches a lower overall accuracy. The codebase is available under https://github.com/oleksost/LMC.

</p>
</details>

<details><summary><b>Inverse-Weighted Survival Games</b>
<a href="https://arxiv.org/abs/2111.08175">arxiv:2111.08175</a>
&#x1F4C8; 8 <br>
<p>Xintian Han, Mark Goldstein, Aahlad Puli, Thomas Wies, Adler J Perotte, Rajesh Ranganath</p></summary>
<p>

**Abstract:** Deep models trained through maximum likelihood have achieved state-of-the-art results for survival analysis. Despite this training scheme, practitioners evaluate models under other criteria, such as binary classification losses at a chosen set of time horizons, e.g. Brier score (BS) and Bernoulli log likelihood (BLL). Models trained with maximum likelihood may have poor BS or BLL since maximum likelihood does not directly optimize these criteria. Directly optimizing criteria like BS requires inverse-weighting by the censoring distribution, estimation of which itself also requires inverse-weighted by the failure distribution. But neither are known. To resolve this dilemma, we introduce Inverse-Weighted Survival Games to train both failure and censoring models with respect to criteria such as BS or BLL. In these games, objectives for each model are built from re-weighted estimates featuring the other model, where the re-weighting model is held fixed during training. When the loss is proper, we show that the games always have the true failure and censoring distributions as a stationary point. This means models in the game do not leave the correct distributions once reached. We construct one case where this stationary point is unique. We show that these games optimize BS on simulations and then apply these principles on real world cancer and critically-ill patient data.

</p>
</details>

<details><summary><b>Faster First-Order Algorithms for Monotone Strongly DR-Submodular Maximization</b>
<a href="https://arxiv.org/abs/2111.07990">arxiv:2111.07990</a>
&#x1F4C8; 8 <br>
<p>Omid Sadeghi, Maryam Fazel</p></summary>
<p>

**Abstract:** Continuous DR-submodular functions are a class of generally non-convex/non-concave functions that satisfy the Diminishing Returns (DR) property, which implies that they are concave along non-negative directions. Existing work has studied monotone continuous DR-submodular maximization subject to a convex constraint and provided efficient algorithms with approximation guarantees. In many applications, such as computing the stability number of a graph, the monotone DR-submodular objective function has the additional property of being strongly concave along non-negative directions (i.e., strongly DR-submodular). In this paper, we consider a subclass of $L$-smooth monotone DR-submodular functions that are strongly DR-submodular and have a bounded curvature, and we show how to exploit such additional structure to obtain faster algorithms with stronger guarantees for the maximization problem. We propose a new algorithm that matches the provably optimal $1-\frac{c}{e}$ approximation ratio after only $\lceil\frac{L}μ\rceil$ iterations, where $c\in[0,1]$ and $μ\geq 0$ are the curvature and the strong DR-submodularity parameter. Furthermore, we study the Projected Gradient Ascent (PGA) method for this problem, and provide a refined analysis of the algorithm with an improved $\frac{1}{1+c}$ approximation ratio (compared to $\frac{1}{2}$ in prior works) and a linear convergence rate. Experimental results illustrate and validate the efficiency and effectiveness of our proposed algorithms.

</p>
</details>

<details><summary><b>High-Quality Real Time Facial Capture Based on Single Camera</b>
<a href="https://arxiv.org/abs/2111.07556">arxiv:2111.07556</a>
&#x1F4C8; 8 <br>
<p>Hongwei Xu, Leijia Dai, Jianxing Fu, Xiangyuan Wang, Quanwei Wang</p></summary>
<p>

**Abstract:** We propose a real time deep learning framework for video-based facial expression capture. Our process uses a high-end facial capture pipeline based on FACEGOOD to capture facial expression. We train a convolutional neural network to produce high-quality continuous blendshape weight output from video training. Since this facial capture is fully automated, our system can drastically reduce the amount of labor involved in the development of modern narrative-driven video games or films involving realistic digital doubles of actors and potentially hours of animated dialogue per character. We demonstrate compelling animation inference in challenging areas such as eyes and lips.

</p>
</details>

<details><summary><b>Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization</b>
<a href="https://arxiv.org/abs/2111.07999">arxiv:2111.07999</a>
&#x1F4C8; 6 <br>
<p>Youngwoon Lee, Joseph J. Lim, Anima Anandkumar, Yuke Zhu</p></summary>
<p>

**Abstract:** Skill chaining is a promising approach for synthesizing complex behaviors by sequentially combining previously learned skills. Yet, a naive composition of skills fails when a policy encounters a starting state never seen during its training. For successful skill chaining, prior approaches attempt to widen the policy's starting state distribution. However, these approaches require larger state distributions to be covered as more policies are sequenced, and thus are limited to short skill sequences. In this paper, we propose to chain multiple policies without excessively large initial state distributions by regularizing the terminal state distributions in an adversarial learning framework. We evaluate our approach on two complex long-horizon manipulation tasks of furniture assembly. Our results have shown that our method establishes the first model-free reinforcement learning algorithm to solve these tasks; whereas prior skill chaining approaches fail. The code and videos are available at https://clvrai.com/skill-chaining

</p>
</details>

<details><summary><b>LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations</b>
<a href="https://arxiv.org/abs/2111.08094">arxiv:2111.08094</a>
&#x1F4C8; 5 <br>
<p>Weronika Hryniewska, Adrianna Grudzień, Przemysław Biecek</p></summary>
<p>

**Abstract:** The increased interest in deep learning applications, and their hard-to-detect biases result in the need to validate and explain complex models. However, current explanation methods are limited as far as both the explanation of the reasoning process and prediction results are concerned. They usually only show the location in the image that was important for model prediction. The lack of possibility to interact with explanations makes it difficult to verify and understand exactly how the model works. This creates a significant risk when using the model. It is compounded by the fact that explanations do not take into account the semantic meaning of the explained objects. To escape from the trap of static explanations, we propose an approach called LIMEcraft that allows a user to interactively select semantically consistent areas and thoroughly examine the prediction for the image instance in case of many image features. Experiments on several models showed that our method improves model safety by inspecting model fairness for image pieces that may indicate model bias. The code is available at: http://github.com/MI2DataLab/LIMEcraft

</p>
</details>

<details><summary><b>Disparities in Dermatology AI: Assessments Using Diverse Clinical Images</b>
<a href="https://arxiv.org/abs/2111.08006">arxiv:2111.08006</a>
&#x1F4C8; 5 <br>
<p>Roxana Daneshjou, Kailas Vodrahalli, Weixin Liang, Roberto A Novoa, Melissa Jenkins, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, Pritam Mukherjee, Michelle Phung, Kiana Yekrang, Bradley Fong, Rachna Sahasrabudhe, James Zou, Albert Chiou</p></summary>
<p>

**Abstract:** More than 3 billion people lack access to care for skin disease. AI diagnostic tools may aid in early skin cancer detection; however most models have not been assessed on images of diverse skin tones or uncommon diseases. To address this, we curated the Diverse Dermatology Images (DDI) dataset - the first publicly available, pathologically confirmed images featuring diverse skin tones. We show that state-of-the-art dermatology AI models perform substantially worse on DDI, with ROC-AUC dropping 29-40 percent compared to the models' original results. We find that dark skin tones and uncommon diseases, which are well represented in the DDI dataset, lead to performance drop-offs. Additionally, we show that state-of-the-art robust training methods cannot correct for these biases without diverse training data. Our findings identify important weaknesses and biases in dermatology AI that need to be addressed to ensure reliable application to diverse patients and across all disease.

</p>
</details>

<details><summary><b>Progress in Self-Certified Neural Networks</b>
<a href="https://arxiv.org/abs/2111.07737">arxiv:2111.07737</a>
&#x1F4C8; 5 <br>
<p>Maria Perez-Ortiz, Omar Rivasplata, Emilio Parrado-Hernandez, Benjamin Guedj, John Shawe-Taylor</p></summary>
<p>

**Abstract:** A learning method is self-certified if it uses all available data to simultaneously learn a predictor and certify its quality with a tight statistical certificate that is valid on unseen data. Recent work has shown that neural network models trained by optimising PAC-Bayes bounds lead not only to accurate predictors, but also to tight risk certificates, bearing promise towards achieving self-certified learning. In this context, learning and certification strategies based on PAC-Bayes bounds are especially attractive due to their ability to leverage all data to learn a posterior and simultaneously certify its risk with a tight numerical certificate. In this paper, we assess the progress towards self-certification in probabilistic neural networks learnt by PAC-Bayes inspired objectives. We empirically compare (on 4 classification datasets) classical test set bounds for deterministic predictors and a PAC-Bayes bound for randomised self-certified predictors. We first show that both of these generalisation bounds are not too far from out-of-sample test set errors. We then show that in data starvation regimes, holding out data for the test set bounds adversely affects generalisation performance, while self-certified strategies based on PAC-Bayes bounds do not suffer from this drawback, proving that they might be a suitable choice for the small data regime. We also find that probabilistic neural networks learnt by PAC-Bayes inspired objectives lead to certificates that can be surprisingly competitive with commonly used test set bounds.

</p>
</details>

<details><summary><b>Counterfactual Temporal Point Processes</b>
<a href="https://arxiv.org/abs/2111.07603">arxiv:2111.07603</a>
&#x1F4C8; 5 <br>
<p>Kimia Noorbakhsh, Manuel Gomez Rodriguez</p></summary>
<p>

**Abstract:** Machine learning models based on temporal point processes are the state of the art in a wide variety of applications involving discrete events in continuous time. However, these models lack the ability to answer counterfactual questions, which are increasingly relevant as these models are being used to inform targeted interventions. In this work, our goal is to fill this gap. To this end, we first develop a causal model of thinning for temporal point processes that builds upon the Gumbel-Max structural causal model. This model satisfies a desirable counterfactual monotonicity condition, which is sufficient to identify counterfactual dynamics in the process of thinning. Then, given an observed realization of a temporal point process with a given intensity function, we develop a sampling algorithm that uses the above causal model of thinning and the superposition theorem to simulate counterfactual realizations of the temporal point process under a given alternative intensity function. Simulation experiments using synthetic and real epidemiological data show that the counterfactual realizations provided by our algorithm may give valuable insights to enhance targeted interventions.

</p>
</details>

<details><summary><b>Say What? Collaborative Pop Lyric Generation Using Multitask Transfer Learning</b>
<a href="https://arxiv.org/abs/2111.07592">arxiv:2111.07592</a>
&#x1F4C8; 5 <br>
<p>Naveen Ram, Tanay Gummadi, Rahul Bhethanabotla, Richard J. Savery, Gil Weinberg</p></summary>
<p>

**Abstract:** Lyric generation is a popular sub-field of natural language generation that has seen growth in recent years. Pop lyrics are of unique interest due to the genre's unique style and content, in addition to the high level of collaboration that goes on behind the scenes in the professional pop songwriting process. In this paper, we present a collaborative line-level lyric generation system that utilizes transfer-learning via the T5 transformer model, which, till date, has not been used to generate pop lyrics. By working and communicating directly with professional songwriters, we develop a model that is able to learn lyrical and stylistic tasks like rhyming, matching line beat requirements, and ending lines with specific target words. Our approach compares favorably to existing methods for multiple datasets and yields positive results from our online studies and interviews with industry songwriters.

</p>
</details>

<details><summary><b>The Neural Correlates of Image Texture in the Human Vision Using Magnetoencephalography</b>
<a href="https://arxiv.org/abs/2111.09118">arxiv:2111.09118</a>
&#x1F4C8; 4 <br>
<p>Elaheh Hatamimajoumerd, Alireza Talebpour</p></summary>
<p>

**Abstract:** Undoubtedly, textural property of an image is one of the most important features in object recognition task in both human and computer vision applications. Here, we investigated the neural signatures of four well-known statistical texture features including contrast, homogeneity, energy, and correlation computed from the gray level co-occurrence matrix (GLCM) of the images viewed by the participants in the process of magnetoencephalography (MEG) data collection. To trace these features in the human visual system, we used multivariate pattern analysis (MVPA) and trained a linear support vector machine (SVM) classifier on every timepoint of MEG data representing the brain activity and compared it with the textural descriptors of images using the Spearman correlation. The result of this study demonstrates that hierarchical structure in the processing of these four texture descriptors in the human brain with the order of contrast, homogeneity, energy, and correlation. Additionally, we found that energy, which carries broad texture property of the images, shows a more sustained statistically meaningful correlation with the brain activity in the course of time.

</p>
</details>

<details><summary><b>Comparative Analysis of Machine Learning Models for Predicting Travel Time</b>
<a href="https://arxiv.org/abs/2111.08226">arxiv:2111.08226</a>
&#x1F4C8; 4 <br>
<p>Armstrong Aboah, Elizabeth Arthur</p></summary>
<p>

**Abstract:** In this paper, five different deep learning models are being compared for predicting travel time. These models are autoregressive integrated moving average (ARIMA) model, recurrent neural network (RNN) model, autoregressive (AR) model, Long-short term memory (LSTM) model, and gated recurrent units (GRU) model. The aim of this study is to investigate the performance of each developed model for forecasting travel time. The dataset used in this paper consists of travel time and travel speed information from the state of Missouri. The learning rate used for building each model was varied from 0.0001-0.01. The best learning rate was found to be 0.001. The study concluded that the ARIMA model was the best model architecture for travel time prediction and forecasting.

</p>
</details>

<details><summary><b>ShapeY: Measuring Shape Recognition Capacity Using Nearest Neighbor Matching</b>
<a href="https://arxiv.org/abs/2111.08174">arxiv:2111.08174</a>
&#x1F4C8; 4 <br>
<p>Jong Woo Nam, Amanda S. Rios, Bartlett W. Mel</p></summary>
<p>

**Abstract:** Object recognition in humans depends primarily on shape cues. We have developed a new approach to measuring the shape recognition performance of a vision system based on nearest neighbor view matching within the system's embedding space. Our performance benchmark, ShapeY, allows for precise control of task difficulty, by enforcing that view matching span a specified degree of 3D viewpoint change and/or appearance change. As a first test case we measured the performance of ResNet50 pre-trained on ImageNet. Matching error rates were high. For example, a 27 degree change in object pitch led ResNet50 to match the incorrect object 45% of the time. Appearance changes were also highly disruptive. Examination of false matches indicates that ResNet50's embedding space is severely "tangled". These findings suggest ShapeY can be a useful tool for charting the progress of artificial vision systems towards human-level shape recognition capabilities.

</p>
</details>

<details><summary><b>Revisiting C.S.Peirce's Experiment: 150 Years Later</b>
<a href="https://arxiv.org/abs/2111.08054">arxiv:2111.08054</a>
&#x1F4C8; 4 <br>
<p>Deep Mukhopadhyay</p></summary>
<p>

**Abstract:** An iconoclastic philosopher and polymath, Charles Sanders Peirce (1837-1914) is among the greatest of American minds. In 1872, Peirce conducted a series of experiments to determine the distribution of response times to an auditory stimulus, which is widely regarded as one of the most significant statistical investigations in the history of nineteenth-century American mathematical research (Stigler, 1978). On the 150th anniversary of this historic experiment, we look back at Peirce's view on empirical modeling through a modern statistical lens.

</p>
</details>

<details><summary><b>Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement</b>
<a href="https://arxiv.org/abs/2111.07975">arxiv:2111.07975</a>
&#x1F4C8; 4 <br>
<p>Walter Goodwin, Sagar Vaze, Ioannis Havoutis, Ingmar Posner</p></summary>
<p>

**Abstract:** Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene configuration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-specific training data. In this work, we explore the deterioration of existing methods' ability to infer matches between objects as the visual shift between observed and goal scenes increases. We find that a fundamental limitation of the current setting is that source and target images must contain the same $\textit{instance}$ of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object $\textit{instances}$ with the robot's scene.

</p>
</details>

<details><summary><b>Large-Scale Hyperspectral Image Clustering Using Contrastive Learning</b>
<a href="https://arxiv.org/abs/2111.07945">arxiv:2111.07945</a>
&#x1F4C8; 4 <br>
<p>Yaoming Cai, Zijia Zhang, Yan Liu, Pedram Ghamisi, Kun Li, Xiaobo Liu, Zhihua Cai</p></summary>
<p>

**Abstract:** Clustering of hyperspectral images is a fundamental but challenging task. The recent development of hyperspectral image clustering has evolved from shallow models to deep and achieved promising results in many benchmark datasets. However, their poor scalability, robustness, and generalization ability, mainly resulting from their offline clustering scenarios, greatly limit their application to large-scale hyperspectral data. To circumvent these problems, we present a scalable deep online clustering model, named Spectral-Spatial Contrastive Clustering (SSCC), based on self-supervised learning. Specifically, we exploit a symmetric twin neural network comprised of a projection head with a dimensionality of the cluster number to conduct dual contrastive learning from a spectral-spatial augmentation pool. We define the objective function by implicitly encouraging within-cluster similarity and reducing between-cluster redundancy. The resulting approach is trained in an end-to-end fashion by batch-wise optimization, making it robust in large-scale data and resulting in good generalization ability for unseen data. Extensive experiments on three hyperspectral image benchmarks demonstrate the effectiveness of our approach and show that we advance the state-of-the-art approaches by large margins.

</p>
</details>

<details><summary><b>Deep Semantic Manipulation of Facial Videos</b>
<a href="https://arxiv.org/abs/2111.07902">arxiv:2111.07902</a>
&#x1F4C8; 4 <br>
<p>Girish Kumar Solanki, Anastasios Roussos</p></summary>
<p>

**Abstract:** Editing and manipulating facial features in videos is an interesting and important field of research with a plethora of applications, ranging from movie post-production and visual effects to realistic avatars for video games and virtual assistants. To the best of our knowledge, this paper proposes the first method to perform photorealistic manipulation of facial expressions in videos. Our method supports semantic video manipulation based on neural rendering and 3D-based facial expression modelling. We focus on interactive manipulation of the videos by altering and controlling the facial expressions, achieving promising photorealistic results. The proposed method is based on a disentangled representation and estimation of the 3D facial shape and activity, providing the user with intuitive and easy-to-use control of the facial expressions in the input video. We also introduce a user-friendly, interactive AI tool that processes human-readable semantic labels about the desired emotion manipulations in specific parts of the input video and synthesizes photorealistic manipulated videos. We achieve that by mapping the emotion labels to valence-arousal (VA) values, which in turn are mapped to disentangled 3D facial expressions through an especially designed and trained expression decoder network. The paper presents detailed qualitative and quantitative experiments, which demonstrate the effectiveness of our system and the promising results it achieves. Additional results and videos can be found at the supplementary material (https://github.com/Girish-03/DeepSemManipulation).

</p>
</details>

<details><summary><b>Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection</b>
<a href="https://arxiv.org/abs/2111.07819">arxiv:2111.07819</a>
&#x1F4C8; 4 <br>
<p>Jan Philip Wahle, Nischal Ashok, Terry Ruas, Norman Meuschke, Tirthankar Ghosal, Bela Gipp</p></summary>
<p>

**Abstract:** A drastic rise in potentially life-threatening misinformation has been a by-product of the COVID-19 pandemic. Computational support to identify false information within the massive body of data on the topic is crucial to prevent harm. Researchers proposed many methods for flagging online misinformation related to COVID-19. However, these methods predominantly target specific content types (e.g., news) or platforms (e.g., Twitter). The methods' capabilities to generalize were largely unclear so far. We evaluate fifteen Transformer-based models on five COVID-19 misinformation datasets that include social media posts, news articles, and scientific papers to fill this gap. We show tokenizers and models tailored to COVID-19 data do not provide a significant advantage over general-purpose ones. Our study provides a realistic assessment of models for detecting COVID-19 misinformation. We expect that evaluating a broad spectrum of datasets and models will benefit future research in developing misinformation detection systems.

</p>
</details>

<details><summary><b>Generate plane quad mesh with neural networks and tree search</b>
<a href="https://arxiv.org/abs/2111.07613">arxiv:2111.07613</a>
&#x1F4C8; 4 <br>
<p>Hua Tong</p></summary>
<p>

**Abstract:** The quality of mesh generation has long been considered a vital aspect in providing engineers with reliable simulation results throughout the history of the Finite Element Method (FEM). The element extraction method, which is currently the most robust method, is used in business software. However, in order to speed up extraction, the approach is done by finding the next element that optimizes a target function, which can result in local mesh of bad quality after many time steps. We provide TreeMesh, a method that uses this method in conjunction with reinforcement learning (also possible with supervised learning) and a novel Monte-Carlo tree search (MCTS) (Coulom(2006), Kocsis and Szepesvári(2006), Browne et~al.(2012)). The algorithm is based on a previously proposed approach (Pan et~al.(2021)). After making many improvements on DRL (algorithm, state-action-reward setting) and adding a MCTS, it outperforms the former work on the same boundary. Furthermore, using tree search, our program reveals much preponderance on seed-density-changing boundaries, which is common on thin-film materials.

</p>
</details>

<details><summary><b>Rationale production to support clinical decision-making</b>
<a href="https://arxiv.org/abs/2111.07611">arxiv:2111.07611</a>
&#x1F4C8; 4 <br>
<p>Niall Taylor, Lei Sha, Dan W Joyce, Thomas Lukasiewicz, Alejo Nevado-Holgado, Andrey Kormilitzin</p></summary>
<p>

**Abstract:** The development of neural networks for clinical artificial intelligence (AI) is reliant on interpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clinical free-text contained within EHRs. In this work, we apply InfoCal, the current state-of-the-art model that produces extractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extractive rationales produced by InfoCal to competitive transformer-based models pretrained on clinical text data and for which the attention mechanism can be used for interpretation. We find each presented model with selected interpretability or feature importance methods yield varying results, with clinical language domain expertise and pretraining critical to performance and subsequent interpretability.

</p>
</details>

<details><summary><b>Attention based end to end Speech Recognition for Voice Search in Hindi and English</b>
<a href="https://arxiv.org/abs/2111.10208">arxiv:2111.10208</a>
&#x1F4C8; 3 <br>
<p>Raviraj Joshi, Venkateshan Kannan</p></summary>
<p>

**Abstract:** We describe here our work with automatic speech recognition (ASR) in the context of voice search functionality on the Flipkart e-Commerce platform. Starting with the deep learning architecture of Listen-Attend-Spell (LAS), we build upon and expand the model design and attention mechanisms to incorporate innovative approaches including multi-objective training, multi-pass training, and external rescoring using language models and phoneme based losses. We report a relative WER improvement of 15.7% on top of state-of-the-art LAS models using these modifications. Overall, we report an improvement of 36.9% over the phoneme-CTC system. The paper also provides an overview of different components that can be tuned in a LAS-based system.

</p>
</details>

<details><summary><b>Casting graph isomorphism as a point set registration problem using a simplex embedding and sampling</b>
<a href="https://arxiv.org/abs/2111.09696">arxiv:2111.09696</a>
&#x1F4C8; 3 <br>
<p>Yigit Oktar</p></summary>
<p>

**Abstract:** Graph isomorphism is an important problem as its worst-case time complexity is not yet fully understood. In this study, we try to draw parallels between a related optimization problem called point set registration. A graph can be represented as a point set in enough dimensions using a simplex embedding and sampling. Given two graphs, the isomorphism of them corresponds to the existence of a perfect registration between the point set forms of the graphs. In the case of non-isomorphism, the point set form optimization result can be used as a distance measure between two graphs having the same number of vertices and edges. The related idea of equivalence classes suggests that graph canonization may be an important tool in tackling graph isomorphism problem and an orthogonal transformation invariant feature extraction based on this high dimensional point set representation may be fruitful. The concepts presented can also be extended to automorphism, and subgraph isomorphism problems and can also be applied on hypergraphs with certain modifications.

</p>
</details>

<details><summary><b>An adaptive dimension reduction algorithm for latent variables of variational autoencoder</b>
<a href="https://arxiv.org/abs/2111.08493">arxiv:2111.08493</a>
&#x1F4C8; 3 <br>
<p>Yiran Dong, Chuanhou Gao</p></summary>
<p>

**Abstract:** Constructed by the neural network, variational autoencoder has the overfitting problem caused by setting too many neural units, we develop an adaptive dimension reduction algorithm that can automatically learn the dimension of latent variable vector, moreover, the dimension of every hidden layer. This approach not only apply to the variational autoencoder but also other variants like Conditional VAE(CVAE), and we show the empirical results on six data sets which presents the universality and efficiency of this algorithm. The key advantages of this algorithm is that it can converge the dimension of latent variable vector which approximates the dimension reaches minimum loss of variational autoencoder(VAE), also speeds up the generating and computing speed by reducing the neural units.

</p>
</details>

<details><summary><b>Learning Augmentation Distributions using Transformed Risk Minimization</b>
<a href="https://arxiv.org/abs/2111.08190">arxiv:2111.08190</a>
&#x1F4C8; 3 <br>
<p>Evangelos Chatzipantazis, Stefanos Pertigkiozoglou, Edgar Dobriban, Kostas Daniilidis</p></summary>
<p>

**Abstract:** Adapting to the structure of data distributions (such as symmetry and transformation invariances) is an important challenge in machine learning. Invariances can be built into the learning process by architecture design, or by augmenting the dataset. Both require a priori knowledge about the exact nature of the symmetries. Absent this knowledge, practitioners resort to expensive and time-consuming tuning. To address this problem, we propose a new approach to learn distributions of augmentation transforms, in a new \emph{Transformed Risk Minimization} (TRM) framework. In addition to predictive models, we also optimize over transformations chosen from a hypothesis space. As an algorithmic framework, our TRM method is (1) efficient (jointly learns augmentations and models in a \emph{single training loop}), (2) modular (works with \emph{any} training algorithm), and (3) general (handles \emph{both discrete and continuous} augmentations). We theoretically compare TRM with standard risk minimization, and give a PAC-Bayes upper bound on its generalization error. We propose to optimize this bound over a rich augmentation space via a new parametrization over compositions of blocks, leading to the new \emph{Stochastic Compositional Augmentation Learning} (SCALE) algorithm. We compare SCALE experimentally with prior methods (Fast AutoAugment and Augerino) on CIFAR10/100, SVHN . Additionally, we show that SCALE can correctly learn certain symmetries in the data distribution (recovering rotations on rotated MNIST) and can also improve calibration of the learned model.

</p>
</details>

<details><summary><b>Metric-based multimodal meta-learning for human movement identification via footstep recognition</b>
<a href="https://arxiv.org/abs/2111.07979">arxiv:2111.07979</a>
&#x1F4C8; 3 <br>
<p>Muhammad Shakeel, Katsutoshi Itoyama, Kenji Nishida, Kazuhiro Nakadai</p></summary>
<p>

**Abstract:** We describe a novel metric-based learning approach that introduces a multimodal framework and uses deep audio and geophone encoders in siamese configuration to design an adaptable and lightweight supervised model. This framework eliminates the need for expensive data labeling procedures and learns general-purpose representations from low multisensory data obtained from omnipresent sensing systems. These sensing systems provide numerous applications and various use cases in activity recognition tasks. Here, we intend to explore the human footstep movements from indoor environments and analyze representations from a small self-collected dataset of acoustic and vibration-based sensors. The core idea is to learn plausible similarities between two sensory traits and combining representations from audio and geophone signals. We present a generalized framework to learn embeddings from temporal and spatial features extracted from audio and geophone signals. We then extract the representations in a shared space to maximize the learning of a compatibility function between acoustic and geophone features. This, in turn, can be used effectively to carry out a classification task from the learned model, as demonstrated by assigning high similarity to the pairs with a human footstep movement and lower similarity to pairs containing no footstep movement. Performance analyses show that our proposed multimodal framework achieves a 19.99\% accuracy increase (in absolute terms) and avoided overfitting on the evaluation set when the training samples were increased from 200 pairs to just 500 pairs while satisfactorily learning the audio and geophone representations. Our results employ a metric-based contrastive learning approach for multi-sensor data to mitigate the impact of data scarcity and perform human movement identification with limited data size.

</p>
</details>

<details><summary><b>Target Layer Regularization for Continual Learning Using Cramer-Wold Generator</b>
<a href="https://arxiv.org/abs/2111.07928">arxiv:2111.07928</a>
&#x1F4C8; 3 <br>
<p>Marcin Mazur, Łukasz Pustelnik, Szymon Knop, Patryk Pagacz, Przemysław Spurek</p></summary>
<p>

**Abstract:** We propose an effective regularization strategy (CW-TaLaR) for solving continual learning problems. It uses a penalizing term expressed by the Cramer-Wold distance between two probability distributions defined on a target layer of an underlying neural network that is shared by all tasks, and the simple architecture of the Cramer-Wold generator for modeling output data representation. Our strategy preserves target layer distribution while learning a new task but does not require remembering previous tasks' datasets. We perform experiments involving several common supervised frameworks, which prove the competitiveness of the CW-TaLaR method in comparison to a few existing state-of-the-art continual learning models.

</p>
</details>

<details><summary><b>Learning Representations for Pixel-based Control: What Matters and Why?</b>
<a href="https://arxiv.org/abs/2111.07775">arxiv:2111.07775</a>
&#x1F4C8; 3 <br>
<p>Manan Tomar, Utkarsh A. Mishra, Amy Zhang, Matthew E. Taylor</p></summary>
<p>

**Abstract:** Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.

</p>
</details>

<details><summary><b>Interactive Medical Image Segmentation with Self-Adaptive Confidence Calibration</b>
<a href="https://arxiv.org/abs/2111.07716">arxiv:2111.07716</a>
&#x1F4C8; 3 <br>
<p>Wenhao Li, Qisen Xu, Chuyun Shen, Bin Hu, Fengping Zhu, Yuxin Li, Bo Jin, Xiangfeng Wang</p></summary>
<p>

**Abstract:** Medical image segmentation is one of the fundamental problems for artificial intelligence-based clinical decision systems. Current automatic medical image segmentation methods are often failed to meet clinical requirements. As such, a series of interactive segmentation algorithms are proposed to utilize expert correction information. However, existing methods suffer from some segmentation refining failure problems after long-term interactions and some cost problems from expert annotation, which hinder clinical applications. This paper proposes an interactive segmentation framework, called interactive MEdical segmentation with self-adaptive Confidence CAlibration (MECCA), by introducing the corrective action evaluation, which combines the action-based confidence learning and multi-agent reinforcement learning (MARL). The evaluation is established through a novel action-based confidence network, and the corrective actions are obtained from MARL. Based on the confidential information, a self-adaptive reward function is designed to provide more detailed feedback, and a simulated label generation mechanism is proposed on unsupervised data to reduce over-reliance on labeled data. Experimental results on various medical image datasets have shown the significant performance of the proposed algorithm.

</p>
</details>

<details><summary><b>Calculating Question Similarity is Enough: A New Method for KBQA Tasks</b>
<a href="https://arxiv.org/abs/2111.07658">arxiv:2111.07658</a>
&#x1F4C8; 3 <br>
<p>Hanyu Zhao, Sha Yuan, Jiahong Leng, Xiang Pan, Guoqiang Wang</p></summary>
<p>

**Abstract:** Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. The KBQA task pipeline contains several steps, including entity recognition, entity linking, answering selection, etc. This kind of pipeline method means that errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by only retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art.

</p>
</details>

<details><summary><b>Multimodal Generalized Zero Shot Learning for Gleason Grading using Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2111.07646">arxiv:2111.07646</a>
&#x1F4C8; 3 <br>
<p>Dwarikanath Mahapatra</p></summary>
<p>

**Abstract:** Gleason grading from histopathology images is essential for accurate prostate cancer (PCa) diagnosis. Since such images are obtained after invasive tissue resection quick diagnosis is challenging under the existing paradigm. We propose a method to predict Gleason grades from magnetic resonance (MR) images which are non-interventional and easily acquired. We solve the problem in a generalized zero-shot learning (GZSL) setting since we may not access training images of every disease grade. Synthetic MRI feature vectors of unseen grades (classes) are generated by exploiting Gleason grades' ordered nature through a conditional variational autoencoder (CVAE) incorporating self-supervised learning. Corresponding histopathology features are generated using cycle GANs, and combined with MR features to predict Gleason grades of test images. Experimental results show our method outperforms competing feature generating approaches for GZSL, and comes close to performance of fully supervised methods.

</p>
</details>

<details><summary><b>CoReS: Compatible Representations via Stationarity</b>
<a href="https://arxiv.org/abs/2111.07632">arxiv:2111.07632</a>
&#x1F4C8; 3 <br>
<p>Niccolo Biondi, Federico Pernici, Matteo Bruni, Alberto Del Bimbo</p></summary>
<p>

**Abstract:** In this paper, we propose a novel method to learn internal feature representation models that are \textit{compatible} with previously learned ones. Compatible features enable for direct comparison of old and new learned features, allowing them to be used interchangeably over time. This eliminates the need for visual search systems to extract new features for all previously seen images in the gallery-set when sequentially upgrading the representation model. Extracting new features is typically quite expensive or infeasible in the case of very large gallery-sets and/or real time systems (i.e., face-recognition systems, social networks, life-long learning systems, robotics and surveillance systems). Our approach, called Compatible Representations via Stationarity (CoReS), achieves compatibility by encouraging stationarity to the learned representation model without relying on previously learned models. Stationarity allows features' statistical properties not to change under time shift so that the current learned features are inter-operable with the old ones. We evaluate single and sequential multi-model upgrading in growing large-scale training datasets and we show that our method improves the state-of-the-art in achieving compatible features by a large margin. In particular, upgrading ten times with training data taken from CASIA-WebFace and evaluating in Labeled Face in the Wild (LFW), we obtain a 49\% increase in measuring the average number of times compatibility is achieved, which is a 544\% relative improvement over previous state-of-the-art.

</p>
</details>

<details><summary><b>Property Inference Attacks Against GANs</b>
<a href="https://arxiv.org/abs/2111.07608">arxiv:2111.07608</a>
&#x1F4C8; 3 <br>
<p>Junhao Zhou, Yufei Chen, Chao Shen, Yang Zhang</p></summary>
<p>

**Abstract:** While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN's training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.

</p>
</details>

<details><summary><b>Residual fourier neural operator for thermochemical curing of composites</b>
<a href="https://arxiv.org/abs/2111.10262">arxiv:2111.10262</a>
&#x1F4C8; 2 <br>
<p>Gengxiang Chen, Yingguang Li, Xu liu, Qinglu Meng, Jing Zhou, Xiaozhong Hao</p></summary>
<p>

**Abstract:** During the curing process of composites, the temperature history heavily determines the evolutions of the field of degree of cure as well as the residual stress, which will further influence the mechanical properties of composite, thus it is important to simulate the real temperature history to optimize the curing process of composites. Since thermochemical analysis using Finite Element (FE) simulations requires heavy computational loads and data-driven approaches suffer from the complexity of highdimensional mapping. This paper proposes a Residual Fourier Neural Operator (ResFNO) to establish the direct high-dimensional mapping from any given cure cycle to the corresponding temperature histories. By integrating domain knowledge into a time-resolution independent parameterized neural network, the mapping between cure cycles to temperature histories can be learned using limited number of labelled data. Besides, a novel Fourier residual mapping is designed based on mode decomposition to accelerate the training and boost the performance significantly. Several cases are carried out to evaluate the superior performance and generalizability of the proposed method comprehensively.

</p>
</details>

<details><summary><b>Route Optimization via Environment-Aware Deep Network and Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2111.09124">arxiv:2111.09124</a>
&#x1F4C8; 2 <br>
<p>Pengzhan Guo, Keli Xiao, Zeyang Ye, Wei Zhu</p></summary>
<p>

**Abstract:** Vehicle mobility optimization in urban areas is a long-standing problem in smart city and spatial data analysis. Given the complex urban scenario and unpredictable social events, our work focuses on developing a mobile sequential recommendation system to maximize the profitability of vehicle service providers (e.g., taxi drivers). In particular, we treat the dynamic route optimization problem as a long-term sequential decision-making task. A reinforcement-learning framework is proposed to tackle this problem, by integrating a self-check mechanism and a deep neural network for customer pick-up point monitoring. To account for unexpected situations (e.g., the COVID-19 outbreak), our method is designed to be capable of handling related environment changes with a self-adaptive parameter determination mechanism. Based on the yellow taxi data in New York City and vicinity before and after the COVID-19 outbreak, we have conducted comprehensive experiments to evaluate the effectiveness of our method. The results show consistently excellent performance, from hourly to weekly measures, to support the superiority of our method over the state-of-the-art methods (i.e., with more than 98% improvement in terms of the profitability for taxi drivers).

</p>
</details>

<details><summary><b>Inverting brain grey matter models with likelihood-free inference: a tool for trustable cytoarchitecture measurements</b>
<a href="https://arxiv.org/abs/2111.08693">arxiv:2111.08693</a>
&#x1F4C8; 2 <br>
<p>Maëliss Jallais, Pedro Rodrigues, Alexandre Gramfort, Demian Wassermann</p></summary>
<p>

**Abstract:** Effective characterisation of the brain grey matter cytoarchitecture with quantitative sensitivity to soma density and volume remains an unsolved challenge in diffusion MRI (dMRI). Solving the problem of relating the dMRI signal with cytoarchitectural characteristics calls for the definition of a mathematical model that describes brain tissue via a handful of physiologically-relevant parameters and an algorithm for inverting the model. To address this issue, we propose a new forward model, specifically a new system of equations, requiring a few relatively sparse b-shells. We then apply modern tools from Bayesian analysis known as likelihood-free inference (LFI) to invert our proposed model. As opposed to other approaches from the literature, our algorithm yields not only an estimation of the parameter vector $θ$ that best describes a given observed data point $x_0$, but also a full posterior distribution $p(θ|x_0)$ over the parameter space. This enables a richer description of the model inversion, providing indicators such as credible intervals for the estimated parameters and a complete characterization of the parameter regions where the model may present indeterminacies. We approximate the posterior distribution using deep neural density estimators, known as normalizing flows, and fit them using a set of repeated simulations from the forward model. We validate our approach on simulations using dmipy and then apply the whole pipeline on two publicly available datasets.

</p>
</details>

<details><summary><b>On the Importance of Difficulty Calibration in Membership Inference Attacks</b>
<a href="https://arxiv.org/abs/2111.08440">arxiv:2111.08440</a>
&#x1F4C8; 2 <br>
<p>Lauren Watson, Chuan Guo, Graham Cormode, Alex Sablayrolles</p></summary>
<p>

**Abstract:** The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. However, existing attacks mostly remain impractical due to having high false positive rates, where non-member samples are often erroneously predicted as members. This type of error makes the predicted membership signal unreliable, especially since most samples are non-members in real world applications. In this work, we argue that membership inference attacks can benefit drastically from \emph{difficulty calibration}, where an attack's predicted membership score is adjusted to the difficulty of correctly classifying the target sample. We show that difficulty calibration can significantly reduce the false positive rate of a variety of existing attacks without a loss in accuracy.

</p>
</details>

<details><summary><b>Fairness-aware Online Price Discrimination with Nonparametric Demand Models</b>
<a href="https://arxiv.org/abs/2111.08221">arxiv:2111.08221</a>
&#x1F4C8; 2 <br>
<p>Xi Chen, Xuan Zhang, Yuan Zhou</p></summary>
<p>

**Abstract:** Price discrimination, which refers to the strategy of setting different prices for different customer groups, has been widely used in online retailing. Although it helps boost the collected revenue for online retailers, it might create serious concern in fairness, which even violates the regulation and law. This paper studies the problem of dynamic discriminatory pricing under fairness constraints. In particular, we consider a finite selling horizon of length $T$ for a single product with two groups of customers. Each group of customers has its unknown demand function that needs to be learned. For each selling period, the seller determines the price for each group and observes their purchase behavior. While existing literature mainly focuses on maximizing revenue, ensuring fairness among different customers has not been fully explored in the dynamic pricing literature. In this work, we adopt the fairness notion from (Cohen et al. 2021a). For price fairness, we propose an optimal dynamic pricing policy in terms of regret, which enforces the strict price fairness constraint. In contrast to the standard $\sqrt{T}$-type regret in online learning, we show that the optimal regret in our case is $\tildeΘ(T^{4/5})$. We further extend our algorithm to a more general notion of fairness, which includes demand fairness as a special case. To handle this general class, we propose a soft fairness constraint and develop the dynamic pricing policy that achieves $\tilde{O}(T^{4/5})$ regret.

</p>
</details>

<details><summary><b>Sparse Graph Learning Under Laplacian-Related Constraints</b>
<a href="https://arxiv.org/abs/2111.08161">arxiv:2111.08161</a>
&#x1F4C8; 2 <br>
<p>Jitendra K. Tugnait</p></summary>
<p>

**Abstract:** We consider the problem of learning a sparse undirected graph underlying a given set of multivariate data. We focus on graph Laplacian-related constraints on the sparse precision matrix that encodes conditional dependence between the random variables associated with the graph nodes. Under these constraints the off-diagonal elements of the precision matrix are non-positive (total positivity), and the precision matrix may not be full-rank. We investigate modifications to widely used penalized log-likelihood approaches to enforce total positivity but not the Laplacian structure. The graph Laplacian can then be extracted from the off-diagonal precision matrix. An alternating direction method of multipliers (ADMM) algorithm is presented and analyzed for constrained optimization under Laplacian-related constraints and lasso as well as adaptive lasso penalties. Numerical results based on synthetic data show that the proposed constrained adaptive lasso approach significantly outperforms existing Laplacian-based approaches. We also evaluate our approach on real financial data.

</p>
</details>

<details><summary><b>On the utility of power spectral techniques with feature selection techniques for effective mental task classification in noninvasive BCI</b>
<a href="https://arxiv.org/abs/2111.08154">arxiv:2111.08154</a>
&#x1F4C8; 2 <br>
<p>Akshansh Gupta, Ramesh Kumar Agrawal, Jyoti Singh Kirar, Javier Andreu-Perez, Wei-Ping Ding, Chin-Teng Lin, Mukesh Prasad</p></summary>
<p>

**Abstract:** In this paper classification of mental task-root Brain-Computer Interfaces (BCI) is being investigated, as those are a dominant area of investigations in BCI and are of utmost interest as these systems can be augmented life of people having severe disabilities. The BCI model's performance is primarily dependent on the size of the feature vector, which is obtained through multiple channels. In the case of mental task classification, the availability of training samples to features are minimal. Very often, feature selection is used to increase the ratio for the mental task classification by getting rid of irrelevant and superfluous features. This paper proposes an approach to select relevant and non-redundant spectral features for the mental task classification. This can be done by using four very known multivariate feature selection methods viz, Bhattacharya's Distance, Ratio of Scatter Matrices, Linear Regression and Minimum Redundancy & Maximum Relevance. This work also deals with a comparative analysis of multivariate and univariate feature selection for mental task classification. After applying the above-stated method, the findings demonstrate substantial improvements in the performance of the learning model for mental task classification. Moreover, the efficacy of the proposed approach is endorsed by carrying out a robust ranking algorithm and Friedman's statistical test for finding the best combinations and comparing different combinations of power spectral density and feature selection methods.

</p>
</details>

<details><summary><b>Bayesian inference of the climbing grade scale</b>
<a href="https://arxiv.org/abs/2111.08140">arxiv:2111.08140</a>
&#x1F4C8; 2 <br>
<p>Alexei Drummond, Alex Popinga</p></summary>
<p>

**Abstract:** Climbing grades are used to classify a climbing route based on its perceived difficulty, and have come to play a central role in the sport of rock climbing. Recently, the first statistically rigorous method for estimating climbing grades from whole-history ascent data was described, based on the dynamic Bradley-Terry model for games between players of time-varying ability. In this paper, we implement inference under the whole-history rating model using Markov chain Monte Carlo and apply the method to a curated data set made up of climbers who climb regularly. We use these data to get an estimate of the model's fundamental scale parameter m, which defines the proportional increase in difficulty associated with an increment of grade. We show that the data conform to assumptions that the climbing grade scale is a logarithmic scale of difficulty, like decibels or stellar magnitude. We estimate that an increment in Ewbank, French and UIAA climbing grade systems corresponds to 2.1, 2.09 and 2.13 times increase in difficulty respectively, assuming a logistic model of probability of success as a function of grade. Whereas we find that the Vermin scale for bouldering (V-grade scale) corresponds to a 3.17 increase in difficulty per grade increment. In addition, we highlight potential connections between the logarithmic properties of climbing grade scales and the psychophysical laws of Weber and Fechner.

</p>
</details>

<details><summary><b>Exploring Story Generation with Multi-task Objectives in Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2111.08133">arxiv:2111.08133</a>
&#x1F4C8; 2 <br>
<p>Zhuohan Xie, Trevor Cohn, Jey Han Lau</p></summary>
<p>

**Abstract:** GPT-2 has been frequently adapted in story generation models as it provides powerful generative capability. However, it still fails to generate consistent stories and lacks diversity. Current story generation models leverage additional information such as plots or commonsense into GPT-2 to guide the generation process. These approaches focus on improving generation quality of stories while our work look at both quality and diversity. We explore combining BERT and GPT-2 to build a variational autoencoder (VAE), and extend it by adding additional objectives to learn global features such as story topic and discourse relations. Our evaluations show our enhanced VAE can provide better quality and diversity trade off, generate less repetitive story content and learn a more informative latent variable.

</p>
</details>

<details><summary><b>VisualEnv: visual Gym environments with Blender</b>
<a href="https://arxiv.org/abs/2111.08096">arxiv:2111.08096</a>
&#x1F4C8; 2 <br>
<p>Andrea Scorsoglio, Roberto Furfaro</p></summary>
<p>

**Abstract:** In this paper VisualEnv, a new tool for creating visual environment for reinforcement learning is introduced. It is the product of an integration of an open-source modelling and rendering software, Blender, and a python module used to generate environment model for simulation, OpenAI Gym. VisualEnv allows the user to create custom environments with photorealistic rendering capabilities and full integration with python. The framework is described and tested on a series of example problems that showcase its features for training reinforcement learning agents.

</p>
</details>

<details><summary><b>Learning Graph Neural Networks for Multivariate Time Series Anomaly Detection</b>
<a href="https://arxiv.org/abs/2111.08082">arxiv:2111.08082</a>
&#x1F4C8; 2 <br>
<p>Saswati Ray, Sana Lakdawala, Mononito Goswami, Chufan Gao</p></summary>
<p>

**Abstract:** In this work, we propose GLUE (Graph Deviation Network with Local Uncertainty Estimation), building on the recently proposed Graph Deviation Network (GDN). GLUE not only automatically learns complex dependencies between variables and uses them to better identify anomalous behavior, but also quantifies its predictive uncertainty, allowing us to account for the variation in the data as well to have more interpretable anomaly detection thresholds. Results on two real world datasets tell us that optimizing the negative Gaussian log likelihood is reasonable because GLUE's forecasting results are at par with GDN and in fact better than the vector autoregressor baseline, which is significant given that GDN directly optimizes the MSE loss. In summary, our experiments demonstrate that GLUE is competitive with GDN at anomaly detection, with the added benefit of uncertainty estimations. We also show that GLUE learns meaningful sensor embeddings which clusters similar sensors together.

</p>
</details>

<details><summary><b>Learning Robust Scheduling with Search and Attention</b>
<a href="https://arxiv.org/abs/2111.08073">arxiv:2111.08073</a>
&#x1F4C8; 2 <br>
<p>David Sandberg, Tor Kvernvik, Francesco Davide Calabrese</p></summary>
<p>

**Abstract:** Allocating physical layer resources to users based on channel quality, buffer size, requirements and constraints represents one of the central optimization problems in the management of radio resources. The solution space grows combinatorially with the cardinality of each dimension making it hard to find optimal solutions using an exhaustive search or even classical optimization algorithms given the stringent time requirements. This problem is even more pronounced in MU-MIMO scheduling where the scheduler can assign multiple users to the same time-frequency physical resources. Traditional approaches thus resort to designing heuristics that trade optimality in favor of feasibility of execution. In this work we treat the MU-MIMO scheduling problem as a tree-structured combinatorial problem and, borrowing from the recent successes of AlphaGo Zero, we investigate the feasibility of searching for the best performing solutions using a combination of Monte Carlo Tree Search and Reinforcement Learning. To cater to the nature of the problem at hand, like the lack of an intrinsic ordering of the users as well as the importance of dependencies between combinations of users, we make fundamental modifications to the neural network architecture by introducing the self-attention mechanism. We then demonstrate that the resulting approach is not only feasible but vastly outperforms state-of-the-art heuristic-based scheduling approaches in the presence of measurement uncertainties and finite buffers.

</p>
</details>

<details><summary><b>Modular Networks Prevent Catastrophic Interference in Model-Based Multi-Task Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2111.08010">arxiv:2111.08010</a>
&#x1F4C8; 2 <br>
<p>Robin Schiewer, Laurenz Wiskott</p></summary>
<p>

**Abstract:** In a multi-task reinforcement learning setting, the learner commonly benefits from training on multiple related tasks by exploiting similarities among them. At the same time, the trained agent is able to solve a wider range of different problems. While this effect is well documented for model-free multi-task methods, we demonstrate a detrimental effect when using a single learned dynamics model for multiple tasks. Thus, we address the fundamental question of whether model-based multi-task reinforcement learning benefits from shared dynamics models in a similar way model-free methods do from shared policy networks. Using a single dynamics model, we see clear evidence of task confusion and reduced performance. As a remedy, enforcing an internal structure for the learned dynamics model by training isolated sub-networks for each task notably improves performance while using the same amount of parameters. We illustrate our findings by comparing both methods on a simple gridworld and a more complex vizdoom multi-task experiment.

</p>
</details>

<details><summary><b>Nonprehensile Riemannian Motion Predictive Control</b>
<a href="https://arxiv.org/abs/2111.07986">arxiv:2111.07986</a>
&#x1F4C8; 2 <br>
<p>Hamid Izadinia, Byron Boots, Steven M. Seitz</p></summary>
<p>

**Abstract:** Nonprehensile manipulation involves long horizon underactuated object interactions and physical contact with different objects that can inherently introduce a high degree of uncertainty. In this work, we introduce a novel Real-to-Sim reward analysis technique, called Riemannian Motion Predictive Control (RMPC), to reliably imagine and predict the outcome of taking possible actions for a real robotic platform. Our proposed RMPC benefits from Riemannian motion policy and second order dynamic model to compute the acceleration command and control the robot at every location on the surface. Our approach creates a 3D object-level recomposed model of the real scene where we can simulate the effect of different trajectories. We produce a closed-loop controller to reactively push objects in a continuous action space. We evaluate the performance of our RMPC approach by conducting experiments on a real robot platform as well as simulation and compare against several baselines. We observe that RMPC is robust in cluttered as well as occluded environments and outperforms the baselines.

</p>
</details>

<details><summary><b>Triggerless Backdoor Attack for NLP Tasks with Clean Labels</b>
<a href="https://arxiv.org/abs/2111.07970">arxiv:2111.07970</a>
&#x1F4C8; 2 <br>
<p>Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, Chun Fan</p></summary>
<p>

**Abstract:** Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model; the changed target label leads the example to be mistakenly labeled and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attacks which do not require an external trigger, and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.

</p>
</details>

<details><summary><b>ReLU Network Approximation in Terms of Intrinsic Parameters</b>
<a href="https://arxiv.org/abs/2111.07964">arxiv:2111.07964</a>
&#x1F4C8; 2 <br>
<p>Zuowei Shen, Haizhao Yang, Shijun Zhang</p></summary>
<p>

**Abstract:** This paper studies the approximation error of ReLU networks in terms of the number of intrinsic parameters (i.e., those depending on the target function $f$). First, we prove by construction that, for any Lipschitz continuous function $f$ on $[0,1]^d$ with a Lipschitz constant $λ>0$, a ReLU network with $n+2$ intrinsic parameters can approximate $f$ with an exponentially small error $5λ\sqrt{d}\,2^{-n}$ measured in the $L^p$-norm for $p\in [1,\infty)$. More generally for an arbitrary continuous function $f$ on $[0,1]^d$ with a modulus of continuity $ω_f(\cdot)$, the approximation error is $ω_f(\sqrt{d}\, 2^{-n})+2^{-n+2}ω_f(\sqrt{d})$. Next, we extend these two results from the $L^p$-norm to the $L^\infty$-norm at a price of $3^d n+2$ intrinsic parameters. Finally, by using a high-precision binary representation and the bit extraction technique via a fixed ReLU network independent of the target function, we design, theoretically, a ReLU network with only three intrinsic parameters to approximate Hölder continuous functions with an arbitrarily small error.

</p>
</details>

<details><summary><b>Stochastic Gradient Line Bayesian Optimization: Reducing Measurement Shots in Optimizing Parameterized Quantum Circuits</b>
<a href="https://arxiv.org/abs/2111.07952">arxiv:2111.07952</a>
&#x1F4C8; 2 <br>
<p>Shiro Tamiya, Hayata Yamasaki</p></summary>
<p>

**Abstract:** Optimization of parameterized quantum circuits is indispensable for applications of near-term quantum devices to computational tasks with variational quantum algorithms (VQAs). However, the existing optimization algorithms for VQAs require an excessive number of quantum-measurement shots in estimating expectation values of observables or iterating updates of circuit parameters, whose cost has been a crucial obstacle for practical use. To address this problem, we develop an efficient framework, \textit{stochastic gradient line Bayesian optimization} (SGLBO), for the circuit optimization with fewer measurement shots. The SGLBO reduces the cost of measurement shots by estimating an appropriate direction of updating the parameters based on stochastic gradient descent (SGD) and further by utilizing Bayesian optimization (BO) to estimate the optimal step size in each iteration of the SGD. We formulate an adaptive measurement-shot strategy to achieve the optimization feasibly without relying on precise expectation-value estimation and many iterations; moreover, we show that a technique of suffix averaging can significantly reduce the effect of statistical and hardware noise in the optimization for the VQAs. Our numerical simulation demonstrates that the SGLBO augmented with these techniques can drastically reduce the required number of measurement shots, improve the accuracy in the optimization, and enhance the robustness against the noise compared to other state-of-art optimizers in representative tasks for the VQAs. These results establish a framework of quantum-circuit optimizers integrating two different optimization approaches, SGD and BO, to reduce the cost of measurement shots significantly.

</p>
</details>

<details><summary><b>Fully Linear Graph Convolutional Networks for Semi-Supervised Learning and Clustering</b>
<a href="https://arxiv.org/abs/2111.07942">arxiv:2111.07942</a>
&#x1F4C8; 2 <br>
<p>Yaoming Cai, Zijia Zhang, Zhihua Cai, Xiaobo Liu, Yao Ding, Pedram Ghamisi</p></summary>
<p>

**Abstract:** This paper presents FLGC, a simple yet effective fully linear graph convolutional network for semi-supervised and unsupervised learning. Instead of using gradient descent, we train FLGC based on computing a global optimal closed-form solution with a decoupled procedure, resulting in a generalized linear framework and making it easier to implement, train, and apply. We show that (1) FLGC is powerful to deal with both graph-structured data and regular data, (2) training graph convolutional models with closed-form solutions improve computational efficiency without degrading performance, and (3) FLGC acts as a natural generalization of classic linear models in the non-Euclidean domain, e.g., ridge regression and subspace clustering. Furthermore, we implement a semi-supervised FLGC and an unsupervised FLGC by introducing an initial residual strategy, enabling FLGC to aggregate long-range neighborhoods and alleviate over-smoothing. We compare our semi-supervised and unsupervised FLGCs against many state-of-the-art methods on a variety of classification and clustering benchmarks, demonstrating that the proposed FLGC models consistently outperform previous methods in terms of accuracy, robustness, and learning efficiency. The core code of our FLGC is released at https://github.com/AngryCai/FLGC.

</p>
</details>

<details><summary><b>Distribution Compression in Near-linear Time</b>
<a href="https://arxiv.org/abs/2111.07941">arxiv:2111.07941</a>
&#x1F4C8; 2 <br>
<p>Abhishek Shetty, Raaz Dwivedi, Lester Mackey</p></summary>
<p>

**Abstract:** In distribution compression, one aims to accurately summarize a probability distribution $\mathbb{P}$ using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling $n$ points from a Markov chain and identifying $\sqrt{n}$ points with $\widetilde{\mathcal{O}}(1/\sqrt{n})$ discrepancy to $\mathbb{P}$. Unfortunately, these algorithms suffer from quadratic or super-quadratic runtime in the sample size $n$. To address this deficiency, we introduce Compress++, a simple meta-procedure for speeding up any thinning algorithm while suffering at most a factor of $4$ in error. When combined with the quadratic-time kernel halving and kernel thinning algorithms of Dwivedi and Mackey (2021), Compress++ delivers $\sqrt{n}$ points with $\mathcal{O}(\sqrt{\log n/n})$ integration error and better-than-Monte-Carlo maximum mean discrepancy in $\mathcal{O}(n \log^3 n)$ time and $\mathcal{O}( \sqrt{n} \log^2 n )$ space. Moreover, Compress++ enjoys the same near-linear runtime given any quadratic-time input and reduces the runtime of super-quadratic algorithms by a square-root factor. In our benchmarks with high-dimensional Monte Carlo samples and Markov chains targeting challenging differential equation posteriors, Compress++ matches or nearly matches the accuracy of its input algorithm in orders of magnitude less time.

</p>
</details>

<details><summary><b>On Sparse High-Dimensional Graphical Model Learning For Dependent Time Series</b>
<a href="https://arxiv.org/abs/2111.07897">arxiv:2111.07897</a>
&#x1F4C8; 2 <br>
<p>Jitendra K. Tugnait</p></summary>
<p>

**Abstract:** We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional stationary multivariate Gaussian time series. A sparse-group lasso-based frequency-domain formulation of the problem based on frequency-domain sufficient statistic for the observed time series is presented. We investigate an alternating direction method of multipliers (ADMM) approach for optimization of the sparse-group lasso penalized log-likelihood. We provide sufficient conditions for convergence in the Frobenius norm of the inverse PSD estimators to the true value, jointly across all frequencies, where the number of frequencies are allowed to increase with sample size. This results also yields a rate of convergence. We also empirically investigate selection of the tuning parameters based on Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</p>
</details>

<details><summary><b>Mitigating Divergence of Latent Factors via Dual Ascent for Low Latency Event Prediction Models</b>
<a href="https://arxiv.org/abs/2111.07866">arxiv:2111.07866</a>
&#x1F4C8; 2 <br>
<p>Alex Shtoff, Yair Koren</p></summary>
<p>

**Abstract:** Real-world content recommendation marketplaces exhibit certain behaviors and are imposed by constraints that are not always apparent in common static offline data sets. One example that is common in ad marketplaces is swift ad turnover. New ads are introduced and old ads disappear at high rates every day. Another example is ad discontinuity, where existing ads may appear and disappear from the market for non negligible amounts of time due to a variety of reasons (e.g., depletion of budget, pausing by the advertiser, flagging by the system, and more). These behaviors sometimes cause the model's loss surface to change dramatically over short periods of time. To address these behaviors, fresh models are highly important, and to achieve this (and for several other reasons) incremental training on small chunks of past events is often employed. These behaviors and algorithmic optimizations occasionally cause model parameters to grow uncontrollably large, or \emph{diverge}. In this work present a systematic method to prevent model parameters from diverging by imposing a carefully chosen set of constraints on the model's latent vectors. We then devise a method inspired by primal-dual optimization algorithms to fulfill these constraints in a manner which both aligns well with incremental model training, and does not require any major modifications to the underlying model training algorithm.
  We analyze, demonstrate, and motivate our method on OFFSET, a collaborative filtering algorithm which drives Yahoo native advertising, which is one of VZM's largest and faster growing businesses, reaching a run-rate of many hundreds of millions USD per year. Finally, we conduct an online experiment which shows a substantial reduction in the number of diverging instances, and a significant improvement to both user experience and revenue.

</p>
</details>

<details><summary><b>Conditional Linear Regression for Heterogeneous Covariances</b>
<a href="https://arxiv.org/abs/2111.07834">arxiv:2111.07834</a>
&#x1F4C8; 2 <br>
<p>Brendan Juba, Leda Liang</p></summary>
<p>

**Abstract:** Often machine learning and statistical models will attempt to describe the majority of the data. However, there may be situations where only a fraction of the data can be fit well by a linear regression model. Here, we are interested in a case where such inliers can be identified by a Disjunctive Normal Form (DNF) formula. We give a polynomial time algorithm for the conditional linear regression task, which identifies a DNF condition together with the linear predictor on the corresponding portion of the data. In this work, we improve on previous algorithms by removing a requirement that the covariances of the data satisfying each of the terms of the condition have to all be very similar in spectral norm to the covariance of the overall condition.

</p>
</details>

<details><summary><b>Three-body renormalization group limit cycles based on unsupervised feature learning</b>
<a href="https://arxiv.org/abs/2111.07820">arxiv:2111.07820</a>
&#x1F4C8; 2 <br>
<p>Bastian Kaspschak, Ulf-G. Meißner</p></summary>
<p>

**Abstract:** Both the three-body system and the inverse square potential carry a special significance in the study of renormalization group limit cycles. In this work, we pursue an exploratory approach and address the question which two-body interactions lead to limit cycles in the three-body system at low energies, without imposing any restrictions upon the scattering length. For this, we train a boosted ensemble of variational autoencoders, that not only provide a severe dimensionality reduction, but also allow to generate further synthetic potentials, which is an important prerequisite in order to efficiently search for limit cycles in low-dimensional latent space. We do so by applying an elitist genetic algorithm to a population of synthetic potentials that minimizes a specially defined limit-cycle-loss. The resulting fittest individuals suggest that the inverse square potential is the only two-body potential that minimizes this limit cycle loss independent of the hyperangle.

</p>
</details>

<details><summary><b>Joint Synthesis of Safety Certificate and Safe Control Policy using Constrained Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2111.07695">arxiv:2111.07695</a>
&#x1F4C8; 2 <br>
<p>Haitong Ma, Changliu Liu, Shengbo Eben Li, Sifa Zheng, Jianyu Chen</p></summary>
<p>

**Abstract:** Safety is the major consideration in controlling complex dynamical systems using reinforcement learning (RL), where the safety certificate can provide provable safety guarantee. A valid safety certificate is an energy function indicating that safe states are with low energy, and there exists a corresponding safe control policy that allows the energy function to always dissipate. The safety certificate and the safe control policy are closely related to each other and both challenging to synthesize. Therefore, existing learning-based studies treat either of them as prior knowledge to learn the other, which limits their applicability with general unknown dynamics. This paper proposes a novel approach that simultaneously synthesizes the energy-function-based safety certificate and learns the safe control policy with CRL. We do not rely on prior knowledge about either an available model-based controller or a perfect safety certificate. In particular, we formulate a loss function to optimize the safety certificate parameters by minimizing the occurrence of energy increases. By adding this optimization procedure as an outer loop to the Lagrangian-based constrained reinforcement learning (CRL), we jointly update the policy and safety certificate parameters and prove that they will converge to their respective local optima, the optimal safe policy and a valid safety certificate. We evaluate our algorithms on multiple safety-critical benchmark environments. The results show that the proposed algorithm learns provably safe policies with no constraint violation. The validity or feasibility of synthesized safety certificate is also verified numerically.

</p>
</details>

<details><summary><b>Contrastive Representation Learning with Trainable Augmentation Channel</b>
<a href="https://arxiv.org/abs/2111.07679">arxiv:2111.07679</a>
&#x1F4C8; 2 <br>
<p>Masanori Koyama, Kentaro Minami, Takeru Miyato, Yarin Gal</p></summary>
<p>

**Abstract:** In contrastive representation learning, data representation is trained so that it can classify the image instances even when the images are altered by augmentations. However, depending on the datasets, some augmentations can damage the information of the images beyond recognition, and such augmentations can result in collapsed representations. We present a partial solution to this problem by formalizing a stochastic encoding process in which there exist a tug-of-war between the data corruption introduced by the augmentations and the information preserved by the encoder. We show that, with the infoMax objective based on this framework, we can learn a data-dependent distribution of augmentations to avoid the collapse of the representation.

</p>
</details>

<details><summary><b>Fast Axiomatic Attribution for Neural Networks</b>
<a href="https://arxiv.org/abs/2111.07668">arxiv:2111.07668</a>
&#x1F4C8; 2 <br>
<p>Robin Hesse, Simone Schaub-Meyer, Stefan Roth</p></summary>
<p>

**Abstract:** Mitigating the dependence on spurious correlations present in the training dataset is a quickly emerging and important topic of deep learning. Recent approaches include priors on the feature attribution of a deep neural network (DNN) into the training process to reduce the dependence on unwanted features. However, until now one needed to trade off high-quality attributions, satisfying desirable axioms, against the time required to compute them. This in turn either led to long training times or ineffective attribution priors. In this work, we break this trade-off by considering a special class of efficiently axiomatically attributable DNNs for which an axiomatic feature attribution can be computed with only a single forward/backward pass. We formally prove that nonnegatively homogeneous DNNs, here termed $\mathcal{X}$-DNNs, are efficiently axiomatically attributable and show that they can be effortlessly constructed from a wide range of regular DNNs by simply removing the bias term of each layer. Various experiments demonstrate the advantages of $\mathcal{X}$-DNNs, beating state-of-the-art generic attribution methods on regular DNNs for training with attribution priors.

</p>
</details>

<details><summary><b>Pseudo-domains in imaging data improve prediction of future disease status in multi-center studies</b>
<a href="https://arxiv.org/abs/2111.07634">arxiv:2111.07634</a>
&#x1F4C8; 2 <br>
<p>Matthias Perkonigg, Peter Mesenbrink, Alexander Goehler, Miljen Martic, Ahmed Ba-Ssalamah, Georg Langs</p></summary>
<p>

**Abstract:** In multi-center randomized clinical trials imaging data can be diverse due to acquisition technology or scanning protocols. Models predicting future outcome of patients are impaired by this data heterogeneity. Here, we propose a prediction method that can cope with a high number of different scanning sites and a low number of samples per site. We cluster sites into pseudo-domains based on visual appearance of scans, and train pseudo-domain specific models. Results show that they improve the prediction accuracy for steatosis after 48 weeks from imaging data acquired at an initial visit and 12-weeks follow-up in liver disease

</p>
</details>

<details><summary><b>Randomized Classifiers vs Human Decision-Makers: Trustworthy AI May Have to Act Randomly and Society Seems to Accept This</b>
<a href="https://arxiv.org/abs/2111.07545">arxiv:2111.07545</a>
&#x1F4C8; 2 <br>
<p>Gábor Erdélyi, Olivia J. Erdélyi, Vladimir Estivill-Castro</p></summary>
<p>

**Abstract:** As \emph{artificial intelligence} (AI) systems are increasingly involved in decisions affecting our lives, ensuring that automated decision-making is fair and ethical has become a top priority. Intuitively, we feel that akin to human decisions, judgments of artificial agents should necessarily be grounded in some moral principles. Yet a decision-maker (whether human or artificial) can only make truly ethical (based on any ethical theory) and fair (according to any notion of fairness) decisions if full information on all the relevant factors on which the decision is based are available at the time of decision-making. This raises two problems: (1) In settings, where we rely on AI systems that are using classifiers obtained with supervised learning, some induction/generalization is present and some relevant attributes may not be present even during learning. (2) Modeling such decisions as games reveals that any -- however ethical -- pure strategy is inevitably susceptible to exploitation.
  Moreover, in many games, a Nash Equilibrium can only be obtained by using mixed strategies, i.e., to achieve mathematically optimal outcomes, decisions must be randomized. In this paper, we argue that in supervised learning settings, there exist random classifiers that perform at least as well as deterministic classifiers, and may hence be the optimal choice in many circumstances. We support our theoretical results with an empirical study indicating a positive societal attitude towards randomized artificial decision-makers, and discuss some policy and implementation issues related to the use of random classifiers that relate to and are relevant for current AI policy and standardization initiatives.

</p>
</details>

<details><summary><b>T-AutoML: Automated Machine Learning for Lesion Segmentation using Transformers in 3D Medical Imaging</b>
<a href="https://arxiv.org/abs/2111.07535">arxiv:2111.07535</a>
&#x1F4C8; 2 <br>
<p>Dong Yang, Andriy Myronenko, Xiaosong Wang, Ziyue Xu, Holger R. Roth, Daguang Xu</p></summary>
<p>

**Abstract:** Lesion segmentation in medical imaging has been an important topic in clinical research. Researchers have proposed various detection and segmentation algorithms to address this task. Recently, deep learning-based approaches have significantly improved the performance over conventional methods. However, most state-of-the-art deep learning methods require the manual design of multiple network components and training strategies. In this paper, we propose a new automated machine learning algorithm, T-AutoML, which not only searches for the best neural architecture, but also finds the best combination of hyper-parameters and data augmentation strategies simultaneously. The proposed method utilizes the modern transformer model, which is introduced to adapt to the dynamic length of the search space embedding and can significantly improve the ability of the search. We validate T-AutoML on several large-scale public lesion segmentation data-sets and achieve state-of-the-art performance.

</p>
</details>

<details><summary><b>Patent Data for Engineering Design: A Review</b>
<a href="https://arxiv.org/abs/2111.08500">arxiv:2111.08500</a>
&#x1F4C8; 1 <br>
<p>Shuo Jiang, Serhad Sarica, Binyang Song, Jie Hu, Jianxi Luo</p></summary>
<p>

**Abstract:** Patent data have been utilized for engineering design research for long because it contains massive amount of design information. Recent advances in artificial intelligence and data science present unprecedented opportunities to mine, analyse and make sense of patent data to develop design theory and methodology. Herein, we survey the patent-for-design literature by their contributions to design theories, methods, tools, and strategies, as well as different forms of patent data and various methods. Our review sheds light on promising future research directions for the field.

</p>
</details>

<details><summary><b>Graph neural network-based fault diagnosis: a review</b>
<a href="https://arxiv.org/abs/2111.08185">arxiv:2111.08185</a>
&#x1F4C8; 1 <br>
<p>Zhiwen Chen, Jiamin Xu, Cesare Alippi, Steven X. Ding, Yuri Shardt, Tao Peng, Chunhua Yang</p></summary>
<p>

**Abstract:** Graph neural network (GNN)-based fault diagnosis (FD) has received increasing attention in recent years, due to the fact that data coming from several application domains can be advantageously represented as graphs. Indeed, this particular representation form has led to superior performance compared to traditional FD approaches. In this review, an easy introduction to GNN, potential applications to the field of fault diagnosis, and future perspectives are given. First, the paper reviews neural network-based FD methods by focusing on their data representations, namely, time-series, images, and graphs. Second, basic principles and principal architectures of GNN are introduced, with attention to graph convolutional networks, graph attention networks, graph sample and aggregate, graph auto-encoder, and spatial-temporal graph convolutional networks. Third, the most relevant fault diagnosis methods based on GNN are validated through the detailed experiments, and conclusions are made that the GNN-based methods can achieve good fault diagnosis performance. Finally, discussions and future challenges are provided.

</p>
</details>

<details><summary><b>Deep Diffusion Models for Robust Channel Estimation</b>
<a href="https://arxiv.org/abs/2111.08177">arxiv:2111.08177</a>
&#x1F4C8; 1 <br>
<p>Marius Arvinte, Jonathan I Tamir</p></summary>
<p>

**Abstract:** Channel estimation is a critical task in digital communications that greatly impacts end-to-end system performance. In this work, we introduce a novel approach for multiple-input multiple-output (MIMO) channel estimation using deep diffusion models. Our method uses a deep neural network that is trained to estimate the gradient of the log-likelihood of wireless channels at any point in high-dimensional space, and leverages this model to solve channel estimation via posterior sampling. We train a deep diffusion model on channel realizations from the CDL-D model for two antenna spacings and show that the approach leads to competitive in- and out-of-distribution performance when compared to generative adversarial network (GAN) and compressed sensing (CS) methods. When tested on CDL-C channels which are never seen during training or fine-tuned on, our approach leads to end-to-end coded performance gains of up to $3$ dB compared to CS methods and losses of only $0.5$ dB compared to ideal channel knowledge. To encourage open and reproducible research, our source code is available at https://github.com/utcsilab/diffusion-channels .

</p>
</details>

<details><summary><b>Learning Optimal Control with Stochastic Models of Hamiltonian Dynamics</b>
<a href="https://arxiv.org/abs/2111.08108">arxiv:2111.08108</a>
&#x1F4C8; 1 <br>
<p>Chandrajit Bajaj, Minh Nguyen</p></summary>
<p>

**Abstract:** Optimal control problems can be solved by first applying the Pontryagin maximum principle, followed by computing a solution of the corresponding unconstrained Hamiltonian dynamical system. In this paper, and to achieve a balance between robustness and efficiency, we learn a reduced Hamiltonian of the unconstrained Hamiltonian. This reduced Hamiltonian is learned by going backward in time and by minimizing the loss function resulting from application of the Pontryagin maximum principle conditions. The robustness of our learning process is then further improved by progressively learning a posterior distribution of reduced Hamiltonians. This leads to a more efficient sampling of the generalized coordinates (position, velocity) of our phase space. Our solution framework applies to not only optimal control problems with finite-dimensional phase (state) spaces but also the infinite dimensional case.

</p>
</details>

<details><summary><b>Common Language for Goal-Oriented Semantic Communications: A Curriculum Learning Framework</b>
<a href="https://arxiv.org/abs/2111.08051">arxiv:2111.08051</a>
&#x1F4C8; 1 <br>
<p>Mohammad Karimzadeh Farshbafan, Walid Saad, Merouane Debbah</p></summary>
<p>

**Abstract:** Semantic communications will play a critical role in enabling goal-oriented services over next-generation wireless systems. However, most prior art in this domain is restricted to specific applications (e.g., text or image), and it does not enable goal-oriented communications in which the effectiveness of the transmitted information must be considered along with the semantics so as to execute a certain task. In this paper, a comprehensive semantic communications framework is proposed for enabling goal-oriented task execution. To capture the semantics between a speaker and a listener, a common language is defined using the concept of beliefs to enable the speaker to describe the environment observations to the listener. Then, an optimization problem is posed to choose the minimum set of beliefs that perfectly describes the observation while minimizing the task execution time and transmission cost. A novel top-down framework that combines curriculum learning (CL) and reinforcement learning (RL) is proposed to solve this problem. Simulation results show that the proposed CL method outperforms traditional RL in terms of convergence time, task execution time, and transmission cost during training.

</p>
</details>

<details><summary><b>Fast and Credible Likelihood-Free Cosmology with Truncated Marginal Neural Ratio Estimation</b>
<a href="https://arxiv.org/abs/2111.08030">arxiv:2111.08030</a>
&#x1F4C8; 1 <br>
<p>Alex Cole, Benjamin Kurt Miller, Samuel J. Witte, Maxwell X. Cai, Meiert W. Grootes, Francesco Nattino, Christoph Weniger</p></summary>
<p>

**Abstract:** Sampling-based inference techniques are central to modern cosmological data analysis; these methods, however, scale poorly with dimensionality and typically require approximate or intractable likelihoods. In this paper we describe how Truncated Marginal Neural Ratio Estimation (TMNRE) (a new approach in so-called simulation-based inference) naturally evades these issues, improving the $(i)$ efficiency, $(ii)$ scalability, and $(iii)$ trustworthiness of the inferred posteriors. Using measurements of the Cosmic Microwave Background (CMB), we show that TMNRE can achieve converged posteriors using orders of magnitude fewer simulator calls than conventional Markov Chain Monte Carlo (MCMC) methods. Remarkably, the required number of samples is effectively independent of the number of nuisance parameters. In addition, a property called \emph{local amortization} allows the performance of rigorous statistical consistency checks that are not accessible to sampling-based methods. TMNRE promises to become a powerful tool for cosmological data analysis, particularly in the context of extended cosmologies, where the timescale required for conventional sampling-based inference methods to converge can greatly exceed that of simple cosmological models such as $Λ$CDM. To perform these computations, we use an implementation of TMNRE via the open-source code \texttt{swyft}.

</p>
</details>

<details><summary><b>Tensor network to learn the wavefunction of data</b>
<a href="https://arxiv.org/abs/2111.08014">arxiv:2111.08014</a>
&#x1F4C8; 1 <br>
<p>Anatoly Dymarsky, Kirill Pavlenko</p></summary>
<p>

**Abstract:** How many different ways are there to handwrite digit 3? To quantify this question imagine extending a dataset of handwritten digits MNIST by sampling additional images until they start repeating. We call the collection of all resulting images of digit 3 the "full set." To study the properties of the full set we introduce a tensor network architecture which simultaneously accomplishes both classification (discrimination) and sampling tasks. Qualitatively, our trained network represents the indicator function of the full set. It therefore can be used to characterize the data itself. We illustrate that by studying the full sets associated with the digits of MNIST.
  Using quantum mechanical interpretation of our network we characterize the full set by calculating its entanglement entropy. We also study its geometric properties such as mean Hamming distance, effective dimension, and size. The latter answers the question above -- the total number of black and white threes written MNIST style is $2^{72}$.

</p>
</details>

<details><summary><b>Advantage of Machine Learning over Maximum Likelihood in Limited-Angle Low-Photon X-Ray Tomography</b>
<a href="https://arxiv.org/abs/2111.08011">arxiv:2111.08011</a>
&#x1F4C8; 1 <br>
<p>Zhen Guo, Jung Ki Song, George Barbastathis, Michael E. Glinsky, Courtenay T. Vaughan, Kurt W. Larson, Bradley K. Alpert, Zachary H. Levine</p></summary>
<p>

**Abstract:** Limited-angle X-ray tomography reconstruction is an ill-conditioned inverse problem in general. Especially when the projection angles are limited and the measurements are taken in a photon-limited condition, reconstructions from classical algorithms such as filtered backprojection may lose fidelity and acquire artifacts due to the missing-cone problem. To obtain satisfactory reconstruction results, prior assumptions, such as total variation minimization and nonlocal image similarity, are usually incorporated within the reconstruction algorithm. In this work, we introduce deep neural networks to determine and apply a prior distribution in the reconstruction process. Our neural networks learn the prior directly from synthetic training samples. The neural nets thus obtain a prior distribution that is specific to the class of objects we are interested in reconstructing. In particular, we used deep generative models with 3D convolutional layers and 3D attention layers which are trained on 3D synthetic integrated circuit (IC) data from a model dubbed CircuitFaker. We demonstrate that, when the projection angles and photon budgets are limited, the priors from our deep generative models can dramatically improve the IC reconstruction quality on synthetic data compared with maximum likelihood estimation. Training the deep generative models with synthetic IC data from CircuitFaker illustrates the capabilities of the learned prior from machine learning. We expect that if the process were reproduced with experimental data, the advantage of the machine learning would persist. The advantages of machine learning in limited angle X-ray tomography may further enable applications in low-photon nanoscale imaging.

</p>
</details>

<details><summary><b>SPLDExtraTrees: Robust machine learning approach for predicting kinase inhibitor resistance</b>
<a href="https://arxiv.org/abs/2111.08008">arxiv:2111.08008</a>
&#x1F4C8; 1 <br>
<p>Ziyi Yang, Zhaofeng Ye, Yijia Xiao, Changyu Hsieh</p></summary>
<p>

**Abstract:** Drug resistance is a major threat to the global health and a significant concern throughout the clinical treatment of diseases and drug development. The mutation in proteins that is related to drug binding is a common cause for adaptive drug resistance. Therefore, quantitative estimations of how mutations would affect the interaction between a drug and the target protein would be of vital significance for the drug development and the clinical practice. Computational methods that rely on molecular dynamics simulations, Rosetta protocols, as well as machine learning methods have been proven to be capable of predicting ligand affinity changes upon protein mutation. However, the severely limited sample size and heavy noise induced overfitting and generalization issues have impeded wide adoption of machine learning for studying drug resistance. In this paper, we propose a robust machine learning method, termed SPLDExtraTrees, which can accurately predict ligand binding affinity changes upon protein mutation and identify resistance-causing mutations. Especially, the proposed method ranks training data following a specific scheme that starts with easy-to-learn samples and gradually incorporates harder and diverse samples into the training, and then iterates between sample weight recalculations and model updates. In addition, we calculate additional physics-based structural features to provide the machine learning model with the valuable domain knowledge on proteins for this data-limited predictive tasks. The experiments substantiate the capability of the proposed method for predicting kinase inhibitor resistance under three scenarios, and achieves predictive accuracy comparable to that of molecular dynamics and Rosetta methods with much less computational costs.

</p>
</details>

<details><summary><b>Short-Term Power Prediction for Renewable Energy Using Hybrid Graph Convolutional Network and Long Short-Term Memory Approach</b>
<a href="https://arxiv.org/abs/2111.07958">arxiv:2111.07958</a>
&#x1F4C8; 1 <br>
<p>Wenlong Liao, Birgitte Bak-Jensen, Jayakrishnan Radhakrishna Pillai, Zhe Yang, Kuangpu Liu</p></summary>
<p>

**Abstract:** Accurate short-term solar and wind power predictions play an important role in the planning and operation of power systems. However, the short-term power prediction of renewable energy has always been considered a complex regression problem, owing to the fluctuation and intermittence of output powers and the law of dynamic change with time due to local weather conditions, i.e. spatio-temporal correlation. To capture the spatio-temporal features simultaneously, this paper proposes a new graph neural network-based short-term power forecasting approach, which combines the graph convolutional network (GCN) and long short-term memory (LSTM). Specifically, the GCN is employed to learn complex spatial correlations between adjacent renewable energies, and the LSTM is used to learn dynamic changes of power curves. The simulation results show that the proposed hybrid approach can model the spatio-temporal correlation of renewable energies, and its performance outperforms popular baselines on real-world datasets.

</p>
</details>

<details><summary><b>Best of Both Worlds: Practical and Theoretically Optimal Submodular Maximization in Parallel</b>
<a href="https://arxiv.org/abs/2111.07917">arxiv:2111.07917</a>
&#x1F4C8; 1 <br>
<p>Yixin Chen, Tonmoy Dey, Alan Kuhnle</p></summary>
<p>

**Abstract:** For the problem of maximizing a monotone, submodular function with respect to a cardinality constraint $k$ on a ground set of size $n$, we provide an algorithm that achieves the state-of-the-art in both its empirical performance and its theoretical properties, in terms of adaptive complexity, query complexity, and approximation ratio; that is, it obtains, with high probability, query complexity of $O(n)$ in expectation, adaptivity of $O(\log(n))$, and approximation ratio of nearly $1-1/e$. The main algorithm is assembled from two components which may be of independent interest. The first component of our algorithm, LINEARSEQ, is useful as a preprocessing algorithm to improve the query complexity of many algorithms. Moreover, a variant of LINEARSEQ is shown to have adaptive complexity of $O( \log (n / k) )$ which is smaller than that of any previous algorithm in the literature. The second component is a parallelizable thresholding procedure THRESHOLDSEQ for adding elements with gain above a constant threshold. Finally, we demonstrate that our main algorithm empirically outperforms, in terms of runtime, adaptive rounds, total queries, and objective values, the previous state-of-the-art algorithm FAST in a comprehensive evaluation with six submodular objective functions.

</p>
</details>

<details><summary><b>Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming</b>
<a href="https://arxiv.org/abs/2111.07875">arxiv:2111.07875</a>
&#x1F4C8; 1 <br>
<p>Dominik Sobania, Martin Briesch, Franz Rothlauf</p></summary>
<p>

**Abstract:** GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability.

</p>
</details>

<details><summary><b>Transfer Learning Capabilities of Untrained Neural Networks for MIMO CSI Recreation</b>
<a href="https://arxiv.org/abs/2111.07858">arxiv:2111.07858</a>
&#x1F4C8; 1 <br>
<p>Brenda Vilas Boas, Wolfgang Zirwas, Martin Haardt</p></summary>
<p>

**Abstract:** Machine learning (ML) applications for wireless communications have gained momentum on the standardization discussions for 5G advanced and beyond. One of the biggest challenges for real world ML deployment is the need for labeled signals and big measurement campaigns. To overcome those problems, we propose the use of untrained neural networks (UNNs) for MIMO channel recreation/estimation and low overhead reporting. The UNNs learn the propagation environment by fitting a few channel measurements and we exploit their learned prior to provide higher channel estimation gains. Moreover, we present a UNN for simultaneous channel recreation for multiple users, or multiple user equipment (UE) positions, in which we have a trade-off between the estimated channel gain and the number of parameters. Our results show that transfer learning techniques are effective in accessing the learned prior on the environment structure as they provide higher channel gain for neighbouring users. Moreover, we indicate how the under-parameterization of UNNs can further enable low-overhead channel state information (CSI) reporting.

</p>
</details>

<details><summary><b>Machine Learning for CSI Recreation Based on Prior Knowledge</b>
<a href="https://arxiv.org/abs/2111.07854">arxiv:2111.07854</a>
&#x1F4C8; 1 <br>
<p>Brenda Vilas Boas, Wolfgang Zirwas, Martin Haardt</p></summary>
<p>

**Abstract:** Knowledge of channel state information (CSI) is fundamental to many functionalities within the mobile wireless communications systems. With the advance of machine learning (ML) and digital maps, i.e., digital twins, we have a big opportunity to learn the propagation environment and design novel methods to derive and report CSI. In this work, we propose to combine untrained neural networks (UNNs) and conditional generative adversarial networks (cGANs) for MIMO channel recreation based on prior knowledge. The UNNs learn the prior-CSI for some locations which are used to build the input to a cGAN. Based on the prior-CSIs, their locations and the location of the desired channel, the cGAN is trained to output the channel expected at the desired location. This combined approach can be used for low overhead CSI reporting as, after training, we only need to report the desired location. Our results show that our method is successful in modelling the wireless channel and robust to location quantization errors in line of sight conditions.

</p>
</details>

<details><summary><b>Spiking CapsNet: A Spiking Neural Network With A Biologically Plausible Routing Rule Between Capsules</b>
<a href="https://arxiv.org/abs/2111.07785">arxiv:2111.07785</a>
&#x1F4C8; 1 <br>
<p>Dongcheng Zhao, Yang Li, Yi Zeng, Jihang Wang, Qian Zhang</p></summary>
<p>

**Abstract:** Spiking neural network (SNN) has attracted much attention due to their powerful spatio-temporal information representation ability. Capsule Neural Network (CapsNet) does well in assembling and coupling features at different levels. Here, we propose Spiking CapsNet by introducing the capsules into the modelling of spiking neural networks. In addition, we propose a more biologically plausible Spike Timing Dependent Plasticity routing mechanism. By fully considering the spatio-temporal relationship between the low-level spiking capsules and the high-level spiking capsules, the coupling ability between them is further improved. We have verified experiments on the MNIST and FashionMNIST datasets. Compared with other excellent SNN models, our algorithm still achieves high performance. Our Spiking CapsNet fully combines the strengthens of SNN and CapsNet, and shows strong robustness to noise and affine transformation. By adding different Salt-Pepper and Gaussian noise to the test dataset, the experimental results demonstrate that our Spiking CapsNet shows a more robust performance when there is more noise, while the artificial neural network can not correctly clarify. As well, our Spiking CapsNet shows strong generalization to affine transformation on the AffNIST dataset.

</p>
</details>

<details><summary><b>Simultaneously Achieving Sublinear Regret and Constraint Violations for Online Convex Optimization with Time-varying Constraints</b>
<a href="https://arxiv.org/abs/2111.07707">arxiv:2111.07707</a>
&#x1F4C8; 1 <br>
<p>Qingsong Liu, Wenfei Wu, Longbo Huang, Zhixuan Fang</p></summary>
<p>

**Abstract:** In this paper, we develop a novel virtual-queue-based online algorithm for online convex optimization (OCO) problems with long-term and time-varying constraints and conduct a performance analysis with respect to the dynamic regret and constraint violations. We design a new update rule of dual variables and a new way of incorporating time-varying constraint functions into the dual variables. To the best of our knowledge, our algorithm is the first parameter-free algorithm to simultaneously achieve sublinear dynamic regret and constraint violations. Our proposed algorithm also outperforms the state-of-the-art results in many aspects, e.g., our algorithm does not require the Slater condition. Meanwhile, for a group of practical and widely-studied constrained OCO problems in which the variation of consecutive constraints is smooth enough across time, our algorithm achieves $O(1)$ constraint violations. Furthermore, we extend our algorithm and analysis to the case when the time horizon $T$ is unknown. Finally, numerical experiments are conducted to validate the theoretical guarantees of our algorithm, and some applications of our proposed framework will be outlined.

</p>
</details>

<details><summary><b>Reachability analysis of neural networks using mixed monotonicity</b>
<a href="https://arxiv.org/abs/2111.07683">arxiv:2111.07683</a>
&#x1F4C8; 1 <br>
<p>Pierre-Jean Meyer</p></summary>
<p>

**Abstract:** This paper presents a new reachability analysis tool to compute an interval over-approximation of the output set of a feedforward neural network under given input uncertainty. The proposed approach adapts to neural networks an existing mixed-monotonicity method for the reachability analysis of dynamical systems and applies it to all possible partial networks within the given neural network. This ensures that the intersection of the obtained results is the tightest interval over-approximation of the output of each layer that can be obtained using mixed-monotonicity. Unlike other tools in the literature that focus on small classes of piecewise-affine or monotone activation functions, the main strength of our approach is its generality in the sense that it can handle neural networks with any Lipschitz-continuous activation function. In addition, the simplicity of the proposed framework allows users to very easily add unimplemented activation functions, by simply providing the function, its derivative and the global extrema and corresponding arguments of the derivative. Our algorithm is tested and compared to five other interval-based tools on 1000 randomly generated neural networks for four activation functions (ReLU, TanH, ELU, SiLU). We show that our tool always outperforms the Interval Bound Propagation method and that we obtain tighter output bounds than ReluVal, Neurify, VeriNet and CROWN (when they are applicable) in 15 to 60 percent of cases.

</p>
</details>

<details><summary><b>Quadratic speedup of global search using a biased crossover of two good solutions</b>
<a href="https://arxiv.org/abs/2111.07680">arxiv:2111.07680</a>
&#x1F4C8; 1 <br>
<p>Takuya Isomura</p></summary>
<p>

**Abstract:** The minimisation of cost functions is crucial in various optimisation fields. However, identifying their global minimum remains challenging owing to the huge computational cost incurred. This work analytically expresses the computational cost to identify an approximate global minimum for a class of cost functions defined under a high-dimensional discrete state space. Then, we derive an optimal global search scheme that minimises the computational cost. Mathematical analyses demonstrate that a combination of the gradient descent algorithm and the selection and crossover algorithm--with a biased crossover weight--maximises the search efficiency. Remarkably, its computational cost is of the square root order in contrast to that of the conventional gradient descent algorithms, indicating a quadratic speedup of global search. We corroborate this proposition using numerical analyses of the travelling salesman problem. The simple computational architecture and minimal computational cost of the proposed scheme are highly desirable for biological organisms and neuromorphic hardware.

</p>
</details>

<details><summary><b>Optimizing Unlicensed Coexistence Network Performance Through Data Learning</b>
<a href="https://arxiv.org/abs/2111.07583">arxiv:2111.07583</a>
&#x1F4C8; 1 <br>
<p>Srikant Manas Kala, Vanlin Sathya, Kunal Dahiya, Teruo Higashino, Hirozumi Yamaguchi</p></summary>
<p>

**Abstract:** Unlicensed LTE-WiFi coexistence networks are undergoing consistent densification to meet the rising mobile data demands. With the increase in coexistence network complexity, it is important to study network feature relationships (NFRs) and utilize them to optimize dense coexistence network performance. This work studies NFRs in unlicensed LTE-WiFi (LTE-U and LTE-LAA) networks through supervised learning of network data collected from real-world experiments. Different 802.11 standards and varying channel bandwidths are considered in the experiments and the learning model selection policy is precisely outlined. Thereafter, a comparative analysis of different LTE-WiFi network configurations is performed through learning model parameters such as R-sq, residual error, outliers, choice of predictor, etc. Further, a Network Feature Relationship based Optimization (NeFRO) framework is proposed. NeFRO improves upon the conventional optimization formulations by utilizing the feature-relationship equations learned from network data. It is demonstrated to be highly suitable for time-critical dense coexistence networks through two optimization objectives, viz., network capacity and signal strength. NeFRO is validated against four recent works on network optimization. NeFRO is successfully able to reduce optimization convergence time by as much as 24% while maintaining accuracy as high as 97.16%, on average.

</p>
</details>

<details><summary><b>Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking</b>
<a href="https://arxiv.org/abs/2111.07786">arxiv:2111.07786</a>
&#x1F4C8; 0 <br>
<p>Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, Andreas Krause</p></summary>
<p>

**Abstract:** Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.

</p>
</details>

<details><summary><b>A Data Quarantine Model to Secure Data in Edge Computing</b>
<a href="https://arxiv.org/abs/2111.07672">arxiv:2111.07672</a>
&#x1F4C8; 0 <br>
<p>Poornima Mahadevappa, Raja Kumar Murugesan</p></summary>
<p>

**Abstract:** Edge computing provides an agile data processing platform for latency-sensitive and communication-intensive applications through a decentralized cloud and geographically distributed edge nodes. Gaining centralized control over the edge nodes can be challenging due to security issues and threats. Among several security issues, data integrity attacks can lead to inconsistent data and intrude edge data analytics. Further intensification of the attack makes it challenging to mitigate and identify the root cause. Therefore, this paper proposes a new concept of data quarantine model to mitigate data integrity attacks by quarantining intruders. The efficient security solutions in cloud, ad-hoc networks, and computer systems using quarantine have motivated adopting it in edge computing. The data acquisition edge nodes identify the intruders and quarantine all the suspected devices through dimensionality reduction. During quarantine, the proposed concept builds the reputation scores to determine the falsely identified legitimate devices and sanitize their affected data to regain data integrity. As a preliminary investigation, this work identifies an appropriate machine learning method, Linear Discriminant Analysis (LDA), for dimensionality reduction. The LDA results in 72.83% quarantine accuracy and 0.9 seconds training time, which is efficient than other state-of-the-art methods. In future, this would be implemented and validated with ground truth data.

</p>
</details>


[Next Page](2021/2021-11/2021-11-14.md)
