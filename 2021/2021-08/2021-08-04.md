## Summary for 2021-08-04, created on 2021-12-18


<details><summary><b>A Pragmatic Look at Deep Imitation Learning</b>
<a href="https://arxiv.org/abs/2108.01867">arxiv:2108.01867</a>
&#x1F4C8; 20 <br>
<p>Kai Arulkumaran, Dan Ogawa Lillrank</p></summary>
<p>

**Abstract:** The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. The GAIL objective can be thought of as 1) matching the expert policy's state distribution; 2) penalising the learned policy's state distribution; and 3) maximising entropy. While theoretically motivated, in practice GAIL can be difficult to apply, not least due to the instabilities of adversarial training. In this paper, we take a pragmatic look at GAIL and related imitation learning algorithms. We implement and automatically tune a range of algorithms in a unified experimental setup, presenting a fair evaluation between the competing methods. From our results, our primary recommendation is to consider non-adversarial methods. Furthermore, we discuss the common components of imitation learning objectives, and present promising avenues for future research.

</p>
</details>

<details><summary><b>Sparse Continuous Distributions and Fenchel-Young Losses</b>
<a href="https://arxiv.org/abs/2108.01988">arxiv:2108.01988</a>
&#x1F4C8; 14 <br>
<p>André F. T. Martins, Marcos Treviso, António Farinhas, Pedro M. Q. Aguiar, Mário A. T. Figueiredo, Mathieu Blondel, Vlad Niculae</p></summary>
<p>

**Abstract:** Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent works on sparse alternatives to softmax (e.g. sparsemax, $α$-entmax, and fusedmax) and corresponding losses, which have varying support.
  This paper expands that line of work in several directions: first, it extends $Ω$-regularized prediction maps and Fenchel-Young losses to arbitrary domains (possibly countably infinite or continuous). For linearly parametrized families, we show that minimization of Fenchel-Young losses is equivalent to moment matching of the statistics, generalizing a fundamental property of exponential families. When $Ω$ is a Tsallis negentropy with parameter $α$, we obtain "deformed exponential families," which include $α$-entmax and sparsemax ($α$ = 2) as particular cases. For quadratic energy functions in continuous domains, the resulting densities are $β$-Gaussians, an instance of elliptical distributions that contain as particular cases the Gaussian, biweight, triweight and Epanechnikov densities, and for which we derive closed-form expressions for the variance, Tsallis entropy, and Fenchel-Young loss. When $Ω$ is a total variation or Sobolev regularizer, we obtain a continuous version of the fusedmax. Finally, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for $α\in \{1, 4/3, 3/2, 2\}$. Using them, we demonstrate our sparse continuous distributions for attention-based audio classification and visual question answering, showing that they allow attending to time intervals and compact regions.

</p>
</details>

<details><summary><b>Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping</b>
<a href="https://arxiv.org/abs/2108.02137">arxiv:2108.02137</a>
&#x1F4C8; 10 <br>
<p>Lukas Kondmann, Xiao Xiang Zhu</p></summary>
<p>

**Abstract:** Humanitarian mapping from space with machine learning helps policy-makers to timely and accurately identify people in need. However, recent concerns around fairness and transparency of algorithmic decision-making are a significant obstacle for applying these methods in practice. In this paper, we study if humanitarian mapping approaches from space are prone to bias in their predictions. We map village-level poverty and electricity rates in India based on nighttime lights (NTLs) with linear regression and random forest and analyze if the predictions systematically show prejudice against scheduled caste or tribe communities. To achieve this, we design a causal approach to measure counterfactual fairness based on propensity score matching. This allows to compare villages within a community of interest to synthetic counterfactuals. Our findings indicate that poverty is systematically overestimated and electricity systematically underestimated for scheduled tribes in comparison to a synthetic counterfactual group of villages. The effects have the opposite direction for scheduled castes where poverty is underestimated and electrification overestimated. These results are a warning sign for a variety of applications in humanitarian mapping where fairness issues would compromise policy goals.

</p>
</details>

<details><summary><b>A FAIR and AI-ready Higgs Boson Decay Dataset</b>
<a href="https://arxiv.org/abs/2108.02214">arxiv:2108.02214</a>
&#x1F4C8; 9 <br>
<p>Yifan Chen, E. A. Huerta, Javier Duarte, Philip Harris, Daniel S. Katz, Mark S. Neubauer, Daniel Diaz, Farouk Mokhtar, Raghav Kansal, Sang Eon Park, Volodymyr V. Kindratenko, Zhizhen Zhao, Roger Rusack</p></summary>
<p>

**Abstract:** To enable the reusability of massive scientific datasets by humans and machines, researchers aim to create scientific datasets that adhere to the principles of findability, accessibility, interoperability, and reusability (FAIR) for data and artificial intelligence (AI) models. This article provides a domain-agnostic, step-by-step assessment guide to evaluate whether or not a given dataset meets each FAIR principle. We then demonstrate how to use this guide to evaluate the FAIRness of an open simulated dataset produced by the CMS Collaboration at the CERN Large Hadron Collider. This dataset consists of Higgs boson decays and quark and gluon background, and is available through the CERN Open Data Portal. We also use other available tools to assess the FAIRness of this dataset, and incorporate feedback from members of the FAIR community to validate our results. This article is accompanied by a Jupyter notebook to facilitate an understanding and exploration of the dataset, including visualization of its elements. This study marks the first in a planned series of articles that will guide scientists in the creation and quantification of FAIRness in high energy particle physics datasets and AI models.

</p>
</details>

<details><summary><b>PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining</b>
<a href="https://arxiv.org/abs/2108.01887">arxiv:2108.01887</a>
&#x1F4C8; 9 <br>
<p>Machel Reid, Mikel Artetxe</p></summary>
<p>

**Abstract:** Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora, and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the reference translation according to a parallel corpus instead of recovering the original sequence. Our experiments on machine translation and cross-lingual natural language inference show an average improvement of 2.0 BLEU points and 6.7 accuracy points from integrating parallel data into pretraining, respectively, obtaining results that are competitive with several popular models at a fraction of their computational cost.

</p>
</details>

<details><summary><b>FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention</b>
<a href="https://arxiv.org/abs/2108.02347">arxiv:2108.02347</a>
&#x1F4C8; 7 <br>
<p>Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang</p></summary>
<p>

**Abstract:** We propose FMMformers, a class of efficient and flexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting particle simulation. FMM decomposes particle-particle interaction into near-field and far-field components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-field and far-field attention, modeling the near-field attention by a banded matrix and the far-field attention by a low-rank matrix. Computing the attention matrix for FMMformers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms of accuracy by a significant margin. For instance, FMMformers achieve an average classification accuracy of $60.74\%$ over the five Long Range Arena tasks, which is significantly better than the standard transformer's average accuracy of $58.70\%$.

</p>
</details>

<details><summary><b>Adversarial learning of cancer tissue representations</b>
<a href="https://arxiv.org/abs/2108.02223">arxiv:2108.02223</a>
&#x1F4C8; 7 <br>
<p>Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Wisuwat Sunhem, Roderick Murray-Smith, Aristotelis Tsirigos, Ke Yuan</p></summary>
<p>

**Abstract:** Deep learning based analysis of histopathology images shows promise in advancing the understanding of tumor progression, tumor micro-environment, and their underpinning biological processes. So far, these approaches have focused on extracting information associated with annotations. In this work, we ask how much information can be learned from the tissue architecture itself.
  We present an adversarial learning model to extract feature representations of cancer tissue, without the need for manual annotations. We show that these representations are able to identify a variety of morphological characteristics across three cancer types: Breast, colon, and lung. This is supported by 1) the separation of morphologic characteristics in the latent space; 2) the ability to classify tissue type with logistic regression using latent representations, with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3) the ability to predict the presence of tumor in Whole Slide Images (WSIs) using multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.
  Our results show that our model captures distinct phenotypic characteristics of real tissue samples, paving the way for further understanding of tumor progression and tumor micro-environment, and ultimately refining histopathological classification for diagnosis and treatment. The code and pretrained models are available at: https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations

</p>
</details>

<details><summary><b>Deep multi-task mining Calabi-Yau four-folds</b>
<a href="https://arxiv.org/abs/2108.02221">arxiv:2108.02221</a>
&#x1F4C8; 7 <br>
<p>Harold Erbin, Riccardo Finotello, Robin Schneider, Mohamed Tamaazousti</p></summary>
<p>

**Abstract:** We continue earlier efforts in computing the dimensions of tangent space cohomologies of Calabi-Yau manifolds using deep learning. In this paper, we consider the dataset of all Calabi-Yau four-folds constructed as complete intersections in products of projective spaces. Employing neural networks inspired by state-of-the-art computer vision architectures, we improve earlier benchmarks and demonstrate that all four non-trivial Hodge numbers can be learned at the same time using a multi-task architecture. With 30% (80%) training ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for $h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for $h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute, and taking into account the linear constraint arising from index computations, we get 100% total accuracy.

</p>
</details>

<details><summary><b>Using Interaction Data to Predict Engagement with Interactive Media</b>
<a href="https://arxiv.org/abs/2108.01949">arxiv:2108.01949</a>
&#x1F4C8; 7 <br>
<p>Jonathan Carlton, Andy Brown, Caroline Jay, John Keane</p></summary>
<p>

**Abstract:** Media is evolving from traditional linear narratives to personalised experiences, where control over information (or how it is presented) is given to individual audience members. Measuring and understanding audience engagement with this media is important in at least two ways: (1) a post-hoc understanding of how engaged audiences are with the content will help production teams learn from experience and improve future productions; (2), this type of media has potential for real-time measures of engagement to be used to enhance the user experience by adapting content on-the-fly. Engagement is typically measured by asking samples of users to self-report, which is time consuming and expensive. In some domains, however, interaction data have been used to infer engagement. Fortuitously, the nature of interactive media facilitates a much richer set of interaction data than traditional media; our research aims to understand if these data can be used to infer audience engagement. In this paper, we report a study using data captured from audience interactions with an interactive TV show to model and predict engagement. We find that temporal metrics, including overall time spent on the experience and the interval between events, are predictive of engagement. The results demonstrate that interaction data can be used to infer users' engagement during and after an experience, and the proposed techniques are relevant to better understand audience preference and responses.

</p>
</details>

<details><summary><b>With One Voice: Composing a Travel Voice Assistant from Re-purposed Models</b>
<a href="https://arxiv.org/abs/2108.11463">arxiv:2108.11463</a>
&#x1F4C8; 6 <br>
<p>Shachaf Poran, Gil Amsalem, Amit Beka, Dmitri Goldenberg</p></summary>
<p>

**Abstract:** Voice assistants provide users a new way of interacting with digital products, allowing them to retrieve information and complete tasks with an increased sense of control and flexibility. Such products are comprised of several machine learning models, like Speech-to-Text transcription, Named Entity Recognition and Resolution, and Text Classification. Building a voice assistant from scratch takes the prolonged efforts of several teams constructing numerous models and orchestrating between components. Alternatives such as using third-party vendors or re-purposing existing models may be considered to shorten time-to-market and development costs. However, each option has its benefits and drawbacks. We present key insights from building a voice search assistant for Booking.com search and recommendation system. Our paper compares the achieved performance and development efforts in dedicated tailor-made solutions against existing re-purposed models. We share and discuss our data-driven decisions about implementation trade-offs and their estimated outcomes in hindsight, showing that a fully functional machine learning product can be built from existing models.

</p>
</details>

<details><summary><b>A Machine-Learning-Ready Dataset Prepared from the Solar and Heliospheric Observatory Mission</b>
<a href="https://arxiv.org/abs/2108.06394">arxiv:2108.06394</a>
&#x1F4C8; 6 <br>
<p>Carl Shneider, Andong Hu, Ajay K. Tiwari, Monica G. Bobra, Karl Battams, Jannis Teunissen, Enrico Camporeale</p></summary>
<p>

**Abstract:** We present a Python tool to generate a standard dataset from solar images that allows for user-defined selection criteria and a range of pre-processing steps. Our Python tool works with all image products from both the Solar and Heliospheric Observatory (SoHO) and Solar Dynamics Observatory (SDO) missions. We discuss a dataset produced from the SoHO mission's multi-spectral images which is free of missing or corrupt data as well as planetary transits in coronagraph images, and is temporally synced making it ready for input to a machine learning system. Machine-learning-ready images are a valuable resource for the community because they can be used, for example, for forecasting space weather parameters. We illustrate the use of this data with a 3-5 day-ahead forecast of the north-south component of the interplanetary magnetic field (IMF) observed at Lagrange point one (L1). For this use case, we apply a deep convolutional neural network (CNN) to a subset of the full SoHO dataset and compare with baseline results from a Gaussian Naive Bayes classifier.

</p>
</details>

<details><summary><b>Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2108.02233">arxiv:2108.02233</a>
&#x1F4C8; 6 <br>
<p>Nitish Bhatt, David Ramon Prados, Nedim Hodzic, Christos Karanassios, H. R. Tizhoosh</p></summary>
<p>

**Abstract:** Lung nodules are commonly missed in chest radiographs. We propose and evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules in radiographs. P-AnoGAN modifies the fast anomaly detection generative adversarial network (f-AnoGAN) by utilizing a progressive GAN and a convolutional encoder-decoder-encoder pipeline. Model training uses only unlabelled healthy lung patches extracted from the Indiana University Chest X-Ray Collection. External validation and testing are performed using healthy and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for Radiological Technology datasets, respectively. Our model robustly identifies patches containing lung nodules in external validation and test data with ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised methods may be useful in challenging tasks such as lung nodule detection in radiographs.

</p>
</details>

<details><summary><b>Physics-based Noise Modeling for Extreme Low-light Photography</b>
<a href="https://arxiv.org/abs/2108.02158">arxiv:2108.02158</a>
&#x1F4C8; 6 <br>
<p>Kaixuan Wei, Ying Fu, Yinqiang Zheng, Jiaolong Yang</p></summary>
<p>

**Abstract:** Enhancing the visibility in extreme low-light environments is a challenging task. Under nearly lightless condition, existing image denoising methods could easily break down due to significantly low SNR. In this paper, we systematically study the noise statistics in the imaging pipeline of CMOS photosensors, and formulate a comprehensive noise model that can accurately characterize the real noise structures. Our novel model considers the noise sources caused by digital camera electronics which are largely overlooked by existing methods yet have significant influence on raw measurement in the dark. It provides a way to decouple the intricate noise structure into different statistical distributions with physical interpretations. Moreover, our noise model can be used to synthesize realistic training data for learning-based low-light denoising algorithms. In this regard, although promising results have been shown recently with deep convolutional neural networks, the success heavily depends on abundant noisy clean image pairs for training, which are tremendously difficult to obtain in practice. Generalizing their trained models to images from new devices is also problematic. Extensive experiments on multiple low-light denoising datasets -- including a newly collected one in this work covering various devices -- show that a deep neural network trained with our proposed noise formation model can reach surprisingly-high accuracy. The results are on par with or sometimes even outperform training with paired real data, opening a new door to real-world extreme low-light photography.

</p>
</details>

<details><summary><b>Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations</b>
<a href="https://arxiv.org/abs/2108.01846">arxiv:2108.01846</a>
&#x1F4C8; 6 <br>
<p>Yuping Luo, Tengyu Ma</p></summary>
<p>

**Abstract:** Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics model and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates, learned via adversarial training, ensure the policy's safety assuming calibrated learned dynamics model. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.

</p>
</details>

<details><summary><b>Mitigating harm in language models with conditional-likelihood filtration</b>
<a href="https://arxiv.org/abs/2108.07790">arxiv:2108.07790</a>
&#x1F4C8; 5 <br>
<p>Helen Ngo, Cooper Raterink, João G. M. Araújo, Ivan Zhang, Carol Chen, Adrien Morisot, Nicholas Frosst</p></summary>
<p>

**Abstract:** Language models trained on large-scale unfiltered datasets curated from the open web acquire systemic biases, prejudices, and harmful views from their training data. We present a methodology for programmatically identifying and removing harmful text from web-scale datasets. A pretrained language model is used to calculate the log-likelihood of researcher-written trigger phrases conditioned on a specific document, which is used to identify and filter documents from the dataset. We demonstrate that models trained on this filtered dataset exhibit lower propensity to generate harmful text, with a marginal decrease in performance on standard language modeling benchmarks compared to unfiltered baselines. We provide a partial explanation for this performance gap by surfacing examples of hate speech and other undesirable content from standard language modeling benchmarks. Finally, we discuss the generalization of this method and how trigger phrases which reflect specific values can be used by researchers to build language models which are more closely aligned with their values.

</p>
</details>

<details><summary><b>The application of predictive analytics to identify at-risk students in health professions education</b>
<a href="https://arxiv.org/abs/2108.07709">arxiv:2108.07709</a>
&#x1F4C8; 5 <br>
<p>Anshul Kumar, Roger Edwards, Lisa Walker</p></summary>
<p>

**Abstract:** Introduction: When a learner fails to reach a milestone, educators often wonder if there had been any warning signs that could have allowed them to intervene sooner. Machine learning is used to predict which students are at risk of failing a national certifying exam. Predictions are made well in advance of the exam, such that educators can meaningfully intervene before students take the exam.
  Methods: Using already-collected, first-year student assessment data from four cohorts in a Master of Physician Assistant Studies program, the authors implement an "adaptive minimum match" version of the k-nearest neighbors algorithm (AMMKNN), using changing numbers of neighbors to predict each student's future exam scores on the Physician Assistant National Certifying Examination (PANCE). Leave-one-out cross validation (LOOCV) was used to evaluate the practical capabilities of this model, before making predictions for new students.
  Results: The best predictive model has an accuracy of 93%, sensitivity of 69%, and specificity of 94%. It generates a predicted PANCE score for each student, one year before they are scheduled to take the exam. Students can then be prospectively categorized into groups that need extra support, optional extra support, or no extra support. The educator then has one year to provide the appropriate customized support to each type of student.
  Conclusions: Predictive analytics can help health professions educators allocate scarce time and resources across their students. Interprofessional educators can use the included methods and code to generate predicted test outcomes for students. The authors recommend that educators using this or similar predictive methods act responsibly and transparently.

</p>
</details>

<details><summary><b>Stochastic Deep Model Reference Adaptive Control</b>
<a href="https://arxiv.org/abs/2108.03120">arxiv:2108.03120</a>
&#x1F4C8; 5 <br>
<p>Girish Joshi, Girish Chowdhary</p></summary>
<p>

**Abstract:** In this paper, we present a Stochastic Deep Neural Network-based Model Reference Adaptive Control. Building on our work "Deep Model Reference Adaptive Control", we extend the controller capability by using Bayesian deep neural networks (DNN) to represent uncertainties and model non-linearities. Stochastic Deep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the output-layer weights of the DNN model in real-time, while a data-driven supervised learning algorithm is used to update the inner-layers parameters. This asynchronous network update ensures boundedness and guaranteed tracking performance with a learning-based real-time feedback controller. A Bayesian approach to DNN learning helped avoid over-fitting the data and provide confidence intervals over the predictions. The controller's stochastic nature also ensured "Induced Persistency of excitation," leading to convergence of the overall system signal.

</p>
</details>

<details><summary><b>O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning</b>
<a href="https://arxiv.org/abs/2108.02359">arxiv:2108.02359</a>
&#x1F4C8; 5 <br>
<p>Fenglin Liu, Xuancheng Ren, Xian Wu, Bang Yang, Shen Ge, Xu Sun</p></summary>
<p>

**Abstract:** Video captioning combines video understanding and language generation. Different from image captioning that describes a static image with details of almost every object, video captioning usually considers a sequence of frames and biases towards focused objects, e.g., the objects that stay in focus regardless of the changing background. Therefore, detecting and properly accommodating focused objects is critical in video captioning. To enforce the description of focused objects and achieve controllable video captioning, we propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs caption generation in three steps: 1) identify the focused objects and predict their locations in the target caption; 2) generate the related attribute words and relation words of these focused objects to form a draft caption; and 3) combine video information to refine the draft caption to a fluent final caption. Since the focused objects are generated and located ahead of other words, it is difficult to apply the word-by-word autoregressive generation process; instead, we adopt a non-autoregressive approach. The experiments on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness of O2NA, which achieves results competitive with the state-of-the-arts but with both higher diversity and higher inference speed.

</p>
</details>

<details><summary><b>Pan-Cancer Integrative Histology-Genomic Analysis via Interpretable Multimodal Deep Learning</b>
<a href="https://arxiv.org/abs/2108.02278">arxiv:2108.02278</a>
&#x1F4C8; 5 <br>
<p>Richard J. Chen, Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen, Jana Lipkova, Muhammad Shaban, Maha Shady, Mane Williams, Bumjin Joo, Zahra Noor, Faisal Mahmood</p></summary>
<p>

**Abstract:** The rapidly emerging field of deep learning-based computational pathology has demonstrated promise in developing objective prognostic models from histology whole slide images. However, most prognostic models are either based on histology or genomics alone and do not address how histology and genomics can be integrated to develop joint image-omic prognostic models. Additionally identifying explainable morphological and molecular descriptors from these models that govern such prognosis is of interest. We used multimodal deep learning to integrate gigapixel whole slide pathology images, RNA-seq abundance, copy number variation, and mutation data from 5,720 patients across 14 major cancer types. Our interpretable, weakly-supervised, multimodal deep learning algorithm is able to fuse these heterogeneous modalities for predicting outcomes and discover prognostic features from these modalities that corroborate with poor and favorable outcomes via multimodal interpretability. We compared our model with unimodal deep learning models trained on histology slides and molecular profiles alone, and demonstrate performance increase in risk stratification on 9 out of 14 cancers. In addition, we analyze morphologic and molecular markers responsible for prognostic predictions across all cancer types. All analyzed data, including morphological and molecular correlates of patient prognosis across the 14 cancer types at a disease and patient level are presented in an interactive open-access database (http://pancancer.mahmoodlab.org) to allow for further exploration and prognostic biomarker discovery. To validate that these model explanations are prognostic, we further analyzed high attention morphological regions in WSIs, which indicates that tumor-infiltrating lymphocyte presence corroborates with favorable cancer prognosis on 9 out of 14 cancer types studied.

</p>
</details>

<details><summary><b>MRI to PET Cross-Modality Translation using Globally and Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease</b>
<a href="https://arxiv.org/abs/2108.02160">arxiv:2108.02160</a>
&#x1F4C8; 5 <br>
<p>Apoorva Sikka,  Skand, Jitender Singh Virk, Deepti R. Bathula</p></summary>
<p>

**Abstract:** Medical imaging datasets are inherently high dimensional with large variability and low sample sizes that limit the effectiveness of deep learning algorithms. Recently, generative adversarial networks (GANs) with the ability to synthesize realist images have shown great potential as an alternative to standard data augmentation techniques. Our work focuses on cross-modality synthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans from structural Magnetic Resonance~(MR) images using generative models to facilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we propose a novel end-to-end, globally and locally aware image-to-image translation GAN (GLA-GAN) with a multi-path architecture that enforces both global structural integrity and fidelity to local details. We further supplement the standard adversarial loss with voxel-level intensity, multi-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based loss components that reduce reconstruction error, enforce structural consistency at different scales and perceive variation in regional sensitivity to AD respectively. Experimental results demonstrate that our GLA-GAN not only generates synthesized FDG-PET scans with enhanced image quality but also superior clinical utility in improving AD diagnosis compared to state-of-the-art models. Finally, we attempt to interpret some of the internal units of the GAN that are closely related to this specific cross-modality generation task.

</p>
</details>

<details><summary><b>Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction</b>
<a href="https://arxiv.org/abs/2108.02110">arxiv:2108.02110</a>
&#x1F4C8; 5 <br>
<p>Minyi Zhao, Yi Xu, Shuigeng Zhou</p></summary>
<p>

**Abstract:** A number of deep learning based algorithms have been proposed to recover high-quality videos from low-quality compressed ones. Among them, some restore the missing details of each frame via exploring the spatiotemporal information of neighboring frames. However, these methods usually suffer from a narrow temporal scope, thus may miss some useful details from some frames outside the neighboring ones. In this paper, to boost artifact removal, on the one hand, we propose a Recursive Fusion (RF) module to model the temporal dependency within a long temporal range. Specifically, RF utilizes both the current reference frames and the preceding hidden state to conduct better spatiotemporal compensation. On the other hand, we design an efficient and effective Deformable Spatiotemporal Attention (DSTA) module such that the model can pay more effort on restoring the artifact-rich areas like the boundary area of a moving object. Extensive experiments show that our method outperforms the existing ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual effect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.

</p>
</details>

<details><summary><b>Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators</b>
<a href="https://arxiv.org/abs/2108.02032">arxiv:2108.02032</a>
&#x1F4C8; 5 <br>
<p>Cosmin Octavian Pene, Amirmasoud Ghiassi, Taraneh Younesian, Robert Birke, Lydia Y. Chen</p></summary>
<p>

**Abstract:** Multi-label learning is an emerging extension of the multi-class classification where an image contains multiple labels. Not only acquiring a clean and fully labeled dataset in multi-label learning is extremely expensive, but also many of the actual labels are corrupted or missing due to the automated or non-expert annotation techniques. Noisy label data decrease the prediction performance drastically. In this paper, we propose a novel Gold Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that operates robust against noisy labels. GALC-SLR estimates the noise confusion matrix using single-label samples, then constructs an asymmetric loss correction via estimated confusion matrix to avoid overfitting to the noisy labels. Empirical results show that our method outperforms the state-of-the-art original asymmetric loss multi-label classifier under all corruption levels, showing mean average precision improvement up to 28.67% on a real world dataset of MS-COCO, yielding a better generalization of the unseen data and increased prediction performance.

</p>
</details>

<details><summary><b>PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations</b>
<a href="https://arxiv.org/abs/2108.01938">arxiv:2108.01938</a>
&#x1F4C8; 5 <br>
<p>Moshe Eliasof, Eldad Haber, Eran Treister</p></summary>
<p>

**Abstract:** Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.

</p>
</details>

<details><summary><b>Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging</b>
<a href="https://arxiv.org/abs/2108.03233">arxiv:2108.03233</a>
&#x1F4C8; 4 <br>
<p>A. Al-Saffar, A. Stancombe, A. Zamani, A. Abbosh</p></summary>
<p>

**Abstract:** Incorporating boundaries of the imaging object as a priori information to imaging algorithms can significantly improve the performance of electromagnetic medical imaging systems. To avoid overly complicating the system by using different sensors and the adverse effect of the subject's movement, a learning-based method is proposed to estimate the boundary (external contour) of the imaged object using the same electromagnetic imaging data. While imaging techniques may discard the reflection coefficients for being dominant and uninformative for imaging, these parameters are made use of for boundary detection. The learned model is verified through independent clinical human trials by using a head imaging system with a 16-element antenna array that works across the band 0.7-1.6 GHz. The evaluation demonstrated that the model achieves average dissimilarity of 0.012 in Hu-moment while detecting head boundary. The model enables fast scan and image creation while eliminating the need for additional devices for accurate boundary estimation.

</p>
</details>

<details><summary><b>Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests</b>
<a href="https://arxiv.org/abs/2108.02356">arxiv:2108.02356</a>
&#x1F4C8; 4 <br>
<p>Siqi Wang, Guang Yu, Zhiping Cai, Xinwang Liu, En Zhu, Jianping Yin</p></summary>
<p>

**Abstract:** Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete "visual cloze tests" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual "word" and "sentence" respectively, we deliberately erase a certain "word" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.

</p>
</details>

<details><summary><b>Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity</b>
<a href="https://arxiv.org/abs/2108.02297">arxiv:2108.02297</a>
&#x1F4C8; 4 <br>
<p>Chang Gao, Tobi Delbruck, Shih-Chii Liu</p></summary>
<p>

**Abstract:** Long Short-Term Memory (LSTM) recurrent networks are frequently used for tasks involving time-sequential data such as speech recognition. However, it is difficult to deploy these networks on hardware to achieve high throughput and low latency because the fully connected structure makes LSTM networks a memory-bounded algorithm. Previous LSTM accelerators either exploited weight spatial sparsity or temporal activation sparsity. This paper proposes a new accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve ultra-low latency inference. The spatial sparsity is induced using our proposed pruning method called Column-Balanced Targeted Dropout (CBTD), which structures sparse weight matrices for balanced workload. It achieved up to 96% weight sparsity with negligible accuracy difference for an LSTM network trained on a TIMIT phone recognition task. To induce temporal sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU method to the LSTM network. This combined sparsity simultaneously saves on the weight memory access and associated arithmetic operations. Spartus was implemented on a Xilinx Zynq-7100 FPGA. The Spartus per-sample latency for a single DeltaLSTM layer of 1024 neurons averages 1 us. Spartus achieved 9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which, respectively, are 4X and 7X higher than the previous state-of-the-art.

</p>
</details>

<details><summary><b>Statistical Analysis of Wasserstein Distributionally Robust Estimators</b>
<a href="https://arxiv.org/abs/2108.02120">arxiv:2108.02120</a>
&#x1F4C8; 4 <br>
<p>Jose Blanchet, Karthyek Murthy, Viet Anh Nguyen</p></summary>
<p>

**Abstract:** We consider statistical methods which invoke a min-max distributionally robust formulation to extract good out-of-sample performance in data-driven optimization and learning problems. Acknowledging the distributional uncertainty in learning from limited samples, the min-max formulations introduce an adversarial inner player to explore unseen covariate data. The resulting Distributionally Robust Optimization (DRO) formulations, which include Wasserstein DRO formulations (our main focus), are specified using optimal transportation phenomena. Upon describing how these infinite-dimensional min-max problems can be approached via a finite-dimensional dual reformulation, the tutorial moves into its main component, namely, explaining a generic recipe for optimally selecting the size of the adversary's budget. This is achieved by studying the limit behavior of an optimal transport projection formulation arising from an inquiry on the smallest confidence region that includes the unknown population risk minimizer. Incidentally, this systematic prescription coincides with those in specific examples in high-dimensional statistics and results in error bounds that are free from the curse of dimensions. Equipped with this prescription, we present a central limit theorem for the DRO estimator and provide a recipe for constructing compatible confidence regions that are useful for uncertainty quantification. The rest of the tutorial is devoted to insights into the nature of the optimizers selected by the min-max formulations and additional applications of optimal transport projections.

</p>
</details>

<details><summary><b>The MIT Supercloud Dataset</b>
<a href="https://arxiv.org/abs/2108.02037">arxiv:2108.02037</a>
&#x1F4C8; 4 <br>
<p>Siddharth Samsi, Matthew L Weiss, David Bestor, Baolin Li, Michael Jones, Albert Reuther, Daniel Edelman, William Arcand, Chansup Byun, John Holodnack, Matthew Hubbell, Jeremy Kepner, Anna Klein, Joseph McDonald, Adam Michaleas, Peter Michaleas, Lauren Milechin, Julia Mullen, Charles Yee, Benjamin Price, Andrew Prout, Antonio Rosa, Allan Vanterpool, Lindsey McEvoy, Anson Cheng</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) and Machine learning (ML) workloads are an increasingly larger share of the compute workloads in traditional High-Performance Computing (HPC) centers and commercial cloud systems. This has led to changes in deployment approaches of HPC clusters and the commercial cloud, as well as a new focus on approaches to optimized resource usage, allocations and deployment of new AI frame- works, and capabilities such as Jupyter notebooks to enable rapid prototyping and deployment. With these changes, there is a need to better understand cluster/datacenter operations with the goal of developing improved scheduling policies, identifying inefficiencies in resource utilization, energy/power consumption, failure prediction, and identifying policy violations. In this paper we introduce the MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the analysis of large scale HPC and datacenter/cloud operations. We provide detailed monitoring logs from the MIT Supercloud system, which include CPU and GPU usage by jobs, memory usage, file system logs, and physical monitoring data. This paper discusses the details of the dataset, collection methodology, data availability, and discusses potential challenge problems being developed using this data. Datasets and future challenge announcements will be available via https://dcc.mit.edu.

</p>
</details>

<details><summary><b>Generic Neural Architecture Search via Regression</b>
<a href="https://arxiv.org/abs/2108.01899">arxiv:2108.01899</a>
&#x1F4C8; 4 <br>
<p>Yuhong Li, Cong Hao, Pan Li, Jinjun Xiong, Deming Chen</p></summary>
<p>

**Abstract:** Most existing neural architecture search (NAS) algorithms are dedicated to and evaluated by the downstream tasks, e.g., image classification in computer vision. However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. In this paper, we attempt to answer two fundamental questions related to NAS. (1) Is it necessary to use the performance of specific downstream tasks to evaluate and search for good neural architectures? (2) Can we perform NAS effectively and efficiently while being agnostic to the downstream tasks? To answer these questions, we propose a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but instead adopts regression on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufficient usage of training samples. Extensive experiments across 13 CNN search spaces and one NLP space demonstrate the remarkable efficiency of GenNAS using regression, in terms of both evaluating the neural architectures (quantified by the ranking correlation Spearman's rho between the approximated performances and the downstream task performances) and the convergence speed for training (within a few seconds).

</p>
</details>

<details><summary><b>Risk Conditioned Neural Motion Planning</b>
<a href="https://arxiv.org/abs/2108.01851">arxiv:2108.01851</a>
&#x1F4C8; 4 <br>
<p>Xin Huang, Meng Feng, Ashkan Jasour, Guy Rosman, Brian Williams</p></summary>
<p>

**Abstract:** Risk-bounded motion planning is an important yet difficult problem for safety-critical tasks. While existing mathematical programming methods offer theoretical guarantees in the context of constrained Markov decision processes, they either lack scalability in solving larger problems or produce conservative plans. Recent advances in deep reinforcement learning improve scalability by learning policy networks as function approximators. In this paper, we propose an extension of soft actor critic model to estimate the execution risk of a plan through a risk critic and produce risk-bounded policies efficiently by adding an extra risk term in the loss function of the policy network. We define the execution risk in an accurate form, as opposed to approximating it through a summation of immediate risks at each time step that leads to conservative plans. Our proposed model is conditioned on a continuous spectrum of risk bounds, allowing the user to adjust the risk-averse level of the agent on the fly. Through a set of experiments, we show the advantage of our model in terms of both computational time and plan quality, compared to a state-of-the-art mathematical programming baseline, and validate its performance in more complicated scenarios, including nonlinear dynamics and larger state space.

</p>
</details>

<details><summary><b>Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2108.03169">arxiv:2108.03169</a>
&#x1F4C8; 3 <br>
<p>Mohammed Abouheaf, Shuzheng Qu, Wail Gueaieb, Rami Abielmona, Moufid Harb</p></summary>
<p>

**Abstract:** This article elaborates on how machine learning (ML) can leverage the solution of a contemporary problem related to the security of maritime domains. The worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents have led to serious environmental and economic consequences which involve drastic changes in our ecosystems in addition to financial losses caused by the depletion of natural resources. The Fisheries and Aquatic Department (FAD) of the United Nation's Food and Agriculture Organization (FAO) issued a report which indicated that the annual losses due to IUU fishing reached $25 Billion. This imposes negative impacts on the future-biodiversity of the marine ecosystem and domestic Gross National Product (GNP). Hence, robust interception mechanisms are increasingly needed for detecting and pursuing the unrelenting illegal fishing incidents in maritime territories. This article addresses the problem of coordinating the motion of a fleet of marine vessels (pursuers) to catch an IUU vessel while still in local waters. The problem is formulated as a pursuer-evader problem that is tackled within an ML framework. One or more pursuers, such as law enforcement vessels, intercept an evader (i.e., the illegal fishing ship) using an online reinforcement learning mechanism that is based on a value iteration process. It employs real-time navigation measurements of the evader ship as well as those of the pursuing vessels and returns back model-free interception strategies.

</p>
</details>

<details><summary><b>Pattern Recognition in Vital Signs Using Spectrograms</b>
<a href="https://arxiv.org/abs/2108.03168">arxiv:2108.03168</a>
&#x1F4C8; 3 <br>
<p>Sidharth Srivatsav Sribhashyam, Md Sirajus Salekin, Dmitry Goldgof, Ghada Zamzmi, Mark Last, Yu Sun</p></summary>
<p>

**Abstract:** Spectrograms visualize the frequency components of a given signal which may be an audio signal or even a time-series signal. Audio signals have higher sampling rate and high variability of frequency with time. Spectrograms can capture such variations well. But, vital signs which are time-series signals have less sampling frequency and low-frequency variability due to which, spectrograms fail to express variations and patterns. In this paper, we propose a novel solution to introduce frequency variability using frequency modulation on vital signs. Then we apply spectrograms on frequency modulated signals to capture the patterns. The proposed approach has been evaluated on 4 different medical datasets across both prediction and classification tasks. Significant results are found showing the efficacy of the approach for vital sign signals. The results from the proposed approach are promising with an accuracy of 91.55% and 91.67% in prediction and classification tasks respectively.

</p>
</details>

<details><summary><b>Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network</b>
<a href="https://arxiv.org/abs/2108.02352">arxiv:2108.02352</a>
&#x1F4C8; 3 <br>
<p>Bowen Xing, Ivor W. Tsang</p></summary>
<p>

**Abstract:** Aspect-level sentiment classification (ASC) aims to predict the fine-grained sentiment polarity towards a given aspect mentioned in a review. Despite recent advances in ASC, enabling machines to preciously infer aspect sentiments is still challenging. This paper tackles two challenges in ASC: (1) due to lack of aspect knowledge, aspect representation derived in prior works is inadequate to represent aspect's exact meaning and property information; (2) prior works only capture either local syntactic information or global relational information, thus missing either one of them leads to insufficient syntactic information. To tackle these challenges, we propose a novel ASC model which not only end-to-end embeds and leverages aspect knowledge but also marries the two kinds of syntactic information and lets them compensate for each other. Our model includes three key components: (1) a knowledge-aware gated recurrent memory network recurrently integrates dynamically summarized aspect knowledge; (2) a dual syntax graph network combines both kinds of syntactic information to comprehensively capture sufficient syntactic information; (3) a knowledge integrating gate re-enhances the final representation with further needed aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the aspect-related semantics from all hidden states into the final representation. Experimental results on several benchmark datasets demonstrate the effectiveness of our model, which overpass previous state-of-the-art models by large margins in terms of both Accuracy and Macro-F1.

</p>
</details>

<details><summary><b>Dynamic Relevance Learning for Few-Shot Object Detection</b>
<a href="https://arxiv.org/abs/2108.02235">arxiv:2108.02235</a>
&#x1F4C8; 3 <br>
<p>Weijie Liu, Chong Wang*, Haohe Li, Shenghao Yu, Song Chen, Xulun Ye, Jiafei Wu</p></summary>
<p>

**Abstract:** Expensive bounding-box annotations have limited the development of object detection task. Thus, it is necessary to focus on more challenging task of few-shot object detection. It requires the detector to recognize objects of novel classes with only a few training samples. Nowadays, many existing popular methods based on meta-learning have achieved promising performance, such as Meta R-CNN series. However, only a single category of support data is used as the attention to guide the detecting of query images each time. Their relevance to each other remains unexploited. Moreover, a lot of recent works treat the support data and query images as independent branch without considering the relationship between them. To address this issue, we propose a dynamic relevance learning model, which utilizes the relationship between all support images and Region of Interest (RoI) on the query images to construct a dynamic graph convolutional network (GCN). By adjusting the prediction distribution of the base detector using the output of this GCN, the proposed model can guide the detector to improve the class representation implicitly. Comprehensive experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves the best overall performance, which shows its effectiveness of learning more generalized features. Our code is available at https://github.com/liuweijie19980216/DRL-for-FSOD.

</p>
</details>

<details><summary><b>Spacetime Neural Network for High Dimensional Quantum Dynamics</b>
<a href="https://arxiv.org/abs/2108.02200">arxiv:2108.02200</a>
&#x1F4C8; 3 <br>
<p>Jiangran Wang, Zhuo Chen, Di Luo, Zhizhen Zhao, Vera Mikyoung Hur, Bryan K. Clark</p></summary>
<p>

**Abstract:** We develop a spacetime neural network method with second order optimization for solving quantum dynamics from the high dimensional Schrödinger equation. In contrast to the standard iterative first order optimization and the time-dependent variational principle, our approach utilizes the implicit mid-point method and generates the solution for all spatial and temporal values simultaneously after optimization. We demonstrate the method in the Schrödinger equation with a self-normalized autoregressive spacetime neural network construction. Future explorations for solving different high dimensional differential equations are discussed.

</p>
</details>

<details><summary><b>Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning</b>
<a href="https://arxiv.org/abs/2108.02148">arxiv:2108.02148</a>
&#x1F4C8; 3 <br>
<p>Ahmed Ibrahim, Ayman El-Refai, Sara Ahmed, Mariam Aboul-Ela, Hesham M. Eraqi, Mohamed Moustafa</p></summary>
<p>

**Abstract:** Due to the mass advancement in ubiquitous technologies nowadays, new pervasive methods have come into the practice to provide new innovative features and stimulate the research on new human-computer interactions. This paper presents a hand gesture recognition method that utilizes the smartphone's built-in speakers and microphones. The proposed system emits an ultrasonic sonar-based signal (inaudible sound) from the smartphone's stereo speakers, which is then received by the smartphone's microphone and processed via a Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data augmentation techniques are proposed to improve the detection accuracy and three dual-channel input fusion methods are compared. The first method merges the dual-channel audio as a single input spectrogram image. The second method adopts early fusion by concatenating the dual-channel spectrograms. The third method adopts late fusion by having two convectional input branches processing each of the dual-channel spectrograms and then the outputs are merged by the last layers. Our experimental results demonstrate a promising detection accuracy for the six gestures presented in our publicly available dataset with an accuracy of 93.58\% as a baseline.

</p>
</details>

<details><summary><b>Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images</b>
<a href="https://arxiv.org/abs/2108.02122">arxiv:2108.02122</a>
&#x1F4C8; 3 <br>
<p>Boon Peng Yap, Beng Koon Ng</p></summary>
<p>

**Abstract:** We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.

</p>
</details>

<details><summary><b>Discovering outliers in the Mars Express thermal power consumption patterns</b>
<a href="https://arxiv.org/abs/2108.02067">arxiv:2108.02067</a>
&#x1F4C8; 3 <br>
<p>Matej Petković, Luke Lucas, Tomaž Stepišnik, Panče Panov, Nikola Simidjievski, Dragi Kocev</p></summary>
<p>

**Abstract:** The Mars Express (MEX) spacecraft has been orbiting Mars since 2004. The operators need to constantly monitor its behavior and handle sporadic deviations (outliers) from the expected patterns of measurements of quantities that the satellite is sending to Earth. In this paper, we analyze the patterns of the electrical power consumption of MEX's thermal subsystem, that maintains the spacecraft's temperature at the desired level. The consumption is not constant, but should be roughly periodic in the short term, with the period that corresponds to one orbit around Mars. By using long short-term memory neural networks, we show that the consumption pattern is more irregular than expected, and successfully detect such irregularities, opening possibility for automatic outlier detection on MEX in the future.

</p>
</details>

<details><summary><b>Signature Verification using Geometrical Features and Artificial Neural Network Classifier</b>
<a href="https://arxiv.org/abs/2108.02029">arxiv:2108.02029</a>
&#x1F4C8; 3 <br>
<p>Anamika Jain, Satish Kumar Singh, Krishna Pratap Singh</p></summary>
<p>

**Abstract:** Signature verification has been one of the major researched areas in the field of computer vision. Many financial and legal organizations use signature verification as access control and authentication. Signature images are not rich in texture; however, they have much vital geometrical information. Through this work, we have proposed a signature verification methodology that is simple yet effective. The technique presented in this paper harnesses the geometrical features of a signature image like center, isolated points, connected components, etc., and with the power of Artificial Neural Network (ANN) classifier, classifies the signature image based on their geometrical features. Publicly available dataset MCYT, BHSig260 (contains the image of two regional languages Bengali and Hindi) has been used in this paper to test the effectiveness of the proposed method. We have received a lower Equal Error Rate (EER) on MCYT 100 dataset and higher accuracy on the BHSig260 dataset.

</p>
</details>

<details><summary><b>Staged trees and asymmetry-labeled DAGs</b>
<a href="https://arxiv.org/abs/2108.01994">arxiv:2108.01994</a>
&#x1F4C8; 3 <br>
<p>Gherardo Varando, Federico Carli, Manuele Leonelli</p></summary>
<p>

**Abstract:** Bayesian networks are a widely-used class of probabilistic graphical models capable of representing symmetric conditional independence between variables of interest using the topology of the underlying graph. They can be seen as a special case of the much more general class of models called staged trees, which can represent any type of non-symmetric conditional independence. Here we formalize the relationship between these two models and introduce a minimal Bayesian network representation of the staged tree, which can be used to read conditional independences in an intuitive way. Furthermore, we define a new labeled graph, termed asymmetry-labeled directed acyclic graph, whose edges are labeled to denote the type of dependence existing between any two random variables. Various datasets are used to illustrate the methodology, highlighting the need to construct models which more flexibly encode and represent non-symmetric structures.

</p>
</details>

<details><summary><b>How to Query Language Models?</b>
<a href="https://arxiv.org/abs/2108.01928">arxiv:2108.01928</a>
&#x1F4C8; 3 <br>
<p>Leonard Adolphs, Shehzaad Dhuliawala, Thomas Hofmann</p></summary>
<p>

**Abstract:** Large pre-trained language models (LMs) are capable of not only recovering linguistic but also factual and commonsense knowledge. To access the knowledge stored in mask-based LMs, we can use cloze-style questions and let the model fill in the blank. The flexibility advantage over structured knowledge bases comes with the drawback of finding the right query for a certain information need. Inspired by human behavior to disambiguate a question, we propose to query LMs by example. To clarify the ambivalent question "Who does Neuer play for?", a successful strategy is to demonstrate the relation using another subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply this approach of querying by example to the LAMA probe and obtain substantial improvements of up to 37.8% for BERT-large on the T-REx data when providing only 10 demonstrations--even outperforming a baseline that queries the model with up to 40 paraphrases of the question. The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass. This suggests that LMs contain more factual and commonsense knowledge than previously assumed--if we query the model in the right way.

</p>
</details>

<details><summary><b>Fake News and Phishing Detection Using a Machine Learning Trained Expert System</b>
<a href="https://arxiv.org/abs/2108.08264">arxiv:2108.08264</a>
&#x1F4C8; 2 <br>
<p>Benjamin Fitzpatrick, Xinyu "Sherwin" Liang, Jeremy Straub</p></summary>
<p>

**Abstract:** Expert systems have been used to enable computers to make recommendations and decisions. This paper presents the use of a machine learning trained expert system (MLES) for phishing site detection and fake news detection. Both topics share a similar goal: to design a rule-fact network that allows a computer to make explainable decisions like domain experts in each respective area. The phishing website detection study uses a MLES to detect potential phishing websites by analyzing site properties (like URL length and expiration time). The fake news detection study uses a MLES rule-fact network to gauge news story truthfulness based on factors such as emotion, the speaker's political affiliation status, and job. The two studies use different MLES network implementations, which are presented and compared herein. The fake news study utilized a more linear design while the phishing project utilized a more complex connection structure. Both networks' inputs are based on commonly available data sets.

</p>
</details>

<details><summary><b>Recommending Insurance products by using Users' Sentiments</b>
<a href="https://arxiv.org/abs/2108.06210">arxiv:2108.06210</a>
&#x1F4C8; 2 <br>
<p>Rohan Parasrampuria, Ayan Ghosh, Suchandra Dutta, Dhrubasish Sarkar</p></summary>
<p>

**Abstract:** In today's tech-savvy world every industry is trying to formulate methods for recommending products by combining several techniques and algorithms to form a pool that would bring forward the most enhanced models for making the predictions. Building on these lines is our paper focused on the application of sentiment analysis for recommendation in the insurance domain. We tried building the following Machine Learning models namely, Logistic Regression, Multinomial Naive Bayes, and the mighty Random Forest for analyzing the polarity of a given feedback line given by a customer. Then we used this polarity along with other attributes like Age, Gender, Locality, Income, and the list of other products already purchased by our existing customers as input for our recommendation model. Then we matched the polarity score along with the user's profiles and generated the list of insurance products to be recommended in descending order. Despite our model's simplicity and the lack of the key data sets, the results seemed very logical and realistic. So, by developing the model with more enhanced methods and with access to better and true data gathered from an insurance industry may be the sector could be very well benefitted from the amalgamation of sentiment analysis with a recommendation.

</p>
</details>

<details><summary><b>A Hybrid Learning Approach to Detecting Regime Switches in Financial Markets</b>
<a href="https://arxiv.org/abs/2108.05801">arxiv:2108.05801</a>
&#x1F4C8; 2 <br>
<p>Peter Akioyamen, Yi Zhou Tang, Hussien Hussien</p></summary>
<p>

**Abstract:** Financial markets are of much interest to researchers due to their dynamic and stochastic nature. With their relations to world populations, global economies and asset valuations, understanding, identifying and forecasting trends and regimes are highly important. Attempts have been made to forecast market trends by employing machine learning methodologies, while statistical techniques have been the primary methods used in developing market regime switching models used for trading and hedging. In this paper we present a novel framework for the detection of regime switches within the US financial markets. Principal component analysis is applied for dimensionality reduction and the k-means algorithm is used as a clustering technique. Using a combination of cluster analysis and classification, we identify regimes in financial markets based on publicly available economic data. We display the efficacy of the framework by constructing and assessing the performance of two trading strategies based on detected regimes.

</p>
</details>

<details><summary><b>Fairness in Algorithmic Profiling: A German Case Study</b>
<a href="https://arxiv.org/abs/2108.04134">arxiv:2108.04134</a>
&#x1F4C8; 2 <br>
<p>Christoph Kern, Ruben L. Bach, Hannah Mautner, Frauke Kreuter</p></summary>
<p>

**Abstract:** Algorithmic profiling is increasingly used in the public sector as a means to allocate limited public resources effectively and objectively. One example is the prediction-based statistical profiling of job seekers to guide the allocation of support measures by public employment services. However, empirical evaluations of potential side-effects such as unintended discrimination and fairness concerns are rare. In this study, we compare and evaluate statistical models for predicting job seekers' risk of becoming long-term unemployed with respect to prediction performance, fairness metrics, and vulnerabilities to data analysis decisions. Focusing on Germany as a use case, we evaluate profiling models under realistic conditions by utilizing administrative data on job seekers' employment histories that are routinely collected by German public employment services. Besides showing that these data can be used to predict long-term unemployment with competitive levels of accuracy, we highlight that different classification policies have very different fairness implications. We therefore call for rigorous auditing processes before such models are put to practice.

</p>
</details>

<details><summary><b>Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption of Battery Electric Vehicles</b>
<a href="https://arxiv.org/abs/2108.04036">arxiv:2108.04036</a>
&#x1F4C8; 2 <br>
<p>Mingming Liu</p></summary>
<p>

**Abstract:** Recently, there has been an increasing interest in the roll-out of electric vehicles (EVs) in the global automotive market. Compared to conventional internal combustion engine vehicles (ICEVs), EVs can not only help users reduce monetary costs in their daily commuting, but also can effectively help mitigate the increasing level of traffic emissions produced in cities. Among many others, battery electric vehicles (BEVs) exclusively use chemical energy stored in their battery packs for propulsion. Hence, it becomes important to understand how much energy can be consumed by such vehicles in various traffic scenarios towards effective energy management. To address this challenge, we propose a novel framework in this paper by leveraging the federated learning approaches for modelling energy consumption for BEVs (Fed-BEV). More specifically, a group of BEVs involved in the Fed-BEV framework can learn from each other to jointly enhance their energy consumption model. We present the design of the proposed system architecture and implementation details in a co-simulation environment. Finally, comparative studies and simulation results are discussed to illustrate the efficacy of our proposed framework for accurate energy modelling of BEVs.

</p>
</details>

<details><summary><b>Mixture of Linear Models Co-supervised by Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2108.04035">arxiv:2108.04035</a>
&#x1F4C8; 2 <br>
<p>Beomseok Seo, Lin Lin, Jia Li</p></summary>
<p>

**Abstract:** Deep neural network (DNN) models have achieved phenomenal success for applications in many domains, ranging from academic research in science and engineering to industry and business. The modeling power of DNN is believed to have come from the complexity and over-parameterization of the model, which on the other hand has been criticized for the lack of interpretation. Although certainly not true for every application, in some applications, especially in economics, social science, healthcare industry, and administrative decision making, scientists or practitioners are resistant to use predictions made by a black-box system for multiple reasons. One reason is that a major purpose of a study can be to make discoveries based upon the prediction function, e.g., to reveal the relationships between measurements. Another reason can be that the training dataset is not large enough to make researchers feel completely sure about a purely data-driven result. Being able to examine and interpret the prediction function will enable researchers to connect the result with existing knowledge or gain insights about new directions to explore. Although classic statistical models are much more explainable, their accuracy often falls considerably below DNN. In this paper, we propose an approach to fill the gap between relatively simple explainable models and DNN such that we can more flexibly tune the trade-off between interpretability and accuracy. Our main idea is a mixture of discriminative models that is trained with the guidance from a DNN. Although mixtures of discriminative models have been studied before, our way of generating the mixture is quite different.

</p>
</details>

<details><summary><b>Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum</b>
<a href="https://arxiv.org/abs/2108.04033">arxiv:2108.04033</a>
&#x1F4C8; 2 <br>
<p>Daniel Rosendo, Alexandru Costan, Gabriel Antoniu, Matthieu Simonin, Jean-Christophe Lombardo, Alexis Joly, Patrick Valduriez</p></summary>
<p>

**Abstract:** In more and more application areas, we are witnessing the emergence of complex workflows that combine computing, analytics and learning. They often require a hybrid execution infrastructure with IoT devices interconnected to cloud/HPC systems (aka Computing Continuum). Such workflows are subject to complex constraints and requirements in terms of performance, resource usage, energy consumption and financial costs. This makes it challenging to optimize their configuration and deployment. We propose a methodology to support the optimization of real-life applications on the Edge-to-Cloud Continuum. We implement it as an extension of E2Clab, a previously proposed framework supporting the complete experimental cycle across the Edge-to-Cloud Continuum. Our approach relies on a rigorous analysis of possible configurations in a controlled testbed environment to understand their behaviour and related performance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a world-wide plant identification application. Our methodology can be generalized to other applications in the Edge-to-Cloud Continuum.

</p>
</details>

<details><summary><b>RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation</b>
<a href="https://arxiv.org/abs/2108.03132">arxiv:2108.03132</a>
&#x1F4C8; 2 <br>
<p>Qiang Zheng, Dongxiao Zhang</p></summary>
<p>

**Abstract:** Random reconstruction of three-dimensional (3D) digital rocks from two-dimensional (2D) slices is crucial for elucidating the microstructure of rocks and its effects on pore-scale flow in terms of numerical modeling, since massive samples are usually required to handle intrinsic uncertainties. Despite remarkable advances achieved by traditional process-based methods, statistical approaches and recently famous deep learning-based models, few works have focused on producing several kinds of rocks with one trained model and allowing the reconstructed samples to satisfy certain given properties, such as porosity. To fill this gap, we propose a new framework, named RockGPT, which is composed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a single 2D slice from the perspective of video generation. The VQ-VAE is utilized to compress high-dimensional input video, i.e., the sequence of continuous rock slices, to discrete latent codes and reconstruct them. In order to obtain diverse reconstructions, the discrete latent codes are modeled using conditional GPT in an autoregressive manner, while incorporating conditional information from a given slice, rock type, and porosity. We conduct two experiments on five kinds of rocks, and the results demonstrate that RockGPT can produce different kinds of rocks with the same model, and the reconstructed samples can successfully meet certain specified porosities. In a broader sense, through leveraging the proposed conditioning scheme, RockGPT constitutes an effective way to build a general model to produce multiple kinds of rocks simultaneously that also satisfy user-defined properties.

</p>
</details>

<details><summary><b>Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features</b>
<a href="https://arxiv.org/abs/2108.02850">arxiv:2108.02850</a>
&#x1F4C8; 2 <br>
<p>Rupam Ojha, C Chandra Sekhar</p></summary>
<p>

**Abstract:** Automatic speech recognition is a difficult problem in pattern recognition because several sources of variability exist in the speech input like the channel variations, the input might be clean or noisy, the speakers may have different accent and variations in the gender, etc. As a result, domain adaptation is important in speech recognition where we train the model for a particular source domain and test it on a different target domain. In this paper, we propose a technique to perform unsupervised gender-based domain adaptation in speech recognition using phonetic features. The experiments are performed on the TIMIT dataset and there is a considerable decrease in the phoneme error rate using the proposed approach.

</p>
</details>

<details><summary><b>Exploring Structure Consistency for Deep Model Watermarking</b>
<a href="https://arxiv.org/abs/2108.02360">arxiv:2108.02360</a>
&#x1F4C8; 2 <br>
<p>Jie Zhang, Dongdong Chen, Jing Liao, Han Fang, Zehua Ma, Weiming Zhang, Gang Hua, Nenghai Yu</p></summary>
<p>

**Abstract:** The intellectual property (IP) of Deep neural networks (DNNs) can be easily ``stolen'' by surrogate model attack. There has been significant progress in solutions to protect the IP of DNN models in classification tasks. However, little attention has been devoted to the protection of DNNs in image processing tasks. By utilizing consistent invisible spatial watermarks, one recent work first considered model watermarking for deep image processing networks and demonstrated its efficacy in many downstream tasks. Nevertheless, it highly depends on the hypothesis that the embedded watermarks in the network outputs are consistent. When the attacker uses some common data augmentation attacks (e.g., rotate, crop, and resize) during surrogate model training, it will totally fail because the underlying watermark consistency is destroyed. To mitigate this issue, we propose a new watermarking methodology, namely ``structure consistency'', based on which a new deep structure-aligned model watermarking algorithm is designed. Specifically, the embedded watermarks are designed to be aligned with physically consistent image structures, such as edges or semantic regions. Experiments demonstrate that our method is much more robust than the baseline method in resisting data augmentation attacks for model IP protection. Besides that, we further test the generalization ability and robustness of our method to a broader range of circumvention attacks.

</p>
</details>

<details><summary><b>BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration</b>
<a href="https://arxiv.org/abs/2108.02313">arxiv:2108.02313</a>
&#x1F4C8; 2 <br>
<p>Caleb Terrill, Fred Chu</p></summary>
<p>

**Abstract:** Modern hardware design trends have shifted towards specialized hardware acceleration for computationally intensive tasks like machine learning and computer vision. While these complex workloads can be accelerated by commercial GPUs, domain-specific hardware is far more optimal when needing to meet the stringent memory, throughput, and power constraints of mobile and embedded devices. This paper proposes and evaluates a Binary-Enabled Architecture for Neural Network Acceleration (BEANNA), a neural network hardware accelerator capable of processing both floating point and binary network layers. Through the use of a novel 16x16 systolic array based matrix multiplier with processing elements that compute both floating point and binary multiply-adds, BEANNA seamlessly switches between high precision floating point and binary neural network layers. Running at a clock speed of 100MHz, BEANNA achieves a peak throughput of 52.8 GigaOps/second when operating in high precision mode, and 820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was performed by comparing a hybrid network with floating point outer layers and binary hidden layers to a network with only floating point layers. The hybrid network accelerated using BEANNA achieved a 194% throughput increase, a 68% memory usage decrease, and a 66% energy consumption decrease per inference, all this at the cost of a mere 0.23% classification accuracy decrease on the MNIST dataset.

</p>
</details>

<details><summary><b>Regret Analysis of Learning-Based MPC with Partially-Unknown Cost Function</b>
<a href="https://arxiv.org/abs/2108.02307">arxiv:2108.02307</a>
&#x1F4C8; 2 <br>
<p>Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</p></summary>
<p>

**Abstract:** The exploration/exploitation trade-off is an inherent challenge in data-driven and adaptive control. Though this trade-off has been studied for multi-armed bandits, reinforcement learning (RL) for finite Markov chains, and RL for linear control systems; it is less well-studied for learning-based control of nonlinear control systems. A significant theoretical challenge in the nonlinear setting is that, unlike the linear case, there is no explicit characterization of an optimal controller for a given set of cost and system parameters. We propose in this paper the use of a finite-horizon oracle controller with perfect knowledge of all system parameters as a reference for optimal control actions. First, this allows us to propose a new regret notion with respect to this oracle finite-horizon controller. Second, this allows us to develop learning-based policies that we prove achieve low regret (i.e., square-root regret up to a log-squared factor) with respect to this oracle finite-horizon controller. This policy is developed in the context of learning-based model predictive control (LBMPC). We conduct a statistical analysis to prove finite sample concentration bounds for the estimation step of our policy, and then we perform a control-theoretic analysis using techniques from MPC- and optimization-theory to show this policy ensures closed-loop stability and achieves low regret. We conclude with numerical experiments on a model of heating, ventilation, and air-conditioning (HVAC) systems that show the low regret of our policy in a setting where the cost function is partially-unknown to the controller.

</p>
</details>

<details><summary><b>Machine Learning Classification Methods and Portfolio Allocation: An Examination of Market Efficiency</b>
<a href="https://arxiv.org/abs/2108.02283">arxiv:2108.02283</a>
&#x1F4C8; 2 <br>
<p>Yang Bai, Kuntara Pukthuanthong</p></summary>
<p>

**Abstract:** We design a novel framework to examine market efficiency through out-of-sample (OOS) predictability. We frame the asset pricing problem as a machine learning classification problem and construct classification models to predict return states. The prediction-based portfolios beat the market with significant OOS economic gains. We measure prediction accuracies directly. For each model, we introduce a novel application of binomial test to test the accuracy of 3.34 million return state predictions. The tests show that our models can extract useful contents from historical information to predict future return states. We provide unique economic insights about OOS predictability and machine learning models.

</p>
</details>

<details><summary><b>Curriculum learning for language modeling</b>
<a href="https://arxiv.org/abs/2108.02170">arxiv:2108.02170</a>
&#x1F4C8; 2 <br>
<p>Daniel Campos</p></summary>
<p>

**Abstract:** Language Models like ELMo and BERT have provided robust representations of natural language, which serve as the language understanding component for a diverse range of downstream tasks.Curriculum learning is a method that employs a structured training regime instead, which has been leveraged in computer vision and machine translation to improve model training speed and model performance. While language models have proven transformational for the natural language processing community, these models have proven expensive, energy-intensive, and challenging to train. In this work, we explore the effect of curriculum learning on language model pretraining using various linguistically motivated curricula and evaluate transfer performance on the GLUE Benchmark. Despite a broad variety of training methodologies and experiments we do not find compelling evidence that curriculum learning methods improve language model training.

</p>
</details>

<details><summary><b>Auto-encoder based Model for High-dimensional Imbalanced Industrial Data</b>
<a href="https://arxiv.org/abs/2108.02083">arxiv:2108.02083</a>
&#x1F4C8; 2 <br>
<p>Chao Zhang, Sthitie Bom</p></summary>
<p>

**Abstract:** With the proliferation of IoT devices, the distributed control systems are now capturing and processing more sensors at higher frequency than ever before. These new data, due to their volume and novelty, cannot be effectively consumed without the help of data-driven techniques. Deep learning is emerging as a promising technique to analyze these data, particularly in soft sensor modeling. The strong representational capabilities of complex data and the flexibility it offers from an architectural perspective make it a topic of active applied research in industrial settings. However, the successful applications of deep learning in soft sensing are still not widely integrated in factory control systems, because most of the research on soft sensing do not have access to large scale industrial data which are varied, noisy and incomplete. The results published in most research papers are therefore not easily reproduced when applied to the variety of data in industrial settings. Here we provide manufacturing data sets that are much larger and more complex than public open soft sensor data. Moreover, the data sets are from Seagate factories on active service with only necessary anonymization, so that they reflect the complex and noisy nature of real-world data. We introduce a variance weighted multi-headed auto-encoder classification model that fits well into the high-dimensional and highly imbalanced data. Besides the use of weighting or sampling methods to handle the highly imbalanced data, the model also simultaneously predicts multiple outputs by exploiting output-supervised representation learning and multi-task weighting.

</p>
</details>

<details><summary><b>The Potential of Using Vision Videos for CrowdRE: Video Comments as a Source of Feedback</b>
<a href="https://arxiv.org/abs/2108.02076">arxiv:2108.02076</a>
&#x1F4C8; 2 <br>
<p>Oliver Karras, Eklekta Kristo, Jil Klünder</p></summary>
<p>

**Abstract:** Vision videos are established for soliciting feedback and stimulating discussions in requirements engineering (RE) practices, such as focus groups. Different researchers motivated the transfer of these benefits into crowd-based RE (CrowdRE) by using vision videos on social media platforms. So far, however, little research explored the potential of using vision videos for CrowdRE in detail. In this paper, we analyze and assess this potential, in particular, focusing on video comments as a source of feedback. In a case study, we analyzed 4505 comments on a vision video from YouTube. We found that the video solicited 2770 comments from 2660 viewers in four days. This is more than 50% of all comments the video received in four years. Even though only a certain fraction of these comments are relevant to RE, the relevant comments address typical intentions and topics of user feedback, such as feature request or problem report. Besides the typical user feedback categories, we found more than 300 comments that address the topic safety, which has not appeared in previous analyses of user feedback. In an automated analysis, we compared the performance of three machine learning algorithms on classifying the video comments. Despite certain differences, the algorithms classified the video comments well. Based on these findings, we conclude that the use of vision videos for CrowdRE has a large potential. Despite the preliminary nature of the case study, we are optimistic that vision videos can motivate stakeholders to actively participate in a crowd and solicit numerous of video comments as a valuable source of feedback.

</p>
</details>

<details><summary><b>MRCpy: A Library for Minimax Risk Classifiers</b>
<a href="https://arxiv.org/abs/2108.01952">arxiv:2108.01952</a>
&#x1F4C8; 2 <br>
<p>Kartheek Bondugula, Santiago Mazuelas, Aritz Pérez</p></summary>
<p>

**Abstract:** Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.

</p>
</details>

<details><summary><b>Automatic hemisphere segmentation in rodent MRI with lesions</b>
<a href="https://arxiv.org/abs/2108.01941">arxiv:2108.01941</a>
&#x1F4C8; 2 <br>
<p>Juan Miguel Valverde, Artem Shatillo, Riccardo de Feo, Jussi Tohka</p></summary>
<p>

**Abstract:** We present MedicDeepLabv3+, a convolutional neural network that is the first completely automatic method to segment brain hemispheres in magnetic resonance (MR) images of rodents with lesions. MedicDeepLabv3+ improves the state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial attention layers and additional skip connections that, as we show in our experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR image preprocessing, such as bias-field correction or registration to a template, produces segmentations in less than a second, and its GPU memory requirements can be adjusted based on the available resources. Using a large dataset of 723 MR rat brain images, we evaluated our MedicDeepLabv3+, two state-of-the-art convolutional neural networks (DeepLabv3+, UNet) and three approaches that were specifically designed for skull-stripping rodent MR images (Demon, RATS and RBET). In our experiments, MedicDeepLabv3+ outperformed the other methods, yielding an average Dice coefficient of 0.952 and 0.944 in the brain and contralateral hemisphere regions. Additionally, we show that despite limiting the GPU memory and the training data to only three images, our MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our method, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus, yielded excellent results in multiple scenarios, demonstrating its capability to reduce human workload in rodent neuroimaging studies.

</p>
</details>

<details><summary><b>Secure and Privacy-Preserving Federated Learning via Co-Utility</b>
<a href="https://arxiv.org/abs/2108.01913">arxiv:2108.01913</a>
&#x1F4C8; 2 <br>
<p>Josep Domingo-Ferrer, Alberto Blanco-Justicia, Jesús Manjón, David Sánchez</p></summary>
<p>

**Abstract:** The decentralized nature of federated learning, that often leverages the power of edge devices, makes it vulnerable to attacks against privacy and security. The privacy risk for a peer is that the model update she computes on her private data may, when sent to the model manager, leak information on those private data. Even more obvious are security attacks, whereby one or several malicious peers return wrong model updates in order to disrupt the learning process and lead to a wrong model being learned. In this paper we build a federated learning framework that offers privacy to the participating peers as well as security against Byzantine and poisoning attacks. Our framework consists of several protocols that provide strong privacy to the participating peers via unlinkable anonymity and that are rationally sustainable based on the co-utility property. In other words, no rational party is interested in deviating from the proposed protocols. We leverage the notion of co-utility to build a decentralized co-utile reputation management system that provides incentives for parties to adhere to the protocols. Unlike privacy protection via differential privacy, our approach preserves the values of model updates and hence the accuracy of plain federated learning; unlike privacy protection via update aggregation, our approach preserves the ability to detect bad model updates while substantially reducing the computational overhead compared to methods based on homomorphic encryption.

</p>
</details>

<details><summary><b>Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application</b>
<a href="https://arxiv.org/abs/2108.01903">arxiv:2108.01903</a>
&#x1F4C8; 2 <br>
<p>Joo Hun Yoo, Ha Min Son, Hyejun Jeong, Eun-Hye Jang, Ah Young Kim, Han Young Yu, Hong Jin Jeon, Tai-Myoung Chung</p></summary>
<p>

**Abstract:** While machine learning techniques are being applied to various fields for their exceptional ability to find complex relations in large datasets, the strengthening of regulations on data ownership and privacy is causing increasing difficulty in its application to medical data. In light of this, Federated Learning has recently been proposed as a solution to train on private data without breach of confidentiality. This conservation of privacy is particularly appealing in the field of healthcare, where patient data is highly confidential. However, many studies have shown that its assumption of Independent and Identically Distributed data is unrealistic for medical data. In this paper, we propose Personalized Federated Cluster Models, a hierarchical clustering-based FL process, to predict Major Depressive Disorder severity from Heart Rate Variability. By allowing clients to receive more personalized model, we address problems caused by non-IID data, showing an accuracy increase in severity prediction. This increase in performance may be sufficient to use Personalized Federated Cluster Models in many existing Federated Learning scenarios.

</p>
</details>

<details><summary><b>Adaptive Path Planning for UAV-based Multi-Resolution Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2108.01884">arxiv:2108.01884</a>
&#x1F4C8; 2 <br>
<p>Felix Stache, Jonas Westheider, Federico Magistri, Marija Popović, Cyrill Stachniss</p></summary>
<p>

**Abstract:** In this paper, we address the problem of adaptive path planning for accurate semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The usage of UAVs for terrain monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. However, a key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. To address this, we propose an online planning algorithm which adapts the UAV paths to obtain high-resolution semantic segmentations necessary in areas on the terrain with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on the application of crop/weed segmentation in precision agriculture using real-world field data.

</p>
</details>

<details><summary><b>Learning Task Agnostic Skills with Data-driven Guidance</b>
<a href="https://arxiv.org/abs/2108.01869">arxiv:2108.01869</a>
&#x1F4C8; 2 <br>
<p>Even Klemsdal, Sverre Herland, Abdulmajid Murad</p></summary>
<p>

**Abstract:** To increase autonomy in reinforcement learning, agents need to learn useful behaviours without reliance on manually designed reward functions. To that end, skill discovery methods have been used to learn the intrinsic options available to an agent using task-agnostic objectives. However, without the guidance of task-specific rewards, emergent behaviours are generally useless due to the under-constrained problem of skill discovery in complex and high-dimensional spaces. This paper proposes a framework for guiding the skill discovery towards the subset of expert-visited states using a learned state projection. We apply our method in various reinforcement learning (RL) tasks and show that such a projection results in more useful behaviours.

</p>
</details>

<details><summary><b>Black-Box and Modular Meta-Learning for Power Control via Random Edge Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2108.13178">arxiv:2108.13178</a>
&#x1F4C8; 1 <br>
<p>Ivana Nikoloska, Osvaldo Simeone</p></summary>
<p>

**Abstract:** In this paper, we consider the problem of power control for a wireless network with an arbitrarily time-varying topology, including the possible addition or removal of nodes. A data-driven design methodology that leverages graph neural networks (GNNs) is adopted in order to efficiently parametrize the power control policy mapping the channel state information (CSI) to transmit powers. The specific GNN architecture, known as random edge GNN (REGNN), defines a non-linear graph convolutional filter whose spatial weights are tied to the channel coefficients. While prior work assumed a joint training approach whereby the REGNN-based policy is shared across all topologies, this paper targets adaptation of the power control policy based on limited CSI data regarding the current topology. To this end, we propose both black-box and modular meta-learning techniques. Black-box meta-learning optimizes a general-purpose adaptation procedure via (stochastic) gradient descent, while modular meta-learning finds a set of reusable modules that can form components of a solution for any new network topology. Numerical results validate the benefits of meta-learning for power control problems over joint training schemes, and demonstrate the advantages of modular meta-learning when data availability is extremely limited.

</p>
</details>

<details><summary><b>Combating Informational Denial-of-Service (IDoS) Attacks: Modeling and Mitigation of Attentional Human Vulnerability</b>
<a href="https://arxiv.org/abs/2108.08255">arxiv:2108.08255</a>
&#x1F4C8; 1 <br>
<p>Linan Huang, Quanyan Zhu</p></summary>
<p>

**Abstract:** This work proposes a new class of proactive attacks called the Informational Denial-of-Service (IDoS) attacks that exploit the attentional human vulnerability. By generating a large volume of feints, IDoS attacks deplete the cognitive resources of human operators to prevent humans from identifying the real attacks hidden among feints. This work aims to formally define IDoS attacks, quantify their consequences, and develop human-assistive security technologies to mitigate the severity level and risks of IDoS attacks. To this end, we use the semi-Markov process to model the sequential arrivals of feints and real attacks with category labels attached in the associated alerts. The assistive technology strategically manages human attention by highlighting selective alerts periodically to prevent the distraction of other alerts. A data-driven approach is applied to evaluate human performance under different Attention Management (AM) strategies. Under a representative special case, we establish the computational equivalency between two dynamic programming representations to reduce the computation complexity and enable online learning with samples of reduced size and zero delays. A case study corroborates the effectiveness of the learning framework. The numerical results illustrate how AM strategies can alleviate the severity level and the risk of IDoS attacks. Furthermore, the results show that the minimum risk is achieved with a proper level of intentional inattention to alerts, which we refer to as the law of rational risk-reduction inattention.

</p>
</details>

<details><summary><b>A data-driven peridynamic continuum model for upscaling molecular dynamics</b>
<a href="https://arxiv.org/abs/2108.04883">arxiv:2108.04883</a>
&#x1F4C8; 1 <br>
<p>Huaiqian You, Yue Yu, Stewart Silling, Marta D'Elia</p></summary>
<p>

**Abstract:** Nonlocal models, including peridynamics, often use integral operators that embed lengthscales in their definition. However, the integrands in these operators are difficult to define from the data that are typically available for a given physical system, such as laboratory mechanical property tests. In contrast, molecular dynamics (MD) does not require these integrands, but it suffers from computational limitations in the length and time scales it can address. To combine the strengths of both methods and to obtain a coarse-grained, homogenized continuum model that efficiently and accurately captures materials' behavior, we propose a learning framework to extract, from MD data, an optimal Linear Peridynamic Solid (LPS) model as a surrogate for MD displacements. To maximize the accuracy of the learnt model we allow the peridynamic influence function to be partially negative, while preserving the well-posedness of the resulting model. To achieve this, we provide sufficient well-posedness conditions for discretized LPS models with sign-changing influence functions and develop a constrained optimization algorithm that minimizes the equation residual while enforcing such solvability conditions. This framework guarantees that the resulting model is mathematically well-posed, physically consistent, and that it generalizes well to settings that are different from the ones used during training. We illustrate the efficacy of the proposed approach with several numerical tests for single layer graphene. Our two-dimensional tests show the robustness of the proposed algorithm on validation data sets that include thermal noise, different domain shapes and external loadings, and discretizations substantially different from the ones used for training.

</p>
</details>

<details><summary><b>Machine learning for modeling the progression of Alzheimer disease dementia using clinical data: a systematic literature review</b>
<a href="https://arxiv.org/abs/2108.04174">arxiv:2108.04174</a>
&#x1F4C8; 1 <br>
<p>Sayantan Kumar, Inez Oh, Suzanne Schindler, Albert M Lai, Philip R O Payne, Aditi Gupta</p></summary>
<p>

**Abstract:** Objective Alzheimer disease (AD) is the most common cause of dementia, a syndrome characterized by cognitive impairment severe enough to interfere with activities of daily life. We aimed to conduct a systematic literature review (SLR) of studies that applied machine learning (ML) methods to clinical data derived from electronic health records in order to model risk for progression of AD dementia.
  Materials and Methods: We searched for articles published between January 1, 2010, and May 31, 2020, in PubMed, Scopus, ScienceDirect, IEEE Explore Digital Library, Association for Computing Machinery Digital Library, and arXiv. We used predefined criteria to select relevant articles and summarized them according to key components of ML analysis such as data characteristics, computational algorithms, and research focus.
  Results: There has been a considerable rise over the past 5 years in the number of research papers using ML-based analysis for AD dementia modeling. We reviewed 64 relevant articles in our SLR. The results suggest that majority of existing research has focused on predicting progression of AD dementia using publicly available datasets containing both neuroimaging and clinical data (neurobehavioral status exam scores, patient demographics, neuroimaging data, and laboratory test values).
  Discussion: Identifying individuals at risk for progression of AD dementia could potentially help to personalize disease management to plan future care. Clinical data consisting of both structured data tables and clinical notes can be effectively used in ML-based approaches to model risk for AD dementia progression. Data sharing and reproducibility of results can enhance the impact, adaptation, and generalizability of this research.

</p>
</details>

<details><summary><b>Incremental learning of LSTM framework for sensor fusion in attitude estimation</b>
<a href="https://arxiv.org/abs/2108.03173">arxiv:2108.03173</a>
&#x1F4C8; 1 <br>
<p>Parag Narkhede, Rahee Walambe, Shashi Poddar, Ketan Kotecha</p></summary>
<p>

**Abstract:** This paper presents a novel method for attitude estimation of an object in 3D space by incremental learning of the Long-Short Term Memory (LSTM) network. Gyroscope, accelerometer, and magnetometer are few widely used sensors in attitude estimation applications. Traditionally, multi-sensor fusion methods such as the Extended Kalman Filter and Complementary Filter are employed to fuse the measurements from these sensors. However, these methods exhibit limitations in accounting for the uncertainty, unpredictability, and dynamic nature of the motion in real-world situations. In this paper, the inertial sensors data are fed to the LSTM network which are then updated incrementally to incorporate the dynamic changes in motion occurring in the run time. The robustness and efficiency of the proposed framework is demonstrated on the dataset collected from a commercially available inertial measurement unit. The proposed framework offers a significant improvement in the results compared to the traditional method, even in the case of a highly dynamic environment. The LSTM framework-based attitude estimation approach can be deployed on a standard AI-supported processing module for real-time applications.

</p>
</details>

<details><summary><b>Predicting Post-Concussion Syndrome Outcomes with Machine Learning</b>
<a href="https://arxiv.org/abs/2108.02570">arxiv:2108.02570</a>
&#x1F4C8; 1 <br>
<p>Minhong Kim</p></summary>
<p>

**Abstract:** In this paper, machine learning models are used to predict outcomes for patients with persistent post-concussion syndrome (PCS). Patients had sustained a concussion at an average of two to three months before the study. By utilizing assessed data, the machine learning models aimed to predict whether or not a patient would continue to have PCS after four to five months. The random forest classifier achieved the highest performance with an 85% accuracy and an area under the receiver operating characteristic curve (AUC) of 0.94. Factors found to be predictive of PCS outcome were Post-Traumatic Stress Disorder (PTSD), perceived injustice, self-rated prognosis, and symptom severity post-injury. The results of this study demonstrate that machine learning models can predict PCS outcomes with high accuracy. With further research, machine learning models may be implemented in healthcare settings to help patients with persistent PCS.

</p>
</details>

<details><summary><b>VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models</b>
<a href="https://arxiv.org/abs/2108.02550">arxiv:2108.02550</a>
&#x1F4C8; 1 <br>
<p>Furui Cheng, Dongyu Liu, Fan Du, Yanna Lin, Alexandra Zytek, Haomin Li, Huamin Qu, Kalyan Veeramachaneni</p></summary>
<p>

**Abstract:** Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.

</p>
</details>

<details><summary><b>Advances in Trajectory Optimization for Space Vehicle Control</b>
<a href="https://arxiv.org/abs/2108.02335">arxiv:2108.02335</a>
&#x1F4C8; 1 <br>
<p>Danylo Malyuta, Yue Yu, Purnanand Elango, Behcet Acikmese</p></summary>
<p>

**Abstract:** Space mission design places a premium on cost and operational efficiency. The search for new science and life beyond Earth calls for spacecraft that can deliver scientific payloads to geologically rich yet hazardous landing sites. At the same time, the last four decades of optimization research have put a suite of powerful optimization tools at the fingertips of the controls engineer. As we enter the new decade, optimization theory, algorithms, and software tooling have reached a critical mass to start seeing serious application in space vehicle guidance and control systems. This survey paper provides a detailed overview of recent advances, successes, and promising directions for optimization-based space vehicle control. The considered applications include planetary landing, rendezvous and proximity operations, small body landing, constrained attitude reorientation, endo-atmospheric flight including ascent and reentry, and orbit transfer and injection. The primary focus is on the last ten years of progress, which have seen a veritable rise in the number of applications using three core technologies: lossless convexification, sequential convex programming, and model predictive control. The reader will come away with a well-rounded understanding of the state-of-the-art in each space vehicle control application, and will be well positioned to tackle important current open problems using convex optimization as a core technology.

</p>
</details>

<details><summary><b>Indoor Localization Under Limited Measurements: A Cross-Environment Joint Semi-Supervised and Transfer Learning Approach</b>
<a href="https://arxiv.org/abs/2108.02257">arxiv:2108.02257</a>
&#x1F4C8; 1 <br>
<p>Mohamed I. AlHajri, Raed M. Shubair, Marwa Chafii</p></summary>
<p>

**Abstract:** The development of highly accurate deep learning methods for indoor localization is often hindered by the unavailability of sufficient data measurements in the desired environment to perform model training. To overcome the challenge of collecting costly measurements, this paper proposes a cross-environment approach that compensates for insufficient labelled measurements via a joint semi-supervised and transfer learning technique to transfer, in an appropriate manner, the model obtained from a rich-data environment to the desired environment for which data is limited. This is achieved via a sequence of operations that exploit the similarity across environments to enhance unlabelled data model training of the desired environment. Numerical experiments demonstrate that the proposed cross-environment approach outperforms the conventional method, convolutional neural network (CNN), with a significant increase in localization accuracy, up to 43%. Moreover, with only 40% data measurements, the proposed cross-environment approach compensates for data inadequacy and replicates the localization accuracy of the conventional method, CNN, which uses 75% data measurements.

</p>
</details>

<details><summary><b>Growing an architecture for a neural network</b>
<a href="https://arxiv.org/abs/2108.02231">arxiv:2108.02231</a>
&#x1F4C8; 1 <br>
<p>Sergey Khashin, Ekaterina Shemyakova</p></summary>
<p>

**Abstract:** We propose a new kind of automatic architecture search algorithm. The algorithm alternates pruning connections and adding neurons, and it is not restricted to layered architectures only. Here architecture is an arbitrary oriented graph with some weights (along with some biases and an activation function), so there may be no layered structure in such a network. The algorithm minimizes the complexity of staying within a given error. We demonstrate our algorithm on the brightness prediction problem of the next point through the previous points on an image. Our second test problem is the approximation of the bivariate function defining the brightness of a black and white image. Our optimized networks significantly outperform the standard solution for neural network architectures in both cases.

</p>
</details>

<details><summary><b>Random Convolution Kernels with Multi-Scale Decomposition for Preterm EEG Inter-burst Detection</b>
<a href="https://arxiv.org/abs/2108.02039">arxiv:2108.02039</a>
&#x1F4C8; 1 <br>
<p>Christopher Lundy, John M. O'Toole</p></summary>
<p>

**Abstract:** Linear classifiers with random convolution kernels are computationally efficient methods that need no design or domain knowledge. Unlike deep neural networks, there is no need to hand-craft a network architecture; the kernels are randomly generated and only the linear classifier needs training. A recently proposed method, RandOm Convolutional KErnel Transforms (ROCKETs), has shown high accuracy across a range of time-series data sets. Here we propose a multi-scale version of this method, using both high- and low-frequency components. We apply our methods to inter-burst detection in a cohort of preterm EEG recorded from 36 neonates <30 weeks gestational age. Two features from the convolution of 10,000 random kernels are combined using ridge regression. The proposed multi-scale ROCKET method out-performs the method without scale: median (interquartile range, IQR) Matthews correlation coefficient (MCC) of 0.859 (0.815 to 0.874) for multi-scale versus 0.841 (0.807 to 0.865) without scale, p<0.001. The proposed method lags behind an existing feature-based machine learning method developed with deep domain knowledge, but is fast to train and can quickly set an initial baseline threshold of performance for generic and biomedical time-series classification.

</p>
</details>

<details><summary><b>Lung Sound Classification Using Co-tuning and Stochastic Normalization</b>
<a href="https://arxiv.org/abs/2108.01991">arxiv:2108.01991</a>
&#x1F4C8; 1 <br>
<p>Truc Nguyen, Franz Pernkopf</p></summary>
<p>

**Abstract:** In this paper, we use pre-trained ResNet models as backbone architectures for classification of adventitious lung sounds and respiratory diseases. The knowledge of the pre-trained model is transferred by using vanilla fine-tuning, co-tuning, stochastic normalization and the combination of the co-tuning and stochastic normalization techniques. Furthermore, data augmentation in both time domain and time-frequency domain is used to account for the class imbalance of the ICBHI and our multi-channel lung sound dataset. Additionally, we apply spectrum correction to consider the variations of the recording device properties on the ICBHI dataset. Empirically, our proposed systems mostly outperform all state-of-the-art lung sound classification systems for the adventitious lung sounds and respiratory diseases of both datasets.

</p>
</details>

<details><summary><b>Core-Stable Committees under Restricted Domains</b>
<a href="https://arxiv.org/abs/2108.01987">arxiv:2108.01987</a>
&#x1F4C8; 1 <br>
<p>Grzegorz Pierczyński, Piotr Skowron</p></summary>
<p>

**Abstract:** We study the setting of committee elections, where a group of individuals needs to collectively select a given size subset of available objects. This model is relevant for a number of real-life scenarios including political elections, participatory budgeting, and facility-location. We focus on the core -- the classic notion of proportionality, stability and fairness. We show that for a number of restricted domains including voter-interval, candidate-interval, single-peaked, and single-crossing preferences the core is non-empty and can be found in polynomial time. We show that the core might be empty for strict top-monotonic preferences, yet we introduce a relaxation of this class, which guarantees non-emptiness of the core. Our algorithms work both in the randomized and discrete models. We also show that the classic known proportional rules do not return committees from the core even for the most restrictive domains among those we consider (in particular for 1D-Euclidean preferences). We additionally prove a number of structural results that give better insights into the nature of some of the restricted domains, and which in particular give a better intuitive understanding of the class of top-monotonic preferences.

</p>
</details>

<details><summary><b>Log-based Anomaly Detection Without Log Parsing</b>
<a href="https://arxiv.org/abs/2108.01955">arxiv:2108.01955</a>
&#x1F4C8; 1 <br>
<p>Van-Hoang Le, Hongyu Zhang</p></summary>
<p>

**Abstract:** Software systems often record important runtime information in system logs for troubleshooting purposes. There have been many studies that use log data to construct machine learning models for detecting system anomalies. Through our empirical study, we find that existing log-based anomaly detection approaches are significantly affected by log parsing errors that are introduced by 1) OOV (out-of-vocabulary) words, and 2) semantic misunderstandings. The log parsing errors could cause the loss of important information for anomaly detection. To address the limitations of existing methods, we propose NeuralLog, a novel log-based anomaly detection approach that does not require log parsing. NeuralLog extracts the semantic meaning of raw log messages and represents them as semantic vectors. These representation vectors are then used to detect anomalies through a Transformer-based classification model, which can capture the contextual information from log sequences. Our experimental results show that the proposed approach can effectively understand the semantic meaning of log messages and achieve accurate anomaly detection results. Overall, NeuralLog achieves F1-scores greater than 0.95 on four public datasets, outperforming the existing approaches.

</p>
</details>

<details><summary><b>SOME/IP Intrusion Detection using Deep Learning-based Sequential Models in Automotive Ethernet Networks</b>
<a href="https://arxiv.org/abs/2108.08262">arxiv:2108.08262</a>
&#x1F4C8; 0 <br>
<p>Natasha Alkhatib, Hadi Ghauch, Jean-Luc Danger</p></summary>
<p>

**Abstract:** Intrusion Detection Systems are widely used to detect cyberattacks, especially on protocols vulnerable to hacking attacks such as SOME/IP. In this paper, we present a deep learning-based sequential model for offline intrusion detection on SOME/IP application layer protocol. To assess our intrusion detection system, we have generated and labeled a dataset with several classes representing realistic intrusions, and a normal class - a significant contribution due to the absence of such publicly available datasets. Furthermore, we also propose a recurrent neural network (RNN), as an instance of deep learning-based sequential model, that we apply to our generated dataset. The numerical results show that RNN excel at predicting in-vehicle intrusions, with F1 Scores and AUC values greater than 0.8 depending on each intrusion type.

</p>
</details>

<details><summary><b>Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation</b>
<a href="https://arxiv.org/abs/2108.03167">arxiv:2108.03167</a>
&#x1F4C8; 0 <br>
<p>Mehmet Yamac, Ugur Akpinar, Erdem Sahin, Serkan Kiranyaz, Moncef Gabbouj</p></summary>
<p>

**Abstract:** In CS literature, the efforts can be divided into two groups: finding a measurement matrix that preserves the compressed information at the maximum level, and finding a reconstruction algorithm for the compressed information. In the traditional CS setup, the measurement matrices are selected as random matrices, and optimization-based iterative solutions are used to recover the signals. However, when we handle large signals, using random matrices become cumbersome especially when it comes to iterative optimization-based solutions. Even though recent deep learning-based solutions boost the reconstruction accuracy performance while speeding up the recovery, still jointly learning the whole measurement matrix is a difficult process. In this work, we introduce a separable multi-linear learning of the CS matrix by representing it as the summation of arbitrary number of tensors. For a special case where the CS operation is set as a single tensor multiplication, the model is reduced to the learning-based separable CS; while a dense CS matrix can be approximated and learned as the summation of multiple tensors. Both cases can be used in CS of two or multi-dimensional signals e.g., images, multi-spectral images, videos, etc. Structural CS matrices can also be easily approximated and learned in our multi-linear separable learning setup with structural tensor sum representation. Hence, our learnable generalized tensor summation CS operation encapsulates most CS setups including separable CS, non-separable CS (traditional vector-matrix multiplication), structural CS, and CS of the multi-dimensional signals. For both gray-scale and RGB images, the proposed scheme surpasses most state-of-the-art solutions, especially in lower measurement rates. Although the performance gain remains limited from tensor to the sum of tensor representation for gray-scale images, it becomes significant in the RGB case.

</p>
</details>

<details><summary><b>An Efficient Dual-reference Training Data Acquisition Method for CNN-Based Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2108.02348">arxiv:2108.02348</a>
&#x1F4C8; 0 <br>
<p>Yanhui Guo, Xiao Shu, Xiaolin Wu</p></summary>
<p>

**Abstract:** For deep learning methods of image super-resolution, the most critical issue is whether the paired low and high resolution images for training accurately reflect the sampling process of real cameras. Low and high resolution (LR$\sim$HR) image pairs synthesized by existing degradation models (e.g. bicubic downsampling) deviate from those in reality; thus the super-resolution CNN trained by these synthesized LR$\sim$HR image pairs does not perform well when being applied to real images. In this paper, we propose a novel method to capture a large set of realistic LR$\sim$HR image pairs using real cameras. The data acquisition is carried out under controllable lab conditions with minimum human intervention and at high throughput (about 500 image pairs per hour). The high level of automation makes it easy to produce a set of real LR$\sim$HR training image pairs for each camera.Our innovation is to shoot images displayed on an ultra-high quality screen at different resolutions. There are three distinctive advantages of our method for image super-resolution. First, as the LR and HR images are taken of a 3D planar surface (the screen) the registration problem fits exactly to a homography model and we can display specially designed markers on the image to improve the registration precision. Second, the displayed digital image file can be exploited as a reference to optimize the high frequency content of the restored image. Third, this high-efficiency data collection method makes it possible to collect a customized dataset for each camera sensor, for which one can train a specific model for the intended camera sensor. Experimental results show that training a super-resolution CNN by our LR$\sim$HR dataset has superior restoration performance than training it by existing datasets on real world images at the inference stage.

</p>
</details>

<details><summary><b>Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\times$ Compression and 2.7$\times$ Faster Inference</b>
<a href="https://arxiv.org/abs/2108.02191">arxiv:2108.02191</a>
&#x1F4C8; 0 <br>
<p>Aditya Desai, Li Chou, Anshumali Shrivastava</p></summary>
<p>

**Abstract:** Deep learning for recommendation data is the one of the most pervasive and challenging AI workload in recent times. State-of-the-art recommendation models are one of the largest models rivalling the likes of GPT-3 and Switch Transformer. Challenges in deep learning recommendation models (DLRM) stem from learning dense embeddings for each of the categorical values. These embedding tables in industrial scale models can be as large as hundreds of terabytes. Such large models lead to a plethora of engineering challenges, not to mention prohibitive communication overheads, and slower training and inference times. Of these, slower inference time directly impacts user experience. Model compression for DLRM is gaining traction and the community has recently shown impressive compression results. In this paper, we present Random Offset Block Embedding Array (ROBE) as a low memory alternative to embedding tables which provide orders of magnitude reduction in memory usage while maintaining accuracy and boosting execution speed. ROBE is a simple fundamental approach in improving both cache performance and the variance of randomized hashing, which could be of independent interest in itself. We demonstrate that we can successfully train DLRM models with same accuracy while using $1000 \times$ less memory. A $1000\times$ compressed model directly results in faster inference without any engineering. In particular, we show that we can train DLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of 0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model of 100GB while achieving about $2.7\times$ (170\%) improvement in inference throughput.

</p>
</details>

<details><summary><b>Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical Image Segmentation with Normalizing Flows</b>
<a href="https://arxiv.org/abs/2108.02155">arxiv:2108.02155</a>
&#x1F4C8; 0 <br>
<p>M. M. A. Valiuddin, C. G. A. Viviers, R. J. G. van Sloun, P. H. N. de With, F. van der Sommen</p></summary>
<p>

**Abstract:** Quantifying uncertainty in medical image segmentation applications is essential, as it is often connected to vital decision-making. Compelling attempts have been made in quantifying the uncertainty in image segmentation architectures, e.g. to learn a density segmentation model conditioned on the input image. Typical work in this field restricts these learnt densities to be strictly Gaussian. In this paper, we propose to use a more flexible approach by introducing Normalizing Flows (NFs), which enables the learnt densities to be more complex and facilitate more accurate modeling for uncertainty. We prove this hypothesis by adopting the Probabilistic U-Net and augmenting the posterior density with an NF, allowing it to be more expressive. Our qualitative as well as quantitative (GED and IoU) evaluations on the multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation datasets, respectively, show a clear improvement. This is mostly apparent in the quantification of aleatoric uncertainty and the increased predictive performance of up to 14 percent. This result strongly indicates that a more flexible density model should be seriously considered in architectures that attempt to capture segmentation ambiguity through density modeling. The benefit of this improved modeling will increase human confidence in annotation and segmentation, and enable eager adoption of the technology in practice.

</p>
</details>


[Next Page](2021/2021-08/2021-08-03.md)
