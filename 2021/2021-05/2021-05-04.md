## Summary for 2021-05-04, created on 2021-12-21


<details><summary><b>MLP-Mixer: An all-MLP Architecture for Vision</b>
<a href="https://arxiv.org/abs/2105.01601">arxiv:2105.01601</a>
&#x1F4C8; 811 <br>
<p>Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy</p></summary>
<p>

**Abstract:** Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.

</p>
</details>

<details><summary><b>Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data</b>
<a href="https://arxiv.org/abs/2105.01764">arxiv:2105.01764</a>
&#x1F4C8; 68 <br>
<p>Hao Sheng, Keniel Yao, Sharad Goel</p></summary>
<p>

**Abstract:** The use of video surveillance in public spaces -- both by government agencies and by private citizens -- has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here, we combine computer vision, human verification, and statistical analysis to estimate the spatial distribution of surveillance cameras. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.2 (in Los Angeles) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents -- a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.

</p>
</details>

<details><summary><b>The Pursuit of Knowledge: Discovering and Localizing Novel Categories using Dual Memory</b>
<a href="https://arxiv.org/abs/2105.01652">arxiv:2105.01652</a>
&#x1F4C8; 45 <br>
<p>Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava</p></summary>
<p>

**Abstract:** We tackle object category discovery, which is the problem of discovering and localizing novel objects in a large unlabeled dataset. While existing methods show results on datasets with less cluttered scenes and fewer object instances per image, we present our results on the challenging COCO dataset. Moreover, we argue that, rather than discovering new categories from scratch, discovery algorithms can benefit from identifying what is already known and focusing their attention on the unknown. We propose a method that exploits prior knowledge about certain object types to discover new categories by leveraging two memory modules, namely Working and Semantic memory. We show the performance of our detector on the COCO minival dataset to demonstrate its in-the-wild capabilities.

</p>
</details>

<details><summary><b>Where and When: Space-Time Attention for Audio-Visual Explanations</b>
<a href="https://arxiv.org/abs/2105.01517">arxiv:2105.01517</a>
&#x1F4C8; 21 <br>
<p>Yanbei Chen, Thomas Hummel, A. Sophia Koepke, Zeynep Akata</p></summary>
<p>

**Abstract:** Explaining the decision of a multi-modal decision-maker requires to determine the evidence from both modalities. Recent advances in XAI provide explanations for models trained on still images. However, when it comes to modeling multiple sensory modalities in a dynamic world, it remains underexplored how to demystify the mysterious dynamics of a complex multi-modal model. In this work, we take a crucial step forward and explore learnable explanations for audio-visual recognition. Specifically, we propose a novel space-time attention network that uncovers the synergistic dynamics of audio and visual data over both space and time. Our model is capable of predicting the audio-visual video events, while justifying its decision by localizing where the relevant visual cues appear, and when the predicted sounds occur in videos. We benchmark our model on three audio-visual video event datasets, comparing extensively to multiple recent multi-modal representation learners and intrinsic explanation models. Experimental results demonstrate the clear superior performance of our model over the existing methods on audio-visual video event recognition. Moreover, we conduct an in-depth study to analyze the explainability of our model based on robustness analysis via perturbation tests and pointing games using human annotations.

</p>
</details>

<details><summary><b>VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding</b>
<a href="https://arxiv.org/abs/2105.01531">arxiv:2105.01531</a>
&#x1F4C8; 20 <br>
<p>Javier Nistal, Cyran Aouameur, Stefan Lattner, GaÃ«l Richard</p></summary>
<p>

**Abstract:** Influenced by the field of Computer Vision, Generative Adversarial Networks (GANs) are often adopted for the audio domain using fixed-size two-dimensional spectrogram representations as the "image data". However, in the (musical) audio domain, it is often desired to generate output of variable duration. This paper presents VQCPC-GAN, an adversarial framework for synthesizing variable-length audio by exploiting Vector-Quantized Contrastive Predictive Coding (VQCPC). A sequence of VQCPC tokens extracted from real audio data serves as conditional input to a GAN architecture, providing step-wise time-dependent features of the generated content. The input noise z (characteristic in adversarial architectures) remains fixed over time, ensuring temporal consistency of global features. We evaluate the proposed model by comparing a diverse set of metrics against various strong baselines. Results show that, even though the baselines score best, VQCPC-GAN achieves comparable performance even when generating variable-length audio. Numerous sound examples are provided in the accompanying website, and we release the code for reproducibility.

</p>
</details>

<details><summary><b>Learning 3D Granular Flow Simulations</b>
<a href="https://arxiv.org/abs/2105.01636">arxiv:2105.01636</a>
&#x1F4C8; 11 <br>
<p>Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp Hochreiter, Johannes Brandstetter</p></summary>
<p>

**Abstract:** Recently, the application of machine learning models has gained momentum in natural sciences and engineering, which is a natural fit due to the abundance of data in these fields. However, the modeling of physical processes from simulation data without first principle solutions remains difficult. Here, we present a Graph Neural Networks approach towards accurate modeling of complex 3D granular flow simulation processes created by the discrete element method LIGGGHTS and concentrate on simulations of physical systems found in real world applications like rotating drums and hoppers. We discuss how to implement Graph Neural Networks that deal with 3D objects, boundary conditions, particle - particle, and particle - boundary interactions such that an accurate modeling of relevant physical quantities is made possible. Finally, we compare the machine learning based trajectories to LIGGGHTS trajectories in terms of particle flows and mixing entropies.

</p>
</details>

<details><summary><b>Envisioning Communities: A Participatory Approach Towards AI for Social Good</b>
<a href="https://arxiv.org/abs/2105.01774">arxiv:2105.01774</a>
&#x1F4C8; 10 <br>
<p>Elizabeth Bondi, Lily Xu, Diana Acosta-Navas, Jackson A. Killian</p></summary>
<p>

**Abstract:** Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be "for" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.

</p>
</details>

<details><summary><b>On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.01648">arxiv:2105.01648</a>
&#x1F4C8; 9 <br>
<p>Marc Aurel Vischer, Robert Tjarko Lange, Henning Sprekeler</p></summary>
<p>

**Abstract:** The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.

</p>
</details>

<details><summary><b>Joint Registration and Segmentation via Multi-Task Learning for Adaptive Radiotherapy of Prostate Cancer</b>
<a href="https://arxiv.org/abs/2105.01844">arxiv:2105.01844</a>
&#x1F4C8; 7 <br>
<p>Mohamed S. Elmahdy, Laurens Beljaards, Sahar Yousefi, Hessam Sokooti, Fons Verbeek, U. A. van der Heide, Marius Staring</p></summary>
<p>

**Abstract:** Medical image registration and segmentation are two of the most frequent tasks in medical image analysis. As these tasks are complementary and correlated, it would be beneficial to apply them simultaneously in a joint manner. In this paper, we formulate registration and segmentation as a joint problem via a Multi-Task Learning (MTL) setting, allowing these tasks to leverage their strengths and mitigate their weaknesses through the sharing of beneficial information. We propose to merge these tasks not only on the loss level, but on the architectural level as well. We studied this approach in the context of adaptive image-guided radiotherapy for prostate cancer, where planning and follow-up CT images as well as their corresponding contours are available for training. The study involves two datasets from different manufacturers and institutes. The first dataset was divided into training (12 patients) and validation (6 patients), and was used to optimize and validate the methodology, while the second dataset (14 patients) was used as an independent test set. We carried out an extensive quantitative comparison between the quality of the automatically generated contours from different network architectures as well as loss weighting methods. Moreover, we evaluated the quality of the generated deformation vector field (DVF). We show that MTL algorithms outperform their Single-Task Learning (STL) counterparts and achieve better generalization on the independent test set. The best algorithm achieved a mean surface distance of $1.06 \pm 0.3$ mm, $1.27 \pm 0.4$ mm, $0.91 \pm 0.4$ mm, and $1.76 \pm 0.8$ mm on the validation set for the prostate, seminal vesicles, bladder, and rectum, respectively. The high accuracy of the proposed method combined with the fast inference speed, makes it a promising method for automatic re-contouring of follow-up scans for adaptive radiotherapy.

</p>
</details>

<details><summary><b>Texture for Colors: Natural Representations of Colors Using Variable Bit-Depth Textures</b>
<a href="https://arxiv.org/abs/2105.01768">arxiv:2105.01768</a>
&#x1F4C8; 7 <br>
<p>Shumeet Baluja</p></summary>
<p>

**Abstract:** Numerous methods have been proposed to transform color and grayscale images to their single bit-per-pixel binary counterparts. Commonly, the goal is to enhance specific attributes of the original image to make it more amenable for analysis. However, when the resulting binarized image is intended for human viewing, aesthetics must also be considered. Binarization techniques, such as half-toning, stippling, and hatching, have been widely used for modeling the original image's intensity profile. We present an automated method to transform an image to a set of binary textures that represent not only the intensities, but also the colors of the original. The foundation of our method is information preservation: creating a set of textures that allows for the reconstruction of the original image's colors solely from the binarized representation. We present techniques to ensure that the textures created are not visually distracting, preserve the intensity profile of the images, and are natural in that they map sets of colors that are perceptually similar to patterns that are similar. The approach uses deep-neural networks and is entirely self-supervised; no examples of good vs. bad binarizations are required. The system yields aesthetically pleasing binary images when tested on a variety of image sources.

</p>
</details>

<details><summary><b>Representation Learning for Clustering via Building Consensus</b>
<a href="https://arxiv.org/abs/2105.01289">arxiv:2105.01289</a>
&#x1F4C8; 7 <br>
<p>Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu, Urun Dogan</p></summary>
<p>

**Abstract:** In this paper, we focus on deep clustering and unsupervised representation learning for images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be closer in the representation space (exemplar consistency), and/or similar images have a similar cluster assignment (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learnt to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a clustering algorithm. We define a clustering loss by performing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, Consensus Clustering using Unsupervised Representation Learning (ConCURL) improves the clustering performance over state-of-the art methods on four out of five image datasets. Further, we extend the evaluation procedure for clustering to reflect the challenges in real world clustering tasks, such as clustering performance in the case of distribution shift. We also perform a detailed ablation study for a deeper understanding of the algorithm.

</p>
</details>

<details><summary><b>Poisoning the Unlabeled Dataset of Semi-Supervised Learning</b>
<a href="https://arxiv.org/abs/2105.01622">arxiv:2105.01622</a>
&#x1F4C8; 6 <br>
<p>Nicholas Carlini</p></summary>
<p>

**Abstract:** Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data.
  We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods.
  We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.

</p>
</details>

<details><summary><b>PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML</b>
<a href="https://arxiv.org/abs/2105.01848">arxiv:2105.01848</a>
&#x1F4C8; 5 <br>
<p>Jiaquan Ye, Xianbiao Qi, Yelin He, Yihao Chen, Dengyi Gu, Peng Gao, Rong Xiao</p></summary>
<p>

**Abstract:** This paper presents our solution for ICDAR 2021 competition on scientific literature parsing taskB: table recognition to HTML. In our method, we divide the table content recognition task into foursub-tasks: table structure recognition, text line detection, text line recognition, and box assignment.Our table structure recognition algorithm is customized based on MASTER [1], a robust image textrecognition algorithm. PSENet [2] is used to detect each text line in the table image. For text linerecognition, our model is also built on MASTER. Finally, in the box assignment phase, we associatedthe text boxes detected by PSENet with the structure item reconstructed by table structure prediction,and fill the recognized content of the text line into the corresponding item. Our proposed methodachieves a 96.84% TEDS score on 9,115 validation samples in the development phase, and a 96.32%TEDS score on 9,064 samples in the final evaluation phase.

</p>
</details>

<details><summary><b>Retrieving Complex Tables with Multi-Granular Graph Representation Learning</b>
<a href="https://arxiv.org/abs/2105.01736">arxiv:2105.01736</a>
&#x1F4C8; 5 <br>
<p>Fei Wang, Kexuan Sun, Muhao Chen, Jay Pujara, Pedro Szekely</p></summary>
<p>

**Abstract:** The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents. Code and data are available at https://github.com/FeiWang96/GTR.

</p>
</details>

<details><summary><b>A reconfigurable neural network ASIC for detector front-end data compression at the HL-LHC</b>
<a href="https://arxiv.org/abs/2105.01683">arxiv:2105.01683</a>
&#x1F4C8; 5 <br>
<p>Giuseppe Di Guglielmo, Farah Fahim, Christian Herwig, Manuel Blanco Valentin, Javier Duarte, Cristian Gingu, Philip Harris, James Hirschauer, Martin Kwok, Vladimir Loncar, Yingyi Luo, Llovizna Miranda, Jennifer Ngadiuba, Daniel Noonan, Seda Ogrenci-Memik, Maurizio Pierini, Sioni Summers, Nhan Tran</p></summary>
<p>

**Abstract:** Despite advances in the programmable logic capabilities of modern trigger systems, a significant bottleneck remains in the amount of data to be transported from the detector to off-detector logic where trigger decisions are made. We demonstrate that a neural network autoencoder model can be implemented in a radiation tolerant ASIC to perform lossy data compression alleviating the data transmission problem while preserving critical information of the detector energy profile. For our application, we consider the high-granularity calorimeter from the CMS experiment at the CERN Large Hadron Collider. The advantage of the machine learning approach is in the flexibility and configurability of the algorithm. By changing the neural network weights, a unique data compression algorithm can be deployed for each sensor in different detector regions, and changing detector or collider conditions. To meet area, performance, and power constraints, we perform a quantization-aware training to create an optimized neural network hardware implementation. The design is achieved through the use of high-level synthesis tools and the hls4ml framework, and was processed through synthesis and physical layout flows based on a LP CMOS 65 nm technology node. The flow anticipates 200 Mrad of ionizing radiation to select gates, and reports a total area of 3.6 mm^2 and consumes 95 mW of power. The simulated energy consumption per inference is 2.4 nJ. This is the first radiation tolerant on-detector ASIC implementation of a neural network that has been designed for particle physics applications.

</p>
</details>

<details><summary><b>Orienting Point Clouds with Dipole Propagation</b>
<a href="https://arxiv.org/abs/2105.01604">arxiv:2105.01604</a>
&#x1F4C8; 5 <br>
<p>Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo, Daniel Cohen-Or</p></summary>
<p>

**Abstract:** Establishing a consistent normal orientation for point clouds is a notoriously difficult problem in geometry processing, requiring attention to both local and global shape characteristics. The normal direction of a point is a function of the local surface neighborhood; yet, point clouds do not disclose the full underlying surface structure. Even assuming known geodesic proximity, calculating a consistent normal orientation requires the global context. In this work, we introduce a novel approach for establishing a globally consistent normal orientation for point clouds. Our solution separates the local and global components into two different sub-problems. In the local phase, we train a neural network to learn a coherent normal direction per patch (i.e., consistently oriented normals within a single patch). In the global phase, we propagate the orientation across all coherent patches using a dipole propagation. Our dipole propagation decides to orient each patch using the electric field defined by all previously orientated patches. This gives rise to a global propagation that is stable, as well as being robust to nearby surfaces, holes, sharp features and noise.

</p>
</details>

<details><summary><b>Training Quantized Neural Networks to Global Optimality via Semidefinite Programming</b>
<a href="https://arxiv.org/abs/2105.01420">arxiv:2105.01420</a>
&#x1F4C8; 5 <br>
<p>Burak Bartan, Mert Pilanci</p></summary>
<p>

**Abstract:** Neural networks (NNs) have been extremely successful across many tasks in machine learning. Quantization of NN weights has become an important topic due to its impact on their energy efficiency, inference time and deployment on hardware. Although post-training quantization is well-studied, training optimal quantized NNs involves combinatorial non-convex optimization problems which appear intractable. In this work, we introduce a convex optimization strategy to train quantized NNs with polynomial activations. Our method leverages hidden convexity in two-layer neural networks from the recent literature, semidefinite lifting, and Grothendieck's identity. Surprisingly, we show that certain quantized NN problems can be solved to global optimality in polynomial-time in all relevant parameters via semidefinite relaxations. We present numerical examples to illustrate the effectiveness of our method.

</p>
</details>

<details><summary><b>Towards End-to-End Deep Learning for Autonomous Racing: On Data Collection and a Unified Architecture for Steering and Throttle Prediction</b>
<a href="https://arxiv.org/abs/2105.01799">arxiv:2105.01799</a>
&#x1F4C8; 4 <br>
<p>Shakti N. Wadekar, Benjamin J. Schwartz, Shyam S. Kannan, Manuel Mar, Rohan Kumar Manna, Vishnu Chellapandi, Daniel J. Gonzalez, Aly El Gamal</p></summary>
<p>

**Abstract:** Deep Neural Networks (DNNs) which are trained end-to-end have been successfully applied to solve complex problems that we have not been able to solve in past decades. Autonomous driving is one of the most complex problems which is yet to be completely solved and autonomous racing adds more complexity and exciting challenges to this problem. Towards the challenge of applying end-to-end learning to autonomous racing, this paper shows results on two aspects: (1) Analyzing the relationship between the driving data used for training and the maximum speed at which the DNN can be successfully applied for predicting steering angle, (2) Neural network architecture and training methodology for learning steering and throttle without any feedback or recurrent connections.

</p>
</details>

<details><summary><b>COVID-19 Detection from Chest X-ray Images using Imprinted Weights Approach</b>
<a href="https://arxiv.org/abs/2105.01710">arxiv:2105.01710</a>
&#x1F4C8; 4 <br>
<p>Jianxing Zhang, Pengcheng Xi, Ashkan Ebadi, Hilda Azimi, Stephane Tremblay, Alexander Wong</p></summary>
<p>

**Abstract:** The COVID-19 pandemic has had devastating effects on the well-being of the global population. The pandemic has been so prominent partly due to the high infection rate of the virus and its variants. In response, one of the most effective ways to stop infection is rapid diagnosis. The main-stream screening method, reverse transcription-polymerase chain reaction (RT-PCR), is time-consuming, laborious and in short supply. Chest radiography is an alternative screening method for the COVID-19 and computer-aided diagnosis (CAD) has proven to be a viable solution at low cost and with fast speed; however, one of the challenges in training the CAD models is the limited number of training data, especially at the onset of the pandemic. This becomes outstanding precisely when the quick and cheap type of diagnosis is critically needed for flattening the infection curve. To address this challenge, we propose the use of a low-shot learning approach named imprinted weights, taking advantage of the abundance of samples from known illnesses such as pneumonia to improve the detection performance on COVID-19.

</p>
</details>

<details><summary><b>PreSizE: Predicting Size in E-Commerce using Transformers</b>
<a href="https://arxiv.org/abs/2105.01564">arxiv:2105.01564</a>
&#x1F4C8; 4 <br>
<p>Yotam Eshel, Or Levi, Haggai Roitman, Alexander Nus</p></summary>
<p>

**Abstract:** Recent advances in the e-commerce fashion industry have led to an exploration of novel ways to enhance buyer experience via improved personalization. Predicting a proper size for an item to recommend is an important personalization challenge, and is being studied in this work. Earlier works in this field either focused on modeling explicit buyer fitment feedback or modeling of only a single aspect of the problem (e.g., specific category, brand, etc.). More recent works proposed richer models, either content-based or sequence-based, better accounting for content-based aspects of the problem or better modeling the buyer's online journey. However, both these approaches fail in certain scenarios: either when encountering unseen items (sequence-based models) or when encountering new users (content-based models).
  To address the aforementioned gaps, we propose PreSizE - a novel deep learning framework which utilizes Transformers for accurate size prediction. PreSizE models the effect of both content-based attributes, such as brand and category, and the buyer's purchase history on her size preferences. Using an extensive set of experiments on a large-scale e-commerce dataset, we demonstrate that PreSizE is capable of achieving superior prediction performance compared to previous state-of-the-art baselines. By encoding item attributes, PreSizE better handles cold-start cases with unseen items, and cases where buyers have little past purchase data. As a proof of concept, we demonstrate that size predictions made by PreSizE can be effectively integrated into an existing production recommender system yielding very effective features and significantly improving recommendations.

</p>
</details>

<details><summary><b>Canonical Saliency Maps: Decoding Deep Face Models</b>
<a href="https://arxiv.org/abs/2105.01386">arxiv:2105.01386</a>
&#x1F4C8; 4 <br>
<p>Thrupthi Ann John, Vineeth N Balasubramanian, C V Jawahar</p></summary>
<p>

**Abstract:** As Deep Neural Network models for face processing tasks approach human-like performance, their deployment in critical applications such as law enforcement and access control has seen an upswing, where any failure may have far-reaching consequences. We need methods to build trust in deployed systems by making their working as transparent as possible. Existing visualization algorithms are designed for object recognition and do not give insightful results when applied to the face domain. In this work, we present 'Canonical Saliency Maps', a new method that highlights relevant facial areas by projecting saliency maps onto a canonical face model. We present two kinds of Canonical Saliency Maps: image-level maps and model-level maps. Image-level maps highlight facial features responsible for the decision made by a deep face model on a given image, thus helping to understand how a DNN made a prediction on the image. Model-level maps provide an understanding of what the entire DNN model focuses on in each task and thus can be used to detect biases in the model. Our qualitative and quantitative results show the usefulness of the proposed canonical saliency maps, which can be used on any deep face model regardless of the architecture.

</p>
</details>

<details><summary><b>The Flipped Classroom model for teaching Conditional Random Fields in an NLP course</b>
<a href="https://arxiv.org/abs/2105.07850">arxiv:2105.07850</a>
&#x1F4C8; 3 <br>
<p>Manex Agirrezabal</p></summary>
<p>

**Abstract:** In this article, we show and discuss our experience in applying the flipped classroom method for teaching Conditional Random Fields in a Natural Language Processing course. We present the activities that we developed together with their relationship to a cognitive complexity model (Bloom's taxonomy). After this, we provide our own reflections and expectations of the model itself. Based on the evaluation got from students, it seems that students learn about the topic and also that the method is rewarding for some students. Additionally, we discuss some shortcomings and we propose possible solutions to them. We conclude the paper with some possible future work.

</p>
</details>

<details><summary><b>Soft-Attention Improves Skin Cancer Classification Performance</b>
<a href="https://arxiv.org/abs/2105.03358">arxiv:2105.03358</a>
&#x1F4C8; 3 <br>
<p>Soumyya Kanti Datta, Mohammad Abuzar Shaikh, Sargur N. Srihari, Mingchen Gao</p></summary>
<p>

**Abstract:** In clinical applications, neural networks must focus on and highlight the most important parts of an input image. Soft-Attention mechanism enables a neural network toachieve this goal. This paper investigates the effectiveness of Soft-Attention in deep neural architectures. The central aim of Soft-Attention is to boost the value of important features and suppress the noise-inducing features. We compare the performance of VGG, ResNet, InceptionResNetv2 and DenseNet architectures with and without the Soft-Attention mechanism, while classifying skin lesions. The original network when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally, Soft-Attention coupling improves the sensitivity score by 3.8% compared to baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly available at github.

</p>
</details>

<details><summary><b>Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature Selection</b>
<a href="https://arxiv.org/abs/2105.02693">arxiv:2105.02693</a>
&#x1F4C8; 3 <br>
<p>Jia-Xing Zhong, Hongbo Zhang</p></summary>
<p>

**Abstract:** In this paper, we present an uncertainty-aware INVASE to quantify predictive confidence of healthcare problem. By introducing learnable Gaussian distributions, we lever-age their variances to measure the degree of uncertainty. Based on the vanilla INVASE, two additional modules are proposed, i.e., an uncertainty quantification module in the predictor, and a reward shaping module in the selector. We conduct extensive experiments on UCI-WDBC dataset. Notably, our method eliminates almost all predictive bias with only about 20% queries, while the uncertainty-agnostic counterpart requires nearly 100% queries. The open-source implementation with a detailed tutorial is available at https://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorialinvase%2B.ipynb.

</p>
</details>

<details><summary><b>Pervasive AI for IoT Applications: Resource-efficient Distributed Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2105.01798">arxiv:2105.01798</a>
&#x1F4C8; 3 <br>
<p>Emna Baccour, Naram Mhaisen, Alaa Awad Abdellatif, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) has witnessed a substantial breakthrough in a variety of Internet of Things (IoT) applications and services, spanning from recommendation systems to robotics control and military surveillance. This is driven by the easier access to sensory data and the enormous scale of pervasive/ubiquitous devices that generate zettabytes (ZB) of real-time data streams. Designing accurate models using such data streams, to predict future insights and revolutionize the decision-taking process, inaugurates pervasive systems as a worthy paradigm for a better quality-of-life. The confluence of pervasive computing and artificial intelligence, Pervasive AI, expanded the role of ubiquitous IoT systems from mainly data collection to executing distributed computations with a promising alternative to centralized learning, presenting various challenges. In this context, a wise cooperation and resource scheduling should be envisaged among IoT devices (e.g., smartphones, smart vehicles) and infrastructure (e.g. edge nodes, and base stations) to avoid communication and computation overheads and ensure maximum performance. In this paper, we conduct a comprehensive survey of the recent techniques developed to overcome these resource challenges in pervasive AI systems. Specifically, we first present an overview of the pervasive computing, its architecture, and its intersection with artificial intelligence. We then review the background, applications and performance metrics of AI, particularly Deep Learning (DL) and online learning, running in a ubiquitous system. Next, we provide a deep literature review of communication-efficient techniques, from both algorithmic and system perspectives, of distributed inference, training and online learning tasks across the combination of IoT devices, edge devices and cloud servers. Finally, we discuss our future vision and research challenges.

</p>
</details>

<details><summary><b>Reconstruction Algorithms for Low-Rank Tensors and Depth-3 Multilinear Circuits</b>
<a href="https://arxiv.org/abs/2105.01751">arxiv:2105.01751</a>
&#x1F4C8; 3 <br>
<p>Vishwas Bhargava, Shubhangi Saraf, Ilya Volkovich</p></summary>
<p>

**Abstract:** We give new and efficient black-box reconstruction algorithms for some classes of depth-$3$ arithmetic circuits. As a consequence, we obtain the first efficient algorithm for computing the tensor rank and for finding the optimal tensor decomposition as a sum of rank-one tensors when then input is a constant-rank tensor. More specifically, we provide efficient learning algorithms that run in randomized polynomial time over general fields and in deterministic polynomial time over the reals and the complex numbers for the following classes:
  (1) Set-multilinear depth-$3$ circuits of constant top fan-in $Î£Î Î£\{\sqcup_j X_j\}(k)$ circuits). As a consequence of our algorithm, we obtain the first polynomial time algorithm for tensor rank computation and optimal tensor decomposition of constant-rank tensors. This result holds for $d$ dimensional tensors for any $d$, but is interesting even for $d=3$.
  (2) Sums of powers of constantly many linear forms ($Î£\wedgeÎ£$ circuits). As a consequence we obtain the first polynomial-time algorithm for tensor rank computation and optimal tensor decomposition of constant-rank symmetric tensors.
  (3) Multilinear depth-3 circuits of constant top fan-in (multilinear $Î£Î Î£(k)$ circuits). Our algorithm works over all fields of characteristic 0 or large enough characteristic. Prior to our work the only efficient algorithms known were over polynomially-sized finite fields (see. Karnin-Shpilka 09').
  Prior to our work, the only polynomial-time or even subexponential-time algorithms known (deterministic or randomized) for subclasses of $Î£Î Î£(k)$ circuits that also work over large/infinite fields were for the setting when the top fan-in $k$ is at most $2$ (see Sinha 16' and Sinha 20').

</p>
</details>

<details><summary><b>Sampling From the Wasserstein Barycenter</b>
<a href="https://arxiv.org/abs/2105.01706">arxiv:2105.01706</a>
&#x1F4C8; 3 <br>
<p>Chiheb Daaloul, Thibaut Le Gouic, Jacques Liandrat, Magali Tournus</p></summary>
<p>

**Abstract:** This work presents an algorithm to sample from the Wasserstein barycenter of absolutely continuous measures. Our method is based on the gradient flow of the multimarginal formulation of the Wasserstein barycenter, with an additive penalization to account for the marginal constraints. We prove that the minimum of this penalized multimarginal formulation is achieved for a coupling that is close to the Wasserstein barycenter. The performances of the algorithm are showcased in several settings.

</p>
</details>

<details><summary><b>Height Estimation of Children under Five Years using Depth Images</b>
<a href="https://arxiv.org/abs/2105.01688">arxiv:2105.01688</a>
&#x1F4C8; 3 <br>
<p>Anusua Trivedi, Mohit Jain, Nikhil Kumar Gupta, Markus Hinsche, Prashant Singh, Markus Matiaschek, Tristan Behrens, Mirco Militeri, Cameron Birge, Shivangi Kaushik, Archisman Mohapatra, Rita Chatterjee, Rahul Dodhia, Juan Lavista Ferres</p></summary>
<p>

**Abstract:** Malnutrition is a global health crisis and is the leading cause of death among children under five. Detecting malnutrition requires anthropometric measurements of weight, height, and middle-upper arm circumference. However, measuring them accurately is a challenge, especially in the global south, due to limited resources. In this work, we propose a CNN-based approach to estimate the height of standing children under five years from depth images collected using a smart-phone. According to the SMART Methodology Manual [5], the acceptable accuracy for height is less than 1.4 cm. On training our deep learning model on 87131 depth images, our model achieved an average mean absolute error of 1.64% on 57064 test images. For 70.3% test images, we estimated height accurately within the acceptable 1.4 cm range. Thus, our proposed solution can accurately detect stunting (low height-for-age) in standing children below five years of age.

</p>
</details>

<details><summary><b>Data-Efficient Reinforcement Learning for Malaria Control</b>
<a href="https://arxiv.org/abs/2105.01620">arxiv:2105.01620</a>
&#x1F4C8; 3 <br>
<p>Lixin Zou, Long Xia, Linfang Hou, Xiangyu Zhao, Dawei Yin</p></summary>
<p>

**Abstract:** Sequential decision-making under cost-sensitive tasks is prohibitively daunting, especially for the problem that has a significant impact on people's daily lives, such as malaria control, treatment recommendation. The main challenge faced by policymakers is to learn a policy from scratch by interacting with a complex environment in a few trials. This work introduces a practical, data-efficient policy learning method, named Variance-Bonus Monte Carlo Tree Search~(VB-MCTS), which can copy with very little data and facilitate learning from scratch in only a few trials. Specifically, the solution is a model-based reinforcement learning method. To avoid model bias, we apply Gaussian Process~(GP) regression to estimate the transitions explicitly. With the GP world model, we propose a variance-bonus reward to measure the uncertainty about the world. Adding the reward to the planning with MCTS can result in more efficient and effective exploration. Furthermore, the derived polynomial sample complexity indicates that VB-MCTS is sample efficient. Finally, outstanding performance on a competitive world-level RL competition and extensive experimental results verify its advantage over the state-of-the-art on the challenging malaria control task.

</p>
</details>

<details><summary><b>Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation</b>
<a href="https://arxiv.org/abs/2105.01593">arxiv:2105.01593</a>
&#x1F4C8; 3 <br>
<p>Daniel Vial, Advait Parulekar, Sanjay Shakkottai, R. Srikant</p></summary>
<p>

**Abstract:** We propose an algorithm that uses linear function approximation (LFA) for stochastic shortest path (SSP). Under minimal assumptions, it obtains sublinear regret, is computationally efficient, and uses stationary policies. To our knowledge, this is the first such algorithm in the LFA literature (for SSP or other formulations). Our algorithm is a special case of a more general one, which achieves regret square root in the number of episodes given access to a certain computation oracle.

</p>
</details>

<details><summary><b>A Finer Calibration Analysis for Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2105.01550">arxiv:2105.01550</a>
&#x1F4C8; 3 <br>
<p>Pranjal Awasthi, Anqi Mao, Mehryar Mohri, Yutao Zhong</p></summary>
<p>

**Abstract:** We present a more general analysis of $H$-calibration for adversarially robust classification. By adopting a finer definition of calibration, we can cover settings beyond the restricted hypothesis sets studied in previous work. In particular, our results hold for most common hypothesis sets used in machine learning. We both fix some previous calibration results (Bao et al., 2020) and generalize others (Awasthi et al., 2021). Moreover, our calibration results, combined with the previous study of consistency by Awasthi et al. (2021), also lead to more general $H$-consistency results covering common hypothesis sets.

</p>
</details>

<details><summary><b>On the Sample Complexity of Rank Regression from Pairwise Comparisons</b>
<a href="https://arxiv.org/abs/2105.01463">arxiv:2105.01463</a>
&#x1F4C8; 3 <br>
<p>Berkan Kadioglu, Peng Tian, Jennifer Dy, Deniz Erdogmus, Stratis Ioannidis</p></summary>
<p>

**Abstract:** We consider a rank regression setting, in which a dataset of $N$ samples with features in $\mathbb{R}^d$ is ranked by an oracle via $M$ pairwise comparisons. Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with $Îµ> 0$ accuracy, it suffices to conduct $M \in Î©(dN\log^3 N/Îµ^2)$ comparisons uniformly at random when $N$ is $Î©(d/Îµ^2)$.

</p>
</details>

<details><summary><b>Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?</b>
<a href="https://arxiv.org/abs/2105.01441">arxiv:2105.01441</a>
&#x1F4C8; 3 <br>
<p>Matthias Kuppler, Christoph Kern, Ruben L. Bach, Frauke Kreuter</p></summary>
<p>

**Abstract:** The advent of powerful prediction algorithms led to increased automation of high-stake decisions regarding the allocation of scarce resources such as government spending and welfare support. This automation bears the risk of perpetuating unwanted discrimination against vulnerable and historically disadvantaged groups. Research on algorithmic discrimination in computer science and other disciplines developed a plethora of fairness metrics to detect and correct discriminatory algorithms. Drawing on robust sociological and philosophical discourse on distributive justice, we identify the limitations and problematic implications of prominent fairness metrics. We show that metrics implementing equality of opportunity only apply when resource allocations are based on deservingness, but fail when allocations should reflect concerns about egalitarianism, sufficiency, and priority. We argue that by cleanly distinguishing between prediction tasks and decision tasks, research on fair machine learning could take better advantage of the rich literature on distributive justice.

</p>
</details>

<details><summary><b>Implicit Regularization in Deep Tensor Factorization</b>
<a href="https://arxiv.org/abs/2105.01346">arxiv:2105.01346</a>
&#x1F4C8; 3 <br>
<p>Paolo Milanesi, Hachem Kadri, StÃ©phane Ayache, Thierry ArtiÃ¨res</p></summary>
<p>

**Abstract:** Attempts of studying implicit regularization associated to gradient descent (GD) have identified matrix completion as a suitable test-bed. Late findings suggest that this phenomenon cannot be phrased as a minimization-norm problem, implying that a paradigm shift is required and that dynamics has to be taken into account. In the present work we address the more general setup of tensor completion by leveraging two popularized tensor factorization, namely Tucker and TensorTrain (TT). We track relevant quantities such as tensor nuclear norm, effective rank, generalized singular values and we introduce deep Tucker and TT unconstrained factorization to deal with the completion task. Experiments on both synthetic and real data show that gradient descent promotes solution with low-rank, and validate the conjecture saying that the phenomenon has to be addressed from a dynamical perspective.

</p>
</details>

<details><summary><b>Hard Choices and Hard Limits for Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2105.07852">arxiv:2105.07852</a>
&#x1F4C8; 2 <br>
<p>Bryce Goodman</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve.

</p>
</details>

<details><summary><b>The Synergy of Complex Event Processing and Tiny Machine Learning in Industrial IoT</b>
<a href="https://arxiv.org/abs/2105.03371">arxiv:2105.03371</a>
&#x1F4C8; 2 <br>
<p>Haoyu Ren, Darko Anicic, Thomas Runkler</p></summary>
<p>

**Abstract:** Focusing on comprehensive networking, big data, and artificial intelligence, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various sensors and field devices play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. The synergy of complex event processing (CEP) and machine learning (ML) has been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive and ubiquitous, concerns are raised since transmitting such amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. The data-centric paradigm can essentially solve these problems by empowering IIoT to perform decentralized on-device ML and CEP, keeping data primarily on edge devices and minimizing communications. However, this is no mean feat because most IIoT edge devices are designed to be computationally constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and micro CEP, we shift the computation from the cloud to the power-constrained IIoT devices and allow users to adapt the on-device ML model and the CEP reasoning logic flexibly on the fly without requiring to reupload the whole program. Lastly, we evaluate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring.

</p>
</details>

<details><summary><b>Out-of-distribution Detection and Generation using Soft Brownian Offset Sampling and Autoencoders</b>
<a href="https://arxiv.org/abs/2105.02965">arxiv:2105.02965</a>
&#x1F4C8; 2 <br>
<p>Felix MÃ¶ller, Diego Botache, Denis Huseljic, Florian Heidecker, Maarten Bieshaar, Bernhard Sick</p></summary>
<p>

**Abstract:** Deep neural networks often suffer from overconfidence which can be partly remedied by improved out-of-distribution detection. For this purpose, we propose a novel approach that allows for the generation of out-of-distribution datasets based on a given in-distribution dataset. This new dataset can then be used to improve out-of-distribution detection for the given dataset and machine learning task at hand. The samples in this dataset are with respect to the feature space close to the in-distribution dataset and therefore realistic and plausible. Hence, this dataset can also be used to safeguard neural networks, i.e., to validate the generalization performance. Our approach first generates suitable representations of an in-distribution dataset using an autoencoder and then transforms them using our novel proposed Soft Brownian Offset method. After transformation, the decoder part of the autoencoder allows for the generation of these implicit out-of-distribution samples. This newly generated dataset then allows for mixing with other datasets and thus improved training of an out-of-distribution classifier, increasing its performance. Experimentally, we show that our approach is promising for time series using synthetic data. Using our new method, we also show in a quantitative case study that we can improve the out-of-distribution detection for the MNIST dataset. Finally, we provide another case study on the synthetic generation of out-of-distribution trajectories, which can be used to validate trajectory prediction algorithms for automated driving.

</p>
</details>

<details><summary><b>Preference learning along multiple criteria: A game-theoretic perspective</b>
<a href="https://arxiv.org/abs/2105.01850">arxiv:2105.01850</a>
&#x1F4C8; 2 <br>
<p>Kush Bhatia, Ashwin Pananjady, Peter L. Bartlett, Anca D. Dragan, Martin J. Wainwright</p></summary>
<p>

**Abstract:** The literature on ranking from ordinal data is vast, and there are several ways to aggregate overall preferences from pairwise comparisons between objects. In particular, it is well known that any Nash equilibrium of the zero sum game induced by the preference matrix defines a natural solution concept (winning distribution over objects) known as a von Neumann winner. Many real-world problems, however, are inevitably multi-criteria, with different pairwise preferences governing the different criteria. In this work, we generalize the notion of a von Neumann winner to the multi-criteria setting by taking inspiration from Blackwell's approachability. Our framework allows for non-linear aggregation of preferences across criteria, and generalizes the linearization-based approach from multi-objective optimization.
  From a theoretical standpoint, we show that the Blackwell winner of a multi-criteria problem instance can be computed as the solution to a convex optimization problem. Furthermore, given random samples of pairwise comparisons, we show that a simple plug-in estimator achieves near-optimal minimax sample complexity. Finally, we showcase the practical utility of our framework in a user study on autonomous driving, where we find that the Blackwell winner outperforms the von Neumann winner for the overall preferences.

</p>
</details>

<details><summary><b>PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Table Image Recognition to Latex</b>
<a href="https://arxiv.org/abs/2105.01846">arxiv:2105.01846</a>
&#x1F4C8; 2 <br>
<p>Yelin He, Xianbiao Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bingcong Li, Xin Tang, Rong Xiao</p></summary>
<p>

**Abstract:** This paper presents our solution for the ICDAR 2021 Competition on Scientific Table Image Recognition to LaTeX. This competition has two sub-tasks: Table Structure Reconstruction (TSR) and Table Content Reconstruction (TCR). We treat both sub-tasks as two individual image-to-sequence recognition problems. We leverage our previously proposed algorithm MASTER \cite{lu2019master}, which is originally proposed for scene text recognition. We optimize the MASTER model from several perspectives: network structure, optimizer, normalization method, pre-trained model, resolution of input image, data augmentation, and model ensemble. Our method achieves 0.7444 Exact Match and 0.8765 Exact Match @95\% on the TSR task, and obtains 0.5586 Exact Match and 0.7386 Exact Match 95\% on the TCR task.

</p>
</details>

<details><summary><b>CUAB: Convolutional Uncertainty Attention Block Enhanced the Chest X-ray Image Analysis</b>
<a href="https://arxiv.org/abs/2105.01840">arxiv:2105.01840</a>
&#x1F4C8; 2 <br>
<p>Chi-Shiang Wang, Fang-Yi Su, Tsung-Lu Michael Lee, Yi-Shan Tsai, Jung-Hsien Chiang</p></summary>
<p>

**Abstract:** In recent years, convolutional neural networks (CNNs) have been successfully implemented to various image recognition applications, such as medical image analysis, object detection, and image segmentation. Many studies and applications have been working on improving the performance of CNN algorithms and models. The strategies that aim to improve the performance of CNNs can be grouped into three major approaches: (1) deeper and wider network architecture, (2) automatic architecture search, and (3) convolutional attention block. Unlike approaches (1) and (2), the convolutional attention block approach is more flexible with lower cost. It enhances the CNN performance by extracting more efficient features. However, the existing attention blocks focus on enhancing the significant features, which lose some potential features in the uncertainty information. Inspired by the test time augmentation and test-time dropout approaches, we developed a novel convolutional uncertainty attention block (CUAB) that can leverage the uncertainty information to improve CNN-based models. The proposed module discovers potential information from the uncertain regions on feature maps in computer vision tasks. It is a flexible functional attention block that can be applied to any position in the convolutional block in CNN models. We evaluated the CUAB with notable backbone models, ResNet and ResNeXt, on a medical image segmentation task. The CUAB achieved a dice score of 73% and 84% in pneumonia and pneumothorax segmentation, respectively, thereby outperforming the original model and other notable attention approaches. The results demonstrated that the CUAB can efficiently utilize the uncertainty information to improve the model performance.

</p>
</details>

<details><summary><b>Curvatures of Stiefel manifolds with deformation metrics</b>
<a href="https://arxiv.org/abs/2105.01834">arxiv:2105.01834</a>
&#x1F4C8; 2 <br>
<p>Du Nguyen</p></summary>
<p>

**Abstract:** We compute curvatures of a family of tractable metrics on Stiefel manifolds, introduced recently by H{Ã¼}per, Markina and Silva Leite, which includes the well-known embedded and canonical metrics on Stiefel manifolds as special cases. The metrics could be identified with the Cheeger deformation metrics. We identify parameter values in the family to make a Stiefel manifold an Einstein manifold and show Stiefel manifolds always carry an Einstein metric. We analyze the sectional curvature range and identify the parameter range where the manifold has non-negative sectional curvature. We provide the exact sectional curvature range when the number of columns in a Stiefel matrix is $2$, and a conjectural range for other cases. We derive the formulas from two approaches, one from a global curvature formula derived in our recent work, another using curvature formulas for left-invariant metrics. The second approach leads to curvature formulas for Cheeger deformation metrics on normal homogeneous spaces.

</p>
</details>

<details><summary><b>Calibration of Human Driving Behavior and Preference Using Naturalistic Traffic Data</b>
<a href="https://arxiv.org/abs/2105.01820">arxiv:2105.01820</a>
&#x1F4C8; 2 <br>
<p>Qi Dai, Di Shen, Jinhong Wang, Suzhou Huang, Dimitar Filev</p></summary>
<p>

**Abstract:** Understanding human driving behaviors quantitatively is critical even in the era when connected and autonomous vehicles and smart infrastructure are becoming ever more prevalent. This is particularly so as that mixed traffic settings, where autonomous vehicles and human driven vehicles co-exist, are expected to persist for quite some time. Towards this end it is necessary that we have a comprehensive modeling framework for decision-making within which human driving preferences can be inferred statistically from observed driving behaviors in realistic and naturalistic traffic settings. Leveraging a recently proposed computational framework for smart vehicles in a smart world using multi-agent based simulation and optimization, we first recapitulate how the forward problem of driving decision-making is modeled as a state space model. We then show how the model can be inverted to estimate driver preferences from naturalistic traffic data using the standard Kalman filter technique. We explicitly illustrate our approach using the vehicle trajectory data from Sugiyama experiment that was originally meant to demonstrate how stop-and-go shockwave can arise spontaneously without bottlenecks. Not only the estimated state filter can fit the observed data well for each individual vehicle, the inferred utility functions can also re-produce quantitatively similar pattern of the observed collective behaviors. One distinct advantage of our approach is the drastically reduced computational burden. This is possible because our forward model treats driving decision process, which is intrinsically dynamic with multi-agent interactions, as a sequence of independent static optimization problems contingent on the state with a finite look ahead anticipation. Consequently we can practically sidestep solving an interacting dynamic inversion problem that would have been much more computationally demanding.

</p>
</details>

<details><summary><b>Real-time Face Mask Detection in Video Data</b>
<a href="https://arxiv.org/abs/2105.01816">arxiv:2105.01816</a>
&#x1F4C8; 2 <br>
<p>Yuchen Ding, Zichen Li, David Yastremsky</p></summary>
<p>

**Abstract:** In response to the ongoing COVID-19 pandemic, we present a robust deep learning pipeline that is capable of identifying correct and incorrect mask-wearing from real-time video streams. To accomplish this goal, we devised two separate approaches and evaluated their performance and run-time efficiency. The first approach leverages a pre-trained face detector in combination with a mask-wearing image classifier trained on a large-scale synthetic dataset. The second approach utilizes a state-of-the-art object detection network to perform localization and classification of faces in one shot, fine-tuned on a small set of labeled real-world images. The first pipeline achieved a test accuracy of 99.97% on the synthetic dataset and maintained 6 FPS running on video data. The second pipeline achieved a mAP(0.5) of 89% on real-world images while sustaining 52 FPS on video data. We have concluded that if a larger dataset with bounding-box labels can be curated, this task is best suited using object detection architectures such as YOLO and SSD due to their superior inference speed and satisfactory performance on key evaluation metrics.

</p>
</details>

<details><summary><b>Training Structured Mechanical Models by Minimizing Discrete Euler-Lagrange Residual</b>
<a href="https://arxiv.org/abs/2105.01811">arxiv:2105.01811</a>
&#x1F4C8; 2 <br>
<p>Kunal Menda, Jayesh K. Gupta, Zachary Manchester, Mykel J. Kochenderfer</p></summary>
<p>

**Abstract:** Model-based paradigms for decision-making and control are becoming ubiquitous in robotics. They rely on the ability to efficiently learn a model of the system from data. Structured Mechanical Models (SMMs) are a data-efficient black-box parameterization of mechanical systems, typically fit to data by minimizing the error between predicted and observed accelerations or next states. In this work, we propose a methodology for fitting SMMs to data by minimizing the discrete Euler-Lagrange residual. To study our methodology, we fit models to joint-angle time-series from undamped and damped double-pendulums, studying the quality of learned models fit to data with and without observation noise. Experiments show that our methodology learns models that are better in accuracy to those of the conventional schemes for fitting SMMs. We identify use cases in which our method is a more appropriate methodology. Source code for reproducing the experiments is available at https://github.com/sisl/delsmm.

</p>
</details>

<details><summary><b>Nonparametric Trace Regression in High Dimensions via Sign Series Representation</b>
<a href="https://arxiv.org/abs/2105.01783">arxiv:2105.01783</a>
&#x1F4C8; 2 <br>
<p>Chanwoo Lee, Lexin Li, Hao Helen Zhang, Miaoyan Wang</p></summary>
<p>

**Abstract:** Learning of matrix-valued data has recently surged in a range of scientific and business applications. Trace regression is a widely used method to model effects of matrix predictors and has shown great success in matrix learning. However, nearly all existing trace regression solutions rely on two assumptions: (i) a known functional form of the conditional mean, and (ii) a global low-rank structure in the entire range of the regression function, both of which may be violated in practice. In this article, we relax these assumptions by developing a general framework for nonparametric trace regression models via structured sign series representations of high dimensional functions. The new model embraces both linear and nonlinear trace effects, and enjoys rank invariance to order-preserving transformations of the response. In the context of matrix completion, our framework leads to a substantially richer model based on what we coin as the "sign rank" of a matrix. We show that the sign series can be statistically characterized by weighted classification tasks. Based on this connection, we propose a learning reduction approach to learn the regression model via a series of classifiers, and develop a parallelable computation algorithm to implement sign series aggregations. We establish the excess risk bounds, estimation error rates, and sample complexities. Our proposal provides a broad nonparametric paradigm to many important matrix learning problems, including matrix regression, matrix completion, multi-task learning, and compressed sensing. We demonstrate the advantages of our method through simulations and two applications, one on brain connectivity study and the other on high-rank image completion.

</p>
</details>

<details><summary><b>Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss</b>
<a href="https://arxiv.org/abs/2105.01778">arxiv:2105.01778</a>
&#x1F4C8; 2 <br>
<p>Yair Carmon, Arun Jambulapati, Yujia Jin, Aaron Sidford</p></summary>
<p>

**Abstract:** We characterize the complexity of minimizing $\max_{i\in[N]} f_i(x)$ for convex, Lipschitz functions $f_1,\ldots, f_N$. For non-smooth functions, existing methods require $O(NÎµ^{-2})$ queries to a first-order oracle to compute an $Îµ$-suboptimal point and $\tilde{O}(NÎµ^{-1})$ queries if the $f_i$ are $O(1/Îµ)$-smooth. We develop methods with improved complexity bounds of $\tilde{O}(NÎµ^{-2/3} + Îµ^{-8/3})$ in the non-smooth case and $\tilde{O}(NÎµ^{-2/3} + \sqrt{N}Îµ^{-1})$ in the $O(1/Îµ)$-smooth case. Our methods consist of a recently proposed ball optimization oracle acceleration algorithm (which we refine) and a careful implementation of said oracle for the softmax function. We also prove an oracle complexity lower bound scaling as $Î©(NÎµ^{-2/3})$, showing that our dependence on $N$ is optimal up to polylogarithmic factors.

</p>
</details>

<details><summary><b>PathBench: A Benchmarking Platform for Classical and Learned Path Planning Algorithms</b>
<a href="https://arxiv.org/abs/2105.01777">arxiv:2105.01777</a>
&#x1F4C8; 2 <br>
<p>Alexandru-Iosif Toma, Hao-Ya Hsueh, Hussein Ali Jaafar, Riku Murai, Paul H. J. Kelly, Sajad Saeedi</p></summary>
<p>

**Abstract:** Path planning is a key component in mobile robotics. A wide range of path planning algorithms exist, but few attempts have been made to benchmark the algorithms holistically or unify their interface. Moreover, with the recent advances in deep neural networks, there is an urgent need to facilitate the development and benchmarking of such learning-based planning algorithms. This paper presents PathBench, a platform for developing, visualizing, training, testing, and benchmarking of existing and future, classical and learned 2D and 3D path planning algorithms, while offering support for Robot Oper-ating System (ROS). Many existing path planning algorithms are supported; e.g. A*, wavefront, rapidly-exploring random tree, value iteration networks, gated path planning networks; and integrating new algorithms is easy and clearly specified. We demonstrate the benchmarking capability of PathBench by comparing implemented classical and learned algorithms for metrics, such as path length, success rate, computational time and path deviation. These evaluations are done on built-in PathBench maps and external path planning environments from video games and real world databases. PathBench is open source.

</p>
</details>

<details><summary><b>Motion Artifact Reduction in Quantitative Susceptibility Mapping using Deep Neural Network</b>
<a href="https://arxiv.org/abs/2105.01746">arxiv:2105.01746</a>
&#x1F4C8; 2 <br>
<p>Chao Li, Hang Zhang, Jinwei Zhang, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang</p></summary>
<p>

**Abstract:** An approach to reduce motion artifacts in Quantitative Susceptibility Mapping using deep learning is proposed. We use an affine motion model with randomly created motion profiles to simulate motion-corrupted QSM images. The simulated QSM image is paired with its motion-free reference to train a neural network using supervised learning. The trained network is tested on unseen simulated motion-corrupted QSM images, in healthy volunteers and in Parkinson's disease patients. The results show that motion artifacts, such as ringing and ghosting, were successfully suppressed.

</p>
</details>

<details><summary><b>HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish</b>
<a href="https://arxiv.org/abs/2105.01735">arxiv:2105.01735</a>
&#x1F4C8; 2 <br>
<p>Robert Mroczkowski, Piotr Rybak, Alina WrÃ³blewska, Ireneusz Gawlik</p></summary>
<p>

**Abstract:** BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model -- HerBERT -- is trained. This model achieves state-of-the-art results on multiple downstream tasks.

</p>
</details>

<details><summary><b>Calibration of prediction rules for life-time outcomes using prognostic Cox regression survival models and multiple imputations to account for missing predictor data with cross-validatory assessment</b>
<a href="https://arxiv.org/abs/2105.01733">arxiv:2105.01733</a>
&#x1F4C8; 2 <br>
<p>Bart J. A. Mertens</p></summary>
<p>

**Abstract:** In this paper, we expand the methodology presented in Mertens et. al (2020, Biometrical Journal) to the study of life-time (survival) outcome which is subject to censoring and when imputation is used to account for missing values. We consider the problem where missing values can occur in both the calibration data as well as newly - to-be-predicted - observations (validation). We focus on the Cox model. Methods are described to combine imputation with predictive calibration in survival modeling subject to censoring. Application to cross-validation is discussed. We demonstrate how conclusions broadly confirm the first paper which restricted to the study of binary outcomes only. Specifically prediction-averaging appears to have superior statistical properties, especially smaller predictive variation, as opposed to a direct application of Rubin's rules. Distinct methods for dealing with the baseline hazards are discussed when using Rubin's rules-based approaches.

</p>
</details>

<details><summary><b>GANs for Urban Design</b>
<a href="https://arxiv.org/abs/2105.01727">arxiv:2105.01727</a>
&#x1F4C8; 2 <br>
<p>Stanislava Fedorova</p></summary>
<p>

**Abstract:** Development and diffusion of machine learning and big data tools provide a new tool for architects and urban planners that could be used as analytical or design instruments. The topic investigated in this paper is the application of Generative Adversarial Networks to the design of an urban block. The research presents a flexible model able to adapt to the morphological characteristics of a city. This method does not define explicitly any of the parameters of an urban block typical for a city, the algorithm learns them from the existing urban context. This approach has been applied to the cities with different morphology: Milan, Amsterdam, Tallinn, Turin, and Bengaluru in order to see the performance of the model and the possibility of style translation between different cities. The data are gathered from Openstreetmap and Open Data portals of the cities. This research presents the results of the experiments and their quantitative and qualitative evaluation.

</p>
</details>

<details><summary><b>Attention-based Stylisation for Exemplar Image Colourisation</b>
<a href="https://arxiv.org/abs/2105.01705">arxiv:2105.01705</a>
&#x1F4C8; 2 <br>
<p>Marc Gorriz Blanch, Issa Khalifeh, Alan Smeaton, Noel O'Connor, Marta Mrak</p></summary>
<p>

**Abstract:** Exemplar-based colourisation aims to add plausible colours to a grayscale image using the guidance of a colour reference image. Most of the existing methods tackle the task as a style transfer problem, using a convolutional neural network (CNN) to obtain deep representations of the content of both inputs. Stylised outputs are then obtained by computing similarities between both feature representations in order to transfer the style of the reference to the content of the target input. However, in order to gain robustness towards dissimilar references, the stylised outputs need to be refined with a second colourisation network, which significantly increases the overall system complexity. This work reformulates the existing methodology introducing a novel end-to-end colourisation network that unifies the feature matching with the colourisation process. The proposed architecture integrates attention modules at different resolutions that learn how to perform the style transfer task in an unsupervised way towards decoding realistic colour predictions. Moreover, axial attention is proposed to simplify the attention operations and to obtain a fast but robust cost-effective architecture. Experimental validations demonstrate efficiency of the proposed methodology which generates high quality and visual appealing colourisation. Furthermore, the complexity of the proposed methodology is reduced compared to the state-of-the-art methods.

</p>
</details>

<details><summary><b>Stochastic gradient descent with noise of machine learning type. Part I: Discrete time analysis</b>
<a href="https://arxiv.org/abs/2105.01650">arxiv:2105.01650</a>
&#x1F4C8; 2 <br>
<p>Stephan Wojtowytsch</p></summary>
<p>

**Abstract:** Stochastic gradient descent (SGD) is one of the most popular algorithms in modern machine learning. The noise encountered in these applications is different from that in many theoretical analyses of stochastic gradient algorithms. In this article, we discuss some of the common properties of energy landscapes and stochastic noise encountered in machine learning problems, and how they affect SGD-based optimization.
  In particular, we show that the learning rate in SGD with machine learning noise can be chosen to be small, but uniformly positive for all times if the energy landscape resembles that of overparametrized deep learning problems. If the objective function satisfies a Lojasiewicz inequality, SGD converges to the global minimum exponentially fast, and even for functions which may have local minima, we establish almost sure convergence to the global minimum at an exponential rate from any finite energy initialization. The assumptions that we make in this result concern the behavior where the objective function is either small or large and the nature of the gradient noise, but the energy landscape is fairly unconstrained on the domain where the objective function takes values in an intermediate regime.

</p>
</details>

<details><summary><b>Implicit differentiation for fast hyperparameter selection in non-smooth convex learning</b>
<a href="https://arxiv.org/abs/2105.01637">arxiv:2105.01637</a>
&#x1F4C8; 2 <br>
<p>Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon</p></summary>
<p>

**Abstract:** Finding the optimal hyperparameters of a model can be cast as a bilevel optimization problem, typically solved using zero-order techniques. In this work we study first-order methods when the inner optimization problem is convex but non-smooth. We show that the forward-mode differentiation of proximal gradient descent and proximal coordinate descent yield sequences of Jacobians converging toward the exact Jacobian. Using implicit differentiation, we show it is possible to leverage the non-smoothness of the inner problem to speed up the computation. Finally, we provide a bound on the error made on the hypergradient when the inner optimization problem is solved approximately. Results on regression and classification problems reveal computational benefits for hyperparameter optimization, especially when multiple hyperparameters are required.

</p>
</details>

<details><summary><b>Federated Multi-View Learning for Private Medical Data Integration and Analysis</b>
<a href="https://arxiv.org/abs/2105.01603">arxiv:2105.01603</a>
&#x1F4C8; 2 <br>
<p>Sicong Che, Hao Peng, Lichao Sun, Yong Chen, Lifang He</p></summary>
<p>

**Abstract:** Along with the rapid expansion of information technology and digitalization of health data, there is an increasing concern on maintaining data privacy while garnering the benefits in medical field. Two critical challenges are identified: Firstly, medical data is naturally distributed across multiple local sites, making it difficult to collectively train machine learning models without data leakage. Secondly, in medical applications, data are often collected from different sources and views, resulting in heterogeneity and complexity that requires reconciliation. This paper aims to provide a generic Federated Multi-View Learning (FedMV) framework for multi-view data leakage prevention, which is based on different types of local data availability and enables to accommodate two types of problems: Vertical Federated Multi-View Learning (V-FedMV) and Horizontal Federated Multi-View Learning (H-FedMV). We experimented with real-world keyboard data collected from BiAffect study. The results demonstrated that the proposed FedMV approach can make full use of multi-view data in a privacy-preserving way, and both V-FedMV and H-FedMV methods perform better than their single-view and pairwise counterparts. Besides, the proposed model can be easily adapted to deal with multi-view sequential data in a federated environment, which has been modeled and experimentally studied. To the best of our knowledge, this framework is the first to consider both vertical and horizontal diversification in the multi-view setting, as well as their sequential federated learning.

</p>
</details>

<details><summary><b>Multipath Graph Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2105.01510">arxiv:2105.01510</a>
&#x1F4C8; 2 <br>
<p>Rangan Das, Bikram Boote, Saumik Bhattacharya, Ujjwal Maulik</p></summary>
<p>

**Abstract:** Graph convolution networks have recently garnered a lot of attention for representation learning on non-Euclidean feature spaces. Recent research has focused on stacking multiple layers like in convolutional neural networks for the increased expressive power of graph convolution networks. However, simply stacking multiple graph convolution layers lead to issues like vanishing gradient, over-fitting and over-smoothing. Such problems are much less when using shallower networks, even though the shallow networks have lower expressive power. In this work, we propose a novel Multipath Graph convolutional neural network that aggregates the output of multiple different shallow networks. We train and test our model on various benchmarks datasets for the task of node property prediction. Results show that the proposed method not only attains increased test accuracy but also requires fewer training epochs to converge. The full implementation is available at https://github.com/rangan2510/MultiPathGCN

</p>
</details>

<details><summary><b>Semantic Extractor-Paraphraser based Abstractive Summarization</b>
<a href="https://arxiv.org/abs/2105.01296">arxiv:2105.01296</a>
&#x1F4C8; 2 <br>
<p>Anubhav Jangra, Raghav Jain, Vaibhav Mavi, Sriparna Saha, Pushpak Bhattacharyya</p></summary>
<p>

**Abstract:** The anthology of spoken languages today is inundated with textual information, necessitating the development of automatic summarization models. In this manuscript, we propose an extractor-paraphraser based abstractive summarization system that exploits semantic overlap as opposed to its predecessors that focus more on syntactic information overlap. Our model outperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word mover similarity (WMS), establishing the superiority of the proposed system via extensive ablation experiments. We have also challenged the summarization capabilities of the state of the art Pointer Generator Network (PGN), and through thorough experimentation, shown that PGN is more of a paraphraser, contrary to the prevailing notion of a summarizer; illustrating it's incapability to accumulate information across multiple sentences.

</p>
</details>

<details><summary><b>Two-Stage Stochastic Optimization via Primal-Dual Decomposition and Deep Unrolling</b>
<a href="https://arxiv.org/abs/2105.01853">arxiv:2105.01853</a>
&#x1F4C8; 1 <br>
<p>An Liu, Rui Yang, Tony Q. S. Quek, Min-Jian Zhao</p></summary>
<p>

**Abstract:** We consider a two-stage stochastic optimization problem, in which a long-term optimization variable is coupled with a set of short-term optimization variables in both objective and constraint functions. Despite that two-stage stochastic optimization plays a critical role in various engineering and scientific applications, there still lack efficient algorithms, especially when the long-term and short-term variables are coupled in the constraints. To overcome the challenge caused by tightly coupled stochastic constraints, we first establish a two-stage primal-dual decomposition (PDD) method to decompose the two-stage problem into a long-term problem and a family of short-term subproblems. Then we propose a PDD-based stochastic successive convex approximation (PDD-SSCA) algorithmic framework to find KKT solutions for two-stage stochastic optimization problems. At each iteration, PDD-SSCA first runs a short-term sub-algorithm to find stationary points of the short-term subproblems associated with a mini-batch of the state samples. Then it constructs a convex surrogate for the long-term problem based on the deep unrolling of the short-term sub-algorithm and the back propagation method. Finally, the optimal solution of the convex surrogate problem is solved to generate the next iterate. We establish the almost sure convergence of PDD-SSCA and customize the algorithmic framework to solve two important application problems. Simulations show that PDD-SSCA can achieve superior performance over existing solutions.

</p>
</details>

<details><summary><b>Lesion Segmentation and RECIST Diameter Prediction via Click-driven Attention and Dual-path Connection</b>
<a href="https://arxiv.org/abs/2105.01828">arxiv:2105.01828</a>
&#x1F4C8; 1 <br>
<p>Youbao Tang, Ke Yan, Jinzheng Cai, Lingyun Huang, Guotong Xie, Jing Xiao, Jingjing Lu, Gigin Lin, Le Lu</p></summary>
<p>

**Abstract:** Measuring lesion size is an important step to assess tumor growth and monitor disease progression and therapy response in oncology image analysis. Although it is tedious and highly time-consuming, radiologists have to work on this task by using RECIST criteria (Response Evaluation Criteria In Solid Tumors) routinely and manually. Even though lesion segmentation may be the more accurate and clinically more valuable means, physicians can not manually segment lesions as now since much more heavy laboring will be required. In this paper, we present a prior-guided dual-path network (PDNet) to segment common types of lesions throughout the whole body and predict their RECIST diameters accurately and automatically. Similar to [1], a click guidance from radiologists is the only requirement. There are two key characteristics in PDNet: 1) Learning lesion-specific attention matrices in parallel from the click prior information by the proposed prior encoder, named click-driven attention; 2) Aggregating the extracted multi-scale features comprehensively by introducing top-down and bottom-up connections in the proposed decoder, named dual-path connection. Experiments show the superiority of our proposed PDNet in lesion segmentation and RECIST diameter prediction using the DeepLesion dataset and an external test set. PDNet learns comprehensive and representative deep image features for our tasks and produces more accurate results on both lesion segmentation and RECIST diameter prediction.

</p>
</details>

<details><summary><b>DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on the Edge</b>
<a href="https://arxiv.org/abs/2105.01803">arxiv:2105.01803</a>
&#x1F4C8; 1 <br>
<p>Zhe Yang, Klara Nahrstedt, Hongpeng Guo, Qian Zhou</p></summary>
<p>

**Abstract:** The ubiquity of smartphone cameras and IoT cameras, together with the recent boom of deep learning and deep neural networks, proliferate various computer vision driven mobile and IoT applications deployed on the edge. This paper focuses on applications which make soft real time requests to perform inference on their data - they desire prompt responses within designated deadlines, but occasional deadline misses are acceptable. Supporting soft real time applications on a multi-tenant edge server is not easy, since the requests sharing the limited GPU computing resources of an edge server interfere with each other. In order to tackle this problem, we comprehensively evaluate how latency and throughput respond to different GPU execution plans. Based on this analysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee to the requests while maintaining high overall system throughput. The key component of DeepRT, DisBatcher, batches data from different requests as much as possible while it is proven to provide latency guarantee for requests admitted by an Admission Control Module. DeepRT also includes an Adaptation Module which tackles overruns. Our evaluation results show that DeepRT outperforms state-of-the-art works in terms of the number of deadline misses and throughput.

</p>
</details>

<details><summary><b>Generative Adversarial Networks (GAN) Powered Fast Magnetic Resonance Imaging -- Mini Review, Comparison and Perspectives</b>
<a href="https://arxiv.org/abs/2105.01800">arxiv:2105.01800</a>
&#x1F4C8; 1 <br>
<p>Guang Yang, Jun Lv, Yutong Chen, Jiahao Huang, Jin Zhu</p></summary>
<p>

**Abstract:** Magnetic Resonance Imaging (MRI) is a vital component of medical imaging. When compared to other image modalities, it has advantages such as the absence of radiation, superior soft tissue contrast, and complementary multiple sequence information. However, one drawback of MRI is its comparatively slow scanning and reconstruction compared to other image modalities, limiting its usage in some clinical applications when imaging time is critical. Traditional compressive sensing based MRI (CS-MRI) reconstruction can speed up MRI acquisition, but suffers from a long iterative process and noise-induced artefacts. Recently, Deep Neural Networks (DNNs) have been used in sparse MRI reconstruction models to recreate relatively high-quality images from heavily undersampled k-space data, allowing for much faster MRI scanning. However, there are still some hurdles to tackle. For example, directly training DNNs based on L1/L2 distance to the target fully sampled images could result in blurry reconstruction because L1/L2 loss can only enforce overall image or patch similarity and does not take into account local information such as anatomical sharpness. It is also hard to preserve fine image details while maintaining a natural appearance. More recently, Generative Adversarial Networks (GAN) based methods are proposed to solve fast MRI with enhanced image perceptual quality. The encoder obtains a latent space for the undersampling image, and the image is reconstructed by the decoder using the GAN loss. In this chapter, we review the GAN powered fast MRI methods with a comparative study on various anatomical datasets to demonstrate the generalisability and robustness of this kind of fast MRI while providing future perspectives.

</p>
</details>

<details><summary><b>WaveGlove: Transformer-based hand gesture recognition using multiple inertial sensors</b>
<a href="https://arxiv.org/abs/2105.01753">arxiv:2105.01753</a>
&#x1F4C8; 1 <br>
<p>Matej KrÃ¡lik, Marek Å uppa</p></summary>
<p>

**Abstract:** Hand Gesture Recognition (HGR) based on inertial data has grown considerably in recent years, with the state-of-the-art approaches utilizing a single handheld sensor and a vocabulary comprised of simple gestures.
  In this work we explore the benefits of using multiple inertial sensors. Using WaveGlove, a custom hardware prototype in the form of a glove with five inertial sensors, we acquire two datasets consisting of over $11000$ samples.
  To make them comparable with prior work, they are normalized along with $9$ other publicly available datasets, and subsequently used to evaluate a range of Machine Learning approaches for gesture recognition, including a newly proposed Transformer-based architecture. Our results show that even complex gestures involving different fingers can be recognized with high accuracy.
  An ablation study performed on the acquired datasets demonstrates the importance of multiple sensors, with an increase in performance when using up to three sensors and no significant improvements beyond that.

</p>
</details>

<details><summary><b>HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation</b>
<a href="https://arxiv.org/abs/2105.01585">arxiv:2105.01585</a>
&#x1F4C8; 1 <br>
<p>Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, Yun Liang</p></summary>
<p>

**Abstract:** Tensor computations overwhelm traditional general-purpose computing devices due to the large amounts of data and operations of the computations. They call for a holistic solution composed of both hardware acceleration and software mapping. Hardware/software (HW/SW) co-design optimizes the hardware and software in concert and produces high-quality solutions. There are two main challenges in the co-design flow. First, multiple methods exist to partition tensor computation and have different impacts on performance and energy efficiency. Besides, the hardware part must be implemented by the intrinsic functions of spatial accelerators. It is hard for programmers to identify and analyze the partitioning methods manually. Second, the overall design space composed of HW/SW partitioning, hardware optimization, and software optimization is huge. The design space needs to be efficiently explored.
  To this end, we propose an agile co-design approach HASCO that provides an efficient HW/SW solution to dense tensor computation. We use tensor syntax trees as the unified IR, based on which we develop a two-step approach to identify partitioning methods. For each method, HASCO explores the hardware and software design spaces. We propose different algorithms for the explorations, as they have distinct objectives and evaluation costs. Concretely, we develop a multi-objective Bayesian optimization algorithm to explore hardware optimization. For software optimization, we use heuristic and Q-learning algorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X latency reduction through HW/SW co-design compared with developing the hardware and software separately.

</p>
</details>

<details><summary><b>Riemannian Geometry with differentiable ambient space and metric operator</b>
<a href="https://arxiv.org/abs/2105.01583">arxiv:2105.01583</a>
&#x1F4C8; 1 <br>
<p>Du Nguyen</p></summary>
<p>

**Abstract:** We show Riemannian geometry could be studied by identifying the tangent bundle of a Riemannian manifold $\mathcal{M}$ with a subbundle of the trivial bundle $\mathcal{M} \times \mathcal{E}$, obtained by embedding $\mathcal{M}$ differentiably in a Euclidean space $\mathcal{E}$. Given such an embedding, we can extend the metric tensor on $\mathcal{M}$ to a (positive-definite) operator-valued function acting on $\mathcal{E}$, giving us an embedded ambient structure. The formulas for the Christoffel symbols and Riemannian curvature in local coordinates have simple generalizations to this setup. For a Riemannian submersion $\mathfrak{q}:\mathcal{M}\to \mathcal{B}$ from an embedded manifold $\mathcal{M}\subset \mathcal{E}$, we define a submersed ambient structure and obtain similar formulas, with the O'Neil tensor expressed in terms of the projection to the horizontal bundle $\mathcal{H}\mathcal{M}$. Using this framework, we provide the embedded and submersed ambient structures for the double tangent bundle $\mathcal{T}\mathcal{T}\mathcal{M}$ and the tangent of the horizontal bundle $\mathcal{T}\mathcal{H}\mathcal{M}$, describe the fibration of a horizontal bundle over the tangent bundle of the base manifold and extend the notion of a canonical flip to the submersion case. We obtain a formula for horizontal lifts of Jacobi fields, and a new closed-form formula for Jacobi fields of naturally reductive homogeneous spaces. We construct natural metrics on these double tangent bundles, in particular, extending Sasaki and other natural metrics to the submersion case. We illustrate by providing explicit calculations for several manifolds.

</p>
</details>

<details><summary><b>Apparel Recommender System based on Bilateral image shape features</b>
<a href="https://arxiv.org/abs/2105.01541">arxiv:2105.01541</a>
&#x1F4C8; 1 <br>
<p>Yichi Lu, Mingtian Gao, Ryosuke Saga</p></summary>
<p>

**Abstract:** Probabilistic matrix factorization (PMF) is a well-known model of recommender systems. With the development of image recognition technology, some PMF recommender systems that combine images have emerged. Some of these systems use the image shape features of the recommended products to achieve better results compared to those of the traditional PMF. However, in the existing methods, no PMF recommender system can combine the image features of products previously purchased by customers and of recommended products. Thus, this study proposes a novel probabilistic model that integrates double convolutional neural networks (CNNs) into PMF. For apparel goods, two trained CNNs from the image shape features of users and items are combined, and the latent variables of users and items are optimized based on the vectorized features of CNNs and ratings. Extensive experiments show that our model predicts outcome more accurately than do other recommender models.

</p>
</details>

<details><summary><b>Intelligent Zero Trust Architecture for 5G/6G Tactical Networks: Principles, Challenges, and the Role of Machine Learning</b>
<a href="https://arxiv.org/abs/2105.01478">arxiv:2105.01478</a>
&#x1F4C8; 1 <br>
<p>Keyvan Ramezanpour, Jithin Jagannath</p></summary>
<p>

**Abstract:** In this position paper, we discuss the critical need for integrating zero trust (ZT) principles into next-generation communication networks (5G/6G) for both tactical and commercial applications. We highlight the challenges and introduce the concept of an intelligent zero trust architecture (i-ZTA) as a security framework in 5G/6G networks with untrusted components. While network virtualization, software-defined networking (SDN), and service-based architectures (SBA) are key enablers of 5G networks, operating in an untrusted environment has also become a key feature of the networks. Further, seamless connectivity to a high volume of devices in multi-radio access technology (RAT) has broadened the attack surface on information infrastructure. Network assurance in a dynamic untrusted environment calls for revolutionary architectures beyond existing static security frameworks. This paper presents the architectural design of an i-ZTA upon which modern artificial intelligence (AI) algorithms can be developed to provide information security in untrusted networks. We introduce key ZT principles as real-time Monitoring of the security state of network assets, Evaluating the risk of individual access requests, and Deciding on access authorization using a dynamic trust algorithm, called MED components. The envisioned architecture adopts an SBA-based design, similar to the 3GPP specification of 5G networks, by leveraging the open radio access network (O-RAN) architecture with appropriate real-time engines and network interfaces for collecting necessary machine learning data. The i-ZTA is also expected to exploit the multi-access edge computing (MEC) technology of 5G as a key enabler of intelligent MED components for resource-constraint devices.

</p>
</details>

<details><summary><b>Two-Stage Facility Location Games with Strategic Clients and Facilities</b>
<a href="https://arxiv.org/abs/2105.01425">arxiv:2105.01425</a>
&#x1F4C8; 1 <br>
<p>Simon Krogmann, Pascal Lenzner, Louise Molitor, Alexander Skopalik</p></summary>
<p>

**Abstract:** We consider non-cooperative facility location games where both facilities and clients act strategically and heavily influence each other. This contrasts established game-theoretic facility location models with non-strategic clients that simply select the closest opened facility. In our model, every facility location has a set of attracted clients and each client has a set of shopping locations and a weight that corresponds to her spending capacity. Facility agents selfishly select a location for opening their facility to maximize the attracted total spending capacity, whereas clients strategically decide how to distribute their spending capacity among the opened facilities in their shopping range. We focus on a natural client behavior similar to classical load balancing: our selfish clients aim for a distribution that minimizes their maximum waiting times for getting serviced, where a facility's waiting time corresponds to its total attracted client weight.
  We show that subgame perfect equilibria exist and give almost tight constant bounds on the Price of Anarchy and the Price of Stability, which even hold for a broader class of games with arbitrary client behavior. Since facilities and clients influence each other, it is crucial for the facilities to anticipate the selfish clients' behavior when selecting their location. For this, we provide an efficient algorithm that also implies an efficient check for equilibrium. Finally, we show that computing a socially optimal facility placement is NP-hard and that this result holds for all feasible client weight distributions.

</p>
</details>

<details><summary><b>An Overview of Laser Injection against Embedded Neural Network Models</b>
<a href="https://arxiv.org/abs/2105.01403">arxiv:2105.01403</a>
&#x1F4C8; 1 <br>
<p>Mathieu Dumont, Pierre-Alain Moellic, Raphael Viera, Jean-Max Dutertre, RÃ©mi Bernhard</p></summary>
<p>

**Abstract:** For many IoT domains, Machine Learning and more particularly Deep Learning brings very efficient solutions to handle complex data and perform challenging and mostly critical tasks. However, the deployment of models in a large variety of devices faces several obstacles related to trust and security. The latest is particularly critical since the demonstrations of severe flaws impacting the integrity, confidentiality and accessibility of neural network models. However, the attack surface of such embedded systems cannot be reduced to abstract flaws but must encompass the physical threats related to the implementation of these models within hardware platforms (e.g., 32-bit microcontrollers). Among physical attacks, Fault Injection Analysis (FIA) are known to be very powerful with a large spectrum of attack vectors. Most importantly, highly focused FIA techniques such as laser beam injection enable very accurate evaluation of the vulnerabilities as well as the robustness of embedded systems. Here, we propose to discuss how laser injection with state-of-the-art equipment, combined with theoretical evidences from Adversarial Machine Learning, highlights worrying threats against the integrity of deep learning inference and claims that join efforts from the theoretical AI and Physical Security communities are a urgent need.

</p>
</details>

<details><summary><b>A Review of Confidentiality Threats Against Embedded Neural Network Models</b>
<a href="https://arxiv.org/abs/2105.01401">arxiv:2105.01401</a>
&#x1F4C8; 1 <br>
<p>RaphaÃ«l Joud, Pierre-Alain Moellic, RÃ©mi Bernhard, Jean-Baptiste Rigaud</p></summary>
<p>

**Abstract:** Utilization of Machine Learning (ML) algorithms, especially Deep Neural Network (DNN) models, becomes a widely accepted standard in many domains more particularly IoT-based systems. DNN models reach impressive performances in several sensitive fields such as medical diagnosis, smart transport or security threat detection, and represent a valuable piece of Intellectual Property. Over the last few years, a major trend is the large-scale deployment of models in a wide variety of devices. However, this migration to embedded systems is slowed down because of the broad spectrum of attacks threatening the integrity, confidentiality and availability of embedded models. In this review, we cover the landscape of attacks targeting the confidentiality of embedded DNN models that may have a major impact on critical IoT systems, with a particular focus on model extraction and data leakage. We highlight the fact that Side-Channel Analysis (SCA) is a relatively unexplored bias by which model's confidentiality can be compromised. Input data, architecture or parameters of a model can be extracted from power or electromagnetic observations, testifying a real need from a security point of view.

</p>
</details>

<details><summary><b>Deep Extended Feedback Codes</b>
<a href="https://arxiv.org/abs/2105.01365">arxiv:2105.01365</a>
&#x1F4C8; 1 <br>
<p>Anahid Robert Safavi, Alberto G. Perotti, Branislav M. Popovic, Mahdi Boloursaz Mashhadi, Deniz Gunduz</p></summary>
<p>

**Abstract:** A new deep-neural-network (DNN) based error correction encoder architecture for channels with feedback, called Deep Extended Feedback (DEF), is presented in this paper. The encoder in the DEF architecture transmits an information message followed by a sequence of parity symbols which are generated based on the message as well as the observations of the past forward channel outputs sent to the transmitter through a feedback channel. DEF codes generalize Deepcode [1] in several ways: parity symbols are generated based on forward-channel output observations over longer time intervals in order to provide better error correction capability; and high-order modulation formats are deployed in the encoder so as to achieve increased spectral efficiency. Performance evaluations show that DEF codes have better performance compared to other DNN-based codes for channels with feedback.

</p>
</details>

<details><summary><b>Signal automata and hidden Markov models</b>
<a href="https://arxiv.org/abs/2105.01341">arxiv:2105.01341</a>
&#x1F4C8; 1 <br>
<p>Teodor Knapik</p></summary>
<p>

**Abstract:** A generic method for inferring a dynamical hidden Markov model from a time series is proposed. Under reasonable hypothesis, the model is updated in constant time whenever a new measurement arrives.

</p>
</details>

<details><summary><b>BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter</b>
<a href="https://arxiv.org/abs/2105.01331">arxiv:2105.01331</a>
&#x1F4C8; 1 <br>
<p>Hasan Kemik, Nusret ÃzateÅ, Meysam Asgari-Chenaghlu, Erik Cambria</p></summary>
<p>

**Abstract:** Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.

</p>
</details>

<details><summary><b>Personalized Algorithm Generation: A Case Study in Meta-Learning ODE Integrators</b>
<a href="https://arxiv.org/abs/2105.01303">arxiv:2105.01303</a>
&#x1F4C8; 1 <br>
<p>Yue Guo, Felix Dietrich, Tom Bertalan, Danimir T. Doncevic, Manuel Dahmen, Ioannis G. Kevrekidis, Qianxiao Li</p></summary>
<p>

**Abstract:** We study the meta-learning of numerical algorithms for scientific computing, which combines the mathematically driven, handcrafted design of general algorithm structure with a data-driven adaptation to specific classes of tasks. This represents a departure from the classical approaches in numerical analysis, which typically do not feature such learning-based adaptations. As a case study, we develop a machine learning approach that automatically learns effective solvers for initial value problems in the form of ordinary differential equations (ODEs), based on the Runge-Kutta (RK) integrator architecture. By combining neural network approximations and meta-learning, we show that we can obtain high-order integrators for targeted families of differential equations without the need for computing integrator coefficients by hand. Moreover, we demonstrate that in certain cases we can obtain superior performance to classical RK methods. This can be attributed to certain properties of the ODE families being identified and exploited by the approach. Overall, this work demonstrates an effective, learning-based approach to the design of algorithms for the numerical solution of differential equations, an approach that can be readily extended to other numerical tasks.

</p>
</details>

<details><summary><b>Drifting Features: Detection and evaluation in the context of automatic RRLs identification in VVV</b>
<a href="https://arxiv.org/abs/2105.01714">arxiv:2105.01714</a>
&#x1F4C8; 0 <br>
<p>J. B. Cabral, M. Lares, S. Gurovich, D. Minniti, P. M. Granitto</p></summary>
<p>

**Abstract:** As most of the modern astronomical sky surveys produce data faster than humans can analyze it, Machine Learning (ML) has become a central tool in Astronomy. Modern ML methods can be characterized as highly resistant to some experimental errors. However, small changes on the data over long distances or long periods of time, which cannot be easily detected by statistical methods, can be harmful to these methods. We develop a new strategy to cope with this problem, also using ML methods in an innovative way, to identify these potentially harmful features. We introduce and discuss the notion of Drifting Features, related with small changes in the properties as measured in the data features. We use the identification of RRLs in VVV based on an earlier work and introduce a method for detecting Drifting Features. Our method forces a classifier to learn the tile of origin of diverse sources (mostly stellar 'point sources'), and select the features more relevant to the task of finding candidates to Drifting Features. We show that this method can efficiently identify a reduced set of features that contains useful information about the tile of origin of the sources. For our particular example of detecting RRLs in VVV, we find that Drifting Features are mostly related to color indices. On the other hand, we show that, even if we have a clear set of Drifting Features in our problem, they are mostly insensitive to the identification of RRLs. Drifting Features can be efficiently identified using ML methods. However, in our example, removing Drifting Features does not improve the identification of RRLs.

</p>
</details>

<details><summary><b>Self-Improving Semantic Perception for Indoor Localisation</b>
<a href="https://arxiv.org/abs/2105.01595">arxiv:2105.01595</a>
&#x1F4C8; 0 <br>
<p>Hermann Blum, Francesco Milano, RenÃ© ZurbrÃ¼gg, Roland Siegward, Cesar Cadena, Abel Gawel</p></summary>
<p>

**Abstract:** We propose a novel robotic system that can improve its perception during deployment. Contrary to the established approach of learning semantics from large datasets and deploying fixed models, we propose a framework in which semantic models are continuously updated on the robot to adapt to the deployment environments. By combining continual learning with self-supervision, our robotic system learns online during deployment without external supervision. We conduct real-world experiments with robots localising in 3D floorplans. Our experiments show how the robot's semantic perception improves during deployment and how this translates into improved localisation, even across drastically different environments. We further study the risk of catastrophic forgetting that such a continuous learning setting poses. We find memory replay an effective measure to reduce forgetting and show how the robotic system can improve even when switching between different environments. On average, our system improves by 60% in segmentation and 10% in localisation accuracy compared to deployment of a fixed model, and it maintains this improvement while adapting to further environments.

</p>
</details>

<details><summary><b>Simplified Klinokinesis using Spiking Neural Networks for Resource-Constrained Navigation on the Neuromorphic Processor Loihi</b>
<a href="https://arxiv.org/abs/2105.01358">arxiv:2105.01358</a>
&#x1F4C8; 0 <br>
<p>Apoorv Kishore, Vivek Saraswat, Udayan Ganguly</p></summary>
<p>

**Abstract:** C. elegans shows chemotaxis using klinokinesis where the worm senses the concentration based on a single concentration sensor to compute the concentration gradient to perform foraging through gradient ascent/descent towards the target concentration followed by contour tracking. The biomimetic implementation requires complex neurons with multiple ion channel dynamics as well as interneurons for control. While this is a key capability of autonomous robots, its implementation on energy-efficient neuromorphic hardware like Intel's Loihi requires adaptation of the network to hardware-specific constraints, which has not been achieved. In this paper, we demonstrate the adaptation of chemotaxis based on klinokinesis to Loihi by implementing necessary neuronal dynamics with only LIF neurons as well as a complete spike-based implementation of all functions e.g. Heaviside function and subtractions. Our results show that Loihi implementation is equivalent to the software counterpart on Python in terms of performance - both during foraging and contour tracking. The Loihi results are also resilient in noisy environments. Thus, we demonstrate a successful adaptation of chemotaxis on Loihi - which can now be combined with the rich array of SNN blocks for SNN based complex robotic control.

</p>
</details>


[Next Page](2021/2021-05/2021-05-03.md)
