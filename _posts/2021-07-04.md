## Summary for 2021-07-04, created on 2021-12-19


<details><summary><b>Elastic Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2107.06996">arxiv:2107.06996</a>
&#x1F4C8; 9 <br>
<p>Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, Jiliang Tang</p></summary>
<p>

**Abstract:** While many existing graph neural networks (GNNs) have been proven to perform $\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\ell_1$ and $\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \url{https://github.com/lxiaorui/ElasticGNN}.

</p>
</details>

<details><summary><b>Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision</b>
<a href="https://arxiv.org/abs/2107.01784">arxiv:2107.01784</a>
&#x1F4C8; 8 <br>
<p>Robin Karlsson, David Robert Wong, Simon Thompson, Kazuya Takeda</p></summary>
<p>

**Abstract:** Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.

</p>
</details>

<details><summary><b>Mutation is all you need</b>
<a href="https://arxiv.org/abs/2107.07343">arxiv:2107.07343</a>
&#x1F4C8; 7 <br>
<p>Lennart Schneider, Florian Pfisterer, Martin Binder, Bernd Bischl</p></summary>
<p>

**Abstract:** Neural architecture search (NAS) promises to make deep learning accessible to non-experts by automating architecture engineering of deep neural networks. BANANAS is one state-of-the-art NAS method that is embedded within the Bayesian optimization framework. Recent experimental findings have demonstrated the strong performance of BANANAS on the NAS-Bench-101 benchmark being determined by its path encoding and not its choice of surrogate model. We present experimental results suggesting that the performance of BANANAS on the NAS-Bench-301 benchmark is determined by its acquisition function optimizer, which minimally mutates the incumbent.

</p>
</details>

<details><summary><b>Controllable cardiac synthesis via disentangled anatomy arithmetic</b>
<a href="https://arxiv.org/abs/2107.01748">arxiv:2107.01748</a>
&#x1F4C8; 5 <br>
<p>Spyridon Thermos, Xiao Liu, Alison O'Neil, Sotirios A. Tsaftaris</p></summary>
<p>

**Abstract:** Acquiring annotated data at scale with rare diseases or conditions remains a challenge. It would be extremely useful to have a method that controllably synthesizes images that can correct such underrepresentation. Assuming a proper latent representation, the idea of a "latent vector arithmetic" could offer the means of achieving such synthesis. A proper representation must encode the fidelity of the input data, preserve invariance and equivariance, and permit arithmetic operations. Motivated by the ability to disentangle images into spatial anatomy (tensor) factors and accompanying imaging (vector) representations, we propose a framework termed "disentangled anatomy arithmetic", in which a generative model learns to combine anatomical factors of different input images such that when they are re-entangled with the desired imaging modality (e.g. MRI), plausible new cardiac images are created with the target characteristics. To encourage a realistic combination of anatomy factors after the arithmetic step, we propose a localized noise injection network that precedes the generator. Our model is used to generate realistic images, pathology labels, and segmentation masks that are used to augment the existing datasets and subsequently improve post-hoc classification and segmentation tasks. Code is publicly available at https://github.com/vios-s/DAA-GAN.

</p>
</details>

<details><summary><b>Autoencoder based Randomized Learning of Feedforward Neural Networks for Regression</b>
<a href="https://arxiv.org/abs/2107.01711">arxiv:2107.01711</a>
&#x1F4C8; 5 <br>
<p>Grzegorz Dudek</p></summary>
<p>

**Abstract:** Feedforward neural networks are widely used as universal predictive models to fit data distribution. Common gradient-based learning, however, suffers from many drawbacks making the training process ineffective and time-consuming. Alternative randomized learning does not use gradients but selects hidden node parameters randomly. This makes the training process extremely fast. However, the problem in randomized learning is how to determine the random parameters. A recently proposed method uses autoencoders for unsupervised parameter learning. This method showed superior performance on classification tasks. In this work, we apply this method to regression problems, and, finding that it has some drawbacks, we show how to improve it. We propose a learning method of autoencoders that controls the produced random weights. We also propose how to determine the biases of hidden nodes. We empirically compare autoencoder based learning with other randomized learning methods proposed recently for regression and find that despite the proposed improvement of the autoencoder based learning, it does not outperform its competitors in fitting accuracy. Moreover, the method is much more complex than its competitors.

</p>
</details>

<details><summary><b>Randomized Neural Networks for Forecasting Time Series with Multiple Seasonality</b>
<a href="https://arxiv.org/abs/2107.01705">arxiv:2107.01705</a>
&#x1F4C8; 5 <br>
<p>Grzegorz Dudek</p></summary>
<p>

**Abstract:** This work contributes to the development of neural forecasting models with novel randomization-based learning methods. These methods improve the fitting abilities of the neural model, in comparison to the standard method, by generating network parameters in accordance with the data and target function features. A pattern-based representation of time series makes the proposed approach useful for forecasting time series with multiple seasonality. In the simulation study, we evaluate the performance of the proposed models and find that they can compete in terms of forecasting accuracy with fully-trained networks. Extremely fast and easy training, simple architecture, ease of implementation, high accuracy as well as dealing with nonstationarity and multiple seasonality in time series make the proposed model very attractive for a wide range of complex time series forecasting problems.

</p>
</details>

<details><summary><b>Data-Driven Learning of Feedforward Neural Networks with Different Activation Functions</b>
<a href="https://arxiv.org/abs/2107.01702">arxiv:2107.01702</a>
&#x1F4C8; 5 <br>
<p>Grzegorz Dudek</p></summary>
<p>

**Abstract:** This work contributes to the development of a new data-driven method (D-DM) of feedforward neural networks (FNNs) learning. This method was proposed recently as a way of improving randomized learning of FNNs by adjusting the network parameters to the target function fluctuations. The method employs logistic sigmoid activation functions for hidden nodes. In this study, we introduce other activation functions, such as bipolar sigmoid, sine function, saturating linear functions, reLU, and softplus. We derive formulas for their parameters, i.e. weights and biases. In the simulation study, we evaluate the performance of FNN data-driven learning with different activation functions. The results indicate that the sigmoid activation functions perform much better than others in the approximation of complex, fluctuated target functions.

</p>
</details>

<details><summary><b>Quantum Annealing Formulation for Binary Neural Networks</b>
<a href="https://arxiv.org/abs/2107.02751">arxiv:2107.02751</a>
&#x1F4C8; 4 <br>
<p>Michele Sasdelli, Tat-Jun Chin</p></summary>
<p>

**Abstract:** Quantum annealing is a promising paradigm for building practical quantum computers. Compared to other approaches, quantum annealing technology has been scaled up to a larger number of qubits. On the other hand, deep learning has been profoundly successful in pushing the boundaries of AI. It is thus natural to investigate potentially game changing technologies such as quantum annealers to augment the capabilities of deep learning. In this work, we explore binary neural networks, which are lightweight yet powerful models typically intended for resource constrained devices. Departing from current training regimes for binary networks that smooth/approximate the activation functions to make the network differentiable, we devise a quadratic unconstrained binary optimization formulation for the training problem. While the problem is intractable, i.e., the cost to estimate the binary weights scales exponentially with network size, we show how the problem can be optimized directly on a quantum annealer, thereby opening up to the potential gains of quantum computing. We experimentally validated our formulation via simulation and testing on an actual quantum annealer (D-Wave Advantage), the latter to the extent allowable by the capacity of current technology.

</p>
</details>

<details><summary><b>Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces</b>
<a href="https://arxiv.org/abs/2107.01667">arxiv:2107.01667</a>
&#x1F4C8; 4 <br>
<p>Nicolò Botteghi, Khaled Alaa, Mannes Poel, Beril Sirmacek, Christoph Brune, Abeje Mersha, Stefano Stramigioli</p></summary>
<p>

**Abstract:** Autonomous robots require high degrees of cognitive and motoric intelligence to come into our everyday life. In non-structured environments and in the presence of uncertainties, such degrees of intelligence are not easy to obtain. Reinforcement learning algorithms have proven to be capable of solving complicated robotics tasks in an end-to-end fashion without any need for hand-crafted features or policies. Especially in the context of robotics, in which the cost of real-world data is usually extremely high, reinforcement learning solutions achieving high sample efficiency are needed. In this paper, we propose a framework combining the learning of a low-dimensional state representation, from high-dimensional observations coming from the robot's raw sensory readings, with the learning of the optimal policy, given the learned state representation. We evaluate our framework in the context of mobile robot navigation in the case of continuous state and action spaces. Moreover, we study the problem of transferring what learned in the simulated virtual environment to the real robot without further retraining using real-world data in the presence of visual and depth distractors, such as lighting changes and moving obstacles.

</p>
</details>

<details><summary><b>The Role of "Live" in Livestreaming Markets: Evidence Using Orthogonal Random Forest</b>
<a href="https://arxiv.org/abs/2107.01629">arxiv:2107.01629</a>
&#x1F4C8; 4 <br>
<p>Ziwei Cong, Jia Liu, Puneet Manchanda</p></summary>
<p>

**Abstract:** The common belief about the growing medium of livestreaming is that its value lies in its "live" component. In this paper, we leverage data from a large livestreaming platform to examine this belief. We are able to do this as this platform also allows viewers to purchase the recorded version of the livestream. We summarize the value of livestreaming content by estimating how demand responds to price before, on the day of, and after the livestream. We do this by proposing a generalized Orthogonal Random Forest framework. This framework allows us to estimate heterogeneous treatment effects in the presence of high-dimensional confounders whose relationships with the treatment policy (i.e., price) are complex but partially known. We find significant dynamics in the price elasticity of demand over the temporal distance to the scheduled livestreaming day and after. Specifically, demand gradually becomes less price sensitive over time to the livestreaming day and is inelastic on the livestreaming day. Over the post-livestream period, demand is still sensitive to price, but much less than the pre-livestream period. This indicates that the vlaue of livestreaming persists beyond the live component. Finally, we provide suggestive evidence for the likely mechanisms driving our results. These are quality uncertainty reduction for the patterns pre- and post-livestream and the potential of real-time interaction with the creator on the day of the livestream.

</p>
</details>

<details><summary><b>Learning in nonatomic games, Part I: Finite action spaces and population games</b>
<a href="https://arxiv.org/abs/2107.01595">arxiv:2107.01595</a>
&#x1F4C8; 4 <br>
<p>Saeed Hadikhanloo, Rida Laraki, Panayotis Mertikopoulos, Sylvain Sorin</p></summary>
<p>

**Abstract:** We examine the long-run behavior of a wide range of dynamics for learning in nonatomic games, in both discrete and continuous time. The class of dynamics under consideration includes fictitious play and its regularized variants, the best-reply dynamics (again, possibly regularized), as well as the dynamics of dual averaging / "follow the regularized leader" (which themselves include as special cases the replicator dynamics and Friedman's projection dynamics). Our analysis concerns both the actual trajectory of play and its time-average, and we cover potential and monotone games, as well as games with an evolutionarily stable state (global or otherwise). We focus exclusively on games with finite action spaces; nonatomic games with continuous action spaces are treated in detail in Part II of this paper.

</p>
</details>

<details><summary><b>Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition</b>
<a href="https://arxiv.org/abs/2107.01569">arxiv:2107.01569</a>
&#x1F4C8; 4 <br>
<p>Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko Takashima, Takafumi Moriya, Takanori Ashihara, Shota Orihashi, Naoki Makishima</p></summary>
<p>

**Abstract:** We propose a cross-modal transformer-based neural correction models that refines the output of an automatic speech recognition (ASR) system so as to exclude ASR errors. Generally, neural correction models are composed of encoder-decoder networks, which can directly model sequence-to-sequence mapping problems. The most successful method is to use both input speech and its ASR output text as the input contexts for the encoder-decoder networks. However, the conventional method cannot take into account the relationships between these two different modal inputs because the input contexts are separately encoded for each modal. To effectively leverage the correlated information between the two different modal inputs, our proposed models encode two different contexts jointly on the basis of cross-modal self-attention using a transformer. We expect that cross-modal self-attention can effectively capture the relationships between two different modals for refining ASR hypotheses. We also introduce a shallow fusion technique to efficiently integrate the first-pass ASR model and our proposed neural correction model. Experiments on Japanese natural language ASR tasks demonstrated that our proposed models achieve better ASR performance than conventional neural correction models.

</p>
</details>

<details><summary><b>Certifiably Robust Interpretation via Renyi Differential Privacy</b>
<a href="https://arxiv.org/abs/2107.01561">arxiv:2107.01561</a>
&#x1F4C8; 4 <br>
<p>Ao Liu, Xiaoyu Chen, Sijia Liu, Lirong Xia, Chuang Gan</p></summary>
<p>

**Abstract:** Motivated by the recent discovery that the interpretation maps of CNNs could easily be manipulated by adversarial attacks against network interpretability, we study the problem of interpretation robustness from a new perspective of \Renyi differential privacy (RDP). The advantages of our Renyi-Robust-Smooth (RDP-based interpretation method) are three-folds. First, it can offer provable and certifiable top-$k$ robustness. That is, the top-$k$ important attributions of the interpretation map are provably robust under any input perturbation with bounded $\ell_d$-norm (for any $d\geq 1$, including $d = \infty$). Second, our proposed method offers $\sim10\%$ better experimental robustness than existing approaches in terms of the top-$k$ attributions. Remarkably, the accuracy of Renyi-Robust-Smooth also outperforms existing approaches. Third, our method can provide a smooth tradeoff between robustness and computational efficiency. Experimentally, its top-$k$ attributions are {\em twice} more robust than existing approaches when the computational resources are highly constrained.

</p>
</details>

<details><summary><b>DebiasedDTA: Model Debiasing to Boost Drug-Target Affinity Prediction</b>
<a href="https://arxiv.org/abs/2107.05556">arxiv:2107.05556</a>
&#x1F4C8; 3 <br>
<p>Rıza Özçelik, Alperen Bağ, Berk Atıl, Arzucan Özgür, Elif Özkırımlı</p></summary>
<p>

**Abstract:** Motivation: Computational models that accurately identify high-affinity protein-compound pairs can accelerate drug discovery pipelines. These models aim to learn binding mechanics through drug-target interaction datasets and use the learned knowledge for predicting the affinity of an input protein-compound pair. However, the datasets they rely on bear misleading patterns that bias models towards memorizing dataset-specific biomolecule properties, instead of learning binding mechanics. This results in models that struggle while predicting drug-target affinities (DTA), especially between de novo biomolecules. Here we present DebiasedDTA, the first DTA model debiasing approach that avoids dataset biases in order to boost affinity prediction for novel biomolecules. DebiasedDTA uses ensemble learning and sample weight adaptation for bias identification and avoidance and is applicable to almost all existing DTA prediction models. Results: The results show that DebiasedDTA can boost models while predicting the interactions between novel biomolecules. Known biomolecules also benefit from the performance improvement, especially when the test biomolecules are dissimilar to the training set. The experiments also show that DebiasedDTA can augment DTA prediction models of different input and model structures and is able to avoid biases of different sources. Availability and Implementation: The source code, the models, and the datasets are freely available for download at https://github.com/boun-tabi/debiaseddta-reproduce, implementation in Python3, and supported for Linux, MacOS and MS Windows. Contact: arzucan.ozgur@boun.edu.tr, elif.ozkirimli@roche.com

</p>
</details>

<details><summary><b>A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources</b>
<a href="https://arxiv.org/abs/2107.01782">arxiv:2107.01782</a>
&#x1F4C8; 3 <br>
<p>Tidor-Vlad Pricope</p></summary>
<p>

**Abstract:** Classifying hand-written digits and letters has taken a big leap with the introduction of ConvNets. However, on very constrained hardware the time necessary to train such models would be high. Our main contribution is twofold. First, we extensively test an end-to-end vanilla neural network (MLP) approach in pure numpy without any pre-processing or feature extraction done beforehand. Second, we show that basic data mining operations can significantly improve the performance of the models in terms of computational time, without sacrificing much accuracy. We illustrate our claims on a simpler variant of the Extended MNIST dataset, called Balanced EMNIST dataset. Our experiments show that, without any data mining, we get increased generalization performance when using more hidden layers and regularization techniques, the best model achieving 84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA we were able to increase that figure to 85.08% with only 10% of the original feature space, reducing the memory size needed by 64%. Finally, adding methods to remove possibly harmful training samples like deviation from the mean helped us to still achieve over 84% test accuracy but with only 32.8% of the original memory size for the training set. This compares favorably to the majority of literature results obtained through similar architectures. Although this approach gets outshined by state-of-the-art models, it does scale to some (AlexNet, VGGNet) trained on 50% of the same dataset.

</p>
</details>

<details><summary><b>Continual Contrastive Self-supervised Learning for Image Classification</b>
<a href="https://arxiv.org/abs/2107.01776">arxiv:2107.01776</a>
&#x1F4C8; 3 <br>
<p>Zhiwei Lin, Yongtao Wang, Hongxiang Lin</p></summary>
<p>

**Abstract:** For artificial learning systems, continual learning over time from a stream of data is essential. The burgeoning studies on supervised continual learning have achieved great progress, while the study of catastrophic forgetting in unsupervised learning is still blank. Among unsupervised learning methods, self-supervise learning method shows tremendous potential on visual representation without any labeled data at scale. To improve the visual representation of self-supervised learning, larger and more varied data is needed. In the real world, unlabeled data is generated at all times. This circumstance provides a huge advantage for the learning of the self-supervised method. However, in the current paradigm, packing previous data and current data together and training it again is a waste of time and resources. Thus, a continual self-supervised learning method is badly needed. In this paper, we make the first attempt to implement the continual contrastive self-supervised learning by proposing a rehearsal method, which keeps a few exemplars from the previous data. Instead of directly combining saved exemplars with the current data set for training, we leverage self-supervised knowledge distillation to transfer contrastive information among previous data to the current network by mimicking similarity score distribution inferred by the old network over a set of saved exemplars. Moreover, we build an extra sample queue to assist the network to distinguish between previous and current data and prevent mutual interference while learning their own feature representation. Experimental results show that our method performs well on CIFAR100 and ImageNet-Sub. Compared with the baselines, which learning tasks without taking any technique, we improve the image classification top-1 accuracy by 1.60% on CIFAR100, 2.86% on ImageNet-Sub and 1.29% on ImageNet-Full under 10 incremental steps setting.

</p>
</details>

<details><summary><b>IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task</b>
<a href="https://arxiv.org/abs/2107.01656">arxiv:2107.01656</a>
&#x1F4C8; 3 <br>
<p>Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal</p></summary>
<p>

**Abstract:** Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.

</p>
</details>

<details><summary><b>Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation</b>
<a href="https://arxiv.org/abs/2107.01598">arxiv:2107.01598</a>
&#x1F4C8; 3 <br>
<p>Mohammad Rostami, Aram Galstyan</p></summary>
<p>

**Abstract:** Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to improve their products and to determine optimal marketing strategies. Due to the existence of a wide range of domains across different products and services, cross-domain sentiment analysis methods have received significant attention. These methods mitigate the domain gap between different applications by training cross-domain generalizable classifiers which help to relax the need for data annotation for each domain. Most existing methods focus on learning domain-agnostic representations that are invariant with respect to both the source and the target domains. As a result, a classifier that is trained using the source domain annotated data would generalize well in a related target domain. We introduce a new domain adaptation method which induces large margins between different classes in an embedding space. This embedding space is trained to be domain-agnostic by matching the data distributions across the domains. Large intraclass margins in the source domain help to reduce the effect of "domain shift" on the classifier performance in the target domain. Theoretical and empirical analysis are provided to demonstrate that the proposed method is effective.

</p>
</details>

<details><summary><b>DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection</b>
<a href="https://arxiv.org/abs/2107.10648">arxiv:2107.10648</a>
&#x1F4C8; 2 <br>
<p>Mohit Mayank, Shakshi Sharma, Rajesh Sharma</p></summary>
<p>

**Abstract:** Fake News on social media platforms has attracted a lot of attention in recent times, primarily for events related to politics (2016 US Presidential elections), healthcare (infodemic during COVID-19), to name a few. Various methods have been proposed for detecting Fake News. The approaches span from exploiting techniques related to network analysis, Natural Language Processing (NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose DEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying Fake News. Our approach is a combination of the NLP -- where we encode the news content, and the GNN technique -- where we encode the Knowledge Graph (KG). A variety of these encodings provides a complementary advantage to our detector. We evaluate our framework using two publicly available datasets containing articles from domains such as politics, business, technology, and healthcare. As part of dataset pre-processing, we also remove the bias, such as the source of the articles, which could impact the performance of the models. DEAP-FAKED obtains an F1-score of 88% and 78% for the two datasets, which is an improvement of 21%, and 3% respectively, which shows the effectiveness of the approach.

</p>
</details>

<details><summary><b>Generalisation in Neural Networks Does not Require Feature Overlap</b>
<a href="https://arxiv.org/abs/2107.06872">arxiv:2107.06872</a>
&#x1F4C8; 2 <br>
<p>Jeff Mitchell, Jeffrey S. Bowers</p></summary>
<p>

**Abstract:** That shared features between train and test data are required for generalisation in artificial neural networks has been a common assumption of both proponents and critics of these models. Here, we show that convolutional architectures avoid this limitation by applying them to two well known challenges, based on learning the identity function and learning rules governing sequences of words. In each case, successful performance on the test set requires generalising to features that were not present in the training data, which is typically not feasible for standard connectionist models. However, our experiments demonstrate that neural networks can succeed on such problems when they incorporate the weight sharing employed by convolutional architectures. In the image processing domain, such architectures are intended to reflect the symmetry under spatial translations of the natural world that such images depict. We discuss the role of symmetry in the two tasks and its connection to generalisation.

</p>
</details>

<details><summary><b>Statistical Theory for Imbalanced Binary Classification</b>
<a href="https://arxiv.org/abs/2107.01777">arxiv:2107.01777</a>
&#x1F4C8; 2 <br>
<p>Shashank Singh, Justin Khim</p></summary>
<p>

**Abstract:** Within the vast body of statistical theory developed for binary classification, few meaningful results exist for imbalanced classification, in which data are dominated by samples from one of the two classes. Existing theory faces at least two main challenges. First, meaningful results must consider more complex performance measures than classification accuracy. To address this, we characterize a novel generalization of the Bayes-optimal classifier to any performance metric computed from the confusion matrix, and we use this to show how relative performance guarantees can be obtained in terms of the error of estimating the class probability function under uniform ($\mathcal{L}_\infty$) loss. Second, as we show, optimal classification performance depends on certain properties of class imbalance that have not previously been formalized. Specifically, we propose a novel sub-type of class imbalance, which we call Uniform Class Imbalance. We analyze how Uniform Class Imbalance influences optimal classifier performance and show that it necessitates different classifier behavior than other types of class imbalance. We further illustrate these two contributions in the case of $k$-nearest neighbor classification, for which we develop novel guarantees. Together, these results provide some of the first meaningful finite-sample statistical theory for imbalanced binary classification.

</p>
</details>

<details><summary><b>Single Model for Influenza Forecasting of Multiple Countries by Multi-task Learning</b>
<a href="https://arxiv.org/abs/2107.01760">arxiv:2107.01760</a>
&#x1F4C8; 2 <br>
<p>Taichi Murayama, Shoko Wakamiya, Eiji Aramaki</p></summary>
<p>

**Abstract:** The accurate forecasting of infectious epidemic diseases such as influenza is a crucial task undertaken by medical institutions. Although numerous flu forecasting methods and models based mainly on historical flu activity data and online user-generated contents have been proposed in previous studies, no flu forecasting model targeting multiple countries using two types of data exists at present. Our paper leverages multi-task learning to tackle the challenge of building one flu forecasting model targeting multiple countries; each country as each task. Also, to develop the flu prediction model with higher performance, we solved two issues; finding suitable search queries, which are part of the user-generated contents, and how to leverage search queries efficiently in the model creation. For the first issue, we propose the transfer approaches from English to other languages. For the second issue, we propose a novel flu forecasting model that takes advantage of search queries using an attention mechanism and extend the model to a multi-task model for multiple countries' flu forecasts. Experiments on forecasting flu epidemics in five countries demonstrate that our model significantly improved the performance by leveraging the search queries and multi-task learning compared to the baselines.

</p>
</details>

<details><summary><b>Polymorphic dynamic programming by algebraic shortcut fusion</b>
<a href="https://arxiv.org/abs/2107.01752">arxiv:2107.01752</a>
&#x1F4C8; 2 <br>
<p>Max A. Little, Ugur Kayas</p></summary>
<p>

**Abstract:** Dynamic programming (DP) is a broadly applicable algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, the design of such algorithms is often presented informally in an ad-hoc manner, and as a result is often difficult to apply correctly. In this paper, we present a rigorous algebraic formalism for systematically deriving novel DP algorithms, either from existing DP algorithms or from simple functional recurrences. These derivations lead to algorithms which are provably correct and polymorphic over any semiring, which means that they can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimization, optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, and relational and provenance queries. The approach, building on many ideas from the existing literature on constructive algorithmics, exploits generic properties of (semiring) polymorphic functions, tupling and formal sums (lifting), and algebraic simplifications arising from constraint algebras. We demonstrate the effectiveness of this formalism for some example applications arising in signal processing, bioinformatics and reliability engineering.

</p>
</details>

<details><summary><b>Latent structure blockmodels for Bayesian spectral graph clustering</b>
<a href="https://arxiv.org/abs/2107.01734">arxiv:2107.01734</a>
&#x1F4C8; 2 <br>
<p>Francesco Sanna Passino, Nicholas A. Heard</p></summary>
<p>

**Abstract:** Spectral embedding of network adjacency matrices often produces node representations living approximately around low-dimensional submanifold structures. In particular, hidden substructure is expected to arise when the graph is generated from a latent position model. Furthermore, the presence of communities within the network might generate community-specific submanifold structures in the embedding, but this is not explicitly accounted for in most statistical models for networks. In this article, a class of models called latent structure block models (LSBM) is proposed to address such scenarios, allowing for graph clustering when community-specific one dimensional manifold structure is present. LSBMs focus on a specific class of latent space model, the random dot product graph (RDPG), and assign a latent submanifold to the latent positions of each community. A Bayesian model for the embeddings arising from LSBMs is discussed, and shown to have a good performance on simulated and real world network data. The model is able to correctly recover the underlying communities living in a one-dimensional manifold, even when the parametric form of the underlying curves is unknown, achieving remarkable results on a variety of real data.

</p>
</details>

<details><summary><b>COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models</b>
<a href="https://arxiv.org/abs/2107.01682">arxiv:2107.01682</a>
&#x1F4C8; 2 <br>
<p>Xiaohong Gao, Yu Qian, Alice Gao</p></summary>
<p>

**Abstract:** This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.

</p>
</details>

<details><summary><b>Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics</b>
<a href="https://arxiv.org/abs/2107.01677">arxiv:2107.01677</a>
&#x1F4C8; 2 <br>
<p>Nicolò Botteghi, Mannes Poel, Beril Sirmacek, Christoph Brune</p></summary>
<p>

**Abstract:** Deep Reinforcement Learning has shown its ability in solving complicated problems directly from high-dimensional observations. However, in end-to-end settings, Reinforcement Learning algorithms are not sample-efficient and requires long training times and quantities of data. In this work, we proposed a framework for sample-efficient Reinforcement Learning that take advantage of state and action representations to transform a high-dimensional problem into a low-dimensional one. Moreover, we seek to find the optimal policy mapping latent states to latent actions. Because now the policy is learned on abstract representations, we enforce, using auxiliary loss functions, the lifting of such policy to the original problem domain. Results show that the novel framework can efficiently learn low-dimensional and interpretable state and action representations and the optimal latent policy.

</p>
</details>

<details><summary><b>Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method</b>
<a href="https://arxiv.org/abs/2107.01658">arxiv:2107.01658</a>
&#x1F4C8; 2 <br>
<p>Aramayis Dallakyan, Mohsen Pourahmadi</p></summary>
<p>

**Abstract:** We establish a novel framework for learning a directed acyclic graph (DAG) when data are generated from a Gaussian, linear structural equation model. It consists of two parts: (1) introduce a permutation matrix as a new parameter within a regularized Gaussian log-likelihood to represent variable ordering; and (2) given the ordering, estimate the DAG structure through sparse Cholesky factor of the inverse covariance matrix. For permutation matrix estimation, we propose a relaxation technique that avoids the NP-hard combinatorial problem of order estimation. Given an ordering, a sparse Cholesky factor is estimated using a cyclic coordinatewise descent algorithm which decouples row-wise. Our framework recovers DAGs without the need for an expensive verification of the acyclicity constraint or enumeration of possible parent sets. We establish numerical convergence of the algorithm, and consistency of the Cholesky factor estimator when the order of variables is known. Through several simulated and macro-economic datasets, we study the scope and performance of the proposed methodology.

</p>
</details>

<details><summary><b>Auxiliary-Classifier GAN for Malware Analysis</b>
<a href="https://arxiv.org/abs/2107.01620">arxiv:2107.01620</a>
&#x1F4C8; 2 <br>
<p>Rakesh Nagaraju, Mark Stamp</p></summary>
<p>

**Abstract:** Generative adversarial networks (GAN) are a class of powerful machine learning techniques, where both a generative and discriminative model are trained simultaneously. GANs have been used, for example, to successfully generate "deep fake" images. A recent trend in malware research consists of treating executables as images and employing image-based analysis techniques. In this research, we generate fake malware images using auxiliary classifier GANs (AC-GAN), and we consider the effectiveness of various techniques for classifying the resulting images. Our results indicate that the resulting multiclass classification problem is challenging, yet we can obtain strong results when restricting the problem to distinguishing between real and fake samples. While the AC-GAN generated images often appear to be very similar to real malware images, we conclude that from a deep learning perspective, the AC-GAN generated samples do not rise to the level of deep fake malware images.

</p>
</details>

<details><summary><b>A Typology of Data Anomalies</b>
<a href="https://arxiv.org/abs/2107.01615">arxiv:2107.01615</a>
&#x1F4C8; 2 <br>
<p>Ralph Foorthuis</p></summary>
<p>

**Abstract:** Anomalies are cases that are in some way unusual and do not appear to fit the general patterns present in the dataset. Several conceptualizations exist to distinguish between different types of anomalies. However, these are either too specific to be generally applicable or so abstract that they neither provide concrete insight into the nature of anomaly types nor facilitate the functional evaluation of anomaly detection algorithms. With the recent criticism on 'black box' algorithms and analytics it has become clear that this is an undesirable situation. This paper therefore introduces a general typology of anomalies that offers a clear and tangible definition of the different types of anomalies in datasets. The typology also facilitates the evaluation of the functional capabilities of anomaly detection algorithms and as a framework assists in analyzing the conceptual levels of data, patterns and anomalies. Finally, it serves as an analytical tool for studying anomaly types from other typologies.

</p>
</details>

<details><summary><b>A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification</b>
<a href="https://arxiv.org/abs/2107.01606">arxiv:2107.01606</a>
&#x1F4C8; 2 <br>
<p>Geir K. Nilsen, Antonella Z. Munthe-Kaas, Hans J. Skaug, Morten Brun</p></summary>
<p>

**Abstract:** We validate the recently introduced deep learning classification adapted Delta method by a comparison with the classical Bootstrap. We show that there is a strong linear relationship between the quantified predictive epistemic uncertainty levels obtained from the two methods when applied on two LeNet-based neural network classifiers using the MNIST and CIFAR-10 datasets. Furthermore, we demonstrate that the Delta method offers a five times computation time reduction compared to the Bootstrap.

</p>
</details>

<details><summary><b>Deep Gaussian Process Emulation using Stochastic Imputation</b>
<a href="https://arxiv.org/abs/2107.01590">arxiv:2107.01590</a>
&#x1F4C8; 2 <br>
<p>Deyu Ming, Daniel Williamson, Serge Guillas</p></summary>
<p>

**Abstract:** We propose a novel deep Gaussian process (DGP) inference method for computer model emulation using stochastic imputation. By stochastically imputing the latent layers, the approach transforms the DGP into the linked GP, a state-of-the-art surrogate model formed by linking a system of feed-forward coupled GPs. This transformation renders a simple while efficient DGP training procedure that only involves optimizations of conventional stationary GPs. In addition, the analytically tractable mean and variance of the linked GP allows one to implement predictions from DGP emulators in a fast and accurate manner. We demonstrate the method in a series of synthetic examples and real-world applications, and show that it is a competitive candidate for efficient DGP surrogate modeling in comparison to the variational inference and the fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$ implementing the method is also produced and available at https://github.com/mingdeyu/DGP.

</p>
</details>

<details><summary><b>Random Neural Networks in the Infinite Width Limit as Gaussian Processes</b>
<a href="https://arxiv.org/abs/2107.01562">arxiv:2107.01562</a>
&#x1F4C8; 2 <br>
<p>Boris Hanin</p></summary>
<p>

**Abstract:** This article gives a new proof that fully connected neural networks with random weights and biases converge to Gaussian processes in the regime where the input dimension, output dimension, and depth are kept fixed, while the hidden layer widths tend to infinity. Unlike prior work, convergence is shown assuming only moment conditions for the distribution of weights and for quite general non-linearities.

</p>
</details>

<details><summary><b>Leveraging Evidential Deep Learning Uncertainties with Graph-based Clustering to Detect Anomalies</b>
<a href="https://arxiv.org/abs/2107.01557">arxiv:2107.01557</a>
&#x1F4C8; 2 <br>
<p>Sandeep Kumar Singh, Jaya Shradha Fowdur, Jakob Gawlikowski, Daniel Medina</p></summary>
<p>

**Abstract:** Understanding and representing traffic patterns are key to detecting anomalies in the maritime domain. To this end, we propose a novel graph-based traffic representation and association scheme to cluster trajectories of vessels using automatic identification system (AIS) data. We utilize the (un)clustered data to train a recurrent neural network (RNN)-based evidential regression model, which can predict a vessel's trajectory at future timesteps with its corresponding prediction uncertainty. This paper proposes the usage of a deep learning (DL)-based uncertainty estimation in detecting maritime anomalies, such as unusual vessel maneuvering. Furthermore, we utilize the evidential deep learning classifiers to detect unusual turns of vessels and the loss of AIS signal using predicted class probabilities with associated uncertainties. Our experimental results suggest that using graph-based clustered data improves the ability of the DL models to learn the temporal-spatial correlation of data and associated uncertainties. Using different AIS datasets and experiments, we demonstrate that the estimated prediction uncertainty yields fundamental information for the detection of traffic anomalies in the maritime and, possibly in other domains.

</p>
</details>

<details><summary><b>Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules</b>
<a href="https://arxiv.org/abs/2107.06773">arxiv:2107.06773</a>
&#x1F4C8; 1 <br>
<p>Yan Ding, Xiaoqian Jiang, Yejin Kim</p></summary>
<p>

**Abstract:** The evaluation of the BBB penetrating ability of drug molecules is a critical step in brain drug development. Computational prediction based on machine learning has proved to be an efficient way to conduct the evaluation. However, performance of the established models has been limited by their incapability of dealing with the interactions between drugs and proteins, which play an important role in the mechanism behind BBB penetrating behaviors. To address this issue, we employed the relational graph convolutional network (RGCN) to handle the drug-protein (denoted by the encoding gene) relations as well as the features of each individual drug. In addition, drug-drug similarity was also introduced to connect structurally similar drugs in the graph. The RGCN model was initially trained without input of any drug features. And the performance was already promising, demonstrating the significant role of the drug-protein/drug-drug relations in the prediction of BBB permeability. Moreover, molecular embeddings from a pre-trained knowledge graph were used as the drug features to further enhance the predictive ability of the model. Finally, the best performing RGCN model was built with a large number of unlabeled drugs integrated into the graph.

</p>
</details>

<details><summary><b>Learning Delaunay Triangulation using Self-attention and Domain Knowledge</b>
<a href="https://arxiv.org/abs/2107.01759">arxiv:2107.01759</a>
&#x1F4C8; 1 <br>
<p>Jaeseung Lee, Woojin Choi, Jibum Kim</p></summary>
<p>

**Abstract:** Delaunay triangulation is a well-known geometric combinatorial optimization problem with various applications. Many algorithms can generate Delaunay triangulation given an input point set, but most are nontrivial algorithms requiring an understanding of geometry or the performance of additional geometric operations, such as the edge flip. Deep learning has been used to solve various combinatorial optimization problems; however, generating Delaunay triangulation based on deep learning remains a difficult problem, and very few research has been conducted due to its complexity. In this paper, we propose a novel deep-learning-based approach for learning Delaunay triangulation using a new attention mechanism based on self-attention and domain knowledge. The proposed model is designed such that the model efficiently learns point-to-point relationships using self-attention in the encoder. In the decoder, a new attention score function using domain knowledge is proposed to provide a high penalty when the geometric requirement is not satisfied. The strength of the proposed attention score function lies in its ability to extend its application to solving other combinatorial optimization problems involving geometry. When the proposed neural net model is well trained, it is simple and efficient because it automatically predicts the Delaunay triangulation for an input point set without requiring any additional geometric operations. We conduct experiments to demonstrate the effectiveness of the proposed model and conclude that it exhibits better performance compared with other deep-learning-based approaches.

</p>
</details>

<details><summary><b>Attribute-aware Explainable Complementary Clothing Recommendation</b>
<a href="https://arxiv.org/abs/2107.01655">arxiv:2107.01655</a>
&#x1F4C8; 1 <br>
<p>Yang Li, Tong Chen, Zi Huang</p></summary>
<p>

**Abstract:** Modelling mix-and-match relationships among fashion items has become increasingly demanding yet challenging for modern E-commerce recommender systems. When performing clothes matching, most existing approaches leverage the latent visual features extracted from fashion item images for compatibility modelling, which lacks explainability of generated matching results and can hardly convince users of the recommendations. Though recent methods start to incorporate pre-defined attribute information (e.g., colour, style, length, etc.) for learning item representations and improving the model interpretability, their utilisation of attribute information is still mainly reserved for enhancing the learned item representations and generating explanations via post-processing. As a result, this creates a severe bottleneck when we are trying to advance the recommendation accuracy and generating fine-grained explanations since the explicit attributes have only loose connections to the actual recommendation process. This work aims to tackle the explainability challenge in fashion recommendation tasks by proposing a novel Attribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender assesses the outfit compatibility by explicitly leveraging the extracted attribute-level representations from each item's visual feature. The attributes serve as the bridge between two fashion items, where we quantify the affinity of a pair of items through the learned compatibility between their attributes. Extensive experiments have demonstrated that, by making full use of the explicit attributes in the recommendation process, AFRec is able to achieve state-of-the-art recommendation accuracy and generate intuitive explanations at the same time.

</p>
</details>

<details><summary><b>Machine Learning for Malware Evolution Detection</b>
<a href="https://arxiv.org/abs/2107.01627">arxiv:2107.01627</a>
&#x1F4C8; 1 <br>
<p>Lolitha Sresta Tupadha, Mark Stamp</p></summary>
<p>

**Abstract:** Malware evolves over time and antivirus must adapt to such evolution. Hence, it is critical to detect those points in time where malware has evolved so that appropriate countermeasures can be undertaken. In this research, we perform a variety of experiments on a significant number of malware families to determine when malware evolution is likely to have occurred. All of the evolution detection techniques that we consider are based on machine learning and can be fully automated -- in particular, no reverse engineering or other labor-intensive manual analysis is required. Specifically, we consider analysis based on hidden Markov models (HMM) and the word embedding techniques HMM2Vec and Word2Vec.

</p>
</details>

<details><summary><b>The Composability of Intermediate Values in Composable Inductive Programming</b>
<a href="https://arxiv.org/abs/2107.01621">arxiv:2107.01621</a>
&#x1F4C8; 1 <br>
<p>Edward McDaid, Sarah McDaid</p></summary>
<p>

**Abstract:** It is believed that mechanisms including intermediate values enable composable inductive programming (CIP) to be used to produce software of any size. We present the results of a study that investigated the relationships between program size, the number of intermediate values and the number of test cases used to specify programs using CIP. In the study 96,000 programs of various sizes were randomly generated, decomposed into fragments and transformed into test cases. The test cases were then used to regenerate new versions of the original programs using Zoea. The results show linear relationships between the number of intermediate values and regenerated program size, and between the number of test cases and regenerated program size within the size range studied. In addition, as program size increases there is increasing scope for trading off the number of test cases against the number of intermediate values and vice versa.

</p>
</details>

<details><summary><b>Smoothed Differential Privacy</b>
<a href="https://arxiv.org/abs/2107.01559">arxiv:2107.01559</a>
&#x1F4C8; 1 <br>
<p>Ao Liu, Lirong Xia</p></summary>
<p>

**Abstract:** Differential privacy (DP) is a widely-accepted and widely-applied notion of privacy based on worst-case analysis. Often, DP classifies most mechanisms without external noise as non-private [Dwork et al., 2014], and external noises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are introduced to improve privacy. In many real-world applications, however, adding external noise is undesirable and sometimes prohibited. For example, presidential elections often require a deterministic rule to be used [Liu et al., 2020], and small noises can lead to dramatic decreases in the prediction accuracy of deep neural networks, especially the underrepresented classes [Bagdasaryan et al., 2019].
  In this paper, we propose a natural extension and relaxation of DP following the worst average-case idea behind the celebrated smoothed analysis [Spielman and Teng, 2004]. Our notion, the smoothed DP, can effectively measure the privacy leakage of mechanisms without external noises under realistic settings.
  We prove several strong properties of the smoothed DP, including composability, robustness to post-processing and etc. We proved that any discrete mechanism with sampling procedures is more private than what DP predicts. In comparison, many continuous mechanisms with sampling procedures are still non-private under smoothed DP. Experimentally, we first verified that the discrete sampling mechanisms are private in real-world elections. Then, we apply the smoothed DP notion on quantized gradient descent, which indicates some neural networks can be private without adding any extra noises. We believe that these results contribute to the theoretical foundation of realistic privacy measures beyond worst-case analysis.

</p>
</details>


[Next Page](2021/2021-07/2021-07-03.md)
