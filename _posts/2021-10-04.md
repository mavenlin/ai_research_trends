## Summary for 2021-10-04, created on 2021-12-16


<details><summary><b>Blindness (Diabetic Retinopathy) Severity Scale Detection</b>
<a href="https://arxiv.org/abs/2110.01333">arxiv:2110.01333</a>
&#x1F4C8; 76 <br>
<p>Ramya Bygari, Rachita Naik, Uday Kumar P</p></summary>
<p>

**Abstract:** Diabetic retinopathy (DR) is a severe complication of diabetes that can cause permanent blindness. Timely diagnosis and treatment of DR are critical to avoid total loss of vision. Manual diagnosis is time consuming and error-prone. In this paper, we propose a novel deep learning based method for automatic screening of retinal fundus images to detect and classify DR based on the severity. The method uses a dual-path configuration of deep neural networks to achieve the objective. In the first step, a modified UNet++ based retinal vessel segmentation is used to create a fundus image that emphasises elements like haemorrhages, cotton wool spots, and exudates that are vital to identify the DR stages. Subsequently, two convolutional neural networks (CNN) classifiers take the original image and the newly created fundus image respectively as inputs and identify the severity of DR on a scale of 0 to 4. These two scores are then passed through a shallow neural network classifier (ANN) to predict the final DR stage. The public datasets STARE, DRIVE, CHASE DB1, and APTOS are used for training and evaluation. Our method achieves an accuracy of 94.80% and Quadratic Weighted Kappa (QWK) score of 0.9254, and outperform many state-of-the-art methods.

</p>
</details>

<details><summary><b>Inductive learning for product assortment graph completion</b>
<a href="https://arxiv.org/abs/2110.01677">arxiv:2110.01677</a>
&#x1F4C8; 71 <br>
<p>Haris Dukic, Georgios Deligiorgis, Pierpaolo Sepe, Davide Bacciu, Marco Trincavelli</p></summary>
<p>

**Abstract:** Global retailers have assortments that contain hundreds of thousands of products that can be linked by several types of relationships like style compatibility, "bought together", "watched together", etc. Graphs are a natural representation for assortments, where products are nodes and relations are edges. Relations like style compatibility are often produced by a manual process and therefore do not cover uniformly the whole graph. We propose to use inductive learning to enhance a graph encoding style compatibility of a fashion assortment, leveraging rich node information comprising textual descriptions and visual data. Then, we show how the proposed graph enhancement improves substantially the performance on transductive tasks with a minor impact on graph sparsity.

</p>
</details>

<details><summary><b>Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble</b>
<a href="https://arxiv.org/abs/2110.01548">arxiv:2110.01548</a>
&#x1F4C8; 48 <br>
<p>Gaon An, Seungyong Moon, Jang-Hyun Kim, Hyun Oh Song</p></summary>
<p>

**Abstract:** Offline reinforcement learning (offline RL), which aims to find an optimal policy from a previously collected static dataset, bears algorithmic difficulties due to function approximation errors from out-of-distribution (OOD) data points. To this end, offline RL algorithms adopt either a constraint or a penalty term that explicitly guides the policy to stay close to the given dataset. However, prior methods typically require accurate estimation of the behavior policy or sampling from OOD data points, which themselves can be a non-trivial problem. Moreover, these methods under-utilize the generalization ability of deep neural networks and often fall into suboptimal solutions too close to the given dataset. In this work, we propose an uncertainty-based offline RL method that takes into account the confidence of the Q-value prediction and does not require any estimation or sampling of the data distribution. We show that the clipped Q-learning, a technique widely used in online RL, can be leveraged to successfully penalize OOD data points with high prediction uncertainties. Surprisingly, we find that it is possible to substantially outperform existing offline RL methods on various tasks by simply increasing the number of Q-networks along with the clipped Q-learning. Based on this observation, we propose an ensemble-diversified actor-critic algorithm that reduces the number of required ensemble networks down to a tenth compared to the naive ensemble while achieving state-of-the-art performance on most of the D4RL benchmarks considered.

</p>
</details>

<details><summary><b>Implicit Riemannian Concave Potential Maps</b>
<a href="https://arxiv.org/abs/2110.01288">arxiv:2110.01288</a>
&#x1F4C8; 25 <br>
<p>Danilo J. Rezende, Sébastien Racanière</p></summary>
<p>

**Abstract:** We are interested in the challenging problem of modelling densities on Riemannian manifolds with a known symmetry group using normalising flows. This has many potential applications in physical sciences such as molecular dynamics and quantum simulations. In this work we combine ideas from implicit neural layers and optimal transport theory to propose a generalisation of existing work on exponential map flows, Implicit Riemannian Concave Potential Maps, IRCPMs. IRCPMs have some nice properties such as simplicity of incorporating symmetries and are less expensive than ODE-flows. We provide an initial theoretical analysis of its properties and layout sufficient conditions for stable optimisation. Finally, we illustrate the properties of IRCPMs with density estimation experiments on tori and spheres.

</p>
</details>

<details><summary><b>3D-Transformer: Molecular Representation with Transformer in 3D Space</b>
<a href="https://arxiv.org/abs/2110.01191">arxiv:2110.01191</a>
&#x1F4C8; 21 <br>
<p>Fang Wu, Qiang Zhang, Dragomir Radev, Jiyu Cui, Wen Zhang, Huabin Xing, Ningyu Zhang, Huajun Chen</p></summary>
<p>

**Abstract:** Spatial structures in the 3D space are important to determine molecular properties. Recent papers use geometric deep learning to represent molecules and predict properties. These papers, however, are computationally expensive in capturing long-range dependencies of input atoms; and have not considered the non-uniformity of interatomic distances, thus failing to learn context-dependent representations at different scales. To deal with such issues, we introduce 3D-Transformer, a variant of the Transformer for molecular representations that incorporates 3D spatial information. 3D-Transformer operates on a fully-connected graph with direct connections between atoms. To cope with the non-uniformity of interatomic distances, we develop a multi-scale self-attention module that exploits local fine-grained patterns with increasing contextual scales. As molecules of different sizes rely on different kinds of spatial features, we design an adaptive position encoding module that adopts different position encoding methods for small and large molecules. Finally, to attain the molecular representation from atom embeddings, we propose an attentive farthest point sampling algorithm that selects a portion of atoms with the assistance of attention scores, overcoming handicaps of the virtual node and previous distance-dominant downsampling methods. We validate 3D-Transformer across three important scientific domains: quantum chemistry, material science, and proteomics. Our experiments show significant improvements over state-of-the-art models on the crystal property prediction task and the protein-ligand binding affinity prediction task, and show better or competitive performance in quantum chemistry molecular datasets. This work provides clear evidence that biochemical tasks can gain consistent benefits from 3D molecular representations and different tasks require different position encoding methods.

</p>
</details>

<details><summary><b>Information-theoretic generalization bounds for black-box learning algorithms</b>
<a href="https://arxiv.org/abs/2110.01584">arxiv:2110.01584</a>
&#x1F4C8; 13 <br>
<p>Hrayr Harutyunyan, Maxim Raginsky, Greg Ver Steeg, Aram Galstyan</p></summary>
<p>

**Abstract:** We derive information-theoretic generalization bounds for supervised learning algorithms based on the information contained in predictions rather than in the output of the training algorithm. These bounds improve over the existing information-theoretic bounds, are applicable to a wider range of algorithms, and solve two key challenges: (a) they give meaningful results for deterministic algorithms and (b) they are significantly easier to estimate. We show experimentally that the proposed bounds closely follow the generalization gap in practical scenarios for deep learning.

</p>
</details>

<details><summary><b>Differentiable Spline Approximations</b>
<a href="https://arxiv.org/abs/2110.01532">arxiv:2110.01532</a>
&#x1F4C8; 9 <br>
<p>Minsu Cho, Aditya Balu, Ameya Joshi, Anjana Deva Prasad, Biswajit Khara, Soumik Sarkar, Baskar Ganapathysubramanian, Adarsh Krishnamurthy, Chinmay Hegde</p></summary>
<p>

**Abstract:** The paradigm of differentiable programming has significantly enhanced the scope of machine learning via the judicious use of gradient-based optimization. However, standard differentiable programming methods (such as autodiff) typically require that the machine learning models be differentiable, limiting their applicability. Our goal in this paper is to use a new, principled approach to extend gradient-based optimization to functions well modeled by splines, which encompass a large family of piecewise polynomial models. We derive the form of the (weak) Jacobian of such functions and show that it exhibits a block-sparse structure that can be computed implicitly and efficiently. Overall, we show that leveraging this redesigned Jacobian in the form of a differentiable "layer" in predictive models leads to improved performance in diverse applications such as image segmentation, 3D point cloud reconstruction, and finite element analysis.

</p>
</details>

<details><summary><b>Fine-Grained Neural Network Explanation by Identifying Input Features with Predictive Information</b>
<a href="https://arxiv.org/abs/2110.01471">arxiv:2110.01471</a>
&#x1F4C8; 9 <br>
<p>Yang Zhang, Ashkan Khakzar, Yawei Li, Azade Farshad, Seong Tae Kim, Nassir Navab</p></summary>
<p>

**Abstract:** One principal approach for illuminating a black-box neural network is feature attribution, i.e. identifying the importance of input features for the network's prediction. The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identified for latent features by placing an information bottleneck within the network. We propose a method to identify features with predictive information in the input domain. The method results in fine-grained identification of input features' information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code is publicly available.

</p>
</details>

<details><summary><b>On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2110.01811">arxiv:2110.01811</a>
&#x1F4C8; 8 <br>
<p>Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, Zhaopeng Tu</p></summary>
<p>

**Abstract:** Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT.

</p>
</details>

<details><summary><b>Hit and Lead Discovery with Explorative RL and Fragment-based Molecule Generation</b>
<a href="https://arxiv.org/abs/2110.01219">arxiv:2110.01219</a>
&#x1F4C8; 7 <br>
<p>Soojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, Sung Ju Hwang</p></summary>
<p>

**Abstract:** Recently, utilizing reinforcement learning (RL) to generate molecules with desired properties has been highlighted as a promising strategy for drug design. A molecular docking program - a physical simulation that estimates protein-small molecule binding affinity - can be an ideal reward scoring function for RL, as it is a straightforward proxy of the therapeutic potential. Still, two imminent challenges exist for this task. First, the models often fail to generate chemically realistic and pharmacochemically acceptable molecules. Second, the docking score optimization is a difficult exploration problem that involves many local optima and less smooth surfaces with respect to molecular structure. To tackle these challenges, we propose a novel RL framework that generates pharmacochemically acceptable molecules with large docking scores. Our method - Fragment-based generative RL with Explorative Experience replay for Drug design (FREED) - constrains the generated molecules to a realistic and qualified chemical space and effectively explores the space to find drugs by coupling our fragment-based generation method and a novel error-prioritized experience replay (PER). We also show that our model performs well on both de novo and scaffold-based schemes. Our model produces molecules of higher quality compared to existing methods while achieving state-of-the-art performance on two of three targets in terms of the docking scores of the generated molecules. We further show with ablation studies that our method, predictive error-PER (FREED(PE)), significantly improves the model performance.

</p>
</details>

<details><summary><b>A moment-matching metric for latent variable generative models</b>
<a href="https://arxiv.org/abs/2111.00875">arxiv:2111.00875</a>
&#x1F4C8; 6 <br>
<p>Cédric Beaulac</p></summary>
<p>

**Abstract:** It can be difficult to assess the quality of a fitted model when facing unsupervised learning problems. Latent variable models, such as variation autoencoders and Gaussian mixture models, are often trained with likelihood-based approaches. In scope of Goodhart's law, when a metric becomes a target it ceases to be a good metric and therefore we should not use likelihood to assess the quality of the fit of these models. The solution we propose is a new metric for model comparison or regularization that relies on moments. The concept is to study the difference between the data moments and the model moments using a matrix norm, such as the Frobenius norm. We show how to use this new metric for model comparison and then for regularization. It is common to draw samples from the fitted distribution when evaluating latent variable models and we show that our proposed metric is faster to compute and has a smaller variance that this alternative. We conclude this article with a proof of concept of both applications and we discuss future work.

</p>
</details>

<details><summary><b>ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts</b>
<a href="https://arxiv.org/abs/2110.01799">arxiv:2110.01799</a>
&#x1F4C8; 6 <br>
<p>Yuta Koreeda, Christopher D. Manning</p></summary>
<p>

**Abstract:** Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose "document-level natural language inference (NLI) for contracts", a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as "Some obligations of Agreement may survive termination.") and a contract, and it is asked to classify whether each hypothesis is "entailed by", "contradicting to" or "not mentioned by" (neutral to) the contract as well as identifying "evidence" for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (1) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (2) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.

</p>
</details>

<details><summary><b>CertainNet: Sampling-free Uncertainty Estimation for Object Detection</b>
<a href="https://arxiv.org/abs/2110.01604">arxiv:2110.01604</a>
&#x1F4C8; 6 <br>
<p>Stefano Gasperini, Jan Haug, Mohammad-Ali Nikouei Mahani, Alvaro Marcos-Ramiro, Nassir Navab, Benjamin Busam, Federico Tombari</p></summary>
<p>

**Abstract:** Estimating the uncertainty of a neural network plays a fundamental role in safety-critical settings. In perception for autonomous driving, measuring the uncertainty means providing additional calibrated information to downstream tasks, such as path planning, that can use it towards safe navigation. In this work, we propose a novel sampling-free uncertainty estimation method for object detection. We call it CertainNet, and it is the first to provide separate uncertainties for each output signal: objectness, class, location and size. To achieve this, we propose an uncertainty-aware heatmap, and exploit the neighboring bounding boxes provided by the detector at inference time. We evaluate the detection performance and the quality of the different uncertainty estimates separately, also with challenging out-of-domain samples: BDD100K and nuImages with models trained on KITTI. Additionally, we propose a new metric to evaluate location and size uncertainties. When transferring to unseen datasets, CertainNet generalizes substantially better than previous methods and an ensemble, while being real-time and providing high quality and comprehensive uncertainty estimates.

</p>
</details>

<details><summary><b>Skill Induction and Planning with Latent Language</b>
<a href="https://arxiv.org/abs/2110.01517">arxiv:2110.01517</a>
&#x1F4C8; 6 <br>
<p>Pratyusha Sharma, Antonio Torralba, Jacob Andreas</p></summary>
<p>

**Abstract:** We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions. We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level subtasks, using only a small number of seed annotations to ground language in action. In trained models, the space of natural language commands indexes a combinatorial library of skills; agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals. We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10% of demonstrations. It completes more than twice as many tasks as a standard approach to learning from demonstrations, matching the performance of instruction following models with access to ground-truth plans during both training and evaluation.

</p>
</details>

<details><summary><b>Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus</b>
<a href="https://arxiv.org/abs/2110.15710">arxiv:2110.15710</a>
&#x1F4C8; 5 <br>
<p>Sohrab Ferdowsi, Nikolay Borissov, Julien Knafou, Poorya Amini, Douglas Teodoro</p></summary>
<p>

**Abstract:** We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across different documents. We applied our model to classify clinical trial (CT) protocols into completed and terminated categories. We use bag-of-words based, as well as pre-trained transformer-based embeddings to featurize the graph nodes, achieving f1-scores around 0.85 on a publicly available large scale CT registry of around 360K protocols. We further demonstrate how the selective pooling can add insights into the CT termination status prediction. We make the source code and dataset splits accessible.

</p>
</details>

<details><summary><b>Towards General-purpose Infrastructure for Protecting Scientific Data Under Study</b>
<a href="https://arxiv.org/abs/2110.01315">arxiv:2110.01315</a>
&#x1F4C8; 5 <br>
<p>Andrew Trask, Kritika Prakash</p></summary>
<p>

**Abstract:** The scientific method presents a key challenge to privacy because it requires many samples to support a claim. When samples are commercially valuable or privacy-sensitive enough, their owners have strong reasons to avoid releasing them for scientific study. Privacy techniques seek to mitigate this tension by enforcing limits on one's ability to use studied samples for secondary purposes. Recent work has begun combining these techniques into end-to-end systems for protecting data. In this work, we assemble the first such combination which is sufficient for a privacy-layman to use familiar tools to experiment over private data while the infrastructure automatically prohibits privacy leakage. We support this theoretical system with a prototype within the Syft privacy platform using the PyTorch framework.

</p>
</details>

<details><summary><b>Reinforcement Learning for Admission Control in Wireless Virtual Network Embedding</b>
<a href="https://arxiv.org/abs/2110.01262">arxiv:2110.01262</a>
&#x1F4C8; 5 <br>
<p>Haitham Afifi, Fabian Sauer, Holger Karl</p></summary>
<p>

**Abstract:** Using Service Function Chaining (SFC) in wireless networks became popular in many domains like networking and multimedia. It relies on allocating network resources to incoming SFCs requests, via a Virtual Network Embedding (VNE) algorithm, so that it optimizes the performance of the SFC. When the load of incoming requests -- competing for the limited network resources - increases, it becomes challenging to decide which requests should be admitted and which one should be rejected.
  In this work, we propose a deep Reinforcement learning (RL) solution that can learn the admission policy for different dependencies, such as the service lifetime and the priority of incoming requests. We compare the deep RL solution to a first-come-first-serve baseline that admits a request whenever there are available resources. We show that deep RL outperforms the baseline and provides higher acceptance rate with low rejections even when there are enough resources.

</p>
</details>

<details><summary><b>Differentiable Equilibrium Computation with Decision Diagrams for Stackelberg Models of Combinatorial Congestion Games</b>
<a href="https://arxiv.org/abs/2110.01773">arxiv:2110.01773</a>
&#x1F4C8; 4 <br>
<p>Shinsaku Sakaue, Kengo Nakamura</p></summary>
<p>

**Abstract:** We address Stackelberg models of combinatorial congestion games (CCGs); we aim to optimize the parameters of CCGs so that the selfish behavior of non-atomic players attains desirable equilibria. This model is essential for designing such social infrastructures as traffic and communication networks. Nevertheless, computational approaches to the model have not been thoroughly studied due to two difficulties: (I) bilevel-programming structures and (II) the combinatorial nature of CCGs. We tackle them by carefully combining (I) the idea of \textit{differentiable} optimization and (II) data structures called \textit{zero-suppressed binary decision diagrams} (ZDDs), which can compactly represent sets of combinatorial strategies. Our algorithm numerically approximates the equilibria of CCGs, which we can differentiate with respect to parameters of CCGs by automatic differentiation. With the resulting derivatives, we can apply gradient-based methods to Stackelberg models of CCGs. Our method is tailored to induce Nesterov's acceleration and can fully utilize the empirical compactness of ZDDs. These technical advantages enable us to deal with CCGs with a vast number of combinatorial strategies. Experiments on real-world network design instances demonstrate the practicality of our method.

</p>
</details>

<details><summary><b>Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping</b>
<a href="https://arxiv.org/abs/2110.01765">arxiv:2110.01765</a>
&#x1F4C8; 4 <br>
<p>James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha Sohl-Dickstein, Samuel S. Schoenholz</p></summary>
<p>

**Abstract:** Using an extended and formalized version of the Q/C map analysis of Poole et al. (2016), along with Neural Tangent Kernel theory, we identify the main pathologies present in deep networks that prevent them from training fast and generalizing to unseen data, and show how these can be avoided by carefully controlling the "shape" of the network's initialization-time kernel function. We then develop a method called Deep Kernel Shaping (DKS), which accomplishes this using a combination of precise parameter initialization, activation function transformations, and small architectural tweaks, all of which preserve the model class. In our experiments we show that DKS enables SGD training of residual networks without normalization layers on Imagenet and CIFAR-10 classification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet models, with only a small decrease in generalization performance. And when using K-FAC as the optimizer, we achieve similar results for networks without skip connections. Our results apply for a large variety of activation functions, including those which traditionally perform very badly, such as the logistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip connections, normalization layers, special activation functions like RELU and SELU, and various initialization schemes, explaining their effectiveness as alternative (and ultimately incomplete) ways of "shaping" the network's initialization-time kernel.

</p>
</details>

<details><summary><b>Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults</b>
<a href="https://arxiv.org/abs/2110.01641">arxiv:2110.01641</a>
&#x1F4C8; 4 <br>
<p>Anoop Krishnan, Ali Almadan, Ajita Rattani</p></summary>
<p>

**Abstract:** A number of studies suggest bias of the face biometrics, i.e., face recognition and soft-biometric estimation methods, across gender, race, and age groups. There is a recent urge to investigate the bias of different biometric modalities toward the deployment of fair and trustworthy biometric solutions. Ocular biometrics has obtained increased attention from academia and industry due to its high accuracy, security, privacy, and ease of use in mobile devices. A recent study in $2020$ also suggested the fairness of ocular-based user recognition across males and females. This paper aims to evaluate the fairness of ocular biometrics in the visible spectrum among age groups; young, middle, and older adults. Thanks to the availability of the latest large-scale 2020 UFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79 years, to facilitate this study. Experimental results suggest the overall equivalent performance of ocular biometrics across gender and age groups in user verification and gender classification. Performance difference for older adults at lower false match rate and young adults was noted at user verification and age classification, respectively. This could be attributed to inherent characteristics of the biometric data from these age groups impacting specific applications, which suggest a need for advancement in sensor technology and software solutions.

</p>
</details>

<details><summary><b>Inferring dark matter substructure with astrometric lensing beyond the power spectrum</b>
<a href="https://arxiv.org/abs/2110.01620">arxiv:2110.01620</a>
&#x1F4C8; 4 <br>
<p>Siddharth Mishra-Sharma</p></summary>
<p>

**Abstract:** Astrometry -- the precise measurement of positions and motions of celestial objects -- has emerged as a promising avenue for characterizing the dark matter population in our Galaxy. By leveraging recent advances in simulation-based inference and neural network architectures, we introduce a novel method to search for global dark matter-induced gravitational lensing signatures in astrometric datasets. Our method based on neural likelihood-ratio estimation shows significantly enhanced sensitivity to a cold dark matter population and more favorable scaling with measurement noise compared to existing approaches based on two-point correlation statistics, establishing machine learning as a powerful tool for characterizing dark matter using astrometric data.

</p>
</details>

<details><summary><b>Effectiveness of Optimization Algorithms in Deep Image Classification</b>
<a href="https://arxiv.org/abs/2110.01598">arxiv:2110.01598</a>
&#x1F4C8; 4 <br>
<p>Zhaoyang Zhu, Haozhe Sun, Chi Zhang</p></summary>
<p>

**Abstract:** Adam is applied widely to train neural networks. Different kinds of Adam methods with different features pop out. Recently two new adam optimizers, AdaBelief and Padam are introduced among the community. We analyze these two adam optimizers and compare them with other conventional optimizers (Adam, SGD + Momentum) in the scenario of image classification. We evaluate the performance of these optimization algorithms on AlexNet and simplified versions of VGGNet, ResNet using the EMNIST dataset. (Benchmark algorithm is available at \hyperref[https://github.com/chuiyunjun/projectCSC413]{https://github.com/chuiyunjun/projectCSC413}).

</p>
</details>

<details><summary><b>Traffic Flow Forecasting with Maintenance Downtime via Multi-Channel Attention-Based Spatio-Temporal Graph Convolutional Networks</b>
<a href="https://arxiv.org/abs/2110.01535">arxiv:2110.01535</a>
&#x1F4C8; 4 <br>
<p>Yuanjie Lu, Parastoo Kamranfar, David Lattanzi, Amarda Shehu</p></summary>
<p>

**Abstract:** Forecasting traffic flows is a central task in intelligent transportation system management. Graph structures have shown promise as a modeling framework, with recent advances in spatio-temporal modeling via graph convolution neural networks, improving the performance or extending the prediction horizon on traffic flows. However, a key shortcoming of state-of-the-art methods is their inability to take into account information of various modalities, for instance the impact of maintenance downtime on traffic flows. This is the issue we address in this paper. Specifically, we propose a novel model to predict traffic speed under the impact of construction work. The model is based on the powerful attention-based spatio-temporal graph convolution architecture but utilizes various channels to integrate different sources of information, explicitly builds spatio-temporal dependencies among traffic states, captures the relationships between heterogeneous roadway networks, and then predicts changes in traffic flow resulting from maintenance downtime events. The model is evaluated on two benchmark datasets and a novel dataset we have collected over the bustling Tyson's corner region in Northern Virginia. Extensive comparative experiments and ablation studies show that the proposed model can capture complex and nonlinear spatio-temporal relationships across a transportation corridor, outperforming baseline models.

</p>
</details>

<details><summary><b>Distributed Learning Approaches for Automated Chest X-Ray Diagnosis</b>
<a href="https://arxiv.org/abs/2110.01474">arxiv:2110.01474</a>
&#x1F4C8; 4 <br>
<p>Edoardo Giacomello, Michele Cataldo, Daniele Loiacono, Pier Luca Lanzi</p></summary>
<p>

**Abstract:** Deep Learning has established in the latest years as a successful approach to address a great variety of tasks. Healthcare is one of the most promising field of application for Deep Learning approaches since it would allow to help clinicians to analyze patient data and perform diagnoses. However, despite the vast amount of data collected every year in hospitals and other clinical institutes, privacy regulations on sensitive data - such as those related to health - pose a serious challenge to the application of these methods. In this work, we focus on strategies to cope with privacy issues when a consortium of healthcare institutions needs to train machine learning models for identifying a particular disease, comparing the performances of two recent distributed learning approaches - Federated Learning and Split Learning - on the task of Automated Chest X-Ray Diagnosis. In particular, in our analysis we investigated the impact of different data distributions in client data and the possible policies on the frequency of data exchange between the institutions.

</p>
</details>

<details><summary><b>Context-Aware Unsupervised Clustering for Person Search</b>
<a href="https://arxiv.org/abs/2110.01341">arxiv:2110.01341</a>
&#x1F4C8; 4 <br>
<p>Byeong-Ju Han, Kuhyeun Ko, Jae-Young Sim</p></summary>
<p>

**Abstract:** The existing person search methods use the annotated labels of person identities to train deep networks in a supervised manner that requires a huge amount of time and effort for human labeling. In this paper, we first introduce a novel framework of person search that is able to train the network in the absence of the person identity labels, and propose efficient unsupervised clustering methods to substitute the supervision process using annotated person identity labels. Specifically, we propose a hard negative mining scheme based on the uniqueness property that only a single person has the same identity to a given query person in each image. We also propose a hard positive mining scheme by using the contextual information of co-appearance that neighboring persons in one image tend to appear simultaneously in other images. The experimental results show that the proposed method achieves comparable performance to that of the state-of-the-art supervised person search methods, and furthermore outperforms the extended unsupervised person re-identification methods on the benchmark person search datasets.

</p>
</details>

<details><summary><b>Consistency Regularization Can Improve Robustness to Label Noise</b>
<a href="https://arxiv.org/abs/2110.01242">arxiv:2110.01242</a>
&#x1F4C8; 4 <br>
<p>Erik Englesson, Hossein Azizpour</p></summary>
<p>

**Abstract:** Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings test-time robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for training-time robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and(ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-of-the-art results.

</p>
</details>

<details><summary><b>AASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks</b>
<a href="https://arxiv.org/abs/2110.01200">arxiv:2110.01200</a>
&#x1F4C8; 4 <br>
<p>Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu, Nicholas Evans</p></summary>
<p>

**Abstract:** Artefacts that differentiate spoofed from bona-fide utterances can reside in spectral or temporal domains. Their reliable detection usually depends upon computationally demanding ensemble systems where each subsystem is tuned to some specific artefacts. We seek to develop an efficient, single system that can detect a broad range of different spoofing attacks without score-level ensembles. We propose a novel heterogeneous stacking graph attention layer which models artefacts spanning heterogeneous temporal and spectral domains with a heterogeneous attention mechanism and a stack node. With a new max graph operation that involves a competitive mechanism and an extended readout scheme, our approach, named AASIST, outperforms the current state-of-the-art by 20% relative. Even a lightweight variant, AASIST-L, with only 85K parameters, outperforms all competing systems.

</p>
</details>

<details><summary><b>Towards efficient end-to-end speech recognition with biologically-inspired neural networks</b>
<a href="https://arxiv.org/abs/2110.02743">arxiv:2110.02743</a>
&#x1F4C8; 3 <br>
<p>Thomas Bohnstingl, Ayush Garg, Stanisław Woźniak, George Saon, Evangelos Eleftheriou, Angeliki Pantazi</p></summary>
<p>

**Abstract:** Automatic speech recognition (ASR) is a capability which enables a program to process human speech into a written form. Recent developments in artificial intelligence (AI) have led to high-accuracy ASR systems based on deep neural networks, such as the recurrent neural network transducer (RNN-T). However, the core components and the performed operations of these approaches depart from the powerful biological counterpart, i.e., the human brain. On the other hand, the current developments in biologically-inspired ASR models, based on spiking neural networks (SNNs), lag behind in terms of accuracy and focus primarily on small scale applications. In this work, we revisit the incorporation of biologically-plausible models into deep learning and we substantially enhance their capabilities, by taking inspiration from the diverse neural and synaptic dynamics found in the brain. In particular, we introduce neural connectivity concepts emulating the axo-somatic and the axo-axonic synapses. Based on this, we propose novel deep learning units with enriched neuro-synaptic dynamics and integrate them into the RNN-T architecture. We demonstrate for the first time, that a biologically realistic implementation of a large-scale ASR model can yield competitive performance levels compared to the existing deep learning models. Specifically, we show that such an implementation bears several advantages, such as a reduced computational cost and a lower latency, which are critical for speech recognition applications.

</p>
</details>

<details><summary><b>Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition</b>
<a href="https://arxiv.org/abs/2110.02220">arxiv:2110.02220</a>
&#x1F4C8; 3 <br>
<p>Tsendsuren Munkhdalai, Khe Chai Sim, Angad Chandorkar, Fan Gao, Mason Chua, Trevor Strohman, Françoise Beaufays</p></summary>
<p>

**Abstract:** Fast contextual adaptation has shown to be effective in improving Automatic Speech Recognition (ASR) of rare words and when combined with an on-device personalized training, it can yield an even better recognition result. However, the traditional re-scoring approaches based on an external language model is prone to diverge during the personalized training. In this work, we introduce a model-based end-to-end contextual adaptation approach that is decoder-agnostic and amenable to on-device personalization. Our on-device simulation experiments demonstrate that the proposed approach outperforms the traditional re-scoring technique by 12% relative WER and 15.7% entity mention specific F1-score in a continues personalization scenario.

</p>
</details>

<details><summary><b>Deep Synoptic Monte Carlo Planning in Reconnaissance Blind Chess</b>
<a href="https://arxiv.org/abs/2110.01810">arxiv:2110.01810</a>
&#x1F4C8; 3 <br>
<p>Gregory Clark</p></summary>
<p>

**Abstract:** This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on "synopses," a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.

</p>
</details>

<details><summary><b>Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images</b>
<a href="https://arxiv.org/abs/2110.01761">arxiv:2110.01761</a>
&#x1F4C8; 3 <br>
<p>Kang Zhou, Jing Li, Weixin Luo, Zhengxin Li, Jianlong Yang, Huazhu Fu, Jun Cheng, Jiang Liu, Shenghua Gao</p></summary>
<p>

**Abstract:** Anomaly detection in medical images refers to the identification of abnormal images with only normal images in the training set. Most existing methods solve this problem with a self-reconstruction framework, which tends to learn an identity mapping and reduces the sensitivity to anomalies. To mitigate this problem, in this paper, we propose a novel Proxy-bridged Image Reconstruction Network (ProxyAno) for anomaly detection in medical images. Specifically, we use an intermediate proxy to bridge the input image and the reconstructed image. We study different proxy types, and we find that the superpixel-image (SI) is the best one. We set all pixels' intensities within each superpixel as their average intensity, and denote this image as SI. The proposed ProxyAno consists of two modules, a Proxy Extraction Module and an Image Reconstruction Module. In the Proxy Extraction Module, a memory is introduced to memorize the feature correspondence for normal image to its corresponding SI, while the memorized correspondence does not apply to the abnormal images, which leads to the information loss for abnormal image and facilitates the anomaly detection. In the Image Reconstruction Module, we map an SI to its reconstructed image. Further, we crop a patch from the image and paste it on the normal SI to mimic the anomalies, and enforce the network to reconstruct the normal image even with the pseudo abnormal SI. In this way, our network enlarges the reconstruction error for anomalies. Extensive experiments on brain MR images, retinal OCT images and retinal fundus images verify the effectiveness of our method for both image-level and pixel-level anomaly detection.

</p>
</details>

<details><summary><b>Learning, Computing, and Trustworthiness in Intelligent IoT Environments: Performance-Energy Tradeoffs</b>
<a href="https://arxiv.org/abs/2110.01686">arxiv:2110.01686</a>
&#x1F4C8; 3 <br>
<p>Beatriz Soret, Lam D. Nguyen, Jan Seeger, Arne Bröring, Chaouki Ben Issaid, Sumudu Samarakoon, Anis El Gabli, Vivek Kulkarni, Mehdi Bennis, Petar Popovski</p></summary>
<p>

**Abstract:** An Intelligent IoT Environment (iIoTe) is comprised of heterogeneous devices that can collaboratively execute semi-autonomous IoT applications, examples of which include highly automated manufacturing cells or autonomously interacting harvesting machines. Energy efficiency is key in such edge environments, since they are often based on an infrastructure that consists of wireless and battery-run devices, e.g., e-tractors, drones, Automated Guided Vehicle (AGV)s and robots. The total energy consumption draws contributions from multipleiIoTe technologies that enable edge computing and communication, distributed learning, as well as distributed ledgers and smart contracts. This paper provides a state-of-the-art overview of these technologies and illustrates their functionality and performance, with special attention to the tradeoff among resources, latency, privacy and energy consumption. Finally, the paper provides a vision for integrating these enabling technologies in energy-efficient iIoTe and a roadmap to address the open research challenges

</p>
</details>

<details><summary><b>An Experimental Evaluation on Deepfake Detection using Deep Face Recognition</b>
<a href="https://arxiv.org/abs/2110.01640">arxiv:2110.01640</a>
&#x1F4C8; 3 <br>
<p>Sreeraj Ramachandran, Aakash Varma Nadimpalli, Ajita Rattani</p></summary>
<p>

**Abstract:** Significant advances in deep learning have obtained hallmark accuracy rates for various computer vision applications. However, advances in deep generative models have also led to the generation of very realistic fake content, also known as deepfakes, causing a threat to privacy, democracy, and national security. Most of the current deepfake detection methods are deemed as a binary classification problem in distinguishing authentic images or videos from fake ones using two-class convolutional neural networks (CNNs). These methods are based on detecting visual artifacts, temporal or color inconsistencies produced by deep generative models. However, these methods require a large amount of real and fake data for model training and their performance drops significantly in cross dataset evaluation with samples generated using advanced deepfake generation techniques. In this paper, we thoroughly evaluate the efficacy of deep face recognition in identifying deepfakes, using different loss functions and deepfake generation techniques. Experimental investigations on challenging Celeb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep face recognition in identifying deepfakes over two-class CNNs and the ocular modality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and an Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition on the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER obtained for the two-class CNN and the ocular modality on the Celeb-DF dataset. Further on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were obtained. The use of biometric facial recognition technology has the advantage of bypassing the need for a large amount of fake data for model training and obtaining better generalizability to evolving deepfake creation techniques.

</p>
</details>

<details><summary><b>Assessing glaucoma in retinal fundus photographs using Deep Feature Consistent Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2110.01534">arxiv:2110.01534</a>
&#x1F4C8; 3 <br>
<p>Sayan Mandal, Alessandro A. Jammal, Felipe A. Medeiros</p></summary>
<p>

**Abstract:** One of the leading causes of blindness is glaucoma, which is challenging to detect since it remains asymptomatic until the symptoms are severe. Thus, diagnosis is usually possible until the markers are easy to identify, i.e., the damage has already occurred. Early identification of glaucoma is generally made based on functional, structural, and clinical assessments. However, due to the nature of the disease, researchers still debate which markers qualify as a consistent glaucoma metric. Deep learning methods have partially solved this dilemma by bypassing the marker identification stage and analyzing high-level information directly to classify the data. Although favorable, these methods make expert analysis difficult as they provide no insight into the model discrimination process. In this paper, we overcome this using deep generative networks, a deep learning model that learns complicated, high-dimensional probability distributions. We train a Deep Feature consistent Variational Autoencoder (DFC-VAE) to reconstruct optic disc images. We show that a small-sized latent space obtained from the DFC-VAE can learn the high-dimensional glaucoma data distribution and provide discriminatory evidence between normal and glaucoma eyes. Latent representations of size as low as 128 from our model got a 0.885 area under the receiver operating characteristic curve when trained with Support Vector Classifier.

</p>
</details>

<details><summary><b>Balanced Masked and Standard Face Recognition</b>
<a href="https://arxiv.org/abs/2110.01521">arxiv:2110.01521</a>
&#x1F4C8; 3 <br>
<p>Delong Qi, Kangli Hu, Weijun Tan, Qi Yao, Jingfeng Liu</p></summary>
<p>

**Abstract:** We present the improved network architecture, data augmentation, and training strategies for the Webface track and Insightface/Glint360K track of the masked face recognition challenge of ICCV2021. One of the key goals is to have a balanced performance of masked and standard face recognition. In order to prevent the overfitting for the masked face recognition, we control the total number of masked faces by not more than 10\% of the total face recognition in the training dataset. We propose a few key changes to the face recognition network including a new stem unit, drop block, face detection and alignment using YOLO5Face, feature concatenation, a cycle cosine learning rate, etc. With this strategy, we achieve good and balanced performance for both masked and standard face recognition.

</p>
</details>

<details><summary><b>DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models</b>
<a href="https://arxiv.org/abs/2110.01509">arxiv:2110.01509</a>
&#x1F4C8; 3 <br>
<p>Gregor Betz, Kyle Richardson</p></summary>
<p>

**Abstract:** In this paper, we present and implement a multi-dimensional, modular framework for performing deep argument analysis (DeepA2) using current pre-trained language models (PTLMs). ArgumentAnalyst -- a T5 model (Raffel et al. 2020) set up and trained within DeepA2 -- reconstructs argumentative texts, which advance an informal argumentation, as valid arguments: It inserts, e.g., missing premises and conclusions, formalizes inferences, and coherently links the logical reconstruction to the source text. We create a synthetic corpus for deep argument analysis, and evaluate ArgumentAnalyst on this new dataset as well as on existing data, specifically EntailmentBank (Dalvi et al. 2021). Our empirical findings vindicate the overall framework and highlight the advantages of a modular design, in particular its ability to emulate established heuristics (such as hermeneutic cycles), to explore the model's uncertainty, to cope with the plurality of correct solutions (underdetermination), and to exploit higher-order evidence.

</p>
</details>

<details><summary><b>Learning Domain-Invariant Relationship with Instrumental Variable for Domain Generalization</b>
<a href="https://arxiv.org/abs/2110.01438">arxiv:2110.01438</a>
&#x1F4C8; 3 <br>
<p>Junkun Yuan, Xu Ma, Kun Kuang, Ruoxuan Xiong, Mingming Gong, Lanfen Lin</p></summary>
<p>

**Abstract:** Domain generalization (DG) aims to learn from multiple source domains a model that generalizes well on unseen target domains. Existing methods mainly learn input feature representations with invariant marginal distribution, while the invariance of the conditional distribution is more essential for unknown domain generalization. This paper proposes an instrumental variable-based approach to learn the domain-invariant relationship between input features and labels contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we design a simple yet effective framework to learn the Domain-invariant Relationship with Instrumental VariablE (DRIVE) via a two-stage IV method. Specifically, it first learns the conditional distribution of input features of one domain given input features of another domain, and then it estimates the domain-invariant relationship by predicting labels with the learned conditional distribution. Simulation experiments show the proposed method accurately captures the domain-invariant relationship. Extensive experiments on several datasets consistently demonstrate that DRIVE yields state-of-the-art results.

</p>
</details>

<details><summary><b>Automated Aerial Animal Detection When Spatial Resolution Conditions Are Varied</b>
<a href="https://arxiv.org/abs/2110.01329">arxiv:2110.01329</a>
&#x1F4C8; 3 <br>
<p>Jasper Brown, Yongliang Qiao, Cameron Clark, Sabrina Lomax, Khalid Rafique, Salah Sukkarieh</p></summary>
<p>

**Abstract:** Knowing where livestock are located enables optimized management and mustering. However, Australian farms are large meaning that many of Australia's livestock are unmonitored which impacts farm profit, animal welfare and the environment. Effective animal localisation and counting by analysing satellite imagery overcomes this management hurdle however, high resolution satellite imagery is expensive. Thus, to minimise cost the lowest spatial resolution data that enables accurate livestock detection should be selected. In our work, we determine the association between object detector performance and spatial degradation for cattle, sheep and dogs. Accurate ground truth was established using high resolution drone images which were then downsampled to various ground sample distances (GSDs). Both circular and cassegrain aperture optics were simulated to generate point spread functions (PSFs) corresponding to various optical qualities. By simulating the PSF, rather than approximating it as a Gaussian, the images were accurately degraded to match the spatial resolution and blurring structure of satellite imagery.
  Two existing datasets were combined and used to train and test a YoloV5 object detection network. Detector performance was found to drop steeply around a GSD of 0.5m/px and was associated with PSF matrix structure within this GSD region. Detector mAP performance fell by 52 percent when a cassegrain, rather than circular, aperture was used at a 0.5m/px GSD. Overall blurring magnitude also had a small impact when matched to GSD, as did the internal network resolution. Our results here inform the selection of remote sensing data requirements for animal detection tasks, allowing farmers and ecologists to use more accessible medium resolution imagery with confidence.

</p>
</details>

<details><summary><b>Incremental Class Learning using Variational Autoencoders with Similarity Learning</b>
<a href="https://arxiv.org/abs/2110.01303">arxiv:2110.01303</a>
&#x1F4C8; 3 <br>
<p>Jiahao Huo, Terence L. van Zyl</p></summary>
<p>

**Abstract:** Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. It is of significant interest to understand how similarity learning loss functions would be affected by catastrophic forgetting. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are angular, contrastive, centre, and triplet loss. Our results show that the rate of catastrophic forgetting is different across loss functions on multiple datasets. The angular loss was least affected, followed by contrastive, triplet loss, and centre loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We further proposed our novel technique using VAEs to generate representation as exemplars that are passed through intermediate layers of the network. Our method outperformed the three existing techniques. We have shown that we do not require stored images as exemplars for incremental learning with similarity learning. The generated representations can help preserve regions of the embedding space used by prior knowledge so that new knowledge will not ``overwrite'' prior knowledge.

</p>
</details>

<details><summary><b>Posture Recognition in the Critical Care Settings using Wearable Devices</b>
<a href="https://arxiv.org/abs/2110.02768">arxiv:2110.02768</a>
&#x1F4C8; 2 <br>
<p>Anis Davoudi, Patrick J. Tighe, Azra Bihorac, Parisa Rashidi</p></summary>
<p>

**Abstract:** Low physical activity levels in the intensive care units (ICU) patients have been linked to adverse clinical outcomes. Therefore, there is a need for continuous and objective measurement of physical activity in the ICU to quantify the association between physical activity and patient outcomes. This measurement would also help clinicians evaluate the efficacy of proposed rehabilitation and physical therapy regimens in improving physical activity. In this study, we examined the feasibility of posture recognition in an ICU population using data from wearable sensors.

</p>
</details>

<details><summary><b>DA-DRN: Degradation-Aware Deep Retinex Network for Low-Light Image Enhancement</b>
<a href="https://arxiv.org/abs/2110.01809">arxiv:2110.01809</a>
&#x1F4C8; 2 <br>
<p>Xinxu Wei, Xianshi Zhang, Shisen Wang, Cheng Cheng, Yanlin Huang, Kaifu Yang, Yongjie Li</p></summary>
<p>

**Abstract:** Images obtained in real-world low-light conditions are not only low in brightness, but they also suffer from many other types of degradation, such as color distortion, unknown noise, detail loss and halo artifacts. In this paper, we propose a Degradation-Aware Deep Retinex Network (denoted as DA-DRN) for low-light image enhancement and tackle the above degradation. Based on Retinex Theory, the decomposition net in our model can decompose low-light images into reflectance and illumination maps and deal with the degradation in the reflectance during the decomposition phase directly. We propose a Degradation-Aware Module (DA Module) which can guide the training process of the decomposer and enable the decomposer to be a restorer during the training phase without additional computational cost in the test phase. DA Module can achieve the purpose of noise removal while preserving detail information into the illumination map as well as tackle color distortion and halo artifacts. We introduce Perceptual Loss to train the enhancement network to generate the brightness-improved illumination maps which are more consistent with human visual perception. We train and evaluate the performance of our proposed model over the LOL real-world and LOL synthetic datasets, and we also test our model over several other frequently used datasets without Ground-Truth (LIME, DICM, MEF and NPE datasets). We conduct extensive experiments to demonstrate that our approach achieves a promising effect with good rubustness and generalization and outperforms many other state-of-the-art methods qualitatively and quantitatively. Our method only takes 7 ms to process an image with 600x400 resolution on a TITAN Xp GPU.

</p>
</details>

<details><summary><b>Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression</b>
<a href="https://arxiv.org/abs/2110.01805">arxiv:2110.01805</a>
&#x1F4C8; 2 <br>
<p>Somdyuti Paul, Andrey Norkin, Alan C. Bovik</p></summary>
<p>

**Abstract:** Block based motion estimation is integral to inter prediction processes performed in hybrid video codecs. Prevalent block matching based methods that are used to compute block motion vectors (MVs) rely on computationally intensive search procedures. They also suffer from the aperture problem, which can worsen as the block size is reduced. Moreover, the block matching criteria used in typical codecs do not account for the resulting levels of perceptual quality of the motion compensated pictures that are created upon decoding. Towards achieving the elusive goal of perceptually optimized motion estimation, we propose a search-free block motion estimation framework using a multi-stage convolutional neural network, which is able to conduct motion estimation on multiple block sizes simultaneously, using a triplet of frames as input. This composite block translation network (CBT-Net) is trained in a self-supervised manner on a large database that we created from publicly available uncompressed video content. We deploy the multi-scale structural similarity (MS-SSIM) loss function to optimize the perceptual quality of the motion compensated predicted frames. Our experimental results highlight the computational efficiency of our proposed model relative to conventional block matching based motion estimation algorithms, for comparable prediction errors. Further, when used to perform inter prediction in AV1, the MV predictions of the perceptually optimized model result in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and -1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion (VMAF) quality metrics, respectively as compared to the block matching based motion estimation system employed in the SVT-AV1 encoder.

</p>
</details>

<details><summary><b>Deep Subspace analysing for Semi-Supervised multi-label classification of Diabetic Foot Ulcer</b>
<a href="https://arxiv.org/abs/2110.01795">arxiv:2110.01795</a>
&#x1F4C8; 2 <br>
<p>Azadeh Alavi</p></summary>
<p>

**Abstract:** Diabetes is a global raising pandemic. Diabetes patients are at risk of developing foot ulcer that usually leads to limb amputation. In order to develop a self monitoring mobile application, in this work, we propose a novel deep subspace analysis pipeline for semi-supervised diabetic foot ulcer mulit-label classification. To avoid any chance of over-fitting, unlike recent state of the art deep semi-supervised methods, the proposed pipeline dose not include any data augmentation. Whereas, after extracting deep features, in order to make the representation shift invariant, we employ variety of data augmentation methods on each image and generate an image-sets, which is then mapped into a linear subspace. Moreover, the proposed pipeline reduces the cost of retraining when more new unlabelled data become available. Thus, the first stage of the pipeline employs the concept of transfer learning for feature extraction purpose through modifying and retraining a deep convolutional network architect known as Xception. Then, the output of a mid-layer is extracted to generate an image set representer of any given image with help of data augmentation methods. At this stage, each image is transferred to a linear subspace which is a point on a Grassmann Manifold topological space. Hence, to perform analyse them, the geometry of such manifold must be considered. As such, each labelled image is represented as a vector of distances to number of unlabelled images using geodesic distance on Grassmann manifold. Finally, Random Forest is trained for multi-label classification of diabetic foot ulcer images. The method is then evaluated on the blind test set provided by DFU2021 competition, and the result considerable improvement compared to using classical transfer learning with data augmentation.

</p>
</details>

<details><summary><b>MetaPix: Domain Transfer for Semantic Segmentation by Meta Pixel Weighting</b>
<a href="https://arxiv.org/abs/2110.01777">arxiv:2110.01777</a>
&#x1F4C8; 2 <br>
<p>Yiren Jian, Chongyang Gao</p></summary>
<p>

**Abstract:** Training a deep neural model for semantic segmentation requires collecting a large amount of pixel-level labeled data. To alleviate the data scarcity problem presented in the real world, one could utilize synthetic data whose label is easy to obtain. Previous work has shown that the performance of a semantic segmentation model can be improved by training jointly with real and synthetic examples with a proper weighting on the synthetic data. Such weighting was learned by a heuristic to maximize the similarity between synthetic and real examples. In our work, we instead learn a pixel-level weighting of the synthetic data by meta-learning, i.e., the learning of weighting should only be minimizing the loss on the target task. We achieve this by gradient-on-gradient technique to propagate the target loss back into the parameters of the weighting model. The experiments show that our method with only one single meta module can outperform a complicated combination of an adversarial feature alignment, a reconstruction loss, plus a hierarchical heuristic weighting at pixel, region and image levels.

</p>
</details>

<details><summary><b>Bottom-up Hierarchical Classification Using Confusion-based Logit Compression</b>
<a href="https://arxiv.org/abs/2110.01756">arxiv:2110.01756</a>
&#x1F4C8; 2 <br>
<p>Tong Liang, Jim Davis, Roman Ilin</p></summary>
<p>

**Abstract:** In this work, we propose a method to efficiently compute label posteriors of a base flat classifier in the presence of few validation examples within a bottom-up hierarchical inference framework. A stand-alone validation set (not used to train the base classifier) is preferred for posterior estimation to avoid overfitting the base classifier, however a small validation set limits the number of features one can effectively use. We propose a simple, yet robust, logit vector compression approach based on generalized logits and label confusions for the task of label posterior estimation within the context of hierarchical classification. Extensive comparative experiments with other compression techniques are provided across multiple sized validation sets, and a comparison with related hierarchical classification approaches is also conducted. The proposed approach mitigates the problem of not having enough validation examples for reliable posterior estimation while maintaining strong hierarchical classification performance.

</p>
</details>

<details><summary><b>AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit's Activation via Adjoint Operators</b>
<a href="https://arxiv.org/abs/2110.01736">arxiv:2110.01736</a>
&#x1F4C8; 2 <br>
<p>Qing Wan, Yoonsuck Choe</p></summary>
<p>

**Abstract:** Adjoint operators have been found to be effective in the exploration of CNN's inner workings [1]. However, the previous no-bias assumption restricted its generalization. We overcome the restriction via embedding input images into an extended normed space that includes bias in all CNN layers as part of the extended input space and propose an adjoint-operator-based algorithm that maps high-level weights back to the extended input space for reconstructing an effective hypersurface. Such hypersurface can be computed for an arbitrary unit in the CNN, and we prove that this reconstructed hypersurface, when multiplied by the original input (through an inner product), will precisely replicate the output value of each unit. We show experimental results based on the CIFAR-10 dataset that the proposed approach achieves near $0$ reconstruction error.

</p>
</details>

<details><summary><b>Clustering a Mixture of Gaussians with Unknown Covariance</b>
<a href="https://arxiv.org/abs/2110.01602">arxiv:2110.01602</a>
&#x1F4C8; 2 <br>
<p>Damek Davis, Mateo Díaz, Kaizheng Wang</p></summary>
<p>

**Abstract:** We investigate a clustering problem with data from a mixture of Gaussians that share a common but unknown, and potentially ill-conditioned, covariance matrix. We start by considering Gaussian mixtures with two equally-sized components and derive a Max-Cut integer program based on maximum likelihood estimation. We prove its solutions achieve the optimal misclassification rate when the number of samples grows linearly in the dimension, up to a logarithmic factor. However, solving the Max-cut problem appears to be computationally intractable. To overcome this, we develop an efficient spectral algorithm that attains the optimal rate but requires a quadratic sample size. Although this sample complexity is worse than that of the Max-cut problem, we conjecture that no polynomial-time method can perform better. Furthermore, we gather numerical and theoretical evidence that supports the existence of a statistical-computational gap. Finally, we generalize the Max-Cut program to a $k$-means program that handles multi-component mixtures with possibly unequal weights. It enjoys similar optimality guarantees for mixtures of distributions that satisfy a transportation-cost inequality, encompassing Gaussian and strongly log-concave distributions.

</p>
</details>

<details><summary><b>Generalized Kernel Thinning</b>
<a href="https://arxiv.org/abs/2110.01593">arxiv:2110.01593</a>
&#x1F4C8; 2 <br>
<p>Raaz Dwivedi, Lester Mackey</p></summary>
<p>

**Abstract:** The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel. Third, we prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In our experiments with target KT and KT+, we witness significant improvements in integration error even in $100$ dimensions and when compressing challenging differential equation posteriors.

</p>
</details>

<details><summary><b>A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning</b>
<a href="https://arxiv.org/abs/2110.01515">arxiv:2110.01515</a>
&#x1F4C8; 2 <br>
<p>Iris A. M. Huijben, Wouter Kool, Max B. Paulus, Ruud J. G. van Sloun</p></summary>
<p>

**Abstract:** The Gumbel-max trick is a method to draw a sample from a categorical distribution, given by its unnormalized (log-)probabilities. Over the past years, the machine learning community has proposed several extensions of this trick to facilitate, e.g., drawing multiple samples, sampling from structured domains, or gradient estimation for error backpropagation in neural network optimization. The goal of this survey article is to present background about the Gumbel-max trick, and to provide a structured overview of its extensions to ease algorithm selection. Moreover, it presents a comprehensive outline of (machine learning) literature in which Gumbel-based algorithms have been leveraged, reviews commonly-made design choices, and sketches a future perspective.

</p>
</details>

<details><summary><b>Multi-Agent Path Planning Using Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2110.01460">arxiv:2110.01460</a>
&#x1F4C8; 2 <br>
<p>Mert Çetinkaya</p></summary>
<p>

**Abstract:** In this paper a deep reinforcement based multi-agent path planning approach is introduced. The experiments are realized in a simulation environment and in this environment different multi-agent path planning problems are produced. The produced problems are actually similar to a vehicle routing problem and they are solved using multi-agent deep reinforcement learning. In the simulation environment, the model is trained on different consecutive problems in this way and, as the time passes, it is observed that the model's performance to solve a problem increases. Always the same simulation environment is used and only the location of target points for the agents to visit is changed. This contributes the model to learn its environment and the right attitude against a problem as the episodes pass. At the end, a model who has already learned a lot to solve a path planning or routing problem in this environment is obtained and this model can already find a nice and instant solution to a given unseen problem even without any training. In routing problems, standard mathematical modeling or heuristics seem to suffer from high computational time to find the solution and it is also difficult and critical to find an instant solution. In this paper a new solution method against these points is proposed and its efficiency is proven experimentally.

</p>
</details>

<details><summary><b>Causality and Generalizability: Identifiability and Learning Methods</b>
<a href="https://arxiv.org/abs/2110.01430">arxiv:2110.01430</a>
&#x1F4C8; 2 <br>
<p>Martin Emil Jakobsen</p></summary>
<p>

**Abstract:** This PhD thesis contains several contributions to the field of statistical causal modeling. Statistical causal models are statistical models embedded with causal assumptions that allow for the inference and reasoning about the behavior of stochastic systems affected by external manipulation (interventions). This thesis contributes to the research areas concerning the estimation of causal effects, causal structure learning, and distributionally robust (out-of-distribution generalizing) prediction methods. We present novel and consistent linear and non-linear causal effects estimators in instrumental variable settings that employ data-dependent mean squared prediction error regularization. Our proposed estimators show, in certain settings, mean squared error improvements compared to both canonical and state-of-the-art estimators. We show that recent research on distributionally robust prediction methods has connections to well-studied estimators from econometrics. This connection leads us to prove that general K-class estimators possess distributional robustness properties. We, furthermore, propose a general framework for distributional robustness with respect to intervention-induced distributions. In this framework, we derive sufficient conditions for the identifiability of distributionally robust prediction methods and present impossibility results that show the necessity of several of these conditions. We present a new structure learning method applicable in additive noise models with directed trees as causal graphs. We prove consistency in a vanishing identifiability setup and provide a method for testing substructure hypotheses with asymptotic family-wise error control that remains valid post-selection. Finally, we present heuristic ideas for learning summary graphs of nonlinear time-series models.

</p>
</details>

<details><summary><b>Automating Privilege Escalation with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2110.01362">arxiv:2110.01362</a>
&#x1F4C8; 2 <br>
<p>Kalle Kujanpää, Willie Victor, Alexander Ilin</p></summary>
<p>

**Abstract:** AI-based defensive solutions are necessary to defend networks and information assets against intelligent automated attacks. Gathering enough realistic data for training machine learning-based defenses is a significant practical challenge. An intelligent red teaming agent capable of performing realistic attacks can alleviate this problem. However, there is little scientific evidence demonstrating the feasibility of fully automated attacks using machine learning. In this work, we exemplify the potential threat of malicious actors using deep reinforcement learning to train automated agents. We present an agent that uses a state-of-the-art reinforcement learning algorithm to perform local privilege escalation. Our results show that the autonomous agent can escalate privileges in a Windows 7 environment using a wide variety of different techniques depending on the environment configuration it encounters. Hence, our agent is usable for generating realistic attack sensor data for training and evaluating intrusion detection systems.

</p>
</details>

<details><summary><b>Learning to Assist Agents by Observing Them</b>
<a href="https://arxiv.org/abs/2110.01311">arxiv:2110.01311</a>
&#x1F4C8; 2 <br>
<p>Antti Keurulainen, Isak Westerlund, Samuel Kaski, Alexander Ilin</p></summary>
<p>

**Abstract:** The ability of an AI agent to assist other agents, such as humans, is an important and challenging goal, which requires the assisting agent to reason about the behavior and infer the goals of the assisted agent. Training such an ability by using reinforcement learning usually requires large amounts of online training, which is difficult and costly. On the other hand, offline data about the behavior of the assisted agent might be available, but is non-trivial to take advantage of by methods such as offline reinforcement learning. We introduce methods where the capability to create a representation of the behavior is first pre-trained with offline data, after which only a small amount of interaction data is needed to learn an assisting policy. We test the setting in a gridworld where the helper agent has the capability to manipulate the environment of the assisted artificial agents, and introduce three different scenarios where the assistance considerably improves the performance of the assisted agents.

</p>
</details>

<details><summary><b>Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values</b>
<a href="https://arxiv.org/abs/2110.01307">arxiv:2110.01307</a>
&#x1F4C8; 2 <br>
<p>Alexandre Heuillet, Fabien Couthouis, Natalia Díaz-Rodríguez</p></summary>
<p>

**Abstract:** While Explainable Artificial Intelligence (XAI) is increasingly expanding more areas of application, little has been applied to make deep Reinforcement Learning (RL) more comprehensible. As RL becomes ubiquitous and used in critical and general public applications, it is essential to develop methods that make it better understood and more interpretable. This study proposes a novel approach to explain cooperative strategies in multiagent RL using Shapley values, a game theory concept used in XAI that successfully explains the rationale behind decisions taken by Machine Learning algorithms. Through testing common assumptions of this technique in two cooperation-centered socially challenging multi-agent environments environments, this article argues that Shapley values are a pertinent way to evaluate the contribution of players in a cooperative multi-agent RL context. To palliate the high overhead of this method, Shapley values are approximated using Monte Carlo sampling. Experimental results on Multiagent Particle and Sequential Social Dilemmas show that Shapley values succeed at estimating the contribution of each agent. These results could have implications that go beyond games in economics, (e.g., for non-discriminatory decision making, ethical and responsible AI-derived decisions or policy making under fairness constraints). They also expose how Shapley values only give general explanations about a model and cannot explain a single run, episode nor justify precise actions taken by agents. Future work should focus on addressing these critical aspects.

</p>
</details>

<details><summary><b>Synthetic Velocity Mapping Cardiac MRI Coupled with Automated Left Ventricle Segmentation</b>
<a href="https://arxiv.org/abs/2110.01304">arxiv:2110.01304</a>
&#x1F4C8; 2 <br>
<p>Xiaodan Xing, Yinzhe Wu, David Firmin, Peter Gatehouse, Guang Yang</p></summary>
<p>

**Abstract:** Temporal patterns of cardiac motion provide important information for cardiac disease diagnosis. This pattern could be obtained by three-directional CINE multi-slice left ventricular myocardial velocity mapping (3Dir MVM), which is a cardiac MR technique providing magnitude and phase information of the myocardial motion simultaneously. However, long acquisition time limits the usage of this technique by causing breathing artifacts, while shortening the time causes low temporal resolution and may provide an inaccurate assessment of cardiac motion. In this study, we proposed a frame synthesis algorithm to increase the temporal resolution of 3Dir MVM data. Our algorithm is featured by 1) three attention-based encoders which accept magnitude images, phase images, and myocardium segmentation masks respectively as inputs; 2) three decoders that output the interpolated frames and corresponding myocardium segmentation results; and 3) loss functions highlighting myocardium pixels. Our algorithm can not only increase the temporal resolution 3Dir MVMs, but can also generates the myocardium segmentation results at the same time.

</p>
</details>

<details><summary><b>Light-weight Deformable Registration using Adversarial Learning with Distilling Knowledge</b>
<a href="https://arxiv.org/abs/2110.01293">arxiv:2110.01293</a>
&#x1F4C8; 2 <br>
<p>Minh Q. Tran, Tuong Do, Huy Tran, Erman Tjiputra, Quang D. Tran, Anh Nguyen</p></summary>
<p>

**Abstract:** Deformable registration is a crucial step in many medical procedures such as image-guided surgery and radiation therapy. Most recent learning-based methods focus on improving the accuracy by optimizing the non-linear spatial correspondence between the input images. Therefore, these methods are computationally expensive and require modern graphic cards for real-time deployment. In this paper, we introduce a new Light-weight Deformable Registration network that significantly reduces the computational cost while achieving competitive accuracy. In particular, we propose a new adversarial learning with distilling knowledge algorithm that successfully leverages meaningful information from the effective but expensive teacher network to the student network. We design the student network such as it is light-weight and well suitable for deployment on a typical CPU. The extensively experimental results on different public datasets show that our proposed method achieves state-of-the-art accuracy while significantly faster than recent methods. We further show that the use of our adversarial learning algorithm is essential for a time-efficiency deformable registration method. Finally, our source code and trained models are available at: https://github.com/aioz-ai/LDR_ALDK.

</p>
</details>

<details><summary><b>An AO-ADMM approach to constraining PARAFAC2 on all modes</b>
<a href="https://arxiv.org/abs/2110.01278">arxiv:2110.01278</a>
&#x1F4C8; 2 <br>
<p>Marie Roald, Carla Schenker, Rasmus Bro, Jeremy E. Cohen, Evrim Acar</p></summary>
<p>

**Abstract:** Analyzing multi-way measurements with variations across one mode of the dataset is a challenge in various fields including data mining, neuroscience and chemometrics. For example, measurements may evolve over time or have unaligned time profiles. The PARAFAC2 model has been successfully used to analyze such data by allowing the underlying factor matrices in one mode (i.e., the evolving mode) to change across slices. The traditional approach to fit a PARAFAC2 model is to use an alternating least squares-based algorithm, which handles the constant cross-product constraint of the PARAFAC2 model by implicitly estimating the evolving factor matrices. This approach makes imposing regularization on these factor matrices challenging. There is currently no algorithm to flexibly impose such regularization with general penalty functions and hard constraints. In order to address this challenge and to avoid the implicit estimation, in this paper, we propose an algorithm for fitting PARAFAC2 based on alternating optimization with the alternating direction method of multipliers (AO-ADMM). With numerical experiments on simulated data, we show that the proposed PARAFAC2 AO-ADMM approach allows for flexible constraints, recovers the underlying patterns accurately, and is computationally efficient compared to the state-of-the-art. We also apply our model to a real-world chromatography dataset, and show that constraining the evolving mode improves the interpretability of the extracted patterns.

</p>
</details>

<details><summary><b>Git: Clustering Based on Graph of Intensity Topology</b>
<a href="https://arxiv.org/abs/2110.01274">arxiv:2110.01274</a>
&#x1F4C8; 2 <br>
<p>Zhangyang Gao, Haitao Lin, Cheng Tan, Lirong Wu, Stan. Z Li</p></summary>
<p>

**Abstract:** \textbf{A}ccuracy, \textbf{R}obustness to noises and scales, \textbf{I}nterpretability, \textbf{S}peed, and \textbf{E}asy to use (ARISE) are crucial requirements of a good clustering algorithm. However, achieving these goals simultaneously is challenging, and most advanced approaches only focus on parts of them. Towards an overall consideration of these aspects, we propose a novel clustering algorithm, namely GIT (Clustering Based on \textbf{G}raph of \textbf{I}ntensity \textbf{T}opology). GIT considers both local and global data structures: firstly forming local clusters based on intensity peaks of samples, and then estimating the global topological graph (topo-graph) between these local clusters. We use the Wasserstein Distance between the predicted and prior class proportions to automatically cut noisy edges in the topo-graph and merge connected local clusters as final clusters. Then, we compare GIT with seven competing algorithms on five synthetic datasets and nine real-world datasets. With fast local cluster detection, robust topo-graph construction and accurate edge-cutting, GIT shows attractive ARISE performance and significantly exceeds other non-convex clustering methods. For example, GIT outperforms its counterparts about $10\%$ (F1-score) on MNIST and FashionMNIST. Code is available at \color{red}{https://github.com/gaozhangyang/GIT}.

</p>
</details>

<details><summary><b>Row-clustering of a Point Process-valued Matrix</b>
<a href="https://arxiv.org/abs/2110.01207">arxiv:2110.01207</a>
&#x1F4C8; 2 <br>
<p>Lihao Yin, Ganggang Xu, Huiyan Sang, Yongtao Guan</p></summary>
<p>

**Abstract:** Structured point process data harvested from various platforms poses new challenges to the machine learning community. By imposing a matrix structure to repeatedly observed marked point processes, we propose a novel mixture model of multi-level marked point processes for identifying potential heterogeneity in the observed data. Specifically, we study a matrix whose entries are marked log-Gaussian Cox processes and cluster rows of such a matrix. An efficient semi-parametric Expectation-Solution (ES) algorithm combined with functional principal component analysis (FPCA) of point processes is proposed for model estimation. The effectiveness of the proposed framework is demonstrated through simulation studies and a real data analysis.

</p>
</details>

<details><summary><b>LegalNLP -- Natural Language Processing methods for the Brazilian Legal Language</b>
<a href="https://arxiv.org/abs/2110.15709">arxiv:2110.15709</a>
&#x1F4C8; 1 <br>
<p>Felipe Maia Polo, Gabriel Caiaffa Floriano Mendonça, Kauê Capellato J. Parreira, Lucka Gianvechio, Peterson Cordeiro, Jonathan Batista Ferreira, Leticia Maria Paz de Lima, Antônio Carlos do Amaral Maia, Renato Vicente</p></summary>
<p>

**Abstract:** We present and make available pre-trained language models (Phraser, Word2Vec, Doc2Vec, FastText, and BERT) for the Brazilian legal language, a Python package with functions to facilitate their use, and a set of demonstrations/tutorials containing some applications involving them. Given that our material is built upon legal texts coming from several Brazilian courts, this initiative is extremely helpful for the Brazilian legal field, which lacks other open and specific tools and language models. Our main objective is to catalyze the use of natural language processing tools for legal texts analysis by the Brazilian industry, government, and academia, providing the necessary tools and accessible material.

</p>
</details>

<details><summary><b>Modeling Effect of Lockdowns and Other Effects on India Covid-19 Infections Using SEIR Model and Machine Learning</b>
<a href="https://arxiv.org/abs/2110.03422">arxiv:2110.03422</a>
&#x1F4C8; 1 <br>
<p>Sathiyanarayanan Sampath, Joy Bose</p></summary>
<p>

**Abstract:** The SEIR model is a widely used epidemiological model used to predict the rise in infections. This model has been widely used in different countries to predict the number of Covid-19 cases. But the original SEIR model does not take into account the effect of factors such as lockdowns, vaccines, and re-infections. In India the first wave of Covid started in March 2020 and the second wave in April 2021. In this paper, we modify the SEIR model equations to model the effect of lockdowns and other influencers, and fit the model on data of the daily Covid-19 infections in India using lmfit, a python library for least squares minimization for curve fitting. We modify R0 parameter in the standard SEIR model as a rectangle in order to account for the effect of lockdowns. Our modified SEIR model accurately fits the available data of infections.

</p>
</details>

<details><summary><b>A Deep Reinforcement Learning Framework for Contention-Based Spectrum Sharing</b>
<a href="https://arxiv.org/abs/2110.02736">arxiv:2110.02736</a>
&#x1F4C8; 1 <br>
<p>Akash Doshi, Srinivas Yerramalli, Lorenzo Ferrari, Taesang Yoo, Jeffrey G. Andrews</p></summary>
<p>

**Abstract:** The increasing number of wireless devices operating in unlicensed spectrum motivates the development of intelligent adaptive approaches to spectrum access. We consider decentralized contention-based medium access for base stations (BSs) operating on unlicensed shared spectrum, where each BS autonomously decides whether or not to transmit on a given resource. The contention decision attempts to maximize not its own downlink throughput, but rather a network-wide objective. We formulate this problem as a decentralized partially observable Markov decision process with a novel reward structure that provides long term proportional fairness in terms of throughput. We then introduce a two-stage Markov decision process in each time slot that uses information from spectrum sensing and reception quality to make a medium access decision. Finally, we incorporate these features into a distributed reinforcement learning framework for contention-based spectrum access. Our formulation provides decentralized inference, online adaptability and also caters to partial observability of the environment through recurrent Q-learning. Empirically, we find its maximization of the proportional fairness metric to be competitive with a genie-aided adaptive energy detection threshold, while being robust to channel fading and small contention windows.

</p>
</details>

<details><summary><b>A Survey On Neural Word Embeddings</b>
<a href="https://arxiv.org/abs/2110.01804">arxiv:2110.01804</a>
&#x1F4C8; 1 <br>
<p>Erhan Sezerer, Selma Tekir</p></summary>
<p>

**Abstract:** Understanding human language has been a sub-challenge on the way of intelligent machines. The study of meaning in natural language processing (NLP) relies on the distributional hypothesis where language elements get meaning from the words that co-occur within contexts. The revolutionary idea of distributed representation for a concept is close to the working of a human mind in that the meaning of a word is spread across several neurons, and a loss of activation will only slightly affect the memory retrieval process.
  Neural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this survey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe existing work by an interplay between word embeddings and language modelling. We provide broad coverage on neural word embeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme embeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings' performance evaluation and downstream tasks along with the performance results of/due to word embeddings.

</p>
</details>

<details><summary><b>A Modified Q-Learning Algorithm for Rate-Profiling of Polarization Adjusted Convolutional (PAC) Codes</b>
<a href="https://arxiv.org/abs/2110.01563">arxiv:2110.01563</a>
&#x1F4C8; 1 <br>
<p>Samir Kumar Mishra, Digvijay Katyal, Sarvesha Anegundi Ganapathi</p></summary>
<p>

**Abstract:** In this paper, we propose a reinforcement learning based algorithm for rate-profile construction of Arikan's Polarization Assisted Convolutional (PAC) codes. This method can be used for any blocklength, rate, list size under successive cancellation list (SCL) decoding and convolutional precoding polynomial. To the best of our knowledge, we present, for the first time, a set of new reward and update strategies which help the reinforcement learning agent discover much better rate-profiles than those present in existing literature. Simulation results show that PAC codes constructed with the proposed algorithm perform better in terms of frame erasure rate (FER) compared to the PAC codes constructed with contemporary rate profiling designs for various list lengths. Further, by using a (64, 32) PAC code as an example, it is shown that the choice of convolutional precoding polynomial can have a significant impact on rate-profile construction of PAC codes.

</p>
</details>

<details><summary><b>Perhaps PTLMs Should Go to School -- A Task to Assess Open Book and Closed Book QA</b>
<a href="https://arxiv.org/abs/2110.01552">arxiv:2110.01552</a>
&#x1F4C8; 1 <br>
<p>Manuel R. Ciosici, Joe Cecil, Alex Hedges, Dong-Ho Lee, Marjorie Freedman, Ralph Weischedel</p></summary>
<p>

**Abstract:** Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true/false statements based on review questions written by the textbook authors, validation/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be ~50%. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook's content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5's pre-training), yields at best minor improvement (56%), suggesting that the PTLM may not have "understood" the textbook (or perhaps misunderstood the questions). Performance is better (~60%) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question).

</p>
</details>

<details><summary><b>Scaling Graph-based Deep Learning models to larger networks</b>
<a href="https://arxiv.org/abs/2110.01261">arxiv:2110.01261</a>
&#x1F4C8; 1 <br>
<p>Miquel Ferriol-Galmés, José Suárez-Varela, Krzysztof Rusek, Pere Barlet-Ros, Albert Cabellos-Aparicio</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNN) have shown a strong potential to be integrated into commercial products for network control and management. Early works using GNN have demonstrated an unprecedented capability to learn from different network characteristics that are fundamentally represented as graphs, such as the topology, the routing configuration, or the traffic that flows along a series of nodes in the network. In contrast to previous solutions based on Machine Learning (ML), GNN enables to produce accurate predictions even in other networks unseen during the training phase. Nowadays, GNN is a hot topic in the Machine Learning field and, as such, we are witnessing great efforts to leverage its potential in many different fields (e.g., chemistry, physics, social networks). In this context, the Graph Neural Networking challenge 2021 brings a practical limitation of existing GNN-based solutions for networking: the lack of generalization to larger networks. This paper approaches the scalability problem by presenting a GNN-based solution that can effectively scale to larger networks including higher link capacities and aggregated traffic on links.

</p>
</details>

<details><summary><b>Optimal Placement of Roadside Infrastructure Sensors towards Safer Autonomous Vehicle Deployments</b>
<a href="https://arxiv.org/abs/2110.01251">arxiv:2110.01251</a>
&#x1F4C8; 1 <br>
<p>Roshan Vijay, Jim Cherian, Rachid Riah, Niels de Boer, Apratim Choudhury</p></summary>
<p>

**Abstract:** Vehicles with driving automation are increasingly being developed for deployment across the world. However, the onboard sensing and perception capabilities of such automated or autonomous vehicles (AV) may not be sufficient to ensure safety under all scenarios and contexts. Infrastructure-augmented environment perception using roadside infrastructure sensors can be considered as an effective solution, at least for selected regions of interest such as urban road intersections or curved roads that present occlusions to the AV. However, they incur significant costs for procurement, installation and maintenance. Therefore these sensors must be placed strategically and optimally to yield maximum benefits in terms of the overall safety of road users. In this paper, we propose a novel methodology towards obtaining an optimal placement of V2X (Vehicle-to-everything) infrastructure sensors, which is particularly attractive to urban AV deployments, with various considerations including costs, coverage and redundancy. We combine the latest advances made in raycasting and linear optimization literature to deliver a tool for urban city planners, traffic analysis and AV deployment operators. Through experimental evaluation in representative environments, we prove the benefits and practicality of our approach.

</p>
</details>

<details><summary><b>Inducing Equilibria via Incentives: Simultaneous Design-and-Play Finds Global Optima</b>
<a href="https://arxiv.org/abs/2110.01212">arxiv:2110.01212</a>
&#x1F4C8; 1 <br>
<p>Boyi Liu, Jiayang Li, Zhuoran Yang, Hoi-To Wai, Mingyi Hong, Yu Marco Nie, Zhaoran Wang</p></summary>
<p>

**Abstract:** To regulate a social system comprised of self-interested agents, economic incentives (e.g., taxes, tolls, and subsidies) are often required to induce a desirable outcome. This incentive design problem naturally possesses a bi-level structure, in which an upper-level "designer" modifies the payoffs of the agents with incentives while anticipating the response of the agents at the lower level, who play a non-cooperative game that converges to an equilibrium. The existing bi-level optimization algorithms developed in machine learning raise a dilemma when applied to this problem: anticipating how incentives affect the agents at equilibrium requires solving the equilibrium problem repeatedly, which is computationally inefficient; bypassing the time-consuming step of equilibrium-finding can reduce the computational cost, but may lead the designer to a sub-optimal solution. To address such a dilemma, we propose a method that tackles the designer's and agents' problems simultaneously in a single loop. In particular, at each iteration, both the designer and the agents only move one step based on the first-order information. In the proposed scheme, although the designer does not solve the equilibrium problem repeatedly, it can anticipate the overall influence of the incentives on the agents, which guarantees optimality. We prove that the algorithm converges to the global optima at a sublinear rate for a broad class of games.

</p>
</details>

<details><summary><b>Solving even-parity problems using traceless genetic programming</b>
<a href="https://arxiv.org/abs/2110.02014">arxiv:2110.02014</a>
&#x1F4C8; 0 <br>
<p>Mihai Oltean</p></summary>
<p>

**Abstract:** A genetic programming (GP) variant called traceless genetic programming (TGP) is proposed in this paper. TGP is a hybrid method combining a technique for building individuals and a technique for representing individuals. The main difference between TGP and other GP techniques is that TGP does not explicitly store the evolved computer programs. Two genetic operators are used in conjunction with TGP: crossover and insertion. TGP is applied for evolving digital circuits for the even-parity problem. Numerical experiments show that TGP outperforms standard GP with several orders of magnitude.

</p>
</details>

<details><summary><b>An Improved Genetic Algorithm and Its Application in Neural Network Adversarial Attack</b>
<a href="https://arxiv.org/abs/2110.01818">arxiv:2110.01818</a>
&#x1F4C8; 0 <br>
<p>Dingming Yang, Zeyu Yu, Hongqiang Yuan, Yanrong Cui</p></summary>
<p>

**Abstract:** The choice of crossover and mutation strategies plays a crucial role in the search ability, convergence efficiency and precision of genetic algorithms. In this paper, a novel improved genetic algorithm is proposed by improving the crossover and mutation operation of the simple genetic algorithm, and it is verified by four test functions. Simulation results show that, comparing with three other mainstream swarm intelligence optimization algorithms, the algorithm can not only improve the global search ability, convergence efficiency and precision, but also increase the success rate of convergence to the optimal value under the same experimental conditions. Finally, the algorithm is applied to neural networks adversarial attacks. The applied results show that the method does not need the structure and parameter information inside the neural network model, and it can obtain the adversarial samples with high confidence in a brief time just by the classification and confidence information output from the neural network.

</p>
</details>

<details><summary><b>Feasible Architecture for Quantum Fully Convolutional Networks</b>
<a href="https://arxiv.org/abs/2110.01771">arxiv:2110.01771</a>
&#x1F4C8; 0 <br>
<p>Yusui Chen, Wenhao Hu, Xiang Li</p></summary>
<p>

**Abstract:** Fully convolutional networks are robust in performing semantic segmentation, with many applications from signal processing to computer vision. From the fundamental principles of variational quantum algorithms, we propose a feasible pure quantum architecture that can be operated on noisy intermediate-scale quantum devices. In this work, a parameterized quantum circuit consisting of three layers, convolutional, pooling, and upsampling, is characterized by generative one-qubit and two-qubit gates and driven by a classical optimizer. This architecture supplies a solution for realizing the dynamical programming on a one-way quantum computer and maximally taking advantage of quantum computing throughout the calculation. Moreover, our algorithm works on many physical platforms, and particularly the upsampling layer can use either conventional qubits or multiple-level systems. Through numerical simulations, our study represents the successful training of a pure quantum fully convolutional network and discusses advantages by comparing it with the hybrid solution.

</p>
</details>

<details><summary><b>Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning</b>
<a href="https://arxiv.org/abs/2110.01770">arxiv:2110.01770</a>
&#x1F4C8; 0 <br>
<p>Jing Bi, Jiebo Luo, Chenliang Xu</p></summary>
<p>

**Abstract:** Learning new skills by observing humans' behaviors is an essential capability of AI. In this work, we leverage instructional videos to study humans' decision-making processes, focusing on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous latent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We address these limitations with a new formulation of procedure planning and propose novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Experiments conducted on real-world instructional videos show that our method can achieve state-of-the-art performance in reaching the indicated goals. Furthermore, the learned contextual information presents interesting features for planning in a latent space.

</p>
</details>

<details><summary><b>Controlled-Variable Selection based on Chaos Theory for the Tennessee Eastman Plant</b>
<a href="https://arxiv.org/abs/2110.01759">arxiv:2110.01759</a>
&#x1F4C8; 0 <br>
<p>S. F. Yapur</p></summary>
<p>

**Abstract:** This work explores a link between chaotic signals and the selection of controlled variables for plantwide control system design. Some results are shown for the Tennessee Eastman plant, which is well-known for being a challenging process in the field of plant-wide control. This article provides a systematic, data-driven method to select which variables should be controlled. However, since plantwide control problems are inherently complex, this work does not intend to provide a definite solution, but a complementary analysis to take into account towards the final control system design. The discussion highlights the potential hidden in the chaos theory to reduce the complexity of the resulting control system.

</p>
</details>

<details><summary><b>RASA: Efficient Register-Aware Systolic Array Matrix Engine for CPU</b>
<a href="https://arxiv.org/abs/2110.01752">arxiv:2110.01752</a>
&#x1F4C8; 0 <br>
<p>Geonhwa Jeong, Eric Qin, Ananda Samajdar, Christopher J. Hughes, Sreenivas Subramoney, Hyesoon Kim, Tushar Krishna</p></summary>
<p>

**Abstract:** As AI-based applications become pervasive, CPU vendors are starting to incorporate matrix engines within the datapath to boost efficiency. Systolic arrays have been the premier architectural choice as matrix engines in offload accelerators. However, we demonstrate that incorporating them inside CPUs can introduce under-utilization and stalls due to limited register storage to amortize the fill and drain times of the array. To address this, we propose RASA, Register-Aware Systolic Array. We develop techniques to divide an execution stage into several sub-stages and overlap instructions to hide overheads and run them concurrently. RASA-based designs improve performance significantly with negligible area and power overhead.

</p>
</details>

<details><summary><b>Seizure Classification Using Parallel Genetic Naive Bayes Classifiers</b>
<a href="https://arxiv.org/abs/2110.01742">arxiv:2110.01742</a>
&#x1F4C8; 0 <br>
<p>Scot Davidson, Niamh McCallan, Kok Yew Ng, Pardis Biglarbeigi, Dewar Finlay, Boon Leong Lan, James McLaughlin</p></summary>
<p>

**Abstract:** Epilepsy affects 50 million people worldwide and is one of the most common serious brain disorders. Seizure detection and classification is a valuable tool for maintaining the condition. An automated detection algorithm will allow for accurate diagnosis. This study proposes a method using unique features with a novel parallel classifier trained using a genetic algorithm. Ictal states from the EEG are segmented into 1.8 s windows, where the epochs are then further decomposed into 13 different features from the first IMF. All of the features are fed into a genetic algorithm (Binary Grey Wolf Optimisation Option 1) with a Naive Bayes classifier. Combining the simple partial and complex partial seizures provides the highest accuracy of all the models tested.

</p>
</details>

<details><summary><b>Stochastic functional analysis with applications to robust machine learning</b>
<a href="https://arxiv.org/abs/2110.01729">arxiv:2110.01729</a>
&#x1F4C8; 0 <br>
<p>Julio Enrique Castrillon-Candas, Dingning Liu, Mark Kon</p></summary>
<p>

**Abstract:** It is well-known that machine learning protocols typically under-utilize information on the probability distributions of feature vectors and related data, and instead directly compute regression or classification functions of feature vectors. In this paper we introduce a set of novel features for identifying underlying stochastic behavior of input data using the Karhunen-Loéve (KL) expansion, where classification is treated as detection of anomalies from a (nominal) signal class. These features are constructed from the recent Functional Data Analysis (FDA) theory for anomaly detection. The related signal decomposition is an exact hierarchical tensor product expansion with known optimality properties for approximating stochastic processes (random fields) with finite dimensional function spaces. In principle these primary low dimensional spaces can capture most of the stochastic behavior of `underlying signals' in a given nominal class, and can reject signals in alternative classes as stochastic anomalies. Using a hierarchical finite dimensional KL expansion of the nominal class, a series of orthogonal nested subspaces is constructed for detecting anomalous signal components. Projection coefficients of input data in these subspaces are then used to train an ML classifier. However, due to the split of the signal into nominal and anomalous projection components, clearer separation surfaces of the classes arise. In fact we show that with a sufficiently accurate estimation of the covariance structure of the nominal class, a sharp classification can be obtained. We carefully formulate this concept and demonstrate it on a number of high-dimensional datasets in cancer diagnostics. This method leads to a significant increase in precision and accuracy over the current top benchmarks for the Global Cancer Map (GCM) gene expression network dataset.

</p>
</details>

<details><summary><b>Wireless Link Scheduling via Graph Representation Learning: A Comparative Study of Different Supervision Levels</b>
<a href="https://arxiv.org/abs/2110.01722">arxiv:2110.01722</a>
&#x1F4C8; 0 <br>
<p>Navid Naderializadeh</p></summary>
<p>

**Abstract:** We consider the problem of binary power control, or link scheduling, in wireless interference networks, where the power control policy is trained using graph representation learning. We leverage the interference graph of the wireless network as an underlying topology for a graph neural network (GNN) backbone, which converts the channel matrix to a set of node embeddings for all transmitter-receiver pairs. We show how the node embeddings can be trained in several ways, including via supervised, unsupervised, and self-supervised learning, and we compare the impact of different supervision levels on the performance of these methods in terms of the system-level throughput, convergence behavior, sample efficiency, and generalization capability.

</p>
</details>

<details><summary><b>Learning to shortcut and shortlist order fulfillment deciding</b>
<a href="https://arxiv.org/abs/2110.01668">arxiv:2110.01668</a>
&#x1F4C8; 0 <br>
<p>Brian Quanz, Ajay Deshpande, Dahai Xing, Xuan Liu</p></summary>
<p>

**Abstract:** With the increase of order fulfillment options and business objectives taken into consideration in the deciding process, order fulfillment deciding is becoming more and more complex. For example, with the advent of ship from store retailers now have many more fulfillment nodes to consider, and it is now common to take into account many and varied business goals in making fulfillment decisions. With increasing complexity, efficiency of the deciding process can become a real concern. Finding the optimal fulfillment assignments among all possible ones may be too costly to do for every order especially during peak times. In this work, we explore the possibility of exploiting regularity in the fulfillment decision process to reduce the burden on the deciding system. By using data mining we aim to find patterns in past fulfillment decisions that can be used to efficiently predict most likely assignments for future decisions. Essentially, those assignments that can be predicted with high confidence can be used to shortcut, or bypass, the expensive deciding process, or else a set of most likely assignments can be used for shortlisting -- sending a much smaller set of candidates for consideration by the fulfillment deciding system.

</p>
</details>

<details><summary><b>Estimating Potential Outcome Distributions with Collaborating Causal Networks</b>
<a href="https://arxiv.org/abs/2110.01664">arxiv:2110.01664</a>
&#x1F4C8; 0 <br>
<p>Tianhui Zhou, David Carlson</p></summary>
<p>

**Abstract:** Many causal inference approaches have focused on identifying an individual's outcome change due to a potential treatment, or the individual treatment effect (ITE), from observational studies. Rather than only estimating the ITE, we propose Collaborating Causal Networks (CCN) to estimate the full potential outcome distributions. This modification facilitates estimating the utility of each treatment and allows for individual variation in utility functions (e.g., variability in risk tolerance). We show that CCN learns distributions that asymptotically capture the correct potential outcome distributions under standard causal inference assumptions. Furthermore, we develop a new adjustment approach that is empirically effective in alleviating sample imbalance between treatment groups in observational studies. We evaluate CCN by extensive empirical experiments and demonstrate improved distribution estimates compared to existing Bayesian and Generative Adversarial Network-based methods. Additionally, CCN empirically improves decisions over a variety of utility functions.

</p>
</details>

<details><summary><b>Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction</b>
<a href="https://arxiv.org/abs/2110.01661">arxiv:2110.01661</a>
&#x1F4C8; 0 <br>
<p>Pit Schneider</p></summary>
<p>

**Abstract:** Iterating with new and improved OCR solutions enforces decisions to be taken when it comes to targeting the right reprocessing candidates. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those exact decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. As an extension of this technique, another contribution comes in the form of a regression model that takes the enhancement potential of a new OCR engine into account. They both mark promising approaches, especially for cultural institutions dealing with historic data of lower quality.

</p>
</details>

<details><summary><b>Improved architectures and training algorithms for deep operator networks</b>
<a href="https://arxiv.org/abs/2110.01654">arxiv:2110.01654</a>
&#x1F4C8; 0 <br>
<p>Sifan Wang, Hanwen Wang, Paris Perdikaris</p></summary>
<p>

**Abstract:** Operator learning techniques have recently emerged as a powerful tool for learning maps between infinite-dimensional Banach spaces. Trained under appropriate constraints, they can also be effective in learning the solution operator of partial differential equations (PDEs) in an entirely self-supervised manner. In this work we analyze the training dynamics of deep operator networks (DeepONets) through the lens of Neural Tangent Kernel (NTK) theory, and reveal a bias that favors the approximation of functions with larger magnitudes. To correct this bias we propose to adaptively re-weight the importance of each training example, and demonstrate how this procedure can effectively balance the magnitude of back-propagated gradients during training via gradient descent. We also propose a novel network architecture that is more resilient to vanishing gradient pathologies. Taken together, our developments provide new insights into the training of DeepONets and consistently improve their predictive accuracy by a factor of 10-50x, demonstrated in the challenging setting of learning PDE solution operators in the absence of paired input-output observations. All code and data accompanying this manuscript are publicly available at \url{https://github.com/PredictiveIntelligenceLab/ImprovedDeepONets.}

</p>
</details>

<details><summary><b>Learning to Solve the AC Optimal Power Flow via a Lagrangian Approach</b>
<a href="https://arxiv.org/abs/2110.01653">arxiv:2110.01653</a>
&#x1F4C8; 0 <br>
<p>Ling Zhang, Baosen Zhang</p></summary>
<p>

**Abstract:** Using deep neural networks to predict the solutions of AC optimal power flow (ACOPF) problems has been an active direction of research. However, because the ACOPF is nonconvex, it is difficult to construct a good data set that contains mostly globally optimal solutions. To overcome the challenge that the training data may contain suboptimal solutions, we propose a Lagrangian-based approach. First, we use a neural network to learn the dual variables of the ACOPF problem. Then we use a second neural network to predict solutions of the partial Lagrangian from the predicted dual variables. Since the partial Lagrangian has a much better optimization landscape, we use the predicted solutions from the neural network as a warm start for the ACOPF problem. Using standard and modified IEEE 22-bus, 39-bus, and 118-bus networks, we show that our approach is able to obtain the globally optimal cost even when the training data is mostly comprised of suboptimal solutions.

</p>
</details>

<details><summary><b>An energy-based model for neuro-symbolic reasoning on knowledge graphs</b>
<a href="https://arxiv.org/abs/2110.01639">arxiv:2110.01639</a>
&#x1F4C8; 0 <br>
<p>Dominik Dold, Josep Soler Garrido</p></summary>
<p>

**Abstract:** Machine learning on graph-structured data has recently become a major topic in industry and research, finding many exciting applications such as recommender systems and automated theorem proving. We propose an energy-based graph embedding algorithm to characterize industrial automation systems, integrating knowledge from different domains like industrial automation, communications and cybersecurity. By combining knowledge from multiple domains, the learned model is capable of making context-aware predictions regarding novel system events and can be used to evaluate the severity of anomalies that might be indicative of, e.g., cybersecurity breaches. The presented model is mappable to a biologically-inspired neural architecture, serving as a first bridge between graph embedding methods and neuromorphic computing - uncovering a promising edge application for this upcoming technology.

</p>
</details>

<details><summary><b>Learning Online Visual Invariances for Novel Objects via Supervised and Self-Supervised Training</b>
<a href="https://arxiv.org/abs/2110.01476">arxiv:2110.01476</a>
&#x1F4C8; 0 <br>
<p>Valerio Biscione, Jeffrey S. Bowers</p></summary>
<p>

**Abstract:** Humans can identify objects following various spatial transformations such as scale and viewpoint. This extends to novel objects, after a single presentation at a single pose, sometimes referred to as online invariance. CNNs have been proposed as a compelling model of human vision, but their ability to identify objects across transformations is typically tested on held-out samples of trained categories after extensive data augmentation. This paper assesses whether standard CNNs can support human-like online invariance by training models to recognize images of synthetic 3D objects that undergo several transformations: rotation, scaling, translation, brightness, contrast, and viewpoint. Through the analysis of models' internal representations, we show that standard supervised CNNs trained on transformed objects can acquire strong invariances on novel classes even when trained with as few as 50 objects taken from 10 classes. This extended to a different dataset of photographs of real objects. We also show that these invariances can be acquired in a self-supervised way, through solving the same/different task. We suggest that this latter approach may be similar to how humans acquire invariances.

</p>
</details>

<details><summary><b>Pharmacoprint -- a combination of pharmacophore fingerprint and artificial intelligence as a tool for computer-aided drug design</b>
<a href="https://arxiv.org/abs/2110.01339">arxiv:2110.01339</a>
&#x1F4C8; 0 <br>
<p>Dawid Warszycki, Łukasz Struski, Marek Śmieja, Rafał Kafel, Rafał Kurczab</p></summary>
<p>

**Abstract:** Structural fingerprints and pharmacophore modeling are methodologies that have been used for at least two decades in various fields of cheminformatics: from similarity searching to machine learning (ML). Advances in silico techniques consequently led to combining both these methodologies into a new approach known as pharmacophore fingerprint. Herein, we propose a high-resolution, pharmacophore fingerprint called Pharmacoprint that encodes the presence, types, and relationships between pharmacophore features of a molecule. Pharmacoprint was evaluated in classification experiments by using ML algorithms (logistic regression, support vector machines, linear support vector machines, and neural networks) and outperformed other popular molecular fingerprints (i.e., Estate, MACCS, PubChem, Substructure, Klekotha-Roth, CDK, Extended, and GraphOnly) and ChemAxon Pharmacophoric Features fingerprint. Pharmacoprint consisted of 39973 bits; several methods were applied for dimensionality reduction, and the best algorithm not only reduced the length of bit string but also improved the efficiency of ML tests. Further optimization allowed us to define the best parameter settings for using Pharmacoprint in discrimination tests and for maximizing statistical parameters. Finally, Pharmacoprint generated for 3D structures with defined hydrogens as input data was applied to neural networks with a supervised autoencoder for selecting the most important bits and allowed to maximize Matthews Correlation Coefficient up to 0.962. The results show the potential of Pharmacoprint as a new, perspective tool for computer-aided drug design.

</p>
</details>

<details><summary><b>Analysis of the relation between smartphone usage changes during the COVID-19 pandemic and usage preferences on apps</b>
<a href="https://arxiv.org/abs/2110.01331">arxiv:2110.01331</a>
&#x1F4C8; 0 <br>
<p>Yuxuan Yang, Maiko Shigeno</p></summary>
<p>

**Abstract:** Since the World Health Organization announced the COVID-19 pandemic in March 2020, curbing the spread of the virus has become an international priority. It has greatly affected people's lifestyles. In this article, we observe and analyze the impact of the pandemic on people's lives using changes in smartphone application usage. First, through observing the daily usage change trends of all users during the pandemic, we can understand and analyze the effects of restrictive measures and policies during the pandemic on people's lives. In addition, it is also helpful for the government and health departments to take more appropriate restrictive measures in the case of future pandemics. Second, we defined the usage change features and found 9 different usage change patterns during the pandemic according to clusters of users and show the diversity of daily usage changes. It helps to understand and analyze the different impacts of the pandemic and restrictive measures on different types of people in more detail. Finally, according to prediction models, we discover the main related factors of each usage change type from user preferences and demographic information. It helps to predict changes in smartphone activity during future pandemics or when other restrictive measures are implemented, which may become a new indicator to judge and manage the risks of measures or events.

</p>
</details>


[Next Page](2021/2021-10/2021-10-03.md)
