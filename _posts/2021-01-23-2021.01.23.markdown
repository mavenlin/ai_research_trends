Prev: [2021.01.22]({{ '/2021/01/22/2021.01.22.html' | relative_url }})  Next: [2021.01.24]({{ '/2021/01/24/2021.01.24.html' | relative_url }})
{% raw %}
## Summary for 2021-01-23, created on 2021-12-24


<details><summary><b>BF++: a language for general-purpose program synthesis</b>
<a href="https://arxiv.org/abs/2101.09571">arxiv:2101.09571</a>
&#x1F4C8; 217 <br>
<p>Vadim Liventsev, Aki Härmä, Milan Petković</p></summary>
<p>

**Abstract:** Most state of the art decision systems based on Reinforcement Learning (RL) are data-driven black-box neural models, where it is often difficult to incorporate expert knowledge into the models or let experts review and validate the learned decision mechanisms. Knowledge-insertion and model review are important requirements in many applications involving human health and safety. One way to bridge the gap between data and knowledge driven systems is program synthesis: replacing a neural network that outputs decisions with a symbolic program generated by a neural network or by means of genetic programming. We propose a new programming language, BF++, designed specifically for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.

</p>
</details>

<details><summary><b>Memory-Efficient Semi-Supervised Continual Learning: The World is its Own Replay Buffer</b>
<a href="https://arxiv.org/abs/2101.09536">arxiv:2101.09536</a>
&#x1F4C8; 14 <br>
<p>James Smith, Jonathan Balloch, Yen-Chang Hsu, Zsolt Kira</p></summary>
<p>

**Abstract:** Rehearsal is a critical component for class-incremental continual learning, yet it requires a substantial memory budget. Our work investigates whether we can significantly reduce this memory budget by leveraging unlabeled data from an agent's environment in a realistic and challenging continual learning paradigm. Specifically, we explore and formalize a novel semi-supervised continual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d. unlabeled data from the agent's environment is plentiful. Importantly, data distributions in the SSCL setting are realistic and therefore reflect object class correlations between, and among, the labeled and unlabeled data distributions. We show that a strategy built on pseudo-labeling, consistency regularization, Out-of-Distribution (OoD) detection, and knowledge distillation reduces forgetting in this setting. Our approach, DistillMatch, increases performance over the state-of-the-art by no less than 8.7% average task accuracy and up to 54.5% average task accuracy in SSCL CIFAR-100 experiments. Moreover, we demonstrate that DistillMatch can save up to 0.23 stored images per processed unlabeled image compared to the next best method which only saves 0.08. Our results suggest that focusing on realistic correlated distributions is a significantly new perspective, which accentuates the importance of leveraging the world's structure as a continual learning strategy.

</p>
</details>

<details><summary><b>Explainable Artificial Intelligence Approaches: A Survey</b>
<a href="https://arxiv.org/abs/2101.09429">arxiv:2101.09429</a>
&#x1F4C8; 10 <br>
<p>Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, Mohiuddin Ahmed</p></summary>
<p>

**Abstract:** The lack of explainability of a decision from an Artificial Intelligence (AI) based "black box" system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.

</p>
</details>

<details><summary><b>A Methodology for the Development of RL-Based Adaptive Traffic Signal Controllers</b>
<a href="https://arxiv.org/abs/2101.09614">arxiv:2101.09614</a>
&#x1F4C8; 8 <br>
<p>Guilherme S. Varela, Pedro P. Santos, Alberto Sardinha, Francisco S. Melo</p></summary>
<p>

**Abstract:** This article proposes a methodology for the development of adaptive traffic signal controllers using reinforcement learning. Our methodology addresses the lack of standardization in the literature that renders the comparison of approaches in different works meaningless, due to differences in metrics, environments, and even experimental design and methodology. The proposed methodology thus comprises all the steps necessary to develop, deploy and evaluate an adaptive traffic signal controller -- from simulation setup to problem formulation and experimental design. We illustrate the proposed methodology in two simple scenarios, highlighting how its different steps address limitations found in the current literature.

</p>
</details>

<details><summary><b>A Raspberry Pi-based Traumatic Brain Injury Detection System for Single-Channel Electroencephalogram</b>
<a href="https://arxiv.org/abs/2101.10869">arxiv:2101.10869</a>
&#x1F4C8; 7 <br>
<p>Navjodh Singh Dhillon, Agustinus Sutandi, Manoj Vishwanath, Miranda M. Lim, Hung Cao, Dong Si</p></summary>
<p>

**Abstract:** Traumatic Brain Injury (TBI) is a common cause of death and disability. However, existing tools for TBI diagnosis are either subjective or require extensive clinical setup and expertise. The increasing affordability and reduction in size of relatively high-performance computing systems combined with promising results from TBI related machine learning research make it possible to create compact and portable systems for early detection of TBI. This work describes a Raspberry Pi based portable, real-time data acquisition, and automated processing system that uses machine learning to efficiently identify TBI and automatically score sleep stages from a single-channel Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and verification of the system that can digitize EEG signal using an Analog to Digital Converter (ADC) and perform real-time signal classification to detect the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN) and XGBoost based predictive models to evaluate the performance and demonstrate the versatility of the system to operate with multiple types of predictive models. We achieve a peak classification accuracy of more than 90% with a classification time of less than 1 s across 16 s - 64 s epochs for TBI vs control conditions. This work can enable development of systems suitable for field use without requiring specialized medical equipment for early TBI detection applications and TBI research. Further, this work opens avenues to implement connected, real-time TBI related health and wellness monitoring systems.

</p>
</details>

<details><summary><b>Generative hypergraph clustering: from blockmodels to modularity</b>
<a href="https://arxiv.org/abs/2101.09611">arxiv:2101.09611</a>
&#x1F4C8; 7 <br>
<p>Philip S. Chodrow, Nate Veldt, Austin R. Benson</p></summary>
<p>

**Abstract:** Hypergraphs are a natural modeling paradigm for a wide range of complex relational systems. A standard analysis task is to identify clusters of closely related or densely interconnected nodes. Many graph algorithms for this task are based on variants of the stochastic blockmodel, a random graph with flexible cluster structure. However, there are few models and algorithms for hypergraph clustering. Here, we propose a Poisson degree-corrected hypergraph stochastic blockmodel (DCHSBM), a generative model of clustered hypergraphs with heterogeneous node degrees and edge sizes. Approximate maximum-likelihood inference in the DCHSBM naturally leads to a clustering objective that generalizes the popular modularity objective for graphs. We derive a general Louvain-type algorithm for this objective, as well as a a faster, specialized "All-Or-Nothing" (AON) variant in which edges are expected to lie fully within clusters. This special case encompasses a recent proposal for modularity in hypergraphs, while also incorporating flexible resolution and edge-size parameters. We show that AON hypergraph Louvain is highly scalable, including as an example an experiment on a synthetic hypergraph of one million nodes. We also demonstrate through synthetic experiments that the detectability regimes for hypergraph community detection differ from methods based on dyadic graph projections. We use our generative model to analyze different patterns of higher-order structure in school contact networks, U.S. congressional bill cosponsorship, U.S. congressional committees, product categories in co-purchasing behavior, and hotel locations from web browsing sessions, finding interpretable higher-order structure. We then study the behavior of our AON hypergraph Louvain algorithm, finding that it is able to recover ground truth clusters in empirical data sets exhibiting the corresponding higher-order structure.

</p>
</details>

<details><summary><b>Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art</b>
<a href="https://arxiv.org/abs/2101.09505">arxiv:2101.09505</a>
&#x1F4C8; 7 <br>
<p>Youngmin Kim, Richard Allmendinger, Manuel López-Ibáñez</p></summary>
<p>

**Abstract:** Safe learning and optimization deals with learning and optimization problems that avoid, as much as possible, the evaluation of non-safe input points, which are solutions, policies, or strategies that cause an irrecoverable loss (e.g., breakage of a machine or equipment, or life threat). Although a comprehensive survey of safe reinforcement learning algorithms was published in 2015, a number of new algorithms have been proposed thereafter, and related works in active learning and in optimization were not considered. This paper reviews those algorithms from a number of domains including reinforcement learning, Gaussian process regression and classification, evolutionary algorithms, and active learning. We provide the fundamental concepts on which the reviewed algorithms are based and a characterization of the individual algorithms. We conclude by explaining how the algorithms are connected and suggestions for future research.

</p>
</details>

<details><summary><b>MinConvNets: A new class of multiplication-less Neural Networks</b>
<a href="https://arxiv.org/abs/2101.09492">arxiv:2101.09492</a>
&#x1F4C8; 6 <br>
<p>Xuecan Yang, Sumanta Chaudhuri, Laurence Likforman, Lirida Naviner</p></summary>
<p>

**Abstract:** Convolutional Neural Networks have achieved unprecedented success in image classification, recognition, or detection applications. However, their large-scale deployment in embedded devices is still limited by the huge computational requirements, i.e., millions of MAC operations per layer. In this article, MinConvNets where the multiplications in the forward propagation are approximated by minimum comparator operations are introduced. Hardware implementation of minimum operation is much simpler than multipliers. Firstly, a methodology to find approximate operations based on statistical correlation is presented. We show that it is possible to replace multipliers by minimum operations in the forward propagation under certain constraints, i.e. given similar mean and variances of the feature and the weight vectors. A modified training method which guarantees the above constraints is proposed. And it is shown that equivalent precision can be achieved during inference with MinConvNets by using transfer learning from well trained exact CNNs.

</p>
</details>

<details><summary><b>Feature Selection Using Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2101.09460">arxiv:2101.09460</a>
&#x1F4C8; 6 <br>
<p>Sali Rasoul, Sodiq Adewole, Alphonse Akakpo</p></summary>
<p>

**Abstract:** With the decreasing cost of data collection, the space of variables or features that can be used to characterize a particular predictor of interest continues to grow exponentially. Therefore, identifying the most characterizing features that minimizes the variance without jeopardizing the bias of our models is critical to successfully training a machine learning model. In addition, identifying such features is critical for interpretability, prediction accuracy and optimal computation cost. While statistical methods such as subset selection, shrinkage, dimensionality reduction have been applied in selecting the best set of features, some other approaches in literature have approached feature selection task as a search problem where each state in the search space is a possible feature subset. In this paper, we solved the feature selection problem using Reinforcement Learning. Formulating the state space as a Markov Decision Process (MDP), we used Temporal Difference (TD) algorithm to select the best subset of features. Each state was evaluated using a robust and low cost classifier algorithm which could handle any non-linearities in the dataset.

</p>
</details>

<details><summary><b>ReliefE: Feature Ranking in High-dimensional Spaces via Manifold Embeddings</b>
<a href="https://arxiv.org/abs/2101.09577">arxiv:2101.09577</a>
&#x1F4C8; 5 <br>
<p>Blaž Škrlj, Sašo Džeroski, Nada Lavrač, Matej Petković</p></summary>
<p>

**Abstract:** Feature ranking has been widely adopted in machine learning applications such as high-throughput biology and social sciences. The approaches of the popular Relief family of algorithms assign importances to features by iteratively accounting for nearest relevant and irrelevant instances. Despite their high utility, these algorithms can be computationally expensive and not-well suited for high-dimensional sparse input spaces. In contrast, recent embedding-based methods learn compact, low-dimensional representations, potentially facilitating down-stream learning capabilities of conventional learners. This paper explores how the Relief branch of algorithms can be adapted to benefit from (Riemannian) manifold-based embeddings of instance and target spaces, where a given embedding's dimensionality is intrinsic to the dimensionality of the considered data set. The developed ReliefE algorithm is faster and can result in better feature rankings, as shown by our evaluation on 20 real-life data sets for multi-class and multi-label classification tasks. The utility of ReliefE for high-dimensional data sets is ensured by its implementation that utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE to other ranking algorithms is studied via the Fuzzy Jaccard Index.

</p>
</details>

<details><summary><b>Neural Relational Inference with Efficient Message Passing Mechanisms</b>
<a href="https://arxiv.org/abs/2101.09486">arxiv:2101.09486</a>
&#x1F4C8; 5 <br>
<p>Siyuan Chen, Jiahai Wang, Guoqing Li</p></summary>
<p>

**Abstract:** Many complex processes can be viewed as dynamical systems of interacting agents. In many cases, only the state sequences of individual agents are observed, while the interacting relations and the dynamical rules are unknown. The neural relational inference (NRI) model adopts graph neural networks that pass messages over a latent graph to jointly learn the relations and the dynamics based on the observed data. However, NRI infers the relations independently and suffers from error accumulation in multi-step prediction at dynamics learning procedure. Besides, relation reconstruction without prior knowledge becomes more difficult in more complex systems. This paper introduces efficient message passing mechanisms to the graph neural networks with structural prior knowledge to address these problems. A relation interaction mechanism is proposed to capture the coexistence of all relations, and a spatio-temporal message passing mechanism is proposed to use historical information to alleviate error accumulation. Additionally, the structural prior knowledge, symmetry as a special case, is introduced for better relation prediction in more complex systems. The experimental results on simulated physics systems show that the proposed method outperforms existing state-of-the-art methods.

</p>
</details>

<details><summary><b>Image Compression with Encoder-Decoder Matched Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2101.09642">arxiv:2101.09642</a>
&#x1F4C8; 4 <br>
<p>Trinh Man Hoang, Jinjia Zhou, Yibo Fan</p></summary>
<p>

**Abstract:** In recent years, layered image compression is demonstrated to be a promising direction, which encodes a compact representation of the input image and apply an up-sampling network to reconstruct the image. To further improve the quality of the reconstructed image, some works transmit the semantic segment together with the compressed image data. Consequently, the compression ratio is also decreased because extra bits are required for transmitting the semantic segment. To solve this problem, we propose a new layered image compression framework with encoder-decoder matched semantic segmentation (EDMS). And then, followed by the semantic segmentation, a special convolution neural network is used to enhance the inaccurate semantic segment. As a result, the accurate semantic segment can be obtained in the decoder without requiring extra bits. The experimental results show that the proposed EDMS framework can get up to 35.31% BD-rate reduction over the HEVC-based (BPG) codec, 5% bitrate, and 24% encoding time saving compare to the state-of-the-art semantic-based image codec.

</p>
</details>

<details><summary><b>DenseNet for Breast Tumor Classification in Mammographic Images</b>
<a href="https://arxiv.org/abs/2101.09637">arxiv:2101.09637</a>
&#x1F4C8; 4 <br>
<p>Yuliana Jiménez Gaona, María José Rodriguez-Alvarez, Hector Espinó Morató, Darwin Castillo Malla, Vasudevan Lakshminarayanan</p></summary>
<p>

**Abstract:** Breast cancer is the most common invasive cancer in women, and the second main cause of death. Breast cancer screening is an efficient method to detect indeterminate breast lesions early. The common approaches of screening for women are tomosynthesis and mammography images. However, the traditional manual diagnosis requires an intense workload by pathologists, who are prone to diagnostic errors. Thus, the aim of this study is to build a deep convolutional neural network method for automatic detection, segmentation, and classification of breast lesions in mammography images. Based on deep learning the Mask-CNN (RoIAlign) method was developed to features selection and extraction; and the classification was carried out by DenseNet architecture. Finally, the precision and accuracy of the model is evaluated by cross validation matrix and AUC curve. To summarize, the findings of this study may provide a helpful to improve the diagnosis and efficiency in the automatic tumor localization through the medical image classification.

</p>
</details>

<details><summary><b>S-BEV: Semantic Birds-Eye View Representation for Weather and Lighting Invariant 3-DoF Localization</b>
<a href="https://arxiv.org/abs/2101.09569">arxiv:2101.09569</a>
&#x1F4C8; 4 <br>
<p>Mokshith Voodarla, Shubham Shrivastava, Sagar Manglani, Ankit Vora, Siddharth Agarwal, Punarjay Chakravarty</p></summary>
<p>

**Abstract:** We describe a light-weight, weather and lighting invariant, Semantic Bird's Eye View (S-BEV) signature for vision-based vehicle re-localization. A topological map of S-BEV signatures is created during the first traversal of the route, which are used for coarse localization in subsequent route traversal. A fine-grained localizer is then trained to output the global 3-DoF pose of the vehicle using its S-BEV and its coarse localization. We conduct experiments on vKITTI2 virtual dataset and show the potential of the S-BEV to be robust to weather and lighting. We also demonstrate results with 2 vehicles on a 22 km long highway route in the Ford AV dataset.

</p>
</details>

<details><summary><b>Unsupervised clustering of series using dynamic programming</b>
<a href="https://arxiv.org/abs/2101.09512">arxiv:2101.09512</a>
&#x1F4C8; 4 <br>
<p>Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan, Vasileios-Marios Gkortsas, François Fleuret</p></summary>
<p>

**Abstract:** We are interested in clustering parts of a given single multi-variate series in an unsupervised manner. We would like to segment and cluster the series such that the resulting blocks present in each cluster are coherent with respect to a known model (e.g. physics model). Data points are said to be coherent if they can be described using this model with the same parameters. We have designed an algorithm based on dynamic programming with constraints on the number of clusters, the number of transitions as well as the minimal size of a block such that the clusters are coherent with this process. We present an use-case: clustering of petrophysical series using the Waxman-Smits equation.

</p>
</details>

<details><summary><b>Unlabeled Principal Component Analysis</b>
<a href="https://arxiv.org/abs/2101.09446">arxiv:2101.09446</a>
&#x1F4C8; 4 <br>
<p>Yunzhen Yao, Liangzu Peng, Manolis C. Tsakiris</p></summary>
<p>

**Abstract:** We consider the problem of principal component analysis from a data matrix where the entries of each column have undergone some unknown permutation, termed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry, we establish that for generic enough data, and up to a permutation of the coordinates of the ambient space, there is a unique subspace of minimal dimension that explains the data. We show that a permutation-invariant system of polynomial equations has finitely many solutions, with each solution corresponding to a row permutation of the ground-truth data matrix. Allowing for missing entries on top of permutations leads to the problem of unlabeled matrix completion, for which we give theoretical results of similar flavor. We also propose a two-stage algorithmic pipeline for UPCA suitable for the practically relevant case where only a fraction of the data has been permuted. Stage-I of this pipeline employs robust-PCA methods to estimate the ground-truth column-space. Equipped with the column-space, stage-II applies methods for linear regression without correspondences to restore the permuted data. A computational study reveals encouraging findings, including the ability of UPCA to handle face images from the Extended Yale-B database with arbitrarily permuted patches of arbitrary size in $0.3$ seconds on a standard desktop computer.

</p>
</details>

<details><summary><b>Towards Natural Language Question Answering over Earth Observation Linked Data using Attention-based Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2101.09427">arxiv:2101.09427</a>
&#x1F4C8; 4 <br>
<p>Abhishek V. Potnis, Rajat C. Shinde, Surya S. Durbha</p></summary>
<p>

**Abstract:** With an increase in Geospatial Linked Open Data being adopted and published over the web, there is a need to develop intuitive interfaces and systems for seamless and efficient exploratory analysis of such rich heterogeneous multi-modal datasets. This work is geared towards improving the exploration process of Earth Observation (EO) Linked Data by developing a natural language interface to facilitate querying. Questions asked over Earth Observation Linked Data have an inherent spatio-temporal dimension and can be represented using GeoSPARQL. This paper seeks to study and analyze the use of RNN-based neural machine translation with attention for transforming natural language questions into GeoSPARQL queries. Specifically, it aims to assess the feasibility of a neural approach for identifying and mapping spatial predicates in natural language to GeoSPARQL's topology vocabulary extension including - Egenhofer and RCC8 relations. The queries can then be executed over a triple store to yield answers for the natural language questions. A dataset consisting of mappings from natural language questions to GeoSPARQL queries over the Corine Land Cover(CLC) Linked Data has been created to train and validate the deep neural network. From our experiments, it is evident that neural machine translation with attention is a promising approach for the task of translating spatial predicates in natural language questions to GeoSPARQL queries.

</p>
</details>

<details><summary><b>Analyzing Team Performance with Embeddings from Multiparty Dialogues</b>
<a href="https://arxiv.org/abs/2101.09421">arxiv:2101.09421</a>
&#x1F4C8; 4 <br>
<p>Ayesha Enayet, Gita Sukthankar</p></summary>
<p>

**Abstract:** Good communication is indubitably the foundation of effective teamwork. Over time teams develop their own communication styles and often exhibit entrainment, a conversational phenomena in which humans synchronize their linguistic choices. This paper examines the problem of predicting team performance from embeddings learned from multiparty dialogues such that teams with similar conflict scores lie close to one another in vector space. Embeddings were extracted from three types of features: 1) dialogue acts 2) sentiment polarity 3) syntactic entrainment. Although all of these features can be used to effectively predict team performance, their utility varies by the teamwork phase. We separate the dialogues of players playing a cooperative game into stages: 1) early (knowledge building) 2) middle (problem-solving) and 3) late (culmination). Unlike syntactic entrainment, both dialogue act and sentiment embeddings are effective for classifying team performance, even during the initial phase. This finding has potential ramifications for the development of conversational agents that facilitate teaming.

</p>
</details>

<details><summary><b>Indoor Group Activity Recognition using Multi-Layered HMMs</b>
<a href="https://arxiv.org/abs/2101.10857">arxiv:2101.10857</a>
&#x1F4C8; 3 <br>
<p>Vinayak Elangovan</p></summary>
<p>

**Abstract:** Discovery and recognition of Group Activities (GA) based on imagery data processing have significant applications in persistent surveillance systems, which play an important role in some Internet services. The process is involved with analysis of sequential imagery data with spatiotemporal associations. Discretion of video imagery requires a proper inference system capable of discriminating and differentiating cohesive observations and interlinking them to known ontologies. We propose an Ontology based GAR with a proper inference model that is capable of identifying and classifying a sequence of events in group activities. A multi-layered Hidden Markov Model (HMM) is proposed to recognize different levels of abstract GA. The multi-layered HMM consists of N layers of HMMs where each layer comprises of M number of HMMs running in parallel. The number of layers depends on the order of information to be extracted. At each layer, by matching and correlating attributes of detected group events, the model attempts to associate sensory observations to known ontology perceptions. This paper demonstrates and compares performance of three different implementation of HMM, namely, concatenated N-HMM, cascaded C-HMM and hybrid H-HMM for building effective multi-layered HMM.

</p>
</details>

<details><summary><b>On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths</b>
<a href="https://arxiv.org/abs/2101.09612">arxiv:2101.09612</a>
&#x1F4C8; 3 <br>
<p>Quynh Nguyen</p></summary>
<p>

**Abstract:** We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $Ω(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.

</p>
</details>

<details><summary><b>Real-Time, Flight-Ready, Non-Cooperative Spacecraft Pose Estimation Using Monocular Imagery</b>
<a href="https://arxiv.org/abs/2101.09553">arxiv:2101.09553</a>
&#x1F4C8; 3 <br>
<p>Kevin Black, Shrivu Shankar, Daniel Fonseka, Jacob Deutsch, Abhimanyu Dhir, Maruthi R. Akella</p></summary>
<p>

**Abstract:** A key requirement for autonomous on-orbit proximity operations is the estimation of a target spacecraft's relative pose (position and orientation). It is desirable to employ monocular cameras for this problem due to their low cost, weight, and power requirements. This work presents a novel convolutional neural network (CNN)-based monocular pose estimation system that achieves state-of-the-art accuracy with low computational demand. In combination with a Blender-based synthetic data generation scheme, the system demonstrates the ability to generalize from purely synthetic training data to real in-space imagery of the Northrop Grumman Enhanced Cygnus spacecraft. Additionally, the system achieves real-time performance on low-power flight-like hardware.

</p>
</details>

<details><summary><b>Stochastic Image Denoising by Sampling from the Posterior Distribution</b>
<a href="https://arxiv.org/abs/2101.09552">arxiv:2101.09552</a>
&#x1F4C8; 3 <br>
<p>Bahjat Kawar, Gregory Vaksman, Michael Elad</p></summary>
<p>

**Abstract:** Image denoising is a well-known and well studied problem, commonly targeting a minimization of the mean squared error (MSE) between the outcome and the original image. Unfortunately, especially for severe noise levels, such Minimum MSE (MMSE) solutions may lead to blurry output images. In this work we propose a novel stochastic denoising approach that produces viable and high perceptual quality results, while maintaining a small MSE. Our method employs Langevin dynamics that relies on a repeated application of any given MMSE denoiser, obtaining the reconstructed image by effectively sampling from the posterior distribution. Due to its stochasticity, the proposed algorithm can produce a variety of high-quality outputs for a given noisy input, all shown to be legitimate denoising results. In addition, we present an extension of our algorithm for handling the inpainting problem, recovering missing pixels while removing noise from partially given data.

</p>
</details>

<details><summary><b>Acceleration Methods</b>
<a href="https://arxiv.org/abs/2101.09545">arxiv:2101.09545</a>
&#x1F4C8; 3 <br>
<p>Alexandre d'Aspremont, Damien Scieur, Adrien Taylor</p></summary>
<p>

**Abstract:** This monograph covers some recent advances in a range of acceleration techniques frequently used in convex optimization. We first use quadratic optimization problems to introduce two key families of methods, namely momentum and nested optimization schemes. They coincide in the quadratic case to form the Chebyshev method. We discuss momentum methods in detail, starting with the seminal work of Nesterov and structure convergence proofs using a few master templates, such as that for optimized gradient methods, which provide the key benefit of showing how momentum methods optimize convergence guarantees. We further cover proximal acceleration, at the heart of the Catalyst and Accelerated Hybrid Proximal Extragradient frameworks, using similar algorithmic patterns. Common acceleration techniques rely directly on the knowledge of some of the regularity parameters in the problem at hand. We conclude by discussing restart schemes, a set of simple techniques for reaching nearly optimal convergence rates while adapting to unobserved regularity parameters.

</p>
</details>

<details><summary><b>Show or Suppress? Managing Input Uncertainty in Machine Learning Model Explanations</b>
<a href="https://arxiv.org/abs/2101.09498">arxiv:2101.09498</a>
&#x1F4C8; 3 <br>
<p>Danding Wang, Wencan Zhang, Brian Y. Lim</p></summary>
<p>

**Abstract:** Feature attribution is widely used in interpretable machine learning to explain how influential each measured input feature value is for an output inference. However, measurements can be uncertain, and it is unclear how the awareness of input uncertainty can affect the trust in explanations. We propose and study two approaches to help users to manage their perception of uncertainty in a model explanation: 1) transparently show uncertainty in feature attributions to allow users to reflect on, and 2) suppress attribution to features with uncertain measurements and shift attribution to other features by regularizing with an uncertainty penalty. Through simulation experiments, qualitative interviews, and quantitative user evaluations, we identified the benefits of moderately suppressing attribution uncertainty, and concerns regarding showing attribution uncertainty. This work adds to the understanding of handling and communicating uncertainty for model interpretability.

</p>
</details>

<details><summary><b>Error Diffusion Halftoning Against Adversarial Examples</b>
<a href="https://arxiv.org/abs/2101.09451">arxiv:2101.09451</a>
&#x1F4C8; 3 <br>
<p>Shao-Yuan Lo, Vishal M. Patel</p></summary>
<p>

**Abstract:** Adversarial examples contain carefully crafted perturbations that can fool deep neural networks (DNNs) into making wrong predictions. Enhancing the adversarial robustness of DNNs has gained considerable interest in recent years. Although image transformation-based defenses were widely considered at an earlier time, most of them have been defeated by adaptive attacks. In this paper, we propose a new image transformation defense based on error diffusion halftoning, and combine it with adversarial training to defend against adversarial examples. Error diffusion halftoning projects an image into a 1-bit space and diffuses quantization error to neighboring pixels. This process can remove adversarial perturbations from a given image while maintaining acceptable image quality in the meantime in favor of recognition. Experimental results demonstrate that the proposed method is able to improve adversarial robustness even under advanced adaptive attacks, while most of the other image transformation-based defenses do not. We show that a proper image transformation can still be an effective defense approach. Code: https://github.com/shaoyuanlo/Halftoning-Defense

</p>
</details>

<details><summary><b>B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows</b>
<a href="https://arxiv.org/abs/2101.10870">arxiv:2101.10870</a>
&#x1F4C8; 2 <br>
<p>Florenc Demrozi, Cristian Turetta, Graziano Pravadelli</p></summary>
<p>

**Abstract:** Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, standardization, and development of a baseline framework in order to evaluate and compare HAR methodologies. It implements the most popular data processing methods for data preparation and the most commonly used machine and deep learning pattern recognition models.

</p>
</details>

<details><summary><b>Multi-Task Time Series Forecasting With Shared Attention</b>
<a href="https://arxiv.org/abs/2101.09645">arxiv:2101.09645</a>
&#x1F4C8; 2 <br>
<p>Zekai Chen, Jiaze E, Xiao Zhang, Hao Sheng, Xiuzheng Cheng</p></summary>
<p>

**Abstract:** Time series forecasting is a key component in many industrial and business decision processes and recurrent neural network (RNN) based models have achieved impressive progress on various time series forecasting tasks. However, most of the existing methods focus on single-task forecasting problems by learning separately based on limited supervised objectives, which often suffer from insufficient training instances. As the Transformer architecture and other attention-based models have demonstrated its great capability of capturing long term dependency, we propose two self-attention based sharing schemes for multi-task time series forecasting which can train jointly across multiple tasks. We augment a sequence of paralleled Transformer encoders with an external public multi-head attention function, which is updated by all data of all tasks. Experiments on a number of real-world multi-task time series forecasting tasks show that our proposed architectures can not only outperform the state-of-the-art single-task forecasting baselines but also outperform the RNN-based multi-task forecasting method.

</p>
</details>

<details><summary><b>Disentangled Sequence Clustering for Human Intention Inference</b>
<a href="https://arxiv.org/abs/2101.09500">arxiv:2101.09500</a>
&#x1F4C8; 2 <br>
<p>Mark Zolotas, Yiannis Demiris</p></summary>
<p>

**Abstract:** Equipping robots with the ability to infer human intent is a vital precondition for effective collaboration. Most computational approaches towards this objective employ probabilistic reasoning to recover a distribution of "intent" conditioned on the robot's perceived sensory state. However, these approaches typically assume task-specific notions of human intent (e.g. labelled goals) are known a priori. To overcome this constraint, we propose the Disentangled Sequence Clustering Variational Autoencoder (DiSCVAE), a clustering framework that can be used to learn such a distribution of intent in an unsupervised manner. The DiSCVAE leverages recent advances in unsupervised learning to derive a disentangled latent representation of sequential data, separating time-varying local features from time-invariant global aspects. Though unlike previous frameworks for disentanglement, the proposed variant also infers a discrete variable to form a latent mixture model and enable clustering of global sequence concepts, e.g. intentions from observed human behaviour. To evaluate the DiSCVAE, we first validate its capacity to discover classes from unlabelled sequences using video datasets of bouncing digits and 2D animations. We then report results from a real-world human-robot interaction experiment conducted on a robotic wheelchair. Our findings glean insights into how the inferred discrete variable coincides with human intent and thus serves to improve assistance in collaborative settings, such as shared control.

</p>
</details>

<details><summary><b>Symbiotic System of Systems Design for Safe and Resilient Autonomous Robotics in Offshore Wind Farms</b>
<a href="https://arxiv.org/abs/2101.09491">arxiv:2101.09491</a>
&#x1F4C8; 2 <br>
<p>Daniel Mitchell, Jamie Blanche, Osama Zaki, Joshua Roe, Leo Kong, Samuel Harper, Valentin Robu, Theodore Lim, David Flynn</p></summary>
<p>

**Abstract:** To reduce Operation and Maintenance (O&M) costs on offshore wind farms, wherein 80% of the O&M cost relates to deploying personnel, the offshore wind sector looks to Robotics and Artificial Intelligence (RAI) for solutions. Barriers to Beyond Visual Line of Sight (BVLOS) robotics include operational safety compliance and resilience, inhibiting the commercialization of autonomous services offshore. To address safety and resilience challenges we propose a Symbiotic System Of Systems Approach (SSOSA), reflecting the lifecycle learning and co-evolution with knowledge sharing for mutual gain of robotic platforms and remote human operators. Our novel methodology enables the run-time verification of safety, reliability and resilience during autonomous missions. To achieve this, a Symbiotic Digital Architecture (SDA) was developed to synchronize digital models of the robot, environment, infrastructure, and integrate front-end analytics and bidirectional communication for autonomous adaptive mission planning and situation reporting to a remote operator. A reliability ontology for the deployed robot, based on our holistic hierarchical-relational model, supports computationally efficient platform data analysis. We demonstrate an asset inspection mission within a confined space through Cooperative, Collaborative and Corroborative (C3) governance (internal and external symbiosis) via decision-making processes and the associated structures. We create a hyper enabled human interaction capability to analyze the mission status, diagnostics of critical sub-systems within the robot to provide automatic updates to our AI-driven run-time reliability ontology. This enables faults to be translated into failure modes for decision-making during the mission.

</p>
</details>

<details><summary><b>An Optimal Reduction of TV-Denoising to Adaptive Online Learning</b>
<a href="https://arxiv.org/abs/2101.09438">arxiv:2101.09438</a>
&#x1F4C8; 2 <br>
<p>Dheeraj Baby, Xuandong Zhao, Yu-Xiang Wang</p></summary>
<p>

**Abstract:** We consider the problem of estimating a function from $n$ noisy samples whose discrete Total Variation (TV) is bounded by $C_n$. We reveal a deep connection to the seemingly disparate problem of Strongly Adaptive online learning (Daniely et al, 2015) and provide an $O(n \log n)$ time algorithm that attains the near minimax optimal rate of $\tilde O (n^{1/3}C_n^{2/3})$ under squared error loss. The resulting algorithm runs online and optimally adapts to the unknown smoothness parameter $C_n$. This leads to a new and more versatile alternative to wavelets-based methods for (1) adaptively estimating TV bounded functions; (2) online forecasting of TV bounded trends in time series.

</p>
</details>

<details><summary><b>Autoregressive Belief Propagation for Decoding Block Codes</b>
<a href="https://arxiv.org/abs/2103.11780">arxiv:2103.11780</a>
&#x1F4C8; 1 <br>
<p>Eliya Nachmani, Lior Wolf</p></summary>
<p>

**Abstract:** We revisit recent methods that employ graph neural networks for decoding error correcting codes and employ messages that are computed in an autoregressive manner. The outgoing messages of the variable nodes are conditioned not only on the incoming messages, but also on an estimation of the SNR and on the inferred codeword and on two downstream computations: (i) an extended vector of parity check outcomes, (ii) the mismatch between the inferred codeword and the re-encoding of the information bits of this codeword. Unlike most learned methods in the field, our method violates the symmetry conditions that enable the other methods to train exclusively with the zero-word. Despite not having the luxury of training on a single word, and the inability to train on more than a small fraction of the relevant sample space, we demonstrate effective training. The new method obtains a bit error rate that outperforms the latest methods by a sizable margin.

</p>
</details>

<details><summary><b>Episodic memory governs choices: An RNN-based reinforcement learning model for decision-making task</b>
<a href="https://arxiv.org/abs/2103.03679">arxiv:2103.03679</a>
&#x1F4C8; 1 <br>
<p>Xiaohan Zhang, Lu Liu, Guodong Long, Jing Jiang, Shenquan Liu</p></summary>
<p>

**Abstract:** Typical methods to study cognitive function are to record the electrical activities of animal neurons during the training of animals performing behavioral tasks. A key problem is that they fail to record all the relevant neurons in the animal brain. To alleviate this problem, we develop an RNN-based Actor-Critic framework, which is trained through reinforcement learning (RL) to solve two tasks analogous to the monkeys' decision-making tasks. The trained model is capable of reproducing some features of neural activities recorded from animal brain, or some behavior properties exhibited in animal experiments, suggesting that it can serve as a computational platform to explore other cognitive functions. Furthermore, we conduct behavioral experiments on our framework, trying to explore an open question in neuroscience: which episodic memory in the hippocampus should be selected to ultimately govern future decisions. We find that the retrieval of salient events sampled from episodic memories can effectively shorten deliberation time than common events in the decision-making process. The results indicate that salient events stored in the hippocampus could be prioritized to propagate reward information, and thus allow decision-makers to learn a strategy faster.

</p>
</details>

<details><summary><b>Multi-intersection Traffic Optimisation: A Benchmark Dataset and a Strong Baseline</b>
<a href="https://arxiv.org/abs/2101.09640">arxiv:2101.09640</a>
&#x1F4C8; 1 <br>
<p>Hu Wang, Hao Chen, Qi Wu, Congbo Ma, Yidong Li, Chunhua Shen</p></summary>
<p>

**Abstract:** The control of traffic signals is fundamental and critical to alleviate traffic congestion in urban areas. However, it is challenging since traffic dynamics are complicated in real-world scenarios. Because of the high complexity of the optimisation problem for modelling the traffic, experimental settings of existing works are often inconsistent. Moreover, it is not trivial to control multiple intersections properly in real complex traffic scenarios due to its vast state and action space. Failing to take intersection topology relations into account also results in inferior solutions. To address these issues, in this work we carefully design our settings and propose a new dataset including both synthetic and real traffic data in more complex scenarios. Additionally, we propose a novel baseline model with strong performance. It is based on deep reinforcement learning with an encoder-decoder structure: an edge-weighted graph convolutional encoder to excavate multi-intersection relations; and an unified structure decoder to jointly model multiple junctions in a comprehensive manner, which significantly reduces the number of the model parameters. By doing so, the proposed model is able to effectively deal with the multi-intersection traffic optimisation problem. Models are trained/tested on both synthetic and real maps and traffic data with the Simulation of Urban Mobility (SUMO) simulator. Experimental results show that the proposed model surpasses multiple competitive methods.

</p>
</details>

<details><summary><b>A Pressure Ulcer Care System For Remote Medical Assistance: Residual U-Net with an Attention Model Based for Wound Area Segmentation</b>
<a href="https://arxiv.org/abs/2101.09433">arxiv:2101.09433</a>
&#x1F4C8; 1 <br>
<p>Jinyeong Chae, Ki Yong Hong, Jihie Kim</p></summary>
<p>

**Abstract:** Increasing numbers of patients with disabilities or elderly people with mobility issues often suffer from a pressure ulcer. The affected areas need regular checks, but they have a difficulty in accessing a hospital. Some remote diagnosis systems are being used for them, but there are limitations in checking a patient's status regularly. In this paper, we present a remote medical assistant that can help pressure ulcer management with image processing techniques. The proposed system includes a mobile application with a deep learning model for wound segmentation and analysis. As there are not enough data to train the deep learning model, we make use of a pretrained model from a relevant domain and data augmentation that is appropriate for this task. First of all, an image preprocessing method using bilinear interpolation is used to resize images and normalize the images. Second, for data augmentation, we use rotation, reflection, and a watershed algorithm. Third, we use a pretrained deep learning model generated from skin wound images similar to pressure ulcer images. Finally, we added an attention module that can provide hints on the pressure ulcer image features. The resulting model provides an accuracy of 99.0%, an intersection over union (IoU) of 99.99%, and a dice similarity coefficient (DSC) of 93.4% for pressure ulcer segmentation, which is better than existing results.

</p>
</details>

<details><summary><b>A Machine Learning Approach to Predicting Continuous Tie Strengths</b>
<a href="https://arxiv.org/abs/2101.09417">arxiv:2101.09417</a>
&#x1F4C8; 1 <br>
<p>James Flamino, Ross DeVito, Boleslaw K. Szymanski, Omar Lizardo</p></summary>
<p>

**Abstract:** Relationships between people constantly evolve, altering interpersonal behavior and defining social groups. Relationships between nodes in social networks can be represented by a tie strength, often empirically assessed using surveys. While this is effective for taking static snapshots of relationships, such methods are difficult to scale to dynamic networks. In this paper, we propose a system that allows for the continuous approximation of relationships as they evolve over time. We evaluate this system using the NetSense study, which provides comprehensive communication records of students at the University of Notre Dame over the course of four years. These records are complemented by semesterly ego network surveys, which provide discrete samples over time of each participant's true social tie strength with others. We develop a pair of powerful machine learning models (complemented by a suite of baselines extracted from past works) that learn from these surveys to interpret the communications records as signals. These signals represent dynamic tie strengths, accurately recording the evolution of relationships between the individuals in our social networks. With these evolving tie values, we are able to make several empirically derived observations which we compare to past works.

</p>
</details>

<details><summary><b>Are Top School Students More Critical of Their Professors? Mining Comments on RateMyProfessor.com</b>
<a href="https://arxiv.org/abs/2101.12339">arxiv:2101.12339</a>
&#x1F4C8; 0 <br>
<p>Ziqi Tang, Yutong Wang, Jiebo Luo</p></summary>
<p>

**Abstract:** Student reviews and comments on RateMyProfessor.com reflect realistic learning experiences of students. Such information provides a large-scale data source to examine the teaching quality of the lecturers. In this paper, we propose an in-depth analysis of these comments. First, we partition our data into different comparison groups. Next, we perform exploratory data analysis to delve into the data. Furthermore, we employ Latent Dirichlet Allocation and sentiment analysis to extract topics and understand the sentiments associated with the comments. We uncover interesting insights about the characteristics of both college students and professors. Our study proves that student reviews and comments contain crucial information and can serve as essential references for enrollment in courses and universities.

</p>
</details>

<details><summary><b>Predicting the Mechanical Properties of Biopolymer Gels Using Neural Networks Trained on Discrete Fiber Network Data</b>
<a href="https://arxiv.org/abs/2101.11712">arxiv:2101.11712</a>
&#x1F4C8; 0 <br>
<p>Yue Leng, Vahidullah Tac, Sarah Calve, Adrian Buganza Tepole</p></summary>
<p>

**Abstract:** Biopolymer gels, such as those made out of fibrin or collagen, are widely used in tissue engineering applications and biomedical research. Moreover, fibrin naturally assembles into gels in vivo during wound healing and thrombus formation. Macroscale biopolymer gel mechanics are dictated by the microscale fiber network. Hence, accurate description of biopolymer gels can be achieved using representative volume elements (RVE) that explicitly model the discrete fiber networks of the microscale. These RVE models, however, cannot be efficiently used to model the macroscale due to the challenges and computational demands of multiscale coupling. Here, we propose the use of an artificial, fully connected neural network (FCNN) to efficiently capture the behavior of the RVE models. The FCNN was trained on 1100 fiber networks subjected to 121 biaxial deformations. The stress data from the RVE, together with the total energy and the condition of incompressibility of the surrounding matrix, were used to determine the derivatives of an unknown strain energy function with respect to the deformation invariants. During training, the loss function was modified to ensure convexity of the strain energy function and symmetry of its Hessian. A general FCNN model was coded into a user material subroutine (UMAT) in the software Abaqus. In this work, the FCNN trained on the discrete fiber network data was used in finite element simulations of fibrin gels using our UMAT. We anticipate that this work will enable further integration of machine learning tools with computational mechanics. It will also improve computational modeling of biological materials characterized by a multiscale structure.

</p>
</details>

<details><summary><b>ARTH: Algorithm For Reading Text Handily -- An AI Aid for People having Word Processing Issues</b>
<a href="https://arxiv.org/abs/2101.09464">arxiv:2101.09464</a>
&#x1F4C8; 0 <br>
<p>Akanksha Malhotra, Sudhir Kamle</p></summary>
<p>

**Abstract:** The objective of this project is to solve one of the major problems faced by the people having word processing issues like trauma, or mild mental disability. "ARTH" is the short form of Algorithm for Reading Handily. ARTH is a self-learning set of algorithms that is an intelligent way of fulfilling the need for "reading and understanding the text effortlessly" which adjusts according to the needs of every user. The research project propagates in two steps. In the first step, the algorithm tries to identify the difficult words present in the text based on two features -- the number of syllables and usage frequency -- using a clustering algorithm. After the analysis of the clusters, the algorithm labels these clusters, according to their difficulty level. In the second step, the algorithm interacts with the user. It aims to test the user's comprehensibility of the text and his/her vocabulary level by taking an automatically generated quiz. The algorithm identifies the clusters which are difficult for the user, based on the result of the analysis. The meaning of perceived difficult words is displayed next to them. The technology "ARTH" focuses on the revival of the joy of reading among those people, who have a poor vocabulary or any word processing issues.

</p>
</details>

<details><summary><b>Hierarchical Variational Auto-Encoding for Unsupervised Domain Generalization</b>
<a href="https://arxiv.org/abs/2101.09436">arxiv:2101.09436</a>
&#x1F4C8; 0 <br>
<p>Xudong Sun, Florian Buettner</p></summary>
<p>

**Abstract:** We address the task of domain generalization, where the goal is to train a predictive model such that it is able to generalize to a new, previously unseen domain. We choose a hierarchical generative approach within the framework of variational autoencoders and propose a domain-unsupervised algorithm that is able to generalize to new domains without domain supervision. We show that our method is able to learn representations that disentangle domain-specific information from class-label specific information even in complex settings where domain structure is not observed during training. Our interpretable method outperforms previously proposed generative algorithms for domain generalization as well as other non-generative state-of-the-art approaches in several hierarchical domain settings including sequential overlapped near continuous domain shift. It also achieves competitive performance on the standard domain generalization benchmark dataset PACS compared to state-of-the-art approaches which rely on observing domain-specific information during training, as well as another domain unsupervised method. Additionally, we proposed model selection purely based on Evidence Lower Bound (ELBO) and also proposed weak domain supervision where implicit domain information can be added into the algorithm.

</p>
</details>

<details><summary><b>Deep Anti-aliasing of Whole Focal Stack Using Slice Spectrum</b>
<a href="https://arxiv.org/abs/2101.09420">arxiv:2101.09420</a>
&#x1F4C8; 0 <br>
<p>Yaning Li, Xue Wang, Hao Zhu, Guoqing Zhou, Qing Wang</p></summary>
<p>

**Abstract:** The paper aims at removing the aliasing effects of the whole focal stack generated from a sparse-sampled {4D} light field, while keeping the consistency across all the focal layers. We first explore the structural characteristics embedded in the focal stack slice and its corresponding frequency-domain representation, i.e., the Focal Stack Spectrum (FSS). We observe that the energy distribution of the FSS always resides within the same triangular area under different angular sampling rates, additionally the continuity of the Point Spread Function (PSF) is intrinsically maintained in the FSS. Based on these two observations, we propose a learning-based FSS reconstruction approach for one-time aliasing removing over the whole focal stack. Moreover, a novel conjugate-symmetric loss function is proposed for the optimization. Compared to previous works, our method avoids an explicit depth estimation, and can handle challenging large-disparity scenarios. Experimental results on both synthetic and real light field datasets show the superiority of the proposed approach for different scenes and various angular sampling rates.

</p>
</details>


{% endraw %}
Prev: [2021.01.22]({{ '/2021/01/22/2021.01.22.html' | relative_url }})  Next: [2021.01.24]({{ '/2021/01/24/2021.01.24.html' | relative_url }})