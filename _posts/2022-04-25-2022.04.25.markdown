Prev: [2022.04.24]({{ '/2022/04/24/2022.04.24.html' | relative_url }})  Next: [2022.04.26]({{ '/2022/04/26/2022.04.26.html' | relative_url }})
{% raw %}
## Summary for 2022-04-25, created on 2022-04-29


<details><summary><b>StyleGAN-Human: A Data-Centric Odyssey of Human Generation</b>
<a href="https://arxiv.org/abs/2204.11823">arxiv:2204.11823</a>
&#x1F4C8; 6930 <br>
<p>Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy, Wayne Wu, Ziwei Liu</p></summary>
<p>

**Abstract:** Unconditional human image generation is an important task in vision and graphics, which enables various applications in the creative industry. Existing studies in this field mainly focus on "network engineering" such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in "data engineering", which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are needed to train a high-fidelity unconditional human generation model with vanilla StyleGAN. 2) A balanced training set helps improve the generation quality with rare face poses compared to the long-tailed counterpart, whereas simply balancing the clothing texture distribution does not effectively bring an improvement. 3) Human GAN models with body centers for alignment outperform models trained using face centers or pelvis points as alignment anchors. In addition, a model zoo and human editing applications are demonstrated to facilitate future research in the community.

</p>
</details>

<details><summary><b>Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation</b>
<a href="https://arxiv.org/abs/2204.11788">arxiv:2204.11788</a>
&#x1F4C8; 44 <br>
<p>Vivian Lai, Samuel Carton, Rajat Bhatnagar, Q. Vera Liao, Yunfeng Zhang, Chenhao Tan</p></summary>
<p>

**Abstract:** Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, e.g., moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model. Using content moderation as a testbed, we develop novel interfaces to assist humans in creating conditional delegation rules and conduct a randomized experiment with two datasets to simulate in-distribution and out-of-distribution scenarios. Our study demonstrates the promise of conditional delegation in improving model performance and provides insights into design for this novel paradigm, including the effect of AI explanations.

</p>
</details>

<details><summary><b>GDGRU-DTA: Predicting Drug-Target Binding Affinity Based on GNN and Double GRU</b>
<a href="https://arxiv.org/abs/2204.11857">arxiv:2204.11857</a>
&#x1F4C8; 29 <br>
<p>Lyu Zhijian, Jiang Shaohua, Liang Yigao, Gao Min</p></summary>
<p>

**Abstract:** The work for predicting drug and target affinity(DTA) is crucial for drug development and repurposing. In this work, we propose a novel method called GDGRU-DTA to predict the binding affinity between drugs and targets, which is based on GraphDTA, but we consider that protein sequences are long sequences, so simple CNN cannot capture the context dependencies in protein sequences well. Therefore, we improve it by interpreting the protein sequences as time series and extracting their features using Gate Recurrent Unit(GRU) and Bidirectional Gate Recurrent Unit(BiGRU). For the drug, our processing method is similar to that of GraphDTA, but uses two different graph convolution methods. Subsequently, the representation of drugs and proteins are concatenated for final prediction. We evaluate the proposed model on two benchmark datasets. Our model outperforms some state-of-the-art deep learning methods, and the results demonstrate the feasibility and excellent feature capture ability of our model.

</p>
</details>

<details><summary><b>HyperNCA: Growing Developmental Networks with Neural Cellular Automata</b>
<a href="https://arxiv.org/abs/2204.11674">arxiv:2204.11674</a>
&#x1F4C8; 20 <br>
<p>Elias Najarro, Shyam Sudhakaran, Claire Glanois, Sebastian Risi</p></summary>
<p>

**Abstract:** In contrast to deep reinforcement learning agents, biological neural networks are grown through a self-organized developmental process. Here we propose a new hypernetwork approach to grow artificial neural networks based on neural cellular automata (NCA). Inspired by self-organising systems and information-theoretic approaches to developmental biology, we show that our HyperNCA method can grow neural networks capable of solving common reinforcement learning tasks. Finally, we explore how the same approach can be used to build developmental metamorphosis networks capable of transforming their weights to solve variations of the initial RL task.

</p>
</details>

<details><summary><b>Towards Evaluating Adaptivity of Model-Based Reinforcement Learning Methods</b>
<a href="https://arxiv.org/abs/2204.11464">arxiv:2204.11464</a>
&#x1F4C8; 18 <br>
<p>Yi Wan, Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Sarath Chandar, Harm van Seijen</p></summary>
<p>

**Abstract:** In recent years, a growing number of deep model-based reinforcement learning (RL) methods have been introduced. The interest in deep model-based RL is not surprising, given its many potential benefits, such as higher sample efficiency and the potential for fast adaption to changes in the environment. However, we demonstrate, using an improved version of the recently introduced Local Change Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and DreamerV2 perform poorly in their ability to adapt to local environmental changes. Combined with prior work that made a similar observation about the other popular model-based method, MuZero, a trend appears to emerge, suggesting that current deep model-based methods have serious limitations. We dive deeper into the causes of this poor performance, by identifying elements that hurt adaptive behavior and linking these to underlying techniques frequently used in deep model-based RL. We empirically validate these insights in the case of linear function approximation by demonstrating that a modified version of linear Dyna achieves effective adaptation to local changes. Furthermore, we provide detailed insights into the challenges of building an adaptive nonlinear model-based method, by experimenting with a nonlinear version of Dyna.

</p>
</details>

<details><summary><b>Skill-based Meta-Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2204.11828">arxiv:2204.11828</a>
&#x1F4C8; 14 <br>
<p>Taewook Nam, Shao-Hua Sun, Karl Pertsch, Sung Ju Hwang, Joseph J Lim</p></summary>
<p>

**Abstract:** While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks.

</p>
</details>

<details><summary><b>Task-Induced Representation Learning</b>
<a href="https://arxiv.org/abs/2204.11827">arxiv:2204.11827</a>
&#x1F4C8; 14 <br>
<p>Jun Yamada, Karl Pertsch, Anisha Gunjal, Joseph J. Lim</p></summary>
<p>

**Abstract:** In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional inputs. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown substantial learning efficiency gains. Yet, they have mostly been evaluated in clean laboratory or simulated settings. In contrast, real environments are visually complex and contain substantial amounts of clutter and distractors. Unsupervised representations will learn to model such distractors, potentially impairing the agent's learning efficiency. In contrast, an alternative class of approaches, which we call task-induced representation learning, leverages task information such as rewards or demonstrations from prior tasks to focus on task-relevant parts of the scene and ignore distractors. We investigate the effectiveness of unsupervised and task-induced representation learning approaches on four visually complex environments, from Distracting DMControl to the CARLA driving simulator. For both, RL and imitation learning, we find that representation learning generally improves sample efficiency on unseen tasks even in visually complex scenes and that task-induced representations can double learning efficiency compared to unsupervised alternatives. Code is available at https://clvrai.com/tarp.

</p>
</details>

<details><summary><b>Translation between Molecules and Natural Language</b>
<a href="https://arxiv.org/abs/2204.11817">arxiv:2204.11817</a>
&#x1F4C8; 11 <br>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Heng Ji</p></summary>
<p>

**Abstract:** Joint representations between images and text have been deeply investigated in the literature. In computer vision, the benefits of incorporating natural language have become clear for enabling semantic-level control of images. In this work, we present $\textbf{MolT5}-$a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. $\textbf{MolT5}$ allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Furthermore, since $\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Additionally, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. By interfacing molecules with natural language, we enable a higher semantic level of control over molecule discovery and understanding--a critical task for scientific domains such as drug discovery and material design. Our results show that $\textbf{MolT5}$-based models are able to generate outputs, both molecule and text, which in many cases are high quality and match the input modality. On molecule generation, our best model achieves 30% exact matching test accuracy (i.e., it generates the correct structure for about one-third of the captions in our held-out test set).

</p>
</details>

<details><summary><b>Goal-driven Self-Attentive Recurrent Networks for Trajectory Prediction</b>
<a href="https://arxiv.org/abs/2204.11561">arxiv:2204.11561</a>
&#x1F4C8; 10 <br>
<p>Luigi Filippo Chiara, Pasquale Coscia, Sourav Das, Simone Calderara, Rita Cucchiara, Lamberto Ballan</p></summary>
<p>

**Abstract:** Human trajectory forecasting is a key component of autonomous vehicles, social-aware robots and advanced video-surveillance applications. This challenging task typically requires knowledge about past motion, the environment and likely destination areas. In this context, multi-modality is a fundamental aspect and its effective modeling can be beneficial to any architecture. Inferring accurate trajectories is nevertheless challenging, due to the inherently uncertain nature of the future. To overcome these difficulties, recent models use different inputs and propose to model human intentions using complex fusion mechanisms. In this respect, we propose a lightweight attention-based recurrent backbone that acts solely on past observed positions. Although this backbone already provides promising results, we demonstrate that its prediction accuracy can be improved considerably when combined with a scene-aware goal-estimation module. To this end, we employ a common goal module, based on a U-Net architecture, which additionally extracts semantic information to predict scene-compliant destinations. We conduct extensive experiments on publicly-available datasets (i.e. SDD, inD, ETH/UCY) and show that our approach performs on par with state-of-the-art techniques while reducing model complexity.

</p>
</details>

<details><summary><b>Scheduling Virtual Conferences Fairly: Achieving Equitable Participant and Speaker Satisfaction</b>
<a href="https://arxiv.org/abs/2204.12062">arxiv:2204.12062</a>
&#x1F4C8; 9 <br>
<p>Gourab K. Patro, Prithwish Jana, Abhijnan Chakraborty, Krishna P. Gummadi, Niloy Ganguly</p></summary>
<p>

**Abstract:** Recently, almost all conferences have moved to virtual mode due to the pandemic-induced restrictions on travel and social gathering. Contrary to in-person conferences, virtual conferences face the challenge of efficiently scheduling talks, accounting for the availability of participants from different timezones and their interests in attending different talks. A natural objective for conference organizers is to maximize efficiency, e.g., total expected audience participation across all talks. However, we show that optimizing for efficiency alone can result in an unfair virtual conference schedule, where individual utilities for participants and speakers can be highly unequal. To address this, we formally define fairness notions for participants and speakers, and derive suitable objectives to account for them. As the efficiency and fairness objectives can be in conflict with each other, we propose a joint optimization framework that allows conference organizers to design schedules that balance (i.e., allow trade-offs) among efficiency, participant fairness and speaker fairness objectives. While the optimization problem can be solved using integer programming to schedule smaller conferences, we provide two scalable techniques to cater to bigger conferences. Extensive evaluations over multiple real-world datasets show the efficacy and flexibility of our proposed approaches.

</p>
</details>

<details><summary><b>Can Rationalization Improve Robustness?</b>
<a href="https://arxiv.org/abs/2204.11790">arxiv:2204.11790</a>
&#x1F4C8; 9 <br>
<p>Howard Chen, Jacqueline He, Karthik Narasimhan, Danqi Chen</p></summary>
<p>

**Abstract:** A growing line of work has investigated the development of neural NLP models that can produce rationales--subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales ("rationalizer") before making predictions ("predictor"), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end, we systematically generate various types of 'AddText' attacks for both token and sentence-level rationalization tasks, and perform an extensive empirical evaluation of state-of-the-art rationale models across five different tasks. Our experiments reveal that the rationale models show the promise to improve robustness, while they struggle in certain scenarios--when the rationalizer is sensitive to positional bias or lexical choices of attack text. Further, leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize-then-predict framework.

</p>
</details>

<details><summary><b>A Survey on Word Meta-Embedding Learning</b>
<a href="https://arxiv.org/abs/2204.11660">arxiv:2204.11660</a>
&#x1F4C8; 9 <br>
<p>Danushka Bollegala, James O'Neill</p></summary>
<p>

**Abstract:** Meta-embedding (ME) learning is an emerging approach that attempts to learn more accurate word embeddings given existing (source) word embeddings as the sole input.
  Due to their ability to incorporate semantics from multiple source embeddings in a compact manner with superior performance, ME learning has gained popularity among practitioners in NLP.
  To the best of our knowledge, there exist no prior systematic survey on ME learning and this paper attempts to fill this need.
  We classify ME learning methods according to multiple factors such as whether they (a) operate on static or contextualised embeddings, (b) trained in an unsupervised manner or (c) fine-tuned for a particular task/domain.
  Moreover, we discuss the limitations of existing ME learning methods and highlight potential future research directions.

</p>
</details>

<details><summary><b>Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset</b>
<a href="https://arxiv.org/abs/2204.11642">arxiv:2204.11642</a>
&#x1F4C8; 9 <br>
<p>Leon Sixt, Martin Schuessler, Oana-Iuliana Popescu, Philipp Weiß, Tim Landgraf</p></summary>
<p>

**Abstract:** A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model's respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies. For code see, https://github.com/berleon/do_users_benefit_from_interpretable_vision

</p>
</details>

<details><summary><b>Boundary Smoothing for Named Entity Recognition</b>
<a href="https://arxiv.org/abs/2204.12031">arxiv:2204.12031</a>
&#x1F4C8; 7 <br>
<p>Enwei Zhu, Jinpeng Li</p></summary>
<p>

**Abstract:** Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity probabilities from annotated spans to the surrounding ones. Built on a simple but strong baseline, our model achieves results better than or competitive with previous state-of-the-art systems on eight well-known NER benchmarks. Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes.

</p>
</details>

<details><summary><b>LightDefectNet: A Highly Compact Deep Anti-Aliased Attention Condenser Neural Network Architecture for Light Guide Plate Surface Defect Detection</b>
<a href="https://arxiv.org/abs/2204.11765">arxiv:2204.11765</a>
&#x1F4C8; 7 <br>
<p>Carol Xu, Mahmoud Famouri, Gautam Bathla, Mohammad Javad Shafiee, Alexander Wong</p></summary>
<p>

**Abstract:** Light guide plates are essential optical components widely used in a diverse range of applications ranging from medical lighting fixtures to back-lit TV displays. An essential step in the manufacturing of light guide plates is the quality inspection of defects such as scratches, bright/dark spots, and impurities. This is mainly done in industry through manual visual inspection for plate pattern irregularities, which is time-consuming and prone to human error and thus act as a significant barrier to high-throughput production. Advances in deep learning-driven computer vision has led to the exploration of automated visual quality inspection of light guide plates to improve inspection consistency, accuracy, and efficiency. However, given the cost constraints in visual inspection scenarios, the widespread adoption of deep learning-driven computer vision methods for inspecting light guide plates has been greatly limited due to high computational requirements. In this study, we explore the utilization of machine-driven design exploration with computational and "best-practices" constraints as well as L$_1$ paired classification discrepancy loss to create LightDefectNet, a highly compact deep anti-aliased attention condenser neural network architecture tailored specifically for light guide plate surface defect detection in resource-constrained scenarios. Experiments show that LightDetectNet achieves a detection accuracy of $\sim$98.2% on the LGPSDD benchmark while having just 770K parameters ($\sim$33$\times$ and $\sim$6.9$\times$ lower than ResNet-50 and EfficientNet-B0, respectively) and $\sim$93M FLOPs ($\sim$88$\times$ and $\sim$8.4$\times$ lower than ResNet-50 and EfficientNet-B0, respectively) and $\sim$8.8$\times$ faster inference speed than EfficientNet-B0 on an embedded ARM processor.

</p>
</details>

<details><summary><b>Integrating Prior Knowledge in Post-hoc Explanations</b>
<a href="https://arxiv.org/abs/2204.11634">arxiv:2204.11634</a>
&#x1F4C8; 7 <br>
<p>Adulam Jeyasothy, Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Marcin Detyniecki</p></summary>
<p>

**Abstract:** In the field of eXplainable Artificial Intelligence (XAI), post-hoc interpretability methods aim at explaining to a user the predictions of a trained decision model. Integrating prior knowledge into such interpretability methods aims at improving the explanation understandability and allowing for personalised explanations adapted to each user. In this paper, we propose to define a cost function that explicitly integrates prior knowledge into the interpretability objectives: we present a general framework for the optimization problem of post-hoc interpretability methods, and show that user knowledge can thus be integrated to any method by adding a compatibility term in the cost function. We instantiate the proposed formalization in the case of counterfactual explanations and propose a new interpretability method called Knowledge Integration in Counterfactual Explanation (KICE) to optimize it. The paper performs an experimental study on several benchmark data sets to characterize the counterfactual instances generated by KICE, as compared to reference methods.

</p>
</details>

<details><summary><b>Zero-Shot Logit Adjustment</b>
<a href="https://arxiv.org/abs/2204.11822">arxiv:2204.11822</a>
&#x1F4C8; 6 <br>
<p>Dubing Chen, Yuming Shen, Haofeng Zhang, Philip H. S. Torr</p></summary>
<p>

**Abstract:** Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses challenges in recognizing the novel classes in the test phase. The development of generative models enables current GZSL techniques to probe further into the semantic-visual link, culminating in a two-stage form that includes a generator and a classifier. However, existing generation-based methods focus on enhancing the generator's effect while neglecting the improvement of the classifier. In this paper, we first conduct an analysis of two properties of the generated pseudo unseen sample: bias and homogeneity. Then, we perform variational Bayesian inference to back-derive the evaluation metrics, which reflects the balance of the seen and unseen classes. As a consequence of our derivation, the aforementioned two properties are incorporated into the classifier training as seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further puts semantic-based classifiers into effect in generation-based GZSL. Our experiments demonstrate that the proposed technique achieves the state of the art when combined with the basic generator, and it can improve various generative zero-shot learning frameworks. Our codes are available on \url{https://github.com/cdb342/IJCAI-2022-ZLA}.

</p>
</details>

<details><summary><b>PVNAS: 3D Neural Architecture Search with Point-Voxel Convolution</b>
<a href="https://arxiv.org/abs/2204.11797">arxiv:2204.11797</a>
&#x1F4C8; 6 <br>
<p>Zhijian Liu, Haotian Tang, Shengyu Zhao, Kevin Shao, Song Han</p></summary>
<p>

**Abstract:** 3D neural networks are widely used in real-world applications (e.g., AR/VR headsets, self-driving cars). They are required to be fast and accurate; however, limited hardware resources on edge devices make these requirements rather challenging. Previous work processes 3D data using either voxel-based or point-based neural networks, but both types of 3D models are not hardware-efficient due to the large memory footprint and random memory access. In this paper, we study 3D deep learning from the efficiency perspective. We first systematically analyze the bottlenecks of previous 3D methods. We then combine the best from point-based and voxel-based models together and propose a novel hardware-efficient 3D primitive, Point-Voxel Convolution (PVConv). We further enhance this primitive with the sparse convolution to make it more effective in processing large (outdoor) scenes. Based on our designed 3D primitive, we introduce 3D Neural Architecture Search (3D-NAS) to explore the best 3D network architecture given a resource constraint. We evaluate our proposed method on six representative benchmark datasets, achieving state-of-the-art performance with 1.8-23.7x measured speedup. Furthermore, our method has been deployed to the autonomous racing vehicle of MIT Driverless, achieving larger detection range, higher accuracy and lower latency.

</p>
</details>

<details><summary><b>Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications</b>
<a href="https://arxiv.org/abs/2204.11786">arxiv:2204.11786</a>
&#x1F4C8; 6 <br>
<p>Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, Song Han</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) have achieved unprecedented success in the field of artificial intelligence (AI), including computer vision, natural language processing and speech recognition. However, their superior performance comes at the considerable cost of computational complexity, which greatly hinders their applications in many resource-constrained devices, such as mobile phones and Internet of Things (IoT) devices. Therefore, methods and techniques that are able to lift the efficiency bottleneck while preserving the high accuracy of DNNs are in great demand in order to enable numerous edge AI applications. This paper provides an overview of efficient deep learning methods, systems and applications. We start from introducing popular model compression methods, including pruning, factorization, quantization as well as compact model design. To reduce the large design cost of these manual solutions, we discuss the AutoML framework for each of them, such as neural architecture search (NAS) and automated pruning and quantization. We then cover efficient on-device training to enable user customization based on the local data on mobile devices. Apart from general acceleration techniques, we also showcase several task-specific accelerations for point cloud, video and natural language processing by exploiting their spatial sparsity and temporal/token redundancy. Finally, to support all these algorithmic advancements, we introduce the efficient deep learning system design from both software and hardware perspectives.

</p>
</details>

<details><summary><b>Masked Image Modeling Advances 3D Medical Image Analysis</b>
<a href="https://arxiv.org/abs/2204.11716">arxiv:2204.11716</a>
&#x1F4C8; 6 <br>
<p>Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem Safta, Mariann Micsinai Balan, Venkat Sethuraman, Kevin Brown</p></summary>
<p>

**Abstract:** Recently, masked image modeling (MIM) has gained considerable attention due to its capacity to learn from vast amounts of unlabeled data and has been demonstrated to be effective on a wide variety of vision tasks involving natural images. Meanwhile, the potential of self-supervised learning in modeling 3D medical images is anticipated to be immense due to the high quantities of unlabeled images, and the expense and difficulty of quality labels. However, MIM's applicability to medical images remains uncertain. In this paper, we demonstrate that masked image modeling approaches can also advance 3D medical images analysis in addition to natural images. We study how masked image modeling strategies leverage performance from the viewpoints of 3D medical image segmentation as a representative downstream task: i) when compared to naive contrastive learning, masked image modeling approaches accelerate the convergence of supervised training even faster (1.40$\times$) and ultimately produce a higher dice score; ii) predicting raw voxel values with a high masking ratio and a relatively smaller patch size is non-trivial self-supervised pretext-task for medical images modeling; iii) a lightweight decoder or projection head design for reconstruction is powerful for masked image modeling on 3D medical images which speeds up training and reduce cost; iv) finally, we also investigate the effectiveness of MIM methods under different practical scenarios where different image resolutions and labeled data ratios are applied.

</p>
</details>

<details><summary><b>A Simple Structure For Building A Robust Model</b>
<a href="https://arxiv.org/abs/2204.11596">arxiv:2204.11596</a>
&#x1F4C8; 6 <br>
<p>Xiao Tan, JingBo Gao, Ruolin Li</p></summary>
<p>

**Abstract:** As deep learning applications, especially programs of computer vision, are increasingly deployed in our lives, we have to think more urgently about the security of these applications.One effective way to improve the security of deep learning models is to perform adversarial training, which allows the model to be compatible with samples that are deliberately created for use in attacking the model.Based on this, we propose a simple architecture to build a model with a certain degree of robustness, which improves the robustness of the trained network by adding an adversarial sample detection network for cooperative training.At the same time, we design a new data sampling strategy that incorporates multiple existing attacks, allowing the model to adapt to many different adversarial attacks with a single training.We conducted some experiments to test the effectiveness of this design based on Cifar10 dataset, and the results indicate that it has some degree of positive effect on the robustness of the model.Our code could be found at https://github.com/dowdyboy/simple_structure_for_robust_model.

</p>
</details>

<details><summary><b>Rethinking Multi-Modal Alignment in Video Question Answering from Feature and Sample Perspectives</b>
<a href="https://arxiv.org/abs/2204.11544">arxiv:2204.11544</a>
&#x1F4C8; 6 <br>
<p>Shaoning Xiao, Long Chen, Kaifeng Gao, Zhao Wang, Yi Yang, Jun Xiao</p></summary>
<p>

**Abstract:** Reasoning about causal and temporal event relations in videos is a new destination of Video Question Answering (VideoQA).The major stumbling block to achieve this purpose is the semantic gap between language and video since they are at different levels of abstraction. Existing efforts mainly focus on designing sophisticated architectures while utilizing frame- or object-level visual representations. In this paper, we reconsider the multi-modal alignment problem in VideoQA from feature and sample perspectives to achieve better performance. From the view of feature,we break down the video into trajectories and first leverage trajectory feature in VideoQA to enhance the alignment between two modalities. Moreover, we adopt a heterogeneous graph architecture and design a hierarchical framework to align both trajectory-level and frame-level visual feature with language feature. In addition, we found that VideoQA models are largely dependent on language priors and always neglect visual-language interactions. Thus, two effective yet portable training augmentation strategies are designed to strengthen the cross-modal correspondence ability of our model from the view of sample. Extensive results show that our method outperforms all the state-of-the-art models on the challenging NExT-QA benchmark, which demonstrates the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>Exoplanet Cartography using Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2204.11821">arxiv:2204.11821</a>
&#x1F4C8; 5 <br>
<p>K. Meinke, D. M. Stam, P. M. Visser</p></summary>
<p>

**Abstract:** In the near-future, dedicated telescopes observe Earth-like exoplanets in reflected light, allowing their characterization. Because of the huge distances, every exoplanet will be a single pixel, but temporal variations in its spectral flux hold information about the planet's surface and atmosphere. We test convolutional neural networks for retrieving a planet's rotation axis, surface and cloud map from simulated single-pixel flux and polarization observations. We investigate the assumption that the planets reflect Lambertian in the retrieval while their actual reflection is bidirectional, and of including polarization in retrievals. We simulate observations along a planet's orbit using a radiative transfer algorithm that includes polarization and bidirectional reflection by vegetation, desert, oceans, water clouds, and Rayleigh scattering in 6 spectral bands from 400 to 800 nm, at various photon noise levels. The surface-types and cloud patterns of the facets covering a model planet are based on probability distributions. Our networks are trained with simulated observations of millions of planets before retrieving maps of test planets. The neural networks can constrain rotation axes with a mean squared error (MSE) as small as 0.0097, depending on the orbital inclination. On a bidirectionally reflecting planet, 92% of ocean and 85% of vegetation, desert, and cloud facets are correctly retrieved, in the absence of noise. With realistic noise, it should still be possible to retrieve the main map features with a dedicated telescope. Except for face-on orbits, a network trained with Lambertian reflecting planets, yields significant retrieval errors when given observations of bidirectionally reflecting planets, in particular, brightness artefacts around a planet's pole. Including polarization improves retrieving the rotation axis and the accuracy of the retrieval of ocean and cloud facets.

</p>
</details>

<details><summary><b>Performer: A Novel PPG to ECG Reconstruction Transformer For a Digital Biomarker of Cardiovascular Disease Detection</b>
<a href="https://arxiv.org/abs/2204.11795">arxiv:2204.11795</a>
&#x1F4C8; 5 <br>
<p>Ella Lan</p></summary>
<p>

**Abstract:** Cardiovascular diseases (CVDs) have become the top one cause of death; three-quarters of these deaths occur in lower-income communities. Electrocardiography (ECG), an electrical measurement capturing the cardiac activities, is a gold-standard to diagnose CVDs. However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. Meanwhile, photoplethysmography (PPG) is easy to collect, but the limited accuracy constrains its clinical usage. In this research, a novel Transformer-based architecture, Performer, is invented to reconstruct ECG from PPG and to create a novel digital biomarker, PPG along with its reconstructed ECG, as multiple modalities for CVD detection. This architecture, for the first time, performs Transformer sequence to sequence translation on biomedical waveforms, while also utilizing the advantages of the easily accessible PPG and the well-studied base of ECG. Shifted Patch-based Attention (SPA) is created to maximize the signal features by fetching the various sequence lengths as hierarchical stages into the training while also capturing cross-patch connections through the shifted patch mechanism. This architecture generates a state-of-the-art performance of 0.29 RMSE for reconstructing ECG from PPG, achieving an average of 95.9% diagnosis for CVDs on the MIMIC III dataset and 75.9% for diabetes on the PPG-BP dataset. Performer, along with its novel digital biomarker, offers a low-cost and non-invasive solution for continuous cardiac monitoring, only requiring the easily extractable PPG data to reconstruct the not-as-accessible ECG data. As a prove of concept, an earring wearable, named PEARL (prototype), is designed to scale up the point-of-care (POC) healthcare system.

</p>
</details>

<details><summary><b>Adversarial Attention for Human Motion Synthesis</b>
<a href="https://arxiv.org/abs/2204.11751">arxiv:2204.11751</a>
&#x1F4C8; 5 <br>
<p>Matthew Malek-Podjaski, Fani Deligianni</p></summary>
<p>

**Abstract:** Analysing human motions is a core topic of interest for many disciplines, from Human-Computer Interaction, to entertainment, Virtual Reality and healthcare. Deep learning has achieved impressive results in capturing human pose in real-time. On the other hand, due to high inter-subject variability, human motion analysis models often suffer from not being able to generalise to data from unseen subjects due to very limited specialised datasets available in fields such as healthcare. However, acquiring human motion datasets is highly time-consuming, challenging, and expensive. Hence, human motion synthesis is a crucial research problem within deep learning and computer vision. We present a novel method for controllable human motion synthesis by applying attention-based probabilistic deep adversarial models with end-to-end training. We show that we can generate synthetic human motion over both short- and long-time horizons through the use of adversarial attention. Furthermore, we show that we can improve the classification performance of deep learning models in cases where there is inadequate real data, by supplementing existing datasets with synthetic motions.

</p>
</details>

<details><summary><b>Tac2Pose: Tactile Object Pose Estimation from the First Touch</b>
<a href="https://arxiv.org/abs/2204.11701">arxiv:2204.11701</a>
&#x1F4C8; 5 <br>
<p>Maria Bauza, Antonia Bronars, Alberto Rodriguez</p></summary>
<p>

**Abstract:** In this paper, we present Tac2Pose, an object-specific approach to tactile pose estimation from the first touch for known objects. Given the object geometry, we learn a tailored perception model in simulation that estimates a probability distribution over possible object poses given a tactile observation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor, we match it against the pre-computed set using an object-specific embedding learned using contrastive learning. We obtain contact shapes from the sensor with an object-agnostic calibration step that maps RGB tactile observations to binary contact shapes. This mapping, which can be reused across object and sensor instances, is the only step trained with real sensor data. This results in a perception model that localizes objects from the first real tactile observation. Importantly, it produces pose distributions and can incorporate additional pose constraints coming from other perception systems, contacts, or priors.
  We provide quantitative results for 20 objects. Tac2Pose provides high accuracy pose estimations from distinctive tactile observations while regressing meaningful pose distributions to account for those contact shapes that could result from different object poses. We also test Tac2Pose on object models reconstructed from a 3D scanner, to evaluate the robustness to uncertainty in the object model. Finally, we demonstrate the advantages of Tac2Pose compared with three baseline methods for tactile pose estimation: directly regressing the object pose with a neural network, matching an observed contact to a set of possible contacts using a standard classification neural network, and direct pixel comparison of an observed contact with a set of possible contacts.
  Website: http://mcube.mit.edu/research/tac2pose.html

</p>
</details>

<details><summary><b>Hybrid ISTA: Unfolding ISTA With Convergence Guarantees Using Free-Form Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2204.11640">arxiv:2204.11640</a>
&#x1F4C8; 5 <br>
<p>Ziyang Zheng, Wenrui Dai, Duoduo Xue, Chenglin Li, Junni Zou, Hongkai Xiong</p></summary>
<p>

**Abstract:** It is promising to solve linear inverse problems by unfolding iterative algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep neural networks (DNNs) with learnable parameters. However, existing ISTA-based unfolded algorithms restrict the network architectures for iterative updates with the partial weight coupling structure to guarantee convergence. In this paper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned parameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible and reasonable network architectures), while ensuring theoretical convergence. We first develop HCISTA to improve the efficiency and flexibility of classical ISTA (with pre-computed parameters) without compromising the convergence rate in theory. Furthermore, the DNN-based hybrid algorithm is generalized to popular variants of learned ISTA, dubbed HLISTA, to enable a free architecture of learned parameters with a guarantee of linear convergence. To our best knowledge, this paper is the first to provide a convergence-provable framework that enables free-form DNNs in ISTA-based unfolded algorithms. This framework is general to endow arbitrary DNNs for solving linear inverse problems with convergence guarantees. Extensive experiments demonstrate that hybrid ISTA can reduce the reconstruction error with an improved convergence rate in the tasks of sparse recovery and compressive sensing.

</p>
</details>

<details><summary><b>IMDeception: Grouped Information Distilling Super-Resolution Network</b>
<a href="https://arxiv.org/abs/2204.11463">arxiv:2204.11463</a>
&#x1F4C8; 5 <br>
<p>Mustafa Ayazoglu</p></summary>
<p>

**Abstract:** Single-Image-Super-Resolution (SISR) is a classical computer vision problem that has benefited from the recent advancements in deep learning methods, especially the advancements of convolutional neural networks (CNN). Although state-of-the-art methods improve the performance of SISR on several datasets, direct application of these networks for practical use is still an issue due to heavy computational load. For this purpose, recently, researchers have focused on more efficient and high-performing network structures. Information multi-distilling network (IMDN) is one of the highly efficient SISR networks with high performance and low computational load. IMDN achieves this efficiency with various mechanisms such as Intermediate Information Collection (IIC), working in a global setting, Progressive Refinement Module (PRM), and Contrast Aware Channel Attention (CCA), employed in a local setting. These mechanisms, however, do not equally contribute to the efficiency and performance of IMDN. In this work, we propose the Global Progressive Refinement Module (GPRM) as a less parameter-demanding alternative to the IIC module for feature aggregation. To further decrease the number of parameters and floating point operations persecond (FLOPS), we also propose Grouped Information Distilling Blocks (GIDB). Using the proposed structures, we design an efficient SISR network called IMDeception. Experiments reveal that the proposed network performs on par with state-of-the-art models despite having a limited number of parameters and FLOPS. Furthermore, using grouped convolutions as a building block of GIDB increases room for further optimization during deployment. To show its potential, the proposed model was deployed on NVIDIA Jetson Xavier AGX and it has been shown that it can run in real-time on this edge device

</p>
</details>

<details><summary><b>AI Personification: Estimating the Personality of Language Models</b>
<a href="https://arxiv.org/abs/2204.12000">arxiv:2204.12000</a>
&#x1F4C8; 4 <br>
<p>Saketh Reddy Karra, Son Nguyen, Theja Tulabandhula</p></summary>
<p>

**Abstract:** Technology for open-ended language generation, a key application of artificial intelligence, has advanced to a great extent in recent years. Large-scale language models, which are trained on large corpora of text, are being used in a wide range of applications everywhere, from virtual assistants to conversational bots. While these language models output fluent text, existing research shows that these models can and do capture human biases. Many of these biases, especially those that could potentially cause harm, are being well investigated. On the other hand, studies that infer and change personality traits inherited by these models have been scarce or non-existent. In this work, we explore the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them. Our work builds on the popular Big Five factors and develops robust methods that quantify the personality traits of these models and their underlying datasets. In particular, we trigger the models with a questionnaire designed for personality assessment and subsequently classify the text responses into quantifiable traits using a Zero-shot classifier. Our classification sheds light on an important anthropomorphic element found in such AI models and can help stakeholders decide how they should be applied and how society could perceive them. We augment our analysis by studying approaches that can alter these personalities.

</p>
</details>

<details><summary><b>Offline Vehicle Routing Problem with Online Bookings: A Novel Problem Formulation with Applications to Paratransit</b>
<a href="https://arxiv.org/abs/2204.11992">arxiv:2204.11992</a>
&#x1F4C8; 4 <br>
<p>Amutheezan Sivagnanam, Salah Uddin Kadir, Ayan Mukhopadhyay, Philip Pugliese, Abhishek Dubey, Samitha Samaranayake, Aron Laszka</p></summary>
<p>

**Abstract:** Vehicle routing problems (VRPs) can be divided into two major categories: offline VRPs, which consider a given set of trip requests to be served, and online VRPs, which consider requests as they arrive in real-time. Based on discussions with public transit agencies, we identify a real-world problem that is not addressed by existing formulations: booking trips with flexible pickup windows (e.g., 3 hours) in advance (e.g., the day before) and confirming tight pickup windows (e.g., 30 minutes) at the time of booking. Such a service model is often required in paratransit service settings, where passengers typically book trips for the next day over the phone. To address this gap between offline and online problems, we introduce a novel formulation, the offline vehicle routing problem with online bookings. This problem is very challenging computationally since it faces the complexity of considering large sets of requests -- similar to offline VRPs -- but must abide by strict constraints on running time -- similar to online VRPs. To solve this problem, we propose a novel computational approach, which combines an anytime algorithm with a learning-based policy for real-time decisions. Based on a paratransit dataset obtained from our partner transit agency, we demonstrate that our novel formulation and computational approach lead to significantly better outcomes in this service setting than existing algorithms.

</p>
</details>

<details><summary><b>Multi-objective Pointer Network for Combinatorial Optimization</b>
<a href="https://arxiv.org/abs/2204.11860">arxiv:2204.11860</a>
&#x1F4C8; 4 <br>
<p>Le-yang Gao, Rui Wang, Chuang Liu, Zhao-hong Jia</p></summary>
<p>

**Abstract:** Multi-objective combinatorial optimization problems (MOCOPs), one type of complex optimization problems, widely exist in various real applications. Although meta-heuristics have been successfully applied to address MOCOPs, the calculation time is often much longer. Recently, a number of deep reinforcement learning (DRL) methods have been proposed to generate approximate optimal solutions to the combinatorial optimization problems. However, the existing studies on DRL have seldom focused on MOCOPs. This study proposes a single-model deep reinforcement learning framework, called multi-objective Pointer Network (MOPN), where the input structure of PN is effectively improved so that the single PN is capable of solving MOCOPs. In addition, two training strategies, based on representative model and transfer learning, respectively, are proposed to further enhance the performance of MOPN in different application scenarios. Moreover, compared to classical meta-heuristics, MOPN only consumes much less time on forward propagation to obtain the Pareto front. Meanwhile, MOPN is insensitive to problem scale, meaning that a trained MOPN is able to address MOCOPs with different scales. To verify the performance of MOPN, extensive experiments are conducted on three multi-objective traveling salesman problems, in comparison with one state-of-the-art model DRL-MOA and three classical multi-objective meta-heuristics. Experimental results demonstrate that the proposed model outperforms all the comparative methods with only 20\% to 40\% training time of DRL-MOA.

</p>
</details>

<details><summary><b>Data Uncertainty without Prediction Models</b>
<a href="https://arxiv.org/abs/2204.11858">arxiv:2204.11858</a>
&#x1F4C8; 4 <br>
<p>Bongjoon Park, Eunkyung Koh</p></summary>
<p>

**Abstract:** Data acquisition processes for machine learning are often costly. To construct a high-performance prediction model with fewer data, a degree of difficulty in prediction is often deployed as the acquisition function in adding a new data point. The degree of difficulty is referred to as uncertainty in prediction models. We propose an uncertainty estimation method named a Distance-weighted Class Impurity without explicit use of prediction models. We estimated uncertainty using distances and class impurities around the location, and compared it with several methods based on prediction models for uncertainty estimation by active learning tasks. We verified that the Distance-weighted Class Impurity works effectively regardless of prediction models.

</p>
</details>

<details><summary><b>Discovering Gateway Ports in Maritime Using Temporal Graph Neural Network Port Classification</b>
<a href="https://arxiv.org/abs/2204.11855">arxiv:2204.11855</a>
&#x1F4C8; 4 <br>
<p>Dogan Altan, Mohammad Etemad, Dusica Marijan, Tetyana Kholodna</p></summary>
<p>

**Abstract:** Vessel navigation is influenced by various factors, such as dynamic environmental factors that change over time or static features such as vessel type or depth of the ocean. These dynamic and static navigational factors impose limitations on vessels, such as long waiting times in regions outside the actual ports, and we call these waiting regions gateway ports. Identifying gateway ports and their associated features such as congestion and available utilities can enhance vessel navigation by planning on fuel optimization or saving time in cargo operation. In this paper, we propose a novel temporal graph neural network (TGNN) based port classification method to enable vessels to discover gateway ports efficiently, thus optimizing their operations. The proposed method processes vessel trajectory data to build dynamic graphs capturing spatio-temporal dependencies between a set of static and dynamic navigational features in the data, and it is evaluated in terms of port classification accuracy on a real-world data set collected from ten vessels operating in Halifax, NS, Canada. The experimental results indicate that our TGNN-based port classification method provides an f-score of 95% in classifying ports.

</p>
</details>

<details><summary><b>Graph Auto-Encoders for Network Completion</b>
<a href="https://arxiv.org/abs/2204.11852">arxiv:2204.11852</a>
&#x1F4C8; 4 <br>
<p>Zhang Zhang, Ruyi Tao, Yongzai Tao, Jiang Zhang</p></summary>
<p>

**Abstract:** Completing a graph means inferring the missing nodes and edges from a partially observed network. Different methods have been proposed to solve this problem, but none of them employed the pattern similarity of parts of the graph. In this paper, we propose a model to use the learned pattern of connections from the observed part of the network based on the Graph Auto-Encoder technique and generalize these patterns to complete the whole graph. Our proposed model achieved competitive performance with less information needed. Empirical analysis of synthetic datasets and real-world datasets from different domains show that our model can complete the network with higher accuracy compared with baseline prediction models in most cases. Furthermore, we also studied the character of the model and found it is particularly suitable to complete a network that has more complex local connection patterns.

</p>
</details>

<details><summary><b>CellDefectNet: A Machine-designed Attention Condenser Network for Electroluminescence-based Photovoltaic Cell Defect Inspection</b>
<a href="https://arxiv.org/abs/2204.11766">arxiv:2204.11766</a>
&#x1F4C8; 4 <br>
<p>Carol Xu, Mahmoud Famouri, Gautam Bathla, Saeejith Nair, Mohammad Javad Shafiee, Alexander Wong</p></summary>
<p>

**Abstract:** Photovoltaic cells are electronic devices that convert light energy to electricity, forming the backbone of solar energy harvesting systems. An essential step in the manufacturing process for photovoltaic cells is visual quality inspection using electroluminescence imaging to identify defects such as cracks, finger interruptions, and broken cells. A big challenge faced by industry in photovoltaic cell visual inspection is the fact that it is currently done manually by human inspectors, which is extremely time consuming, laborious, and prone to human error. While deep learning approaches holds great potential to automating this inspection, the hardware resource-constrained manufacturing scenario makes it challenging for deploying complex deep neural network architectures. In this work, we introduce CellDefectNet, a highly efficient attention condenser network designed via machine-driven design exploration specifically for electroluminesence-based photovoltaic cell defect detection on the edge. We demonstrate the efficacy of CellDefectNet on a benchmark dataset comprising of a diversity of photovoltaic cells captured using electroluminescence imagery, achieving an accuracy of ~86.3% while possessing just 410K parameters (~13$\times$ lower than EfficientNet-B0, respectively) and ~115M FLOPs (~12$\times$ lower than EfficientNet-B0) and ~13$\times$ faster on an ARM Cortex A-72 embedded processor when compared to EfficientNet-B0.

</p>
</details>

<details><summary><b>LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback</b>
<a href="https://arxiv.org/abs/2204.11545">arxiv:2204.11545</a>
&#x1F4C8; 4 <br>
<p>Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng</p></summary>
<p>

**Abstract:** Pseudo-relevance feedback (PRF) has proven to be an effective query reformulation technique to improve retrieval accuracy. It aims to alleviate the mismatch of linguistic expressions between a query and its potential relevant documents. Existing PRF methods independently treat revised queries originating from the same query but using different numbers of feedback documents, resulting in severe query drift. Without comparing the effects of two different revisions from the same query, a PRF model may incorrectly focus on the additional irrelevant information increased in the more feedback, and thus reformulate a query that is less effective than the revision using the less feedback. Ideally, if a PRF model can distinguish between irrelevant and relevant information in the feedback, the more feedback documents there are, the better the revised query will be. To bridge this gap, we propose the Loss-over-Loss (LoL) framework to compare the reformulation losses between different revisions of the same query during training. Concretely, we revise an original query multiple times in parallel using different amounts of feedback and compute their reformulation losses. Then, we introduce an additional regularization loss on these reformulation losses to penalize revisions that use more feedback but gain larger losses. With such comparative regularization, the PRF model is expected to learn to suppress the extra increased irrelevant information by comparing the effects of different revised queries. Further, we present a differentiable query reformulation method to implement this framework. This method revises queries in the vector space and directly optimizes the retrieval performance of query vectors, applicable for both sparse and dense retrieval models. Empirical evaluation demonstrates the effectiveness and robustness of our method for two typical sparse and dense retrieval models.

</p>
</details>

<details><summary><b>Faculty Distillation with Optimal Transport</b>
<a href="https://arxiv.org/abs/2204.11526">arxiv:2204.11526</a>
&#x1F4C8; 4 <br>
<p>Su Lu, Han-Jia Ye, De-Chuan Zhan</p></summary>
<p>

**Abstract:** Knowledge distillation (KD) has shown its effectiveness in improving a student classifier given a suitable teacher. The outpouring of diverse and plentiful pre-trained models may provide abundant teacher resources for KD. However, these models are often trained on different tasks from the student, which requires the student to precisely select the most contributive teacher and enable KD across different label spaces. These restrictions disclose the insufficiency of standard KD and motivate us to study a new paradigm called faculty distillation. Given a group of teachers (faculty), a student needs to select the most relevant teacher and perform generalized knowledge reuse. To this end, we propose to link teacher's task and student's task by optimal transport. Based on the semantic relationship between their label spaces, we can bridge the support gap between output distributions by minimizing Sinkhorn distances. The transportation cost also acts as a measurement of teachers' adaptability so that we can rank the teachers efficiently according to their relatedness. Experiments under various settings demonstrate the succinctness and versatility of our method.

</p>
</details>

<details><summary><b>End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network</b>
<a href="https://arxiv.org/abs/2204.11479">arxiv:2204.11479</a>
&#x1F4C8; 4 <br>
<p>Avi Gazneli, Gadi Zimerman, Tal Ridnik, Gilad Sharir, Asaf Noy</p></summary>
<p>

**Abstract:** While efficient architectures and a plethora of augmentations for end-to-end image classification tasks have been suggested and heavily investigated, state-of-the-art techniques for audio classifications still rely on numerous representations of the audio signal together with large architectures, fine-tuned from large datasets. By utilizing the inherited lightweight nature of audio and novel audio augmentations, we were able to present an efficient end-to-end network with strong generalization ability. Experiments on a variety of sound classification sets demonstrate the effectiveness and robustness of our approach, by achieving state-of-the-art results in various settings. Public code will be available.

</p>
</details>

<details><summary><b>Imitation Learning from Observations under Transition Model Disparity</b>
<a href="https://arxiv.org/abs/2204.11446">arxiv:2204.11446</a>
&#x1F4C8; 4 <br>
<p>Tanmay Gangwani, Yuan Zhou, Jian Peng</p></summary>
<p>

**Abstract:** Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch.

</p>
</details>

<details><summary><b>Spontaneous Emergence of Computation in Network Cascades</b>
<a href="https://arxiv.org/abs/2204.11956">arxiv:2204.11956</a>
&#x1F4C8; 3 <br>
<p>Galen Wilkerson, Sotiris Moschoyiannis, Henrik Jeldtoft Jensen</p></summary>
<p>

**Abstract:** Neuronal network computation and computation by avalanche supporting networks are of interest to the fields of physics, computer science (computation theory as well as statistical or machine learning) and neuroscience. Here we show that computation of complex Boolean functions arises spontaneously in threshold networks as a function of connectivity and antagonism (inhibition), computed by logic automata (motifs) in the form of computational cascades. We explain the emergent inverse relationship between the computational complexity of the motifs and their rank-ordering by function probabilities due to motifs, and its relationship to symmetry in function space. We also show that the optimal fraction of inhibition observed here supports results in computational neuroscience, relating to optimal information processing.

</p>
</details>

<details><summary><b>How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language</b>
<a href="https://arxiv.org/abs/2204.11909">arxiv:2204.11909</a>
&#x1F4C8; 3 <br>
<p>Shiyue Zhang, Ben Frey, Mohit Bansal</p></summary>
<p>

**Abstract:** More than 43% of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize endangered languages. We first suggest three principles that may help NLP practitioners to foster mutual understanding and collaboration with language communities, and we discuss three ways in which NLP can potentially assist in language education. We then take Cherokee, a severely-endangered Native American language, as a case study. After reviewing the language's history, linguistic features, and existing resources, we (in collaboration with Cherokee community members) arrive at a few meaningful ways NLP practitioners can collaborate with community partners. We suggest two approaches to enrich the Cherokee language's resources with machine-in-the-loop processing, and discuss several NLP tools that people from the Cherokee community have shown interest in. We hope that our work serves not only to inform the NLP community about Cherokee, but also to provide inspiration for future work on endangered languages in general. Our code and data will be open-sourced at https://github.com/ZhangShiyue/RevitalizeCherokee

</p>
</details>

<details><summary><b>Mapping Research Trajectories</b>
<a href="https://arxiv.org/abs/2204.11859">arxiv:2204.11859</a>
&#x1F4C8; 3 <br>
<p>Bastian Schäfermeier, Gerd Stumme, Tom Hanika</p></summary>
<p>

**Abstract:** Steadily growing amounts of information, such as annually published scientific papers, have become so large that they elude an extensive manual analysis. Hence, to maintain an overview, automated methods for the mapping and visualization of knowledge domains are necessary and important, e.g., for scientific decision makers. Of particular interest in this field is the development of research topics of different entities (e.g., scientific authors and venues) over time. However, existing approaches for their analysis are only suitable for single entity types, such as venues, and they often do not capture the research topics or the time dimension in an easily interpretable manner.
  Hence, we propose a principled approach for \emph{mapping research trajectories}, which is applicable to all kinds of scientific entities that can be represented by sets of published papers. For this, we transfer ideas and principles from the geographic visualization domain, specifically trajectory maps and interactive geographic maps. Our visualizations depict the research topics of entities over time in a straightforward interpr. manner. They can be navigated by the user intuitively and restricted to specific elements of interest. The maps are derived from a corpus of research publications (i.e., titles and abstracts) through a combination of unsupervised machine learning methods.
  In a practical demonstrator application, we exemplify the proposed approach on a publication corpus from machine learning. We observe that our trajectory visualizations of 30 top machine learning venues and 1000 major authors in this field are well interpretable and are consistent with background knowledge drawn from the entities' publications. Next to producing interactive, interpr. visualizations supporting different kinds of analyses, our computed trajectories are suitable for trajectory mining applications in the future.

</p>
</details>

<details><summary><b>KnowAugNet: Multi-Source Medical Knowledge Augmented Medication Prediction Network with Multi-Level Graph Contrastive Learning</b>
<a href="https://arxiv.org/abs/2204.11736">arxiv:2204.11736</a>
&#x1F4C8; 3 <br>
<p>Yang An, Bo Jin, Xiaopeng Wei</p></summary>
<p>

**Abstract:** Predicting medications is a crucial task in many intelligent healthcare systems. It can assist doctors in making informed medication decisions for patients according to electronic medical records (EMRs). However, medication prediction is a challenging data mining task due to the complex relations between medical codes. Most existing studies usually focus on mining the temporal relations between medical codes while neglecting the valuable spatial relations between heterogeneous or homogeneous medical codes, and the inherent relations between homogeneous medical codes from hierarchical ontology graph, which further limits the prediction performance. Therefore, to address these limitations, this paper proposes \textbf{KnowAugNet}, a multi-sourced medical knowledge augmented medication prediction network which can fully capture the diverse relations between medical codes via multi-level graph contrastive learning framework. Specifically, KnowAugNet first leverages the graph contrastive learning using graph attention network as the encoder to capture the implicit relations between homogeneous medical codes from the medical ontology graph and obtains the knowledge augmented medical codes embedding vectors. Then, it utilizes the graph contrastive learning using a weighted graph convolutional network as the encoder to capture the correlative relations between homogeneous or heterogeneous medical codes from the constructed medical prior relation graph and obtains the relation augmented medical codes embedding vectors. Finally, the augmented medical codes embedding vectors and the supervised medical codes embedding vectors are retrieved and input to the sequential learning network to capture the temporal relations of medical codes and predict medications for patients.

</p>
</details>

<details><summary><b>On the Performance of Machine Learning Methods for Breakthrough Curve Prediction</b>
<a href="https://arxiv.org/abs/2204.11719">arxiv:2204.11719</a>
&#x1F4C8; 3 <br>
<p>Daria Fokina, Oleg Iliev, Pavel Toktaliev, Ivan Oseledets, Felix Schindler</p></summary>
<p>

**Abstract:** Reactive flows are important part of numerous technical and environmental processes. Often monitoring the flow and species concentrations within the domain is not possible or is expensive, in contrast, outlet concentration is straightforward to measure. In connection with reactive flows in porous media, the term breakthrough curve is used to denote the time dependency of the outlet concentration with prescribed conditions at the inlet. In this work we apply several machine learning methods to predict breakthrough curves from the given set of parameters. In our case the parameters are the Damköhler and Peclet numbers. We perform a thorough analysis for the one-dimensional case and also provide the results for the three-dimensional case.

</p>
</details>

<details><summary><b>Data-driven prediction and control of extreme events in a chaotic flow</b>
<a href="https://arxiv.org/abs/2204.11682">arxiv:2204.11682</a>
&#x1F4C8; 3 <br>
<p>Alberto Racca, Luca Magri</p></summary>
<p>

**Abstract:** An extreme event is a sudden and violent change in the state of a nonlinear system. In fluid dynamics, extreme events can have adverse effects on the system's optimal design and operability, which calls for accurate methods for their prediction and control. In this paper, we propose a data-driven methodology for the prediction and control of extreme events in a chaotic shear flow. The approach is based on echo state networks, which are a type of reservoir computing that learn temporal correlations within a time-dependent dataset. The objective is five-fold. First, we exploit ad-hoc metrics from binary classification to analyse (i) how many of the extreme events predicted by the network actually occur in the test set (precision), and (ii) how many extreme events are missed by the network (recall). We apply a principled strategy for optimal hyperparameter selection, which is key to the networks' performance. Second, we focus on the time-accurate prediction of extreme events. We show that echo state networks are able to predict extreme events well beyond the predictability time, i.e., up to more than five Lyapunov times. Third, we focus on the long-term prediction of extreme events from a statistical point of view. By training the networks with datasets that contain non-converged statistics, we show that the networks are able to learn and extrapolate the flow's long-term statistics. In other words, the networks are able to extrapolate in time from relatively short time series. Fourth, we design a simple and effective control strategy to prevent extreme events from occurring. The control strategy decreases the occurrence of extreme events up to one order of magnitude with respect to the uncontrolled system. Finally, we analyse the robustness of the results for a range of Reynolds numbers. We show that the networks perform well across a wide range of regimes.

</p>
</details>

<details><summary><b>Supervised Attention in Sequence-to-Sequence Models for Speech Recognition</b>
<a href="https://arxiv.org/abs/2204.12308">arxiv:2204.12308</a>
&#x1F4C8; 2 <br>
<p>Gene-Ping Yang, Hao Tang</p></summary>
<p>

**Abstract:** Attention mechanism in sequence-to-sequence models is designed to model the alignments between acoustic features and output tokens in speech recognition. However, attention weights produced by models trained end to end do not always correspond well with actual alignments, and several studies have further argued that attention weights might not even correspond well with the relevance attribution of frames. Regardless, visual similarity between attention weights and alignments is widely used during training as an indicator of the models quality. In this paper, we treat the correspondence between attention weights and alignments as a learning problem by imposing a supervised attention loss. Experiments have shown significant improved performance, suggesting that learning the alignments well during training critically determines the performance of sequence-to-sequence models.

</p>
</details>

<details><summary><b>PP-MARL: Efficient Privacy-Preserving MARL for Cooperative Intelligence in Communication</b>
<a href="https://arxiv.org/abs/2204.12064">arxiv:2204.12064</a>
&#x1F4C8; 2 <br>
<p>Tingting Yuan, Hwei-Ming Chung, Xiaoming Fu</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) has been introduced in communication networks and services to improve efficiency via self-optimization. Cooperative intelligence (CI), also known as collective intelligence and collaborative intelligence, is expected to become an integral element in next-generation networks because it can aggregate the capabilities and intelligence of multiple devices. However, privacy issues may intimidate, obstruct, and hinder the deployment of CI in practice because collaboration heavily relies on data and information sharing. Additional practical constraints in communication (e.g., limited bandwidth) further limit the performance of CI. To overcome these challenges, we propose PP-MARL, an efficient privacy-preserving learning scheme based on multi-agent reinforcement learning (MARL). We apply and evaluate our scheme in two communication-related use cases: mobility management in drone-assisted communication and network control with edge intelligence. Simulation results reveal that the proposed scheme can achieve efficient and reliable collaboration with 1.1-6 times better privacy protection and lower overheads (e.g., 84-91% reduction in bandwidth) than state-of-the-art approaches.

</p>
</details>

<details><summary><b>Information Fusion: Scaling Subspace-Driven Approaches</b>
<a href="https://arxiv.org/abs/2204.12035">arxiv:2204.12035</a>
&#x1F4C8; 2 <br>
<p>Sally Ghanem, Hamid Krim</p></summary>
<p>

**Abstract:** In this work, we seek to exploit the deep structure of multi-modal data to robustly exploit the group subspace distribution of the information using the Convolutional Neural Network (CNN) formalism. Upon unfolding the set of subspaces constituting each data modality, and learning their corresponding encoders, an optimized integration of the generated inherent information is carried out to yield a characterization of various classes. Referred to as deep Multimodal Robust Group Subspace Clustering (DRoGSuRe), this approach is compared against the independently developed state-of-the-art approach named Deep Multimodal Subspace Clustering (DMSC). Experiments on different multimodal datasets show that our approach is competitive and more robust in the presence of noise.

</p>
</details>

<details><summary><b>Discrete-Continuous Smoothing and Mapping</b>
<a href="https://arxiv.org/abs/2204.11936">arxiv:2204.11936</a>
&#x1F4C8; 2 <br>
<p>Kevin J. Doherty, Ziqi Lu, Kurran Singh, John J. Leonard</p></summary>
<p>

**Abstract:** We describe a general approach to smoothing and mapping with a class of discrete-continuous factor graphs commonly encountered in robotics applications. While there are openly available tools providing flexible and easy-to-use interfaces for specifying and solving optimization problems formulated in terms of either discrete or continuous graphical models, at present, no similarly general tools exist enabling the same functionality for hybrid discrete-continuous problems. We aim to address this problem. In particular, we provide a library, DC-SAM, extending existing tools for optimization problems defined in terms of factor graphs to the setting of discrete-continuous models. A key contribution of our work is a novel solver for efficiently recovering approximate solutions to discrete-continuous optimization problems. The key insight to our approach is that while joint inference over continuous and discrete state spaces is often hard, many commonly encountered discrete-continuous problems can naturally be split into a "discrete part" and a "continuous part" that can individually be solved easily. Leveraging this structure, we optimize discrete and continuous variables in an alternating fashion. In consequence, our proposed work enables straightforward representation of and approximate inference in discrete-continuous graphical models. We also provide a method to recover the uncertainty in estimates of both discrete and continuous variables. We demonstrate the versatility of our approach through its application to three distinct robot perception applications: point-cloud registration, robust pose graph optimization, and object-based mapping and localization.

</p>
</details>

<details><summary><b>Real or Virtual: A Video Conferencing Background Manipulation-Detection System</b>
<a href="https://arxiv.org/abs/2204.11853">arxiv:2204.11853</a>
&#x1F4C8; 2 <br>
<p>Ehsan Nowroozi, Yassine Mekdad, Mauro Conti, Simone Milani, Selcuk Uluagac, Berrin Yanikoglu</p></summary>
<p>

**Abstract:** Recently, the popularity and wide use of the last-generation video conferencing technologies created an exponential growth in its market size. Such technology allows participants in different geographic regions to have a virtual face-to-face meeting. Additionally, it enables users to employ a virtual background to conceal their own environment due to privacy concerns or to reduce distractions, particularly in professional settings. Nevertheless, in scenarios where the users should not hide their actual locations, they may mislead other participants by claiming their virtual background as a real one. Therefore, it is crucial to develop tools and strategies to detect the authenticity of the considered virtual background. In this paper, we present a detection strategy to distinguish between real and virtual video conferencing user backgrounds. We demonstrate that our detector is robust against two attack scenarios. The first scenario considers the case where the detector is unaware about the attacks and inn the second scenario, we make the detector aware of the adversarial attacks, which we refer to Adversarial Multimedia Forensics (i.e, the forensically-edited frames are included in the training set). Given the lack of publicly available dataset of virtual and real backgrounds for video conferencing, we created our own dataset and made them publicly available [1]. Then, we demonstrate the robustness of our detector against different adversarial attacks that the adversary considers. Ultimately, our detector's performance is significant against the CRSPAM1372 [2] features, and post-processing operations such as geometric transformations with different quality factors that the attacker may choose. Moreover, our performance results shows that we can perfectly identify a real from a virtual background with an accuracy of 99.80%.

</p>
</details>

<details><summary><b>Blind Equalization and Channel Estimation in Coherent Optical Communications Using Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2204.11776">arxiv:2204.11776</a>
&#x1F4C8; 2 <br>
<p>Vincent Lauinger, Fred Buchali, Laurent Schmalen</p></summary>
<p>

**Abstract:** We investigate the potential of adaptive blind equalizers based on variational inference for carrier recovery in optical communications. These equalizers are based on a low-complexity approximation of maximum likelihood channel estimation. We generalize the concept of variational autoencoder (VAE) equalizers to higher order modulation formats encompassing probabilistic constellation shaping (PCS), ubiquitous in optical communications, oversampling at the receiver, and dual-polarization transmission. Besides black-box equalizers based on convolutional neural networks, we propose a model-based equalizer based on a linear butterfly filter and train the filter coefficients using the variational inference paradigm. As a byproduct, the VAE also provides a reliable channel estimation. We analyze the VAE in terms of performance and flexibility over a classical additive white Gaussian noise (AWGN) channel with inter-symbol interference (ISI) and over a dispersive linear optical dual-polarization channel. We show that it can extend the application range of blind adaptive equalizers by outperforming the state-of-the-art constant-modulus algorithm (CMA) for PCS for both fixed but also time-varying channels. The evaluation is accompanied with a hyperparameter analysis.

</p>
</details>

<details><summary><b>A feasibility study proposal of the predictive model to enable the prediction of population susceptibility to COVID-19 by analysis of vaccine utilization for advising deployment of a booster dose</b>
<a href="https://arxiv.org/abs/2204.11747">arxiv:2204.11747</a>
&#x1F4C8; 2 <br>
<p>Chottiwatt Jittprasong</p></summary>
<p>

**Abstract:** With the present highly infectious dominant SARS-CoV-2 strain of B1.1.529 or Omicron spreading around the globe, there is concern that the COVID-19 pandemic will not end soon and that it will be a race against time until a more contagious and virulent variant emerges. One of the most promising approaches for preventing virus propagation is to maintain continuous high vaccination efficacy among the population, thereby strengthening the population protective effect and preventing the majority of infection in the vaccinated population, as is known to occur with the Omicron variant frequently. Countries must structure vaccination programs in accordance with their populations' susceptibility to infection, optimizing vaccination efforts by delivering vaccines progressively enough to protect the majority of the population. We present a feasibility study proposal for maintaining optimal continuous vaccination by assessing the susceptible population, the decline of vaccine efficacy in the population, and advising booster dosage deployment to maintain the population's protective efficacy through the use of a predictive model. Numerous studies have been conducted in the direction of analyzing vaccine utilization; however, very little study has been conducted to substantiate the optimal deployment of booster dosage vaccination with the help of a predictive model based on machine learning algorithms.

</p>
</details>

<details><summary><b>Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking</b>
<a href="https://arxiv.org/abs/2204.11673">arxiv:2204.11673</a>
&#x1F4C8; 2 <br>
<p>Qian Dong, Yiding Liu, Suqi Cheng, Shuaiqiang Wang, Zhicong Cheng, Shuzi Niu, Dawei Yin</p></summary>
<p>

**Abstract:** Passage re-ranking is to obtain a permutation over the candidate passage set from retrieval stage. Re-rankers have been boomed by Pre-trained Language Models (PLMs) due to their overwhelming advantages in natural language understanding. However, existing PLM based re-rankers may easily suffer from vocabulary mismatch and lack of domain specific knowledge. To alleviate these problems, explicit knowledge contained in knowledge graph is carefully introduced in our work. Specifically, we employ the existing knowledge graph which is incomplete and noisy, and first apply it in passage re-ranking task. To leverage a reliable knowledge, we propose a novel knowledge graph distillation method and obtain a knowledge meta graph as the bridge between query and passage. To align both kinds of embedding in the latent space, we employ PLM as text encoder and graph neural network over knowledge meta graph as knowledge encoder. Besides, a novel knowledge injector is designed for the dynamic interaction between text and knowledge encoder. Experimental results demonstrate the effectiveness of our method especially in queries requiring in-depth domain knowledge.

</p>
</details>

<details><summary><b>A global analysis of metrics used for measuring performance in natural language processing</b>
<a href="https://arxiv.org/abs/2204.11574">arxiv:2204.11574</a>
&#x1F4C8; 2 <br>
<p>Kathrin Blagec, Georg Dorffner, Milad Moradi, Simon Ott, Matthias Samwald</p></summary>
<p>

**Abstract:** Measuring the performance of natural language processing models is challenging. Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages. In the past 15 years, a wide range of alternative metrics have been proposed. However, it is unclear to what extent this has had an impact on NLP benchmarking efforts. Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository 'Papers with Code' to enable a global and comprehensive analysis. Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models' performance. Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.

</p>
</details>

<details><summary><b>Determinantal Point Process Likelihoods for Sequential Recommendation</b>
<a href="https://arxiv.org/abs/2204.11562">arxiv:2204.11562</a>
&#x1F4C8; 2 <br>
<p>Yuli Liu, Christian Walder, Lexing Xie</p></summary>
<p>

**Abstract:** Sequential recommendation is a popular task in academic research and close to real-world application scenarios, where the goal is to predict the next action(s) of the user based on his/her previous sequence of actions. In the training process of recommender systems, the loss function plays an essential role in guiding the optimization of recommendation models to generate accurate suggestions for users. However, most existing sequential recommendation techniques focus on designing algorithms or neural network architectures, and few efforts have been made to tailor loss functions that fit naturally into the practical application scenario of sequential recommender systems.
  Ranking-based losses, such as cross-entropy and Bayesian Personalized Ranking (BPR) are widely used in the sequential recommendation area. We argue that such objective functions suffer from two inherent drawbacks: i) the dependencies among elements of a sequence are overlooked in these loss formulations; ii) instead of balancing accuracy (quality) and diversity, only generating accurate results has been over emphasized. We therefore propose two new loss functions based on the Determinantal Point Process (DPP) likelihood, that can be adaptively applied to estimate the subsequent item or items. The DPP-distributed item set captures natural dependencies among temporal actions, and a quality vs. diversity decomposition of the DPP kernel pushes us to go beyond accuracy-oriented loss functions. Experimental results using the proposed loss functions on three real-world datasets show marked improvements over state-of-the-art sequential recommendation methods in both quality and diversity metrics.

</p>
</details>

<details><summary><b>Multimodal Dual Emotion with Fusion of Visual Sentiment for Rumor Detection</b>
<a href="https://arxiv.org/abs/2204.11515">arxiv:2204.11515</a>
&#x1F4C8; 2 <br>
<p>Ge Wang, Li Tan, Ziliang Shang, He Liu</p></summary>
<p>

**Abstract:** In recent years, rumors have had a devastating impact on society, making rumor detection a significant challenge. However, the studies on rumor detection ignore the intense emotions of images in the rumor content. This paper verifies that the image emotion improves the rumor detection efficiency. A Multimodal Dual Emotion feature in rumor detection, which consists of visual and textual emotions, is proposed. To the best of our knowledge, this is the first study which uses visual emotion in rumor detection. The experiments on real datasets verify that the proposed features outperform the state-of-the-art sentiment features, and can be extended in rumor detectors while improving their performance.

</p>
</details>

<details><summary><b>Quantifying Unknown Quantum Entanglement via a Hybrid Quantum-Classical Machine Learning Framework</b>
<a href="https://arxiv.org/abs/2204.11500">arxiv:2204.11500</a>
&#x1F4C8; 2 <br>
<p>Xiaodie Lin, Zhenyu Chen, Zhaohui Wei</p></summary>
<p>

**Abstract:** Quantifying unknown quantum entanglement experimentally is a difficult task, but also becomes more and more necessary because of the fast development of quantum engineering. Machine learning provides practical solutions to this fundamental problem, where one has to train a proper machine learning model to predict entanglement measures of unknown quantum states based on experimentally measurable data, say moments or correlation data produced by local measurements. In this paper, we compare the performance of these two different machine learning approaches systematically. Particularly, we first show that the approach based on moments enjoys a remarkable advantage over that based on correlation data, though the cost of measuring moments is much higher. Next, since correlation data is much easier to obtain experimentally, we try to better its performance by proposing a hybrid quantum-classical machine learning framework for this problem, where the key is to train optimal local measurements to generate more informative correlation data. Our numerical simulations show that the new framework brings us comparable performance with the approach based on moments to quantify unknown entanglement. Our work implies that it is already practical to fulfill such tasks on near-term quantum devices.

</p>
</details>

<details><summary><b>Self-supervision versus synthetic datasets: which is the lesser evil in the context of video denoising?</b>
<a href="https://arxiv.org/abs/2204.11493">arxiv:2204.11493</a>
&#x1F4C8; 2 <br>
<p>Valéry Dewil, Aranud Barral, Gabriele Facciolo, Pablo Arias</p></summary>
<p>

**Abstract:** Supervised training has led to state-of-the-art results in image and video denoising. However, its application to real data is limited since it requires large datasets of noisy-clean pairs that are difficult to obtain. For this reason, networks are often trained on realistic synthetic data. More recently, some self-supervised frameworks have been proposed for training such denoising networks directly on the noisy data without requiring ground truth. On synthetic denoising problems supervised training outperforms self-supervised approaches, however in recent years the gap has become narrower, especially for video. In this paper, we propose a study aiming to determine which is the best approach to train denoising networks for real raw videos: supervision on synthetic realistic data or self-supervision on real data. A complete study with quantitative results in case of natural videos with real motion is impossible since no dataset with clean-noisy pairs exists. We address this issue by considering three independent experiments in which we compare the two frameworks. We found that self-supervision on the real data outperforms supervision on synthetic data, and that in normal illumination conditions the drop in performance is due to the synthetic ground truth generation, not the noise model.

</p>
</details>

<details><summary><b>AQuaMoHo: Localized Low-Cost Outdoor Air Quality Sensing over a Thermo-Hygrometer</b>
<a href="https://arxiv.org/abs/2204.11484">arxiv:2204.11484</a>
&#x1F4C8; 2 <br>
<p>Prithviraj Pramanik, Prasenjit Karmakar, Praveen Kumar Sharma, Soumyajit Chatterjee, Subrata Nandi, Sandip Chakraborty, Mousumi Saha, Sujoy Saha</p></summary>
<p>

**Abstract:** Efficient air quality sensing serves as one of the essential services provided in any recent smart city. Mostly facilitated by sparsely deployed Air Quality Monitoring Stations (AQMSs) that are difficult to install and maintain, the overall spatial variation heavily impacts air quality monitoring for locations far enough from these pre-deployed public infrastructures. To mitigate this, we in this paper propose a framework named AQuaMoHo that can annotate data obtained from a low-cost thermo-hygrometer (as the sole physical sensing device) with the AQI labels, with the help of additional publicly crawled Spatio-temporal information of that locality. At its core, AQuaMoHo exploits the temporal patterns from a set of readily available spatial features using an LSTM-based model and further enhances the overall quality of the annotation using temporal attention. From a thorough study of two different cities, we observe that AQuaMoHo can significantly help annotate the air quality data on a personal scale.

</p>
</details>

<details><summary><b>"Think Before You Speak": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs</b>
<a href="https://arxiv.org/abs/2204.11481">arxiv:2204.11481</a>
&#x1F4C8; 2 <br>
<p>Shuo Zhang, Junzhou Zhao, Pinghui Wang, Yu Li, Yi Huang, Junlan Feng</p></summary>
<p>

**Abstract:** Multi-action dialog policy (MADP), which generates multiple atomic dialog actions per turn, has been widely applied in task-oriented dialog systems to provide expressive and efficient system responses. Existing MADP models usually imitate action combinations from the labeled multi-action dialog samples. Due to data limitations, they generalize poorly toward unseen dialog flows. While interactive learning and reinforcement learning algorithms can be applied to incorporate external data sources of real users and user simulators, they take significant manual effort to build and suffer from instability. To address these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel multi-task learning framework that learns single-action dialog dynamics to enhance multi-action prediction. Our PEDP method employs model-based planning for conceiving what to express before deciding the current response through simulating single-action dialogs. Experimental results on the MultiWOZ dataset demonstrate that our fully supervised learning-based method achieves a solid task success rate of 90.6%, improving 3% compared to the state-of-the-art methods.

</p>
</details>

<details><summary><b>Deep Reinforcement Learning for Online Routing of Unmanned Aerial Vehicles with Wireless Power Transfer</b>
<a href="https://arxiv.org/abs/2204.11477">arxiv:2204.11477</a>
&#x1F4C8; 2 <br>
<p>Kaiwen Li, Tao Zhang, Rui Wang, Ling Wang</p></summary>
<p>

**Abstract:** The unmanned aerial vehicle (UAV) plays an vital role in various applications such as delivery, military mission, disaster rescue, communication, etc., due to its flexibility and versatility. This paper proposes a deep reinforcement learning method to solve the UAV online routing problem with wireless power transfer, which can charge the UAV remotely without wires, thus extending the capability of the battery-limited UAV. Our study considers the power consumption of the UAV and the wireless charging process. Unlike the previous works, we solve the problem by a designed deep neural network. The model is trained using a deep reinforcement learning method offline, and is used to optimize the UAV routing problem online. On small and large scale instances, the proposed model runs from four times to 500 times faster than Google OR-tools, the state-of-the-art combinatorial optimization solver, with identical solution quality. It also outperforms different types of heuristic and local search methods in terms of both run-time and optimality. In addition, once the model is trained, it can scale to new generated problem instances with arbitrary topology that are not seen during training. The proposed method is practically applicable when the problem scale is large and the response time is crucial.

</p>
</details>

<details><summary><b>High-Efficiency Lossy Image Coding Through Adaptive Neighborhood Information Aggregation</b>
<a href="https://arxiv.org/abs/2204.11448">arxiv:2204.11448</a>
&#x1F4C8; 2 <br>
<p>Ming Lu, Zhan Ma</p></summary>
<p>

**Abstract:** Questing for lossy image coding (LIC) with superior efficiency on both compression performance and computation throughput is challenging. The vital factor behind is how to intelligently explore Adaptive Neighborhood Information Aggregation (ANIA) in transform and entropy coding modules. To this aim, Integrated Convolution and Self-Attention (ICSA) unit is first proposed to form content-adaptive transform to dynamically characterize and embed neighborhood information conditioned on the input. Then a Multistage Context Model (MCM) is developed to stagewisely execute context prediction using necessary neighborhood elements for accurate and parallel entropy probability estimation. Both ICSA and MCM are stacked under a Variational Auto-Encoder (VAE) architecture to derive rate-distortion optimized compact representation of input image via end-to-end training. Our method reports the superior compression performance surpassing the VVC Intra with $\approx$15% BD-rate improvement averaged across Kodak, CLIC and Tecnick datasets; and also demonstrates $\approx$10$\times$ speedup of image decoding when compared with other notable learned LIC approaches. All materials are made publicly accessible at https://njuvision.github.io/TinyLIC for reproducible research.

</p>
</details>

<details><summary><b>Topological Data Analysis for Anomaly Detection in Host-Based Logs</b>
<a href="https://arxiv.org/abs/2204.12919">arxiv:2204.12919</a>
&#x1F4C8; 1 <br>
<p>Thomas Davies</p></summary>
<p>

**Abstract:** Topological Data Analysis (TDA) gives practioners the ability to analyse the global structure of cybersecurity data. We use TDA for anomaly detection in host-based logs collected with the open-source Logging Made Easy (LME) project. We present an approach that builds a filtration of simplicial complexes directly from Windows logs, enabling analysis of their intrinsic structure using topological tools. We compare the efficacy of persistent homology and the spectrum of graph and hypergraph Laplacians as feature vectors against a standard log embedding that counts events, and find that topological and spectral embeddings of computer logs contain discriminative information for classifying anomalous logs that is complementary to standard embeddings. We end by discussing the potential for our methods to be used as part of an explainable framework for anomaly detection.

</p>
</details>

<details><summary><b>AI-Assisted Authentication: State of the Art, Taxonomy and Future Roadmap</b>
<a href="https://arxiv.org/abs/2204.12492">arxiv:2204.12492</a>
&#x1F4C8; 1 <br>
<p>Guangyi Zhu, Yasir Al-Qaraghuli</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) has found its applications in a variety of environments ranging from data science to cybersecurity. AI helps break through the limitations of traditional algorithms and provides more efficient and flexible methods for solving problems. In this paper, we focus on the applications of artificial intelligence in authentication, which is used in a wide range of scenarios including facial recognition to access buildings, keystroke dynamics to unlock smartphones. With the emerging AI-assisted authentication schemes, our comprehensive survey provides an overall understanding on a high level, which paves the way for future research in this area. In contrast to other relevant surveys, our research is the first of its kind to focus on the roles of AI in authentication.

</p>
</details>

<details><summary><b>On Machine Learning-Driven Surrogates for Sound Transmission Loss Simulations</b>
<a href="https://arxiv.org/abs/2204.12290">arxiv:2204.12290</a>
&#x1F4C8; 1 <br>
<p>Barbara Cunha, Abdel-Malek Zine, Mohamed Ichchou, Christophe Droz, Stéphane Foulard</p></summary>
<p>

**Abstract:** Surrogate models are data-based approximations of computationally expensive simulations that enable efficient exploration of the model's design space and informed decision-making in many physical domains. The usage of surrogate models in the vibroacoustic domain, however, is challenging due to the non-smooth, complex behavior of wave phenomena. This paper investigates four Machine Learning (ML) approaches in the modelling of surrogates of Sound Transmission Loss (STL). Feature importance and feature engineering are used to improve the models' accuracy while increasing their interpretability and physical consistency. The transfer of the proposed techniques to other problems in the vibroacoustic domain and possible limitations of the models are discussed.

</p>
</details>

<details><summary><b>Low-dimensional representation of infant and adult vocalization acoustics</b>
<a href="https://arxiv.org/abs/2204.12279">arxiv:2204.12279</a>
&#x1F4C8; 1 <br>
<p>Silvia Pagliarini, Sara Schneider, Christopher T. Kello, Anne S. Warlaumont</p></summary>
<p>

**Abstract:** During the first years of life, infant vocalizations change considerably, as infants develop the vocalization skills that enable them to produce speech sounds. Characterizations based on specific acoustic features, protophone categories, or phonetic transcription are able to provide a representation of the sounds infants make at different ages and in different contexts but do not fully describe how sounds are perceived by listeners, can be inefficient to obtain at large scales, and are difficult to visualize in two dimensions without additional statistical processing. Machine-learning-based approaches provide the opportunity to complement these characterizations with purely data-driven representations of infant sounds. Here, we use spectral features extraction and unsupervised machine learning, specifically Uniform Manifold Approximation (UMAP), to obtain a novel 2-dimensional spatial representation of infant and caregiver vocalizations extracted from day-long home recordings. UMAP yields a continuous and well-distributed space conducive to certain analyses of infant vocal development. For instance, we found that the dispersion of infant vocalization acoustics within the 2-D space over a day increased from 3 to 9 months, and then decreased from 9 to 18 months. The method also permits analysis of similarity between infant and adult vocalizations, which also shows changes with infant age.

</p>
</details>

<details><summary><b>ISTRBoost: Importance Sampling Transfer Regression using Boosting</b>
<a href="https://arxiv.org/abs/2204.12044">arxiv:2204.12044</a>
&#x1F4C8; 1 <br>
<p>Shrey Gupta, Jianzhao Bi, Yang Liu, Avani Wildani</p></summary>
<p>

**Abstract:** Current Instance Transfer Learning (ITL) methodologies use domain adaptation and sub-space transformation to achieve successful transfer learning. However, these methodologies, in their processes, sometimes overfit on the target dataset or suffer from negative transfer if the test dataset has a high variance. Boosting methodologies have been shown to reduce the risk of overfitting by iteratively re-weighing instances with high-residual. However, this balance is usually achieved with parameter optimization, as well as reducing the skewness in weights produced due to the size of the source dataset. While the former can be achieved, the latter is more challenging and can lead to negative transfer. We introduce a simpler and more robust fix to this problem by building upon the popular boosting ITL regression methodology, two-stage TrAdaBoost.R2. Our methodology,~\us{}, is a boosting and random-forest based ensemble methodology that utilizes importance sampling to reduce the skewness due to the source dataset. We show that~\us{}~performs better than competitive transfer learning methodologies $63\%$ of the time. It also displays consistency in its performance over diverse datasets with varying complexities, as opposed to the sporadic results observed for other transfer learning methodologies.

</p>
</details>

<details><summary><b>Adaptive Pseudo-Siamese Policy Network for Temporal Knowledge Prediction</b>
<a href="https://arxiv.org/abs/2204.12036">arxiv:2204.12036</a>
&#x1F4C8; 1 <br>
<p>Pengpeng Shao, Tong Liu, Feihu Che, Dawei Zhang, Jianhua Tao</p></summary>
<p>

**Abstract:** Temporal knowledge prediction is a crucial task for the event early warning that has gained increasing attention in recent years, which aims to predict the future facts by using relevant historical facts on the temporal knowledge graphs. There are two main difficulties in this prediction task. First, from the historical facts point of view, how to model the evolutionary patterns of the facts to predict the query accurately. Second, from the query perspective, how to handle the two cases where the query contains seen and unseen entities in a unified framework. Driven by the two problems, we propose a novel adaptive pseudo-siamese policy network for temporal knowledge prediction based on reinforcement learning. Specifically, we design the policy network in our model as a pseudo-siamese policy network that consists of two sub-policy networks. In sub-policy network I, the agent searches for the answer for the query along the entity-relation paths to capture the static evolutionary patterns. And in sub-policy network II, the agent searches for the answer for the query along the relation-time paths to deal with unseen entities. Moreover, we develop a temporal relation encoder to capture the temporal evolutionary patterns. Finally, we design a gating mechanism to adaptively integrate the results of the two sub-policy networks to help the agent focus on the destination answer. To assess our model performance, we conduct link prediction on four benchmark datasets, the experimental results demonstrate that our method obtains considerable performance compared with existing methods.

</p>
</details>

<details><summary><b>Estimating the Resize Parameter in End-to-end Learned Image Compression</b>
<a href="https://arxiv.org/abs/2204.12022">arxiv:2204.12022</a>
&#x1F4C8; 1 <br>
<p>Li-Heng Chen, Christos G. Bampis, Zhi Li, Lukáš Krasula, Alan C. Bovik</p></summary>
<p>

**Abstract:** We describe a search-free resizing framework that can further improve the rate-distortion tradeoff of recent learned image compression models. Our approach is simple: compose a pair of differentiable downsampling/upsampling layers that sandwich a neural compression model. To determine resize factors for different inputs, we utilize another neural network jointly trained with the compression model, with the end goal of minimizing the rate-distortion objective. Our results suggest that "compression friendly" downsampled representations can be quickly determined during encoding by using an auxiliary network and differentiable image warping. By conducting extensive experimental tests on existing deep image compression models, we show results that our new resizing parameter estimation framework can provide Bjøntegaard-Delta rate (BD-rate) improvement of about 10% against leading perceptual quality engines. We also carried out a subjective quality study, the results of which show that our new approach yields favorable compressed images. To facilitate reproducible research in this direction, the implementation used in this paper is being made freely available online at: https://github.com/treammm/ResizeCompression.

</p>
</details>

<details><summary><b>Robust Dual-Graph Regularized Moving Object Detection</b>
<a href="https://arxiv.org/abs/2204.11939">arxiv:2204.11939</a>
&#x1F4C8; 1 <br>
<p>Jing Qin, Ruilong Shen, Ruihan Zhu, Biyun Xie</p></summary>
<p>

**Abstract:** Moving object detection and its associated background-foreground separation have been widely used in a lot of applications, including computer vision, transportation and surveillance. Due to the presence of the static background, a video can be naturally decomposed into a low-rank background and a sparse foreground. Many regularization techniques, such as matrix nuclear norm, have been imposed on the background. In the meanwhile, sparsity or smoothness based regularizations, such as total variation and $\ell_1$, can be imposed on the foreground. Moreover, graph Laplacians are further imposed to capture the complicated geometry of background images. Recently, weighted regularization techniques including the weighted nuclear norm regularization have been proposed in the image processing community to promote adaptive sparsity while achieving efficient performance. In this paper, we propose a robust dual-graph regularized moving object detection model based on the weighted nuclear norm regularization, which is solved by the alternating direction method of multipliers (ADMM). Numerical experiments on body movement data sets have demonstrated the effectiveness of this method in separating moving objects from background, and the great potential in robotic applications.

</p>
</details>

<details><summary><b>Learning High-Dimensional McKean-Vlasov Forward-Backward Stochastic Differential Equations with General Distribution Dependence</b>
<a href="https://arxiv.org/abs/2204.11924">arxiv:2204.11924</a>
&#x1F4C8; 1 <br>
<p>Jiequn Han, Ruimeng Hu, Jihao Long</p></summary>
<p>

**Abstract:** One of the core problems in mean-field control and mean-field games is to solve the corresponding McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs). Most existing methods are tailored to special cases in which the mean-field interaction only depends on expectation or other moments and thus inadequate to solve problems when the mean-field interaction has full distribution dependence. In this paper, we propose a novel deep learning method for computing MV-FBSDEs with a general form of mean-field interactions. Specifically, built on fictitious play, we recast the problem into repeatedly solving standard FBSDEs with explicit coefficient functions. These coefficient functions are used to approximate the MV-FBSDEs' model coefficients with full distribution dependence, and are updated by solving another supervising learning problem using training data simulated from the last iteration's FBSDE solutions. We use deep neural networks to solve standard BSDEs and approximate coefficient functions in order to solve high-dimensional MV-FBSDEs. Under proper assumptions on the learned functions, we prove that the convergence of the proposed method is free of the curse of dimensionality (CoD) by using the generalized maximum mean discrepancy metric previously developed in [Han, Hu and Long, arXiv:2104.12036]. The proved theorem shows the advantage of the method in high dimensions. We present the numerical performance in high-dimensional MV-FBSDE problems, including a mean-field game example of the well-known Cucker-Smale model whose cost depends on the full distribution of the forward process.

</p>
</details>

<details><summary><b>ProCST: Boosting Semantic Segmentation using Progressive Cyclic Style-Transfer</b>
<a href="https://arxiv.org/abs/2204.11891">arxiv:2204.11891</a>
&#x1F4C8; 1 <br>
<p>Shahaf Ettedgui, Shady Abu-Hussein, Raja Giryes</p></summary>
<p>

**Abstract:** Using synthetic data for training neural networks that achieve good performance on real-world data is an important task as it has the potential to reduce the need for costly data annotation. Yet, a network that is trained on synthetic data alone does not perform well on real data due to the domain gap between the two. Reducing this gap, also known as domain adaptation, has been widely studied in recent years. In the unsupervised domain adaptation (UDA) framework, unlabeled real data is used during training with labeled synthetic data to obtain a neural network that performs well on real data. In this work, we focus on image data. For the semantic segmentation task, it has been shown that performing image-to-image translation from source to target, and then training a network for segmentation on source annotations - leads to poor results. Therefore a joint training of both is essential, which has been a common practice in many techniques. Yet, closing the large domain gap between the source and the target by directly performing the adaptation between the two is challenging. In this work, we propose a novel two-stage framework for improving domain adaptation techniques. In the first step, we progressively train a multi-scale neural network to perform an initial transfer between the source data to the target data. We denote the new transformed data as "Source in Target" (SiT). Then, we use the generated SiT data as the input to any standard UDA approach. This new data has a reduced domain gap from the desired target domain, and the applied UDA approach further closes the gap. We demonstrate the improvement achieved by our framework with two state-of-the-art methods for semantic segmentation, DAFormer and ProDA, on two UDA tasks, GTA5 to Cityscapes and Synthia to Cityscapes. Code and state-of-the-art checkpoints of ProCST+DAFormer are provided.

</p>
</details>

<details><summary><b>Evolutionary latent space search for driving human portrait generation</b>
<a href="https://arxiv.org/abs/2204.11887">arxiv:2204.11887</a>
&#x1F4C8; 1 <br>
<p>Benjamín Machín, Sergio Nesmachnow, Jamal Toutouh</p></summary>
<p>

**Abstract:** This article presents an evolutionary approach for synthetic human portraits generation based on the latent space exploration of a generative adversarial network. The idea is to produce different human face images very similar to a given target portrait. The approach applies StyleGAN2 for portrait generation and FaceNet for face similarity evaluation. The evolutionary search is based on exploring the real-coded latent space of StyleGAN2. The main results over both synthetic and real images indicate that the proposed approach generates accurate and diverse solutions, which represent realistic human portraits. The proposed research can contribute to improving the security of face recognition systems.

</p>
</details>

<details><summary><b>Multi-scale reconstruction of undersampled spectral-spatial OCT data for coronary imaging using deep learning</b>
<a href="https://arxiv.org/abs/2204.11769">arxiv:2204.11769</a>
&#x1F4C8; 1 <br>
<p>Xueshen Li, Shengting Cao, Hongshan Liu, Xinwen Yao, Brigitta C. Brott, Silvio H. Litovsky, Xiaoyu Song, Yuye Ling, Yu Gan</p></summary>
<p>

**Abstract:** Coronary artery disease (CAD) is a cardiovascular condition with high morbidity and mortality. Intravascular optical coherence tomography (IVOCT) has been considered as an optimal imagining system for the diagnosis and treatment of CAD. Constrained by Nyquist theorem, dense sampling in IVOCT attains high resolving power to delineate cellular structures/ features. There is a trade-off between high spatial resolution and fast scanning rate for coronary imaging. In this paper, we propose a viable spectral-spatial acquisition method that down-scales the sampling process in both spectral and spatial domain while maintaining high quality in image reconstruction. The down-scaling schedule boosts data acquisition speed without any hardware modifications. Additionally, we propose a unified multi-scale reconstruction framework, namely Multiscale- Spectral-Spatial-Magnification Network (MSSMN), to resolve highly down-scaled (compressed) OCT images with flexible magnification factors. We incorporate the proposed methods into Spectral Domain OCT (SD-OCT) imaging of human coronary samples with clinical features such as stent and calcified lesions. Our experimental results demonstrate that spectral-spatial downscaled data can be better reconstructed than data that is downscaled solely in either spectral or spatial domain. Moreover, we observe better reconstruction performance using MSSMN than using existing reconstruction methods. Our acquisition method and multi-scale reconstruction framework, in combination, may allow faster SD-OCT inspection with high resolution during coronary intervention.

</p>
</details>

<details><summary><b>Deep-learning-enabled Brain Hemodynamic Mapping Using Resting-state fMRI</b>
<a href="https://arxiv.org/abs/2204.11669">arxiv:2204.11669</a>
&#x1F4C8; 1 <br>
<p>Xirui Hou, Pengfei Guo, Puyang Wang, Peiying Liu, Doris D. M. Lin, Hongli Fan, Yang Li, Zhiliang Wei, Zixuan Lin, Dengrong Jiang, Jin Jin, Catherine Kelly, Jay J. Pillai, Judy Huang, Marco C. Pinho, Binu P. Thomas, Babu G. Welch, Denise C. Park, Vishal M. Patel, Argye E. Hillis, Hanzhang Lu</p></summary>
<p>

**Abstract:** Cerebrovascular disease is a leading cause of death globally. Prevention and early intervention are known to be the most effective forms of its management. Non-invasive imaging methods hold great promises for early stratification, but at present lack the sensitivity for personalized prognosis. Resting-state functional magnetic resonance imaging (rs-fMRI), a powerful tool previously used for mapping neural activity, is available in most hospitals. Here we show that rs-fMRI can be used to map cerebral hemodynamic function and delineate impairment. By exploiting time variations in breathing pattern during rs-fMRI, deep learning enables reproducible mapping of cerebrovascular reactivity (CVR) and bolus arrive time (BAT) of the human brain using resting-state CO2 fluctuations as a natural 'contrast media'. The deep-learning network was trained with CVR and BAT maps obtained with a reference method of CO2-inhalation MRI, which included data from young and older healthy subjects and patients with Moyamoya disease and brain tumors. We demonstrate the performance of deep-learning cerebrovascular mapping in the detection of vascular abnormalities, evaluation of revascularization effects, and vascular alterations in normal aging. In addition, cerebrovascular maps obtained with the proposed method exhibited excellent reproducibility in both healthy volunteers and stroke patients. Deep-learning resting-state vascular imaging has the potential to become a useful tool in clinical cerebrovascular imaging.

</p>
</details>

<details><summary><b>FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server</b>
<a href="https://arxiv.org/abs/2204.11536">arxiv:2204.11536</a>
&#x1F4C8; 1 <br>
<p>Hong Zhang, Ji Liu, Juncheng Jia, Yang Zhou, Huaiyu Dai, Dejing Dou</p></summary>
<p>

**Abstract:** Despite achieving remarkable performance, Federated Learning (FL) suffers from two critical challenges, i.e., limited computational resources and low training efficiency. In this paper, we propose a novel FL framework, i.e., FedDUAP, with two original contributions, to exploit the insensitive data on the server and the decentralized data in edge devices to further improve the training efficiency. First, a dynamic server update algorithm is designed to exploit the insensitive data on the server, in order to dynamically determine the optimal steps of the server update for improving the convergence and accuracy of the global model. Second, a layer-adaptive model pruning method is developed to perform unique pruning operations adapted to the different dimensions and importance of multiple layers, to achieve a good balance between efficiency and effectiveness. By integrating the two original techniques together, our proposed FL model, FedDUAP, significantly outperforms baseline approaches in terms of accuracy (up to 4.8% higher), efficiency (up to 2.8 times faster), and computational cost (up to 61.9% smaller).

</p>
</details>

<details><summary><b>Information Retrieval in Friction Stir Welding of Aluminum Alloys by using Natural Language Processing based Algorithms</b>
<a href="https://arxiv.org/abs/2204.12309">arxiv:2204.12309</a>
&#x1F4C8; 0 <br>
<p>Akshansh Mishra</p></summary>
<p>

**Abstract:** Text summarization is a technique for condensing a big piece of text into a few key elements that give a general impression of the content. When someone requires a quick and precise summary of a large amount of information, it becomes vital. If done manually, summarizing text can be costly and time-consuming. Natural Language Processing (NLP) is the sub-division of Artificial Intelligence that narrows down the gap between technology and human cognition by extracting the relevant information from the pile of data. In the present work, scientific information regarding the Friction Stir Welding of Aluminum alloys was collected from the abstract of scholarly research papers. For extracting the relevant information from these research abstracts four Natural Language Processing based algorithms i.e. Latent Semantic Analysis (LSA), Luhn Algorithm, Lex Rank Algorithm, and KL-Algorithm were used. In order to evaluate the accuracy score of these algorithms, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) was used. The results showed that the Luhn Algorithm resulted in the highest f1-Score of 0.413 in comparison to other algorithms.

</p>
</details>

<details><summary><b>One-pass additive-error subset selection for $\ell_{p}$ subspace approximation</b>
<a href="https://arxiv.org/abs/2204.12073">arxiv:2204.12073</a>
&#x1F4C8; 0 <br>
<p>Amit Deshpande, Rameshwar Pratap</p></summary>
<p>

**Abstract:** We consider the problem of subset selection for $\ell_{p}$ subspace approximation, that is, to efficiently find a \emph{small} subset of data points such that solving the problem optimally for this subset gives a good approximation to solving the problem optimally for the original input. Previously known subset selection algorithms based on volume sampling and adaptive sampling \cite{DeshpandeV07}, for the general case of $p \in [1, \infty)$, require multiple passes over the data. In this paper, we give a one-pass subset selection with an additive approximation guarantee for $\ell_{p}$ subspace approximation, for any $p \in [1, \infty)$. Earlier subset selection algorithms that give a one-pass multiplicative $(1+ε)$ approximation work under the special cases. Cohen \textit{et al.} \cite{CohenMM17} gives a one-pass subset section that offers multiplicative $(1+ε)$ approximation guarantee for the special case of $\ell_{2}$ subspace approximation. Mahabadi \textit{et al.} \cite{MahabadiRWZ20} gives a one-pass \emph{noisy} subset selection with $(1+ε)$ approximation guarantee for $\ell_{p}$ subspace approximation when $p \in \{1, 2\}$. Our subset selection algorithm gives a weaker, additive approximation guarantee, but it works for any $p \in [1, \infty)$.

</p>
</details>

<details><summary><b>Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study</b>
<a href="https://arxiv.org/abs/2204.12037">arxiv:2204.12037</a>
&#x1F4C8; 0 <br>
<p>Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin</p></summary>
<p>

**Abstract:** Spatial-temporal representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the existing visual methods rely heavily on large-scale data annotations and supervised learning to learn a powerful big model. However, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the bottleneck problems of these models, which hinders the progress of interpretable and reliable artificial intelligence. The majority of the existing methods are based on correlation learning with the assumption that the data are independent and identically distributed, which lack an unified guidance and analysis about why modern spatial-temporal representation learning methods have limited interpretability and easily collapse into dataset bias. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great effort in developing causal reasoning paradigms to realize robust representation and model learning with good interpretability. In this paper, we conduct a comprehensive review of existing causal reasoning methods for spatial-temporal representation learning, covering fundamental theories, models, and datasets. The limitations of current methods and datasets are also discussed. Moreover, we propose some primary challenges, opportunities, and future research directions for benchmarking causal reasoning algorithms in spatial-temporal representation learning.

</p>
</details>

<details><summary><b>Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs</b>
<a href="https://arxiv.org/abs/2204.12013">arxiv:2204.12013</a>
&#x1F4C8; 0 <br>
<p>John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang, Ravi Netravali, Guoqing Harry Xu</p></summary>
<p>

**Abstract:** DNN models across many domains continue to grow in size, resulting in high resource requirements for effective training, and unpalatable (and often unaffordable) costs for organizations and research labs across scales. This paper aims to significantly reduce training costs with effective use of preemptible instances, i.e., those that can be obtained at a much cheaper price while idle, but may be preempted whenever requested by priority users. Doing so, however, requires new forms of resiliency and efficiency to cope with the possibility of frequent preemptions - a failure model that is drastically different from the occasional failures in normal cluster settings that existing checkpointing techniques target.
  We present Bamboo, a distributed system that tackles these challenges by introducing redundant computations into the training pipeline, i.e., whereby one node performs computations over not only its own layers but also over some layers in its neighbor. Our key insight is that training large models often requires pipeline parallelism where "pipeline bubbles" naturally exist. Bamboo carefully fills redundant computations into these bubbles, providing resilience at a low cost. Across a variety of widely used DNN models, Bamboo outperforms traditional checkpointing by 3.7x in training throughput, and reduces costs by 2.4x compared to a setting where on-demand instances are used.

</p>
</details>

<details><summary><b>Assessing the ability of generative adversarial networks to learn canonical medical image statistics</b>
<a href="https://arxiv.org/abs/2204.12007">arxiv:2204.12007</a>
&#x1F4C8; 0 <br>
<p>Varun A. Kelkar, Dimitrios S. Gotsis, Frank J. Brooks, Prabhat KC, Kyle J. Myers, Rongping Zeng, Mark A. Anastasio</p></summary>
<p>

**Abstract:** In recent years, generative adversarial networks (GANs) have gained tremendous popularity for potential applications in medical imaging, such as medical image synthesis, restoration, reconstruction, translation, as well as objective image quality assessment. Despite the impressive progress in generating high-resolution, perceptually realistic images, it is not clear if modern GANs reliably learn the statistics that are meaningful to a downstream medical imaging application. In this work, the ability of a state-of-the-art GAN to learn the statistics of canonical stochastic image models (SIMs) that are relevant to objective assessment of image quality is investigated. It is shown that although the employed GAN successfully learned several basic first- and second-order statistics of the specific medical SIMs under consideration and generated images with high perceptual quality, it failed to correctly learn several per-image statistics pertinent to the these SIMs, highlighting the urgent need to assess medical image GANs in terms of objective measures of image quality.

</p>
</details>

<details><summary><b>gLaSDI: Parametric Physics-informed Greedy Latent Space Dynamics Identification</b>
<a href="https://arxiv.org/abs/2204.12005">arxiv:2204.12005</a>
&#x1F4C8; 0 <br>
<p>Xiaolong He, Youngsoo Choi, William D. Fries, Jon Belof, Jiun-Shyan Chen</p></summary>
<p>

**Abstract:** A parametric adaptive physics-informed greedy Latent Space Dynamics Identification (gLaSDI) method is proposed for accurate, efficient, and robust data-driven reduced-order modeling of high-dimensional nonlinear dynamical systems. In the proposed gLaSDI framework, an autoencoder discovers intrinsic nonlinear latent representations of high-dimensional data, while dynamics identification (DI) models capture local latent-space dynamics. An interactive training algorithm is adopted for the autoencoder and local DI models, which enables identification of simple latent-space dynamics and enhances accuracy and efficiency of data-driven reduced-order modeling. To maximize and accelerate the exploration of the parameter space for the optimal model performance, an adaptive greedy sampling algorithm integrated with a physics-informed residual-based error indicator and random-subset evaluation is introduced to search for the optimal training samples on-the-fly. Further, to exploit local latent-space dynamics captured by the local DI models for an improved modeling accuracy with a minimum number of local DI models in the parameter space, an efficient k-nearest neighbor convex interpolation scheme is employed. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including Burgers equations, nonlinear heat conduction, and radial advection. The proposed adaptive greedy sampling outperforms the conventional predefined uniform sampling in terms of accuracy. Compared with the high-fidelity models, gLaSDI achieves 66 to 4,417x speed-up with 1 to 5% relative errors.

</p>
</details>

<details><summary><b>Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System</b>
<a href="https://arxiv.org/abs/2204.11970">arxiv:2204.11970</a>
&#x1F4C8; 0 <br>
<p>Tobias Schlosser, Frederik Beuth, Trixy Meyer, Arunodhayan Sampath Kumar, Gabriel Stolze, Olga Furashova, Katrin Engelmann, Danny Kowerko</p></summary>
<p>

**Abstract:** In ophthalmology, intravitreal operative medication therapy (IVOM) is widespread treatment for diseases such as the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. Within our proposed multistage system, we classify the VA progression into the three groups of therapy "winners", "stabilizers", and "losers" (WSL scheme). Our OCT biomarker classification using an ensemble of deep neural networks results in a classification accuracy (F1-score) of over 98 %, enabling us to complete incomplete OCT documentations while allowing us to exploit them for a more precise VA modelling process. Our VA prediction requires at least four VA examinations and optionally OCT biomarkers from the same time period to predict the VA progression within a forecasted time frame. While achieving a prediction accuracy of up to 69 % (macro average F1-score) when considering all three WSL-based progression groups, this corresponds to an improvement by 11 % in comparison to our ophthalmic expertise (58 %).

</p>
</details>

<details><summary><b>Crystal Transformer: Self-learning neural language model for Generative and Tinkering Design of Materials</b>
<a href="https://arxiv.org/abs/2204.11953">arxiv:2204.11953</a>
&#x1F4C8; 0 <br>
<p>Lai Wei, Qinyang Li, Yuqi Song, Stanislav Stefanov, Edirisuriya M. D. Siriwardane, Fanglin Chen, Jianjun Hu</p></summary>
<p>

**Abstract:** Self-supervised neural language models have recently achieved unprecedented success, from natural language processing to learning the languages of biological sequences and organic molecules. These models have demonstrated superior performance in the generation, structure classification, and functional predictions for proteins and molecules with learned representations. However, most of the masking-based pre-trained language models are not designed for generative design, and their black-box nature makes it difficult to interpret their design logic. Here we propose BLMM Crystal Transformer, a neural network based probabilistic generative model for generative and tinkering design of inorganic materials. Our model is built on the blank filling language model for text generation and has demonstrated unique advantages in learning the "materials grammars" together with high-quality generation, interpretability, and data efficiency. It can generate chemically valid materials compositions with as high as 89.7\% charge neutrality and 84.8\% balanced electronegativity, which are more than 4 and 8 times higher compared to a pseudo random sampling baseline. The probabilistic generation process of BLMM allows it to recommend tinkering operations based on learned materials chemistry and makes it useful for materials doping. Combined with the TCSP crysal structure prediction algorithm, We have applied our model to discover a set of new materials as validated using DFT calculations. Our work thus brings the unsupervised transformer language models based generative artificial intelligence to inorganic materials. A user-friendly web app has been developed for computational materials doping and can be accessed freely at \url{www.materialsatlas.org/blmtinker}.

</p>
</details>

<details><summary><b>Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks</b>
<a href="https://arxiv.org/abs/2204.11922">arxiv:2204.11922</a>
&#x1F4C8; 0 <br>
<p>Navid Rezaei, Marek Z. Reformat</p></summary>
<p>

**Abstract:** Pre-trained language models have shown excellent results in few-shot learning scenarios using in-context learning. Although it is impressive, the size of language models can be prohibitive to make them usable in on-device applications, such as sensors or smartphones. With smaller language models, task-specific data annotation is needed to fine-tune the language model for a specific purpose. However, data annotation can have a substantial financial and time burden for small research groups, startups, and even companies. In this paper, we analyze different prompt-based fine-tuning techniques to improve results on both language and multimodal causal transformer models. To evaluate our results, we use a dataset focusing on visual commonsense reasoning in time. Our results show that by simple model-agnostic prompt-based fine-tuning, comparable results can be reached by only using 35%-40% of the fine-tuning training dataset. The proposed approaches result in significant time and financial savings. As the proposed methods make minimal architectural assumptions, other researchers can use the results in their transformer models with minimal adaptations. We plan to release the source code freely to make it easier for the community to use and contribute to our work.

</p>
</details>


{% endraw %}
Prev: [2022.04.24]({{ '/2022/04/24/2022.04.24.html' | relative_url }})  Next: [2022.04.26]({{ '/2022/04/26/2022.04.26.html' | relative_url }})