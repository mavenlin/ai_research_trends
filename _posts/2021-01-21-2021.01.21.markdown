Prev: [2021.01.20]({{ '/2021/01/20/2021.01.20.html' | relative_url }})  Next: [2021.01.22]({{ '/2021/01/22/2021.01.22.html' | relative_url }})
{% raw %}
## Summary for 2021-01-21, created on 2021-12-24


<details><summary><b>Noisy intermediate-scale quantum (NISQ) algorithms</b>
<a href="https://arxiv.org/abs/2101.08448">arxiv:2101.08448</a>
&#x1F4C8; 124 <br>
<p>Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias Haug, Sumner Alperin-Lea, Abhinav Anand, Matthias Degroote, Hermanni Heimonen, Jakob S. Kottmann, Tim Menke, Wai-Keong Mok, Sukin Sim, Leong-Chuan Kwek, Alán Aspuru-Guzik</p></summary>
<p>

**Abstract:** A universal fault-tolerant quantum computer that can solve efficiently problems such as integer factorization and unstructured database search requires millions of qubits with low error rates and long coherence times. While the experimental advancement towards realizing such devices will potentially take decades of research, noisy intermediate-scale quantum (NISQ) computers already exist. These computers are composed of hundreds of noisy qubits, i.e. qubits that are not error-corrected, and therefore perform imperfect operations in a limited coherence time. In the search for quantum advantage with these devices, algorithms have been proposed for applications in various disciplines spanning physics, machine learning, quantum chemistry and combinatorial optimization. The goal of such algorithms is to leverage the limited available resources to perform classically challenging tasks. In this review, we provide a thorough summary of NISQ computational paradigms and algorithms. We discuss the key structure of these algorithms, their limitations, and advantages. We additionally provide a comprehensive overview of various benchmarking and software tools useful for programming and testing NISQ devices.

</p>
</details>

<details><summary><b>LEAF: A Learnable Frontend for Audio Classification</b>
<a href="https://arxiv.org/abs/2101.08596">arxiv:2101.08596</a>
&#x1F4C8; 113 <br>
<p>Neil Zeghidour, Olivier Teboul, Félix de Chaumont Quitry, Marco Tagliasacchi</p></summary>
<p>

**Abstract:** Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.

</p>
</details>

<details><summary><b>Pre-training without Natural Images</b>
<a href="https://arxiv.org/abs/2101.08515">arxiv:2101.08515</a>
&#x1F4C8; 84 <br>
<p>Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada, Nakamasa Inoue, Akio Nakamura, Yutaka Satoh</p></summary>
<p>

**Abstract:** Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.

</p>
</details>

<details><summary><b>Clairvoyant Prefetching for Distributed Machine Learning I/O</b>
<a href="https://arxiv.org/abs/2101.08734">arxiv:2101.08734</a>
&#x1F4C8; 45 <br>
<p>Nikoli Dryden, Roman Böhringer, Tal Ben-Nun, Torsten Hoefler</p></summary>
<p>

**Abstract:** I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments. Indeed, at large scale, I/O takes as much as 85% of training time. Addressing this I/O bottleneck necessitates careful optimization, as optimal data ingestion pipelines differ between systems, and require a delicate balance between access to local storage, external filesystems, and remote nodes. We introduce NoPFS, a machine learning I/O middleware, which provides a scalable, flexible, and easy-to-use solution to the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the random access pattern for training with SGD, it can exactly predict when and where a sample will be accessed. We combine this with an analysis of access patterns and a performance model to provide distributed caching policies that adapt to different datasets and storage hierarchies. NoPFS reduces I/O times and improves end-to-end training by up to 5.4x on the ImageNet-1k, ImageNet-22k, and CosmoFlow datasets.

</p>
</details>

<details><summary><b>MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd Segmentation in Video Scenes</b>
<a href="https://arxiv.org/abs/2101.08609">arxiv:2101.08609</a>
&#x1F4C8; 44 <br>
<p>Jinhai Yang, Hua Yang</p></summary>
<p>

**Abstract:** Crowd segmentation is a fundamental task serving as the basis of crowded scene analysis, and it is highly desirable to obtain refined pixel-level segmentation maps. However, it remains a challenging problem, as existing approaches either require dense pixel-level annotations to train deep learning models or merely produce rough segmentation maps from optical or particle flows with physical models. In this paper, we propose the Motion Prior-Aware Siamese Network (MPASNET) for unsupervised crowd semantic segmentation. This model not only eliminates the need for annotation but also yields high-quality segmentation maps. Specially, we first analyze the coherent motion patterns across the frames and then apply a circular region merging strategy on the collective particles to generate pseudo-labels. Moreover, we equip MPASNET with siamese branches for augmentation-invariant regularization and siamese feature aggregation. Experiments over benchmark datasets indicate that our model outperforms the state-of-the-arts by more than 12% in terms of mIoU.

</p>
</details>

<details><summary><b>Boost then Convolve: Gradient Boosting Meets Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2101.08543">arxiv:2101.08543</a>
&#x1F4C8; 43 <br>
<p>Sergei Ivanov, Liudmila Prokhorenkova</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and, as we show, are suboptimal in the heterogeneous setting. In this work, we propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. Our model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, we demonstrate a significant increase in performance on a variety of graphs with tabular features. The code is available: https://github.com/nd7141/bgnn.

</p>
</details>

<details><summary><b>Regularization via deep generative models: an analysis point of view</b>
<a href="https://arxiv.org/abs/2101.08661">arxiv:2101.08661</a>
&#x1F4C8; 33 <br>
<p>Thomas Oberlin, Mathieu Verm</p></summary>
<p>

**Abstract:** This paper proposes a new way of regularizing an inverse problem in imaging (e.g., deblurring or inpainting) by means of a deep generative neural network. Compared to end-to-end models, such approaches seem particularly interesting since the same network can be used for many different problems and experimental conditions, as soon as the generative model is suited to the data. Previous works proposed to use a synthesis framework, where the estimation is performed on the latent vector, the solution being obtained afterwards via the decoder. Instead, we propose an analysis formulation where we directly optimize the image itself and penalize the latent vector. We illustrate the interest of such a formulation by running experiments of inpainting, deblurring and super-resolution. In many cases our technique achieves a clear improvement of the performance and seems to be more robust, in particular with respect to initialization.

</p>
</details>

<details><summary><b>Activity Graph Transformer for Temporal Action Localization</b>
<a href="https://arxiv.org/abs/2101.08540">arxiv:2101.08540</a>
&#x1F4C8; 30 <br>
<p>Megha Nawhal, Greg Mori</p></summary>
<p>

**Abstract:** We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin.

</p>
</details>

<details><summary><b>Characterizing signal propagation to close the performance gap in unnormalized ResNets</b>
<a href="https://arxiv.org/abs/2101.08692">arxiv:2101.08692</a>
&#x1F4C8; 29 <br>
<p>Andrew Brock, Soham De, Samuel L. Smith</p></summary>
<p>

**Abstract:** Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet.

</p>
</details>

<details><summary><b>Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from Black-box Models?</b>
<a href="https://arxiv.org/abs/2101.08717">arxiv:2101.08717</a>
&#x1F4C8; 26 <br>
<p>Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos</p></summary>
<p>

**Abstract:** Convolutional neural networks have been successful lately enabling companies to develop neural-based products, which demand an expensive process, involving data acquisition and annotation; and model generation, usually requiring experts. With all these costs, companies are concerned about the security of their models against copies and deliver them as black-boxes accessed by APIs. Nonetheless, we argue that even black-box models still have some vulnerabilities. In a preliminary work, we presented a simple, yet powerful, method to copy black-box models by querying them with natural random images. In this work, we consolidate and extend the copycat method: (i) some constraints are waived; (ii) an extensive evaluation with several problems is performed; (iii) models are copied between different architectures; and, (iv) a deeper analysis is performed by looking at the copycat behavior. Results show that natural random images are effective to generate copycats for several problems.

</p>
</details>

<details><summary><b>Arabic Speech Recognition by End-to-End, Modular Systems and Human</b>
<a href="https://arxiv.org/abs/2101.08454">arxiv:2101.08454</a>
&#x1F4C8; 23 <br>
<p>Amir Hussein, Shinji Watanabe, Ahmed Ali</p></summary>
<p>

**Abstract:** Recent advances in automatic speech recognition (ASR) have achieved accuracy levels comparable to human transcribers, which led researchers to debate if the machine has reached human performance. Previous work focused on the English language and modular hidden Markov model-deep neural network (HMM-DNN) systems. In this paper, we perform a comprehensive benchmarking for end-to-end transformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the Arabic language and its dialects. For the HSR, we evaluate linguist performance and lay-native speaker performance on a new dataset collected as a part of this study. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new performance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our results suggest that human performance in the Arabic language is still considerably better than the machine with an absolute WER gap of 3.5% on average.

</p>
</details>

<details><summary><b>Item Recommendation from Implicit Feedback</b>
<a href="https://arxiv.org/abs/2101.08769">arxiv:2101.08769</a>
&#x1F4C8; 22 <br>
<p>Steffen Rendle</p></summary>
<p>

**Abstract:** The task of item recommendation is to select the best items for a user from a large catalogue of items. Item recommenders are commonly trained from implicit feedback which consists of past actions that are positive only. Core challenges of item recommendation are (1) how to formulate a training objective from implicit feedback and (2) how to efficiently train models over a large item catalogue. This article provides an overview of item recommendation, its unique characteristics and some common approaches. It starts with an introduction to the problem and discusses different training objectives. The main body deals with learning algorithms and presents sampling based algorithms for general recommenders and more efficient algorithms for dot product models. Finally, the application of item recommenders for retrieval tasks is discussed.

</p>
</details>

<details><summary><b>Customer Price Sensitivities in Competitive Automobile Insurance Markets</b>
<a href="https://arxiv.org/abs/2101.08551">arxiv:2101.08551</a>
&#x1F4C8; 21 <br>
<p>Robert Matthijs Verschuren</p></summary>
<p>

**Abstract:** Insurers are increasingly adopting more demand-based strategies to incorporate the indirect effect of premium changes on their policyholders' willingness to stay. However, since in practice both insurers' renewal premia and customers' responses to these premia typically depend on the customer's level of risk, it remains challenging in these strategies to determine how to properly control for this confounding. We therefore consider a causal inference approach in this paper to account for customer price sensitivities and to deduce optimal, multi-period profit maximizing premium renewal offers. More specifically, we extend the discrete treatment framework of Guelman and Guillén (2014) by Extreme Gradient Boosting, or XGBoost, and by multiple imputation to better account for the uncertainty in the counterfactual responses. We additionally introduce the continuous treatment framework with XGBoost to the insurance literature to allow identification of the exact optimal renewal offers and account for any competition in the market by including competitor offers. The application of the two treatment frameworks to a Dutch automobile insurance portfolio suggests that a policy's competitiveness in the market is crucial for a customer's price sensitivity and that XGBoost is more appropriate to describe this than the traditional logistic regression. Moreover, an efficient frontier of both frameworks indicates that substantially more profit can be gained on the portfolio than realized, also already with less churn and in particular if we allow for continuous rate changes. A multi-period renewal optimization confirms these findings and demonstrates that the competitiveness enables temporal feedback of previous rate changes on future demand.

</p>
</details>

<details><summary><b>UNIT: Unifying Tensorized Instruction Compilation</b>
<a href="https://arxiv.org/abs/2101.08458">arxiv:2101.08458</a>
&#x1F4C8; 16 <br>
<p>Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, Tony Nowatzki</p></summary>
<p>

**Abstract:** Because of the increasing demand for computation in DNN, researchers develope both hardware and software mechanisms to reduce the compute and memory burden. A widely adopted approach is to use mixed precision data types. However, it is hard to leverage mixed precision without hardware support because of the overhead of data casting. Hardware vendors offer tensorized instructions for mixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT. These instructions involve a computing idiom that reduces multiple low precision elements into one high precision element. The lack of compilation techniques for this makes it hard to utilize these instructions: Using vendor-provided libraries for computationally-intensive kernels is inflexible and prevents further optimizations, and manually writing hardware intrinsics is error-prone and difficult for programmers. Some prior works address this problem by creating compilers for each instruction. This requires excessive effort when it comes to many tensorized instructions. In this work, we develop a compiler framework to unify the compilation for these instructions -- a unified semantics abstraction eases the integration of new instructions, and reuses the analysis and transformations. Tensorized instructions from different platforms can be compiled via UNIT with moderate effort for favorable performance. Given a tensorized instruction and a tensor operation, UNIT automatically detects the applicability, transforms the loop organization of the operation,and rewrites the loop body to leverage the tensorized instruction. According to our evaluation, UNIT can target various mainstream hardware platforms. The generated end-to-end inference model achieves 1.3x speedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an NvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on an ARM CPU.

</p>
</details>

<details><summary><b>Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</b>
<a href="https://arxiv.org/abs/2101.08482">arxiv:2101.08482</a>
&#x1F4C8; 14 <br>
<p>Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, Stefano Soatto</p></summary>
<p>

**Abstract:** We present a plug-in replacement for batch normalization (BN) called exponential moving average normalization (EMAN), which improves the performance of existing student-teacher based self- and semi-supervised learning techniques. Unlike the standard BN, where the statistics are computed within each batch, EMAN, used in the teacher, updates its statistics by exponential moving average from the BN statistics of the student. This design reduces the intrinsic cross-sample dependency of BN and enhances the generalization of the teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2 points and semi-supervised learning by about 7/2 points, when 1%/10% supervised labels are available on ImageNet. These improvements are consistent across methods, network architectures, training duration, and datasets, demonstrating the general effectiveness of this technique. The code is available at https://github.com/amazon-research/exponential-moving-average-normalization.

</p>
</details>

<details><summary><b>Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</b>
<a href="https://arxiv.org/abs/2101.08452">arxiv:2101.08452</a>
&#x1F4C8; 12 <br>
<p>Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh</p></summary>
<p>

**Abstract:** We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.

</p>
</details>

<details><summary><b>MLPF: Efficient machine-learned particle-flow reconstruction using graph neural networks</b>
<a href="https://arxiv.org/abs/2101.08578">arxiv:2101.08578</a>
&#x1F4C8; 10 <br>
<p>Joosep Pata, Javier Duarte, Jean-Roch Vlimant, Maurizio Pierini, Maria Spiropulu</p></summary>
<p>

**Abstract:** In general-purpose particle detectors, the particle-flow algorithm may be used to reconstruct a comprehensive particle-level view of the event by combining information from the calorimeters and the trackers, significantly improving the detector resolution for jets and the missing transverse momentum. In view of the planned high-luminosity upgrade of the CERN Large Hadron Collider (LHC), it is necessary to revisit existing reconstruction algorithms and ensure that both the physics and computational performance are sufficient in an environment with many simultaneous proton-proton interactions (pileup). Machine learning may offer a prospect for computationally efficient event reconstruction that is well-suited to heterogeneous computing platforms, while significantly improving the reconstruction quality over rule-based algorithms for granular detectors. We introduce MLPF, a novel, end-to-end trainable, machine-learned particle-flow algorithm based on parallelizable, computationally efficient, and scalable graph neural networks optimized using a multi-task objective on simulated events. We report the physics and computational performance of the MLPF algorithm on a Monte Carlo dataset of top quark-antiquark pairs produced in proton-proton collisions in conditions similar to those expected for the high-luminosity LHC. The MLPF algorithm improves the physics response with respect to a rule-based benchmark algorithm and demonstrates computationally scalable particle-flow reconstruction in a high-pileup environment.

</p>
</details>

<details><summary><b>A two-stage data association approach for 3D Multi-object Tracking</b>
<a href="https://arxiv.org/abs/2101.08684">arxiv:2101.08684</a>
&#x1F4C8; 9 <br>
<p>Minh-Quan Dao, Vincent Frémont</p></summary>
<p>

**Abstract:** Multi-object tracking (MOT) is an integral part of any autonomous driving pipelines because itproduces trajectories which has been taken by other moving objects in the scene and helps predicttheir future motion. Thanks to the recent advances in 3D object detection enabled by deep learning,track-by-detection has become the dominant paradigm in 3D MOT. In this paradigm, a MOT systemis essentially made of an object detector and a data association algorithm which establishes track-to-detection correspondence. While 3D object detection has been actively researched, associationalgorithms for 3D MOT seem to settle at a bipartie matching formulated as a linear assignmentproblem (LAP) and solved by the Hungarian algorithm. In this paper, we adapt a two-stage dataassociation method which was successful in image-based tracking to the 3D setting, thus providingan alternative for data association for 3D MOT. Our method outperforms the baseline using one-stagebipartie matching for data association by achieving 0.587 AMOTA in NuScenes validation set.

</p>
</details>

<details><summary><b>Mindless Attractor: A False-Positive Resistant Intervention for Drawing Attention Using Auditory Perturbation</b>
<a href="https://arxiv.org/abs/2101.08621">arxiv:2101.08621</a>
&#x1F4C8; 9 <br>
<p>Riku Arakawa, Hiromu Yakura</p></summary>
<p>

**Abstract:** Explicitly alerting users is not always an optimal intervention, especially when they are not motivated to obey. For example, in video-based learning, learners who are distracted from the video would not follow an alert asking them to pay attention. Inspired by the concept of Mindless Computing, we propose a novel intervention approach, Mindless Attractor, that leverages the nature of human speech communication to help learners refocus their attention without relying on their motivation. Specifically, it perturbs the voice in the video to direct their attention without consuming their conscious awareness. Our experiments not only confirmed the validity of the proposed approach but also emphasized its advantages in combination with a machine learning-based sensing module. Namely, it would not frustrate users even though the intervention is activated by false-positive detection of their attentive state. Our intervention approach can be a reliable way to induce behavioral change in human-AI symbiosis.

</p>
</details>

<details><summary><b>Efficient Pure Exploration for Combinatorial Bandits with Semi-Bandit Feedback</b>
<a href="https://arxiv.org/abs/2101.08534">arxiv:2101.08534</a>
&#x1F4C8; 9 <br>
<p>Marc Jourdan, Mojmír Mutný, Johannes Kirschner, Andreas Krause</p></summary>
<p>

**Abstract:** Combinatorial bandits with semi-bandit feedback generalize multi-armed bandits, where the agent chooses sets of arms and observes a noisy reward for each arm contained in the chosen set. The action set satisfies a given structure such as forming a base of a matroid or a path in a graph. We focus on the pure-exploration problem of identifying the best arm with fixed confidence, as well as a more general setting, where the structure of the answer set differs from the one of the action set. Using the recently popularized game framework, we interpret this problem as a sequential zero-sum game and develop a CombGame meta-algorithm whose instances are asymptotically optimal algorithms with finite time guarantees. In addition to comparing two families of learners to instantiate our meta-algorithm, the main contribution of our work is a specific oracle efficient instance for best-arm identification with combinatorial actions. Based on a projection-free online learning algorithm for convex polytopes, it is the first computationally efficient algorithm which is asymptotically optimal and has competitive empirical performance.

</p>
</details>

<details><summary><b>Out-of-Distribution Generalization Analysis via Influence Function</b>
<a href="https://arxiv.org/abs/2101.08521">arxiv:2101.08521</a>
&#x1F4C8; 9 <br>
<p>Haotian Ye, Chuanlong Xie, Yue Liu, Zhenguo Li</p></summary>
<p>

**Abstract:** The mismatch between training and target data is one major challenge for current machine learning systems. When training data is collected from multiple domains and the target domains include all training domains and other new domains, we are facing an Out-of-Distribution (OOD) generalization problem that aims to find a model with the best OOD accuracy. One of the definitions of OOD accuracy is worst-domain accuracy. In general, the set of target domains is unknown, and the worst over target domains may be unseen when the number of observed domains is limited. In this paper, we show that the worst accuracy over the observed domains may dramatically fail to identify the OOD accuracy. To this end, we introduce Influence Function, a classical tool from robust statistics, into the OOD generalization problem and suggest the variance of influence function to monitor the stability of a model on training domains. We show that the accuracy on test domains and the proposed index together can help us discern whether OOD algorithms are needed and whether a model achieves good OOD generalization.

</p>
</details>

<details><summary><b>Self-Adaptive Training: Bridging the Supervised and Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2101.08732">arxiv:2101.08732</a>
&#x1F4C8; 8 <br>
<p>Lang Huang, Chao Zhang, Hongyang Zhang</p></summary>
<p>

**Abstract:** We propose self-adaptive training -- a unified training algorithm that dynamically calibrates and enhances training process by model predictions without incurring extra computational cost -- to advance both supervised and self-supervised learning of deep neural networks. We analyze the training dynamics of deep networks on training data that are corrupted by, e.g., random noise and adversarial examples. Our analysis shows that model predictions are able to magnify useful underlying information in data and this phenomenon occurs broadly even in the absence of \emph{any} label information, highlighting that model predictions could substantially benefit the training process: self-adaptive training improves the generalization of deep networks under noise and enhances the self-supervised representation learning. The analysis also sheds light on understanding deep learning, e.g., a potential explanation of the recently-discovered double-descent phenomenon in empirical risk minimization and the collapsing issue of the state-of-the-art self-supervised learning algorithms. Experiments on the CIFAR, STL and ImageNet datasets verify the effectiveness of our approach in three applications: classification with label noise, selective classification and linear evaluation. To facilitate future research, the code has been made public available at https://github.com/LayneH/self-adaptive-training.

</p>
</details>

<details><summary><b>A Note on Connectivity of Sublevel Sets in Deep Learning</b>
<a href="https://arxiv.org/abs/2101.08576">arxiv:2101.08576</a>
&#x1F4C8; 8 <br>
<p>Quynh Nguyen</p></summary>
<p>

**Abstract:** It is shown that for deep neural networks, a single wide layer of width $N+1$ ($N$ being the number of training samples) suffices to prove the connectivity of sublevel sets of the training loss function. In the two-layer setting, the same property may not hold even if one has just one neuron less (i.e. width $N$ can lead to disconnected sublevel sets).

</p>
</details>

<details><summary><b>Adv-OLM: Generating Textual Adversaries via OLM</b>
<a href="https://arxiv.org/abs/2101.08523">arxiv:2101.08523</a>
&#x1F4C8; 8 <br>
<p>Vijit Malik, Ashwani Bhat, Ashutosh Modi</p></summary>
<p>

**Abstract:** Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.

</p>
</details>

<details><summary><b>Unifying Cardiovascular Modelling with Deep Reinforcement Learning for Uncertainty Aware Control of Sepsis Treatment</b>
<a href="https://arxiv.org/abs/2101.08477">arxiv:2101.08477</a>
&#x1F4C8; 8 <br>
<p>Thesath Nanayakkara, Gilles Clermont, Christopher James Langmead, David Swigon</p></summary>
<p>

**Abstract:** Sepsis is a potentially life threatening inflammatory response to infection or severe tissue damage. It has a highly variable clinical course, requiring constant monitoring of the patient's state to guide the management of intravenous fluids and vasopressors, among other interventions. Despite decades of research, there's still debate among experts on optimal treatment. Here, we combine for the first time, distributional deep reinforcement learning with mechanistic physiological models to find personalized sepsis treatment strategies. Our method handles partial observability by leveraging known cardiovascular physiology, introducing a novel physiology-driven recurrent autoencoder, and quantifies the uncertainty of its own results. Moreover, we introduce a framework for uncertainty aware decision support with humans in the loop. We show that our method learns physiologically explainable, robust policies that are consistent with clinical knowledge. Further our method consistently identifies high risk states that lead to death, which could potentially benefit from more frequent vasopressor administration, providing valuable guidance for future research

</p>
</details>

<details><summary><b>Validating Label Consistency in NER Data Annotation</b>
<a href="https://arxiv.org/abs/2101.08698">arxiv:2101.08698</a>
&#x1F4C8; 7 <br>
<p>Qingkai Zeng, Mengxia Yu, Wenhao Yu, Tianwen Jiang, Meng Jiang</p></summary>
<p>

**Abstract:** Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with annotation. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an empirical method to explore the relationship between label (in-)consistency and NER model performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in the corrected version of both datasets.

</p>
</details>

<details><summary><b>Monitoring nonstationary processes based on recursive cointegration analysis and elastic weight consolidation</b>
<a href="https://arxiv.org/abs/2101.08579">arxiv:2101.08579</a>
&#x1F4C8; 7 <br>
<p>Jingxin Zhang, Donghua Zhou, Maoyin Chen</p></summary>
<p>

**Abstract:** This paper considers the problem of nonstationary process monitoring under frequently varying operating conditions. Traditional approaches generally misidentify the normal dynamic deviations as faults and thus lead to high false alarms. Besides, they generally consider single relatively steady operating condition and suffer from the catastrophic forgetting issue when learning successive operating conditions. In this paper, recursive cointegration analysis (RCA) is first proposed to distinguish the real faults from normal systems changes, where the model is updated once a new normal sample arrives and can adapt to slow change of cointegration relationship. Based on the long-term equilibrium information extracted by RCA, the remaining short-term dynamic information is monitored by recursive principal component analysis (RPCA). Thus a comprehensive monitoring framework is built. When the system enters a new operating condition, the RCA-RPCA model is rebuilt to deal with the new condition. Meanwhile, elastic weight consolidation (EWC) is employed to settle the `catastrophic forgetting' issue inherent in RPCA, where significant information of influential parameters is enhanced to avoid the abrupt performance degradation for similar modes. The effectiveness of the proposed method is illustrated by a practical industrial system.

</p>
</details>

<details><summary><b>Orthogonal Least Squares Based Fast Feature Selection for Linear Classification</b>
<a href="https://arxiv.org/abs/2101.08539">arxiv:2101.08539</a>
&#x1F4C8; 7 <br>
<p>Sikai Zhang, Zi-Qiang Lang</p></summary>
<p>

**Abstract:** An Orthogonal Least Squares (OLS) based feature selection method is proposed for both binomial and multinomial classification. The novel Squared Orthogonal Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR) in OLS and used as the feature ranking criterion. The equivalence between the canonical correlation coefficient, Fisher's criterion, and the sum of the SOCCs is revealed, which unveils the statistical implication of ERR in OLS for the first time. It is also shown that the OLS based feature selection method has speed advantages when applied for greedy search. The proposed method is comprehensively compared with the mutual information based feature selection methods and the embedded methods using both synthetic and real world datasets. The results show that the proposed method is always in the top 5 among the 12 candidate methods. Besides, the proposed method can be directly applied to continuous features without discretisation, which is another significant advantage over mutual information based methods.

</p>
</details>

<details><summary><b>GhostSR: Learning Ghost Features for Efficient Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2101.08525">arxiv:2101.08525</a>
&#x1F4C8; 7 <br>
<p>Ying Nie, Kai Han, Zhenhua Liu, An Xiao, Yiping Deng, Chunjing Xu, Yunhe Wang</p></summary>
<p>

**Abstract:** Modern single image super-resolution (SISR) system based on convolutional neural networks (CNNs) achieves fancy performance while requires huge computational costs. The problem on feature redundancy is well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation to generate the redundant features (i.e., Ghost features). Compared with depth-wise convolution which is not friendly to GPUs or NPUs, shift operation can bring practical inference acceleration for CNNs on common hardware. We analyze the benefits of shift operation for SISR and make the shift orientation learnable based on Gumbel-Softmax trick. For a given pre-trained model, we first cluster all filters in each convolutional layer to identify the intrinsic ones for generating intrinsic features. Ghost features will be derived by moving these intrinsic features along a specific orientation. The complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR models embedded in our proposed module can achieve comparable performance to that of their baselines with large reduction of parameters, FLOPs and GPU latency. For instance, we reduce the parameters by 47%, FLOPs by 46% and GPU latency by 41% of EDSR x2 network without significant performance degradation.

</p>
</details>

<details><summary><b>Ensemble learning and iterative training (ELIT) machine learning: applications towards uncertainty quantification and automated experiment in atom-resolved microscopy</b>
<a href="https://arxiv.org/abs/2101.08449">arxiv:2101.08449</a>
&#x1F4C8; 7 <br>
<p>Ayana Ghosh, Bobby G. Sumpter, Ondrej Dyck, Sergei V. Kalinin, Maxim Ziatdinov</p></summary>
<p>

**Abstract:** Deep learning has emerged as a technique of choice for rapid feature extraction across imaging disciplines, allowing rapid conversion of the data streams to spatial or spatiotemporal arrays of features of interest. However, applications of deep learning in experimental domains are often limited by the out-of-distribution drift between the experiments, where the network trained for one set of imaging conditions becomes sub-optimal for different ones. This limitation is particularly stringent in the quest to have an automated experiment setting, where retraining or transfer learning becomes impractical due to the need for human intervention and associated latencies. Here we explore the reproducibility of deep learning for feature extraction in atom-resolved electron microscopy and introduce workflows based on ensemble learning and iterative training to greatly improve feature detection. This approach both allows incorporating uncertainty quantification into the deep learning analysis and also enables rapid automated experimental workflows where retraining of the network to compensate for out-of-distribution drift due to subtle change in imaging conditions is substituted for a human operator or programmatic selection of networks from the ensemble. This methodology can be further applied to machine learning workflows in other imaging areas including optical and chemical imaging.

</p>
</details>

<details><summary><b>Cain: Automatic Code Generation for Simultaneous Convolutional Kernels on Focal-plane Sensor-processors</b>
<a href="https://arxiv.org/abs/2101.08715">arxiv:2101.08715</a>
&#x1F4C8; 6 <br>
<p>Edward Stow, Riku Murai, Sajad Saeedi, Paul H. J. Kelly</p></summary>
<p>

**Abstract:** Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low power, high frame rate computation, making them suitable for edge computation. Unfortunately, these devices' limited instruction sets and registers make developing complex algorithms difficult. In this work, we present Cain - a compiler that targets SCAMP-5, a general-purpose FPSP - which generates code from multiple convolutional kernels. As an example, given the convolutional kernels for an MNIST digit recognition neural network, Cain produces code that is half as long, when compared to the other available compilers for SCAMP-5.

</p>
</details>

<details><summary><b>Distilling Large Language Models into Tiny and Effective Students using pQRNN</b>
<a href="https://arxiv.org/abs/2101.08890">arxiv:2101.08890</a>
&#x1F4C8; 5 <br>
<p>Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson</p></summary>
<p>

**Abstract:** Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It's important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projection-based embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pre-trained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9\% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1\% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.

</p>
</details>

<details><summary><b>Geometric Moment Invariants to Motion Blur</b>
<a href="https://arxiv.org/abs/2101.08647">arxiv:2101.08647</a>
&#x1F4C8; 5 <br>
<p>Hongxiang Hao., Hanlin Mo., Hua Li</p></summary>
<p>

**Abstract:** In this paper, we focus on removing interference of motion blur by the derivation of motion blur invariants.Unlike earlier work, we don't restore any blurred image. Based on geometric moment and mathematical model of motion blur, we prove that geometric moments of blurred image and original image are linearly related. Depending on this property, we can analyse whether an existing moment-based feature is invariant to motion blur. Surprisingly, we find some geometric moment invariants are invariants to not only spatial transform but also motion blur. Meanwhile, we test invariance and robustness of these invariants using synthetic and real blur image datasets. And the results show these invariants outperform some widely used blur moment invariants and non-moment image features in image retrieval, classification and template matching.

</p>
</details>

<details><summary><b>Boosting in Univariate Nonparametric Maximum Likelihood Estimation</b>
<a href="https://arxiv.org/abs/2101.08505">arxiv:2101.08505</a>
&#x1F4C8; 5 <br>
<p>YunPeng Li, ZhaoHui Ye</p></summary>
<p>

**Abstract:** Nonparametric maximum likelihood estimation is intended to infer the unknown density distribution while making as few assumptions as possible. To alleviate the over parameterization in nonparametric data fitting, smoothing assumptions are usually merged into the estimation. In this paper a novel boosting-based method is introduced to the nonparametric estimation in univariate cases. We deduce the boosting algorithm by the second-order approximation of nonparametric log-likelihood. Gaussian kernel and smooth spline are chosen as weak learners in boosting to satisfy the smoothing assumptions. Simulations and real data experiments demonstrate the efficacy of the proposed approach.

</p>
</details>

<details><summary><b>Weighted Fuzzy-Based PSNR for Watermarking</b>
<a href="https://arxiv.org/abs/2101.08502">arxiv:2101.08502</a>
&#x1F4C8; 5 <br>
<p>Maedeh Jamali, Nader Karimi, Shadrokh Samavi</p></summary>
<p>

**Abstract:** One of the problems of conventional visual quality evaluation criteria such as PSNR and MSE is the lack of appropriate standards based on the human visual system (HVS). They are calculated based on the difference of the corresponding pixels in the original and manipulated image. Hence, they practically do not provide a correct understanding of the image quality. Watermarking is an image processing application in which the image's visual quality is an essential criterion for its evaluation. Watermarking requires a criterion based on the HVS that provides more accurate values than conventional measures such as PSNR. This paper proposes a weighted fuzzy-based criterion that tries to find essential parts of an image based on the HVS. Then these parts will have larger weights in computing the final value of PSNR. We compare our results against standard PSNR, and our experiments show considerable consequences.

</p>
</details>

<details><summary><b>Estimating Average Treatment Effects via Orthogonal Regularization</b>
<a href="https://arxiv.org/abs/2101.08490">arxiv:2101.08490</a>
&#x1F4C8; 5 <br>
<p>Tobias Hatt, Stefan Feuerriegel</p></summary>
<p>

**Abstract:** Decision-making often requires accurate estimation of treatment effects from observational data. This is challenging as outcomes of alternative decisions are not observed and have to be estimated. Previous methods estimate outcomes based on unconfoundedness but neglect any constraints that unconfoundedness imposes on the outcomes. In this paper, we propose a novel regularization framework for estimating average treatment effects that exploits unconfoundedness. To this end, we formalize unconfoundedness as an orthogonality constraint, which ensures that the outcomes are orthogonal to the treatment assignment. This orthogonality constraint is then included in the loss function via a regularization. Based on our regularization framework, we develop deep orthogonal networks for unconfounded treatments (DONUT), which learn outcomes that are orthogonal to the treatment assignment. Using a variety of benchmark datasets for estimating average treatment effects, we demonstrate that DONUT outperforms the state-of-the-art substantially.

</p>
</details>

<details><summary><b>Game-Theoretic and Machine Learning-based Approaches for Defensive Deception: A Survey</b>
<a href="https://arxiv.org/abs/2101.10121">arxiv:2101.10121</a>
&#x1F4C8; 4 <br>
<p>Mu Zhu, Ahmed H. Anwar, Zelin Wan, Jin-Hee Cho, Charles Kamhoua, Munindar P. Singh</p></summary>
<p>

**Abstract:** Defensive deception is a promising approach for cyber defense. Via defensive deception, the defender can anticipate attacker actions; it can mislead or lure attacker, or hide real resources. Although defensive deception is increasingly popular in the research community, there has not been a systematic investigation of its key components, the underlying principles, and its tradeoffs in various problem settings. This survey paper focuses on defensive deception research centered on game theory and machine learning, since these are prominent families of artificial intelligence approaches that are widely employed in defensive deception. This paper brings forth insights, lessons, and limitations from prior work. It closes with an outline of some research directions to tackle major gaps in current defensive deception research.

</p>
</details>

<details><summary><b>A Two-stream Neural Network for Pose-based Hand Gesture Recognition</b>
<a href="https://arxiv.org/abs/2101.08926">arxiv:2101.08926</a>
&#x1F4C8; 4 <br>
<p>Chuankun Li, Shuai Li, Yanbo Gao, Xiang Zhang, Wanqing Li</p></summary>
<p>

**Abstract:** Pose based hand gesture recognition has been widely studied in the recent years. Compared with full body action recognition, hand gesture involves joints that are more spatially closely distributed with stronger collaboration. This nature requires a different approach from action recognition to capturing the complex spatial features. Many gesture categories, such as "Grab" and "Pinch", have very similar motion or temporal patterns posing a challenge on temporal processing. To address these challenges, this paper proposes a two-stream neural network with one stream being a self-attention based graph convolutional network (SAGCN) extracting the short-term temporal information and hierarchical spatial information, and the other being a residual-connection enhanced bidirectional Independently Recurrent Neural Network (RBi-IndRNN) for extracting long-term temporal information. The self-attention based graph convolutional network has a dynamic self-attention mechanism to adaptively exploit the relationships of all hand joints in addition to the fixed topology and local feature extraction in the GCN. On the other hand, the residual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of bidirectional processing for temporal modelling. The two streams are fused together for recognition. The Dynamic Hand Gesture dataset and First-Person Hand Action dataset are used to validate its effectiveness, and our method achieves state-of-the-art performance.

</p>
</details>

<details><summary><b>Generative Replay-based Continual Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2101.08894">arxiv:2101.08894</a>
&#x1F4C8; 4 <br>
<p>Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh Sundaram</p></summary>
<p>

**Abstract:** Zero-shot learning is a new paradigm to classify objects from classes that are not available at training time. Zero-shot learning (ZSL) methods have attracted considerable attention in recent years because of their ability to classify unseen/novel class examples. Most of the existing approaches on ZSL works when all the samples from seen classes are available to train the model, which does not suit real life. In this paper, we tackle this hindrance by developing a generative replay-based continual ZSL (GRCZSL). The proposed method endows traditional ZSL to learn from streaming data and acquire new knowledge without forgetting the previous tasks' gained experience. We handle catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen classes, which have appeared in the earlier tasks. These synthetic samples are synthesized using the trained conditional variational autoencoder (VAE) over the immediate past task. Moreover, we only require the current and immediate previous VAE at any time for training and testing. The proposed GRZSL method is developed for a single-head setting of continual learning, simulating a real-world problem setting. In this setting, task identity is given during training but unavailable during testing. GRCZSL performance is evaluated on five benchmark datasets for the generalized setup of ZSL with fixed and dynamic (incremental class) settings of continual learning. The existing class setting presented recently in the literature is not suitable for a class-incremental setting. Therefore, this paper proposes a new setting to address this issue. Experimental results show that the proposed method significantly outperforms the baseline and the state-of-the-art method and makes it more suitable for real-world applications.

</p>
</details>

<details><summary><b>Efficient MPI-based Communication for GPU-Accelerated Dask Applications</b>
<a href="https://arxiv.org/abs/2101.08878">arxiv:2101.08878</a>
&#x1F4C8; 4 <br>
<p>Aamir Shafi, Jahanzeb Maqbool Hashmi, Hari Subramoni, Dhabaleswar K. Panda</p></summary>
<p>

**Abstract:** Dask is a popular parallel and distributed computing framework, which rivals Apache Spark to enable task-based scalable processing of big data. The Dask Distributed library forms the basis of this computing engine and provides support for adding new communication devices. It currently has two communication devices: one for TCP and the other for high-speed networks using UCX-Py -- a Cython wrapper to UCX. This paper presents the design and implementation of a new communication backend for Dask -- called MPI4Dask -- that is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits mpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message Passing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous I/O communication coroutines, which are non-blocking concurrent operations defined using the async/await keywords from the Python's asyncio framework. Our latency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x for 1 Byte message and 4x for large messages (2 MBytes and beyond) respectively. We also conduct comparative performance evaluation of MPI4Dask with UCX using two benchmark applications: 1) sum of cuPy array with its transpose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of the two applications by an average of 3.47x and 3.11x respectively on an in-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We also perform scalability analysis of MPI4Dask against UCX for these applications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32 NVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution time for cuPy and cuDF applications by an average of 1.71x and 2.91x respectively for 1-32 Dask workers on the Frontera (GPU) system.

</p>
</details>

<details><summary><b>Crossbreeding in Random Forest</b>
<a href="https://arxiv.org/abs/2101.08585">arxiv:2101.08585</a>
&#x1F4C8; 4 <br>
<p>Abolfazl Nadi, Hadi Moradi, Khalil Taheri</p></summary>
<p>

**Abstract:** Ensemble learning methods are designed to benefit from multiple learning algorithms for better predictive performance. The tradeoff of this improved performance is slower speed and larger size of ensemble learning systems compared to single learning systems. In this paper, we present a novel approach to deal with this problem in Random Forest (RF) as one of the most powerful ensemble methods. The method is based on crossbreeding of the best tree branches to increase the performance of RF in space and speed while keeping the performance in the classification measures. The proposed approach has been tested on a group of synthetic and real datasets and compared to the standard RF approach. Several evaluations have been conducted to determine the effects of the Crossbred RF (CRF) on the accuracy and the number of trees in a forest. The results show better performance of CRF compared to RF.

</p>
</details>

<details><summary><b>Content-Based Textual File Type Detection at Scale</b>
<a href="https://arxiv.org/abs/2101.08508">arxiv:2101.08508</a>
&#x1F4C8; 4 <br>
<p>Francesca Del Bonifro, Maurizio Gabbrielli, Stefano Zacchiroli</p></summary>
<p>

**Abstract:** Programming language detection is a common need in the analysis of large source code bases. It is supported by a number of existing tools that rely on several features, and most notably file extensions, to determine file types. We consider the problem of accurately detecting the type of files commonly found in software code bases, based solely on textual file content. Doing so is helpful to classify source code that lack file extensions (e.g., code snippets posted on the Web or executable scripts), to avoid misclassifying source code that has been recorded with wrong or uncommon file extensions, and also shed some light on the intrinsic recognizability of source code files. We propose a simple model that (a) use a language-agnostic word tokenizer for textual files, (b) group tokens in 1-/2-grams, (c) build feature vectors based on N-gram frequencies, and (d) use a simple fully connected neural network as classifier. As training set we use textual files extracted from GitHub repositories with at least 1000 stars, using existing file extensions as ground truth. Despite its simplicity the proposed model reaches 85% in our experiments for a relatively high number of recognized classes (more than 130 file types).

</p>
</details>

<details><summary><b>What we are is more than what we do</b>
<a href="https://arxiv.org/abs/2102.04219">arxiv:2102.04219</a>
&#x1F4C8; 3 <br>
<p>Larissa Albantakis, Giulio Tononi</p></summary>
<p>

**Abstract:** If we take the subjective character of consciousness seriously, consciousness becomes a matter of "being" rather than "doing". Because "doing" can be dissociated from "being", functional criteria alone are insufficient to decide whether a system possesses the necessary requirements for being a physical substrate of consciousness. The dissociation between "being" and "doing" is most salient in artificial general intelligence, which may soon replicate any human capacity: computers can perform complex functions (in the limit resembling human behavior) in the absence of consciousness. Complex behavior becomes meaningless if it is not performed by a conscious being.

</p>
</details>

<details><summary><b>Snapshot Hyperspectral Imaging Based on Weighted High-order Singular Value Regularization</b>
<a href="https://arxiv.org/abs/2101.08923">arxiv:2101.08923</a>
&#x1F4C8; 3 <br>
<p>Niankai Cheng, Hua Huang, Lei Zhang, Lizhi Wang</p></summary>
<p>

**Abstract:** Snapshot hyperspectral imaging can capture the 3D hyperspectral image (HSI) with a single 2D measurement and has attracted increasing attention recently. Recovering the underlying HSI from the compressive measurement is an ill-posed problem and exploiting the image prior is essential for solving this ill-posed problem. However, existing reconstruction methods always start from modeling image prior with the 1D vector or 2D matrix and cannot fully exploit the structurally spectral-spatial nature in 3D HSI, thus leading to a poor fidelity. In this paper, we propose an effective high-order tensor optimization based method to boost the reconstruction fidelity for snapshot hyperspectral imaging. We first build high-order tensors by exploiting the spatial-spectral correlation in HSI. Then, we propose a weight high-order singular value regularization (WHOSVR) based low-rank tensor recovery model to characterize the structure prior of HSI. By integrating the structure prior in WHOSVR with the system imaging process, we develop an optimization framework for HSI reconstruction, which is finally solved via the alternating minimization algorithm. Extensive experiments implemented on two representative systems demonstrate that our method outperforms state-of-the-art methods.

</p>
</details>

<details><summary><b>Applications of artificial intelligence in drug development using real-world data</b>
<a href="https://arxiv.org/abs/2101.08904">arxiv:2101.08904</a>
&#x1F4C8; 3 <br>
<p>Zhaoyi Chen, Xiong Liu, William Hogan, Elizabeth Shenkman, Jiang Bian</p></summary>
<p>

**Abstract:** The US Food and Drug Administration (FDA) has been actively promoting the use of real-world data (RWD) in drug development. RWD can generate important real-world evidence reflecting the real-world clinical environment where the treatments are used. Meanwhile, artificial intelligence (AI), especially machine- and deep-learning (ML/DL) methods, have been increasingly used across many stages of the drug development process. Advancements in AI have also provided new strategies to analyze large, multidimensional RWD. Thus, we conducted a rapid review of articles from the past 20 years, to provide an overview of the drug development studies that use both AI and RWD. We found that the most popular applications were adverse event detection, trial recruitment, and drug repurposing. Here, we also discuss current research gaps and future opportunities.

</p>
</details>

<details><summary><b>A Spike Learning System for Event-driven Object Recognition</b>
<a href="https://arxiv.org/abs/2101.08850">arxiv:2101.08850</a>
&#x1F4C8; 3 <br>
<p>Shibo Zhou, Wei Wang, Xiaohua Li, Zhanpeng Jin</p></summary>
<p>

**Abstract:** Event-driven sensors such as LiDAR and dynamic vision sensor (DVS) have found increased attention in high-resolution and high-speed applications. A lot of work has been conducted to enhance recognition accuracy. However, the essential topic of recognition delay or time efficiency is largely under-explored. In this paper, we present a spiking learning system that uses the spiking neural network (SNN) with a novel temporal coding for accurate and fast object recognition. The proposed temporal coding scheme maps each event's arrival time and data into SNN spike time so that asynchronously-arrived events are processed immediately without delay. The scheme is integrated nicely with the SNN's asynchronous processing capability to enhance time efficiency. A key advantage over existing systems is that the event accumulation time for each recognition task is determined automatically by the system rather than pre-set by the user. The system can finish recognition early without waiting for all the input events. Extensive experiments were conducted over a list of 7 LiDAR and DVS datasets. The results demonstrated that the proposed system had state-of-the-art recognition accuracy while achieving remarkable time efficiency. Recognition delay was shown to reduce by 56.3% to 91.7% in various experiment settings over the popular KITTI dataset.

</p>
</details>

<details><summary><b>Model-based Policy Search for Partially Measurable Systems</b>
<a href="https://arxiv.org/abs/2101.08740">arxiv:2101.08740</a>
&#x1F4C8; 3 <br>
<p>Fabio Amadio, Alberto Dalla Libera, Ruggero Carli, Daniel Nikovski, Diego Romeres</p></summary>
<p>

**Abstract:** In this paper, we propose a Model-Based Reinforcement Learning (MBRL) algorithm for Partially Measurable Systems (PMS), i.e., systems where the state can not be directly measured, but must be estimated through proper state observers. The proposed algorithm, named Monte Carlo Probabilistic Inference for Learning COntrol for Partially Measurable Systems (MC-PILCO4PMS), relies on Gaussian Processes (GPs) to model the system dynamics, and on a Monte Carlo approach to update the policy parameters. W.r.t. previous GP-based MBRL algorithms, MC-PILCO4PMS models explicitly the presence of state observers during policy optimization, allowing to deal PMS. The effectiveness of the proposed algorithm has been tested both in simulation and in two real systems.

</p>
</details>

<details><summary><b>PalmTree: Learning an Assembly Language Model for Instruction Embedding</b>
<a href="https://arxiv.org/abs/2103.03809">arxiv:2103.03809</a>
&#x1F4C8; 2 <br>
<p>Xuezixiang Li, Qu Yu, Heng Yin</p></summary>
<p>

**Abstract:** Deep learning has demonstrated its strengths in numerous binary analysis tasks, including function boundary detection, binary code search, function prototype inference, value set analysis, etc. When applying deep learning to binary analysis tasks, we need to decide what input should be fed into the neural network model. More specifically, we need to answer how to represent an instruction in a fixed-length vector. The idea of automatically learning instruction representations is intriguing, however the existing schemes fail to capture the unique characteristics of disassembly. These schemes ignore the complex intra-instruction structures and mainly rely on control flow in which the contextual information is noisy and can be influenced by compiler optimizations.
  In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora. PalmTree utilizes three pre-training tasks to capture various characteristics of assembly language. These training tasks overcome the problems in existing schemes, thus can help to generate high-quality representations. We conduct both intrinsic and extrinsic evaluations, and compare PalmTree with other instruction embedding schemes. PalmTree has the best performance for intrinsic metrics, and outperforms the other instruction embedding schemes for all downstream tasks.

</p>
</details>

<details><summary><b>Challenges Encountered in Turkish Natural Language Processing Studies</b>
<a href="https://arxiv.org/abs/2101.11436">arxiv:2101.11436</a>
&#x1F4C8; 2 <br>
<p>Kadir Tohma, Yakup Kutlu</p></summary>
<p>

**Abstract:** Natural language processing is a branch of computer science that combines artificial intelligence with linguistics. It aims to analyze a language element such as writing or speaking with software and convert it into information. Considering that each language has its own grammatical rules and vocabulary diversity, the complexity of the studies in this field is somewhat understandable. For instance, Turkish is a very interesting language in many ways. Examples of this are agglutinative word structure, consonant/vowel harmony, a large number of productive derivational morphemes (practically infinite vocabulary), derivation and syntactic relations, a complex emphasis on vocabulary and phonological rules. In this study, the interesting features of Turkish in terms of natural language processing are mentioned. In addition, summary info about natural language processing techniques, systems and various sources developed for Turkish are given.

</p>
</details>

<details><summary><b>Analysis of Basic Emotions in Texts Based on BERT Vector Representation</b>
<a href="https://arxiv.org/abs/2101.11433">arxiv:2101.11433</a>
&#x1F4C8; 2 <br>
<p>A. Artemov, A. Veselovskiy, I. Khasenevich, I. Bolokhov</p></summary>
<p>

**Abstract:** In the following paper the authors present a GAN-type model and the most important stages of its development for the task of emotion recognition in text. In particular, we propose an approach for generating a synthetic dataset of all possible emotions combinations based on manually labelled incomplete data.

</p>
</details>

<details><summary><b>Toxicity Detection in Drug Candidates using Simplified Molecular-Input Line-Entry System</b>
<a href="https://arxiv.org/abs/2101.10831">arxiv:2101.10831</a>
&#x1F4C8; 2 <br>
<p>Mriganka Nath, Subhasish Goswami</p></summary>
<p>

**Abstract:** The need for analysis of toxicity in new drug candidates and the requirement of doing it fast have asked the consideration of scientists towards the use of artificial intelligence tools to examine toxicity levels and to develop models to a degree where they can be used commercially to measure toxicity levels efficiently in upcoming drugs. Artificial Intelligence based models can be used to predict the toxic nature of a chemical using Quantitative Structure Activity Relationship techniques. Convolutional Neural Network models have demonstrated great outcomes in predicting the qualitative analysis of chemicals in order to determine the toxicity. This paper goes for the study of Simplified Molecular Input Line-Entry System (SMILES) as a parameter to develop Long short term memory (LSTM) based models in order to examine the toxicity of a molecule and the degree to which the need can be fulfilled for practical use alongside its future outlooks for the purpose of real world applications.

</p>
</details>

<details><summary><b>A scalable approach for developing clinical risk prediction applications in different hospitals</b>
<a href="https://arxiv.org/abs/2101.10268">arxiv:2101.10268</a>
&#x1F4C8; 2 <br>
<p>Hong Sun, Kristof Depraetere, Laurent Meesseman, Jos De Roo, Martijn Vanbiervliet, Jos De Baerdemaeker, Herman Muys, Vera von Dossow, Nikolai Hulde, Ralph Szymanowsky</p></summary>
<p>

**Abstract:** Objective: Machine learning algorithms are now widely used in predicting acute events for clinical applications. While most of such prediction applications are developed to predict the risk of a particular acute event at one hospital, few efforts have been made in extending the developed solutions to other events or to different hospitals. We provide a scalable solution to extend the process of clinical risk prediction model development of multiple diseases and their deployment in different Electronic Health Records (EHR) systems.
  Materials and Methods: We defined a generic process for clinical risk prediction model development. A calibration tool has been created to automate the model generation process. We applied the model calibration process at four hospitals, and generated risk prediction models for delirium, sepsis and acute kidney injury (AKI) respectively at each of these hospitals.
  Results: The delirium risk prediction models achieved area under the receiver-operating characteristic curve (AUROC) ranging from 0.82 to 0.95 over different stages of a hospital stay on the test datasets of the four hospitals. The sepsis models achieved AUROC ranging from 0.88 to 0.95, and the AKI models achieved AUROC ranging from 0.85 to 0.92.
  Discussion: The scalability discussed in this paper is based on building common data representations (syntactic interoperability) between EHRs stored in different hospitals. Semantic interoperability, a more challenging requirement that different EHRs share the same meaning of data, e.g. a same lab coding system, is not mandated with our approach.
  Conclusions: Our study describes a method to develop and deploy clinical risk prediction models in a scalable way. We demonstrate its feasibility by developing risk prediction models for three diseases across four hospitals.

</p>
</details>

<details><summary><b>Superiorities of Deep Extreme Learning Machines against Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2101.10265">arxiv:2101.10265</a>
&#x1F4C8; 2 <br>
<p>Gokhan Altan, Yakup Kutlu</p></summary>
<p>

**Abstract:** Deep Learning (DL) is a machine learning procedure for artificial intelligence that analyzes the input data in detail by increasing neuron sizes and number of the hidden layers. DL has a popularity with the common improvements on the graphical processing unit capabilities. Increasing number of the neuron sizes at each layer and hidden layers is directly related to the computation time and training speed of the classifier models. The classification parameters including neuron weights, output weights, and biases need to be optimized for obtaining an optimum model. Most of the popular DL algorithms require long training times for optimization of the parameters with feature learning progresses and back-propagated training procedures. Reducing the training time and providing a real-time decision system are the basic focus points of the novel approaches. Deep Extreme Learning machines (Deep ELM) classifier model is one of the fastest and effective way to meet fast classification problems. In this study, Deep ELM model, its superiorities and weaknesses are discussed, the problems that are more suitable for the classifiers against Convolutional neural network based DL algorithms.

</p>
</details>

<details><summary><b>Generative Autoencoder Kernels on Deep Learning for Brain Activity Analysis</b>
<a href="https://arxiv.org/abs/2101.10263">arxiv:2101.10263</a>
&#x1F4C8; 2 <br>
<p>Gokhan Altan, Yakup Kutlu</p></summary>
<p>

**Abstract:** Deep Learning (DL) is a two-step classification model that consists feature learning, generating feature representations using unsupervised ways and the supervised learning stage at the last step of model using at least two hidden layers on the proposed structures by fully connected layers depending on of the artificial neural networks. The optimization of the predefined classification parameters for the supervised models eases reaching the global optimality with exact zero training error. The autoencoder (AE) models are the highly generalized ways of the unsupervised stages for the DL to define the output weights of the hidden neurons with various representations. As alternatively to the conventional Extreme Learning Machines (ELM) AE, Hessenberg decomposition-based ELM autoencoder (HessELM-AE) is a novel kernel to generate different presentations of the input data within the intended sizes of the models. The aim of the study is analyzing the performance of the novel Deep AE kernel for clinical availability on electroencephalogram (EEG) with stroke patients. The slow cortical potentials (SCP) training in stroke patients during eight neurofeedback sessions were analyzed using Hilbert-Huang Transform. The statistical features of different frequency modulations were fed into the Deep ELM model for generative AE kernels. The novel Deep ELM-AE kernels have discriminated the brain activity with high classification performances for positivity and negativity tasks in stroke patients.

</p>
</details>

<details><summary><b>Machine Learning Based Early Fire Detection System using a Low-Cost Drone</b>
<a href="https://arxiv.org/abs/2101.09362">arxiv:2101.09362</a>
&#x1F4C8; 2 <br>
<p>Ayşegül Yanık, Mehmet Serdar Güzel, Mertkan Yanık, Erkan Bostancı</p></summary>
<p>

**Abstract:** This paper proposes a new machine learning based system for forest fire earlier detection in a low-cost and accurate manner. Accordingly, it is aimed to bring a new and definite perspective to visual detection in forest fires. A drone is constructed for this purpose. The microcontroller in the system has been programmed by training with deep learning methods, and the unmanned aerial vehicle has been given the ability to recognize the smoke, the earliest sign of fire detection. The common problem in the prevalent algorithms used in fire detection is the high false alarm and overlook rates. Confirming the result obtained from the visualization with an additional supervision stage will increase the reliability of the system as well as guarantee the accuracy of the result. Due to the mobile vision ability of the unmanned aerial vehicle, the data can be controlled from any point of view clearly and continuously. System performance are validated by conducting experiments in both simulation and physical environments.

</p>
</details>

<details><summary><b>Enriching Non-Autoregressive Transformer with Syntactic and SemanticStructures for Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2101.08942">arxiv:2101.08942</a>
&#x1F4C8; 2 <br>
<p>Ye Liu, Yao Wan, Jian-Guo Zhang, Wenting Zhao, Philip S. Yu</p></summary>
<p>

**Abstract:** The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in the existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structures of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several state-of-the-art non-autoregressive models.

</p>
</details>

<details><summary><b>SGA: A Robust Algorithm for Partial Recovery of Tree-Structured Graphical Models with Noisy Samples</b>
<a href="https://arxiv.org/abs/2101.08917">arxiv:2101.08917</a>
&#x1F4C8; 2 <br>
<p>Anshoo Tandon, Aldric H. J. Yuan, Vincent Y. F. Tan</p></summary>
<p>

**Abstract:** We consider learning Ising tree models when the observations from the nodes are corrupted by independent but non-identically distributed noise with unknown statistics. Katiyar et al. (2020) showed that although the exact tree structure cannot be recovered, one can recover a partial tree structure; that is, a structure belonging to the equivalence class containing the true tree. This paper presents a systematic improvement of Katiyar et al. (2020). First, we present a novel impossibility result by deriving a bound on the necessary number of samples for partial recovery. Second, we derive a significantly improved sample complexity result in which the dependence on the minimum correlation $ρ_{\min}$ is $ρ_{\min}^{-8}$ instead of $ρ_{\min}^{-24}$. Finally, we propose Symmetrized Geometric Averaging (SGA), a more statistically robust algorithm for partial tree recovery. We provide error exponent analyses and extensive numerical results on a variety of trees to show that the sample complexity of SGA is significantly better than the algorithm of Katiyar et al. (2020). SGA can be readily extended to Gaussian models and is shown via numerical experiments to be similarly superior.

</p>
</details>

<details><summary><b>A Two-Stage Deep Learning Detection Classifier for the ATLAS Asteroid Survey</b>
<a href="https://arxiv.org/abs/2101.08912">arxiv:2101.08912</a>
&#x1F4C8; 2 <br>
<p>Amandin Chyba Rabeendran, Larry Denneau</p></summary>
<p>

**Abstract:** In this paper we present a two-step neural network model to separate detections of solar system objects from optical and electronic artifacts in data obtained with the "Asteroid Terrestrial-impact Last Alert System" (ATLAS), a near-Earth asteroid sky survey system [arXiv:1802.00879]. A convolutional neural network [arXiv:1807.10912] is used to classify small "postage-stamp" images of candidate detections of astronomical sources into eight classes, followed by a multi-layered perceptron that provides a probability that a temporal sequence of four candidate detections represents a real astronomical source. The goal of this work is to reduce the time delay between Near-Earth Object (NEO) detections and submission to the Minor Planet Center. Due to the rare and hazardous nature of NEOs [Harris and D'Abramo, 2015], a low false negative rate is a priority for the model. We show that the model reaches 99.6\% accuracy on real asteroids in ATLAS data with a 0.4\% false negative rate. Deployment of this model on ATLAS has reduced the amount of NEO candidates that astronomers must screen by 90%, thereby bringing ATLAS one step closer to full autonomy.

</p>
</details>

<details><summary><b>Single Neuron Segmentation using Graph-based Global Reasoning with Auxiliary Skeleton Loss from 3D Optical Microscope Images</b>
<a href="https://arxiv.org/abs/2101.08910">arxiv:2101.08910</a>
&#x1F4C8; 2 <br>
<p>Heng Wang, Yang Song, Chaoyi Zhang, Jianhui Yu, Siqi Liu, Hanchuan Peng, Weidong Cai</p></summary>
<p>

**Abstract:** One of the critical steps in improving accurate single neuron reconstruction from three-dimensional (3D) optical microscope images is the neuronal structure segmentation. However, they are always hard to segment due to the lack in quality. Despite a series of attempts to apply convolutional neural networks (CNNs) on this task, noise and disconnected gaps are still challenging to alleviate with the neglect of the non-local features of graph-like tubular neural structures. Hence, we present an end-to-end segmentation network by jointly considering the local appearance and the global geometry traits through graph reasoning and a skeleton-based auxiliary loss. The evaluation results on the Janelia dataset from the BigNeuron project demonstrate that our proposed method exceeds the counterpart algorithms in performance.

</p>
</details>

<details><summary><b>Analyzing Epistemic and Aleatoric Uncertainty for Drusen Segmentation in Optical Coherence Tomography Images</b>
<a href="https://arxiv.org/abs/2101.08888">arxiv:2101.08888</a>
&#x1F4C8; 2 <br>
<p>Tinu Theckel Joy, Suman Sedai, Rahil Garnavi</p></summary>
<p>

**Abstract:** Age-related macular degeneration (AMD) is one of the leading causes of permanent vision loss in people aged over 60 years. Accurate segmentation of biomarkers such as drusen that points to the early stages of AMD is crucial in preventing further vision impairment. However, segmenting drusen is extremely challenging due to their varied sizes and appearances, low contrast and noise resemblance. Most existing literature, therefore, have focused on size estimation of drusen using classification, leaving the challenge of accurate segmentation less tackled. Additionally, obtaining the pixel-wise annotations is extremely costly and such labels can often be noisy, suffering from inter-observer and intra-observer variability. Quantification of uncertainty associated with segmentation tasks offers principled measures to inspect the segmentation output. Realizing its utility in identifying erroneous segmentation and the potential applications in clinical decision making, here we develop a U-Net based drusen segmentation model and quantify the segmentation uncertainty. We investigate epistemic and aleatoric uncertainty capturing model confidence and data uncertainty respectively. We present segmentation results and show how uncertainty can help formulate robust evaluation strategies. We visually inspect the pixel-wise uncertainty and segmentation results on test images. We finally analyze the correlation between segmentation uncertainty and accuracy. Our results demonstrate the utility of leveraging uncertainties in developing and explaining segmentation models for medical image analysis.

</p>
</details>

<details><summary><b>Bridging the gap between Human Action Recognition and Online Action Detection</b>
<a href="https://arxiv.org/abs/2101.08851">arxiv:2101.08851</a>
&#x1F4C8; 2 <br>
<p>Alban Main de Boissiere, Rita Noumeir</p></summary>
<p>

**Abstract:** Action recognition, early prediction, and online action detection are complementary disciplines that are often studied independently. Most online action detection networks use a pre-trained feature extractor, which might not be optimal for its new task. We address the task-specific feature extraction with a teacher-student framework between the aforementioned disciplines, and a novel training strategy. Our network, Online Knowledge Distillation Action Detection network (OKDAD), embeds online early prediction and online temporal segment proposal subnetworks in parallel. Low interclass and high intraclass similarity are encouraged during teacher training. Knowledge distillation to the OKDAD network is ensured via layer reuse and cosine similarity between teacher-student feature vectors. Layer reuse and similarity learning significantly improve our baseline which uses a generic feature extractor. We evaluate our framework on infrared videos from two popular datasets, NTU RGB+D (action recognition, early prediction) and PKU MMD (action detection). Unlike previous attempts on those datasets, our student networks perform without any knowledge of the future. Even with this added difficulty, we achieve state-of-the-art results on both datasets. Moreover, our networks use infrared from RGB-D cameras, which we are the first to use for online action detection, to our knowledge.

</p>
</details>

<details><summary><b>Occlusion Handling in Generic Object Detection: A Review</b>
<a href="https://arxiv.org/abs/2101.08845">arxiv:2101.08845</a>
&#x1F4C8; 2 <br>
<p>Kaziwa Saleh, Sándor Szénási, Zoltán Vámossy</p></summary>
<p>

**Abstract:** The significant power of deep learning networks has led to enormous development in object detection. Over the last few years, object detector frameworks have achieved tremendous success in both accuracy and efficiency. However, their ability is far from that of human beings due to several factors, occlusion being one of them. Since occlusion can happen in various locations, scale, and ratio, it is very difficult to handle. In this paper, we address the challenges in occlusion handling in generic object detection in both outdoor and indoor scenes, then we refer to the recent works that have been carried out to overcome these challenges. Finally, we discuss some possible future directions of research.

</p>
</details>

<details><summary><b>Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma</b>
<a href="https://arxiv.org/abs/2101.08757">arxiv:2101.08757</a>
&#x1F4C8; 2 <br>
<p>Chao Li, Wenjian Huang, Xi Chen, Yiran Wei, Stephen J. Price, Carola-Bibiane Schönlieb</p></summary>
<p>

**Abstract:** We present an Expectation-Maximization (EM) Regularized Deep Learning (EMReDL) model for weakly supervised tumor segmentation. The proposed framework is tailored to glioblastoma, a type of malignant tumor characterized by its diffuse infiltration into the surrounding brain tissue, which poses significant challenge to treatment target and tumor burden estimation using conventional structural MRI. Although physiological MRI provides more specific information regarding tumor infiltration, the relatively low resolution hinders a precise full annotation. This has motivated us to develop a weakly supervised deep learning solution that exploits the partial labelled tumor regions.
  EMReDL contains two components: a physiological prior prediction model and EM-regularized segmentation model. The physiological prior prediction model exploits the physiological MRI by training a classifier to generate a physiological prior map. This map is passed to the segmentation model for regularization using the EM algorithm. We evaluated the model on a glioblastoma dataset with the pre-operative multiparametric and recurrence MRI available. EMReDL showed to effectively segment the infiltrated tumor from the partially labelled region of potential infiltration. The segmented core tumor and infiltrated tumor demonstrated high consistency with the tumor burden labelled by experts. The performance comparisons showed that EMReDL achieved higher accuracy than published state-of-the-art models. On MR spectroscopy, the segmented region displayed more aggressive features than other partial labelled region. The proposed model can be generalized to other segmentation tasks that rely on partial labels, with the CNN architecture flexible in the framework.

</p>
</details>

<details><summary><b>Artificial Intelligence based Sensor Data Analytics Framework for Remote Electricity Network Condition Monitoring</b>
<a href="https://arxiv.org/abs/2102.03356">arxiv:2102.03356</a>
&#x1F4C8; 1 <br>
<p>Tharmakulasingam Sirojan</p></summary>
<p>

**Abstract:** Rural electrification demands the use of inexpensive technologies such as single wire earth return (SWER) networks. There is a steadily growing energy demand from remote consumers, and the capacity of existing lines may become inadequate soon. Furthermore, high impedance arcing faults (HIF) from SWER lines can cause catastrophic bushfires such as the 2009 Black Saturday event. As a solution, reliable remote electricity networks can be established through breaking the existing systems down into microgrids, and existing SWER lines can be utilised to interconnect those microgrids. The development of such reliable networks with better energy demand management will rely on having an integrated network-wide condition monitoring system. As the first contribution of this thesis, a distributed online monitoring platform is developed that incorporates power quality monitoring, real-time HIF identification and transient classification in SWER network. Artificial Intelligence (AI) based techniques are developed to classify faults and transients. The proposed approach demonstrates higher HIF detection accuracy (98.67%) and reduced detection latency (115.2 ms). Secondly, a remote consumer load identification methodology is developed to detect the load type from its transients. An edge computing-based architecture is proposed to facilitate the high-frequency analysis for load identification. The proposed approach is evaluated in real-time, and it achieves an average accuracy of 98% in identifying different loads. Finally, a deep neural network-based energy disaggregation framework is developed to separate the load specific energy usage from an aggregated signal. The proposed framework is evaluated using a real-world data set. It improves the signal aggregate error by 44% and mean aggregate error by 19% in comparison with the state-of-the-art techniques.

</p>
</details>

<details><summary><b>Online LDA based brain-computer interface system to aid disabled people</b>
<a href="https://arxiv.org/abs/2101.11435">arxiv:2101.11435</a>
&#x1F4C8; 1 <br>
<p>Apdullah Yayik, Yakup Kutlu</p></summary>
<p>

**Abstract:** This paper aims to develop brain-computer interface system based on electroencephalography that can aid disabled people in daily life. The system relies on one of the most effective event-related potential wave, P300, which can be elicited by oddball paradigm. Developed application has a basic interaction tool that enables disabled people to convey their needs to other people selecting related objects. These objects pseudo-randomly flash in a visual interface on computer screen. The user must focus on related object to convey desired needs. The system can convey desired needs correctly by detecting P300 wave in acquired 14-channel EEG signal and classifying using linear discriminant analysis classifier just in 15 seconds. Experiments have been carried out on 19 volunteers to validate developed BCI system. As a result, accuracy rate of 90.83% is achieved in online performance

</p>
</details>

<details><summary><b>Analysis of Relation between Motor Activity and Imaginary EEG Records</b>
<a href="https://arxiv.org/abs/2101.10215">arxiv:2101.10215</a>
&#x1F4C8; 1 <br>
<p>Enver Kaan Alpturk, Yakup Kutlu</p></summary>
<p>

**Abstract:** Electroencephalography (EEG) signals signals are often used to learn about brain structure and to learn what thinking. EEG signals can be easily affected by external factors. For this reason, they should be applied various pre-process during their analysis. In this study, it is used the EEG signals received from 109 subjects when opening and closing their right or left fists and performing hand and foot movements and imagining the same movements. The relationship between motor activities and imaginary of that motor activities were investigated. Algorithms with high performance rates have been used for feature extraction , selection and classification using the nearest neighbour algorithm.

</p>
</details>

<details><summary><b>Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir Computing</b>
<a href="https://arxiv.org/abs/2101.08884">arxiv:2101.08884</a>
&#x1F4C8; 1 <br>
<p>Matthew Denton, Herman Schmit</p></summary>
<p>

**Abstract:** Reservoir computing systems rely on the recurrent multiplication of a very large, sparse, fixed matrix. We argue that direct spatial implementation of these fixed matrices minimizes the work performed in the computation, and allows for significant reduction in latency and power through constant propagation and logic minimization. Bit-serial arithmetic enables massive static matrices to be implemented. We present the structure of our bit-serial matrix multiplier, and evaluate using canonical signed digit representation to further reduce logic utilization. We have implemented these matrices on a large FPGA and provide a cost model that is simple and extensible. These FPGA implementations, on average, reduce latency by 50x up to 86x versus GPU libraries. Comparing against a recent sparse DNN accelerator, we measure a 4.1x to 47x reduction in latency depending on matrix dimension and sparsity. Throughput of the FPGA solution is also competitive for a wide range of matrix dimensions and batch sizes. Finally, we discuss ways these techniques could be deployed in ASICs, making them applicable for dynamic sparse matrix computations.

</p>
</details>

<details><summary><b>GPU-Accelerated Optimizer-Aware Evaluation of Submodular Exemplar Clustering</b>
<a href="https://arxiv.org/abs/2101.08763">arxiv:2101.08763</a>
&#x1F4C8; 1 <br>
<p>Philipp-Jan Honysz, Sebastian Buschjäger, Katharina Morik</p></summary>
<p>

**Abstract:** The optimization of submodular functions constitutes a viable way to perform clustering. Strong approximation guarantees and feasible optimization w.r.t. streaming data make this clustering approach favorable. Technically, submodular functions map subsets of data to real values, which indicate how "representative" a specific subset is. Optimal sets might then be used to partition the data space and to infer clusters. Exemplar-based clustering is one of the possible submodular functions, but suffers from high computational complexity. However, for practical applications, the particular real-time or wall-clock run-time is decisive. In this work, we present a novel way to evaluate this particular function on GPUs, which keeps the necessities of optimizers in mind and reduces wall-clock run-time. To discuss our GPU algorithm, we investigated both the impact of different run-time critical problem properties, like data dimensionality and the number of data points in a subset, and the influence of required floating-point precision. In reproducible experiments, our GPU algorithm was able to achieve competitive speedups of up to 72x depending on whether multi-threaded computation on CPUs was used for comparison and the type of floating-point precision required. Half-precision GPU computation led to large speedups of up to 452x compared to single-precision, single-thread CPU computations.

</p>
</details>

<details><summary><b>Adversarial Machine Learning for Flooding Attacks on 5G Radio Access Network Slicing</b>
<a href="https://arxiv.org/abs/2101.08724">arxiv:2101.08724</a>
&#x1F4C8; 1 <br>
<p>Yi Shi, Yalin E. Sagduyu</p></summary>
<p>

**Abstract:** Network slicing manages network resources as virtual resource blocks (RBs) for the 5G Radio Access Network (RAN). Each communication request comes with quality of experience (QoE) requirements such as throughput and latency/deadline, which can be met by assigning RBs, communication power, and processing power to the request. For a completed request, the achieved reward is measured by the weight (priority) of this request. Then, the reward is maximized over time by allocating resources, e.g., with reinforcement learning (RL). In this paper, we introduce a novel flooding attack on 5G network slicing, where an adversary generates fake network slicing requests to consume the 5G RAN resources that would be otherwise available to real requests. The adversary observes the spectrum and builds a surrogate model on the network slicing algorithm through RL that decides on how to craft fake requests to minimize the reward of real requests over time. We show that the portion of the reward achieved by real requests may be much less than the reward that would be achieved when there was no attack. We also show that this flooding attack is more effective than other benchmark attacks such as random fake requests and fake requests with the minimum resource requirement (lowest QoE requirement). Fake requests may be detected due to their fixed weight. As an attack enhancement, we present schemes to randomize weights of fake requests and show that it is still possible to reduce the reward of real requests while maintaining the balance on weight distributions.

</p>
</details>

<details><summary><b>Variable Division and Optimization for Constrained Multiobjective Portfolio Problems</b>
<a href="https://arxiv.org/abs/2101.08552">arxiv:2101.08552</a>
&#x1F4C8; 1 <br>
<p>Yi Chen, Aimin Zhou</p></summary>
<p>

**Abstract:** Variable division and optimization (D\&O) is a frequently utilized algorithm design paradigm in Evolutionary Algorithms (EAs). A D\&O EA divides a variable into partial variables and then optimize them respectively. A complicated problem is thus divided into simple subtasks. For example, a variable of portfolio problem can be divided into two partial variables, i.e. the selection of assets and the allocation of capital. Thereby, we optimize these two partial variables respectively. There is no formal discussion about how are the partial variables iteratively optimized and why can it work for both single- and multi-objective problems in D\&O. In this paper, this gap is filled. According to the discussion, an elitist selection method for partial variables in multiobjective problems is developed. Then this method is incorporated into the Decomposition-Based Multiobjective Evolutionary Algorithm (D\&O-MOEA/D). With the help of a mathematical programming optimizer, it is achieved on the constrained multiobjective portfolio problems. In the empirical study, D\&O-MOEA/D is implemented for 20 instances and recent Chinese stock markets. The results show the superiority and versatility of D\&O-MOEA/D on large-scale instances while the performance of it on small-scale problems is also not bad. The former targets convergence towards the Pareto front and the latter helps promote diversity among the non-dominated solutions during the search process.

</p>
</details>

<details><summary><b>Multimedia Respiratory Database (RespiratoryDatabase@TR): Auscultation Sounds and Chest X-rays</b>
<a href="https://arxiv.org/abs/2101.10946">arxiv:2101.10946</a>
&#x1F4C8; 0 <br>
<p>Gokhan Altan, Yakup Kutlu, Yusuf Garbi, Adnan Ozhan Pekmezci, Serkan Nural</p></summary>
<p>

**Abstract:** Auscultation is a method for diagnosis of especially internal medicine diseases such as cardiac, pulmonary and cardio-pulmonary by listening the internal sounds from the body parts. It is the simplest and the most common physical examination in the assessment processes of the clinical skills. In this study, the lung and heart sounds are recorded synchronously from left and right sides of posterior and anterior chest wall and back using two digital stethoscopes in Antakya State Hospital. The chest X-rays and the pulmonary function test variables and spirometric curves, the St. George respiratory questionnaire (SGRQ-C) are collected as multimedia and clinical functional analysis variables of the patients. The 4 channels of heart sounds are focused on aortic, pulmonary, tricuspid and mitral areas. The 12 channels of lung sounds are focused on upper lung, middle lung, lower lung and costophrenic angle areas of posterior and anterior sides of the chest. The recordings are validated and labelled by two pulmonologists evaluating the collected chest x-ray, PFT and auscultation sounds of the subjects. The database consists of 30 healthy subjects and 45 subjects with pulmonary diseases such as asthma, chronic obstructive pulmonary disease, bronchitis. The novelties of the database are the combination ability between auscultation sound results, chest X-ray and PFT; synchronously assessment capability of the lungs sounds; image processing based computerized analysis of the respiratory using chest X-ray and providing opportunity for improving analysis of both lung sounds and heart sounds on pulmonary and cardiac diseases.

</p>
</details>

<details><summary><b>DataLoc+: A Data Augmentation Technique for Machine Learning in Room-Level Indoor Localization</b>
<a href="https://arxiv.org/abs/2101.10833">arxiv:2101.10833</a>
&#x1F4C8; 0 <br>
<p>Amr E Hilal, Ismail Arai, Samy El-Tawab</p></summary>
<p>

**Abstract:** Indoor localization has been a hot area of research over the past two decades. Since its advent, it has been steadily utilizing the emerging technologies to improve accuracy, and machine learning has been at the heart of that. Machine learning has been increasingly used in fingerprint-based indoor localization to replace or emulate the radio map that is used to predict locations given a location signature. The prediction quality of a machine learning model primarily depends on how well the model was trained, which relies on the amount and quality of data used to train it. Data augmentation has been used to improve quality of the trained models by synthetically producing more training data, and several approaches were used in the literature that tackles the problem of lack of training data from different angles. In this paper, we propose DataLoc+, a data augmentation technique for room-level indoor localization that combines different approaches in a simple algorithm. We evaluate the technique by comparing it to the typical direct snapshot approach using data collected from a field experiment conducted in a hospital. Our evaluation shows that the model trained using the proposed technique achieves higher accuracy. We also show that the technique adapts to larger problems using a limited dataset while maintaining high accuracy.

</p>
</details>

<details><summary><b>Improving prediction of the terrestrial water storage anomalies during the GRACE and GRACE-FO gap with Bayesian convolutional neural networks</b>
<a href="https://arxiv.org/abs/2101.09361">arxiv:2101.09361</a>
&#x1F4C8; 0 <br>
<p>Shaoxing Mo, Yulong Zhong, Xiaoqing Shi, Wei Feng, Xin Yin, Jichun Wu</p></summary>
<p>

**Abstract:** The Gravity Recovery and Climate Experiment (GRACE) satellite and its successor GRACE Follow-On (GRACE-FO) provide valuable and accurate observations of terrestrial water storage anomalies (TWSAs) at a global scale. However, there is an approximately one-year observation gap of TWSAs between GRACE and GRACE-FO. This poses a challenge for practical applications, as discontinuity in the TWSA observations may introduce significant biases and uncertainties in the hydrological model predictions and consequently mislead decision making. To tackle this challenge, a Bayesian convolutional neural network (BCNN) driven by climatic data is proposed in this study to bridge this gap at a global scale. Enhanced by integrating recent advances in deep learning, including the attention mechanisms and the residual and dense connections, BCNN can automatically and efficiently extract important features for TWSA predictions from multi-source input data. The predicted TWSAs are compared to the hydrological model outputs and three recent TWSA prediction products. The comparison suggests the superior performance of BCNN in providing improved predictions of TWSAs during the gap in particular in the relatively arid regions. The BCNN's ability to identify the extreme dry and wet events during the gap period is further discussed and comprehensively demonstrated by comparing with the precipitation anomalies, drought index, ground/surface water levels. Results indicate that BCNN is capable of offering a reliable solution to maintain the TWSA data continuity and quantify the impacts of climate extremes during the gap.

</p>
</details>

<details><summary><b>Differentially Private SGD with Non-Smooth Losses</b>
<a href="https://arxiv.org/abs/2101.08925">arxiv:2101.08925</a>
&#x1F4C8; 0 <br>
<p>Puyu Wang, Yunwen Lei, Yiming Ying, Hai Zhang</p></summary>
<p>

**Abstract:** In this paper, we are concerned with differentially private {stochastic gradient descent (SGD)} algorithms in the setting of stochastic convex optimization (SCO). Most of the existing work requires the loss to be Lipschitz continuous and strongly smooth, and the model parameter to be uniformly bounded. However, these assumptions are restrictive as many popular losses violate these conditions including the hinge loss for SVM, the absolute loss in robust regression, and even the least square loss in an unbounded domain. We significantly relax these restrictive assumptions and establish privacy and generalization (utility) guarantees for private SGD algorithms using output and gradient perturbations associated with non-smooth convex losses. Specifically, the loss function is relaxed to have an $α$-Hölder continuous gradient (referred to as $α$-Hölder smoothness) which instantiates the Lipschitz continuity ($α=0$) and the strong smoothness ($α=1$). We prove that noisy SGD with $α$-Hölder smooth losses using gradient perturbation can guarantee $(ε,δ)$-differential privacy (DP) and attain optimal excess population risk $\mathcal{O}\Big(\frac{\sqrt{d\log(1/δ)}}{nε}+\frac{1}{\sqrt{n}}\Big)$, up to logarithmic terms, with the gradient complexity $ \mathcal{O}( n^{2-α\over 1+α}+ n).$ This shows an important trade-off between $α$-Hölder smoothness of the loss and the computational complexity for private SGD with statistically optimal performance. In particular, our results indicate that $α$-Hölder smoothness with $α\ge {1/2}$ is sufficient to guarantee $(ε,δ)$-DP of noisy SGD algorithms while achieving optimal excess risk with the linear gradient complexity $\mathcal{O}(n).$

</p>
</details>

<details><summary><b>Knowledge Generation -- Variational Bayes on Knowledge Graphs</b>
<a href="https://arxiv.org/abs/2101.08857">arxiv:2101.08857</a>
&#x1F4C8; 0 <br>
<p>Florian Wolf</p></summary>
<p>

**Abstract:** This thesis is a proof of concept for the potential of Variational Auto-Encoder (VAE) on representation learning of real-world Knowledge Graphs (KG). Inspired by successful approaches to the generation of molecular graphs, we evaluate the capabilities of our model, the Relational Graph Variational Auto-Encoder (RGVAE). The impact of the modular hyperparameter choices, encoding through graph convolutions, graph matching and latent space prior, is compared. The RGVAE is first evaluated on link prediction. The mean reciprocal rank (MRR) scores on the two datasets FB15K-237 and WN18RR are compared to the embedding-based model DistMult. A variational DistMult and a RGVAE without latent space prior constraint are implemented as control models. The results show that between different settings, the RGVAE with relaxed latent space, scores highest on both datasets, yet does not outperform the DistMult. Further, we investigate the latent space in a twofold experiment: first, linear interpolation between the latent representation of two triples, then the exploration of each latent dimension in a $95\%$ confidence interval. Both interpolations show that the RGVAE learns to reconstruct the adjacency matrix but fails to disentangle. For the last experiment we introduce a new validation method for the FB15K-237 data set. The relation type-constrains of generated triples are filtered and matched with entity types. The observed rate of valid generated triples is insignificantly higher than the random threshold. All generated and valid triples are unseen. A comparison between different latent space priors, using the $δ$-VAE method, reveals a decoder collapse. Finally we analyze the limiting factors of our approach compared to molecule generation and propose solutions for the decoder collapse and successful representation learning of multi-relational KGs.

</p>
</details>

<details><summary><b>Sum-Rate-Distortion Function for Indirect Multiterminal Source Coding in Federated Learning</b>
<a href="https://arxiv.org/abs/2101.08696">arxiv:2101.08696</a>
&#x1F4C8; 0 <br>
<p>Naifu Zhang, Meixia Tao, Jia Wang</p></summary>
<p>

**Abstract:** One of the main focus in federated learning (FL) is the communication efficiency since a large number of participating edge devices send their updates to the edge server at each round of the model training. Existing works reconstruct each model update from edge devices and implicitly assume that the local model updates are independent over edge devices. In FL, however, the model update is an indirect multi-terminal source coding problem, also called as the CEO problem where each edge device cannot observe directly the gradient that is to be reconstructed at the decoder, but is rather provided only with a noisy version. The existing works do not leverage the redundancy in the information transmitted by different edges. This paper studies the rate region for the indirect multiterminal source coding problem in FL. The goal is to obtain the minimum achievable rate at a particular upper bound of gradient variance. We obtain the rate region for the quadratic vector Gaussian CEO problem under unbiased estimator and derive an explicit formula of the sum-rate-distortion function in the special case where gradient are identical over edge device and dimension. Finally, we analyse communication efficiency of convex Minibatched SGD and non-convex Minibatched SGD based on the sum-rate-distortion function, respectively.

</p>
</details>

<details><summary><b>HMC, an example of Functional Analysis applied to Algorithms in Data Mining. The convergence in $L^p$</b>
<a href="https://arxiv.org/abs/2101.08688">arxiv:2101.08688</a>
&#x1F4C8; 0 <br>
<p>Soumyadip Ghosh, Yingdong Lu, Tomasz Nowicki</p></summary>
<p>

**Abstract:** We present a proof of convergence of the Hamiltonian Monte Carlo algorithm in terms of Functional Analysis. We represent the algorithm as an operator on the density functions, and prove the convergence of iterations of this operator in $L^p$, for $1<p<\infty$, and strong convergence for $2\le p<\infty$.

</p>
</details>

<details><summary><b>ItNet: iterative neural networks with small graphs for accurate, efficient and anytime semantic segmentation</b>
<a href="https://arxiv.org/abs/2101.08685">arxiv:2101.08685</a>
&#x1F4C8; 0 <br>
<p>Thomas Pfeil</p></summary>
<p>

**Abstract:** Deep neural networks have usually to be compressed and accelerated for their usage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware accelerators were developed that offer high throughput and low latency at low power by utilizing in-memory computation. However, to exploit these benefits the computational graph of a neural network has to fit into the in-computation memory of these hardware systems that is usually rather limited in size. In this study, we introduce a class of network models that have a small memory footprint in terms of their computational graphs. To this end, the graph is designed to contain loops by iteratively executing a single network building block. Furthermore, the trade-off between accuracy and latency of these so-called iterative neural networks is improved by adding multiple intermediate outputs during both training and inference. We show state-of-the-art results for semantic segmentation on the CamVid and Cityscapes datasets that are especially demanding in terms of computational resources. In ablation studies, the improvement of network training by intermediate network outputs as well as the trade-off between weight sharing over iterations and the network size are investigated.

</p>
</details>

<details><summary><b>Synwalk -- Community Detection via Random Walk Modelling</b>
<a href="https://arxiv.org/abs/2101.08623">arxiv:2101.08623</a>
&#x1F4C8; 0 <br>
<p>Christian Toth, Denis Helic, Bernhard C. Geiger</p></summary>
<p>

**Abstract:** Complex systems, abstractly represented as networks, are ubiquitous in everyday life. Analyzing and understanding these systems requires, among others, tools for community detection. As no single best community detection algorithm can exist, robustness across a wide variety of problem settings is desirable. In this work, we present Synwalk, a random walk-based community detection method. Synwalk builds upon a solid theoretical basis and detects communities by synthesizing the random walk induced by the given network from a class of candidate random walks. We thoroughly validate the effectiveness of our approach on synthetic and empirical networks, respectively, and compare Synwalk's performance with the performance of Infomap and Walktrap. Our results indicate that Synwalk performs robustly on networks with varying mixing parameters and degree distributions. We outperform Infomap on networks with high mixing parameter, and Infomap and Walktrap on networks with many small communities and low average degree. Our work has a potential to inspire further development of community detection via synthesis of random walks and we provide concrete ideas for future research.

</p>
</details>


{% endraw %}
Prev: [2021.01.20]({{ '/2021/01/20/2021.01.20.html' | relative_url }})  Next: [2021.01.22]({{ '/2021/01/22/2021.01.22.html' | relative_url }})