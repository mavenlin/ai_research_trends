Prev: [2022.09.17]({{ '/2022/09/17/2022.09.17.html' | relative_url }})  Next: [2022.09.19]({{ '/2022/09/19/2022.09.19.html' | relative_url }})
{% raw %}
## Summary for 2022-09-18, created on 2022-09-28


<details><summary><b>A Computational Model of Learning Flexible Navigation in a Maze by Layout-Conforming Replay of Place Cells</b>
<a href="https://arxiv.org/abs/2209.08572">arxiv:2209.08572</a>
&#x1F4C8; 64 <br>
<p>Yuanxiang Gao</p></summary>
<p>

**Abstract:** Recent experimental observations have shown that the reactivation of hippocampal place cells (PC) during sleep or immobility depicts trajectories that can go around barriers and can flexibly adapt to a changing maze layout. Such layout-conforming replay sheds a light on how the activity of place cells supports the learning of flexible navigation of an animal in a dynamically changing maze. However, existing computational models of replay fall short of generating layout-conforming replay, restricting their usage to simple environments, like linear tracks or open fields. In this paper, we propose a computational model that generates layout-conforming replay and explains how such replay drives the learning of flexible navigation in a maze. First, we propose a Hebbian-like rule to learn the inter-PC synaptic strength during exploring a maze. Then we use a continuous attractor network (CAN) with feedback inhibition to model the interaction among place cells and hippocampal interneurons. The activity bump of place cells drifts along a path in the maze, which models layout-conforming replay. During replay in rest, the synaptic strengths from place cells to striatal medium spiny neurons (MSN) are learned by a novel dopamine-modulated three-factor rule to store place-reward associations. During goal-directed navigation, the CAN periodically generates replay trajectories from the animal's location for path planning, and the trajectory leading to a maximal MSN activity is followed by the animal. We have implemented our model into a high-fidelity virtual rat in the MuJoCo physics simulator. Extensive experiments have demonstrated that its superior flexibility during navigation in a maze is due to a continuous re-learning of inter-PC and PC-MSN synaptic strength.

</p>
</details>

<details><summary><b>Quantifying How Hateful Communities Radicalize Online Users</b>
<a href="https://arxiv.org/abs/2209.08697">arxiv:2209.08697</a>
&#x1F4C8; 10 <br>
<p>Matheus Schmitz, Keith Burghardt, Goran Muric</p></summary>
<p>

**Abstract:** While online social media offers a way for ignored or stifled voices to be heard, it also allows users a platform to spread hateful speech. Such speech usually originates in fringe communities, yet it can spill over into mainstream channels. In this paper, we measure the impact of joining fringe hateful communities in terms of hate speech propagated to the rest of the social network. We leverage data from Reddit to assess the effect of joining one type of echo chamber: a digital community of like-minded users exhibiting hateful behavior. We measure members' usage of hate speech outside the studied community before and after they become active participants. Using Interrupted Time Series (ITS) analysis as a causal inference method, we gauge the spillover effect, in which hateful language from within a certain community can spread outside that community by using the level of out-of-community hate word usage as a proxy for learned hate. We investigate four different Reddit sub-communities (subreddits) covering three areas of hate speech: racism, misogyny and fat-shaming. In all three cases we find an increase in hate speech outside the originating community, implying that joining such community leads to a spread of hate speech throughout the platform. Moreover, users are found to pick up this new hateful speech for months after initially joining the community. We show that the harmful speech does not remain contained within the community. Our results provide new evidence of the harmful effects of echo chambers and the potential benefit of moderating them to reduce adoption of hateful speech.

</p>
</details>

<details><summary><b>AdvDO: Realistic Adversarial Attacks for Trajectory Prediction</b>
<a href="https://arxiv.org/abs/2209.08744">arxiv:2209.08744</a>
&#x1F4C8; 9 <br>
<p>Yulong Cao, Chaowei Xiao, Anima Anandkumar, Danfei Xu, Marco Pavone</p></summary>
<p>

**Abstract:** Trajectory prediction is essential for autonomous vehicles (AVs) to plan correct and safe driving behaviors. While many prior works aim to achieve higher prediction accuracy, few study the adversarial robustness of their methods. To bridge this gap, we propose to study the adversarial robustness of data-driven trajectory prediction systems. We devise an optimization-based adversarial attack framework that leverages a carefully-designed differentiable dynamic model to generate realistic adversarial trajectories. Empirically, we benchmark the adversarial robustness of state-of-the-art prediction models and show that our attack increases the prediction error for both general metrics and planning-aware metrics by more than 50% and 37%. We also show that our attack can lead an AV to drive off road or collide into other vehicles in simulation. Finally, we demonstrate how to mitigate the adversarial attacks using an adversarial training scheme.

</p>
</details>

<details><summary><b>ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding</b>
<a href="https://arxiv.org/abs/2209.08569">arxiv:2209.08569</a>
&#x1F4C8; 8 <br>
<p>Wenjin Wang, Zhengjie Huang, Bin Luo, Qianglong Chen, Qiming Peng, Yinxu Pan, Weichong Yin, Shikun Feng, Yu Sun, Dianhai Yu, Yin Zhang</p></summary>
<p>

**Abstract:** Recent efforts of multimodal Transformers have improved Visually Rich Document Understanding (VrDU) tasks via incorporating visual and textual information. However, existing approaches mainly focus on fine-grained elements such as words and document image patches, making it hard for them to learn from coarse-grained elements, including natural lexical units like phrases and salient visual regions like prominent image regions. In this paper, we attach more importance to coarse-grained elements containing high-density information and consistent semantics, which are valuable for document understanding. At first, a document graph is proposed to model complex relationships among multi-grained multimodal elements, in which salient visual regions are detected by a cluster-based method. Then, a multi-grained multimodal Transformer called mmLayout is proposed to incorporate coarse-grained information into existing pre-trained fine-grained multimodal Transformers based on the graph. In mmLayout, coarse-grained information is aggregated from fine-grained, and then, after further processing, is fused back into fine-grained for final prediction. Furthermore, common sense enhancement is introduced to exploit the semantic information of natural lexical units. Experimental results on four tasks, including information extraction and document question answering, show that our method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Qualitative analyses show that our method can capture consistent semantics in coarse-grained elements.

</p>
</details>

<details><summary><b>Perception-Distortion Trade-off in the SR Space Spanned by Flow Models</b>
<a href="https://arxiv.org/abs/2209.08564">arxiv:2209.08564</a>
&#x1F4C8; 8 <br>
<p>Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan, Erkut Erdem, Aykut Erdem</p></summary>
<p>

**Abstract:** Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature ($τ$) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photo-realistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion trade-off compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.

</p>
</details>

<details><summary><b>MMSR: Multiple-Model Learned Image Super-Resolution Benefiting From Class-Specific Image Priors</b>
<a href="https://arxiv.org/abs/2209.08568">arxiv:2209.08568</a>
&#x1F4C8; 7 <br>
<p>Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan</p></summary>
<p>

**Abstract:** Assuming a known degradation model, the performance of a learned image super-resolution (SR) model depends on how well the variety of image characteristics within the training set matches those in the test set. As a result, the performance of an SR model varies noticeably from image to image over a test set depending on whether characteristics of specific images are similar to those in the training set or not. Hence, in general, a single SR model cannot generalize well enough for all types of image content. In this work, we show that training multiple SR models for different classes of images (e.g., for text, texture, etc.) to exploit class-specific image priors and employing a post-processing network that learns how to best fuse the outputs produced by these multiple SR models surpasses the performance of state-of-the-art generic SR models. Experimental results clearly demonstrate that the proposed multiple-model SR (MMSR) approach significantly outperforms a single pre-trained state-of-the-art SR model both quantitatively and visually. It even exceeds the performance of the best single class-specific SR model trained on similar text or texture images.

</p>
</details>

<details><summary><b>Adaptive Multi-stage Density Ratio Estimation for Learning Latent Space Energy-based Model</b>
<a href="https://arxiv.org/abs/2209.08739">arxiv:2209.08739</a>
&#x1F4C8; 6 <br>
<p>Zhisheng Xiao, Tian Han</p></summary>
<p>

**Abstract:** This paper studies the fundamental problem of learning energy-based model (EBM) in the latent space of the generator model. Learning such prior model typically requires running costly Markov Chain Monte Carlo (MCMC). Instead, we propose to use noise contrastive estimation (NCE) to discriminatively learn the EBM through density ratio estimation between the latent prior density and latent posterior density. However, the NCE typically fails to accurately estimate such density ratio given large gap between two densities. To effectively tackle this issue and learn more expressive prior models, we develop the adaptive multi-stage density ratio estimation which breaks the estimation into multiple stages and learn different stages of density ratio sequentially and adaptively. The latent prior model can be gradually learned using ratio estimated in previous stage so that the final latent space EBM prior can be naturally formed by product of ratios in different stages. The proposed method enables informative and much sharper prior than existing baselines, and can be trained efficiently. Our experiments demonstrate strong performances in image generation and reconstruction as well as anomaly detection.

</p>
</details>

<details><summary><b>Heterogeneous Federated Learning on a Graph</b>
<a href="https://arxiv.org/abs/2209.08737">arxiv:2209.08737</a>
&#x1F4C8; 6 <br>
<p>Huiyuan Wang, Xuyang Zhao, Wei Lin</p></summary>
<p>

**Abstract:** Federated learning, where algorithms are trained across multiple decentralized devices without sharing local data, is increasingly popular in distributed machine learning practice. Typically, a graph structure $G$ exists behind local devices for communication. In this work, we consider parameter estimation in federated learning with data distribution and communication heterogeneity, as well as limited computational capacity of local devices. We encode the distribution heterogeneity by parametrizing distributions on local devices with a set of distinct $p$-dimensional vectors. We then propose to jointly estimate parameters of all devices under the $M$-estimation framework with the fused Lasso regularization, encouraging an equal estimate of parameters on connected devices in $G$. We provide a general result for our estimator depending on $G$, which can be further calibrated to obtain convergence rates for various specific problem setups. Surprisingly, our estimator attains the optimal rate under certain graph fidelity condition on $G$, as if we could aggregate all samples sharing the same distribution. If the graph fidelity condition is not met, we propose an edge selection procedure via multiple testing to ensure the optimality. To ease the burden of local computation, a decentralized stochastic version of ADMM is provided, with convergence rate $O(T^{-1}\log T)$ where $T$ denotes the number of iterations. We highlight that, our algorithm transmits only parameters along edges of $G$ at each iteration, without requiring a central machine, which preserves privacy. We further extend it to the case where devices are randomly inaccessible during the training process, with a similar algorithmic convergence guarantee. The computational and statistical efficiency of our method is evidenced by simulation experiments and the 2020 US presidential election data set.

</p>
</details>

<details><summary><b>BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach</b>
<a href="https://arxiv.org/abs/2209.08709">arxiv:2209.08709</a>
&#x1F4C8; 6 <br>
<p>Mao Ye, Bo Liu, Stephen Wright, Peter Stone, Qiang Liu</p></summary>
<p>

**Abstract:** Bilevel optimization (BO) is useful for solving a variety of important machine learning problems including but not limited to hyperparameter optimization, meta-learning, continual learning, and reinforcement learning. Conventional BO methods need to differentiate through the low-level optimization process with implicit differentiation, which requires expensive calculations related to the Hessian matrix. There has been a recent quest for first-order methods for BO, but the methods proposed to date tend to be complicated and impractical for large-scale deep learning applications. In this work, we propose a simple first-order BO algorithm that depends only on first-order gradient information, requires no implicit differentiation, and is practical and efficient for large-scale non-convex functions in deep learning. We provide non-asymptotic convergence analysis of the proposed method to stationary points for non-convex objectives and present empirical results that show its superior practical performance.

</p>
</details>

<details><summary><b>Learn the Time to Learn: Replay Scheduling in Continual Learning</b>
<a href="https://arxiv.org/abs/2209.08660">arxiv:2209.08660</a>
&#x1F4C8; 6 <br>
<p>Marcus Klasson, Hedvig Kjellström, Cheng Zhang</p></summary>
<p>

**Abstract:** Replay methods have shown to be successful in mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world applications, yet replaying all historical data would be prohibited due to processing time constraints. In such settings, we propose learning the time to learn for a continual learning system, in which we learn replay schedules over which tasks to replay at different time steps. To demonstrate the importance of learning the time to learn, we first use Monte Carlo tree search to find the proper replay schedule and show that it can outperform fixed scheduling policies in terms of continual learning performance. Moreover, to improve the scheduling efficiency itself, we propose to use reinforcement learning to learn the replay scheduling policies that can generalize to new continual learning scenarios without added computational cost. In our experiments, we show the advantages of learning the time to learn, which brings current continual learning research closer to real-world needs.

</p>
</details>

<details><summary><b>Predicting Performances of Mutual Funds using Deep Learning and Ensemble Techniques</b>
<a href="https://arxiv.org/abs/2209.09649">arxiv:2209.09649</a>
&#x1F4C8; 5 <br>
<p>Nghia Chu, Binh Dao, Nga Pham, Huy Nguyen, Hien Tran</p></summary>
<p>

**Abstract:** Predicting fund performance is beneficial to both investors and fund managers, and yet is a challenging task. In this paper, we have tested whether deep learning models can predict fund performance more accurately than traditional statistical techniques. Fund performance is typically evaluated by the Sharpe ratio, which represents the risk-adjusted performance to ensure meaningful comparability across funds. We calculated the annualised Sharpe ratios based on the monthly returns time series data for more than 600 open-end mutual funds investing in listed large-cap equities in the United States. We find that long short-term memory (LSTM) and gated recurrent units (GRUs) deep learning methods, both trained with modern Bayesian optimization, provide higher accuracy in forecasting funds' Sharpe ratios than traditional statistical ones. An ensemble method, which combines forecasts from LSTM and GRUs, achieves the best performance of all models. There is evidence to say that deep learning and ensembling offer promising solutions in addressing the challenge of fund performance forecasting.

</p>
</details>

<details><summary><b>Importance Tempering: Group Robustness for Overparameterized Models</b>
<a href="https://arxiv.org/abs/2209.08745">arxiv:2209.08745</a>
&#x1F4C8; 5 <br>
<p>Yiping Lu, Wenlong Ji, Zachary Izzo, Lexing Ying</p></summary>
<p>

**Abstract:** Although overparameterized models have shown their success on many machine learning tasks, the accuracy could drop on the testing distribution that is different from the training one. This accuracy drop still limits applying machine learning in the wild. At the same time, importance weighting, a traditional technique to handle distribution shifts, has been demonstrated to have less or even no effect on overparameterized models both empirically and theoretically. In this paper, we propose importance tempering to improve the decision boundary and achieve consistently better results for overparameterized models. Theoretically, we justify that the selection of group temperature can be different under label shift and spurious correlation setting. At the same time, we also prove that properly selected temperatures can extricate the minority collapse for imbalanced classification. Empirically, we achieve state-of-the-art results on worst group classification tasks using importance tempering.

</p>
</details>

<details><summary><b>Magnetic Resonance Fingerprinting with compressed sensing and distance metric learning</b>
<a href="https://arxiv.org/abs/2209.08734">arxiv:2209.08734</a>
&#x1F4C8; 5 <br>
<p>Zhe Wang, Hongsheng Li, Qinwei Zhang, Jing Yuan, Xiaogang Wang</p></summary>
<p>

**Abstract:** Magnetic Resonance Fingerprinting (MRF) is a novel technique that simultaneously estimates multiple tissue-related parameters, such as the longitudinal relaxation time T1, the transverse relaxation time T2, off resonance frequency B0 and proton density, from a scanned object in just tens of seconds. However, the MRF method suffers from aliasing artifacts because it significantly undersamples the k-space data. In this work, we propose a compressed sensing (CS) framework for simultaneously estimating multiple tissue-related parameters based on the MRF method. It is more robust to low sampling ratio and is therefore more efficient in estimating MR parameters for all voxels of an object. Furthermore, the MRF method requires identifying the nearest atoms of the query fingerprints from the MR-signal-evolution dictionary with the L2 distance. However, we observed that the L2 distance is not always a proper metric to measure the similarities between MR Fingerprints. Adaptively learning a distance metric from the undersampled training data can significantly improve the matching accuracy of the query fingerprints. Numerical results on extensive simulated cases show that our method substantially outperforms stateof-the-art methods in terms of accuracy of parameter estimation.

</p>
</details>

<details><summary><b>RVSL: Robust Vehicle Similarity Learning in Real Hazy Scenes Based on Semi-supervised Learning</b>
<a href="https://arxiv.org/abs/2209.08630">arxiv:2209.08630</a>
&#x1F4C8; 5 <br>
<p>Wei-Ting Chen, I-Hsiang Chen, Chih-Yuan Yeh, Hao-Hsiang Yang, Hua-En Chang, Jian-Jiun Ding, Sy-Yen Kuo</p></summary>
<p>

**Abstract:** Recently, vehicle similarity learning, also called re-identification (ReID), has attracted significant attention in computer vision. Several algorithms have been developed and obtained considerable success. However, most existing methods have unpleasant performance in the hazy scenario due to poor visibility. Though some strategies are possible to resolve this problem, they still have room to be improved due to the limited performance in real-world scenarios and the lack of real-world clear ground truth. Thus, to resolve this problem, inspired by CycleGAN, we construct a training paradigm called \textbf{RVSL} which integrates ReID and domain transformation techniques. The network is trained on semi-supervised fashion and does not require to employ the ID labels and the corresponding clear ground truths to learn hazy vehicle ReID mission in the real-world haze scenes. To further constrain the unsupervised learning process effectively, several losses are developed. Experimental results on synthetic and real-world datasets indicate that the proposed method can achieve state-of-the-art performance on hazy vehicle ReID problems. It is worth mentioning that although the proposed method is trained without real-world label information, it can achieve competitive performance compared to existing supervised methods trained on complete label information.

</p>
</details>

<details><summary><b>Membership Inference Attacks and Generalization: A Causal Perspective</b>
<a href="https://arxiv.org/abs/2209.08615">arxiv:2209.08615</a>
&#x1F4C8; 5 <br>
<p>Teodora Baluta, Shiqi Shen, S. Hitarth, Shruti Tople, Prateek Saxena</p></summary>
<p>

**Abstract:** Membership inference (MI) attacks highlight a privacy weakness in present stochastic training methods for neural networks. It is not well understood, however, why they arise. Are they a natural consequence of imperfect generalization only? Which underlying causes should we address during training to mitigate these attacks? Towards answering such questions, we propose the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning. We offer causal graphs that quantitatively explain the observed MI attack performance achieved for $6$ attack variants. We refute several prior non-quantitative hypotheses that over-simplify or over-estimate the influence of underlying causes, thereby failing to capture the complex interplay between several factors. Our causal models also show a new connection between generalization and MI attacks via their shared causal factors. Our causal models have high predictive power ($0.90$), i.e., their analytical predictions match with observations in unseen experiments often, which makes analysis via them a pragmatic alternative.

</p>
</details>

<details><summary><b>Low-cost machine learning approach to the prediction of transition metal phosphor excited state properties</b>
<a href="https://arxiv.org/abs/2209.08595">arxiv:2209.08595</a>
&#x1F4C8; 5 <br>
<p>Gianmarco Terrones, Chenru Duan, Aditya Nandy, Heather J. Kulik</p></summary>
<p>

**Abstract:** Photoactive iridium complexes are of broad interest due to their applications ranging from lighting to photocatalysis. However, the excited state property prediction of these complexes challenges ab initio methods such as time-dependent density functional theory (TDDFT) both from an accuracy and a computational cost perspective, complicating high throughput virtual screening (HTVS). We instead leverage low-cost machine learning (ML) models to predict the excited state properties of photoactive iridium complexes. We use experimental data of 1,380 iridium complexes to train and evaluate the ML models and identify the best-performing and most transferable models to be those trained on electronic structure features from low-cost density functional theory tight binding calculations. Using these models, we predict the three excited state properties considered, mean emission energy of phosphorescence, excited state lifetime, and emission spectral integral, with accuracy competitive with or superseding TDDFT. We conduct feature importance analysis to identify which iridium complex attributes govern excited state properties and we validate these trends with explicit examples. As a demonstration of how our ML models can be used for HTVS and the acceleration of chemical discovery, we curate a set of novel hypothetical iridium complexes and identify promising ligands for the design of new phosphors.

</p>
</details>

<details><summary><b>Imbalanced Nodes Classification for Graph Neural Networks Based on Valuable Sample Mining</b>
<a href="https://arxiv.org/abs/2209.08514">arxiv:2209.08514</a>
&#x1F4C8; 5 <br>
<p>Min Liu, Siwen Jin, Luo Jin, Shuohan Wang, Yu Fang, Yuliang Shi</p></summary>
<p>

**Abstract:** Node classification is an important task in graph neural networks, but most existing studies assume that samples from different classes are balanced. However, the class imbalance problem is widespread and can seriously affect the model's performance. Reducing the adverse effects of imbalanced datasets on model training is crucial to improve the model's performance. Therefore, a new loss function FD-Loss is reconstructed based on the traditional algorithm-level approach to the imbalance problem. Firstly, we propose sample mismeasurement distance to filter edge-hard samples and simple samples based on the distribution. Then, the weight coefficients are defined based on the mismeasurement distance and used in the loss function weighting term, so that the loss function focuses only on valuable samples. Experiments on several benchmarks demonstrate that our loss function can effectively solve the sample node imbalance problem and improve the classification accuracy by 4% compared to existing methods in the node classification task.

</p>
</details>

<details><summary><b>Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input</b>
<a href="https://arxiv.org/abs/2209.08752">arxiv:2209.08752</a>
&#x1F4C8; 4 <br>
<p>Yiye Chen, Yunzhi Lin, Patricio Vela</p></summary>
<p>

**Abstract:** Great success has been achieved in the 6-DoF grasp learning from the point cloud input, yet the computational cost due to the point set orderlessness remains a concern. Alternatively, we explore the grasp generation from the RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects the projection of the gripper keypoints in the image space and then recover the SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive shape and the grasp family is constructed to examine our idea. Metric-based evaluation reveals that our method outperforms the baselines in terms of the grasp proposal accuracy, diversity, and the time cost. Finally, robot experiments show high success rate, demonstrating the potential of the idea in the real-world applications.

</p>
</details>

<details><summary><b>Knowledge-based Analogical Reasoning in Neuro-symbolic Latent Spaces</b>
<a href="https://arxiv.org/abs/2209.08750">arxiv:2209.08750</a>
&#x1F4C8; 4 <br>
<p>Vishwa Shah, Aditya Sharma, Gautam Shroff, Lovekesh Vig, Tirtharaj Dash, Ashwin Srinivasan</p></summary>
<p>

**Abstract:** Analogical Reasoning problems challenge both connectionist and symbolic AI systems as these entail a combination of background knowledge, reasoning and pattern recognition. While symbolic systems ingest explicit domain knowledge and perform deductive reasoning, they are sensitive to noise and require inputs be mapped to preset symbolic features. Connectionist systems on the other hand can directly ingest rich input spaces such as images, text or speech and recognize pattern even with noisy inputs. However, connectionist models struggle to include explicit domain knowledge for deductive reasoning. In this paper, we propose a framework that combines the pattern recognition abilities of neural networks with symbolic reasoning and background knowledge for solving a class of Analogical Reasoning problems where the set of attributes and possible relations across them are known apriori. We take inspiration from the 'neural algorithmic reasoning' approach [DeepMind 2020] and use problem-specific background knowledge by (i) learning a distributed representation based on a symbolic model of the problem (ii) training neural-network transformations reflective of the relations involved in the problem and finally (iii) training a neural network encoder from images to the distributed representation in (i). These three elements enable us to perform search-based reasoning using neural networks as elementary functions manipulating distributed representations. We test this on visual analogy problems in RAVENs Progressive Matrices, and achieve accuracy competitive with human performance and, in certain cases, superior to initial end-to-end neural-network based approaches. While recent neural models trained at scale yield SOTA, our novel neuro-symbolic reasoning approach is a promising direction for this problem, and is arguably more general, especially for problems where domain knowledge is available.

</p>
</details>

<details><summary><b>Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search</b>
<a href="https://arxiv.org/abs/2209.08687">arxiv:2209.08687</a>
&#x1F4C8; 4 <br>
<p>Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon</p></summary>
<p>

**Abstract:** High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this context are highly complex, based on Boolean logic, include free-text terms and index terms from standardised terminologies (e.g., the Medical Subject Headings (MeSH) thesaurus), and are difficult and time-consuming to build. The use of MeSH terms, in particular, has been shown to improve the quality of the search results. However, identifying the correct MeSH terms to include in a query is difficult: information experts are often unfamiliar with the MeSH database and unsure about the appropriateness of MeSH terms for a query. Naturally, the full value of the MeSH terminology is often not fully exploited. This article investigates methods to suggest MeSH terms based on an initial Boolean query that includes only free-text terms. In this context, we devise lexical and pre-trained language models based methods. These methods promise to automatically identify highly effective MeSH terms for inclusion in a systematic review query. Our study contributes an empirical evaluation of several MeSH term suggestion methods. We further contribute an extensive analysis of MeSH term suggestions for each method and how these suggestions impact the effectiveness of Boolean queries.

</p>
</details>

<details><summary><b>Offline Evaluation of Reward-Optimizing Recommender Systems: The Case of Simulation</b>
<a href="https://arxiv.org/abs/2209.08642">arxiv:2209.08642</a>
&#x1F4C8; 4 <br>
<p>Imad Aouali, Amine Benhalloum, Martin Bompaire, Benjamin Heymann, Olivier Jeunen, David Rohde, Otmane Sakhi, Flavian Vasile</p></summary>
<p>

**Abstract:** Both in academic and industry-based research, online evaluation methods are seen as the golden standard for interactive applications like recommendation systems. Naturally, the reason for this is that we can directly measure utility metrics that rely on interventions, being the recommendations that are being shown to users. Nevertheless, online evaluation methods are costly for a number of reasons, and a clear need remains for reliable offline evaluation procedures. In industry, offline metrics are often used as a first-line evaluation to generate promising candidate models to evaluate online. In academic work, limited access to online systems makes offline metrics the de facto approach to validating novel methods. Two classes of offline metrics exist: proxy-based methods, and counterfactual methods. The first class is often poorly correlated with the online metrics we care about, and the latter class only provides theoretical guarantees under assumptions that cannot be fulfilled in real-world environments. Here, we make the case that simulation-based comparisons provide ways forward beyond offline metrics, and argue that they are a preferable means of evaluation.

</p>
</details>

<details><summary><b>Multi-level Explanation of Deep Reinforcement Learning-based Scheduling</b>
<a href="https://arxiv.org/abs/2209.09645">arxiv:2209.09645</a>
&#x1F4C8; 3 <br>
<p>Shaojun Zhang, Chen Wang, Albert Zomaya</p></summary>
<p>

**Abstract:** Dependency-aware job scheduling in the cluster is NP-hard. Recent work shows that Deep Reinforcement Learning (DRL) is capable of solving it. It is difficult for the administrator to understand the DRL-based policy even though it achieves remarkable performance gain. Therefore the complex model-based scheduler is not easy to gain trust in the system where simplicity is favored. In this paper, we give the multi-level explanation framework to interpret the policy of DRL-based scheduling. We dissect its decision-making process to job level and task level and approximate each level with interpretable models and rules, which align with operational practices. We show that the framework gives the system administrator insights into the state-of-the-art scheduler and reveals the robustness issue in regards to its behavior pattern.

</p>
</details>

<details><summary><b>Joint Language Semantic and Structure Embedding for Knowledge Graph Completion</b>
<a href="https://arxiv.org/abs/2209.08721">arxiv:2209.08721</a>
&#x1F4C8; 3 <br>
<p>Jianhao Shen, Chenguang Wang, Linyuan Gong, Dawn Song</p></summary>
<p>

**Abstract:** The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.

</p>
</details>

<details><summary><b>HiPart: Hierarchical Divisive Clustering Toolbox</b>
<a href="https://arxiv.org/abs/2209.08680">arxiv:2209.08680</a>
&#x1F4C8; 3 <br>
<p>Panagiotis Anagnostou, Sotiris Tasoulis, Vassilis Plagianakos, Dimitris Tasoulis</p></summary>
<p>

**Abstract:** This paper presents the HiPart package, an open-source native python library that provides efficient and interpret-able implementations of divisive hierarchical clustering algorithms. HiPart supports interactive visualizations for the manipulation of the execution steps allowing the direct intervention of the clustering outcome. This package is highly suited for Big Data applications as the focus has been given to the computational efficiency of the implemented clustering methodologies. The dependencies used are either Python build-in packages or highly maintained stable external packages. The software is provided under the MIT license. The package's source code and documentation can be found at https://github.com/panagiotisanagnostou/HiPart.

</p>
</details>

<details><summary><b>DeepTOP: Deep Threshold-Optimal Policy for MDPs and RMABs</b>
<a href="https://arxiv.org/abs/2209.08646">arxiv:2209.08646</a>
&#x1F4C8; 3 <br>
<p>Khaled Nakhleh, I-Hong Hou</p></summary>
<p>

**Abstract:** We consider the problem of learning the optimal threshold policy for control problems. Threshold policies make control decisions by evaluating whether an element of the system state exceeds a certain threshold, whose value is determined by other elements of the system state. By leveraging the monotone property of threshold policies, we prove that their policy gradients have a surprisingly simple expression. We use this simple expression to build an off-policy actor-critic algorithm for learning the optimal threshold policy. Simulation results show that our policy significantly outperforms other reinforcement learning algorithms due to its ability to exploit the monotone property. In addition, we show that the Whittle index, a powerful tool for restless multi-armed bandit problems, is equivalent to the optimal threshold policy for an alternative problem. This observation leads to a simple algorithm that finds the Whittle index by learning the optimal threshold policy in the alternative problem. Simulation results show that our algorithm learns the Whittle index much faster than several recent studies that learn the Whittle index through indirect means.

</p>
</details>

<details><summary><b>PIM-QAT: Neural Network Quantization for Processing-In-Memory (PIM) Systems</b>
<a href="https://arxiv.org/abs/2209.08617">arxiv:2209.08617</a>
&#x1F4C8; 3 <br>
<p>Qing Jin, Zhiyu Chen, Jian Ren, Yanyu Li, Yanzhi Wang, Kaiyuan Yang</p></summary>
<p>

**Abstract:** Processing-in-memory (PIM), an increasingly studied neuromorphic hardware, promises orders of energy and throughput improvements for deep learning inference. Leveraging the massively parallel and efficient analog computing inside memories, PIM circumvents the bottlenecks of data movements in conventional digital hardware. However, an extra quantization step (i.e. PIM quantization), typically with limited resolution due to hardware constraints, is required to convert the analog computing results into digital domain. Meanwhile, non-ideal effects extensively exist in PIM quantization because of the imperfect analog-to-digital interface, which further compromises the inference accuracy.
  In this paper, we propose a method for training quantized networks to incorporate PIM quantization, which is ubiquitous to all PIM systems. Specifically, we propose a PIM quantization aware training (PIM-QAT) algorithm, and introduce rescaling techniques during backward and forward propagation by analyzing the training dynamics to facilitate training convergence. We also propose two techniques, namely batch normalization (BN) calibration and adjusted precision training, to suppress the adverse effects of non-ideal linearity and stochastic thermal noise involved in real PIM chips. Our method is validated on three mainstream PIM decomposition schemes, and physically on a prototype chip. Comparing with directly deploying conventionally trained quantized model on PIM systems, which does not take into account this extra quantization step and thus fails, our method provides significant improvement. It also achieves comparable inference accuracy on PIM systems as that of conventionally quantized models on digital hardware, across CIFAR10 and CIFAR100 datasets using various network depths for the most popular network topology.

</p>
</details>

<details><summary><b>Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features</b>
<a href="https://arxiv.org/abs/2209.08614">arxiv:2209.08614</a>
&#x1F4C8; 3 <br>
<p>Megan A. Witherow, Manar D. Samad, Norou Diawara, Haim Y. Bar, Khan M. Iftekharuddin</p></summary>
<p>

**Abstract:** Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood, especially for monitoring lifelong conditions like Autism Spectrum Disorder. Deep convolutional neural networks have shown promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perform poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space to ensure robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial Expressions fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for adult-child facial expression classification. For the first time in the literature, a mixture of Beta distributions is used to decompose and select facial features based on correlations with expression, domain, and identity factors. We evaluate FACE-BE-SELF on two pairs of adult-child data sets. Our proposed FACE-BE-SELF approach outperforms adult-child transfer learning and other baseline domain adaptation methods in aligning latent representations of adult and child expressions.

</p>
</details>

<details><summary><b>Distribution inference risks: Identifying and mitigating sources of leakage</b>
<a href="https://arxiv.org/abs/2209.08541">arxiv:2209.08541</a>
&#x1F4C8; 3 <br>
<p>Valentin Hartmann, Léo Meynent, Maxime Peyrard, Dimitrios Dimitriadis, Shruti Tople, Robert West</p></summary>
<p>

**Abstract:** A large body of work shows that machine learning (ML) models can leak sensitive or confidential information about their training data. Recently, leakage due to distribution inference (or property inference) attacks is gaining attention. In this attack, the goal of an adversary is to infer distributional information about the training data. So far, research on distribution inference has focused on demonstrating successful attacks, with little attention given to identifying the potential causes of the leakage and to proposing mitigations. To bridge this gap, as our main contribution, we theoretically and empirically analyze the sources of information leakage that allows an adversary to perpetrate distribution inference attacks. We identify three sources of leakage: (1) memorizing specific information about the $\mathbb{E}[Y|X]$ (expected label given the feature values) of interest to the adversary, (2) wrong inductive bias of the model, and (3) finiteness of the training data. Next, based on our analysis, we propose principled mitigation techniques against distribution inference attacks. Specifically, we demonstrate that causal learning techniques are more resilient to a particular type of distribution inference risk termed distributional membership inference than associative learning methods. And lastly, we present a formalization of distribution inference that allows for reasoning about more general adversaries than was previously possible.

</p>
</details>

<details><summary><b>RDD2022: A multi-national image dataset for automatic Road Damage Detection</b>
<a href="https://arxiv.org/abs/2209.08538">arxiv:2209.08538</a>
&#x1F4C8; 3 <br>
<p>Deeksha Arya, Hiroya Maeda, Sanjay Kumar Ghosh, Durga Toshniwal, Yoshihide Sekimoto</p></summary>
<p>

**Abstract:** The data article describes the Road Damage Dataset, RDD2022, which comprises 47,420 road images from six countries, Japan, India, the Czech Republic, Norway, the United States, and China. The images have been annotated with more than 55,000 instances of road damage. Four types of road damage, namely longitudinal cracks, transverse cracks, alligator cracks, and potholes, are captured in the dataset. The annotated dataset is envisioned for developing deep learning-based methods to detect and classify road damage automatically. The dataset has been released as a part of the Crowd sensing-based Road Damage Detection Challenge (CRDDC2022). The challenge CRDDC2022 invites researchers from across the globe to propose solutions for automatic road damage detection in multiple countries. The municipalities and road agencies may utilize the RDD2022 dataset, and the models trained using RDD2022 for low-cost automatic monitoring of road conditions. Further, computer vision and machine learning researchers may use the dataset to benchmark the performance of different algorithms for other image-based applications of the same type (classification, object detection, etc.).

</p>
</details>

<details><summary><b>A Non-parametric Skill Representation with Soft Null Space Projectors for Fast Generalization</b>
<a href="https://arxiv.org/abs/2209.08522">arxiv:2209.08522</a>
&#x1F4C8; 3 <br>
<p>João Silvério, Yanlong Huang</p></summary>
<p>

**Abstract:** Over the last two decades, the robotics community witnessed the emergence of various motion representations that have been used extensively, particularly in behavorial cloning, to compactly encode and generalize skills. Among these, probabilistic approaches have earned a relevant place, owing to their encoding of variations, correlations and adaptability to new task conditions. Modulating such primitives, however, is often cumbersome due to the need for parameter re-optimization which frequently entails computationally costly operations. In this paper we derive a non-parametric movement primitive formulation that contains a null space projector. We show that such formulation allows for fast and efficient motion generation with computational complexity O(n2) without involving matrix inversions, whose complexity is O(n3). This is achieved by using the null space to track secondary targets, with a precision determined by the training dataset. Using a 2D example associated with time input we show that our non-parametric solution compares favourably with a state-of-the-art parametric approach. For demonstrated skills with high-dimensional inputs we show that it permits on-the-fly adaptation as well.

</p>
</details>

<details><summary><b>GLARE: A Dataset for Traffic Sign Detection in Sun Glare</b>
<a href="https://arxiv.org/abs/2209.08716">arxiv:2209.08716</a>
&#x1F4C8; 2 <br>
<p>Nicholas Gray, Megan Moraes, Jiang Bian, Allen Tian, Alex Wang, Haoyi Xiong, Zhishan Guo</p></summary>
<p>

**Abstract:** Real-time machine learning detection algorithms are often found within autonomous vehicle technology and depend on quality datasets. It is essential that these algorithms work correctly in everyday conditions as well as under strong sun glare. Reports indicate glare is one of the two most prominent environment-related reasons for crashes. However, existing datasets, such as LISA and the German Traffic Sign Recognition Benchmark, do not reflect the existence of sun glare at all. This paper presents the GLARE traffic sign dataset: a collection of images with U.S based traffic signs under heavy visual interference by sunlight. GLARE contains 2,157 images of traffic signs with sun glare, pulled from 33 videos of dashcam footage of roads in the United States. It provides an essential enrichment to the widely used LISA Traffic Sign dataset. Our experimental study shows that although several state-of-the-art baseline methods demonstrate superior performance when trained and tested against traffic sign datasets without sun glare, they greatly suffer when tested against GLARE (e.g., ranging from 9% to 21% mean mAP, which is significantly lower than the performances on LISA dataset). We also notice that current architectures have better detection accuracy (e.g., on average 42% mean mAP gain for mainstream algorithms) when trained on images of traffic signs in sun glare.

</p>
</details>

<details><summary><b>Towards Robust Off-Policy Evaluation via Human Inputs</b>
<a href="https://arxiv.org/abs/2209.08682">arxiv:2209.08682</a>
&#x1F4C8; 2 <br>
<p>Harvineet Singh, Shalmali Joshi, Finale Doshi-Velez, Himabindu Lakkaraju</p></summary>
<p>

**Abstract:** Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.

</p>
</details>

<details><summary><b>Allocation Schemes in Analytic Evaluation: Applicant-Centric Holistic or Attribute-Centric Segmented?</b>
<a href="https://arxiv.org/abs/2209.08665">arxiv:2209.08665</a>
&#x1F4C8; 2 <br>
<p>Jingyan Wang, Carmel Baharav, Nihar B. Shah, Anita Williams Woolley, R Ravi</p></summary>
<p>

**Abstract:** Many applications such as hiring and university admissions involve evaluation and selection of applicants. These tasks are fundamentally difficult, and require combining evidence from multiple different aspects (what we term "attributes"). In these applications, the number of applicants is often large, and a common practice is to assign the task to multiple evaluators in a distributed fashion. Specifically, in the often-used holistic allocation, each evaluator is assigned a subset of the applicants, and is asked to assess all relevant information for their assigned applicants. However, such an evaluation process is subject to issues such as miscalibration (evaluators see only a small fraction of the applicants and may not get a good sense of relative quality), and discrimination (evaluators are influenced by irrelevant information about the applicants). We identify that such attribute-based evaluation allows alternative allocation schemes. Specifically, we consider assigning each evaluator more applicants but fewer attributes per applicant, termed segmented allocation. We compare segmented allocation to holistic allocation on several dimensions via theoretical and experimental methods. We establish various tradeoffs between these two approaches, and identify conditions under which one approach results in more accurate evaluation than the other.

</p>
</details>

<details><summary><b>Through a fair looking-glass: mitigating bias in image datasets</b>
<a href="https://arxiv.org/abs/2209.08648">arxiv:2209.08648</a>
&#x1F4C8; 2 <br>
<p>Amirarsalan Rajabi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay, Gita Sukthankar</p></summary>
<p>

**Abstract:** With the recent growth in computer vision applications, the question of how fair and unbiased they are has yet to be explored. There is abundant evidence that the bias present in training data is reflected in the models, or even amplified. Many previous methods for image dataset de-biasing, including models based on augmenting datasets, are computationally expensive to implement. In this study, we present a fast and effective model to de-bias an image dataset through reconstruction and minimizing the statistical dependence between intended variables. Our architecture includes a U-net to reconstruct images, combined with a pre-trained classifier which penalizes the statistical dependence between target attribute and the protected attribute. We evaluate our proposed model on CelebA dataset, compare the results with a state-of-the-art de-biasing method, and show that the model achieves a promising fairness-accuracy combination.

</p>
</details>

<details><summary><b>Energy Efficient Automatic Streetlight Controlling System using Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2209.08633">arxiv:2209.08633</a>
&#x1F4C8; 2 <br>
<p>Md Sakib Ullah Sourav, Huidong Wang</p></summary>
<p>

**Abstract:** This study aims to develop a novel streetlight management system powered by computer vision technology mounted with the close circuit television (CCTV) camera that allows the light emitting diode (LED) streetlight to automatically light up with proper brightness by recognizing the presence of pedestrians or vehicles and reversely dimming the streetlight in their absence by semantic image segmentation from video.

</p>
</details>

<details><summary><b>Comparative study of machine learning and deep learning methods on ASD classification</b>
<a href="https://arxiv.org/abs/2209.08601">arxiv:2209.08601</a>
&#x1F4C8; 2 <br>
<p>Ramchandra Rimal, Mitchell Brannon, Yingxin Wang</p></summary>
<p>

**Abstract:** The autism dataset is studied to identify the differences between autistic and healthy groups. For this, the resting-state Functional Magnetic Resonance Imaging (rs-fMRI) data of the two groups are analyzed, and networks of connections between brain regions were created. Several classification frameworks are developed to distinguish the connectivity patterns between the groups. The best models for statistical inference and precision were compared, and the tradeoff between precision and model interpretability was analyzed. Finally, the classification accuracy measures were reported to justify the performance of our framework. Our best model can classify autistic and healthy patients on the multisite ABIDE I data with 71% accuracy.

</p>
</details>

<details><summary><b>RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection</b>
<a href="https://arxiv.org/abs/2209.08590">arxiv:2209.08590</a>
&#x1F4C8; 2 <br>
<p>Yue Song, Nicu Sebe, Wei Wang</p></summary>
<p>

**Abstract:** The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \texttt{RankFeat}, a simple yet effective \texttt{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature (\emph{i.e.,} $\mathbf{X}{-} \mathbf{s}_{1}\mathbf{u}_{1}\mathbf{v}_{1}^{T}$). \texttt{RankFeat} achieves the \emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\% compared with the previous best method. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results.

</p>
</details>

<details><summary><b>Bivariate Causal Discovery for Categorical Data via Classification with Optimal Label Permutation</b>
<a href="https://arxiv.org/abs/2209.08579">arxiv:2209.08579</a>
&#x1F4C8; 2 <br>
<p>Yang Ni</p></summary>
<p>

**Abstract:** Causal discovery for quantitative data has been extensively studied but less is known for categorical data. We propose a novel causal model for categorical data based on a new classification model, termed classification with optimal label permutation (COLP). By design, COLP is a parsimonious classifier, which gives rise to a provably identifiable causal model. A simple learning algorithm via comparing likelihood functions of causal and anti-causal models suffices to learn the causal direction. Through experiments with synthetic and real data, we demonstrate the favorable performance of the proposed COLP-based causal model compared to state-of-the-art methods. We also make available an accompanying R package COLP, which contains the proposed causal discovery algorithm and a benchmark dataset of categorical cause-effect pairs.

</p>
</details>

<details><summary><b>Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions</b>
<a href="https://arxiv.org/abs/2209.08554">arxiv:2209.08554</a>
&#x1F4C8; 2 <br>
<p>Murad Tukan, Loay Mualem, Alaa Maalouf</p></summary>
<p>

**Abstract:** Pruning is one of the predominant approaches for compressing deep neural networks (DNNs). Lately, coresets (provable data summarizations) were leveraged for pruning DNNs, adding the advantage of theoretical guarantees on the trade-off between the compression rate and the approximation error. However, coresets in this domain were either data-dependent or generated under restrictive assumptions on both the model's weights and inputs. In real-world scenarios, such assumptions are rarely satisfied, limiting the applicability of coresets. To this end, we suggest a novel and robust framework for computing such coresets under mild assumptions on the model's weights and without any assumption on the training data. The idea is to compute the importance of each neuron in each layer with respect to the output of the following layer. This is achieved by a combination of Löwner ellipsoid and Caratheodory theorem. Our method is simultaneously data-independent, applicable to various networks and datasets (due to the simplified assumptions), and theoretically supported. Experimental results show that our method outperforms existing coreset based neural pruning approaches across a wide range of networks and datasets. For example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet with $1.09\%$ drop in accuracy.

</p>
</details>

<details><summary><b>Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances</b>
<a href="https://arxiv.org/abs/2209.08529">arxiv:2209.08529</a>
&#x1F4C8; 2 <br>
<p>Yike Wu, Yu Zhao, Shiwan Zhao, Ying Zhang, Xiaojie Yuan, Guoqing Zhao, Ning Jiang</p></summary>
<p>

**Abstract:** Despite the great progress of Visual Question Answering (VQA), current VQA models heavily rely on the superficial correlation between the question type and its corresponding frequent answers (i.e., language priors) to make predictions, without really understanding the input. In this work, we define the training instances with the same question type but different answers as \textit{superficially similar instances}, and attribute the language priors to the confusion of VQA model on such instances. To solve this problem, we propose a novel training framework that explicitly encourages the VQA model to distinguish between the superficially similar instances. Specifically, for each training instance, we first construct a set that contains its superficially similar counterparts. Then we exploit the proposed distinguishing module to increase the distance between the instance and its counterparts in the answer space. In this way, the VQA model is forced to further focus on the other parts of the input beyond the question type, which helps to overcome the language priors. Experimental results show that our method achieves the state-of-the-art performance on VQA-CP v2. Codes are available at \href{https://github.com/wyk-nku/Distinguishing-VQA.git}{Distinguishing-VQA}.

</p>
</details>

<details><summary><b>Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classification</b>
<a href="https://arxiv.org/abs/2209.08527">arxiv:2209.08527</a>
&#x1F4C8; 2 <br>
<p>Yuqing Hu, Stéphane Pateux, Vincent Gripon</p></summary>
<p>

**Abstract:** Transductive Few-Shot learning has gained increased attention nowadays considering the cost of data annotations along with the increased accuracy provided by unlabelled samples in the domain of few shot. Especially in Few-Shot Classification (FSC), recent works explore the feature distributions aiming at maximizing likelihoods or posteriors with respect to the unknown parameters. Following this vein, and considering the parallel between FSC and clustering, we seek for better taking into account the uncertainty in estimation due to lack of data, as well as better statistical properties of the clusters associated with each class. Therefore in this paper we propose a new clustering method based on Variational Bayesian inference, further improved by Adaptive Dimension Reduction based on Probabilistic Linear Discriminant Analysis. Our proposed method significantly improves accuracy in the realistic unbalanced transductive setting on various Few-Shot benchmarks when applied to features used in previous studies, with a gain of up to $6\%$ in accuracy. In addition, when applied to balanced setting, we obtain very competitive results without making use of the class-balance artefact which is disputable for practical use cases. We also provide the performance of our method on a high performing pretrained backbone, with the reported results further surpassing the current state-of-the-art accuracy, suggesting the genericity of the proposed method.

</p>
</details>

<details><summary><b>A Benchmark for Understanding and Generating Dialogue between Characters in Stories</b>
<a href="https://arxiv.org/abs/2209.08524">arxiv:2209.08524</a>
&#x1F4C8; 2 <br>
<p>Jianzhu Yao, Ziqi Liu, Jian Guan, Minlie Huang</p></summary>
<p>

**Abstract:** Many classical fairy tales, fiction, and screenplays leverage dialogue to advance story plots and establish characters. We present the first study to explore whether machines can understand and generate dialogue in stories, which requires capturing traits of different characters and the relationships between them. To this end, we propose two new tasks including Masked Dialogue Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue turns and predicting speakers for specified dialogue turns, respectively. We build a new dataset DialStory, which consists of 105k Chinese stories with a large amount of dialogue weaved into the plots to support the evaluation. We show the difficulty of the proposed tasks by testing existing models with automatic and manual evaluation on DialStory. Furthermore, we propose to learn explicit character representations to improve performance on these tasks. Extensive experiments and case studies show that our approach can generate more coherent and informative dialogue, and achieve higher speaker recognition accuracy than strong baselines.

</p>
</details>

<details><summary><b>VisTaNet: Attention Guided Deep Fusion for Surface Roughness Classification</b>
<a href="https://arxiv.org/abs/2209.08516">arxiv:2209.08516</a>
&#x1F4C8; 2 <br>
<p>Prasanna Kumar Routray, Aditya Sanjiv Kanade, Jay Bhanushali, Manivannan Muniyandi</p></summary>
<p>

**Abstract:** Human texture perception is a weighted average of multi-sensory inputs: visual and tactile. While the visual sensing mechanism extracts global features, the tactile mechanism complements it by extracting local features. The lack of coupled visuotactile datasets in the literature is a challenge for studying multimodal fusion strategies analogous to human texture perception. This paper presents a visual dataset that augments an existing tactile dataset. We propose a novel deep fusion architecture that fuses visual and tactile data using four types of fusion strategies: summation, concatenation, max-pooling, and attention. Our model shows significant performance improvements (97.22%) in surface roughness classification accuracy over tactile only (SVM - 92.60%) and visual only (FENet-50 - 85.01%) architectures. Among the several fusion techniques, attention-guided architecture results in better classification accuracy. Our study shows that analogous to human texture perception, the proposed model chooses a weighted combination of the two modalities (visual and tactile), thus resulting in higher surface roughness classification accuracy; and it chooses to maximize the weightage of the tactile modality where the visual modality fails and vice-versa.

</p>
</details>

<details><summary><b>Empirical Analysis on Top-k Gradient Sparsification for Distributed Deep Learning in a Supercomputing Environment</b>
<a href="https://arxiv.org/abs/2209.08497">arxiv:2209.08497</a>
&#x1F4C8; 2 <br>
<p>Daegun Yoon, Sangyoon Oh</p></summary>
<p>

**Abstract:** To train deep learning models faster, distributed training on multiple GPUs is the very popular scheme in recent years. However, the communication bandwidth is still a major bottleneck of training performance. To improve overall training performance, recent works have proposed gradient sparsification methods that reduce the communication traffic significantly. Most of them require gradient sorting to select meaningful gradients such as Top-k gradient sparsification (Top-k SGD). However, Top-k SGD has a limit to increase the speed up overall training performance because gradient sorting is significantly inefficient on GPUs. In this paper, we conduct experiments that show the inefficiency of Top-k SGD and provide the insight of the low performance. Based on observations from our empirical analysis, we plan to yield a high performance gradient sparsification method as a future work.

</p>
</details>

<details><summary><b>Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2209.08483">arxiv:2209.08483</a>
&#x1F4C8; 2 <br>
<p>Hua Wei, Jingxiao Chen, Xiyang Ji, Hongyang Qin, Minwen Deng, Siqin Li, Liang Wang, Weinan Zhang, Yong Yu, Lin Liu, Lanxiao Huang, Deheng Ye, Qiang Fu, Wei Yang</p></summary>
<p>

**Abstract:** This paper introduces Honor of Kings Arena, a reinforcement learning (RL) environment based on Honor of Kings, one of the world's most popular games at present. Compared to other environments studied in most previous work, ours presents new generalization challenges for competitive reinforcement learning. It is a multi-agent problem with one agent competing against its opponent; and it requires the generalization ability as it has diverse targets to control and diverse opponents to compete with. We describe the observation, action, and reward specifications for the Honor of Kings domain and provide an open-source Python-based interface for communicating with the game engine. We provide twenty target heroes with a variety of tasks in Honor of Kings Arena and present initial baseline results for RL-based methods with feasible computing resources. Finally, we showcase the generalization challenges imposed by Honor of Kings Arena and possible remedies to the challenges. All of the software, including the environment-class, are publicly available at https://github.com/tencent-ailab/hok_env . The documentation is available at https://aiarena.tencent.com/hok/doc/ .

</p>
</details>

<details><summary><b>Fault Detection in Ball Bearings</b>
<a href="https://arxiv.org/abs/2209.11041">arxiv:2209.11041</a>
&#x1F4C8; 1 <br>
<p>Joshua Pickard, Sarah Moll</p></summary>
<p>

**Abstract:** Ball bearing joints are a critical component in all rotating machinery, and detecting and locating faults in these joints is a significant problem in industry and research. Intelligent fault detection (IFD) is the process of applying machine learning and other statistical methods to monitor the health states of machines. This paper explores the construction of vibration images, a preprocessing technique that has been previously used to train convolutional neural networks for ball bearing joint IFD. The main results demonstrate the robustness of this technique by applying it to a larger dataset than previously used and exploring the hyperparameters used in constructing the vibration images.

</p>
</details>

<details><summary><b>Online Regenerative Learning</b>
<a href="https://arxiv.org/abs/2209.08657">arxiv:2209.08657</a>
&#x1F4C8; 1 <br>
<p>Owen Shen</p></summary>
<p>

**Abstract:** We study a type of Online Linear Programming (OLP) problem that maximizes the objective function with stochastic inputs. The performance of various algorithms that analyze this type of OLP is well studied when the stochastic inputs follow some i.i.d distribution. The two central questions to ask are: (i) can the algorithms achieve the same efficiency if the stochastic inputs are not i.i.d but still stationary, and (ii) how can we modify our algorithms if we know the stochastic inputs are trendy, hence not stationary. We answer the first question by analyzing a regenerative type of input and show the regret of two popular algorithms are bounded by the same order as their i.i.d counterpart. We discuss the second question in the context of linearly growing inputs and propose two trend-adaptive algorithms. We provide numerical simulations to illustrate the performance of our algorithms under both regenerative and trendy inputs.

</p>
</details>

<details><summary><b>Enabling Conversational Interaction with Mobile UI using Large Language Models</b>
<a href="https://arxiv.org/abs/2209.08655">arxiv:2209.08655</a>
&#x1F4C8; 1 <br>
<p>Bryan Wang, Gang Li, Yang Li</p></summary>
<p>

**Abstract:** Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We propose a design space to categorize conversations between the user and the agent when collaboratively accomplishing mobile tasks. We design prompting techniques to adapt an LLM to conversational tasks on mobile UIs. The experiments show that our approach enables various conversational interactions with decent performances, manifesting its feasibility. We discuss the use cases of our work and its implications for language-based mobile interaction.

</p>
</details>

<details><summary><b>Infrared: A Meta Bug Detector</b>
<a href="https://arxiv.org/abs/2209.08510">arxiv:2209.08510</a>
&#x1F4C8; 1 <br>
<p>Chi Zhang, Yu Wang, Linzhang Wang</p></summary>
<p>

**Abstract:** The recent breakthroughs in deep learning methods have sparked a wave of interest in learning-based bug detectors. Compared to the traditional static analysis tools, these bug detectors are directly learned from data, thus, easier to create. On the other hand, they are difficult to train, requiring a large amount of data which is not readily available. In this paper, we propose a new approach, called meta bug detection, which offers three crucial advantages over existing learning-based bug detectors: bug-type generic (i.e., capable of catching the types of bugs that are totally unobserved during training), self-explainable (i.e., capable of explaining its own prediction without any external interpretability methods) and sample efficient (i.e., requiring substantially less training data than standard bug detectors). Our extensive evaluation shows our meta bug detector (MBD) is effective in catching a variety of bugs including null pointer dereference, array index out-of-bound, file handle leak, and even data races in concurrent programs; in the process MBD also significantly outperforms several noteworthy baselines including Facebook Infer, a prominent static analysis tool, and FICS, the latest anomaly detection method.

</p>
</details>

<details><summary><b>A Map-matching Algorithm with Extraction of Multi-group Information for Low-frequency Data</b>
<a href="https://arxiv.org/abs/2209.08500">arxiv:2209.08500</a>
&#x1F4C8; 1 <br>
<p>Jie Fang, Xiongwei Wu, Dianchao Lin, Mengyun Xu, Huahua Wu, Xuesong Wu, Ting Bi</p></summary>
<p>

**Abstract:** The growing use of probe vehicles generates a huge number of GNSS data. Limited by the satellite positioning technology, further improving the accuracy of map-matching is challenging work, especially for low-frequency trajectories. When matching a trajectory, the ego vehicle's spatial-temporal information of the present trip is the most useful with the least amount of data. In addition, there are a large amount of other data, e.g., other vehicles' state and past prediction results, but it is hard to extract useful information for matching maps and inferring paths. Most map-matching studies only used the ego vehicle's data and ignored other vehicles' data. Based on it, this paper designs a new map-matching method to make full use of "Big data". We first sort all data into four groups according to their spatial and temporal distance from the present matching probe which allows us to sort for their usefulness. Then we design three different methods to extract valuable information (scores) from them: a score for speed and bearing, a score for historical usage, and a score for traffic state using the spectral graph Markov neutral network. Finally, we use a modified top-K shortest-path method to search the candidate paths within an ellipse region and then use the fused score to infer the path (projected location). We test the proposed method against baseline algorithms using a real-world dataset in China. The results show that all scoring methods can enhance map-matching accuracy. Furthermore, our method outperforms the others, especially when GNSS probing frequency is less than 0.01 Hz.

</p>
</details>


{% endraw %}
Prev: [2022.09.17]({{ '/2022/09/17/2022.09.17.html' | relative_url }})  Next: [2022.09.19]({{ '/2022/09/19/2022.09.19.html' | relative_url }})