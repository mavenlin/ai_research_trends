## Summary for 2021-05-05, created on 2021-12-21


<details><summary><b>RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition</b>
<a href="https://arxiv.org/abs/2105.01883">arxiv:2105.01883</a>
&#x1F4C8; 72 <br>
<p>Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, Guiguang Ding</p></summary>
<p>

**Abstract:** We propose RepMLP, a multi-layer-perceptron-style neural network building block for image recognition, which is composed of a series of fully-connected (FC) layers. Compared to convolutional layers, FC layers are more efficient, better at modeling the long-range dependencies and positional patterns, but worse at capturing the local structures, hence usually less favored for image recognition. We propose a structural re-parameterization technique that adds local prior into an FC to make it powerful for image recognition. Specifically, we construct convolutional layers inside a RepMLP during training and merge them into the FC for inference. On CIFAR, a simple pure-MLP model shows performance very close to CNN. By inserting RepMLP in traditional CNN, we improve ResNets by 1.8% accuracy on ImageNet, 2.9% for face recognition, and 2.3% mIoU on Cityscapes with lower FLOPs. Our intriguing findings highlight that combining the global representational capacity and positional perception of FC with the local prior of convolution can improve the performance of neural network with faster speed on both the tasks with translation invariance (e.g., semantic segmentation) and those with aligned images and positional patterns (e.g., face recognition). The code and models are available at https://github.com/DingXiaoH/RepMLP.

</p>
</details>

<details><summary><b>Software Engineering for AI-Based Systems: A Survey</b>
<a href="https://arxiv.org/abs/2105.01984">arxiv:2105.01984</a>
&#x1F4C8; 45 <br>
<p>Silverio Martínez-Fernández, Justus Bogner, Xavier Franch, Marc Oriol, Julien Siebert, Adam Trendowicz, Anna Maria Vollmer, Stefan Wagner</p></summary>
<p>

**Abstract:** AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image- and speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state of the art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.

</p>
</details>

<details><summary><b>Development of an expected possession value model to analyse team attacking performances in rugby league</b>
<a href="https://arxiv.org/abs/2105.05303">arxiv:2105.05303</a>
&#x1F4C8; 42 <br>
<p>Thomas Sawczuk, Anna Palczewska, Ben Jones</p></summary>
<p>

**Abstract:** This study aimed to provide a framework to evaluate team attacking performances in rugby league using 59,233 plays from 180 Super League matches via expected possession value (EPV) models. The EPV-308 split the pitch into 308 5m x 5m zones, the EPV-77 split the pitch into 77 10m x 10m zones and the EPV-19 split the pitch in 19 zones of variable size dependent on the total zone value generated during a match. Attacking possessions were considered as Markov Chains, allowing the value of each zone visited to be estimated based on the outcome of the possession. The Kullback-Leibler Divergence was used to evaluate the reproducibility of the value generated from each zone (the reward distribution) by teams between matches. The EPV-308 had the greatest variability and lowest reproducibility, compared to EPV-77 and EPV-19. When six previous matches were considered, the team's subsequent match attacking performances had a similar reward distribution for EPV-19, EPV-77 and EPV-308 on 95 +/- 4%, 51 +/- 12% and 0 +/- 0% of occasions. This study supports the use of EPV-19 to evaluate team attacking performance in rugby league and provides a simple framework through which attacking performances can be compared between teams.

</p>
</details>

<details><summary><b>Self-Supervised Learning from Automatically Separated Sound Scenes</b>
<a href="https://arxiv.org/abs/2105.02132">arxiv:2105.02132</a>
&#x1F4C8; 32 <br>
<p>Eduardo Fonseca, Aren Jansen, Daniel P. W. Ellis, Scott Wisdom, Marco Tagliasacchi, John R. Hershey, Manoj Plakal, Shawn Hershey, R. Channing Moore, Xavier Serra</p></summary>
<p>

**Abstract:** Real-world sound scenes consist of time-varying collections of sound sources, each generating characteristic sound events that are mixed together in audio recordings. The association of these constituent sound events with their mixture and each other is semantically constrained: the sound scene contains the union of source classes and not all classes naturally co-occur. With this motivation, this paper explores the use of unsupervised automatic sound separation to decompose unlabeled sound scenes into multiple semantically-linked views for use in self-supervised contrastive learning. We find that learning to associate input mixtures with their automatically separated outputs yields stronger representations than past approaches that use the mixtures alone. Further, we discover that optimal source separation is not required for successful contrastive learning by demonstrating that a range of separation system convergence states all lead to useful and often complementary example transformations. Our best system incorporates these unsupervised separation models into a single augmentation front-end and jointly optimizes similarity maximization and coincidence prediction objectives across the views. The result is an unsupervised audio representation that rivals state-of-the-art alternatives on the established shallow AudioSet classification benchmark.

</p>
</details>

<details><summary><b>Real-time Multi-Adaptive-Resolution-Surfel 6D LiDAR Odometry using Continuous-time Trajectory Optimization</b>
<a href="https://arxiv.org/abs/2105.02010">arxiv:2105.02010</a>
&#x1F4C8; 23 <br>
<p>Jan Quenzel, Sven Behnke</p></summary>
<p>

**Abstract:** Simultaneous Localization and Mapping (SLAM) is an essential capability for autonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is challenging. We propose a real-time method for 6D LiDAR odometry. Our approach combines a continuous-time B-Spline trajectory representation with a Gaussian Mixture Model (GMM) formulation to jointly align local multi-resolution surfel maps. Sparse voxel grids and permutohedral lattices ensure fast access to map surfels, and an adaptive resolution selection scheme effectively speeds up registration. A thorough experimental evaluation shows the performance of our approach on multiple datasets and during real-robot experiments.

</p>
</details>

<details><summary><b>Self-Supervised Multi-Frame Monocular Scene Flow</b>
<a href="https://arxiv.org/abs/2105.02216">arxiv:2105.02216</a>
&#x1F4C8; 10 <br>
<p>Junhwa Hur, Stefan Roth</p></summary>
<p>

**Abstract:** Estimating 3D scene flow from a sequence of monocular images has been gaining increased attention due to the simple, economical capture setup. Owing to the severe ill-posedness of the problem, the accuracy of current methods has been limited, especially that of efficient, real-time approaches. In this paper, we introduce a multi-frame monocular scene flow network based on self-supervised learning, improving the accuracy over previous networks while retaining real-time efficiency. Based on an advanced two-frame baseline with a split-decoder design, we propose (i) a multi-frame model using a triple frame input and convolutional LSTM connections, (ii) an occlusion-aware census loss for better accuracy, and (iii) a gradient detaching strategy to improve training stability. On the KITTI dataset, we observe state-of-the-art accuracy among monocular scene flow methods based on self-supervised learning.

</p>
</details>

<details><summary><b>When Fair Ranking Meets Uncertain Inference</b>
<a href="https://arxiv.org/abs/2105.02091">arxiv:2105.02091</a>
&#x1F4C8; 10 <br>
<p>Avijit Ghosh, Ritam Dutt, Christo Wilson</p></summary>
<p>

**Abstract:** Existing fair ranking systems, especially those designed to be demographically fair, assume that accurate demographic information about individuals is available to the ranking algorithm. In practice, however, this assumption may not hold -- in real-world contexts like ranking job applicants or credit seekers, social and legal barriers may prevent algorithm operators from collecting peoples' demographic information. In these cases, algorithm operators may attempt to infer peoples' demographics and then supply these inferences as inputs to the ranking algorithm.
  In this study, we investigate how uncertainty and errors in demographic inference impact the fairness offered by fair ranking algorithms. Using simulations and three case studies with real datasets, we show how demographic inferences drawn from real systems can lead to unfair rankings. Our results suggest that developers should not use inferred demographic data as input to fair ranking algorithms, unless the inferences are extremely accurate.

</p>
</details>

<details><summary><b>MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering</b>
<a href="https://arxiv.org/abs/2105.01899">arxiv:2105.01899</a>
&#x1F4C8; 10 <br>
<p>Tsung Wei Tsai, Chongxuan Li, Jun Zhu</p></summary>
<p>

**Abstract:** We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner. To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results than various previous methods and a strong contrastive learning baseline.

</p>
</details>

<details><summary><b>MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space</b>
<a href="https://arxiv.org/abs/2105.01879">arxiv:2105.01879</a>
&#x1F4C8; 10 <br>
<p>Rui Huang, Yixuan Li</p></summary>
<p>

**Abstract:** Detecting out-of-distribution (OOD) inputs is a central challenge for safely deploying machine learning models in the real world. Existing solutions are mainly driven by small datasets, with low resolution and very few class labels (e.g., CIFAR). As a result, OOD detection for large-scale image classification tasks remains largely unexplored. In this paper, we bridge this critical gap by proposing a group-based OOD detection framework, along with a novel OOD scoring function termed MOS. Our key idea is to decompose the large semantic space into smaller groups with similar concepts, which allows simplifying the decision boundaries between in- vs. out-of-distribution data for effective OOD detection. Our method scales substantially better for high-dimensional class space than previous approaches. We evaluate models trained on ImageNet against four carefully curated OOD datasets, spanning diverse semantics. MOS establishes state-of-the-art performance, reducing the average FPR95 by 14.33% while achieving 6x speedup in inference compared to the previous best method.

</p>
</details>

<details><summary><b>Policy Learning with Adaptively Collected Data</b>
<a href="https://arxiv.org/abs/2105.02344">arxiv:2105.02344</a>
&#x1F4C8; 8 <br>
<p>Ruohan Zhan, Zhimei Ren, Susan Athey, Zhengyuan Zhou</p></summary>
<p>

**Abstract:** Learning optimal policies from historical data enables the gains from personalization to be realized in a wide variety of applications. The growing policy learning literature focuses on a setting where the treatment assignment policy does not adapt to the data. However, adaptive data collection is becoming more common in practice, from two primary sources: 1) data collected from adaptive experiments that are designed to improve inferential efficiency; 2) data collected from production systems that are adaptively evolving an operational policy to improve performance over time (e.g. contextual bandits). In this paper, we aim to address the challenge of learning the optimal policy with adaptively collected data and provide one of the first theoretical inquiries into this problem. We propose an algorithm based on generalized augmented inverse propensity weighted estimators and establish its finite-sample regret bound. We complement this regret upper bound with a lower bound that characterizes the fundamental difficulty of policy learning with adaptive data. Finally, we demonstrate our algorithm's effectiveness using both synthetic data and public benchmark datasets.

</p>
</details>

<details><summary><b>Do Natural Language Explanations Represent Valid Logical Arguments? Verifying Entailment in Explainable NLI Gold Standards</b>
<a href="https://arxiv.org/abs/2105.01974">arxiv:2105.01974</a>
&#x1F4C8; 8 <br>
<p>Marco Valentino, Ian Pratt-Hartmann, André Freitas</p></summary>
<p>

**Abstract:** An emerging line of research in Explainable NLP is the creation of datasets enriched with human-annotated explanations and rationales, used to build and evaluate models with step-wise inference and explanation generation capabilities. While human-annotated explanations are used as ground-truth for the inference, there is a lack of systematic assessment of their consistency and rigour. In an attempt to provide a critical quality assessment of Explanation Gold Standards (XGSs) for NLI, we propose a systematic annotation methodology, named Explanation Entailment Verification (EEV), to quantify the logical validity of human-annotated explanations. The application of EEV on three mainstream datasets reveals the surprising conclusion that a majority of the explanations, while appearing coherent on the surface, represent logically invalid arguments, ranging from being incomplete to containing clearly identifiable logical errors. This conclusion confirms that the inferential properties of explanations are still poorly formalised and understood, and that additional work on this line of research is necessary to improve the way Explanation Gold Standards are constructed.

</p>
</details>

<details><summary><b>Continual Learning on the Edge with TensorFlow Lite</b>
<a href="https://arxiv.org/abs/2105.01946">arxiv:2105.01946</a>
&#x1F4C8; 8 <br>
<p>Giorgos Demosthenous, Vassilis Vassiliades</p></summary>
<p>

**Abstract:** Deploying sophisticated deep learning models on embedded devices with the purpose of solving real-world problems is a struggle using today's technology. Privacy and data limitations, network connection issues, and the need for fast model adaptation are some of the challenges that constitute today's approaches unfit for many applications on the edge and make real-time on-device training a necessity. Google is currently working on tackling these challenges by embedding an experimental transfer learning API to their TensorFlow Lite, machine learning library. In this paper, we show that although transfer learning is a good first step for on-device model training, it suffers from catastrophic forgetting when faced with more realistic scenarios. We present this issue by testing a simple transfer learning model on the CORe50 benchmark as well as by demonstrating its limitations directly on an Android application we developed. In addition, we expand the TensorFlow Lite library to include continual learning capabilities, by integrating a simple replay approach into the head of the current transfer learning model. We test our continual learning model on the CORe50 benchmark to show that it tackles catastrophic forgetting, and we demonstrate its ability to continually learn, even under non-ideal conditions, using the application we developed. Finally, we open-source the code of our Android application to enable developers to integrate continual learning to their own smartphone applications, as well as to facilitate further development of continual learning functionality into the TensorFlow Lite environment.

</p>
</details>

<details><summary><b>FLEX: Parameter-free Multi-view 3D Human Motion Reconstruction</b>
<a href="https://arxiv.org/abs/2105.01937">arxiv:2105.01937</a>
&#x1F4C8; 8 <br>
<p>Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or</p></summary>
<p>

**Abstract:** The increasing availability of video recordings made by multiple cameras has offered new means for mitigatingocclusion and depth ambiguities in pose and motion reconstruction methods. Yet, multi-view algorithms strongly depend on camera parameters; particularly, the relativepositions between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an end-to-end parameter-free multi-viewmodel. FLEX is parameter-free in the sense that it does not require any camera parameters, neither intrinsic nor extrinsic. Our key idea is that the 3D angles between skeletal parts, as well as bone lengths, are invariant to the camera position. Hence, learning 3D rotations and bone lengths rather than locations allows predicting common values for all camera views. Our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. We demonstrate quantitative and qualitative results on the Human3.6M and KTH Multi-view Football II datasets, and on synthetic multi-person video streams captured by dynamic cameras. We compare our model to state-of-the-art methods that are not parameter-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. Code, trained models, video examples, and more material will be available on our project page.

</p>
</details>

<details><summary><b>Explainable Artificial Intelligence for Human Decision-Support System in Medical Domain</b>
<a href="https://arxiv.org/abs/2105.02357">arxiv:2105.02357</a>
&#x1F4C8; 7 <br>
<p>Samanta Knapič, Avleen Malhi, Rohit Saluja, Kary Främling</p></summary>
<p>

**Abstract:** In the present paper we present the potential of Explainable Artificial Intelligence methods for decision-support in medical image analysis scenarios. With three types of explainable methods applied to the same medical image data set our aim was to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). The visual explanations were provided on in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with the goal of increasing the health professionals' trust in the black box predictions. We implemented two post-hoc interpretable machine learning methods LIME and SHAP and the alternative explanation approach CIU, centered on the Contextual Value and Utility (CIU). The produced explanations were evaluated using human evaluation. We conducted three user studies based on the explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in the web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n=20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We have found that, as hypothesized, the CIU explainable method performed better than both LIME and SHAP methods in terms of increasing support for human decision-making as well as being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that can with future improvements in implementation be generalized on different medical data sets and can provide great decision-support for medical experts.

</p>
</details>

<details><summary><b>How Fine-Tuning Allows for Effective Meta-Learning</b>
<a href="https://arxiv.org/abs/2105.02221">arxiv:2105.02221</a>
&#x1F4C8; 7 <br>
<p>Kurtland Chua, Qi Lei, Jason D. Lee</p></summary>
<p>

**Abstract:** Representation learning has been widely studied in the context of meta-learning, enabling rapid learning of new tasks through shared representations. Recent works such as MAML have explored using fine-tuning-based metrics, which measure the ease by which fine-tuning can achieve good performance, as proxies for obtaining representations. We present a theoretical framework for analyzing representations derived from a MAML-like algorithm, assuming the available tasks use approximately the same underlying representation. We then provide risk bounds on the best predictor found by fine-tuning via gradient descent, demonstrating that the algorithm can provably leverage the shared structure. The upper bound applies to general function classes, which we demonstrate by instantiating the guarantees of our framework in the logistic regression and neural network settings. In contrast, we establish the existence of settings where any algorithm, using a representation trained with no consideration for task-specific fine-tuning, performs as well as a learner with no access to source tasks in the worst case. This separation result underscores the benefit of fine-tuning-based methods, such as MAML, over methods with "frozen representation" objectives in few-shot learning.

</p>
</details>

<details><summary><b>DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data</b>
<a href="https://arxiv.org/abs/2105.02340">arxiv:2105.02340</a>
&#x1F4C8; 6 <br>
<p>Damien Dablain, Bartosz Krawczyk, Nitesh V. Chawla</p></summary>
<p>

**Abstract:** Despite over two decades of progress, imbalanced data is still considered a significant challenge for contemporary machine learning models. Modern advances in deep learning have magnified the importance of the imbalanced data problem. The two main approaches to address this issue are based on loss function modifications and instance resampling. Instance sampling is typically based on Generative Adversarial Networks (GANs), which may suffer from mode collapse. Therefore, there is a need for an oversampling method that is specifically tailored to deep learning models, can work on raw images while preserving their properties, and is capable of generating high quality, artificial images that can enhance minority classes and balance the training set. We propose DeepSMOTE - a novel oversampling algorithm for deep learning models. It is simple, yet effective in its design. It consists of three major components: (i) an encoder/decoder framework; (ii) SMOTE-based oversampling; and (iii) a dedicated loss function that is enhanced with a penalty term. An important advantage of DeepSMOTE over GAN-based oversampling is that DeepSMOTE does not require a discriminator, and it generates high-quality artificial images that are both information-rich and suitable for visual inspection. DeepSMOTE code is publicly available at: https://github.com/dd1github/DeepSMOTE

</p>
</details>

<details><summary><b>Goodness of Causal Fit</b>
<a href="https://arxiv.org/abs/2105.02172">arxiv:2105.02172</a>
&#x1F4C8; 6 <br>
<p>Robert R. Tucci</p></summary>
<p>

**Abstract:** We propose a Goodness of Causal Fit (GCF) measure which depends on Pearl "do" interventions. This is different from a measure of Goodness of Fit (GF), which does not use interventions. Given a DAG set ${\cal G}$, to find a good $G\in {\cal G}$, we propose plotting $GCF(G)$ versus $GF(G)$ for all $G\in {\cal G}$, and finding a graph $G\in {\cal G}$ with a large amount of both types of goodness.

</p>
</details>

<details><summary><b>RDMSim: An Exemplar for Evaluation and Comparison of Decision-Making Techniques for Self-Adaptation</b>
<a href="https://arxiv.org/abs/2105.01978">arxiv:2105.01978</a>
&#x1F4C8; 6 <br>
<p>Huma Samin, Luis H. Garcia Paucar, Nelly Bencomo, Cesar M. Carranza Hurtado, Erik M. Fredericks</p></summary>
<p>

**Abstract:** Decision-making for self-adaptation approaches need to address different challenges, including the quantification of the uncertainty of events that cannot be foreseen in advance and their effects, and dealing with conflicting objectives that inherently involve multi-objective decision making (e.g., avoiding costs vs. providing reliable service). To enable researchers to evaluate and compare decision-making techniques for self-adaptation, we present the RDMSim exemplar. RDMSim enables researchers to evaluate and compare techniques for decision-making under environmental uncertainty that support self-adaptation. The focus of the exemplar is on the domain problem related to Remote Data Mirroring, which gives opportunity to face the challenges described above. RDMSim provides probe and effector components for easy integration with external adaptation managers, which are associated with decision-making techniques and based on the MAPE-K loop. Specifically, the paper presents (i) RDMSim, a simulator for real-world experimentation, (ii) a set of realistic simulation scenarios that can be used for experimentation and comparison purposes, (iii) data for the sake of comparison.

</p>
</details>

<details><summary><b>Polygrammar: Grammar for Digital Polymer Representation and Generation</b>
<a href="https://arxiv.org/abs/2105.05278">arxiv:2105.05278</a>
&#x1F4C8; 5 <br>
<p>Minghao Guo, Wan Shou, Liane Makatura, Timothy Erps, Michael Foshey, Wojciech Matusik</p></summary>
<p>

**Abstract:** Polymers are widely-studied materials with diverse properties and applications determined by different molecular structures. It is essential to represent these structures clearly and explore the full space of achievable chemical designs. However, existing approaches are unable to offer comprehensive design models for polymers because of their inherent scale and structural complexity. Here, we present a parametric, context-sensitive grammar designed specifically for the representation and generation of polymers. As a demonstrative example, we implement our grammar for polyurethanes. Using our symbolic hypergraph representation and 14 simple production rules, our PolyGrammar is able to represent and generate all valid polyurethane structures. We also present an algorithm to translate any polyurethane structure from the popular SMILES string format into our PolyGrammar representation. We test the representative power of PolyGrammar by translating a dataset of over 600 polyurethane samples collected from literature. Furthermore, we show that PolyGrammar can be easily extended to the other copolymers and homopolymers such as polyacrylates. By offering a complete, explicit representation scheme and an explainable generative model with validity guarantees, our PolyGrammar takes an important step toward a more comprehensive and practical system for polymer discovery and exploration. As the first bridge between formal languages and chemistry, PolyGrammar also serves as a critical blueprint to inform the design of similar grammars for other chemistries, including organic and inorganic molecules.

</p>
</details>

<details><summary><b>TANGO: Commonsense Generalization in Predicting Tool Interactions for Mobile Manipulators</b>
<a href="https://arxiv.org/abs/2105.04556">arxiv:2105.04556</a>
&#x1F4C8; 4 <br>
<p>Shreshth Tuli, Rajas Bansal, Rohan Paul,  Mausam</p></summary>
<p>

**Abstract:** Robots assisting us in factories or homes must learn to make use of objects as tools to perform tasks, e.g., a tray for carrying objects. We consider the problem of learning commonsense knowledge of when a tool may be useful and how its use may be composed with other tools to accomplish a high-level task instructed by a human. We introduce a novel neural model, termed TANGO, for predicting task-specific tool interactions, trained using demonstrations from human teachers instructing a virtual robot. TANGO encodes the world state, comprising objects and symbolic relationships between them, using a graph neural network. The model learns to attend over the scene using knowledge of the goal and the action history, finally decoding the symbolic action to execute. Crucially, we address generalization to unseen environments where some known tools are missing, but alternative unseen tools are present. We show that by augmenting the representation of the environment with pre-trained embeddings derived from a knowledge-base, the model can generalize effectively to novel environments. Experimental results show a 60.5-78.9% absolute improvement over the baseline in predicting successful symbolic plans in unseen settings for a simulated mobile manipulator.

</p>
</details>

<details><summary><b>Predicting Intensive Care Unit Length of Stay and Mortality Using Patient Vital Signs: Machine Learning Model Development and Validation</b>
<a href="https://arxiv.org/abs/2105.04414">arxiv:2105.04414</a>
&#x1F4C8; 4 <br>
<p>Khalid Alghatani, Nariman Ammar, Abdelmounaam Rezgui, Arash Shaban-Nejad</p></summary>
<p>

**Abstract:** Patient monitoring is vital in all stages of care. We here report the development and validation of ICU length of stay and mortality prediction models. The models will be used in an intelligent ICU patient monitoring module of an Intelligent Remote Patient Monitoring (IRPM) framework that monitors the health status of patients, and generates timely alerts, maneuver guidance, or reports when adverse medical conditions are predicted. We utilized the publicly available Medical Information Mart for Intensive Care (MIMIC) database to extract ICU stay data for adult patients to build two prediction models: one for mortality prediction and another for ICU length of stay. For the mortality model, we applied six commonly used machine learning (ML) binary classification algorithms for predicting the discharge status (survived or not). For the length of stay model, we applied the same six ML algorithms for binary classification using the median patient population ICU stay of 2.64 days. For the regression-based classification, we used two ML algorithms for predicting the number of days. We built two variations of each prediction model: one using 12 baseline demographic and vital sign features, and the other based on our proposed quantiles approach, in which we use 21 extra features engineered from the baseline vital sign features, including their modified means, standard deviations, and quantile percentages. We could perform predictive modeling with minimal features while maintaining reasonable performance using the quantiles approach. The best accuracy achieved in the mortality model was approximately 89% using the random forest algorithm. The highest accuracy achieved in the length of stay model, based on the population median ICU stay (2.64 days), was approximately 65% using the random forest algorithm.

</p>
</details>

<details><summary><b>A Geometric Analysis of Neural Collapse with Unconstrained Features</b>
<a href="https://arxiv.org/abs/2105.02375">arxiv:2105.02375</a>
&#x1F4C8; 4 <br>
<p>Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, Qing Qu</p></summary>
<p>

**Abstract:** We provide the first global optimization landscape analysis of $Neural\;Collapse$ -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that ($i$) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and ($ii$) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified $unconstrained\;feature\;model$, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. In contrast to existing landscape analysis for deep neural networks which is often disconnected from practice, our analysis of the simplified model not only does it explain what kind of features are learned in the last layer, but it also shows why they can be efficiently optimized in the simplified settings, matching the empirical observations in practical deep network architectures. These findings could have profound implications for optimization, generalization, and robustness of broad interests. For example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over $20\%$ on ResNet18 without sacrificing the generalization performance.

</p>
</details>

<details><summary><b>Physics-informed Spline Learning for Nonlinear Dynamics Discovery</b>
<a href="https://arxiv.org/abs/2105.02368">arxiv:2105.02368</a>
&#x1F4C8; 4 <br>
<p>Fangzheng Sun, Yang Liu, Hao Sun</p></summary>
<p>

**Abstract:** Dynamical systems are typically governed by a set of linear/nonlinear differential equations. Distilling the analytical form of these equations from very limited data remains intractable in many disciplines such as physics, biology, climate science, engineering and social science. To address this fundamental challenge, we propose a novel Physics-informed Spline Learning (PiSL) framework to discover parsimonious governing equations for nonlinear dynamics, based on sparsely sampled noisy data. The key concept is to (1) leverage splines to interpolate locally the dynamics, perform analytical differentiation and build the library of candidate terms, (2) employ sparse representation of the governing equations, and (3) use the physics residual in turn to inform the spline learning. The synergy between splines and discovered underlying physics leads to the robust capacity of dealing with high-level data scarcity and noise. A hybrid sparsity-promoting alternating direction optimization strategy is developed for systematically pruning the sparse coefficients that form the structure and explicit expression of the governing equations. The efficacy and superiority of the proposed method have been demonstrated by multiple well-known nonlinear dynamical systems, in comparison with two state-of-the-art methods.

</p>
</details>

<details><summary><b>A unifying tutorial on Approximate Message Passing</b>
<a href="https://arxiv.org/abs/2105.02180">arxiv:2105.02180</a>
&#x1F4C8; 4 <br>
<p>Oliver Y. Feng, Ramji Venkataramanan, Cynthia Rush, Richard J. Samworth</p></summary>
<p>

**Abstract:** Over the last decade or so, Approximate Message Passing (AMP) algorithms have become extremely popular in various structured high-dimensional statistical problems. The fact that the origins of these techniques can be traced back to notions of belief propagation in the statistical physics literature lends a certain mystique to the area for many statisticians. Our goal in this work is to present the main ideas of AMP from a statistical perspective, to illustrate the power and flexibility of the AMP framework. Along the way, we strengthen and unify many of the results in the existing literature.

</p>
</details>

<details><summary><b>Bayesian Logistic Shape Model Inference: application to cochlea image segmentation</b>
<a href="https://arxiv.org/abs/2105.02045">arxiv:2105.02045</a>
&#x1F4C8; 4 <br>
<p>Wang Zihao, Demarcy Thomas, Vandersteen Clair, Gnansia Dan, Raffaelli Charles, Guevara Nicolas, Delingette Hervé</p></summary>
<p>

**Abstract:** Incorporating shape information is essential for the delineation of many organs and anatomical structures in medical images. While previous work has mainly focused on parametric spatial transformations applied on reference template shapes, in this paper, we address the Bayesian inference of parametric shape models for segmenting medical images with the objective to provide interpretable results. The proposed framework defines a likelihood appearance probability and a prior label probability based on a generic shape function through a logistic function. A reference length parameter defined in the sigmoid controls the trade-off between shape and appearance information. The inference of shape parameters is performed within an Expectation-Maximisation approach where a Gauss-Newton optimization stage allows to provide an approximation of the posterior probability of shape parameters. This framework is applied to the segmentation of cochlea structures from clinical CT images constrained by a 10 parameter shape model. It is evaluated on three different datasets, one of which includes more than 200 patient images. The results show performances comparable to supervised methods and better than previously proposed unsupervised ones. It also enables an analysis of parameter distributions and the quantification of segmentation uncertainty including the effect of the shape model.

</p>
</details>

<details><summary><b>Using Synthetic Data to Enhance the Accuracy of Fingerprint-Based Localization: A Deep Learning Approach</b>
<a href="https://arxiv.org/abs/2105.01903">arxiv:2105.01903</a>
&#x1F4C8; 4 <br>
<p>Mohammad Nabati, Hojjat Navidan, Reza Shahbazian, Seyed Ali Ghorashi, David Windridge</p></summary>
<p>

**Abstract:** Human-centered data collection is typically costly and implicates issues of privacy. Various solutions have been proposed in the literature to reduce this cost, such as crowdsourced data collection, or the use of semi-supervised algorithms. However, semi-supervised algorithms require a source of unlabeled data, and crowd-sourcing methods require numbers of active participants. An alternative passive data collection modality is fingerprint-based localization. Such methods use received signal strength (RSS) or channel state information (CSI) in wireless sensor networks to localize users in indoor/outdoor environments. In this paper, we introduce a novel approach to reduce training data collection costs in fingerprint-based localization by using synthetic data. Generative adversarial networks (GANs) are used to learn the distribution of a limited sample of collected data and, following this, to produce synthetic data that can be used to augment the real collected data in order to increase overall positioning accuracy. Experimental results on a benchmark dataset show that by applying the proposed method and using a combination of 10% collected data and 90% synthetic data, we can obtain essentially similar positioning accuracy to that which would be obtained by using the full set of collected data. This means that by employing GAN-generated synthetic data, we can use 90% less real data, thereby reduce data-collection costs while achieving acceptable accuracy.

</p>
</details>

<details><summary><b>Incentivizing Efficient Equilibria in Traffic Networks with Mixed Autonomy</b>
<a href="https://arxiv.org/abs/2106.04678">arxiv:2106.04678</a>
&#x1F4C8; 3 <br>
<p>Erdem Bıyık, Daniel A. Lazar, Ramtin Pedarsani, Dorsa Sadigh</p></summary>
<p>

**Abstract:** Traffic congestion has large economic and social costs. The introduction of autonomous vehicles can potentially reduce this congestion by increasing road capacity via vehicle platooning and by creating an avenue for influencing people's choice of routes. We consider a network of parallel roads with two modes of transportation: (i) human drivers, who will choose the quickest route available to them, and (ii) a ride hailing service, which provides an array of autonomous vehicle route options, each with different prices, to users. We formalize a model of vehicle flow in mixed autonomy and a model of how autonomous service users make choices between routes with different prices and latencies. Developing an algorithm to learn the preferences of the users, we formulate a planning optimization that chooses prices to maximize a social objective. We demonstrate the benefit of the proposed scheme by comparing the results to theoretical benchmarks which we show can be efficiently calculated.

</p>
</details>

<details><summary><b>Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions</b>
<a href="https://arxiv.org/abs/2105.03248">arxiv:2105.03248</a>
&#x1F4C8; 3 <br>
<p>Dan Geiger, David Heckerman</p></summary>
<p>

**Abstract:** We develop simple methods for constructing parameter priors for model choice among Directed Acyclic Graphical (DAG) models. In particular, we introduce several assumptions that permit the construction of parameter priors for a large number of DAG models from a small set of assessments. We then present a method for directly computing the marginal likelihood of every DAG model given a random sample with no missing observations. We apply this methodology to Gaussian DAG models which consist of a recursive set of linear regression models. We show that the only parameter prior for complete Gaussian DAG models that satisfies our assumptions is the normal-Wishart distribution. Our analysis is based on the following new characterization of the Wishart distribution: let $W$ be an $n \times n$, $n \ge 3$, positive-definite symmetric matrix of random variables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if and only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of $\{W_{12},W_{22}\}$ for every block partitioning $W_{11},W_{12}, W'_{12}, W_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart distributions are provided as well.

</p>
</details>

<details><summary><b>LGGNet: Learning from Local-Global-Graph Representations for Brain-Computer Interface</b>
<a href="https://arxiv.org/abs/2105.02786">arxiv:2105.02786</a>
&#x1F4C8; 3 <br>
<p>Yi Ding, Neethu Robinson, Qiuhao Zeng, Cuntai Guan</p></summary>
<p>

**Abstract:** In this paper, we propose LGG, a neurologically inspired graph neural network, to learn local-global-graph representations from Electroencephalography (EEG) for a Brain-Computer Interface (BCI). A temporal convolutional layer with multi-scale 1D convolutional kernels and kernel-level attention fusion is proposed to learn the temporal dynamics of EEG. Inspired by neurological knowledge of cognitive processes in the brain, we propose local and global graph-filtering layers to learn the brain activities within and between different functional areas of the brain to model the complex relations among them during the cognitive processes. Under the robust nested cross-validation settings, the proposed method is evaluated on the publicly available dataset DEAP, and the classification performance is compared with state-of-the-art methods, such as FBFgMDM, FBTSC, Unsupervised learning, DeepConvNet, ShallowConvNet, EEGNet, and TSception. The results show that the proposed method outperforms all these state-of-the-art methods, and the improvements are statistically significant (p<0.05) in most cases. The source code can be found at: https://github.com/yi-ding-cs/LGG

</p>
</details>

<details><summary><b>Non-asymptotic analysis and inference for an outlyingness induced winsorized mean</b>
<a href="https://arxiv.org/abs/2105.02337">arxiv:2105.02337</a>
&#x1F4C8; 3 <br>
<p>Yijun Zuo</p></summary>
<p>

**Abstract:** Robust estimation of a mean vector, a topic regarded as obsolete in the traditional robust statistics community, has recently surged in machine learning literature in the last decade. The latest focus is on the sub-Gaussian performance and computability of the estimators in a non-asymptotic setting. Numerous traditional robust estimators are computationally intractable, which partly contributes to the renewal of the interest in the robust mean estimation.
  Robust centrality estimators, however, include the trimmed mean and the sample median. The latter has the best robustness but suffers a low-efficiency drawback. Trimmed mean and median of means, %as robust alternatives to the sample mean, and achieving sub-Gaussian performance have been proposed and studied in the literature.
  This article investigates the robustness of leading sub-Gaussian estimators of mean and reveals that none of them can resist greater than $25\%$ contamination in data and consequently introduces an outlyingness induced winsorized mean which has the best possible robustness (can resist up to $50\%$ contamination without breakdown) meanwhile achieving high efficiency. Furthermore, it has a sub-Gaussian performance for uncontaminated samples and a bounded estimation error for contaminated samples at a given confidence level in a finite sample setting. It can be computed in linear time.

</p>
</details>

<details><summary><b>Conditional Invertible Neural Networks for Diverse Image-to-Image Translation</b>
<a href="https://arxiv.org/abs/2105.02104">arxiv:2105.02104</a>
&#x1F4C8; 3 <br>
<p>Lynton Ardizzone, Jakob Kruse, Carsten Lüth, Niels Bracher, Carsten Rother, Ullrich Köthe</p></summary>
<p>

**Abstract:** We introduce a new architecture called a conditional invertible neural network (cINN), and use it to address the task of diverse image-to-image translation for natural images. This is not easily possible with existing INN models due to some fundamental limitations. The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning image into maximally informative features. All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INN-based models have received far less attention in the literature than GANs, they have been shown to have some remarkable properties absent in GANs, e.g. apparent immunity to mode collapse. We find that our cINNs leverage these properties for image-to-image translation, demonstrated on day to night translation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.

</p>
</details>

<details><summary><b>Prototype Memory for Large-scale Face Representation Learning</b>
<a href="https://arxiv.org/abs/2105.02103">arxiv:2105.02103</a>
&#x1F4C8; 3 <br>
<p>Evgeny Smirnov, Nikita Garaev, Vasiliy Galyuk</p></summary>
<p>

**Abstract:** Face representation learning using datasets with massive number of identities requires appropriate training methods. Softmax-based approach, currently the state-of-the-art in face recognition, in its usual "full softmax" form is not suitable for datasets with millions of persons. Several methods, based on the "sampled softmax" approach, were proposed to remove this limitation. These methods, however, have a set of disadvantages. One of them is a problem of "prototype obsolescence": classifier weights (prototypes) of the rarely sampled classes, receive too scarce gradients and become outdated and detached from the current encoder state, resulting in an incorrect training signals. This problem is especially serious in ultra-large-scale datasets. In this paper, we propose a novel face representation learning model called Prototype Memory, which alleviates this problem and allows training on a dataset of any size. Prototype Memory consists of the limited-size memory module for storing recent class prototypes and employs a set of algorithms to update it in appropriate way. New class prototypes are generated on the fly using exemplar embeddings in the current mini-batch. These prototypes are enqueued to the memory and used in a role of classifier weights for usual softmax classification-based training. To prevent obsolescence and keep the memory in close connection with encoder, prototypes are regularly refreshed, and oldest ones are dequeued and disposed. Prototype Memory is computationally efficient and independent of dataset size. It can be used with various loss functions, hard example mining algorithms and encoder architectures. We prove the effectiveness of the proposed model by extensive experiments on popular face recognition benchmarks.

</p>
</details>

<details><summary><b>Exploring emotional prototypes in a high dimensional TTS latent space</b>
<a href="https://arxiv.org/abs/2105.01891">arxiv:2105.01891</a>
&#x1F4C8; 3 <br>
<p>Pol van Rijn, Silvan Mertes, Dominik Schiller, Peter M. C. Harrison, Pauline Larrouy-Maestri, Elisabeth André, Nori Jacoby</p></summary>
<p>

**Abstract:** Recent TTS systems are able to generate prosodically varied and realistic speech. However, it is unclear how this prosodic variation contributes to the perception of speakers' emotional states. Here we use the recent psychological paradigm 'Gibbs Sampling with People' to search the prosodic latent space in a trained GST Tacotron model to explore prototypes of emotional prosody. Participants are recruited online and collectively manipulate the latent space of the generative speech model in a sequentially adaptive way so that the stimulus presented to one group of participants is determined by the response of the previous groups. We demonstrate that (1) particular regions of the model's latent space are reliably associated with particular emotions, (2) the resulting emotional prototypes are well-recognized by a separate group of human raters, and (3) these emotional prototypes can be effectively transferred to new sentences. Collectively, these experiments demonstrate a novel approach to the understanding of emotional speech by providing a tool to explore the relation between the latent space of generative models and human semantics.

</p>
</details>

<details><summary><b>Comparison of Traditional and Hybrid Time Series Models for Forecasting COVID-19 Cases</b>
<a href="https://arxiv.org/abs/2105.03266">arxiv:2105.03266</a>
&#x1F4C8; 2 <br>
<p>Samyak Prajapati, Aman Swaraj, Ronak Lalwani, Akhil Narwal, Karan Verma, Ghanshyam Singh, Ashok Kumar</p></summary>
<p>

**Abstract:** Time series forecasting methods play critical role in estimating the spread of an epidemic. The coronavirus outbreak of December 2019 has already infected millions all over the world and continues to spread on. Just when the curve of the outbreak had started to flatten, many countries have again started to witness a rise in cases which is now being referred as the 2nd wave of the pandemic. A thorough analysis of time-series forecasting models is therefore required to equip state authorities and health officials with immediate strategies for future times. This aims of the study are three-fold: (a) To model the overall trend of the spread; (b) To generate a short-term forecast of 10 days in countries with the highest incidence of confirmed cases (USA, India and Brazil); (c) To quantitatively determine the algorithm that is best suited for precise modelling of the linear and non-linear features of the time series. The comparison of forecasting models for the total cumulative cases of each country is carried out by comparing the reported data and the predicted value, and then ranking the algorithms (Prophet, Holt-Winters, LSTM, ARIMA, and ARIMA-NARNN) based on their RMSE, MAE and MAPE values. The hybrid combination of ARIMA and NARNN (Nonlinear Auto-Regression Neural Network) gave the best result among the selected models with a reduced RMSE, which proved to be almost 35.3% better than one of the most prevalent method of time-series prediction (ARIMA). The results demonstrated the efficacy of the hybrid implementation of the ARIMA-NARNN model over other forecasting methods such as Prophet, Holt Winters, LSTM, and the ARIMA model in encapsulating the linear as well as non-linear patterns of the epidemical datasets.

</p>
</details>

<details><summary><b>This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks</b>
<a href="https://arxiv.org/abs/2105.02968">arxiv:2105.02968</a>
&#x1F4C8; 2 <br>
<p>Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler</p></summary>
<p>

**Abstract:** Deep neural networks that yield human interpretable decisions by architectural design have lately become an increasingly popular alternative to post hoc interpretation of traditional black-box models. Among these networks, the arguably most widespread approach is so-called prototype learning, where similarities to learned latent prototypes serve as the basis of classifying an unseen data point. In this work, we point to an important shortcoming of such approaches. Namely, there is a semantic gap between similarity in latent space and similarity in input space, which can corrupt interpretability. We design two experiments that exemplify this issue on the so-called ProtoPNet. Specifically, we find that this network's interpretability mechanism can be led astray by intentionally crafted or even JPEG compression artefacts, which can produce incomprehensible decisions. We argue that practitioners ought to have this shortcoming in mind when deploying prototype-based models in practice.

</p>
</details>

<details><summary><b>Image Embedding and Model Ensembling for Automated Chest X-Ray Interpretation</b>
<a href="https://arxiv.org/abs/2105.02966">arxiv:2105.02966</a>
&#x1F4C8; 2 <br>
<p>Edoardo Giacomello, Pier Luca Lanzi, Daniele Loiacono, Luca Nassano</p></summary>
<p>

**Abstract:** Chest X-ray (CXR) is perhaps the most frequently-performed radiological investigation globally. In this work, we present and study several machine learning approaches to develop automated CXR diagnostic models. In particular, we trained several Convolutional Neural Networks (CNN) on the CheXpert dataset, a large collection of more than 200k CXR labeled images. Then, we used the trained CNNs to compute embeddings of the CXR images, in order to train two sets of tree-based classifiers from them. Finally, we described and compared three ensembling strategies to combine together the classifiers trained. Rather than expecting some performance-wise benefits, our goal in this work is showing that the above two methodologies, i.e., the extraction of image embeddings and models ensembling, can be effective and viable to solve tasks that require medical imaging understanding. Our results in that perspective are encouraging and worthy of further investigation.

</p>
</details>

<details><summary><b>Granger Causality: A Review and Recent Advances</b>
<a href="https://arxiv.org/abs/2105.02675">arxiv:2105.02675</a>
&#x1F4C8; 2 <br>
<p>Ali Shojaie, Emily B. Fox</p></summary>
<p>

**Abstract:** Introduced more than a half century ago, Granger causality has become a popular tool for analyzing time series data in many application domains, from economics and finance to genomics and neuroscience. Despite this popularity, the validity of this notion for inferring causal relationships among time series has remained the topic of continuous debate. Moreover, while the original definition was general, limitations in computational tools have primarily limited the applications of Granger causality to simple bivariate vector auto-regressive processes or pairwise relationships among a set of variables. Starting with a review of early developments and debates, this paper discusses recent advances that address various shortcomings of the earlier approaches, from models for high-dimensional time series to more recent developments that account for nonlinear and non-Gaussian observations and allow for sub-sampled and mixed frequency time series.

</p>
</details>

<details><summary><b>Security Vulnerability Detection Using Deep Learning Natural Language Processing</b>
<a href="https://arxiv.org/abs/2105.02388">arxiv:2105.02388</a>
&#x1F4C8; 2 <br>
<p>Noah Ziems, Shaoen Wu</p></summary>
<p>

**Abstract:** Detecting security vulnerabilities in software before they are exploited has been a challenging problem for decades. Traditional code analysis methods have been proposed, but are often ineffective and inefficient. In this work, we model software vulnerability detection as a natural language processing (NLP) problem with source code treated as texts, and address the automated software venerability detection with recent advanced deep learning NLP models assisted by transfer learning on written English. For training and testing, we have preprocessed the NIST NVD/SARD databases and built a dataset of over 100,000 files in $C$ programming language with 123 types of vulnerabilities. The extensive experiments generate the best performance of over 93\% accuracy in detecting security vulnerabilities.

</p>
</details>

<details><summary><b>Genetic Algorithms For Extractive Summarization</b>
<a href="https://arxiv.org/abs/2105.02365">arxiv:2105.02365</a>
&#x1F4C8; 2 <br>
<p>William Chen, Kensal Ramos, Kalyan Naidu Mullaguri</p></summary>
<p>

**Abstract:** Most current work in NLP utilizes deep learning, which requires a lot of training data and computational power. This paper investigates the strengths of Genetic Algorithms (GAs) for extractive summarization, as we hypothesized that GAs could construct more efficient solutions for the summarization task due to their relative customizability relative to deep learning models. This is done by building a vocabulary set, the words of which are represented as an array of weights, and optimizing those set of weights with the GA. These weights can be used to build an overall weighting of a sentence, which can then be passed to some threshold for extraction. Our results showed that the GA was able to learn a weight representation that could filter out excessive vocabulary and thus dictate sentence importance based on common English words.

</p>
</details>

<details><summary><b>R2U3D: Recurrent Residual 3D U-Net for Lung Segmentation</b>
<a href="https://arxiv.org/abs/2105.02290">arxiv:2105.02290</a>
&#x1F4C8; 2 <br>
<p>Dhaval D. Kadia, Md Zahangir Alom, Ranga Burada, Tam V. Nguyen, Vijayan K. Asari</p></summary>
<p>

**Abstract:** 3D lung segmentation is essential since it processes the volumetric information of the lungs, removes the unnecessary areas of the scan, and segments the actual area of the lungs in a 3D volume. Recently, the deep learning model, such as U-Net outperforms other network architectures for biomedical image segmentation. In this paper, we propose a novel model, namely, Recurrent Residual 3D U-Net (R2U3D), for the 3D lung segmentation task. In particular, the proposed model integrates 3D convolution into the Recurrent Residual Neural Network based on U-Net. It helps learn spatial dependencies in 3D and increases the propagation of 3D volumetric information. The proposed R2U3D network is trained on the publicly available dataset LUNA16 and it achieves state-of-the-art performance on both LUNA16 (testing set) and VESSEL12 dataset. In addition, we show that training the R2U3D model with a smaller number of CT scans, i.e., 100 scans, without applying data augmentation achieves an outstanding result in terms of Soft Dice Similarity Coefficient (Soft-DSC) of 0.9920.

</p>
</details>

<details><summary><b>Physically Inspired Dense Fusion Networks for Relighting</b>
<a href="https://arxiv.org/abs/2105.02209">arxiv:2105.02209</a>
&#x1F4C8; 2 <br>
<p>Amirsaeed Yazdani, Tiantong Guo, Vishal Monga</p></summary>
<p>

**Abstract:** Image relighting has emerged as a problem of significant research interest inspired by augmented reality applications. Physics-based traditional methods, as well as black box deep learning models, have been developed. The existing deep networks have exploited training to achieve a new state of the art; however, they may perform poorly when training is limited or does not represent problem phenomenology, such as the addition or removal of dense shadows. We propose a model which enriches neural networks with physical insight. More precisely, our method generates the relighted image with new illumination settings via two different strategies and subsequently fuses them using a weight map (w). In the first strategy, our model predicts the material reflectance parameters (albedo) and illumination/geometry parameters of the scene (shading) for the relit image (we refer to this strategy as intrinsic image decomposition (IID)). The second strategy is solely based on the black box approach, where the model optimizes its weights based on the ground-truth images and the loss terms in the training stage and generates the relit output directly (we refer to this strategy as direct). While our proposed method applies to both one-to-one and any-to-any relighting problems, for each case we introduce problem-specific components that enrich the model performance: 1) For one-to-one relighting we incorporate normal vectors of the surfaces in the scene to adjust gloss and shadows accordingly in the image. 2) For any-to-any relighting, we propose an additional multiscale block to the architecture to enhance feature extraction. Experimental results on the VIDIT 2020 and the VIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that our proposal can outperform many state-of-the-art methods in terms of well-known fidelity metrics and perceptual loss.

</p>
</details>

<details><summary><b>End-to-End Diarization for Variable Number of Speakers with Local-Global Networks and Discriminative Speaker Embeddings</b>
<a href="https://arxiv.org/abs/2105.02096">arxiv:2105.02096</a>
&#x1F4C8; 2 <br>
<p>Soumi Maiti, Hakan Erdogan, Kevin Wilson, Scott Wisdom, Shinji Watanabe, John R. Hershey</p></summary>
<p>

**Abstract:** We present an end-to-end deep network model that performs meeting diarization from single-channel audio recordings. End-to-end diarization models have the advantage of handling speaker overlap and enabling straightforward handling of discriminative training, unlike traditional clustering-based diarization methods. The proposed system is designed to handle meetings with unknown numbers of speakers, using variable-number permutation-invariant cross-entropy based loss functions. We introduce several components that appear to help with diarization performance, including a local convolutional network followed by a global self-attention module, multi-task transfer learning using a speaker identification component, and a sequential approach where the model is refined with a second stage. These are trained and validated on simulated meeting data based on LibriSpeech and LibriTTS datasets; final evaluations are done using LibriCSS, which consists of simulated meetings recorded using real acoustics via loudspeaker playback. The proposed model performs better than previously proposed end-to-end diarization models on these data.

</p>
</details>

<details><summary><b>AdaBoost and robust one-bit compressed sensing</b>
<a href="https://arxiv.org/abs/2105.02083">arxiv:2105.02083</a>
&#x1F4C8; 2 <br>
<p>Geoffrey Chinot, Felix Kuchelmeister, Matthias Löffler, Sara van de Geer</p></summary>
<p>

**Abstract:** This paper studies binary classification in robust one-bit compressed sensing with adversarial errors. It is assumed that the model is overparameterized and that the parameter of interest is effectively sparse. AdaBoost is considered, and, through its relation to the max-$\ell_1$-margin-classifier, prediction error bounds are derived. The developed theory is general and allows for heavy-tailed feature distributions, requiring only a weak moment assumption and an anti-concentration condition. Improved convergence rates are shown when the features satisfy a small deviation lower bound. In particular, the results provide an explanation why interpolating adversarial noise can be harmless for classification problems. Simulations illustrate the presented theory.

</p>
</details>

<details><summary><b>Towards Interpretable and Transferable Speech Emotion Recognition: Latent Representation Based Analysis of Features, Methods and Corpora</b>
<a href="https://arxiv.org/abs/2105.02055">arxiv:2105.02055</a>
&#x1F4C8; 2 <br>
<p>Sneha Das, Nicole Nadine Lønfeldt, Anne Katrine Pagsberg, Line H. Clemmensen</p></summary>
<p>

**Abstract:** In recent years, speech emotion recognition (SER) has been used in wide ranging applications, from healthcare to the commercial sector. In addition to signal processing approaches, methods for SER now also use deep learning techniques. However, generalizing over languages, corpora and recording conditions is still an open challenge in the field. Furthermore, due to the black-box nature of deep learning algorithms, a newer challenge is the lack of interpretation and transparency in the models and the decision making process. This is critical when the SER systems are deployed in applications that influence human lives. In this work we address this gap by providing an in-depth analysis of the decision making process of the proposed SER system. Towards that end, we present low-complexity SER based on undercomplete- and denoising- autoencoders that achieve an average classification accuracy of over 55\% for four-class emotion classification. Following this, we investigate the clustering of emotions in the latent space to understand the influence of the corpora on the model behavior and to obtain a physical interpretation of the latent embedding. Lastly, we explore the role of each input feature towards the performance of the SER.

</p>
</details>

<details><summary><b>MCGNet: Partial Multi-view Few-shot Learning via Meta-alignment and Context Gated-aggregation</b>
<a href="https://arxiv.org/abs/2105.02046">arxiv:2105.02046</a>
&#x1F4C8; 2 <br>
<p>Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Meng Wang</p></summary>
<p>

**Abstract:** In this paper, we propose a new challenging task named as \textbf{partial multi-view few-shot learning}, which unifies two tasks, i.e. few-shot learning and partial multi-view learning, together. Different from the traditional few-shot learning, this task aims to solve the few-shot learning problem given the incomplete multi-view prior knowledge, which conforms more with the real-world applications. However, this brings about two difficulties within this task. First, the gaps among different views can be large and hard to reduce, especially with sample scarcity. Second, due to the incomplete view information, few-shot learning becomes more challenging than the traditional one. To deal with the above issues, we propose a new \textbf{Meta-alignment and Context Gated-aggregation Network} by equipping meta-alignment and context gated-aggregation with partial multi-view GNNs. Specifically, the meta-alignment effectively maps the features from different views into a more compact latent space, thereby reducing the view gaps. Moreover, the context gated-aggregation alleviates the view-missing influence by leveraging the cross-view context. Extensive experiments are conducted on the PIE and ORL dataset for evaluating our proposed method. By comparing with other few-shot learning methods, our method obtains the state-of-the-art performance especially with heavily-missing views.

</p>
</details>

<details><summary><b>Non-Autoregressive vs Autoregressive Neural Networks for System Identification</b>
<a href="https://arxiv.org/abs/2105.02027">arxiv:2105.02027</a>
&#x1F4C8; 2 <br>
<p>Daniel Weber, Clemens Gühmann</p></summary>
<p>

**Abstract:** The application of neural networks to non-linear dynamic system identification tasks has a long history, which consists mostly of autoregressive approaches. Autoregression, the usage of the model outputs of previous time steps, is a method of transferring a system state between time steps, which is not necessary for modeling dynamic systems with modern neural network structures, such as gated recurrent units (GRUs) and Temporal Convolutional Networks (TCNs). We compare the accuracy and execution performance of autoregressive and non-autoregressive implementations of a GRU and TCN on the simulation task of three publicly available system identification benchmarks. Our results show, that the non-autoregressive neural networks are significantly faster and at least as accurate as their autoregressive counterparts. Comparisons with other state-of-the-art black-box system identification methods show, that our implementation of the non-autoregressive GRU is the best performing neural network-based system identification method, and in the benchmarks without extrapolation, the best performing black-box method.

</p>
</details>

<details><summary><b>Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2105.02001">arxiv:2105.02001</a>
&#x1F4C8; 2 <br>
<p>Robert A. Marsden, Alexander Bartler, Mario Döbler, Bin Yang</p></summary>
<p>

**Abstract:** Deep convolutional neural networks have considerably improved state-of-the-art results for semantic segmentation. Nevertheless, even modern architectures lack the ability to generalize well to a test dataset that originates from a different domain. To avoid the costly annotation of training data for unseen domains, unsupervised domain adaptation (UDA) attempts to provide efficient knowledge transfer from a labeled source domain to an unlabeled target domain. Previous work has mainly focused on minimizing the discrepancy between the two domains by using adversarial training or self-training. While adversarial training may fail to align the correct semantic categories as it minimizes the discrepancy between the global distributions, self-training raises the question of how to provide reliable pseudo-labels. To align the correct semantic categories across domains, we propose a contrastive learning approach that adapts category-wise centroids across domains. Furthermore, we extend our method with self-training, where we use a memory-efficient temporal ensemble to generate consistent and reliable pseudo-labels. Although both contrastive learning and self-training (CLST) through temporal ensembling enable knowledge transfer between two domains, it is their combination that leads to a symbiotic structure. We validate our approach on two domain adaptation benchmarks: GTA5 $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes. Our method achieves better or comparable results than the state-of-the-art. We will make the code publicly available.

</p>
</details>

<details><summary><b>Perceptual Gradient Networks</b>
<a href="https://arxiv.org/abs/2105.01957">arxiv:2105.01957</a>
&#x1F4C8; 2 <br>
<p>Dmitry Nikulin, Roman Suvorov, Aleksei Ivakhnenko, Victor Lempitsky</p></summary>
<p>

**Abstract:** Many applications of deep learning for image generation use perceptual losses for either training or fine-tuning of the generator networks. The use of perceptual loss however incurs repeated forward-backward passes in a large image classification network as well as a considerable memory overhead required to store the activations of this network. It is therefore desirable or sometimes even critical to get rid of these overheads.
  In this work, we propose a way to train generator networks using approximations of perceptual loss that are computed without forward-backward passes. Instead, we use a simpler perceptual gradient network that directly synthesizes the gradient field of a perceptual loss. We introduce the concept of proxy targets, which stabilize the predicted gradient, meaning that learning with it does not lead to divergence or oscillations. In addition, our method allows interpretation of the predicted gradient, providing insight into the internals of perceptual loss and suggesting potential ways to improve it in future work.

</p>
</details>

<details><summary><b>Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder</b>
<a href="https://arxiv.org/abs/2105.01924">arxiv:2105.01924</a>
&#x1F4C8; 2 <br>
<p>Jonas Wurst, Lakshman Balasubramanian, Michael Botsch, Wolfgang Utschick</p></summary>
<p>

**Abstract:** Detecting unknown and untested scenarios is crucial for scenario-based testing. Scenario-based testing is considered to be a possible approach to validate autonomous vehicles. A traffic scenario consists of multiple components, with infrastructure being one of it. In this work, a method to detect novel traffic scenarios based on their infrastructure images is presented. An autoencoder triplet network provides latent representations for infrastructure images which are used for outlier detection. The triplet training of the network is based on the connectivity graphs of the infrastructure. By using the proposed architecture, expert-knowledge is used to shape the latent space such that it incorporates a pre-defined similarity in the neighborhood relationships of an autoencoder. An ablation study on the architecture is highlighting the importance of the triplet autoencoder combination. The best performing architecture is based on vision transformers, a convolution-free attention-based network. The presented method outperforms other state-of-the-art outlier detection approaches.

</p>
</details>

<details><summary><b>Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy</b>
<a href="https://arxiv.org/abs/2105.01893">arxiv:2105.01893</a>
&#x1F4C8; 2 <br>
<p>Zhengxin Yang</p></summary>
<p>

**Abstract:** Simultaneous translation, which starts translating each sentence after receiving only a few words in source sentence, has a vital role in many scenarios. Although the previous prefix-to-prefix framework is considered suitable for simultaneous translation and achieves good performance, it still has two inevitable drawbacks: the high computational resource costs caused by the need to train a separate model for each latency $k$ and the insufficient ability to encode information because each target token can only attend to a specific source prefix. We propose a novel framework that adopts a simple but effective decoding strategy which is designed for full-sentence models. Within this framework, training a single full-sentence model can achieve arbitrary given latency and save computational resources. Besides, with the competence of the full-sentence model to encode the whole sentence, our decoding strategy can enhance the information maintained in the decoded states in real time. Experimental results show that our method achieves better translation quality than baselines on 4 directions: Zh$\rightarrow$En, En$\rightarrow$Ro and En$\leftrightarrow$De.

</p>
</details>

<details><summary><b>Change Matters: Medication Change Prediction with Recurrent Residual Networks</b>
<a href="https://arxiv.org/abs/2105.01876">arxiv:2105.01876</a>
&#x1F4C8; 2 <br>
<p>Chaoqi Yang, Cao Xiao, Lucas Glass, Jimeng Sun</p></summary>
<p>

**Abstract:** Deep learning is revolutionizing predictive healthcare, including recommending medications to patients with complex health conditions. Existing approaches focus on predicting all medications for the current visit, which often overlaps with medications from previous visits. A more clinically relevant task is to identify medication changes.
  In this paper, we propose a new recurrent residual network, named MICRON, for medication change prediction. MICRON takes the changes in patient health records as input and learns to update a hidden medication vector and the medication set recurrently with a reconstruction design. The medication vector is like the memory cell that encodes longitudinal information of medications. Unlike traditional methods that require the entire patient history for prediction, MICRON has a residual-based inference that allows for sequential updating based only on new patient features (e.g., new diagnoses in the recent visit) more efficiently.
  We evaluated MICRON on real inpatient and outpatient datasets. MICRON achieves 3.5% and 7.8% relative improvements over the best baseline in F1 score, respectively. MICRON also requires fewer parameters, which significantly reduces the training time to 38.3s per epoch with 1.5x speed-up.

</p>
</details>

<details><summary><b>Modulating Regularization Frequency for Efficient Compression-Aware Model Training</b>
<a href="https://arxiv.org/abs/2105.01875">arxiv:2105.01875</a>
&#x1F4C8; 2 <br>
<p>Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, Baeseong Park, Yongkweon Jeon</p></summary>
<p>

**Abstract:** While model compression is increasingly important because of large neural network size, compression-aware training is challenging as it needs sophisticated model modifications and longer training time.In this paper, we introduce regularization frequency (i.e., how often compression is performed during training) as a new regularization technique for a practical and efficient compression-aware training method. For various regularization techniques, such as weight decay and dropout, optimizing the regularization strength is crucial to improve generalization in Deep Neural Networks (DNNs). While model compression also demands the right amount of regularization, the regularization strength incurred by model compression has been controlled only by compression ratio. Throughout various experiments, we show that regularization frequency critically affects the regularization strength of model compression. Combining regularization frequency and compression ratio, the amount of weight updates by model compression per mini-batch can be optimized to achieve the best model accuracy. Modulating regularization frequency is implemented by occasional model compression while conventional compression-aware training is usually performed for every mini-batch.

</p>
</details>

<details><summary><b>A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs</b>
<a href="https://arxiv.org/abs/2105.01867">arxiv:2105.01867</a>
&#x1F4C8; 2 <br>
<p>Devansh Bisla, Apoorva Nandini Saridena, Anna Choromanska</p></summary>
<p>

**Abstract:** This paper focuses on understanding how the generalization error scales with the amount of the training data for deep neural networks (DNNs). Existing techniques in statistical learning require computation of capacity measures, such as VC dimension, to provably bound this error. It is however unclear how to extend these measures to DNNs and therefore the existing analyses are applicable to simple neural networks, which are not used in practice, e.g., linear or shallow ones or otherwise multi-layer perceptrons. Moreover, many theoretical error bounds are not empirically verifiable. We derive estimates of the generalization error that hold for deep networks and do not rely on unattainable capacity measures. The enabling technique in our approach hinges on two major assumptions: i) the network achieves zero training error, ii) the probability of making an error on a test point is proportional to the distance between this point and its nearest training point in the feature space and at a certain maximal distance (that we call radius) it saturates. Based on these assumptions we estimate the generalization error of DNNs. The obtained estimate scales as O(1/(δN^{1/d})), where N is the size of the training data and is parameterized by two quantities, the effective dimensionality of the data as perceived by the network (d) and the aforementioned radius (δ), both of which we find empirically. We show that our estimates match with the experimentally obtained behavior of the error on multiple learning tasks using benchmark data-sets and realistic models. Estimating training data requirements is essential for deployment of safety critical applications such as autonomous driving etc. Furthermore, collecting and annotating training data requires a huge amount of financial, computational and human resources. Our empirical estimates will help to efficiently allocate resources.

</p>
</details>

<details><summary><b>Exploiting Vulnerabilities in Deep Neural Networks: Adversarial and Fault-Injection Attacks</b>
<a href="https://arxiv.org/abs/2105.03251">arxiv:2105.03251</a>
&#x1F4C8; 1 <br>
<p>Faiq Khalid, Muhammad Abdullah Hanif, Muhammad Shafique</p></summary>
<p>

**Abstract:** From tiny pacemaker chips to aircraft collision avoidance systems, the state-of-the-art Cyber-Physical Systems (CPS) have increasingly started to rely on Deep Neural Networks (DNNs). However, as concluded in various studies, DNNs are highly susceptible to security threats, including adversarial attacks. In this paper, we first discuss different vulnerabilities that can be exploited for generating security attacks for neural network-based systems. We then provide an overview of existing adversarial and fault-injection-based attacks on DNNs. We also present a brief analysis to highlight different challenges in the practical implementation of adversarial attacks. Finally, we also discuss various prospective ways to develop robust DNN-based systems that are resilient to adversarial and fault-injection attacks.

</p>
</details>

<details><summary><b>Activity-Aware Deep Cognitive Fatigue Assessment using Wearables</b>
<a href="https://arxiv.org/abs/2105.02824">arxiv:2105.02824</a>
&#x1F4C8; 1 <br>
<p>Mohammad Arif Ul Alam</p></summary>
<p>

**Abstract:** Cognitive fatigue has been a common problem among workers which has become an increasing global problem since the emergence of COVID-19 as a global pandemic. While existing multi-modal wearable sensors-aided automatic cognitive fatigue monitoring tools have focused on physical and physiological sensors (ECG, PPG, Actigraphy) analytic on specific group of people (say gamers, athletes, construction workers), activity-awareness is utmost importance due to its different responses on physiology in different person. In this paper, we propose a novel framework, Activity-Aware Recurrent Neural Network (\emph{AcRoNN}), that can generalize individual activity recognition and improve cognitive fatigue estimation significantly. We evaluate and compare our proposed method with state-of-art methods using one real-time collected dataset from 5 individuals and another publicly available dataset from 27 individuals achieving max. 19% improvement.

</p>
</details>

<details><summary><b>A Unified Transferable Model for ML-Enhanced DBMS</b>
<a href="https://arxiv.org/abs/2105.02418">arxiv:2105.02418</a>
&#x1F4C8; 1 <br>
<p>Ziniu Wu, Pei Yu, Peilun Yang, Rong Zhu, Yuxing Han, Yaliang Li, Defu Lian, Kai Zeng, Jingren Zhou</p></summary>
<p>

**Abstract:** Recently, the database management system (DBMS) community has witnessed the power of machine learning (ML) solutions for DBMS tasks. Despite their promising performance, these existing solutions can hardly be considered satisfactory. First, these ML-based methods in DBMS are not effective enough because they are optimized on each specific task, and cannot explore or understand the intrinsic connections between tasks. Second, the training process has serious limitations that hinder their practicality, because they need to retrain the entire model from scratch for a new DB. Moreover, for each retraining, they require an excessive amount of training data, which is very expensive to acquire and unavailable for a new DB. We propose to explore the transferabilities of the ML methods both across tasks and across DBs to tackle these fundamental drawbacks.
  In this paper, we propose a unified model MTMLF that uses a multi-task training procedure to capture the transferable knowledge across tasks and a pre-train fine-tune procedure to distill the transferable meta knowledge across DBs. We believe this paradigm is more suitable for cloud DB service, and has the potential to revolutionize the way how ML is used in DBMS. Furthermore, to demonstrate the predicting power and viability of MTMLF, we provide a concrete and very promising case study on query optimization tasks. Last but not least, we discuss several concrete research opportunities along this line of work.

</p>
</details>

<details><summary><b>Automated Primary Hyperparathyroidism Screening with Neural Networks</b>
<a href="https://arxiv.org/abs/2105.02386">arxiv:2105.02386</a>
&#x1F4C8; 1 <br>
<p>Noah Ziems, Shaoen Wu, Jim Norman</p></summary>
<p>

**Abstract:** Primary Hyperparathyroidism(PHPT) is a relatively common disease, affecting about one in every 1,000 adults. However, screening for PHPT can be difficult, meaning it often goes undiagnosed for long periods of time. While looking at specific blood test results independently can help indicate whether a patient has PHPT, often these blood result levels can all be within their respective normal ranges despite the patient having PHPT. Based on the clinic data from the real world, in this work, we propose a novel approach to screening PHPT with neural network (NN) architecture, achieving over 97\% accuracy with common blood values as inputs. Further, we propose a second model achieving over 99\% accuracy with additional lab test values as inputs. Moreover, compared to traditional PHPT screening methods, our NN models can reduce the false negatives of traditional screening methods by 99\%.

</p>
</details>

<details><summary><b>Survey on Multi-Agent Q-Learning frameworks for resource management in wireless sensor network</b>
<a href="https://arxiv.org/abs/2105.02371">arxiv:2105.02371</a>
&#x1F4C8; 1 <br>
<p>Arvin Tashakori</p></summary>
<p>

**Abstract:** This report aims to survey multi-agent Q-Learning algorithms, analyze different game theory frameworks used, address each framework's applications, and report challenges and future directions. The target application for this study is resource management in the wireless sensor network.
  In the first section, the author provided an introduction regarding the applications of wireless sensor networks. After that, the author presented a summary of the Q-Learning algorithm, a well-known classic solution for model-free reinforcement learning problems.
  In the third section, the author extended the Q-Learning algorithm for multi-agent scenarios and discussed its challenges.
  In the fourth section, the author surveyed sets of game-theoretic frameworks that researchers used to address this problem for resource allocation and task scheduling in the wireless sensor networks. Lastly, the author mentioned some interesting open challenges in this domain.

</p>
</details>

<details><summary><b>Byzantine-Robust and Privacy-Preserving Framework for FedML</b>
<a href="https://arxiv.org/abs/2105.02295">arxiv:2105.02295</a>
&#x1F4C8; 1 <br>
<p>Hanieh Hashemi, Yongqin Wang, Chuan Guo, Murali Annavaram</p></summary>
<p>

**Abstract:** Federated learning has emerged as a popular paradigm for collaboratively training a model from data distributed among a set of clients. This learning setting presents, among others, two unique challenges: how to protect privacy of the clients' data during training, and how to ensure integrity of the trained model. We propose a two-pronged solution that aims to address both challenges under a single framework. First, we propose to create secure enclaves using a trusted execution environment (TEE) within the server. Each client can then encrypt their gradients and send them to verifiable enclaves. The gradients are decrypted within the enclave without the fear of privacy breaches. However, robustness check computations in a TEE are computationally prohibitive. Hence, in the second step, we perform a novel gradient encoding that enables TEEs to encode the gradients and then offloading Byzantine check computations to accelerators such as GPUs. Our proposed approach provides theoretical bounds on information leakage and offers a significant speed-up over the baseline in empirical evaluation.

</p>
</details>

<details><summary><b>Training Quantum Embedding Kernels on Near-Term Quantum Computers</b>
<a href="https://arxiv.org/abs/2105.02276">arxiv:2105.02276</a>
&#x1F4C8; 1 <br>
<p>Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S. Derks, Paul K. Faehrmann, Johannes Jakob Meyer</p></summary>
<p>

**Abstract:** Kernel methods are a cornerstone of classical machine learning. The idea of using quantum computers to compute kernels has recently attracted attention. Quantum embedding kernels (QEKs) constructed by embedding data into the Hilbert space of a quantum computer are a particular quantum kernel technique that allows to gather insights into learning problems and that are particularly suitable for noisy intermediate-scale quantum devices. In this work, we first provide an accessible introduction to quantum embedding kernels and then analyze the practical issues arising when realizing them on a noisy near-term quantum computer. We focus on quantum embedding kernels with variational parameters. These variational parameters are optimized for a given dataset by increasing the kernel-target alignment, a heuristic connected to the achievable classification accuracy. We further show under which conditions noise from device imperfections influences the predicted kernel and provide a strategy to mitigate these detrimental effects which is tailored to quantum embedding kernels. We also address the influence of finite sampling and derive bounds that put guarantees on the quality of the kernel matrix. We illustrate our findings by numerical experiments and tests on actual hardware.

</p>
</details>

<details><summary><b>Information Limits for Detecting a Subhypergraph</b>
<a href="https://arxiv.org/abs/2105.02259">arxiv:2105.02259</a>
&#x1F4C8; 1 <br>
<p>Mingao Yuan, Zuofeng Shang</p></summary>
<p>

**Abstract:** We consider the problem of recovering a subhypergraph based on an observed adjacency tensor corresponding to a uniform hypergraph. The uniform hypergraph is assumed to contain a subset of vertices called as subhypergraph. The edges restricted to the subhypergraph are assumed to follow a different probability distribution than other edges. We consider both weak recovery and exact recovery of the subhypergraph, and establish information-theoretic limits in each case. Specifically, we establish sharp conditions for the possibility of weakly or exactly recovering the subhypergraph from an information-theoretic point of view. These conditions are fundamentally different from their counterparts derived in hypothesis testing literature.

</p>
</details>

<details><summary><b>Rethinking Ultrasound Augmentation: A Physics-Inspired Approach</b>
<a href="https://arxiv.org/abs/2105.02188">arxiv:2105.02188</a>
&#x1F4C8; 1 <br>
<p>Maria Tirindelli, Christine Eilers, Walter Simson, Magdalini Paschali, Mohammad Farid Azampour, Nassir Navab</p></summary>
<p>

**Abstract:** Medical Ultrasound (US), despite its wide use, is characterized by artifacts and operator dependency. Those attributes hinder the gathering and utilization of US datasets for the training of Deep Neural Networks used for Computer-Assisted Intervention Systems. Data augmentation is commonly used to enhance model generalization and performance. However, common data augmentation techniques, such as affine transformations do not align with the physics of US and, when used carelessly can lead to unrealistic US images. To this end, we propose a set of physics-inspired transformations, including deformation, reverb and Signal-to-Noise Ratio, that we apply on US B-mode images for data augmentation. We evaluate our method on a new spine US dataset for the tasks of bone segmentation and classification.

</p>
</details>

<details><summary><b>H-TD2: Hybrid Temporal Difference Learning for Adaptive Urban Taxi Dispatch</b>
<a href="https://arxiv.org/abs/2105.02138">arxiv:2105.02138</a>
&#x1F4C8; 1 <br>
<p>Benjamin Rivière, Soon-Jo Chung</p></summary>
<p>

**Abstract:** We present H-TD2: Hybrid Temporal Difference Learning for Taxi Dispatch, a model-free, adaptive decision-making algorithm to coordinate a large fleet of automated taxis in a dynamic urban environment to minimize expected customer waiting times. Our scalable algorithm exploits the natural transportation network company topology by switching between two behaviors: distributed temporal-difference learning computed locally at each taxi and infrequent centralized Bellman updates computed at the dispatch center. We derive a regret bound and design the trigger condition between the two behaviors to explicitly control the trade-off between computational complexity and the individual taxi policy's bounded sub-optimality; this advances the state of the art by enabling distributed operation with bounded-suboptimality. Additionally, unlike recent reinforcement learning dispatch methods, this policy estimation is adaptive and robust to out-of-training domain events. This result is enabled by a two-step modelling approach: the policy is learned on an agent-agnostic, cell-based Markov Decision Process and individual taxis are coordinated using the learned policy in a distributed game-theoretic task assignment. We validate our algorithm against a receding horizon control baseline in a Gridworld environment with a simulated customer dataset, where the proposed solution decreases average customer waiting time by 50% over a wide range of parameters. We also validate in a Chicago city environment with real customer requests from the Chicago taxi public dataset where the proposed solution decreases average customer waiting time by 26% over irregular customer distributions during a 2016 Major League Baseball World Series game.

</p>
</details>

<details><summary><b>ScissionLite: Accelerating Distributed Deep Neural Networks Using Transfer Layer</b>
<a href="https://arxiv.org/abs/2105.02019">arxiv:2105.02019</a>
&#x1F4C8; 1 <br>
<p>Hyunho Ahn, Munkyu Lee, Cheol-Ho Hong, Blesson Varghese</p></summary>
<p>

**Abstract:** Industrial Internet of Things (IIoT) applications can benefit from leveraging edge computing. For example, applications underpinned by deep neural networks (DNN) models can be sliced and distributed across the IIoT device and the edge of the network for improving the overall performance of inference and for enhancing privacy of the input data, such as industrial product images. However, low network performance between IIoT devices and the edge is often a bottleneck. In this study, we develop ScissionLite, a holistic framework for accelerating distributed DNN inference using the Transfer Layer (TL). The TL is a traffic-aware layer inserted between the optimal slicing point of a DNN model slice in order to decrease the outbound network traffic without a significant accuracy drop. For the TL, we implement a new lightweight down/upsampling network for performance-limited IIoT devices. In ScissionLite, we develop ScissionTL, the Preprocessor, and the Offloader for end-to-end activities for deploying DNN slices with the TL. They decide the optimal slicing point of the DNN, prepare pre-trained DNN slices including the TL, and execute the DNN slices on an IIoT device and the edge. Employing the TL for the sliced DNN models has a negligible overhead. ScissionLite improves the inference latency by up to 16 and 2.8 times when compared to execution on the local device and an existing state-of-the-art model slicing approach respectively.

</p>
</details>

<details><summary><b>Automated scoring of pre-REM sleep in mice with deep learning</b>
<a href="https://arxiv.org/abs/2105.01933">arxiv:2105.01933</a>
&#x1F4C8; 1 <br>
<p>Niklas Grieger, Justus T. C. Schwabedal, Stefanie Wendel, Yvonne Ritze, Stephan Bialonski</p></summary>
<p>

**Abstract:** Reliable automation of the labor-intensive manual task of scoring animal sleep can facilitate the analysis of long-term sleep studies. In recent years, deep-learning-based systems, which learn optimal features from the data, increased scoring accuracies for the classical sleep stages of Wake, REM, and Non-REM. Meanwhile, it has been recognized that the statistics of transitional stages such as pre-REM, found between Non-REM and REM, may hold additional insight into the physiology of sleep and are now under vivid investigation. We propose a classification system based on a simple neural network architecture that scores the classical stages as well as pre-REM sleep in mice. When restricted to the classical stages, the optimized network showed state-of-the-art classification performance with an out-of-sample F1 score of 0.95 in male C57BL/6J mice. When unrestricted, the network showed lower F1 scores on pre-REM (0.5) compared to the classical stages. The result is comparable to previous attempts to score transitional stages in other species such as transition sleep in rats or N1 sleep in humans. Nevertheless, we observed that the sequence of predictions including pre-REM typically transitioned from Non-REM to REM reflecting sleep dynamics observed by human scorers. Our findings provide further evidence for the difficulty of scoring transitional sleep stages, likely because such stages of sleep are under-represented in typical data sets or show large inter-scorer variability. We further provide our source code and an online platform to run predictions with our trained network.

</p>
</details>

<details><summary><b>Partially Interpretable Estimators (PIE): Black-Box-Refined Interpretable Machine Learning</b>
<a href="https://arxiv.org/abs/2105.02410">arxiv:2105.02410</a>
&#x1F4C8; 0 <br>
<p>Tong Wang, Jingyi Yang, Yunyi Li, Boxiang Wang</p></summary>
<p>

**Abstract:** We propose Partially Interpretable Estimators (PIE) which attribute a prediction to individual features via an interpretable model, while a (possibly) small part of the PIE prediction is attributed to the interaction of features via a black-box model, with the goal to boost the predictive performance while maintaining interpretability. As such, the interpretable model captures the main contributions of features, and the black-box model attempts to complement the interpretable piece by capturing the "nuances" of feature interactions as a refinement. We design an iterative training algorithm to jointly train the two types of models. Experimental results show that PIE is highly competitive to black-box models while outperforming interpretable baselines. In addition, the understandability of PIE is comparable to simple linear models as validated via a human evaluation.

</p>
</details>

<details><summary><b>MODS -- A USV-oriented object detection and obstacle segmentation benchmark</b>
<a href="https://arxiv.org/abs/2105.02359">arxiv:2105.02359</a>
&#x1F4C8; 0 <br>
<p>Borja Bovcon, Jon Muhovič, Duško Vranac, Dean Mozetič, Janez Perš, Matej Kristan</p></summary>
<p>

**Abstract:** Small-sized unmanned surface vehicles (USV) are coastal water devices with a broad range of applications such as environmental control and surveillance. A crucial capability for autonomous operation is obstacle detection for timely reaction and collision avoidance, which has been recently explored in the context of camera-based visual scene interpretation. Owing to curated datasets, substantial advances in scene interpretation have been made in a related field of unmanned ground vehicles. However, the current maritime datasets do not adequately capture the complexity of real-world USV scenes and the evaluation protocols are not standardised, which makes cross-paper comparison of different methods difficult and hiders the progress. To address these issues, we introduce a new obstacle detection benchmark MODS, which considers two major perception tasks: maritime object detection and the more general maritime obstacle segmentation. We present a new diverse maritime evaluation dataset containing approximately 81k stereo images synchronized with an on-board IMU, with over 60k objects annotated. We propose a new obstacle segmentation performance evaluation protocol that reflects the detection accuracy in a way meaningful for practical USV navigation. Seventeen recent state-of-the-art object detection and obstacle segmentation methods are evaluated using the proposed protocol, creating a benchmark to facilitate development of the field.

</p>
</details>

<details><summary><b>Attention for Image Registration (AiR): an unsupervised Transformer approach</b>
<a href="https://arxiv.org/abs/2105.02282">arxiv:2105.02282</a>
&#x1F4C8; 0 <br>
<p>Zihao Wang, Hervé Delingette</p></summary>
<p>

**Abstract:** Image registration as an important basis in signal processing task often encounter the problem of stability and efficiency. Non-learning registration approaches rely on the optimization of the similarity metrics between the fix and moving images. Yet, those approaches are usually costly in both time and space complexity. The problem can be worse when the size of the image is large or the deformations between the images are severe. Recently, deep learning, or precisely saying, the convolutional neural network (CNN) based image registration methods have been widely investigated in the research community and show promising effectiveness to overcome the weakness of non-learning based methods. To explore the advanced learning approaches in image registration problem for solving practical issues, we present in this paper a method of introducing attention mechanism in deformable image registration problem. The proposed approach is based on learning the deformation field with a Transformer framework (AiR) that does not rely on the CNN but can be efficiently trained on GPGPU devices also. In a more vivid interpretation: we treat the image registration problem as the same as a language translation task and introducing a Transformer to tackle the problem. Our method learns an unsupervised generated deformation map and is tested on two benchmark datasets. The source code of the AiR will be released at Gitlab.

</p>
</details>

<details><summary><b>Randomized Stochastic Variance-Reduced Methods for Multi-Task Stochastic Bilevel Optimization</b>
<a href="https://arxiv.org/abs/2105.02266">arxiv:2105.02266</a>
&#x1F4C8; 0 <br>
<p>Zhishuai Guo, Quanqi Hu, Lijun Zhang, Tianbao Yang</p></summary>
<p>

**Abstract:** In this paper, we consider non-convex stochastic bilevel optimization (SBO) problems that have many applications in machine learning. Although numerous studies have proposed stochastic algorithms for solving these problems, they are limited in two perspectives: (i) their sample complexities are high, which do not match the state-of-the-art result for non-convex stochastic optimization; (ii) their algorithms are tailored to problems with only one lower-level problem. When there are many lower-level problems, it could be prohibitive to process all these lower-level problems at each iteration. To address these limitations, this paper proposes fast randomized stochastic algorithms for non-convex SBO problems. First, we present a stochastic method for non-convex SBO with only one lower problem and establish its sample complexity of $O(1/ε^3)$ for finding an $ε$-stationary point under Lipschitz continuous conditions of stochastic oracles, matching the lower bound for stochastic smooth non-convex optimization. Second, we present a randomized stochastic method for non-convex SBO with $m>1$ lower level problems (multi-task SBO) by processing a constant number of lower problems at each iteration, and establish its sample complexity no worse than $O(m/ε^3)$, which could be a better complexity than that of simply processing all $m$ lower problems at each iteration. Lastly, we establish even faster convergence results for gradient-dominant functions. To the best of our knowledge, this is the first work considering multi-task SBO and developing state-of-the-art sample complexity results.

</p>
</details>

<details><summary><b>Understanding Short-Range Memory Effects in Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2105.02062">arxiv:2105.02062</a>
&#x1F4C8; 0 <br>
<p>Chengli Tan, Jiangshe Zhang, Junmin Liu</p></summary>
<p>

**Abstract:** Stochastic gradient descent (SGD) is of fundamental importance in deep learning. Despite its simplicity, elucidating its efficacy remains challenging. Conventionally, the success of SGD is ascribed to the stochastic gradient noise (SGN) incurred in the training process. Based on this consensus, SGD is frequently treated and analyzed as the Euler-Maruyama discretization of stochastic differential equations (SDEs) driven by either Brownian or Levy stable motion. In this study, we argue that SGN is neither Gaussian nor Levy stable. Instead, inspired by the short-range correlation emerging in the SGN series, we propose that SGD can be viewed as a discretization of an SDE driven by fractional Brownian motion (FBM). Accordingly, the different convergence behavior of SGD dynamics is well-grounded. Moreover, the first passage time of an SDE driven by FBM is approximately derived. The result suggests a lower escaping rate for a larger Hurst parameter, and thus SGD stays longer in flat minima. This happens to coincide with the well-known phenomenon that SGD favors flat minima that generalize well. Extensive experiments are conducted to validate our conjecture, and it is demonstrated that short-range memory effects persist across various model architectures, datasets, and training strategies. Our study opens up a new perspective and may contribute to a better understanding of SGD.

</p>
</details>

<details><summary><b>Solving Sokoban with forward-backward reinforcement learning</b>
<a href="https://arxiv.org/abs/2105.01904">arxiv:2105.01904</a>
&#x1F4C8; 0 <br>
<p>Yaron Shoham, Gal Elidan</p></summary>
<p>

**Abstract:** Despite seminal advances in reinforcement learning in recent years, many domains where the rewards are sparse, e.g. given only at task completion, remain quite challenging. In such cases, it can be beneficial to tackle the task both from its beginning and end, and make the two ends meet. Existing approaches that do so, however, are not effective in the common scenario where the strategy needed near the end goal is very different from the one that is effective earlier on.
  In this work we propose a novel RL approach for such settings. In short, we first train a backward-looking agent with a simple relaxed goal, and then augment the state representation of the forward-looking agent with straightforward hint features. This allows the learned forward agent to leverage information from backward plans, without mimicking their policy.
  We demonstrate the efficacy of our approach on the challenging game of Sokoban, where we substantially surpass learned solvers that generalize across levels, and are competitive with SOTA performance of the best highly-crafted systems. Impressively, we achieve these results while learning from a small number of practice levels and using simple RL techniques.

</p>
</details>

<details><summary><b>A Robotic Approach towards Quantifying Epipelagic Bound Plastic Using Deep Visual Models</b>
<a href="https://arxiv.org/abs/2105.01882">arxiv:2105.01882</a>
&#x1F4C8; 0 <br>
<p>Gautam Tata, Sarah-Jeanne Royer, Olivier Poirion, Jay Lowe</p></summary>
<p>

**Abstract:** The quantification of positively buoyant marine plastic debris is critical to understanding how plastic litter accumulates across the world's oceans and is also crucial to identifying hotspots for targeted cleanup efforts. Currently, the most common method to quantify marine plastic is using manta trawls for manual sampling. However, this method is cost-intensive and requires human labor. This study removes the need for manual sampling by using an autonomous method using neural networks and computer vision models, which trained on images captured from various layers of the ocean column to perform real-time plastic quantification. The best performing model has a Mean Average Precision of 85% and an F1-Score of 0.89 while maintaining near real-time processing speeds ~2 ms/img.

</p>
</details>


[Next Page](2021/2021-05/2021-05-04.md)
