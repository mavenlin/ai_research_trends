Prev: [2021.12.21]({{ '/2021/12/21/2021.12.21.html' | relative_url }})  Next: [2021.12.23]({{ '/2021/12/23/2021.12.23.html' | relative_url }})
{% raw %}
## Summary for 2021-12-22, created on 2022-01-01


<details><summary><b>Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs</b>
<a href="https://arxiv.org/abs/2112.12582">arxiv:2112.12582</a>
&#x1F4C8; 88 <br>
<p>Lauren M. Sanders, Jason H. Yang, Ryan T. Scott, Amina Ann Qutub, Hector Garcia Martin, Daniel C. Berrios, Jaden J. A. Hastings, Jon Rask, Graham Mackintosh, Adrienne L. Hoarfrost, Stuart Chalk, John Kalantari, Kia Khezeli, Erik L. Antonsen, Joel Babdor, Richard Barker, Sergio E. Baranzini, Afshin Beheshti, Guillermo M. Delgado-Aparicio, Benjamin S. Glicksberg, Casey S. Greene, Melissa Haendel, Arif A. Hamid, Philip Heller, Daniel Jamieson</p></summary>
<p>

**Abstract:** Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space.

</p>
</details>

<details><summary><b>Beyond Low Earth Orbit: Biomonitoring, Artificial Intelligence, and Precision Space Health</b>
<a href="https://arxiv.org/abs/2112.12554">arxiv:2112.12554</a>
&#x1F4C8; 71 <br>
<p>Ryan T. Scott, Erik L. Antonsen, Lauren M. Sanders, Jaden J. A. Hastings, Seung-min Park, Graham Mackintosh, Robert J. Reynolds, Adrienne L. Hoarfrost, Aenor Sawyer, Casey S. Greene, Benjamin S. Glicksberg, Corey A. Theriot, Daniel C. Berrios, Jack Miller, Joel Babdor, Richard Barker, Sergio E. Baranzini, Afshin Beheshti, Stuart Chalk, Guillermo M. Delgado-Aparicio, Melissa Haendel, Arif A. Hamid, Philip Heller, Daniel Jamieson, Katelyn J. Jarvis</p></summary>
<p>

**Abstract:** Human space exploration beyond low Earth orbit will involve missions of significant distance and duration. To effectively mitigate myriad space health hazards, paradigm shifts in data and space health systems are necessary to enable Earth-independence, rather than Earth-reliance. Promising developments in the fields of artificial intelligence and machine learning for biology and health can address these needs. We propose an appropriately autonomous and intelligent Precision Space Health system that will monitor, aggregate, and assess biomedical statuses; analyze and predict personalized adverse health outcomes; adapt and respond to newly accumulated data; and provide preventive, actionable, and timely insights to individual deep space crew members and iterative decision support to their crew medical officer. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration, on future applications of artificial intelligence in space biology and health. In the next decade, biomonitoring technology, biomarker science, spacecraft hardware, intelligent software, and streamlined data management must mature and be woven together into a Precision Space Health system to enable humanity to thrive in deep space.

</p>
</details>

<details><summary><b>Shape Fragments</b>
<a href="https://arxiv.org/abs/2112.11796">arxiv:2112.11796</a>
&#x1F4C8; 21 <br>
<p>Thomas Delva, Anastasia Dimou, Maxime Jakubowski, Jan Van den Bussche</p></summary>
<p>

**Abstract:** In constraint languages for RDF graphs, such as ShEx and SHACL, constraints on nodes and their properties in RDF graphs are known as "shapes". Schemas in these languages list the various shapes that certain targeted nodes must satisfy for the graph to conform to the schema. Using SHACL, we propose in this paper a novel use of shapes, by which a set of shapes is used to extract a subgraph from an RDF graph, the so-called shape fragment. Our proposed mechanism fits in the framework of Linked Data Fragments. In this paper, (i) we define our extraction mechanism formally, building on recently proposed SHACL formalizations; (ii) we establish correctness properties, which relate shape fragments to notions of provenance for database queries; (iii) we compare shape fragments with SPARQL queries; (iv) we discuss implementation options; and (v) we present initial experiments demonstrating that shape fragments are a feasible new idea.

</p>
</details>

<details><summary><b>Graph augmented Deep Reinforcement Learning in the GameRLand3D environment</b>
<a href="https://arxiv.org/abs/2112.11731">arxiv:2112.11731</a>
&#x1F4C8; 21 <br>
<p>Edward Beeching, Maxim Peter, Philippe Marcotte, Jilles Debangoye, Olivier Simonin, Joshua Romoff, Christian Wolf</p></summary>
<p>

**Abstract:** We address planning and navigation in challenging 3D video games featuring maps with disconnected regions reachable by agents using special actions. In this setting, classical symbolic planners are not applicable or difficult to adapt. We introduce a hybrid technique combining a low level policy trained with reinforcement learning and a graph based high level classical planner. In addition to providing human-interpretable paths, the approach improves the generalization performance of an end-to-end approach in unseen maps, where it achieves a 20% absolute increase in success rate over a recurrent end-to-end agent on a point to point navigation task in yet unseen large-scale maps of size 1km x 1km. In an in-depth experimental study, we quantify the limitations of end-to-end Deep RL approaches in vast environments and we also introduce "GameRLand3D", a new benchmark and soon to be released environment can generate complex procedural 3D maps for navigation tasks.

</p>
</details>

<details><summary><b>Squareplus: A Softplus-Like Algebraic Rectifier</b>
<a href="https://arxiv.org/abs/2112.11687">arxiv:2112.11687</a>
&#x1F4C8; 14 <br>
<p>Jonathan T. Barron</p></summary>
<p>

**Abstract:** We present squareplus, an activation function that resembles softplus, but which can be computed using only algebraic operations: addition, multiplication, and square-root. Because squareplus is ~6x faster to evaluate than softplus on a CPU and does not require access to transcendental functions, it may have practical value in resource-limited deep learning applications.

</p>
</details>

<details><summary><b>A Survey of Natural Language Generation</b>
<a href="https://arxiv.org/abs/2112.11739">arxiv:2112.11739</a>
&#x1F4C8; 9 <br>
<p>Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, Min Yang</p></summary>
<p>

**Abstract:** This paper offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text and computational creativity.

</p>
</details>

<details><summary><b>Class-aware Sounding Objects Localization via Audiovisual Correspondence</b>
<a href="https://arxiv.org/abs/2112.11749">arxiv:2112.11749</a>
&#x1F4C8; 8 <br>
<p>Di Hu, Yake Wei, Rui Qian, Weiyao Lin, Ruihua Song, Ji-Rong Wen</p></summary>
<p>

**Abstract:** Audiovisual scenes are pervasive in our daily life. It is commonplace for humans to discriminatively localize different sounding objects but quite challenging for machines to achieve class-aware sounding objects localization without category annotations, i.e., localizing the sounding object and recognizing its category. To address this problem, we propose a two-stage step-by-step learning framework to localize and recognize sounding objects in complex audiovisual scenarios using only the correspondence between audio and vision. First, we propose to determine the sounding area via coarse-grained audiovisual correspondence in the single source cases. Then visual features in the sounding area are leveraged as candidate object representations to establish a category-representation object dictionary for expressive visual character extraction. We generate class-aware object localization maps in cocktail-party scenarios and use audiovisual correspondence to suppress silent areas by referring to this dictionary. Finally, we employ category-level audiovisual consistency as the supervision to achieve fine-grained audio and sounding object distribution alignment. Experiments on both realistic and synthesized videos show that our model is superior in localizing and recognizing objects as well as filtering out silent ones. We also transfer the learned audiovisual network into the unsupervised object detection task, obtaining reasonable performance.

</p>
</details>

<details><summary><b>Multimodal Analysis of memes for sentiment extraction</b>
<a href="https://arxiv.org/abs/2112.11850">arxiv:2112.11850</a>
&#x1F4C8; 7 <br>
<p>Nayan Varma Alluri, Neeli Dheeraj Krishna</p></summary>
<p>

**Abstract:** Memes are one of the most ubiquitous forms of social media communication. The study and processing of memes, which are intrinsically multimedia, is a popular topic right now. The study presented in this research is based on the Memotion dataset, which involves categorising memes based on irony, comedy, motivation, and overall-sentiment. Three separate innovative transformer-based techniques have been developed, and their outcomes have been thoroughly reviewed.The best algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55 for motivation classification, 0.61 for sarcasm classification, and 0.575 for overall sentiment of the meme out of all our techniques.

</p>
</details>

<details><summary><b>Bottom-up approaches for multi-person pose estimation and it's applications: A brief review</b>
<a href="https://arxiv.org/abs/2112.11834">arxiv:2112.11834</a>
&#x1F4C8; 7 <br>
<p>Milan Kresović, Thong Duy Nguyen</p></summary>
<p>

**Abstract:** Human Pose Estimation (HPE) is one of the fundamental problems in computer vision. It has applications ranging from virtual reality, human behavior analysis, video surveillance, anomaly detection, self-driving to medical assistance. The main objective of HPE is to obtain the person's posture from the given input. Among different paradigms for HPE, one paradigm is called bottom-up multi-person pose estimation. In the bottom-up approach, initially, all the key points of the targets are detected, and later in the optimization stage, the detected key points are associated with the corresponding targets. This review paper discussed the recent advancements in bottom-up approaches for the HPE and listed the possible high-quality datasets used to train the models. Additionally, a discussion of the prominent bottom-up approaches and their quantitative results on the standard performance matrices are given. Finally, the limitations of the existing methods are highlighted, and guidelines of the future research directions are given.

</p>
</details>

<details><summary><b>RepBin: Constraint-based Graph Representation Learning for Metagenomic Binning</b>
<a href="https://arxiv.org/abs/2112.11696">arxiv:2112.11696</a>
&#x1F4C8; 7 <br>
<p>Hansheng Xue, Vijini Mallawaarachchi, Yujia Zhang, Vaibhav Rajan, Yu Lin</p></summary>
<p>

**Abstract:** Mixed communities of organisms are found in many environments (from the human gut to marine ecosystems) and can have profound impact on human health and the environment. Metagenomics studies the genomic material of such communities through high-throughput sequencing that yields DNA subsequences for subsequent analysis. A fundamental problem in the standard workflow, called binning, is to discover clusters, of genomic subsequences, associated with the unknown constituent organisms. Inherent noise in the subsequences, various biological constraints that need to be imposed on them and the skewed cluster size distribution exacerbate the difficulty of this unsupervised learning problem. In this paper, we present a new formulation using a graph where the nodes are subsequences and edges represent homophily information. In addition, we model biological constraints providing heterophilous signal about nodes that cannot be clustered together. We solve the binning problem by developing new algorithms for (i) graph representation learning that preserves both homophily relations and heterophily constraints (ii) constraint-based graph clustering method that addresses the problems of skewed cluster size distribution. Extensive experiments, on real and synthetic datasets, demonstrate that our approach, called RepBin, outperforms a wide variety of competing methods. Our constraint-based graph representation learning and clustering methods, that may be useful in other domains as well, advance the state-of-the-art in both metagenomics binning and graph representation learning.

</p>
</details>

<details><summary><b>How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?</b>
<a href="https://arxiv.org/abs/2112.11668">arxiv:2112.11668</a>
&#x1F4C8; 7 <br>
<p>Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang</p></summary>
<p>

**Abstract:** The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.

</p>
</details>

<details><summary><b>A Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drone</b>
<a href="https://arxiv.org/abs/2112.12545">arxiv:2112.12545</a>
&#x1F4C8; 6 <br>
<p>Aigerim Bogyrbayeva. Taehyun Yoon, Hanbum Ko, Sungbin Lim, Hyokun Yun, Changhyun Kwon</p></summary>
<p>

**Abstract:** Reinforcement learning has recently shown promise in learning quality solutions in many combinatorial optimization problems. In particular, the attention-based encoder-decoder models show high effectiveness on various routing problems, including the Traveling Salesman Problem (TSP). Unfortunately, they perform poorly for the TSP with Drone (TSP-D), requiring routing a heterogeneous fleet of vehicles in coordination -- a truck and a drone. In TSP-D, the two vehicles are moving in tandem and may need to wait at a node for the other vehicle to join. State-less attention-based decoder fails to make such coordination between vehicles. We propose an attention encoder-LSTM decoder hybrid model, in which the decoder's hidden state can represent the sequence of actions made. We empirically demonstrate that such a hybrid model improves upon a purely attention-based model for both solution quality and computational efficiency. Our experiments on the min-max Capacitated Vehicle Routing Problem (mmCVRP) also confirm that the hybrid model is more suitable for coordinated routing of multiple vehicles than the attention-based model.

</p>
</details>

<details><summary><b>VoiceMoji: A Novel On-Device Pipeline for Seamless Emoji Insertion in Dictation</b>
<a href="https://arxiv.org/abs/2112.12028">arxiv:2112.12028</a>
&#x1F4C8; 6 <br>
<p>Sumit Kumar, Harichandana B S S, Himanshu Arora</p></summary>
<p>

**Abstract:** Most of the speech recognition systems recover only words in the speech and fail to capture emotions. Users have to manually add emoji(s) in text for adding tone and making communication fun. Though there is much work done on punctuation addition on transcribed speech, the area of emotion addition is untouched. In this paper, we propose a novel on-device pipeline to enrich the voice input experience. It involves, given a blob of transcribed text, intelligently processing and identifying structure where emoji insertion makes sense. Moreover, it includes semantic text analysis to predict emoji for each of the sub-parts for which we propose a novel architecture Attention-based Char Aware (ACA) LSTM which handles Out-Of-Vocabulary (OOV) words as well. All these tasks are executed completely on-device and hence can aid on-device dictation systems. To the best of our knowledge, this is the first work that shows how to add emoji(s) in the transcribed text. We demonstrate that our components achieve comparable results to previous neural approaches for punctuation addition and emoji prediction with 80% fewer parameters. Overall, our proposed model has a very small memory footprint of a mere 4MB to suit on-device deployment.

</p>
</details>

<details><summary><b>Community Detection in Medical Image Datasets: Using Wavelets and Spectral Methods</b>
<a href="https://arxiv.org/abs/2112.12021">arxiv:2112.12021</a>
&#x1F4C8; 6 <br>
<p>Roozbeh Yousefzadeh</p></summary>
<p>

**Abstract:** Medical image datasets can have large number of images representing patients with different health conditions and various disease severity. When dealing with raw unlabeled image datasets, the large number of samples often makes it hard for experts and non-experts to understand the variety of images present in a dataset. Supervised learning methods rely on labeled images which requires a considerable effort by medical experts to first understand the communities of images present in the data and then labeling the images. Here, we propose an algorithm to facilitate the automatic identification of communities in medical image datasets. We further explain that such analysis can also be insightful in a supervised setting, when the images are already labeled. Such insights are useful because in reality, health and disease severity can be considered a continuous spectrum, and within each class, there usually are finer communities worthy of investigation, especially when they have similarities to communities in other classes. In our approach, we use wavelet decomposition of images in tandem with spectral methods. We show that the eigenvalues of a graph Laplacian can reveal the number of notable communities in an image dataset. In our experiments, we use a dataset of images labeled with different conditions for COVID patients. We detect 25 communities in the dataset and then observe that only 6 of those communities contain patients with pneumonia. We also investigate the contents of a colorectal cancer histopathology dataset.

</p>
</details>

<details><summary><b>Lifting Symmetry Breaking Constraints with Inductive Logic Programming</b>
<a href="https://arxiv.org/abs/2112.11806">arxiv:2112.11806</a>
&#x1F4C8; 6 <br>
<p>Alice Tarzariol, Martin Gebser, Konstantin Schekotihin</p></summary>
<p>

**Abstract:** Efficient omission of symmetric solution candidates is essential for combinatorial problem-solving. Most of the existing approaches are instance-specific and focus on the automatic computation of Symmetry Breaking Constraints (SBCs) for each given problem instance. However, the application of such approaches to large-scale instances or advanced problem encodings might be problematic since the computed SBCs are propositional and, therefore, can neither be meaningfully interpreted nor transferred to other instances. As a result, a time-consuming recomputation of SBCs must be done before every invocation of a solver.
  To overcome these limitations, we introduce a new model-oriented approach for Answer Set Programming that lifts the SBCs of small problem instances into a set of interpretable first-order constraints using the Inductive Logic Programming paradigm. Experiments demonstrate the ability of our framework to learn general constraints from instance-specific SBCs for a collection of combinatorial problems. The obtained results indicate that our approach significantly outperforms a state-of-the-art instance-specific method as well as the direct application of a solver.

</p>
</details>

<details><summary><b>Neural-Symbolic Integration for Interactive Learning and Conceptual Grounding</b>
<a href="https://arxiv.org/abs/2112.11805">arxiv:2112.11805</a>
&#x1F4C8; 6 <br>
<p>Benedikt Wagner, Artur d'Avila Garcez</p></summary>
<p>

**Abstract:** We propose neural-symbolic integration for abstract concept explanation and interactive learning. Neural-symbolic integration and explanation allow users and domain-experts to learn about the data-driven decision making process of large neural models. The models are queried using a symbolic logic language. Interaction with the user then confirms or rejects a revision of the neural model using logic-based constraints that can be distilled into the model architecture. The approach is illustrated using the Logic Tensor Network framework alongside Concept Activation Vectors and applied to a Convolutional Neural Network.

</p>
</details>

<details><summary><b>Statistical Feature-based Personal Information Detection in Mobile Network Traffic</b>
<a href="https://arxiv.org/abs/2112.12346">arxiv:2112.12346</a>
&#x1F4C8; 5 <br>
<p>Shuang Zhao, Shuhui Chen, Ziling Wei</p></summary>
<p>

**Abstract:** With the popularity of smartphones, mobile applications (apps) have penetrated the daily life of people. Although apps provide rich functionalities, they also access a large amount of personal information simultaneously. As a result, privacy concerns are raised. To understand what personal information the apps collect, many solutions are presented to detect privacy leaks in apps. Recently, the traffic monitoring-based privacy leak detection method has shown promising performance and strong scalability. However, it still has some shortcomings. Firstly, it suffers from detecting the leakage of personal information with obfuscation. Secondly, it cannot discover the privacy leaks of undefined type. Aiming at solving the above problems, a new personal information detection method based on traffic monitoring is proposed in this paper. In this paper, statistical features of personal information are designed to depict the occurrence patterns of personal information in the traffic, including local patterns and global patterns. Then a detector is trained based on machine learning algorithms to discover potential personal information with similar patterns. Since the statistical features are independent of the value and type of personal information, the trained detector is capable of identifying various types of privacy leaks and obfuscated privacy leaks. As far as we know, this is the first work that detects personal information based on statistical features. Finally, the experimental results show that the proposed method could achieve better performance than the state-of-the-art.

</p>
</details>

<details><summary><b>Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding</b>
<a href="https://arxiv.org/abs/2112.12180">arxiv:2112.12180</a>
&#x1F4C8; 5 <br>
<p>Tanay Agrawal, Dhruv Agarwal, Michal Balazia, Neelabh Sinha, Francois Bremond</p></summary>
<p>

**Abstract:** Personality computing and affective computing have gained recent interest in many research areas. The datasets for the task generally have multiple modalities like video, audio, language and bio-signals. In this paper, we propose a flexible model for the task which exploits all available data. The task involves complex relations and to avoid using a large model for video processing specifically, we propose the use of behaviour encoding which boosts performance with minimal change to the model. Cross-attention using transformers has become popular in recent times and is utilised for fusion of different modalities. Since long term relations may exist, breaking the input into chunks is not desirable, thus the proposed model processes the entire input together. Our experiments show the importance of each of the above contributions

</p>
</details>

<details><summary><b>Meta-Learning and Self-Supervised Pretraining for Real World Image Translation</b>
<a href="https://arxiv.org/abs/2112.11929">arxiv:2112.11929</a>
&#x1F4C8; 5 <br>
<p>Ileana Rugina, Rumen Dangovski, Mark Veillette, Pooya Khorrami, Brian Cheung, Olga Simek, Marin Soljačić</p></summary>
<p>

**Abstract:** Recent advances in deep learning, in particular enabled by hardware advances and big data, have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. Many of these improvements are however constrained to problems with large-scale curated data-sets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning or self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of machine learning by extending deep-learning to the semi-supervised and few-shot domains. We follow this line of work and explore spatio-temporal structure in a recently introduced image-to-image translation problem in order to: i) formulate a novel multi-task few-shot image generation benchmark and ii) explore data augmentations in contrastive pre-training for image translation downstream tasks. We present several baselines for the few-shot problem and discuss trade-offs between different approaches. Our code is available at https://github.com/irugina/meta-image-translation.

</p>
</details>

<details><summary><b>Few-Shot Object Detection: A Survey</b>
<a href="https://arxiv.org/abs/2112.11699">arxiv:2112.11699</a>
&#x1F4C8; 5 <br>
<p>Mona Köhler, Markus Eisenbach, Horst-Michael Gross</p></summary>
<p>

**Abstract:** Humans are able to learn to recognize new objects even from a few examples. In contrast, training deep-learning-based object detectors requires huge amounts of annotated data. To avoid the need to acquire and annotate these huge amounts of data, few-shot object detection aims to learn from few object instances of new categories in the target domain. In this survey, we provide an overview of the state of the art in few-shot object detection. We categorize approaches according to their training scheme and architectural layout. For each type of approaches, we describe the general realization as well as concepts to improve the performance on novel categories. Whenever appropriate, we give short takeaways regarding these concepts in order to highlight the best ideas. Eventually, we introduce commonly used datasets and their evaluation protocols and analyze reported benchmark results. As a result, we emphasize common challenges in evaluation and identify the most promising current trends in this emerging field of few-shot object detection.

</p>
</details>

<details><summary><b>Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization</b>
<a href="https://arxiv.org/abs/2112.11670">arxiv:2112.11670</a>
&#x1F4C8; 5 <br>
<p>Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang</p></summary>
<p>

**Abstract:** The Query Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this paper, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.

</p>
</details>

<details><summary><b>Emulation of greenhouse-gas sensitivities using variational autoencoders</b>
<a href="https://arxiv.org/abs/2112.12524">arxiv:2112.12524</a>
&#x1F4C8; 4 <br>
<p>Laura Cartwright, Andrew Zammit-Mangion, Nicholas M. Deutscher</p></summary>
<p>

**Abstract:** Flux inversion is the process by which sources and sinks of a gas are identified from observations of gas mole fraction. The inversion often involves running a Lagrangian particle dispersion model (LPDM) to generate sensitivities between observations and fluxes over a spatial domain of interest. The LPDM must be run backward in time for every gas measurement, and this can be computationally prohibitive. To address this problem, here we develop a novel spatio-temporal emulator for LPDM sensitivities that is built using a convolutional variational autoencoder (CVAE). With the encoder segment of the CVAE, we obtain approximate (variational) posterior distributions over latent variables in a low-dimensional space. We then use a spatio-temporal Gaussian process emulator on the low-dimensional space to emulate new variables at prediction locations and time points. Emulated variables are then passed through the decoder segment of the CVAE to yield emulated sensitivities. We show that our CVAE-based emulator outperforms the more traditional emulator built using empirical orthogonal functions and that it can be used with different LPDMs. We conclude that our emulation-based approach can be used to reliably reduce the computing time needed to generate LPDM outputs for use in high-resolution flux inversions.

</p>
</details>

<details><summary><b>Neuroevolution deep learning architecture search for estimation of river surface elevation from photogrammetric Digital Surface Models</b>
<a href="https://arxiv.org/abs/2112.12510">arxiv:2112.12510</a>
&#x1F4C8; 4 <br>
<p>Radosław Szostak, Marcin Pietroń, Mirosław Zimnoch, Przemysław Wachniew, Paweł Ćwiąkała, Edyta Puniach</p></summary>
<p>

**Abstract:** Development of the new methods of surface water observation is crucial in the perspective of increasingly frequent extreme hydrological events related to global warming and increasing demand for water. Orthophotos and digital surface models (DSMs) obtained using UAV photogrammetry can be used to determine the Water Surface Elevation (WSE) of a river. However, this task is difficult due to disturbances of the water surface on DSMs caused by limitations of photogrammetric algorithms. In this study, machine learning was used to extract a WSE value from disturbed photogrammetric data. A brand new dataset has been prepared specifically for this purpose by hydrology and photogrammetry experts. The new method is an important step toward automating water surface level measurements with high spatial and temporal resolution. Such data can be used to validate and calibrate of hydrological, hydraulic and hydrodynamic models making hydrological forecasts more accurate, in particular predicting extreme and dangerous events such as floods or droughts. For our knowledge this is the first approach in which dataset was created for this purpose and deep learning models were used for this task. Additionally, neuroevolution algorithm was set to explore different architectures to find local optimal models and non-gradient search was performed to fine-tune the model parameters. The achieved results have better accuracy compared to manual methods of determining WSE from photogrammetric DSMs.

</p>
</details>

<details><summary><b>Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?</b>
<a href="https://arxiv.org/abs/2112.12345">arxiv:2112.12345</a>
&#x1F4C8; 4 <br>
<p>Ziwei Zhang, Xin Wang, Zeyang Zhang, Peng Cui, Wenwu Zhu</p></summary>
<p>

**Abstract:** Geometric deep learning, i.e., designing neural networks to handle the ubiquitous geometric data such as point clouds and graphs, have achieved great successes in the last decade. One critical inductive bias is that the model can maintain invariance towards various transformations such as translation, rotation, and scaling. The existing graph neural network (GNN) approaches can only maintain permutation-invariance, failing to guarantee invariance with respect to other transformations. Besides GNNs, other works design sophisticated transformation-invariant layers, which are computationally expensive and difficult to be extended. To solve this problem, we revisit why the existing neural networks cannot maintain transformation invariance when handling geometric data. Our findings show that transformation-invariant and distance-preserving initial representations are sufficient to achieve transformation invariance rather than needing sophisticated neural layer designs. Motivated by these findings, we propose Transformation Invariant Neural Networks (TinvNN), a straightforward and general framework for geometric data. Specifically, we realize transformation-invariant and distance-preserving initial point representations by modifying multi-dimensional scaling before feeding the representations into neural networks. We prove that TinvNN can strictly guarantee transformation invariance, being general and flexible enough to be combined with the existing neural networks. Extensive experimental results on point cloud analysis and combinatorial optimization demonstrate the effectiveness and general applicability of our proposed method. Based on the experimental results, we advocate that TinvNN should be considered a new starting point and an essential baseline for further studies of transformation-invariant geometric deep learning.

</p>
</details>

<details><summary><b>Nonnegative OPLS for Supervised Design of Filter Banks: Application to Image and Audio Feature Extraction</b>
<a href="https://arxiv.org/abs/2112.12280">arxiv:2112.12280</a>
&#x1F4C8; 4 <br>
<p>Sergio Muñoz-Romero, Jerónimo Arenas García, Vanessa Gómez-Verdejo</p></summary>
<p>

**Abstract:** Audio or visual data analysis tasks usually have to deal with high-dimensional and nonnegative signals. However, most data analysis methods suffer from overfitting and numerical problems when data have more than a few dimensions needing a dimensionality reduction preprocessing. Moreover, interpretability about how and why filters work for audio or visual applications is a desired property, especially when energy or spectral signals are involved. In these cases, due to the nature of these signals, the nonnegativity of the filter weights is a desired property to better understand its working. Because of these two necessities, we propose different methods to reduce the dimensionality of data while the nonnegativity and interpretability of the solution are assured. In particular, we propose a generalized methodology to design filter banks in a supervised way for applications dealing with nonnegative data, and we explore different ways of solving the proposed objective function consisting of a nonnegative version of the orthonormalized partial least-squares method. We analyze the discriminative power of the features obtained with the proposed methods for two different and widely studied applications: texture and music genre classification. Furthermore, we compare the filter banks achieved by our methods with other state-of-the-art methods specifically designed for feature extraction.

</p>
</details>

<details><summary><b>Automatic Estimation of Anthropometric Human Body Measurements</b>
<a href="https://arxiv.org/abs/2112.11992">arxiv:2112.11992</a>
&#x1F4C8; 4 <br>
<p>Dana Škorvánková, Adam Riečický, Martin Madaras</p></summary>
<p>

**Abstract:** Research tasks related to human body analysis have been drawing a lot of attention in computer vision area over the last few decades, considering its potential benefits on our day-to-day life. Anthropometry is a field defining physical measures of a human body size, form, and functional capacities. Specifically, the accurate estimation of anthropometric body measurements from visual human body data is one of the challenging problems, where the solution would ease many different areas of applications, including ergonomics, garment manufacturing, etc. This paper formulates a research in the field of deep learning and neural networks, to tackle the challenge of body measurements estimation from various types of visual input data (such as 2D images or 3D point clouds). Also, we deal with the lack of real human data annotated with ground truth body measurements required for training and evaluation, by generating a synthetic dataset of various human body shapes and performing a skeleton-driven annotation.

</p>
</details>

<details><summary><b>Deep learning for brain metastasis detection and segmentation in longitudinal MRI data</b>
<a href="https://arxiv.org/abs/2112.11833">arxiv:2112.11833</a>
&#x1F4C8; 4 <br>
<p>Yixing Huang, Christoph Bert, Philipp Sommer, Benjamin Frey, Udo Gaipl, Luitpold V. Distel, Thomas Weissmann, Michael Uder, Manuel A. Schmidt, Arnd Dörfler, Andreas Maier, Rainer Fietkau, Florian Putz</p></summary>
<p>

**Abstract:** Brain metastases occur frequently in patients with metastatic cancer. Early and accurate detection of brain metastases is very essential for treatment planning and prognosis in radiation therapy. To improve brain metastasis detection performance with deep learning, a custom detection loss called volume-level sensitivity-specificity (VSS) is proposed, which rates individual metastasis detection sensitivity and specificity in (sub-)volume levels. As sensitivity and precision are always a trade-off in a metastasis level, either a high sensitivity or a high precision can be achieved by adjusting the weights in the VSS loss without decline in dice score coefficient for segmented metastases. To reduce metastasis-like structures being detected as false positive metastases, a temporal prior volume is proposed as an additional input of the neural network. Our proposed VSS loss improves the sensitivity of brain metastasis detection, increasing the sensitivity from 86.7% to 95.5%. Alternatively, it improves the precision from 68.8% to 97.8%. With the additional temporal prior volume, about 45% of the false positive metastases are reduced in the high sensitivity model and the precision reaches 99.6% for the high specificity model. The mean dice coefficient for all metastases is about 0.81. With the ensemble of the high sensitivity and high specificity models, on average only 1.5 false positive metastases per patient needs further check, while the majority of true positive metastases are confirmed. The ensemble learning is able to distinguish high confidence true positive metastases from metastases candidates that require special expert review or further follow-up, being particularly well-fit to the requirements of expert support in real clinical practice.

</p>
</details>

<details><summary><b>The Importance of the Current Input in Sequence Modeling</b>
<a href="https://arxiv.org/abs/2112.11776">arxiv:2112.11776</a>
&#x1F4C8; 4 <br>
<p>Christian Oliva, Luis F. Lago-Fernández</p></summary>
<p>

**Abstract:** The last advances in sequence modeling are mainly based on deep learning approaches. The current state of the art involves the use of variations of the standard LSTM architecture, combined with several tricks that improve the final prediction rates of the trained neural networks. However, in some cases, these adaptations might be too much tuned to the particular problems being addressed. In this article, we show that a very simple idea, to add a direct connection between the input and the output, skipping the recurrent module, leads to an increase of the prediction accuracy in sequence modeling problems related to natural language processing. Experiments carried out on different problems show that the addition of this kind of connection to a recurrent network always improves the results, regardless of the architecture and training-specific details. When this idea is introduced into the models that lead the field, the resulting networks achieve a new state-of-the-art perplexity in language modeling problems.

</p>
</details>

<details><summary><b>Robust learning of data anomalies with analytically-solvable entropic outlier sparsification</b>
<a href="https://arxiv.org/abs/2112.11768">arxiv:2112.11768</a>
&#x1F4C8; 4 <br>
<p>Illia Horenko</p></summary>
<p>

**Abstract:** Entropic Outlier Sparsification (EOS) is proposed as a robust computational strategy for the detection of data anomalies in a broad class of learning methods, including the unsupervised problems (like detection of non-Gaussian outliers in mostly-Gaussian data) and in the supervised learning with mislabeled data. EOS dwells on the derived analytic closed-form solution of the (weighted) expected error minimization problem subject to the Shannon entropy regularization. In contrast to common regularization strategies requiring computational costs that scale polynomial with the data dimension, identified closed-form solution is proven to impose additional iteration costs that depend linearly on statistics size and are independent of data dimension. Obtained analytic results also explain why the mixtures of spherically-symmetric Gaussians - used heuristically in many popular data analysis algorithms - represent an optimal choice for the non-parametric probability distributions when working with squared Euclidean distances, combining expected error minimality, maximal entropy/unbiasedness, and a linear cost scaling. The performance of EOS is compared to a range of commonly-used tools on synthetic problems and on partially-mislabeled supervised classification problems from biomedicine.

</p>
</details>

<details><summary><b>Simple and Effective Balance of Contrastive Losses</b>
<a href="https://arxiv.org/abs/2112.11743">arxiv:2112.11743</a>
&#x1F4C8; 4 <br>
<p>Arnaud Sors, Rafael Sampaio de Rezende, Sarah Ibrahimi, Jean-Marc Andreoli</p></summary>
<p>

**Abstract:** Contrastive losses have long been a key ingredient of deep metric learning and are now becoming more popular due to the success of self-supervised learning. Recent research has shown the benefit of decomposing such losses into two sub-losses which act in a complementary way when learning the representation network: a positive term and an entropy term. Although the overall loss is thus defined as a combination of two terms, the balance of these two terms is often hidden behind implementation details and is largely ignored and sub-optimal in practice. In this work, we approach the balance of contrastive losses as a hyper-parameter optimization problem, and propose a coordinate descent-based search method that efficiently find the hyper-parameters that optimize evaluation performance. In the process, we extend existing balance analyses to the contrastive margin loss, include batch size in the balance, and explain how to aggregate loss elements from the batch to maintain near-optimal performance over a larger range of batch sizes. Extensive experiments with benchmarks from deep metric learning and self-supervised learning show that optimal hyper-parameters are found faster with our method than with other common search methods.

</p>
</details>

<details><summary><b>Hybrid Curriculum Learning for Emotion Recognition in Conversation</b>
<a href="https://arxiv.org/abs/2112.11718">arxiv:2112.11718</a>
&#x1F4C8; 4 <br>
<p>Lin Yang, Yi Shen, Yue Mao, Longjun Cai</p></summary>
<p>

**Abstract:** Emotion recognition in conversation (ERC) aims to detect the emotion label for each utterance. Motivated by recent studies which have proven that feeding training examples in a meaningful order rather than considering them randomly can boost the performance of models, we propose an ERC-oriented hybrid curriculum learning framework. Our framework consists of two curricula: (1) conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In CC, we construct a difficulty measurer based on "emotion shift" frequency within a conversation, then the conversations are scheduled in an "easy to hard" schema according to the difficulty score returned by the difficulty measurer. For UC, it is implemented from an emotion-similarity perspective, which progressively strengthens the model's ability in identifying the confusing emotions. With the proposed model-agnostic hybrid curriculum learning strategy, we observe significant performance boosts over a wide range of existing ERC models and we are able to achieve new state-of-the-art results on four public ERC datasets.

</p>
</details>

<details><summary><b>Model Selection in Batch Policy Optimization</b>
<a href="https://arxiv.org/abs/2112.12320">arxiv:2112.12320</a>
&#x1F4C8; 3 <br>
<p>Jonathan N. Lee, George Tucker, Ofir Nachum, Bo Dai</p></summary>
<p>

**Abstract:** We study the problem of model selection in batch policy optimization: given a fixed, partial-feedback dataset and $M$ model classes, learn a policy with performance that is competitive with the policy derived from the best model class. We formalize the problem in the contextual bandit setting with linear model classes by identifying three sources of error that any model selection algorithm should optimally trade-off in order to be competitive: (1) approximation error, (2) statistical complexity, and (3) coverage. The first two sources are common in model selection for supervised learning, where optimally trading-off these properties is well-studied. In contrast, the third source is unique to batch policy optimization and is due to dataset shift inherent to the setting. We first show that no batch policy optimization algorithm can achieve a guarantee addressing all three simultaneously, revealing a stark contrast between difficulties in batch policy optimization and the positive results available in supervised learning. Despite this negative result, we show that relaxing any one of the three error sources enables the design of algorithms achieving near-oracle inequalities for the remaining two. We conclude with experiments demonstrating the efficacy of these algorithms.

</p>
</details>

<details><summary><b>Selective Multiple Power Iteration: from Tensor PCA to gradient-based exploration of landscapes</b>
<a href="https://arxiv.org/abs/2112.12306">arxiv:2112.12306</a>
&#x1F4C8; 3 <br>
<p>Mohamed Ouerfelli, Mohamed Tamaazousti, Vincent Rivasseau</p></summary>
<p>

**Abstract:** We propose Selective Multiple Power Iterations (SMPI), a new algorithm to address the important Tensor PCA problem that consists in recovering a spike $\bf{v_0}^{\otimes k}$ corrupted by a Gaussian noise tensor $\bf{Z} \in (\mathbb{R}^n)^{\otimes k}$ such that $\bf{T}=\sqrt{n} β\bf{v_0}^{\otimes k} + \bf{Z}$ where $β$ is the signal-to-noise ratio (SNR). SMPI consists in generating a polynomial number of random initializations, performing a polynomial number of symmetrized tensor power iterations on each initialization, then selecting the one that maximizes $\langle \bf{T}, \bf{v}^{\otimes k} \rangle$. Various numerical simulations for $k=3$ in the conventionally considered range $n \leq 1000$ show that the experimental performances of SMPI improve drastically upon existent algorithms and becomes comparable to the theoretical optimal recovery. We show that these unexpected performances are due to a powerful mechanism in which the noise plays a key role for the signal recovery and that takes place at low $β$. Furthermore, this mechanism results from five essential features of SMPI that distinguish it from previous algorithms based on power iteration. These remarkable results may have strong impact on both practical and theoretical applications of Tensor PCA. (i) We provide a variant of this algorithm to tackle low-rank CP tensor decomposition. These proposed algorithms also outperforms existent methods even on real data which shows a huge potential impact for practical applications. (ii) We present new theoretical insights on the behavior of SMPI and gradient descent methods for the optimization in high-dimensional non-convex landscapes that are present in various machine learning problems. (iii) We expect that these results may help the discussion concerning the existence of the conjectured statistical-algorithmic gap.

</p>
</details>

<details><summary><b>Algorithmic Probability of Large Datasets and the Simplicity Bubble Problem in Machine Learning</b>
<a href="https://arxiv.org/abs/2112.12275">arxiv:2112.12275</a>
&#x1F4C8; 3 <br>
<p>Felipe S. Abrahão, Hector Zenil, Fabio Porto, Klaus Wehmuth</p></summary>
<p>

**Abstract:** When mining large datasets in order to predict new data, limitations of the principles behind statistical machine learning pose a serious challenge not only to the Big Data deluge, but also to the traditional assumptions that data generating processes are biased toward low algorithmic complexity. Even when one assumes an underlying algorithmic-informational bias toward simplicity in finite dataset generators, we show that fully automated, with or without access to pseudo-random generators, computable learning algorithms, in particular those of statistical nature used in current approaches to machine learning (including deep learning), can always be deceived, naturally or artificially, by sufficiently large datasets. In particular, we demonstrate that, for every finite learning algorithm, there is a sufficiently large dataset size above which the algorithmic probability of an unpredictable deceiver is an upper bound (up to a multiplicative constant that only depends on the learning algorithm) for the algorithmic probability of any other larger dataset. In other words, very large and complex datasets are as likely to deceive learning algorithms into a "simplicity bubble" as any other particular dataset. These deceiving datasets guarantee that any prediction will diverge from the high-algorithmic-complexity globally optimal solution while converging toward the low-algorithmic-complexity locally optimal solution. We discuss the framework and empirical conditions for circumventing this deceptive phenomenon, moving away from statistical machine learning towards a stronger type of machine learning based on, or motivated by, the intrinsic power of algorithmic information theory and computability theory.

</p>
</details>

<details><summary><b>Human Activity Recognition on wrist-worn accelerometers using self-supervised neural networks</b>
<a href="https://arxiv.org/abs/2112.12272">arxiv:2112.12272</a>
&#x1F4C8; 3 <br>
<p>Niranjan Sridhar, Lance Myers</p></summary>
<p>

**Abstract:** Measures of Activity of Daily Living (ADL) are an important indicator of overall health but difficult to measure in-clinic. Automated and accurate human activity recognition (HAR) using wrist-worn accelerometers enables practical and cost efficient remote monitoring of ADL. Key obstacles in developing high quality HAR is the lack of large labeled datasets and the performance loss when applying models trained on small curated datasets to the continuous stream of heterogeneous data in real-life. In this work we design a self-supervised learning paradigm to create a robust representation of accelerometer data that can generalize across devices and subjects. We demonstrate that this representation can separate activities of daily living and achieve strong HAR accuracy (on multiple benchmark datasets) using very few labels. We also propose a segmentation algorithm which can identify segments of salient activity and boost HAR accuracy on continuous real-life data.

</p>
</details>

<details><summary><b>ML4CO: Is GCNN All You Need? Graph Convolutional Neural Networks Produce Strong Baselines For Combinatorial Optimization Problems, If Tuned and Trained Properly, on Appropriate Data</b>
<a href="https://arxiv.org/abs/2112.12251">arxiv:2112.12251</a>
&#x1F4C8; 3 <br>
<p>Amin Banitalebi-Dehkordi, Yong Zhang</p></summary>
<p>

**Abstract:** The 2021 NeurIPS Machine Learning for Combinatorial Optimization (ML4CO) competition was designed with the goal of improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning models. The competition's main scientific question was the following: is machine learning a viable option for improving traditional combinatorial optimization solvers on specific problem distributions, when historical data is available? This was motivated by the fact that in many practical scenarios, the data changes only slightly between the repetitions of a combinatorial optimization problem, and this is an area where machine learning models are particularly powerful at. This paper summarizes the solution and lessons learned by the Huawei EI-OROAS team in the dual task of the competition. The submission of our team achieved the second place in the final ranking, with a very close distance to the first spot. In addition, our solution was ranked first consistently for several weekly leaderboard updates before the final evaluation. We provide insights gained from a large number of experiments, and argue that a simple Graph Convolutional Neural Network (GCNNs) can achieve state-of-the-art results if trained and tuned properly.

</p>
</details>

<details><summary><b>MC-DGCNN: A Novel DNN Architecture for Multi-Category Point Set Classification</b>
<a href="https://arxiv.org/abs/2112.12219">arxiv:2112.12219</a>
&#x1F4C8; 3 <br>
<p>Majid Farhadloo, Carl Molnar, Gaoxiang Luo, Yan Li, Shashi Shekhar, Rachel L. Maus, Svetomir N. Markovic, Raymond Moore, Alexey Leontovich</p></summary>
<p>

**Abstract:** Point set classification aims to build a representation learning model that distinguishes between spatial and categorical configurations of point set data. This problem is societally important since in many applications domains such as immunology, and microbial ecology. This problem is challenging since the interactions between different categories of points are not always equal; as a result, the representation learning model must selectively learn the most relevant multi-categorical relationships. The related works are limited (1) in learning the importance of different multi-categorical relationships, especially for high-order interactions, and (2) do not fully exploit the spatial distribution of points beyond simply measuring relative distance or applying a feed-forward neural network to coordinates. To overcome these limitations, we leverage the dynamic graph convolutional neural network (DGCNN) architecture to design a novel multi-category DGCNN (MC-DGCNN), contributing location representation and point pair attention layers for multi-categorical point set classification. MC-DGCNN has the ability to identify the categorical importance of each point pair and extends this to N-way spatial relationships, while still preserving all the properties and benefits of DGCNN (e.g., differentiability). Experimental results show that the proposed architecture is computationally efficient and significantly outperforms current deep learning architectures on real-world datasets.

</p>
</details>

<details><summary><b>Fine-grained Multi-Modal Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2112.12182">arxiv:2112.12182</a>
&#x1F4C8; 3 <br>
<p>Duo Wang, Salah Karout</p></summary>
<p>

**Abstract:** Multi-Modal Self-Supervised Learning from videos has been shown to improve model's performance on various downstream tasks. However, such Self-Supervised pre-training requires large batch sizes and a large amount of computation resources due to the noise present in the uncurated data. This is partly due to the fact that the prevalent training scheme is trained on coarse-grained setting, in which vectors representing the whole video clips or natural language sentences are used for computing similarity. Such scheme makes training noisy as part of the video clips can be totally not correlated with the other-modality input such as text description. In this paper, we propose a fine-grained multi-modal self-supervised training scheme that computes the similarity between embeddings at finer-scale (such as individual feature map embeddings and embeddings of phrases), and uses attention mechanisms to reduce noisy pairs' weighting in the loss function. We show that with the proposed pre-training scheme, we can train smaller models, with smaller batch-size and much less computational resources to achieve downstream tasks performances comparable to State-Of-The-Art, for tasks including action recognition and text-image retrievals.

</p>
</details>

<details><summary><b>Simple and near-optimal algorithms for hidden stratification and multi-group learning</b>
<a href="https://arxiv.org/abs/2112.12181">arxiv:2112.12181</a>
&#x1F4C8; 3 <br>
<p>Christopher Tosh, Daniel Hsu</p></summary>
<p>

**Abstract:** Multi-group agnostic learning is a formal learning criterion that is concerned with the conditional risks of predictors within subgroups of a population. The criterion addresses recent practical concerns such as subgroup fairness and hidden stratification. This paper studies the structure of solutions to the multi-group learning problem, and provides simple and near-optimal algorithms for the learning problem.

</p>
</details>

<details><summary><b>Machine learning nonequilibrium electron forces for adiabatic spin dynamics</b>
<a href="https://arxiv.org/abs/2112.12124">arxiv:2112.12124</a>
&#x1F4C8; 3 <br>
<p>Puhan Zhang, Gia-Wei Chern</p></summary>
<p>

**Abstract:** We present a generalized potential theory of nonequilibrium torques for the Landau-Lifshitz equation. The general formulation of exchange forces in terms of two potential energies allows for the implementation of accurate machine learning models for adiabatic spin dynamics of out-of-equilibrium itinerant magnetic systems. To demonstrate our approach, we develop a deep-learning neural network that successfully learns the forces in a driven s-d model computed from the nonequilibrium Green's function method. We show that the Landau-Lifshitz dynamics simulations with forces predicted from the neural-net model accurately reproduce the voltage-driven domain-wall propagation. Our work opens a new avenue for multi-scale modeling of nonequilibrium dynamical phenomena in itinerant magnets and spintronics based on machine-learning models.

</p>
</details>

<details><summary><b>Detect & Reject for Transferability of Black-box Adversarial Attacks Against Network Intrusion Detection Systems</b>
<a href="https://arxiv.org/abs/2112.12095">arxiv:2112.12095</a>
&#x1F4C8; 3 <br>
<p>Islam Debicha, Thibault Debatty, Jean-Michel Dricot, Wim Mees, Tayeb Kenaza</p></summary>
<p>

**Abstract:** In the last decade, the use of Machine Learning techniques in anomaly-based intrusion detection systems has seen much success. However, recent studies have shown that Machine learning in general and deep learning specifically are vulnerable to adversarial attacks where the attacker attempts to fool models by supplying deceptive input. Research in computer vision, where this vulnerability was first discovered, has shown that adversarial images designed to fool a specific model can deceive other machine learning models. In this paper, we investigate the transferability of adversarial network traffic against multiple machine learning-based intrusion detection systems. Furthermore, we analyze the robustness of the ensemble intrusion detection system, which is notorious for its better accuracy compared to a single model, against the transferability of adversarial attacks. Finally, we examine Detect & Reject as a defensive mechanism to limit the effect of the transferability property of adversarial network traffic against machine learning-based intrusion detection systems.

</p>
</details>

<details><summary><b>Catch Me If You GAN: Using Artificial Intelligence for Fake Log Generation</b>
<a href="https://arxiv.org/abs/2112.12006">arxiv:2112.12006</a>
&#x1F4C8; 3 <br>
<p>Christian Toemmel</p></summary>
<p>

**Abstract:** With artificial intelligence (AI) becoming relevant in various parts of everyday life, other technologies are already widely influenced by the new way of handling large amounts of data. Although widespread already, AI has had only punctual influences on the cybersecurity field specifically. Many techniques and technologies used by cybersecurity experts function through manual labor and barely draw on automation, e.g., logs are often reviewed manually by system admins for potentially malicious keywords. This work evaluates the use of a special type of AI called generative adversarial networks (GANs) for log generation. More precisely, three different generative adversarial networks, SeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their performance, focusing on generating new logs as a means of deceiving system admins for red teams. Although static generators for fake logs have been around for a while, their produces are usually easy to reveal as such. Using AI as an approach to this problem has not been widely researched. Identified challenges consist of formatting, dates and times, and overall consistency. Summing up the results, GANs seem not to be a good fit for generating fake logs. Their capability to detect fake logs, however, might be of use in practical scenarios.

</p>
</details>

<details><summary><b>Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment</b>
<a href="https://arxiv.org/abs/2112.11947">arxiv:2112.11947</a>
&#x1F4C8; 3 <br>
<p>Aizaz Sharif, Dusica Marijan</p></summary>
<p>

**Abstract:** Deep reinforcement learning is actively used for training autonomous driving agents in a vision-based urban simulated environment. Due to the large availability of various reinforcement learning algorithms, we are still unsure of which one works better while training autonomous cars in single-agent as well as multi-agent driving environments. A comparison of deep reinforcement learning in vision-based autonomous driving will open up the possibilities for training better autonomous car policies. Also, autonomous cars trained on deep reinforcement learning-based algorithms are known for being vulnerable to adversarial attacks, and we have less information on which algorithms would act as a good adversarial agent. In this work, we provide a systematic evaluation and comparative analysis of 6 deep reinforcement learning algorithms for autonomous and adversarial driving in four-way intersection scenario. Specifically, we first train autonomous cars using state-of-the-art deep reinforcement learning algorithms. Second, we test driving capabilities of the trained autonomous policies in single-agent as well as multi-agent scenarios. Lastly, we use the same deep reinforcement learning algorithms to train adversarial driving agents, in order to test the driving performance of autonomous cars and look for possible collision and offroad driving scenarios. We perform experiments by using vision-only high fidelity urban driving simulated environments.

</p>
</details>

<details><summary><b>Decentralized Task Offloading in Edge Computing: A Multi-User Multi-Armed Bandit Approach</b>
<a href="https://arxiv.org/abs/2112.11818">arxiv:2112.11818</a>
&#x1F4C8; 3 <br>
<p>Xiong Wang, Jiancheng Ye, John C. S. Lui</p></summary>
<p>

**Abstract:** Mobile edge computing facilitates users to offload computation tasks to edge servers for meeting their stringent delay requirements. Previous works mainly explore task offloading when system-side information is given (e.g., server processing speed, cellular data rate), or centralized offloading under system uncertainty. But both generally fall short to handle task placement involving many coexisting users in a dynamic and uncertain environment. In this paper, we develop a multi-user offloading framework considering unknown yet stochastic system-side information to enable a decentralized user-initiated service placement. Specifically, we formulate the dynamic task placement as an online multi-user multi-armed bandit process, and propose a decentralized epoch based offloading (DEBO) to optimize user rewards which are subjected under network delay. We show that DEBO can deduce the optimal user-server assignment, thereby achieving a close-to-optimal service performance and tight O(log T) offloading regret. Moreover, we generalize DEBO to various common scenarios such as unknown reward gap, dynamic entering or leaving of clients, and fair reward distribution, while further exploring when users' offloaded tasks require heterogeneous computing resources. Particularly, we accomplish a sub-linear regret for each of these instances. Real measurements based evaluations corroborate the superiority of our offloading schemes over state-of-the-art approaches in optimizing delay-sensitive rewards.

</p>
</details>

<details><summary><b>DRF Codes: Deep SNR-Robust Feedback Codes</b>
<a href="https://arxiv.org/abs/2112.11789">arxiv:2112.11789</a>
&#x1F4C8; 3 <br>
<p>Mahdi Boloursaz Mashhadi, Deniz Gunduz, Alberto Perotti, Branislav Popovic</p></summary>
<p>

**Abstract:** We present a new deep-neural-network (DNN) based error correction code for fading channels with output feedback, called deep SNR-robust feedback (DRF) code. At the encoder, parity symbols are generated by a long short term memory (LSTM) network based on the message as well as the past forward channel outputs observed by the transmitter in a noisy fashion. The decoder uses a bi-directional LSTM architecture along with a signal to noise ratio (SNR)-aware attention NN to decode the message. The proposed code overcomes two major shortcomings of the previously proposed DNN-based codes over channels with passive output feedback: (i) the SNR-aware attention mechanism at the decoder enables reliable application of the same trained NN over a wide range of SNR values; (ii) curriculum training with batch-size scheduling is used to speed up and stabilize training while improving the SNR-robustness of the resulting code. We show that the DRF codes significantly outperform state-of-the-art in terms of both the SNR-robustness and the error rate in additive white Gaussian noise (AWGN) channel with feedback. In fading channels with perfect phase compensation at the receiver, DRF codes learn to efficiently exploit knowledge of the instantaneous fading amplitude (which is available to the encoder through feedback) to reduce the overhead and complexity associated with channel estimation at the decoder. Finally, we show the effectiveness of DRF codes in multicast channels with feedback, where linear feedback codes are known to be strictly suboptimal.

</p>
</details>

<details><summary><b>Investigating Neighborhood Modeling and Asymmetry Preservation in Digraph Representation Learning</b>
<a href="https://arxiv.org/abs/2112.11734">arxiv:2112.11734</a>
&#x1F4C8; 3 <br>
<p>Honglu Zhou, Advith Chegu, Samuel Sohn, Mubbasir Kapadia</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNNs) traditionally exhibit poor performance for directed graphs (digraphs) due to notable challenges in 1) modeling neighborhoods and 2) preserving asymmetry. In this paper, we address these challenges in traditional GNNs by leveraging hyperbolic collaborative learning from multi-ordered and partitioned neighborhoods, and regularizers inspired by socio-psychological factors. Our resulting formalism, Digraph Hyperbolic Network (D-HYPR) learns node representations in hyperbolic space to avoid structural and semantic distortion of real-world digraphs. We conduct comprehensive experimentation on 4 tasks: link prediction, node classification, sign prediction, and embedding visualization. D-HYPR statistically significantly outperforms the current state of the art on a majority of tasks and datasets, while achieving competitive performance otherwise. Our code and data will be available.

</p>
</details>

<details><summary><b>Adversarial Attacks against Windows PE Malware Detection: A Survey of the State-of-the-Art</b>
<a href="https://arxiv.org/abs/2112.12310">arxiv:2112.12310</a>
&#x1F4C8; 2 <br>
<p>Xiang Ling, Lingfei Wu, Jiangyu Zhang, Zhenqing Qu, Wei Deng, Xiang Chen, Chunming Wu, Shouling Ji, Tianyue Luo, Jingzheng Wu, Yanjun Wu</p></summary>
<p>

**Abstract:** The malware has been being one of the most damaging threats to computers that span across multiple operating systems and various file formats. To defend against the ever-increasing and ever-evolving threats of malware, tremendous efforts have been made to propose a variety of malware detection methods that attempt to effectively and efficiently detect malware. Recent studies have shown that, on the one hand, existing ML and DL enable the superior detection of newly emerging and previously unseen malware. However, on the other hand, ML and DL models are inherently vulnerable to adversarial attacks in the form of adversarial examples, which are maliciously generated by slightly and carefully perturbing the legitimate inputs to confuse the targeted models. Basically, adversarial attacks are initially extensively studied in the domain of computer vision, and some quickly expanded to other domains, including NLP, speech recognition and even malware detection. In this paper, we focus on malware with the file format of portable executable (PE) in the family of Windows operating systems, namely Windows PE malware, as a representative case to study the adversarial attack methods in such adversarial settings. To be specific, we start by first outlining the general learning framework of Windows PE malware detection based on ML/DL and subsequently highlighting three unique challenges of performing adversarial attacks in the context of PE malware. We then conduct a comprehensive and systematic review to categorize the state-of-the-art adversarial attacks against PE malware detection, as well as corresponding defenses to increase the robustness of PE malware detection. We conclude the paper by first presenting other related attacks against Windows PE malware detection beyond the adversarial attacks and then shedding light on future research directions and opportunities.

</p>
</details>

<details><summary><b>Surrogate Likelihoods for Variational Annealed Importance Sampling</b>
<a href="https://arxiv.org/abs/2112.12194">arxiv:2112.12194</a>
&#x1F4C8; 2 <br>
<p>Martin Jankowiak, Du Phan</p></summary>
<p>

**Abstract:** Variational inference is a powerful paradigm for approximate Bayesian inference with a number of appealing properties, including support for model learning and data subsampling. By contrast MCMC methods like Hamiltonian Monte Carlo do not share these properties but remain attractive since, contrary to parametric methods, MCMC is asymptotically unbiased. For these reasons researchers have sought to combine the strengths of both classes of algorithms, with recent approaches coming closer to realizing this vision in practice. However, supporting data subsampling in these hybrid methods can be a challenge, a shortcoming that we address by introducing a surrogate likelihood that can be learned jointly with other variational parameters. We argue theoretically that the resulting algorithm permits the user to make an intuitive trade-off between inference fidelity and computational cost. In an extensive empirical comparison we show that our method performs well in practice and that it is well-suited for black-box inference in probabilistic programming frameworks.

</p>
</details>

<details><summary><b>FLoBC: A Decentralized Blockchain-Based Federated Learning Framework</b>
<a href="https://arxiv.org/abs/2112.11873">arxiv:2112.11873</a>
&#x1F4C8; 2 <br>
<p>Mohamed Ghanem, Fadi Dawoud, Habiba Gamal, Eslam Soliman, Hossam Sharara, Tamer El-Batt</p></summary>
<p>

**Abstract:** The rapid expansion of data worldwide invites the need for more distributed solutions in order to apply machine learning on a much wider scale. The resultant distributed learning systems can have various degrees of centralization. In this work, we demonstrate our solution FLoBC for building a generic decentralized federated learning system using blockchain technology, accommodating any machine learning model that is compatible with gradient descent optimization. We present our system design comprising the two decentralized actors: trainer and validator, alongside our methodology for ensuring reliable and efficient operation of said system. Finally, we utilize FLoBC as an experimental sandbox to compare and contrast the effects of trainer-to-validator ratio, reward-penalty policy, and model synchronization schemes on the overall system performance, ultimately showing by example that a decentralized federated learning system is indeed a feasible alternative to more centralized architectures.

</p>
</details>

<details><summary><b>On Asymptotic Linear Convergence of Projected Gradient Descent for Constrained Least Squares</b>
<a href="https://arxiv.org/abs/2112.11760">arxiv:2112.11760</a>
&#x1F4C8; 2 <br>
<p>Trung Vu, Raviv Raich</p></summary>
<p>

**Abstract:** Many recent problems in signal processing and machine learning such as compressed sensing, image restoration, matrix/tensor recovery, and non-negative matrix factorization can be cast as constrained optimization. Projected gradient descent is a simple yet efficient method for solving such constrained optimization problems. Local convergence analysis furthers our understanding of its asymptotic behavior near the solution, offering sharper bounds on the convergence rate compared to global convergence analysis. However, local guarantees often appear scattered in problem-specific areas of machine learning and signal processing. This manuscript presents a unified framework for the local convergence analysis of projected gradient descent in the context of constrained least squares. The proposed analysis offers insights into pivotal local convergence properties such as the condition of linear convergence, the region of convergence, the exact asymptotic rate of convergence, and the bound on the number of iterations needed to reach a certain level of accuracy. To demonstrate the applicability of the proposed approach, we present a recipe for the convergence analysis of PGD and demonstrate it via a beginning-to-end application of the recipe on four fundamental problems, namely, linearly constrained least squares, sparse recovery, least squares with the unit norm constraint, and matrix completion.

</p>
</details>

<details><summary><b>Towards Malicious address identification in Bitcoin</b>
<a href="https://arxiv.org/abs/2112.11721">arxiv:2112.11721</a>
&#x1F4C8; 2 <br>
<p>Deepesh Chaudhari, Rachit Agarwal, Sandeep Kumar Shukla</p></summary>
<p>

**Abstract:** The temporal aspect of blockchain transactions enables us to study the address's behavior and detect if it is involved in any illicit activity. However, due to the concept of change addresses (used to thwart replay attacks), temporal aspects are not directly applicable in the Bitcoin blockchain. Several pre-processing steps should be performed before such temporal aspects are utilized. We are motivated to study the Bitcoin transaction network and use the temporal features such as burst, attractiveness, and inter-event time along with several graph-based properties such as the degree of node and clustering coefficient to validate the applicability of already existing approaches known for other cryptocurrency blockchains on the Bitcoin blockchain. We generate the temporal and non-temporal feature set and train the Machine Learning (ML) algorithm over different temporal granularities to validate the state-of-the-art methods. We study the behavior of the addresses over different time granularities of the dataset. We identify that after applying change-address clustering, in Bitcoin, existing temporal features can be extracted and ML approaches can be applied. A comparative analysis of results show that the behavior of addresses in Ethereum and Bitcoin is similar with respect to in-degree, out-degree and inter-event time. Further, we identify 3 suspects that showed malicious behavior across different temporal granularities. These suspects are not marked as malicious in Bitcoin.

</p>
</details>

<details><summary><b>Agent Smith: Teaching Question Answering to Jill Watson</b>
<a href="https://arxiv.org/abs/2112.13677">arxiv:2112.13677</a>
&#x1F4C8; 1 <br>
<p>Ashok Goel, Harshvardhan Sikka, Eric Gregori</p></summary>
<p>

**Abstract:** Building AI agents can be costly. Consider a question answering agent such as Jill Watson that automatically answers students' questions on the discussion forums of online classes based on their syllabi and other course materials. Training a Jill on the syllabus of a new online class can take a hundred hours or more. Machine teaching - interactive teaching of an AI agent using synthetic data sets - can reduce the training time because it combines the advantages of knowledge-based AI, machine learning using large data sets, and interactive human-in-loop training. We describe Agent Smith, an interactive machine teaching agent that reduces the time taken to train a Jill for a new online class by an order of magnitude.

</p>
</details>

<details><summary><b>Alpha-Mini: Minichess Agent with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.13666">arxiv:2112.13666</a>
&#x1F4C8; 1 <br>
<p>Michael Sun, Robert Tan</p></summary>
<p>

**Abstract:** We train an agent to compete in the game of Gardner minichess, a downsized variation of chess played on a 5x5 board. We motivated and applied a SOTA actor-critic method Proximal Policy Optimization with Generalized Advantage Estimation. Our initial task centered around training the agent against a random agent. Once we obtained reasonable performance, we then adopted a version of iterative policy improvement adopted by AlphaGo to pit the agent against increasingly stronger versions of itself, and evaluate the resulting performance gain. The final agent achieves a near (.97) perfect win rate against a random agent. We also explore the effects of pretraining the network using a collection of positions obtained via self-play.

</p>
</details>

<details><summary><b>How Much of the Chemical Space Has Been Covered? Measuring and Improving the Variety of Candidate Set in Molecular Generation</b>
<a href="https://arxiv.org/abs/2112.12542">arxiv:2112.12542</a>
&#x1F4C8; 1 <br>
<p>Yutong Xie, Ziqiao Xu, Jiaqi Ma, Qiaozhu Mei</p></summary>
<p>

**Abstract:** Forming a high-quality molecular candidate set that contains a wide range of dissimilar compounds is crucial to the success of drug discovery. However, comparing to the research aiming at optimizing chemical properties, how to measure and improve the variety of drug candidates is relatively understudied. In this paper, we first investigate the problem of properly measuring the molecular variety through both an axiomatic analysis framework and an empirical study. Our analysis suggests that many existing measures are not suitable for evaluating the variety of molecules. We also propose new variety measures based on our analysis. We further explicitly integrate the proposed variety measures into the optimization objective of molecular generation models. Our experiment results demonstrate that this new optimization objective can guide molecular generation models to find compounds that cover a lager chemical space, providing the downstream phases with more distinctive drug candidate choices.

</p>
</details>

<details><summary><b>Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments</b>
<a href="https://arxiv.org/abs/2112.12327">arxiv:2112.12327</a>
&#x1F4C8; 1 <br>
<p>Josh Everts, Xuan Jiang</p></summary>
<p>

**Abstract:** We used a token-wise and document-wise sentiment analysis using both unsupervised and supervised models applied to both news and user reviews dataset. And our token-wise sentiment analysis found a statistically significant difference in sentiment between the two groups (both of which were very large N), our document-wise supervised sentiment analysis found no significant difference in sentiment.

</p>
</details>

<details><summary><b>Investigating Effect of Dialogue History in Multilingual Task Oriented Dialogue Systems</b>
<a href="https://arxiv.org/abs/2112.12318">arxiv:2112.12318</a>
&#x1F4C8; 1 <br>
<p>Michael Sun, Kaili Huang, Mehrad Moradshahi</p></summary>
<p>

**Abstract:** While the English virtual assistants have achieved exciting performance with an enormous amount of training resources, the needs of non-English-speakers have not been satisfied well. Up to Dec 2021, Alexa, one of the most popular smart speakers around the world, is able to support 9 different languages [1], while there are thousands of languages in the world, 91 of which are spoken by more than 10 million people according to statistics published in 2019 [2]. However, training a virtual assistant in other languages than English is often more difficult, especially for those low-resource languages. The lack of high-quality training data restricts the performance of models, resulting in poor user satisfaction. Therefore, we devise an efficient and effective training solution for multilingual task-orientated dialogue systems, using the same dataset generation pipeline and end-to-end dialogue system architecture as BiToD[5], which adopted some key design choices for a minimalistic natural language design where formal dialogue states are used in place of natural language inputs. This reduces the room for error brought by weaker natural language models, and ensures the model can correctly extract the essential slot values needed to perform dialogue state tracking (DST). Our goal is to reduce the amount of natural language encoded at each turn, and the key parameter we investigate is the number of turns (H) to feed as history to model. We first explore the turning point where increasing H begins to yield limiting returns on the overall performance. Then we examine whether the examples a model with small H gets wrong can be categorized in a way for the model to do few-shot finetuning on. Lastly, will explore the limitations of this approach, and whether there is a certain type of examples that this approach will not be able to resolve.

</p>
</details>

<details><summary><b>Entropy-Regularized Partially Observed Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2112.12255">arxiv:2112.12255</a>
&#x1F4C8; 1 <br>
<p>Timothy L. Molloy, Girish N. Nair</p></summary>
<p>

**Abstract:** We investigate partially observed Markov decision processes (POMDPs) with cost functions regularized by entropy terms describing state, observation, and control uncertainty. Standard POMDP techniques are shown to offer bounded-error solutions to these entropy-regularized POMDPs, with exact solutions when the regularization involves the joint entropy of the state, observation, and control trajectories. Our joint-entropy result is particularly surprising since it constitutes a novel, tractable formulation of active state estimation.

</p>
</details>

<details><summary><b>Combinations of Adaptive Filters</b>
<a href="https://arxiv.org/abs/2112.12245">arxiv:2112.12245</a>
&#x1F4C8; 1 <br>
<p>Jerónimo Arenas-García, Luis A. Azpicueta-Ruiz, Magno T. M. Silva, Vitor H. Nascimento, Ali H. Sayed</p></summary>
<p>

**Abstract:** Adaptive filters are at the core of many signal processing applications, ranging from acoustic noise supression to echo cancelation, array beamforming, channel equalization, to more recent sensor network applications in surveillance, target localization, and tracking. A trending approach in this direction is to recur to in-network distributed processing in which individual nodes implement adaptation rules and diffuse their estimation to the network.
  When the a priori knowledge about the filtering scenario is limited or imprecise, selecting the most adequate filter structure and adjusting its parameters becomes a challenging task, and erroneous choices can lead to inadequate performance. To address this difficulty, one useful approach is to rely on combinations of adaptive structures.
  The combination of adaptive filters exploits to some extent the same divide and conquer principle that has also been successfully exploited by the machine-learning community (e.g., in bagging or boosting). In particular, the problem of combining the outputs of several learning algorithms (mixture of experts) has been studied in the computational learning field under a different perspective: rather than studying the expected performance of the mixture, deterministic bounds are derived that apply to individual sequences and, therefore, reflect worst-case scenarios. These bounds require assumptions different from the ones typically used in adaptive filtering, which is the emphasis of this overview article. We review the key ideas and principles behind these combination schemes, with emphasis on design rules. We also illustrate their performance with a variety of examples.

</p>
</details>

<details><summary><b>End to End Software Engineering Research</b>
<a href="https://arxiv.org/abs/2112.11858">arxiv:2112.11858</a>
&#x1F4C8; 1 <br>
<p>Idan Amit</p></summary>
<p>

**Abstract:** End to end learning is machine learning starting in raw data and predicting a desired concept, with all steps done automatically. In software engineering context, we see it as starting from the source code and predicting process metrics. This framework can be used for predicting defects, code quality, productivity and more. End-to-end improves over features based machine learning by not requiring domain experts and being able to extract new knowledge. We describe a dataset of 5M files from 15k projects constructed for this goal. The dataset is constructed in a way that enables not only predicting concepts but also investigating their causes.

</p>
</details>

<details><summary><b>Movie Recommender System using critic consensus</b>
<a href="https://arxiv.org/abs/2112.11854">arxiv:2112.11854</a>
&#x1F4C8; 1 <br>
<p>A Nayan Varma, Kedareshwara Petluri</p></summary>
<p>

**Abstract:** Recommendation systems are perhaps one of the most important agents for industry growth through the modern Internet world. Previous approaches on recommendation systems include collaborative filtering and content based filtering recommendation systems. These 2 methods are disjointed in nature and require the continuous storage of user preferences for a better recommendation. To provide better integration of the two processes, we propose a hybrid recommendation system based on the integration of collaborative and content-based content, taking into account the top critic consensus and movie rating score. We would like to present a novel model that recommends movies based on the combination of user preferences and critical consensus scores.

</p>
</details>

<details><summary><b>Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation</b>
<a href="https://arxiv.org/abs/2112.12218">arxiv:2112.12218</a>
&#x1F4C8; 0 <br>
<p>Agostina Larrazabal, Cesar Martinez, Jose Dolz, Enzo Ferrante</p></summary>
<p>

**Abstract:** Modern deep neural networks have achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. In particular, we design a regularization term that encourages high entropy posteriors for wrong predictions, increasing the network uncertainty in complex scenarios. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging medical image segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also in segmentation quality.

</p>
</details>

<details><summary><b>The Universal $\ell^p$-Metric on Merge Trees</b>
<a href="https://arxiv.org/abs/2112.12165">arxiv:2112.12165</a>
&#x1F4C8; 0 <br>
<p>Robert Cardona, Justin Curry, Tung Lam, Michael Lesnick</p></summary>
<p>

**Abstract:** Adapting a definition given by Bjerkevik and Lesnick for multiparameter persistence modules, we introduce an $\ell^p$-type extension of the interleaving distance on merge trees. We show that our distance is a metric, and that it upper-bounds the $p$-Wasserstein distance between the associated barcodes. For each $p\in[1,\infty]$, we prove that this distance is stable with respect to cellular sublevel filtrations and that it is the universal (i.e., largest) distance satisfying this stability property. In the $p=\infty$ case, this gives a novel proof of universality for the interleaving distance on merge trees.

</p>
</details>


{% endraw %}
Prev: [2021.12.21]({{ '/2021/12/21/2021.12.21.html' | relative_url }})  Next: [2021.12.23]({{ '/2021/12/23/2021.12.23.html' | relative_url }})