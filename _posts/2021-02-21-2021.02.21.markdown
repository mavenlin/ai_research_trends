Prev: [2021.02.20]({{ '/2021/02/20/2021.02.20.html' | relative_url }})  Next: [2021.02.22]({{ '/2021/02/22/2021.02.22.html' | relative_url }})
{% raw %}
## Summary for 2021-02-21, created on 2021-12-24


<details><summary><b>Inductive logic programming at 30</b>
<a href="https://arxiv.org/abs/2102.10556">arxiv:2102.10556</a>
&#x1F4C8; 32 <br>
<p>Andrew Cropper, Sebastijan Dumančić, Richard Evans, Stephen H. Muggleton</p></summary>
<p>

**Abstract:** Inductive logic programming (ILP) is a form of logic-based machine learning. The goal is to induce a hypothesis (a logic program) that generalises given training examples. As ILP turns 30, we review the last decade of research. We focus on (i) new meta-level search methods, (ii) techniques for learning recursive programs, (iii) new approaches for predicate invention, and (iv) the use of different technologies. We conclude by discussing current limitations of ILP and directions for future research.

</p>
</details>

<details><summary><b>Learning Efficient Navigation in Vortical Flow Fields</b>
<a href="https://arxiv.org/abs/2102.10536">arxiv:2102.10536</a>
&#x1F4C8; 23 <br>
<p>Peter Gunnarson, Ioannis Mandralis, Guido Novati, Petros Koumoutsakos, John O. Dabiri</p></summary>
<p>

**Abstract:** Efficient point-to-point navigation in the presence of a background flow field is important for robotic applications such as ocean surveying. In such applications, robots may only have knowledge of their immediate surroundings or be faced with time-varying currents, which limits the use of optimal control techniques for planning trajectories. Here, we apply a novel Reinforcement Learning algorithm to discover time-efficient navigation policies to steer a fixed-speed swimmer through an unsteady two-dimensional flow field. The algorithm entails inputting environmental cues into a deep neural network that determines the swimmer's actions, and deploying Remember and Forget Experience replay. We find that the resulting swimmers successfully exploit the background flow to reach the target, but that this success depends on the type of sensed environmental cue. Surprisingly, a velocity sensing approach outperformed a bio-mimetic vorticity sensing approach by nearly two-fold in success rate. Equipped with local velocity measurements, the reinforcement learning algorithm achieved near 100% success in reaching the target locations while approaching the time-efficiency of paths found by a global optimal control planner.

</p>
</details>

<details><summary><b>Do Generative Models Know Disentanglement? Contrastive Learning is All You Need</b>
<a href="https://arxiv.org/abs/2102.10543">arxiv:2102.10543</a>
&#x1F4C8; 9 <br>
<p>Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng</p></summary>
<p>

**Abstract:** Disentangled generative models are typically trained with an extra regularization term, which encourages the traversal of each latent factor to make a distinct and independent change at the cost of generation quality. When traversing the latent space of generative models trained without the disentanglement term, the generated samples show semantically meaningful change, raising the question: do generative models know disentanglement? We propose an unsupervised and model-agnostic method: Disentanglement via Contrast (DisCo) in the Variation Space. DisCo consists of: (i) a Navigator providing traversal directions in the latent space, and (ii) a $Δ$-Contrastor composed of two shared-weight Encoders, which encode image pairs along these directions to disentangled representations respectively, and a difference operator to map the encoded representations to the Variation Space. We propose two more key techniques for DisCo: entropy-based domination loss to make the encoded representations more disentangled and the strategy of flipping hard negatives to address directions with the same semantic meaning. By optimizing the Navigator to discover disentangled directions in the latent space and Encoders to extract disentangled representations from images with Contrastive Learning, DisCo achieves the state-of-the-art disentanglement given pretrained non-disentangled generative models, including GAN, VAE, and Flow. Project page at https://github.com/xrenaa/DisCo.

</p>
</details>

<details><summary><b>3D Vision-guided Pick-and-Place Using Kuka LBR iiwa Robot</b>
<a href="https://arxiv.org/abs/2102.10710">arxiv:2102.10710</a>
&#x1F4C8; 8 <br>
<p>Hanlin Niu, Ze Ji, Zihang Zhu, Hujun Yin, Joaquin Carrasco</p></summary>
<p>

**Abstract:** This paper presents the development of a control system for vision-guided pick-and-place tasks using a robot arm equipped with a 3D camera. The main steps include camera intrinsic and extrinsic calibration, hand-eye calibration, initial object pose registration, objects pose alignment algorithm, and pick-and-place execution. The proposed system allows the robot be able to to pick and place object with limited times of registering a new object and the developed software can be applied for new object scenario quickly. The integrated system was tested using the hardware combination of kuka iiwa, Robotiq grippers (two finger gripper and three finger gripper) and 3D cameras (Intel realsense D415 camera, Intel realsense D435 camera, Microsoft Kinect V2). The whole system can also be modified for the combination of other robotic arm, gripper and 3D camera.

</p>
</details>

<details><summary><b>Rethinking Content and Style: Exploring Bias for Unsupervised Disentanglement</b>
<a href="https://arxiv.org/abs/2102.10544">arxiv:2102.10544</a>
&#x1F4C8; 7 <br>
<p>Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng</p></summary>
<p>

**Abstract:** Content and style (C-S) disentanglement intends to decompose the underlying explanatory factors of objects into two independent subspaces. From the unsupervised disentanglement perspective, we rethink content and style and propose a formulation for unsupervised C-S disentanglement based on our assumption that different factors are of different importance and popularity for image reconstruction, which serves as a data bias. The corresponding model inductive bias is introduced by our proposed C-S disentanglement Module (C-S DisMo), which assigns different and independent roles to content and style when approximating the real data distributions. Specifically, each content embedding from the dataset, which encodes the most dominant factors for image reconstruction, is assumed to be sampled from a shared distribution across the dataset. The style embedding for a particular image, encoding the remaining factors, is used to customize the shared distribution through an affine transformation. The experiments on several popular datasets demonstrate that our method achieves the state-of-the-art unsupervised C-S disentanglement, which is comparable or even better than supervised methods. We verify the effectiveness of our method by downstream tasks: domain translation and single-view 3D reconstruction. Project page at https://github.com/xrenaa/CS-DisMo.

</p>
</details>

<details><summary><b>Hide and Seek: Outwitting Community Detection Algorithms</b>
<a href="https://arxiv.org/abs/2102.10759">arxiv:2102.10759</a>
&#x1F4C8; 6 <br>
<p>Shravika Mittal, Debarka Sengupta, Tanmoy Chakraborty</p></summary>
<p>

**Abstract:** Community affiliation of a node plays an important role in determining its contextual position in the network, which may raise privacy concerns when a sensitive node wants to hide its identity in a network. Oftentimes, a target community seeks to protect itself from adversaries so that its constituent members remain hidden inside the network. The current study focuses on hiding such sensitive communities so that the community affiliation of the targeted nodes can be concealed. This leads to the problem of community deception which investigates the avenues of minimally rewiring nodes in a network so that a given target community maximally hides from a community detection algorithm. We formalize the problem of community deception and introduce NEURAL, a novel method that greedily optimizes a node-centric objective function to determine the rewiring strategy. Theoretical settings pose a restriction on the number of strategies that can be employed to optimize the objective function, which in turn reduces the overhead of choosing the best strategy from multiple options. We also show that our objective function is submodular and monotone. When tested on both synthetic and 7 real-world networks, NEURAL is able to deceive 6 widely used community detection algorithms. We benchmark its performance with respect to 4 state-of-the-art methods on 4 evaluation metrics. Additionally, our qualitative analysis of 3 other attributed real-world networks reveals that NEURAL, quite strikingly, captures important meta-information about edges that otherwise could not be inferred by observing only their topological structures.

</p>
</details>

<details><summary><b>MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation</b>
<a href="https://arxiv.org/abs/2102.10663">arxiv:2102.10663</a>
&#x1F4C8; 6 <br>
<p>Yen Nhi Truong Vu, Richard Wang, Niranjan Balachandar, Can Liu, Andrew Y. Ng, Pranav Rajpurkar</p></summary>
<p>

**Abstract:** Self-supervised contrastive learning between pairs of multiple views of the same image has been shown to successfully leverage unlabeled data to produce meaningful visual representations for both natural and medical images. However, there has been limited work on determining how to select pairs for medical images, where availability of patient metadata can be leveraged to improve representations. In this work, we develop a method to select positive pairs coming from views of possibly different images through the use of patient metadata. We compare strategies for selecting positive pairs for chest X-ray interpretation including requiring them to be from the same patient, imaging study or laterality. We evaluate downstream task performance by fine-tuning the linear layer on 1% of the labeled dataset for pleural effusion classification. Our best performing positive pair selection strategy, which involves using images from the same patient from the same study across all lateralities, achieves a performance increase of 14.4% in mean AUC from the ImageNet pretrained baseline. Our controlled experiments show that the keys to improving downstream performance on disease classification are (1) using patient metadata to appropriately create positive pairs from different images with the same underlying pathologies, and (2) maximizing the number of different images used in query pairing. In addition, we explore leveraging patient metadata to select hard negative pairs for contrastive learning, but do not find improvement over baselines that do not use metadata. Our method is broadly applicable to medical image interpretation and allows flexibility for incorporating medical insights in choosing pairs for contrastive learning.

</p>
</details>

<details><summary><b>Efficient Two-Stream Network for Violence Detection Using Separable Convolutional LSTM</b>
<a href="https://arxiv.org/abs/2102.10590">arxiv:2102.10590</a>
&#x1F4C8; 6 <br>
<p>Zahidul Islam, Mohammad Rukonuzzaman, Raiyan Ahmed, Md. Hasanul Kabir, Moshiur Farazi</p></summary>
<p>

**Abstract:** Automatically detecting violence from surveillance footage is a subset of activity recognition that deserves special attention because of its wide applicability in unmanned security monitoring systems, internet video filtration, etc. In this work, we propose an efficient two-stream deep learning architecture leveraging Separable Convolutional LSTM (SepConvLSTM) and pre-trained MobileNet where one stream takes in background suppressed frames as inputs and other stream processes difference of adjacent frames. We employed simple and fast input pre-processing techniques that highlight the moving objects in the frames by suppressing non-moving backgrounds and capture the motion in-between frames. As violent actions are mostly characterized by body movements these inputs help produce discriminative features. SepConvLSTM is constructed by replacing convolution operation at each gate of ConvLSTM with a depthwise separable convolution that enables producing robust long-range Spatio-temporal features while using substantially fewer parameters. We experimented with three fusion methods to combine the output feature maps of the two streams. Evaluation of the proposed methods was done on three standard public datasets. Our model outperforms the accuracy on the larger and more challenging RWF-2000 dataset by more than a 2% margin while matching state-of-the-art results on the smaller datasets. Our experiments lead us to conclude, the proposed models are superior in terms of both computational efficiency and detection accuracy.

</p>
</details>

<details><summary><b>Causal Mediation Analysis with Hidden Confounders</b>
<a href="https://arxiv.org/abs/2102.11724">arxiv:2102.11724</a>
&#x1F4C8; 5 <br>
<p>Lu Cheng, Ruocheng Guo, Huan Liu</p></summary>
<p>

**Abstract:** An important problem in causal inference is to break down the total effect of a treatment on an outcome into different causal pathways and to quantify the causal effect in each pathway. For instance, in causal fairness, the total effect of being a male employee (i.e., treatment) constitutes its direct effect on annual income (i.e., outcome) and the indirect effect via the employee's occupation (i.e., mediator). Causal mediation analysis (CMA) is a formal statistical framework commonly used to reveal such underlying causal mechanisms. One major challenge of CMA in observational studies is handling confounders, variables that cause spurious causal relationships among treatment, mediator, and outcome. Conventional methods assume sequential ignorability that implies all confounders can be measured, which is often unverifiable in practice. This work aims to circumvent the stringent sequential ignorability assumptions and consider hidden confounders. Drawing upon proxy strategies and recent advances in deep learning, we propose to simultaneously uncover the latent variables that characterize hidden confounders and estimate the causal effects. Empirical evaluations using both synthetic and semi-synthetic datasets validate the effectiveness of the proposed method. We further show the potentials of our approach for causal fairness analysis.

</p>
</details>

<details><summary><b>Pre-Training BERT on Arabic Tweets: Practical Considerations</b>
<a href="https://arxiv.org/abs/2102.10684">arxiv:2102.10684</a>
&#x1F4C8; 5 <br>
<p>Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Kareem Darwish, Younes Samih</p></summary>
<p>

**Abstract:** Pretraining Bidirectional Encoder Representations from Transformers (BERT) for downstream NLP tasks is a non-trival task. We pretrained 5 BERT models that differ in the size of their training sets, mixture of formal and informal Arabic, and linguistic preprocessing. All are intended to support Arabic dialects and social media. The experiments highlight the centrality of data diversity and the efficacy of linguistically aware segmentation. They also highlight that more data or more training step do not necessitate better models. Our new models achieve new state-of-the-art results on several downstream tasks. The resulting models are released to the community under the name QARiB.

</p>
</details>

<details><summary><b>Pruning the Index Contents for Memory Efficient Open-Domain QA</b>
<a href="https://arxiv.org/abs/2102.10697">arxiv:2102.10697</a>
&#x1F4C8; 4 <br>
<p>Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz</p></summary>
<p>

**Abstract:** This work presents a novel pipeline that demonstrates what is achievable with a combined effort of state-of-the-art approaches. Specifically, it proposes the novel R2-D2 (Rank twice, reaD twice) pipeline composed of retriever, passage reranker, extractive reader, generative reader and a simple way to combine them. Furthermore, previous work often comes with a massive index of external documents that scales in the order of tens of GiB. This work presents a simple approach for pruning the contents of a massive index such that the open-domain QA system altogether with index, OS, and library components fits into 6GiB docker image while retaining only 8% of original index contents and losing only 3% EM accuracy.

</p>
</details>

<details><summary><b>Classification of COVID-19 via Homology of CT-SCAN</b>
<a href="https://arxiv.org/abs/2102.10593">arxiv:2102.10593</a>
&#x1F4C8; 4 <br>
<p>Sohail Iqbal, H. Fareed Ahmed, Talha Qaiser, Muhammad Imran Qureshi, Nasir Rajpoot</p></summary>
<p>

**Abstract:** In this worldwide spread of SARS-CoV-2 (COVID-19) infection, it is of utmost importance to detect the disease at an early stage especially in the hot spots of this epidemic. There are more than 110 Million infected cases on the globe, sofar. Due to its promptness and effective results computed tomography (CT)-scan image is preferred to the reverse-transcription polymerase chain reaction (RT-PCR). Early detection and isolation of the patient is the only possible way of controlling the spread of the disease. Automated analysis of CT-Scans can provide enormous support in this process. In this article, We propose a novel approach to detect SARS-CoV-2 using CT-scan images. Our method is based on a very intuitive and natural idea of analyzing shapes, an attempt to mimic a professional medic. We mainly trace SARS-CoV-2 features by quantifying their topological properties. We primarily use a tool called persistent homology, from Topological Data Analysis (TDA), to compute these topological properties. We train and test our model on the "SARS-CoV-2 CT-scan dataset" \citep{soares2020sars}, an open-source dataset, containing 2,481 CT-scans of normal and COVID-19 patients. Our model yielded an overall benchmark F1 score of $99.42\% $, accuracy $99.416\%$, precision $99.41\%$, and recall $99.42\%$. The TDA techniques have great potential that can be utilized for efficient and prompt detection of COVID-19. The immense potential of TDA may be exploited in clinics for rapid and safe detection of COVID-19 globally, in particular in the low and middle-income countries where RT-PCR labs and/or kits are in a serious crisis.

</p>
</details>

<details><summary><b>Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification</b>
<a href="https://arxiv.org/abs/2102.10592">arxiv:2102.10592</a>
&#x1F4C8; 4 <br>
<p>Naoya Muramatsu, Hai-Tao Yu</p></summary>
<p>

**Abstract:** With the continued innovations of deep neural networks, spiking neural networks (SNNs) that more closely resemble biological brain synapses have attracted attention owing to their low power consumption.However, for continuous data values, they must employ a coding process to convert the values to spike trains.Thus, they have not yet exceeded the performance of artificial neural networks (ANNs), which handle such values directly.To this end, we combine an ANN and an SNN to build versatile hybrid neural networks (HNNs) that improve the concerned performance.To qualify this performance, MNIST and CIFAR-10 image datasets are used for various classification tasks in which the training and coding methods changes.In addition, we present simultaneous and separate methods to train the artificial and spiking layers, considering the coding methods of each.We find that increasing the number of artificial layers at the expense of spiking layers improves the HNN performance.For straightforward datasets such as MNIST, it is easy to achieve the same performance as ANNs by using duplicate coding and separate learning.However, for more complex tasks, the use of Gaussian coding and simultaneous learning is found to improve the accuracy of HNNs while utilizing a smaller number of artificial layers.

</p>
</details>

<details><summary><b>Unsupervised Meta Learning for One Shot Title Compression in Voice Commerce</b>
<a href="https://arxiv.org/abs/2102.10760">arxiv:2102.10760</a>
&#x1F4C8; 3 <br>
<p>Snehasish Mukherjee</p></summary>
<p>

**Abstract:** Product title compression for voice and mobile commerce is a well studied problem with several supervised models proposed so far. However these models have 2 major limitations; they are not designed to generate compressions dynamically based on cues at inference time, and they do not transfer well to different categories at test time. To address these shortcomings we model title compression as a meta learning problem where we ask can we learn a title compression model given only 1 example compression? We adopt an unsupervised approach to meta training by proposing an automatic task generation algorithm that models the observed label generation process as the outcome of 4 unobserved processes. We create parameterized approximations to each of these 4 latent processes to get a principled way of generating random compression rules, which are treated as different tasks. For our main meta learner, we use 2 models; M1 and M2. M1 is a task agnostic embedding generator whose output feeds into M2 which is a task specific label generator. We pre-train M1 on a novel unsupervised segment rank prediction task that allows us to treat M1 as a segment generator that also learns to rank segments during the meta-training process. Our experiments on 16000 crowd generated meta-test examples show that our unsupervised meta training regime is able to acquire a learning algorithm for different tasks after seeing only 1 example for each task. Further, we show that our model trained end to end as a black box meta learner, outperforms non parametric approaches. Our best model obtains an F1 score of 0.8412, beating the baseline by a large margin of 25 F1 points.

</p>
</details>

<details><summary><b>A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization</b>
<a href="https://arxiv.org/abs/2102.10707">arxiv:2102.10707</a>
&#x1F4C8; 3 <br>
<p>HanQin Cai, Yuchen Lou, Daniel McKenzie, Wotao Yin</p></summary>
<p>

**Abstract:** We consider the zeroth-order optimization problem in the huge-scale setting, where the dimension of the problem is so large that performing even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration computational complexity. In addition, we discuss how the memory footprint of ZO-BCD can be reduced even further by the clever use of circulant measurement matrices. As an application of our new method, we propose the idea of crafting adversarial attacks on neural network based classifiers in a wavelet domain, which can result in problem dimensions of over 1.7 million. In particular, we show that crafting adversarial examples to audio classifiers in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9%.

</p>
</details>

<details><summary><b>Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution</b>
<a href="https://arxiv.org/abs/2102.10640">arxiv:2102.10640</a>
&#x1F4C8; 3 <br>
<p>Ahlad Kumar, Harsh Vardhan Singh</p></summary>
<p>

**Abstract:** The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer ($TCL$). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters.

</p>
</details>

<details><summary><b>Web-based Application for Detecting Indonesian Clickbait Headlines using IndoBERT</b>
<a href="https://arxiv.org/abs/2102.10601">arxiv:2102.10601</a>
&#x1F4C8; 3 <br>
<p>Muhammad Noor Fakhruzzaman, Sie Wildan Gunawan</p></summary>
<p>

**Abstract:** With increasing usage of clickbaits in Indonesian Online News, newsworthy articles sometimes get buried among clickbaity news. A reliable and lightweight tool is needed to detect such clickbaits on-the-go. Leveraging state-of-the-art natural language processing model BERT, a RESTful API based application is developed. This study offloaded the computing resources needed to train the model on the cloud server, while the client-side application only needs to send a request to the API and the cloud server will handle the rest. This study proposed the design and developed a web-based application to detect clickbait in Indonesian using IndoBERT as a language model. The application usage is discussed and available for public use with a performance of mean ROC-AUC of 89%.

</p>
</details>

<details><summary><b>Mapping Surgeon's Hand/Finger Motion During Conventional Microsurgery to Enhance Intuitive Surgical Robot Teleoperation</b>
<a href="https://arxiv.org/abs/2102.10585">arxiv:2102.10585</a>
&#x1F4C8; 3 <br>
<p>Mohammad Fattahi Sani, Raimondo Ascione, Sanja Dogramadzi</p></summary>
<p>

**Abstract:** Purpose: Recent developments in robotics and artificial intelligence (AI) have led to significant advances in healthcare technologies enhancing robot-assisted minimally invasive surgery (RAMIS) in some surgical specialties. However, current human-robot interfaces lack intuitive teleoperation and cannot mimic surgeon's hand/finger sensing and fine motion. These limitations make tele-operated robotic surgery not suitable for micro-surgery and difficult to learn for established surgeons. We report a pilot study showing an intuitive way of recording and mapping surgeon's gross hand motion and the fine synergic motion during cardiac micro-surgery as a way to enhance future intuitive teleoperation. Methods: We set to develop a prototype system able to train a Deep Neural Net-work (DNN) by mapping wrist, hand and surgical tool real-time data acquisition(RTDA) inputs during mock-up heart micro-surgery procedures. The trained network was used to estimate the tools poses from refined hand joint angles. Results: Based on surgeon's feedback during mock micro-surgery, the developed wearable system with light-weight sensors for motion tracking did not interfere with the surgery and instrument handling. The wearable motion tracking system used 15 finger-thumb-wrist joint angle sensors to generate meaningful data-sets representing inputs of the DNN network with new hand joint angles added as necessary based on comparing the estimated tool poses against measured tool pose. The DNN architecture was optimized for the highest estimation accuracy and the ability to determine the tool pose with the least mean squared error. This novel approach showed that the surgical instrument's pose, an essential requirement for teleoperation, can be accurately estimated from recorded surgeon's hand/finger movements with a mean squared error (MSE) less than 0.3%

</p>
</details>

<details><summary><b>Contrastive Self-supervised Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2102.10557">arxiv:2102.10557</a>
&#x1F4C8; 3 <br>
<p>Nam Nguyen, J. Morris Chang</p></summary>
<p>

**Abstract:** This paper proposes a novel cell-based neural architecture search algorithm (NAS), which completely alleviates the expensive costs of data labeling inherited from supervised learning. Our algorithm capitalizes on the effectiveness of self-supervised learning for image representations, which is an increasingly crucial topic of computer vision. First, using only a small amount of unlabeled train data under contrastive self-supervised learning allow us to search on a more extensive search space, discovering better neural architectures without surging the computational resources. Second, we entirely relieve the cost for labeled data (by contrastive loss) in the search stage without compromising architectures' final performance in the evaluation phase. Finally, we tackle the inherent discrete search space of the NAS problem by sequential model-based optimization via the tree-parzen estimator (SMBO-TPE), enabling us to reduce the computational expense response surface significantly. An extensive number of experiments empirically show that our search algorithm can achieve state-of-the-art results with better efficiency in data labeling cost, searching time, and accuracy in final validation.

</p>
</details>

<details><summary><b>Genetic Meta-Structure Search for Recommendation on Heterogeneous Information Network</b>
<a href="https://arxiv.org/abs/2102.10550">arxiv:2102.10550</a>
&#x1F4C8; 3 <br>
<p>Zhenyu Han, Fengli Xu, Jinghan Shi, Yu Shang, Haorui Ma, Pan Hui, Yong Li</p></summary>
<p>

**Abstract:** In the past decade, the heterogeneous information network (HIN) has become an important methodology for modern recommender systems. To fully leverage its power, manually designed network templates, i.e., meta-structures, are introduced to filter out semantic-aware information. The hand-crafted meta-structure rely on intense expert knowledge, which is both laborious and data-dependent. On the other hand, the number of meta-structures grows exponentially with its size and the number of node types, which prohibits brute-force search. To address these challenges, we propose Genetic Meta-Structure Search (GEMS) to automatically optimize meta-structure designs for recommendation on HINs. Specifically, GEMS adopts a parallel genetic algorithm to search meaningful meta-structures for recommendation, and designs dedicated rules and a meta-structure predictor to efficiently explore the search space. Finally, we propose an attention based multi-view graph convolutional network module to dynamically fuse information from different meta-structures. Extensive experiments on three real-world datasets suggest the effectiveness of GEMS, which consistently outperforms all baseline methods in HIN recommendation. Compared with simplified GEMS which utilizes hand-crafted meta-paths, GEMS achieves over $6\%$ performance gain on most evaluation metrics. More importantly, we conduct an in-depth analysis on the identified meta-structures, which sheds light on the HIN based recommender system design.

</p>
</details>

<details><summary><b>Uncertainty-Aware Deep Learning for Autonomous Safe Landing Site Selection</b>
<a href="https://arxiv.org/abs/2102.10545">arxiv:2102.10545</a>
&#x1F4C8; 3 <br>
<p>Kento Tomita, Katherine A. Skinner, Koki Ho</p></summary>
<p>

**Abstract:** Hazard detection is critical for enabling autonomous landing on planetary surfaces. Current state-of-the-art methods leverage traditional computer vision approaches to automate identification of safe terrain from input digital elevation models (DEMs). However, performance for these methods can degrade for input DEMs with increased sensor noise. At the same time, deep learning techniques have been developed for various applications. Nevertheless, their applicability to safety-critical space missions has been often limited due to concerns regarding their outputs' reliability. In response to this background, this paper proposes an uncertainty-aware learning-based method for hazard detection and landing site selection. The developed approach enables reliable safe landing site selection by: (i) generating a safety prediction map and its uncertainty map together via Bayesian deep learning and semantic segmentation; and (ii) using the generated uncertainty map to filter out the uncertain pixels in the prediction map so that the safe landing site selection is performed only based on the certain pixels (i.e., pixels for which the model is certain about its safety prediction). Experiments are presented with simulated data based on a Mars HiRISE digital terrain model and varying noise levels to demonstrate the performance of the proposed approach.

</p>
</details>

<details><summary><b>Automatic Code Generation using Pre-Trained Language Models</b>
<a href="https://arxiv.org/abs/2102.10535">arxiv:2102.10535</a>
&#x1F4C8; 3 <br>
<p>Luis Perez, Lizi Ottens, Sudharshan Viswanathan</p></summary>
<p>

**Abstract:** Recent advancements in natural language processing \cite{gpt2} \cite{BERT} have led to near-human performance in multiple natural language tasks. In this paper, we seek to understand whether similar techniques can be applied to a highly structured environment with strict syntax rules. Specifically, we propose an end-to-end machine learning model for code generation in the Python language built on-top of pre-trained language models. We demonstrate that a fine-tuned model can perform well in code generation tasks, achieving a BLEU score of 0.22, an improvement of 46\% over a reasonable sequence-to-sequence baseline. All results and related code used for training and data processing are available on GitHub.

</p>
</details>

<details><summary><b>The Effects of Image Distribution and Task on Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2102.10534">arxiv:2102.10534</a>
&#x1F4C8; 3 <br>
<p>Owen Kunhardt, Arturo Deza, Tomaso Poggio</p></summary>
<p>

**Abstract:** In this paper, we propose an adaptation to the area under the curve (AUC) metric to measure the adversarial robustness of a model over a particular $ε$-interval $[ε_0, ε_1]$ (interval of adversarial perturbation strengths) that facilitates unbiased comparisons across models when they have different initial $ε_0$ performance. This can be used to determine how adversarially robust a model is to different image distributions or task (or some other variable); and/or to measure how robust a model is comparatively to other models. We used this adversarial robustness metric on models of an MNIST, CIFAR-10, and a Fusion dataset (CIFAR-10 + MNIST) where trained models performed either a digit or object recognition task using a LeNet, ResNet50, or a fully connected network (FullyConnectedNet) architecture and found the following: 1) CIFAR-10 models are inherently less adversarially robust than MNIST models; 2) Both the image distribution and task that a model is trained on can affect the adversarial robustness of the resultant model. 3) Pretraining with a different image distribution and task sometimes carries over the adversarial robustness induced by that image distribution and task in the resultant model; Collectively, our results imply non-trivial differences of the learned representation space of one perceptual system over another given its exposure to different image statistics or tasks (mainly objects vs digits). Moreover, these results hold even when model systems are equalized to have the same level of performance, or when exposed to approximately matched image statistics of fusion images but with different tasks.

</p>
</details>

<details><summary><b>Risk Prediction on Traffic Accidents using a Compact Neural Model for Multimodal Information Fusion over Urban Big Data</b>
<a href="https://arxiv.org/abs/2103.05107">arxiv:2103.05107</a>
&#x1F4C8; 2 <br>
<p>Wenshan Wang, Su Yang, Weishan Zhang</p></summary>
<p>

**Abstract:** Predicting risk map of traffic accidents is vital for accident prevention and early planning of emergency response. Here, the challenge lies in the multimodal nature of urban big data. We propose a compact neural ensemble model to alleviate overfitting in fusing multimodal features and develop some new features such as fractal measure of road complexity in satellite images, taxi flows, POIs, and road width and connectivity in OpenStreetMap. The solution is more promising in performance than the baseline methods and the single-modality data based solutions. After visualization from a micro view, the visual patterns of the scenes related to high and low risk are revealed, providing lessons for future road design. From city point of view, the predicted risk map is close to the ground truth, and can act as the base in optimizing spatial configuration of resources for emergency response, and alarming signs. To the best of our knowledge, it is the first work to fuse visual and spatio-temporal features in traffic accident prediction while advances to bridge the gap between data mining based urban computing and computer vision based urban perception.

</p>
</details>

<details><summary><b>Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes</b>
<a href="https://arxiv.org/abs/2102.12894">arxiv:2102.12894</a>
&#x1F4C8; 2 <br>
<p>Sara Sangalli, Ertunc Erdil, Andreas Hoetker, Olivio Donati, Ender Konukoglu</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are notorious for making more mistakes for the classes that have substantially fewer samples than the others during training. Such class imbalance is ubiquitous in clinical applications and very crucial to handle because the classes with fewer samples most often correspond to critical cases (e.g., cancer) where misclassifications can have severe consequences. Not to miss such cases, binary classifiers need to be operated at high True Positive Rates (TPRs) by setting a higher threshold, but this comes at the cost of very high False Positive Rates (FPRs) for problems with class imbalance. Existing methods for learning under class imbalance most often do not take this into account. We argue that prediction accuracy should be improved by emphasizing reducing FPRs at high TPRs for problems where misclassification of the positive, i.e. critical, class samples are associated with higher cost. To this end, we pose the training of a DNN for binary classification as a constrained optimization problem and introduce a novel constraint that can be used with existing loss functions to enforce maximal area under the ROC curve (AUC) through prioritizing FPR reduction at high TPR. We solve the resulting constrained optimization problem using an Augmented Lagrangian method (ALM). Going beyond binary, we also propose two possible extensions of the proposed constraint for multi-class classification problems. We present experimental results for image-based binary and multi-class classification applications using an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results demonstrate that the proposed method improves the baselines in majority of the cases by attaining higher accuracy on critical classes while reducing the misclassification rate for the non-critical class samples.

</p>
</details>

<details><summary><b>Dynamic Graph Modeling of Simultaneous EEG and Eye-tracking Data for Reading Task Identification</b>
<a href="https://arxiv.org/abs/2102.11922">arxiv:2102.11922</a>
&#x1F4C8; 2 <br>
<p>Puneet Mathur, Trisha Mittal, Dinesh Manocha</p></summary>
<p>

**Abstract:** We present a new approach, that we call AdaGTCN, for identifying human reader intent from Electroencephalogram~(EEG) and Eye movement~(EM) data in order to help differentiate between normal reading and task-oriented reading. Understanding the physiological aspects of the reading process~(the cognitive load and the reading intent) can help improve the quality of crowd-sourced annotated data. Our method, Adaptive Graph Temporal Convolution Network (AdaGTCN), uses an Adaptive Graph Learning Layer and Deep Neighborhood Graph Convolution Layer for identifying the reading activities using time-locked EEG sequences recorded during word-level eye-movement fixations. Adaptive Graph Learning Layer dynamically learns the spatial correlations between the EEG electrode signals while the Deep Neighborhood Graph Convolution Layer exploits temporal features from a dense graph neighborhood to establish the state of the art in reading task identification over other contemporary approaches. We compare our approach with several baselines to report an improvement of 6.29% on the ZuCo 2.0 dataset, along with extensive ablation experiments

</p>
</details>

<details><summary><b>Slowly Varying Regression under Sparsity</b>
<a href="https://arxiv.org/abs/2102.10773">arxiv:2102.10773</a>
&#x1F4C8; 2 <br>
<p>Dimitris Bertsimas, Vassilis Digalakis Jr, Michael Linghzi Li, Omar Skali Lami</p></summary>
<p>

**Abstract:** We consider the problem of parameter estimation in slowly varying regression models with sparsity constraints. We formulate the problem as a mixed integer optimization problem and demonstrate that it can be reformulated exactly as a binary convex optimization problem through a novel exact relaxation. The relaxation utilizes a new equality on Moore-Penrose inverses that convexifies the non-convex objective function while coinciding with the original objective on all feasible binary points. This allows us to solve the problem significantly more efficiently and to provable optimality using a cutting plane-type algorithm. We develop a highly optimized implementation of such algorithm, which substantially improves upon the asymptotic computational complexity of a straightforward implementation. We further develop a heuristic method that is guaranteed to produce a feasible solution and, as we empirically illustrate, generates high quality warm-start solutions for the binary optimization problem. We show, on both synthetic and real-world datasets, that the resulting algorithm outperforms competing formulations in comparable times across a variety of metrics including out-of-sample predictive performance, support recovery accuracy, and false positive rate. The algorithm enables us to train models with 10,000s of parameters, is robust to noise, and able to effectively capture the underlying slowly changing support of the data generating process.

</p>
</details>

<details><summary><b>Divide-and-conquer methods for big data analysis</b>
<a href="https://arxiv.org/abs/2102.10771">arxiv:2102.10771</a>
&#x1F4C8; 2 <br>
<p>Xueying Chen, Jerry Q. Cheng, Min-ge Xie</p></summary>
<p>

**Abstract:** In the context of big data analysis, the divide-and-conquer methodology refers to a multiple-step process: first splitting a data set into several smaller ones; then analyzing each set separately; finally combining results from each analysis together. This approach is effective in handling large data sets that are unsuitable to be analyzed entirely by a single computer due to limits either from memory storage or computational time. The combined results will provide a statistical inference which is similar to the one from analyzing the entire data set. This article reviews some recently developments of divide-and-conquer methods in a variety of settings, including combining based on parametric, semiparametric and nonparametric models, online sequential updating methods, among others. Theoretical development on the efficiency of the divide-and-conquer methods is also discussed.

</p>
</details>

<details><summary><b>Coping with Mistreatment in Fair Algorithms</b>
<a href="https://arxiv.org/abs/2102.10750">arxiv:2102.10750</a>
&#x1F4C8; 2 <br>
<p>Ankit Kulshrestha, Ilya Safro</p></summary>
<p>

**Abstract:** Machine learning actively impacts our everyday life in almost all endeavors and domains such as healthcare, finance, and energy. As our dependence on the machine learning increases, it is inevitable that these algorithms will be used to make decisions that will have a direct impact on the society spanning all resolutions from personal choices to world-wide policies. Hence, it is crucial to ensure that (un)intentional bias does not affect the machine learning algorithms especially when they are required to take decisions that may have unintended consequences. Algorithmic fairness techniques have found traction in the machine learning community and many methods and metrics have been proposed to ensure and evaluate fairness in algorithms and data collection.
  In this paper, we study the algorithmic fairness in a supervised learning setting and examine the effect of optimizing a classifier for the Equal Opportunity metric. We demonstrate that such a classifier has an increased false positive rate across sensitive groups and propose a conceptually simple method to mitigate this bias. We rigorously analyze the proposed method and evaluate it on several real world datasets demonstrating its efficacy.

</p>
</details>

<details><summary><b>MetaDelta: A Meta-Learning System for Few-shot Image Classification</b>
<a href="https://arxiv.org/abs/2102.10744">arxiv:2102.10744</a>
&#x1F4C8; 2 <br>
<p>Yudong Chen, Chaoyu Guan, Zhikun Wei, Xin Wang, Wenwu Zhu</p></summary>
<p>

**Abstract:** Meta-learning aims at learning quickly on novel tasks with limited data by transferring generic experience learned from previous tasks. Naturally, few-shot learning has been one of the most popular applications for meta-learning. However, existing meta-learning algorithms rarely consider the time and resource efficiency or the generalization capacity for unknown datasets, which limits their applicability in real-world scenarios. In this paper, we propose MetaDelta, a novel practical meta-learning system for the few-shot image classification. MetaDelta consists of two core components: i) multiple meta-learners supervised by a central controller to ensure efficiency, and ii) a meta-ensemble module in charge of integrated inference and better generalization. In particular, each meta-learner in MetaDelta is composed of a unique pretrained encoder fine-tuned by batch training and parameter-free decoder used for prediction. MetaDelta ranks first in the final phase in the AAAI 2021 MetaDL Challenge\footnote{https://competitions.codalab.org/competitions/26638}, demonstrating the advantages of our proposed system. The codes are publicly available at https://github.com/Frozenmad/MetaDelta.

</p>
</details>

<details><summary><b>Communication Efficient Parallel Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.10740">arxiv:2102.10740</a>
&#x1F4C8; 2 <br>
<p>Mridul Agarwal, Bhargav Ganguly, Vaneet Aggarwal</p></summary>
<p>

**Abstract:** We consider the problem where $M$ agents interact with $M$ identical and independent environments with $S$ states and $A$ actions using reinforcement learning for $T$ rounds. The agents share their data with a central server to minimize their regret. We aim to find an algorithm that allows the agents to minimize the regret with infrequent communication rounds. We provide \NAM\ which runs at each agent and prove that the total cumulative regret of $M$ agents is upper bounded as $\Tilde{O}(DS\sqrt{MAT})$ for a Markov Decision Process with diameter $D$, number of states $S$, and number of actions $A$. The agents synchronize after their visitations to any state-action pair exceeds a certain threshold. Using this, we obtain a bound of $O\left(MSA\log(MT)\right)$ on the total number of communications rounds. Finally, we evaluate the algorithm against multiple environments and demonstrate that the proposed algorithm performs at par with an always communication version of the UCRL2 algorithm, while with significantly lower communication.

</p>
</details>

<details><summary><b>Tractable Computation of Expected Kernels</b>
<a href="https://arxiv.org/abs/2102.10562">arxiv:2102.10562</a>
&#x1F4C8; 2 <br>
<p>Wenzhe Li, Zhe Zeng, Antonio Vergari, Guy Van den Broeck</p></summary>
<p>

**Abstract:** Computing the expectation of kernel functions is a ubiquitous task in machine learning, with applications from classical support vector machines to exploiting kernel embeddings of distributions in probabilistic modeling, statistical inference, causal discovery, and deep learning. In all these scenarios, we tend to resort to Monte Carlo estimates as expectations of kernels are intractable in general. In this work, we characterize the conditions under which we can compute expected kernels exactly and efficiently, by leveraging recent advances in probabilistic circuit representations. We first construct a circuit representation for kernels and propose an approach to such tractable computation. We then demonstrate possible advancements for kernel embedding frameworks by exploiting tractable expected kernels to derive new algorithms for two challenging scenarios: 1) reasoning under missing data with kernel support vector regressors; 2) devising a collapsed black-box importance sampling scheme. Finally, we empirically evaluate both algorithms and show that they outperform standard baselines on a variety of datasets.

</p>
</details>

<details><summary><b>A Comprehensive Review of Computer-aided Whole-slide Image Analysis: from Datasets to Feature Extraction, Segmentation, Classification, and Detection Approaches</b>
<a href="https://arxiv.org/abs/2102.10553">arxiv:2102.10553</a>
&#x1F4C8; 2 <br>
<p>Chen Li, Xintong Li, Md Rahaman, Xiaoyan Li, Hongzan Sun, Hong Zhang, Yong Zhang, Xiaoqi Li, Jian Wu, Yudong Yao, Marcin Grzegorzek</p></summary>
<p>

**Abstract:** With the development of computer-aided diagnosis (CAD) and image scanning technology, Whole-slide Image (WSI) scanners are widely used in the field of pathological diagnosis. Therefore, WSI analysis has become the key to modern digital pathology. Since 2004, WSI has been used more and more in CAD. Since machine vision methods are usually based on semi-automatic or fully automatic computers, they are highly efficient and labor-saving. The combination of WSI and CAD technologies for segmentation, classification, and detection helps histopathologists obtain more stable and quantitative analysis results, save labor costs and improve diagnosis objectivity. This paper reviews the methods of WSI analysis based on machine learning. Firstly, the development status of WSI and CAD methods are introduced. Secondly, we discuss publicly available WSI datasets and evaluation metrics for segmentation, classification, and detection tasks. Then, the latest development of machine learning in WSI segmentation, classification, and detection are reviewed continuously. Finally, the existing methods are studied, the applicabilities of the analysis methods are analyzed, and the application prospects of the analysis methods in this field are forecasted.

</p>
</details>

<details><summary><b>Mastering Terra Mystica: Applying Self-Play to Multi-agent Cooperative Board Games</b>
<a href="https://arxiv.org/abs/2102.10540">arxiv:2102.10540</a>
&#x1F4C8; 2 <br>
<p>Luis Perez</p></summary>
<p>

**Abstract:** In this paper, we explore and compare multiple algorithms for solving the complex strategy game of Terra Mystica, hereafter abbreviated as TM. Previous work in the area of super-human game-play using AI has proven effective, with recent break-through for generic algorithms in games such as Go, Chess, and Shogi \cite{AlphaZero}. We directly apply these breakthroughs to a novel state-representation of TM with the goal of creating an AI that will rival human players. Specifically, we present the initial results of applying AlphaZero to this state-representation and analyze the strategies developed. A brief analysis is presented. We call this modified algorithm with our novel state-representation AlphaTM. In the end, we discuss the success and shortcomings of this method by comparing against multiple baselines and typical human scores. All code used for this paper is available at on \href{https://github.com/kandluis/terrazero}{GitHub}.

</p>
</details>

<details><summary><b>Some Network Optimization Models under Diverse Uncertain Environments</b>
<a href="https://arxiv.org/abs/2103.08327">arxiv:2103.08327</a>
&#x1F4C8; 1 <br>
<p>Saibal Majumder</p></summary>
<p>

**Abstract:** Network models provide an efficient way to represent many real life problems mathematically. In the last few decades, the field of network optimization has witnessed an upsurge of interest among researchers and practitioners. The network models considered in this thesis are broadly classified into four types including transportation problem, shortest path problem, minimum spanning tree problem and maximum flow problem. Quite often, we come across situations, when the decision parameters of network optimization problems are not precise and characterized by various forms of uncertainties arising from the factors, like insufficient or incomplete data, lack of evidence, inappropriate judgements and randomness. Considering the deterministic environment, there exist several studies on network optimization problems. However, in the literature, not many investigations on single and multi objective network optimization problems are observed under diverse uncertain frameworks. This thesis proposes seven different network models under different uncertain paradigms. Here, the uncertain programming techniques used to formulate the uncertain network models are (i) expected value model, (ii) chance constrained model and (iii) dependent chance constrained model. Subsequently, the corresponding crisp equivalents of the uncertain network models are solved using different solution methodologies. The solution methodologies used in this thesis can be broadly categorized as classical methods and evolutionary algorithms. The classical methods, used in this thesis, are Dijkstra and Kruskal algorithms, modified rough Dijkstra algorithm, global criterion method, epsilon constraint method and fuzzy programming method. Whereas, among the evolutionary algorithms, we have proposed the varying population genetic algorithm with indeterminate crossover and considered two multi objective evolutionary algorithms.

</p>
</details>

<details><summary><b>IoT-Enabled Social Relationships Meet Artificial Social Intelligence</b>
<a href="https://arxiv.org/abs/2103.01776">arxiv:2103.01776</a>
&#x1F4C8; 1 <br>
<p>Sahraoui Dhelim, Huansheng Ning, Fadi Farha, Liming Chen, Luigi Atzori, Mahmoud Daneshmand</p></summary>
<p>

**Abstract:** With the recent advances of the Internet of Things, and the increasing accessibility of ubiquitous computing resources and mobile devices, the prevalence of rich media contents, and the ensuing social, economic, and cultural changes, computing technology and applications have evolved quickly over the past decade. They now go beyond personal computing, facilitating collaboration and social interactions in general, causing a quick proliferation of social relationships among IoT entities. The increasing number of these relationships and their heterogeneous social features have led to computing and communication bottlenecks that prevent the IoT network from taking advantage of these relationships to improve the offered services and customize the delivered content, known as relationship explosion. On the other hand, the quick advances in artificial intelligence applications in social computing have led to the emerging of a promising research field known as Artificial Social Intelligence (ASI) that has the potential to tackle the social relationship explosion problem. This paper discusses the role of IoT in social relationships detection and management, the problem of social relationships explosion in IoT and reviews the proposed solutions using ASI, including social-oriented machine-learning and deep-learning techniques.

</p>
</details>

<details><summary><b>MobILE: Model-Based Imitation Learning From Observation Alone</b>
<a href="https://arxiv.org/abs/2102.10769">arxiv:2102.10769</a>
&#x1F4C8; 1 <br>
<p>Rahul Kidambi, Jonathan Chang, Wen Sun</p></summary>
<p>

**Abstract:** This paper studies Imitation Learning from Observations alone (ILFO) where the learner is presented with expert demonstrations that consist only of states visited by an expert (without access to actions taken by the expert). We present a provably efficient model-based framework MobILE to solve the ILFO problem. MobILE involves carefully trading off strategic exploration against imitation - this is achieved by integrating the idea of optimism in the face of uncertainty into the distribution matching imitation learning (IL) framework. We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys strong performance guarantees for classes of MDP dynamics that satisfy certain well studied notions of structural complexity. We also show that the ILFO problem is strictly harder than the standard IL problem by presenting an exponential sample complexity separation between IL and ILFO. We complement these theoretical results with experimental simulations on benchmark OpenAI Gym tasks that indicate the efficacy of MobILE.

</p>
</details>

<details><summary><b>Dither computing: a hybrid deterministic-stochastic computing framework</b>
<a href="https://arxiv.org/abs/2102.10732">arxiv:2102.10732</a>
&#x1F4C8; 1 <br>
<p>Chai Wah Wu</p></summary>
<p>

**Abstract:** Stochastic computing has a long history as an alternative method of performing arithmetic on a computer. While it can be considered an unbiased estimator of real numbers, it has a variance and MSE on the order of $Ω(\frac{1}{N})$. On the other hand, deterministic variants of stochastic computing remove the stochastic aspect, but cannot approximate arbitrary real numbers with arbitrary precision and are biased estimators. However, they have an asymptotically superior MSE on the order of $O(\frac{1}{N^2})$. Recent results in deep learning with stochastic rounding suggest that the bias in the rounding can degrade performance. We proposed an alternative framework, called dither computing, that combines aspects of stochastic computing and its deterministic variants and that can perform computing with similar efficiency, is unbiased, and with a variance and MSE also on the optimal order of $Θ(\frac{1}{N^2})$. We also show that it can be beneficial in stochastic rounding applications as well. We provide implementation details and give experimental results to comparatively show the benefits of the proposed scheme.

</p>
</details>

<details><summary><b>AI-Augmented Behavior Analysis for Children with Developmental Disabilities: Building Towards Precision Treatment</b>
<a href="https://arxiv.org/abs/2102.10635">arxiv:2102.10635</a>
&#x1F4C8; 1 <br>
<p>Shadi Ghafghazi, Amarie Carnett, Leslie Neely, Arun Das, Paul Rad</p></summary>
<p>

**Abstract:** Autism spectrum disorder is a developmental disorder characterized by significant social, communication, and behavioral challenges. Individuals diagnosed with autism, intellectual, and developmental disabilities (AUIDD) typically require long-term care and targeted treatment and teaching. Effective treatment of AUIDD relies on efficient and careful behavioral observations done by trained applied behavioral analysts (ABAs). However, this process overburdens ABAs by requiring the clinicians to collect and analyze data, identify the problem behaviors, conduct pattern analysis to categorize and predict categorical outcomes, hypothesize responsiveness to treatments, and detect the effects of treatment plans. Successful integration of digital technologies into clinical decision-making pipelines and the advancements in automated decision-making using Artificial Intelligence (AI) algorithms highlights the importance of augmenting teaching and treatments using novel algorithms and high-fidelity sensors. In this article, we present an AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to provide personalized treatment and learning plans to AUIDD individuals. By defining systematic experiments along with automated data collection and analysis, AI-ABA can promote self-regulative behavior using reinforcement-based augmented or virtual reality and other mobile platforms. Thus, AI-ABA could assist clinicians to focus on making precise data-driven decisions and increase the quality of individualized interventions for individuals with AUIDD.

</p>
</details>

<details><summary><b>Relative Expressiveness of Defeasible Logics II</b>
<a href="https://arxiv.org/abs/2102.10532">arxiv:2102.10532</a>
&#x1F4C8; 1 <br>
<p>Michael J. Maher</p></summary>
<p>

**Abstract:** (Maher 2012) introduced an approach for relative expressiveness of defeasible logics, and two notions of relative expressiveness were investigated. Using the first of these definitions of relative expressiveness, we show that all the defeasible logics in the DL framework are equally expressive under this formulation of relative expressiveness. The second formulation of relative expressiveness is stronger than the first. However, we show that logics incorporating individual defeat are equally expressive as the corresponding logics with team defeat. Thus the only differences in expressiveness of logics in DL arise from differences in how ambiguity is handled. This completes the study of relative expressiveness in DL begun in \cite{Maher12}.

</p>
</details>

<details><summary><b>CheckSoft : A Scalable Event-Driven Software Architecture for Keeping Track of People and Things in People-Centric Spaces</b>
<a href="https://arxiv.org/abs/2102.10513">arxiv:2102.10513</a>
&#x1F4C8; 1 <br>
<p>Rohan Sarkar, Avinash C. Kak</p></summary>
<p>

**Abstract:** We present CheckSoft, a scalable event-driven software architecture for keeping track of people-object interactions in people-centric applications such as airport checkpoint security areas, automated retail stores, smart libraries, and so on. The architecture works off the video data generated in real time by a network of surveillance cameras. Although there are many different aspects to automating these applications, the most difficult part of the overall problem is keeping track of the interactions between the people and the objects. CheckSoft uses finite-state-machine (FSM) based logic for keeping track of such interactions which allows the system to quickly reject any false detections of the interactions by the video cameras. CheckSoft is easily scalable since the architecture is based on multi-processing in which a separate process is assigned to each human and to each "storage container" for the objects. A storage container may be a shelf on which the objects are displayed or a bin in which the objects are stored, depending on the specific application in which CheckSoft is deployed.

</p>
</details>

<details><summary><b>CSIT-Free Model Aggregation for Federated Edge Learning via Reconfigurable Intelligent Surface</b>
<a href="https://arxiv.org/abs/2102.10749">arxiv:2102.10749</a>
&#x1F4C8; 0 <br>
<p>Hang Liu, Xiaojun Yuan, Ying-Jun Angela Zhang</p></summary>
<p>

**Abstract:** We study over-the-air model aggregation in federated edge learning (FEEL) systems, where channel state information at the transmitters (CSIT) is assumed to be unavailable. We leverage the reconfigurable intelligent surface (RIS) technology to align the cascaded channel coefficients for CSIT-free model aggregation. To this end, we jointly optimize the RIS and the receiver by minimizing the aggregation error under the channel alignment constraint. We then develop a difference-of-convex algorithm for the resulting non-convex optimization. Numerical experiments on image classification show that the proposed method is able to achieve a similar learning accuracy as the state-of-the-art CSIT-based solution, demonstrating the efficiency of our approach in combating the lack of CSIT.

</p>
</details>

<details><summary><b>Inconsistency thresholds for incomplete pairwise comparison matrices</b>
<a href="https://arxiv.org/abs/2102.10558">arxiv:2102.10558</a>
&#x1F4C8; 0 <br>
<p>Kolos Csaba Ágoston, László Csató</p></summary>
<p>

**Abstract:** Pairwise comparison matrices are increasingly used in settings where some pairs are missing. However, there exist few inconsistency indices for similar incomplete data sets and no reasonable measure has an associated threshold. This paper generalises the famous rule of thumb for the acceptable level of inconsistency, proposed by Saaty, to incomplete pairwise comparison matrices. The extension is based on choosing the missing elements such that the maximal eigenvalue of the incomplete matrix is minimised. Consequently, the well-established values of the random index cannot be adopted: the inconsistency of random matrices is found to be the function of matrix size and the number of missing elements, with a nearly linear dependence in the case of the latter variable. Our results can be directly built into decision-making software and used by practitioners as a statistical criterion for accepting or rejecting an incomplete pairwise comparison matrix.

</p>
</details>

<details><summary><b>Semi-supervised learning combining backpropagation and STDP: STDP enhances learning by backpropagation with a small amount of labeled data in a spiking neural network</b>
<a href="https://arxiv.org/abs/2102.10530">arxiv:2102.10530</a>
&#x1F4C8; 0 <br>
<p>Kotaro Furuya, Jun Ohkubo</p></summary>
<p>

**Abstract:** A semi-supervised learning method for spiking neural networks is proposed. The proposed method consists of supervised learning by backpropagation and subsequent unsupervised learning by spike-timing-dependent plasticity (STDP), which is a biologically plausible learning rule. Numerical experiments show that the proposed method improves the accuracy without additional labeling when a small amount of labeled data is used. This feature has not been achieved by existing semi-supervised learning methods of discriminative models. It is possible to implement the proposed learning method for event-driven systems. Hence, it would be highly efficient in real-time problems if it were implemented on neuromorphic hardware. The results suggest that STDP plays an important role other than self-organization when applied after supervised learning, which differs from the previous method of using STDP as pre-training interpreted as self-organization.

</p>
</details>


{% endraw %}
Prev: [2021.02.20]({{ '/2021/02/20/2021.02.20.html' | relative_url }})  Next: [2021.02.22]({{ '/2021/02/22/2021.02.22.html' | relative_url }})