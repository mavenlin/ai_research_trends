## Summary for 2021-04-29, created on 2021-12-22


<details><summary><b>What Are Bayesian Neural Network Posteriors Really Like?</b>
<a href="https://arxiv.org/abs/2104.14421">arxiv:2104.14421</a>
&#x1F4C8; 253 <br>
<p>Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, Andrew Gordon Wilson</p></summary>
<p>

**Abstract:** The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a "cold posterior" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.

</p>
</details>

<details><summary><b>Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation</b>
<a href="https://arxiv.org/abs/2104.14478">arxiv:2104.14478</a>
&#x1F4C8; 63 <br>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, Wolfgang Macherey</p></summary>
<p>

**Abstract:** Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.

</p>
</details>

<details><summary><b>A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</b>
<a href="https://arxiv.org/abs/2104.14558">arxiv:2104.14558</a>
&#x1F4C8; 45 <br>
<p>Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He</p></summary>
<p>

**Abstract:** We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code is made available at https://github.com/facebookresearch/SlowFast

</p>
</details>

<details><summary><b>Ensembling with Deep Generative Views</b>
<a href="https://arxiv.org/abs/2104.14551">arxiv:2104.14551</a>
&#x1F4C8; 43 <br>
<p>Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang</p></summary>
<p>

**Abstract:** Recent generative models can synthesize "views" of artificial images that mimic real-world variations, such as changes in color or pose, simply by learning from unlabeled image collections. Here, we investigate whether such views can be applied to real images to benefit downstream analysis tasks such as image classification. Using a pretrained generator, we first find the latent code corresponding to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classification tasks involving facial attributes, cat faces, and cars. Critically, we find that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmentations and original image, and training the classifier on synthesized images can all impact the result. Currently, we find that while test-time ensembling with GAN-based augmentations can offer some small improvements, the remaining bottlenecks are the efficiency and accuracy of the GAN reconstructions, coupled with classifier sensitivities to artifacts in GAN-generated images.

</p>
</details>

<details><summary><b>Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing</b>
<a href="https://arxiv.org/abs/2104.14754">arxiv:2104.14754</a>
&#x1F4C8; 38 <br>
<p>Hyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, Youngjung Uh</p></summary>
<p>

**Abstract:** Generative adversarial networks (GANs) synthesize realistic images from random latent vectors. Although manipulating the latent vectors controls the synthesized outputs, editing real images with GANs suffers from i) time-consuming optimization for projecting real images to the latent vectors, ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the intermediate latent space has spatial dimensions, and a spatially variant modulation replaces AdaIN. It makes the embedding through an encoder more accurate than existing optimization-based methods while maintaining the properties of GANs. Experimental results demonstrate that our method significantly outperforms state-of-the-art models in various image manipulation tasks such as local editing and image interpolation. Last but not least, conventional editing methods on GANs are still valid on our StyleMapGAN. Source code is available at https://github.com/naver-ai/StyleMapGAN.

</p>
</details>

<details><summary><b>Constructions in combinatorics via neural networks</b>
<a href="https://arxiv.org/abs/2104.14516">arxiv:2104.14516</a>
&#x1F4C8; 33 <br>
<p>Adam Zsolt Wagner</p></summary>
<p>

**Abstract:** We demonstrate how by using a reinforcement learning algorithm, the deep cross-entropy method, one can find explicit constructions and counterexamples to several open conjectures in extremal combinatorics and graph theory. Amongst the conjectures we refute are a question of Brualdi and Cao about maximizing permanents of pattern avoiding matrices, and several problems related to the adjacency and distance eigenvalues of graphs.

</p>
</details>

<details><summary><b>Entailment as Few-Shot Learner</b>
<a href="https://arxiv.org/abs/2104.14690">arxiv:2104.14690</a>
&#x1F4C8; 23 <br>
<p>Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma</p></summary>
<p>

**Abstract:** Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.

</p>
</details>

<details><summary><b>The Logic of Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2104.14624">arxiv:2104.14624</a>
&#x1F4C8; 23 <br>
<p>Martin Grohe</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) are deep learning architectures for machine learning problems on graphs. It has recently been shown that the expressiveness of GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman algorithms and by finite variable counting logics. The correspondence has even led to new, higher-order GNNs corresponding to the WL algorithm in higher dimensions.
  The purpose of this paper is to explain these descriptive characterisations of GNNs.

</p>
</details>

<details><summary><b>On the Emergence of Whole-body Strategies from Humanoid Robot Push-recovery Learning</b>
<a href="https://arxiv.org/abs/2104.14534">arxiv:2104.14534</a>
&#x1F4C8; 23 <br>
<p>Diego Ferigo, Raffaello Camoriano, Paolo Maria Viceconte, Daniele Calandriello, Silvio Traversaro, Lorenzo Rosasco, Daniele Pucci</p></summary>
<p>

**Abstract:** Balancing and push-recovery are essential capabilities enabling humanoid robots to solve complex locomotion tasks. In this context, classical control systems tend to be based on simplified physical models and hard-coded strategies. Although successful in specific scenarios, this approach requires demanding tuning of parameters and switching logic between specifically-designed controllers for handling more general perturbations. We apply model-free Deep Reinforcement Learning for training a general and robust humanoid push-recovery policy in a simulation environment. Our method targets high-dimensional whole-body humanoid control and is validated on the iCub humanoid. Reward components incorporating expert knowledge on humanoid control enable fast learning of several robust behaviors by the same policy, spanning the entire body. We validate our method with extensive quantitative analyses in simulation, including out-of-sample tasks which demonstrate policy robustness and generalization, both key requirements towards real-world robot deployment.

</p>
</details>

<details><summary><b>Cluster-driven Graph Federated Learning over Multiple Domains</b>
<a href="https://arxiv.org/abs/2104.14628">arxiv:2104.14628</a>
&#x1F4C8; 22 <br>
<p>Debora Caldarola, Massimiliano Mancini, Fabio Galasso, Marco Ciccone, Emanuele Rodolà, Barbara Caputo</p></summary>
<p>

**Abstract:** Federated Learning (FL) deals with learning a central model (i.e. the server) in privacy-constrained scenarios, where data are stored on multiple devices (i.e. the clients). The central model has no direct access to the data, but only to the updates of the parameters computed locally by each client. This raises a problem, known as statistical heterogeneity, because the clients may have different data distributions (i.e. domains). This is only partly alleviated by clustering the clients. Clustering may reduce heterogeneity by identifying the domains, but it deprives each cluster model of the data and supervision of others. Here we propose a novel Cluster-driven Graph Federated Learning (FedCG). In FedCG, clustering serves to address statistical heterogeneity, while Graph Convolutional Networks (GCNs) enable sharing knowledge across them. FedCG: i) identifies the domains via an FL-compliant clustering and instantiates domain-specific modules (residual branches) for each domain; ii) connects the domain-specific modules through a GCN at training to learn the interactions among domains and share knowledge; and iii) learns to cluster unsupervised via teacher-student classifier-training iterations and to address novel unseen test domains via their domain soft-assignment scores. Thanks to the unique interplay of GCN over clusters, FedCG achieves the state-of-the-art on multiple FL benchmarks.

</p>
</details>

<details><summary><b>Photonic co-processors in HPC: using LightOn OPUs for Randomized Numerical Linear Algebra</b>
<a href="https://arxiv.org/abs/2104.14429">arxiv:2104.14429</a>
&#x1F4C8; 13 <br>
<p>Daniel Hesslow, Alessandro Cappelli, Igor Carron, Laurent Daudet, Raphaël Lafargue, Kilian Müller, Ruben Ohana, Gustave Pariente, Iacopo Poli</p></summary>
<p>

**Abstract:** Randomized Numerical Linear Algebra (RandNLA) is a powerful class of methods, widely used in High Performance Computing (HPC). RandNLA provides approximate solutions to linear algebra functions applied to large signals, at reduced computational costs. However, the randomization step for dimensionality reduction may itself become the computational bottleneck on traditional hardware. Leveraging near constant-time linear random projections delivered by LightOn Optical Processing Units we show that randomization can be significantly accelerated, at negligible precision loss, in a wide range of important RandNLA algorithms, such as RandSVD or trace estimators.

</p>
</details>

<details><summary><b>A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection</b>
<a href="https://arxiv.org/abs/2104.14535">arxiv:2104.14535</a>
&#x1F4C8; 11 <br>
<p>Shelly Sheynin, Sagie Benaim, Lior Wolf</p></summary>
<p>

**Abstract:** Anomaly detection, the task of identifying unusual samples in data, often relies on a large set of training samples. In this work, we consider the setting of few-shot anomaly detection in images, where only a few images are given at training. We devise a hierarchical generative model that captures the multi-scale patch distribution of each training image. We further enhance the representation of our model by using image transformations and optimize scale-specific patch-discriminators to distinguish between real and fake patches of the image, as well as between different transformations applied to those patches. The anomaly score is obtained by aggregating the patch-based votes of the correct transformation across scales and image regions. We demonstrate the superiority of our method on both the one-shot and few-shot settings, on the datasets of Paris, CIFAR10, MNIST and FashionMNIST as well as in the setting of defect detection on MVTec. In all cases, our method outperforms the recent baseline methods.

</p>
</details>

<details><summary><b>Optimal training of variational quantum algorithms without barren plateaus</b>
<a href="https://arxiv.org/abs/2104.14543">arxiv:2104.14543</a>
&#x1F4C8; 10 <br>
<p>Tobias Haug, M. S. Kim</p></summary>
<p>

**Abstract:** Variational quantum algorithms (VQAs) promise efficient use of near-term quantum computers. However, training VQAs often requires an extensive amount of time and suffers from the barren plateau problem where the magnitude of the gradients vanishes with increasing number of qubits. Here, we show how to optimally train VQAs for learning quantum states. Parameterized quantum circuits can form Gaussian kernels, which we use to derive adaptive learning rates for gradient ascent. We introduce the generalized quantum natural gradient that features stability and optimized movement in parameter space. Both methods together outperform other optimization routines in training VQAs. Our methods also excel at numerically optimizing driving protocols for quantum control problems. The gradients of the VQA do not vanish when the fidelity between the initial state and the state to be learned is bounded from below. We identify a VQA for quantum simulation with such a constraint that thus can be trained free of barren plateaus. Finally, we propose the application of Gaussian kernels for quantum machine learning.

</p>
</details>

<details><summary><b>A neural anisotropic view of underspecification in deep learning</b>
<a href="https://arxiv.org/abs/2104.14372">arxiv:2104.14372</a>
&#x1F4C8; 9 <br>
<p>Guillermo Ortiz-Jimenez, Itamar Franco Salazar-Reque, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard</p></summary>
<p>

**Abstract:** The underspecification of most machine learning pipelines means that we cannot rely solely on validation performance to assess the robustness of deep learning systems to naturally occurring distribution shifts. Instead, making sure that a neural network can generalize across a large number of different situations requires to understand the specific way in which it solves a task. In this work, we propose to study this problem from a geometric perspective with the aim to understand two key characteristics of neural network solutions in underspecified settings: how is the geometry of the learned function related to the data representation? And, are deep networks always biased towards simpler solutions, as conjectured in recent literature? We show that the way neural networks handle the underspecification of these problems is highly dependent on the data representation, affecting both the geometry and the complexity of the learned predictors. Our results highlight that understanding the architectural inductive bias in deep learning is fundamental to address the fairness, robustness, and generalization of these systems.

</p>
</details>

<details><summary><b>ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</b>
<a href="https://arxiv.org/abs/2104.14129">arxiv:2104.14129</a>
&#x1F4C8; 7 <br>
<p>Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W. Mahoney, Joseph E. Gonzalez</p></summary>
<p>

**Abstract:** The increasing size of neural network models has been critical for improvements in their accuracy, but device memory is not growing at the same rate. This creates fundamental challenges for training neural networks within limited memory environments. In this work, we propose ActNN, a memory-efficient training framework that stores randomly quantized activations for back propagation. We prove the convergence of ActNN for general network architectures, and we characterize the impact of quantization on the convergence via an exact expression for the gradient variance. Using our theory, we propose novel mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers. These techniques can be readily applied to existing dynamic graph frameworks, such as PyTorch, simply by substituting the layers. We evaluate ActNN on mainstream computer vision models for classification, detection, and segmentation tasks. On all these tasks, ActNN compresses the activation to 2 bits on average, with negligible accuracy loss. ActNN reduces the memory footprint of the activation by 12x, and it enables training with a 6.6x to 14x larger batch size.

</p>
</details>

<details><summary><b>Studying the Consistency and Composability of Lottery Ticket Pruning Masks</b>
<a href="https://arxiv.org/abs/2104.14753">arxiv:2104.14753</a>
&#x1F4C8; 6 <br>
<p>Rajiv Movva, Jonathan Frankle, Michael Carbin</p></summary>
<p>

**Abstract:** Magnitude pruning is a common, effective technique to identify sparse subnetworks at little cost to accuracy. In this work, we ask whether a particular architecture's accuracy-sparsity tradeoff can be improved by combining pruning information across multiple runs of training. From a shared ResNet-20 initialization, we train several network copies (\emph{siblings}) to completion using different SGD data orders on CIFAR-10. While the siblings' pruning masks are naively not much more similar than chance, starting sibling training after a few epochs of shared pretraining significantly increases pruning overlap. We then choose a subnetwork by either (1) taking all weights that survive pruning in any sibling (mask union), or (2) taking only the weights that survive pruning across all siblings (mask intersection). The resulting subnetwork is retrained. Strikingly, we find that union and intersection masks perform very similarly. Both methods match the accuracy-sparsity tradeoffs of the one-shot magnitude pruning baseline, even when we combine masks from up to $k = 10$ siblings.

</p>
</details>

<details><summary><b>Pre-training of Deep RL Agents for Improved Learning under Domain Randomization</b>
<a href="https://arxiv.org/abs/2104.14386">arxiv:2104.14386</a>
&#x1F4C8; 6 <br>
<p>Artemij Amiranashvili, Max Argus, Lukas Hermann, Wolfram Burgard, Thomas Brox</p></summary>
<p>

**Abstract:** Visual domain randomization in simulated environments is a widely used method to transfer policies trained in simulation to real robots. However, domain randomization and augmentation hamper the training of a policy. As reinforcement learning struggles with a noisy training signal, this additional nuisance can drastically impede training. For difficult tasks it can even result in complete failure to learn. To overcome this problem we propose to pre-train a perception encoder that already provides an embedding invariant to the randomization. We demonstrate that this yields consistently improved results on a randomized version of DeepMind control suite tasks and a stacking environment on arbitrary backgrounds with zero-shot transfer to a physical robot.

</p>
</details>

<details><summary><b>Learning Heterogeneous Temporal Patterns of User Preference for Timely Recommendation</b>
<a href="https://arxiv.org/abs/2104.14200">arxiv:2104.14200</a>
&#x1F4C8; 6 <br>
<p>Junsu Cho, Dongmin Hyun, SeongKu Kang, Hwanjo Yu</p></summary>
<p>

**Abstract:** Recommender systems have achieved great success in modeling user's preferences on items and predicting the next item the user would consume. Recently, there have been many efforts to utilize time information of users' interactions with items to capture inherent temporal patterns of user behaviors and offer timely recommendations at a given time. Existing studies regard the time information as a single type of feature and focus on how to associate it with user preferences on items. However, we argue they are insufficient for fully learning the time information because the temporal patterns of user preference are usually heterogeneous. A user's preference for a particular item may 1) increase periodically or 2) evolve over time under the influence of significant recent events, and each of these two kinds of temporal pattern appears with some unique characteristics. In this paper, we first define the unique characteristics of the two kinds of temporal pattern of user preference that should be considered in time-aware recommender systems. Then we propose a novel recommender system for timely recommendations, called TimelyRec, which jointly learns the heterogeneous temporal patterns of user preference considering all of the defined characteristics. In TimelyRec, a cascade of two encoders captures the temporal patterns of user preference using a proposed attention module for each encoder. Moreover, we introduce an evaluation scenario that evaluates the performance on predicting an interesting item and when to recommend the item simultaneously in top-K recommendation (i.e., item-timing recommendation). Our extensive experiments on a scenario for item recommendation and the proposed scenario for item-timing recommendation on real-world datasets demonstrate the superiority of TimelyRec and the proposed attention modules.

</p>
</details>

<details><summary><b>Human strategic decision making in parametrized games</b>
<a href="https://arxiv.org/abs/2104.14744">arxiv:2104.14744</a>
&#x1F4C8; 5 <br>
<p>Sam Ganzfried</p></summary>
<p>

**Abstract:** Many real-world games contain parameters which can affect payoffs, action spaces, and information states. For fixed values of the parameters, the game can be solved using standard algorithms. However, in many settings agents must act without knowing the values of the parameters that will be encountered in advance. Often the decisions must be made by a human under time and resource constraints, and it is unrealistic to assume that a human can solve the game in real time. We present a new framework that enables human decision makers to make fast decisions without the aid of real-time solvers. We demonstrate applicability to a variety of situations including settings with multiple players and imperfect information.

</p>
</details>

<details><summary><b>Flattening Multiparameter Hierarchical Clustering Functors</b>
<a href="https://arxiv.org/abs/2104.14734">arxiv:2104.14734</a>
&#x1F4C8; 5 <br>
<p>Dan Shiebler</p></summary>
<p>

**Abstract:** We bring together topological data analysis, applied category theory, and machine learning to study multiparameter hierarchical clustering. We begin by introducing a procedure for flattening multiparameter hierarchical clusterings. We demonstrate that this procedure is a functor from a category of multiparameter hierarchical partitions to a category of binary integer programs. We also include empirical results demonstrating its effectiveness. Next, we introduce a Bayesian update algorithm for learning clustering parameters from data. We demonstrate that the composition of this algorithm with our flattening procedure satisfies a consistency property.

</p>
</details>

<details><summary><b>MongeNet: Efficient Sampler for Geometric Deep Learning</b>
<a href="https://arxiv.org/abs/2104.14554">arxiv:2104.14554</a>
&#x1F4C8; 5 <br>
<p>Léo Lebrat, Rodrigo Santa Cruz, Clinton Fookes, Olivier Salvado</p></summary>
<p>

**Abstract:** Recent advances in geometric deep-learning introduce complex computational challenges for evaluating the distance between meshes. From a mesh model, point clouds are necessary along with a robust distance metric to assess surface quality or as part of the loss function for training models. Current methods often rely on a uniform random mesh discretization, which yields irregular sampling and noisy distance estimation. In this paper we introduce MongeNet, a fast and optimal transport based sampler that allows for an accurate discretization of a mesh with better approximation properties. We compare our method to the ubiquitous random uniform sampling and show that the approximation error is almost half with a very small computational overhead.

</p>
</details>

<details><summary><b>PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments</b>
<a href="https://arxiv.org/abs/2104.14380">arxiv:2104.14380</a>
&#x1F4C8; 5 <br>
<p>Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, Nicolas Kourtellis</p></summary>
<p>

**Abstract:** We propose and implement a Privacy-preserving Federated Learning ($PPFL$) framework for mobile systems to limit privacy leakages in federated learning. Leveraging the widespread presence of Trusted Execution Environments (TEEs) in high-end and mobile devices, we utilize TEEs on clients for local training, and on servers for secure aggregation, so that model/gradient updates are hidden from adversaries. Challenged by the limited memory size of current TEEs, we leverage greedy layer-wise training to train each model's layer inside the trusted area until its convergence. The performance evaluation of our implementation shows that $PPFL$ can significantly improve privacy while incurring small system overheads at the client-side. In particular, $PPFL$ can successfully defend the trained model against data reconstruction, property inference, and membership inference attacks. Furthermore, it can achieve comparable model utility with fewer communication rounds (0.54$\times$) and a similar amount of network traffic (1.002$\times$) compared to the standard federated learning of a complete model. This is achieved while only introducing up to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in $PPFL$'s client-side.

</p>
</details>

<details><summary><b>Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaptation</b>
<a href="https://arxiv.org/abs/2104.14203">arxiv:2104.14203</a>
&#x1F4C8; 5 <br>
<p>Chen-Hao Chao, Bo-Wun Cheng, Chun-Yi Lee</p></summary>
<p>

**Abstract:** Recent researches on unsupervised domain adaptation (UDA) have demonstrated that end-to-end ensemble learning frameworks serve as a compelling option for UDA tasks. Nevertheless, these end-to-end ensemble learning methods often lack flexibility as any modification to the ensemble requires retraining of their frameworks. To address this problem, we propose a flexible ensemble-distillation framework for performing semantic segmentation based UDA, allowing any arbitrary composition of the members in the ensemble while still maintaining its superior performance. To achieve such flexibility, our framework is designed to be robust against the output inconsistency and the performance variation of the members within the ensemble. To examine the effectiveness and the robustness of our method, we perform an extensive set of experiments on both GTA5 to Cityscapes and SYNTHIA to Cityscapes benchmarks to quantitatively inspect the improvements achievable by our method. We further provide detailed analyses to validate that our design choices are practical and beneficial. The experimental evidence validates that the proposed method indeed offer superior performance, robustness and flexibility in semantic segmentation based UDA tasks against contemporary baseline methods.

</p>
</details>

<details><summary><b>A Refined Inertial DCA for DC Programming</b>
<a href="https://arxiv.org/abs/2104.14750">arxiv:2104.14750</a>
&#x1F4C8; 4 <br>
<p>Yu You, Yi-Shuai Niu</p></summary>
<p>

**Abstract:** We consider the difference-of-convex (DC) programming problems whose objective function is level-bounded. The classical DC algorithm (DCA) is well-known for solving this kind of problems, which returns a critical point. Recently, de Oliveira and Tcheo incorporated the inertial-force procedure into DCA (InDCA) for potential acceleration and preventing the algorithm from converging to a critical point which is not d(directional)-stationary. In this paper, based on InDCA, we propose two refined inertial DCA (RInDCA) with enlarged inertial step-sizes for better acceleration. We demonstrate the subsequential convergence of our refined versions to a critical point. In addition, by assuming the Kurdyka-Lojasiewicz (KL) property of the objective function, we establish the sequential convergence of RInDCA. Numerical simulations on image restoration problem show the benefit of enlarged step-size.

</p>
</details>

<details><summary><b>Eliminating Multicollinearity Issues in Neural Network Ensembles: Incremental, Negatively Correlated, Optimal Convex Blending</b>
<a href="https://arxiv.org/abs/2104.14715">arxiv:2104.14715</a>
&#x1F4C8; 4 <br>
<p>Pola Lydia Lagari, Lefteri H. Tsoukalas, Salar Safarkhani, Isaac E. Lagaris</p></summary>
<p>

**Abstract:** Given a {features, target} dataset, we introduce an incremental algorithm that constructs an aggregate regressor, using an ensemble of neural networks. It is well known that ensemble methods suffer from the multicollinearity issue, which is the manifestation of redundancy arising mainly due to the common training-dataset. In the present incremental approach, at each stage we optimally blend the aggregate regressor with a newly trained neural network under a convexity constraint which, if necessary, induces negative correlations. Under this framework, collinearity issues do not arise at all, rendering so the method both accurate and robust.

</p>
</details>

<details><summary><b>Soft Mode in the Dynamics of Over-realizable On-line Learning for Soft Committee Machines</b>
<a href="https://arxiv.org/abs/2104.14546">arxiv:2104.14546</a>
&#x1F4C8; 4 <br>
<p>Frederieke Richert, Roman Worschech, Bernd Rosenow</p></summary>
<p>

**Abstract:** Over-parametrized deep neural networks trained by stochastic gradient descent are successful in performing many tasks of practical relevance. One aspect of over-parametrization is the possibility that the student network has a larger expressivity than the data generating process. In the context of a student-teacher scenario, this corresponds to the so-called over-realizable case, where the student network has a larger number of hidden units than the teacher. For on-line learning of a two-layer soft committee machine in the over-realizable case, we find that the approach to perfect learning occurs in a power-law fashion rather than exponentially as in the realizable case. All student nodes learn and replicate one of the teacher nodes if teacher and student outputs are suitably rescaled.

</p>
</details>

<details><summary><b>Distributed Multigrid Neural Solvers on Megavoxel Domains</b>
<a href="https://arxiv.org/abs/2104.14538">arxiv:2104.14538</a>
&#x1F4C8; 4 <br>
<p>Aditya Balu, Sergio Botelho, Biswajit Khara, Vinay Rao, Chinmay Hegde, Soumik Sarkar, Santi Adavani, Adarsh Krishnamurthy, Baskar Ganapathysubramanian</p></summary>
<p>

**Abstract:** We consider the distributed training of large-scale neural networks that serve as PDE solvers producing full field outputs. We specifically consider neural solvers for the generalized 3D Poisson equation over megavoxel domains. A scalable framework is presented that integrates two distinct advances. First, we accelerate training a large model via a method analogous to the multigrid technique used in numerical linear algebra. Here, the network is trained using a hierarchy of increasing resolution inputs in sequence, analogous to the 'V', 'W', 'F', and 'Half-V' cycles used in multigrid approaches. In conjunction with the multi-grid approach, we implement a distributed deep learning framework which significantly reduces the time to solve. We show the scalability of this approach on both GPU (Azure VMs on Cloud) and CPU clusters (PSC Bridges2). This approach is deployed to train a generalized 3D Poisson solver that scales well to predict output full-field solutions up to the resolution of 512x512x512 for a high dimensional family of inputs.

</p>
</details>

<details><summary><b>Online certification of preference-based fairness for personalized recommender systems</b>
<a href="https://arxiv.org/abs/2104.14527">arxiv:2104.14527</a>
&#x1F4C8; 4 <br>
<p>Virginie Do, Sam Corbett-Davies, Jamal Atif, Nicolas Usunier</p></summary>
<p>

**Abstract:** Recommender systems are facing scrutiny because of their growing impact on the opportunities we have access to. Current audits for fairness are limited to coarse-grained parity assessments at the level of sensitive groups. We propose to audit for envy-freeness, a more granular criterion aligned with individual preferences: every user should prefer their recommendations to those of other users. Since auditing for envy requires to estimate the preferences of users beyond their existing recommendations, we cast the audit as a new pure exploration problem in multi-armed bandits. We propose a sample-efficient algorithm with theoretical guarantees that it does not deteriorate user experience. We also study the trade-offs achieved on real-world recommendation datasets.

</p>
</details>

<details><summary><b>ELF-VC: Efficient Learned Flexible-Rate Video Coding</b>
<a href="https://arxiv.org/abs/2104.14335">arxiv:2104.14335</a>
&#x1F4C8; 4 <br>
<p>Oren Rippel, Alexander G. Anderson, Kedar Tatwawadi, Sanjay Nair, Craig Lytle, Lubomir Bourdev</p></summary>
<p>

**Abstract:** While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our approach compares favorably across the entire R-D curve under metrics PSNR, MS-SSIM and VMAF against all mainstream video standards (H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at least 5x faster and has fewer parameters than all ML codecs which report these figures.
  Our contributions include a flexible-rate framework allowing a single model to cover a large and dense range of bitrates, at a negligible increase in computation and parameter count; an efficient backbone optimized for ML-based codecs; and a novel in-loop flow prediction scheme which leverages prior information towards more efficient compression.
  We benchmark our method, which we call ELF-VC (Efficient, Learned and Flexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics PSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate by 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the current best ML codec. At the same time, on an NVIDIA Titan V GPU our approach encodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18 FPS.

</p>
</details>

<details><summary><b>Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning</b>
<a href="https://arxiv.org/abs/2104.14210">arxiv:2104.14210</a>
&#x1F4C8; 4 <br>
<p>Indro Spinelli, Simone Scardapane, Amir Hussain, Aurelio Uncini</p></summary>
<p>

**Abstract:** Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure.

</p>
</details>

<details><summary><b>A Physics-Constrained Deep Learning Model for Simulating Multiphase Flow in 3D Heterogeneous Porous Media</b>
<a href="https://arxiv.org/abs/2105.09467">arxiv:2105.09467</a>
&#x1F4C8; 3 <br>
<p>Bicheng Yan, Dylan Robert Harp, Bailian Chen, Rajesh Pawar</p></summary>
<p>

**Abstract:** In this work, an efficient physics-constrained deep learning model is developed for solving multiphase flow in 3D heterogeneous porous media. The model fully leverages the spatial topology predictive capability of convolutional neural networks, and is coupled with an efficient continuity-based smoother to predict flow responses that need spatial continuity. Furthermore, the transient regions are penalized to steer the training process such that the model can accurately capture flow in these regions. The model takes inputs including properties of porous media, fluid properties and well controls, and predicts the temporal-spatial evolution of the state variables (pressure and saturation). While maintaining the continuity of fluid flow, the 3D spatial domain is decomposed into 2D images for reducing training cost, and the decomposition results in an increased number of training data samples and better training efficiency. Additionally, a surrogate model is separately constructed as a postprocessor to calculate well flow rate based on the predictions of state variables from the deep learning model. We use the example of CO2 injection into saline aquifers, and apply the physics-constrained deep learning model that is trained from physics-based simulation data and emulates the physics process. The model performs prediction with a speedup of ~1400 times compared to physics-based simulations, and the average temporal errors of predicted pressure and saturation plumes are 0.27% and 0.099% respectively. Furthermore, water production rate is efficiently predicted by a surrogate model for well flow rate, with a mean error less than 5%. Therefore, with its unique scheme to cope with the fidelity in fluid flow in porous media, the physics-constrained deep learning model can become an efficient predictive model for computationally demanding inverse problems or other coupled processes.

</p>
</details>

<details><summary><b>Correcting Classification: A Bayesian Framework Using Explanation Feedback to Improve Classification Abilities</b>
<a href="https://arxiv.org/abs/2105.02653">arxiv:2105.02653</a>
&#x1F4C8; 3 <br>
<p>Yanzhe Bekkemoen, Helge Langseth</p></summary>
<p>

**Abstract:** Neural networks (NNs) have shown high predictive performance, however, with shortcomings. Firstly, the reasons behind the classifications are not fully understood. Several explanation methods have been developed, but they do not provide mechanisms for users to interact with the explanations. Explanations are social, meaning they are a transfer of knowledge through interactions. Nonetheless, current explanation methods contribute only to one-way communication. Secondly, NNs tend to be overconfident, providing unreasonable uncertainty estimates on out-of-distribution observations. We overcome these difficulties by training a Bayesian convolutional neural network (CNN) that uses explanation feedback. After training, the model presents explanations of training sample classifications to an annotator. Based on the provided information, the annotator can accept or reject the explanations by providing feedback. Our proposed method utilizes this feedback for fine-tuning to correct the model such that the explanations and classifications improve. We use existing CNN architectures to demonstrate the method's effectiveness on one toy dataset (decoy MNIST) and two real-world datasets (Dogs vs. Cats and ISIC skin cancer). The experiments indicate that few annotated explanations and fine-tuning epochs are needed to improve the model and predictive performance, making the model more trustworthy and understandable.

</p>
</details>

<details><summary><b>What is Going on Inside Recurrent Meta Reinforcement Learning Agents?</b>
<a href="https://arxiv.org/abs/2104.14644">arxiv:2104.14644</a>
&#x1F4C8; 3 <br>
<p>Safa Alver, Doina Precup</p></summary>
<p>

**Abstract:** Recurrent meta reinforcement learning (meta-RL) agents are agents that employ a recurrent neural network (RNN) for the purpose of "learning a learning algorithm". After being trained on a pre-specified task distribution, the learned weights of the agent's RNN are said to implement an efficient learning algorithm through their activity dynamics, which allows the agent to quickly solve new tasks sampled from the same distribution. However, due to the black-box nature of these agents, the way in which they work is not yet fully understood. In this study, we shed light on the internal working mechanisms of these agents by reformulating the meta-RL problem using the Partially Observable Markov Decision Process (POMDP) framework. We hypothesize that the learned activity dynamics is acting as belief states for such agents. Several illustrative experiments suggest that this hypothesis is true, and that recurrent meta-RL agents can be viewed as agents that learn to act optimally in partially observable environments consisting of multiple related tasks. This view helps in understanding their failure cases and some interesting model-based results reported in the literature.

</p>
</details>

<details><summary><b>Modelling Urban Dynamics with Multi-Modal Graph Convolutional Networks</b>
<a href="https://arxiv.org/abs/2104.14633">arxiv:2104.14633</a>
&#x1F4C8; 3 <br>
<p>Krittika D'Silva, Jordan Cambe, Anastasios Noulas, Cecilia Mascolo, Adam Waksman</p></summary>
<p>

**Abstract:** Modelling the dynamics of urban venues is a challenging task as it is multifaceted in nature. Demand is a function of many complex and nonlinear features such as neighborhood composition, real-time events, and seasonality. Recent advances in Graph Convolutional Networks (GCNs) have had promising results as they build a graphical representation of a system and harness the potential of deep learning architectures. However, there has been limited work using GCNs in a temporal setting to model dynamic dependencies of the network. Further, within the context of urban environments, there has been no prior work using dynamic GCNs to support venue demand analysis and prediction. In this paper, we propose a novel deep learning framework which aims to better model the popularity and growth of urban venues. Using a longitudinal dataset from location technology platform Foursquare, we model individual venues and venue types across London and Paris. First, representing cities as connected networks of venues, we quantify their structure and note a strong community structure in these retail networks, an observation that highlights the interplay of cooperative and competitive forces that emerge in local ecosystems of retail businesses. Next, we present our deep learning architecture which integrates both spatial and topological features into a temporal model which predicts the demand of a venue at the subsequent time-step. Our experiments demonstrate that our model can learn spatio-temporal trends of venue demand and consistently outperform baseline models. Relative to state-of-the-art deep learning models, our model reduces the RSME by ~ 28% in London and ~ 13% in Paris. Our approach highlights the power of complex network measures and GCNs in building prediction models for urban environments. The model could have numerous applications within the retail sector to better model venue demand and growth.

</p>
</details>

<details><summary><b>Scalable Semi-supervised Landmark Localization for X-ray Images using Few-shot Deep Adaptive Graph</b>
<a href="https://arxiv.org/abs/2104.14629">arxiv:2104.14629</a>
&#x1F4C8; 3 <br>
<p>Xiao-Yun Zhou, Bolin Lai, Weijian Li, Yirui Wang, Kang Zheng, Fakai Wang, Chihung Lin, Le Lu, Lingyun Huang, Mei Han, Guotong Xie, Jing Xiao, Kuo Chang-Fu, Adam Harrison, Shun Miao</p></summary>
<p>

**Abstract:** Landmark localization plays an important role in medical image analysis. Learning based methods, including CNN and GCN, have demonstrated the state-of-the-art performance. However, most of these methods are fully-supervised and heavily rely on manual labeling of a large training dataset. In this paper, based on a fully-supervised graph-based method, DAG, we proposed a semi-supervised extension of it, termed few-shot DAG, \ie five-shot DAG. It first trains a DAG model on the labeled data and then fine-tunes the pre-trained model on the unlabeled data with a teacher-student SSL mechanism. In addition to the semi-supervised loss, we propose another loss using JS divergence to regulate the consistency of the intermediate feature maps. We extensively evaluated our method on pelvis, hand and chest landmark detection tasks. Our experiment results demonstrate consistent and significant improvements over previous methods.

</p>
</details>

<details><summary><b>AttendSeg: A Tiny Attention Condenser Neural Network for Semantic Segmentation on the Edge</b>
<a href="https://arxiv.org/abs/2104.14623">arxiv:2104.14623</a>
&#x1F4C8; 3 <br>
<p>Xiaoyu Wen, Mahmoud Famouri, Andrew Hryniowski, Alexander Wong</p></summary>
<p>

**Abstract:** In this study, we introduce \textbf{AttendSeg}, a low-precision, highly compact deep neural network tailored for on-device semantic segmentation. AttendSeg possesses a self-attention network architecture comprising of light-weight attention condensers for improved spatial-channel selective attention at a very low complexity. The unique macro-architecture and micro-architecture design properties of AttendSeg strike a strong balance between representational power and efficiency, achieved via a machine-driven design exploration strategy tailored specifically for the task at hand. Experimental results demonstrated that the proposed AttendSeg can achieve segmentation accuracy comparable to much larger deep neural networks with greater complexity while possessing a significantly lower architecture and computational complexity (requiring as much as >27x fewer MACs, >72x fewer parameters, and >288x lower weight memory requirements), making it well-suited for TinyML applications on the edge.

</p>
</details>

<details><summary><b>Learning in Feedforward Neural Networks Accelerated by Transfer Entropy</b>
<a href="https://arxiv.org/abs/2104.14616">arxiv:2104.14616</a>
&#x1F4C8; 3 <br>
<p>Adrian Moldovan, Angel Caţaron, Răzvan Andonie</p></summary>
<p>

**Abstract:** Current neural networks architectures are many times harder to train because of the increasing size and complexity of the used datasets. Our objective is to design more efficient training algorithms utilizing causal relationships inferred from neural networks. The transfer entropy (TE) was initially introduced as an information transfer measure used to quantify the statistical coherence between events (time series). Later, it was related to causality, even if they are not the same. There are only few papers reporting applications of causality or TE in neural networks. Our contribution is an information-theoretical method for analyzing information transfer between the nodes of feedforward neural networks. The information transfer is measured by the TE of feedback neural connections. Intuitively, TE measures the relevance of a connection in the network and the feedback amplifies this connection. We introduce a backpropagation type training algorithm that uses TE feedback connections to improve its performance.

</p>
</details>

<details><summary><b>Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements</b>
<a href="https://arxiv.org/abs/2104.14526">arxiv:2104.14526</a>
&#x1F4C8; 3 <br>
<p>Tian Tong, Cong Ma, Ashley Prater-Bennette, Erin Tripp, Yuejie Chi</p></summary>
<p>

**Abstract:** Tensors, which provide a powerful and flexible model for representing multi-attribute data and multi-way interactions, play an indispensable role in modern data science across various fields in science and engineering. A fundamental task is to faithfully recover the tensor from highly incomplete measurements in a statistically and computationally efficient manner. Harnessing the low-rank structure of tensors in the Tucker decomposition, this paper develops a scaled gradient descent (ScaledGD) algorithm to directly recover the tensor factors with tailored spectral initializations, and shows that it provably converges at a linear rate independent of the condition number of the ground truth tensor for two canonical problems -- tensor completion and tensor regression -- as soon as the sample size is above the order of $n^{3/2}$ ignoring other parameter dependencies, where $n$ is the dimension of the tensor. This leads to an extremely scalable approach to low-rank tensor estimation compared with prior art, which suffers from at least one of the following drawbacks: extreme sensitivity to ill-conditioning, high per-iteration costs in terms of memory and computation, or poor sample complexity guarantees. To the best of our knowledge, ScaledGD is the first algorithm that achieves near-optimal statistical and computational complexities simultaneously for low-rank tensor completion with the Tucker decomposition. Our algorithm highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the underlying symmetry in low-rank tensor factorization.

</p>
</details>

<details><summary><b>Genotype-Guided Radiomics Signatures for Recurrence Prediction of Non-Small-Cell Lung Cancer</b>
<a href="https://arxiv.org/abs/2104.14420">arxiv:2104.14420</a>
&#x1F4C8; 3 <br>
<p>Panyanat Aonpong, Yutaro Iwamoto, Xian-Hua Han, Lanfen Lin, Yen-Wei Chen</p></summary>
<p>

**Abstract:** Non-small cell lung cancer (NSCLC) is a serious disease and has a high recurrence rate after the surgery. Recently, many machine learning methods have been proposed for recurrence prediction. The methods using gene data have high prediction accuracy but require high cost. Although the radiomics signatures using only CT image are not expensive, its accuracy is relatively low. In this paper, we propose a genotype-guided radiomics method (GGR) for obtaining high prediction accuracy with low cost. We used a public radiogenomics dataset of NSCLC, which includes CT images and gene data. The proposed method is a two-step method, which consists of two models. The first model is a gene estimation model, which is used to estimate the gene expression from radiomics features and deep features extracted from computer tomography (CT) image. The second model is used to predict the recurrence using the estimated gene expression data. The proposed GGR method designed based on hybrid features which is combination of handcrafted-based and deep learning-based. The experiments demonstrated that the prediction accuracy can be improved significantly from 78.61% (existing radiomics method) and 79.14% (deep learning method) to 83.28% by the proposed GGR.

</p>
</details>

<details><summary><b>Generalized Linear Models with Structured Sparsity Estimators</b>
<a href="https://arxiv.org/abs/2104.14371">arxiv:2104.14371</a>
&#x1F4C8; 3 <br>
<p>Mehmet Caner</p></summary>
<p>

**Abstract:** In this paper, we introduce structured sparsity estimators in Generalized Linear Models. Structured sparsity estimators in the least squares loss are introduced by Stucky and van de Geer (2018) recently for fixed design and normal errors. We extend their results to debiased structured sparsity estimators with Generalized Linear Model based loss. Structured sparsity estimation means penalized loss functions with a possible sparsity structure used in the chosen norm. These include weighted group lasso, lasso and norms generated from convex cones. The significant difficulty is that it is not clear how to prove two oracle inequalities. The first one is for the initial penalized Generalized Linear Model estimator. Since it is not clear how a particular feasible-weighted nodewise regression may fit in an oracle inequality for penalized Generalized Linear Model, we need a second oracle inequality to get oracle bounds for the approximate inverse for the sample estimate of second-order partial derivative of Generalized Linear Model.
  Our contributions are fivefold: 1. We generalize the existing oracle inequality results in penalized Generalized Linear Models by proving the underlying conditions rather than assuming them. One of the key issues is the proof of a sample one-point margin condition and its use in an oracle inequality. 2. Our results cover even non sub-Gaussian errors and regressors. 3. We provide a feasible weighted nodewise regression proof which generalizes the results in the literature from a simple l_1 norm usage to norms generated from convex cones. 4. We realize that norms used in feasible nodewise regression proofs should be weaker or equal to the norms in penalized Generalized Linear Model loss. 5. We can debias the first step estimator via getting an approximate inverse of the singular-sample second order partial derivative of Generalized Linear Model loss.

</p>
</details>

<details><summary><b>TabAug: Data Driven Augmentation for Enhanced Table Structure Recognition</b>
<a href="https://arxiv.org/abs/2104.14237">arxiv:2104.14237</a>
&#x1F4C8; 3 <br>
<p>Umar Khan, Sohaib Zahid, Muhammad Asad Ali, Adnan ul Hassan, Faisal Shafait</p></summary>
<p>

**Abstract:** Table Structure Recognition is an essential part of end-to-end tabular data extraction in document images. The recent success of deep learning model architectures in computer vision remains to be non-reflective in table structure recognition, largely because extensive datasets for this domain are still unavailable while labeling new data is expensive and time-consuming. Traditionally, in computer vision, these challenges are addressed by standard augmentation techniques that are based on image transformations like color jittering and random cropping. As demonstrated by our experiments, these techniques are not effective for the task of table structure recognition. In this paper, we propose TabAug, a re-imagined Data Augmentation technique that produces structural changes in table images through replication and deletion of rows and columns. It also consists of a data-driven probabilistic model that allows control over the augmentation process. To demonstrate the efficacy of our approach, we perform experimentation on ICDAR 2013 dataset where our approach shows consistent improvements in all aspects of the evaluation metrics, with cell-level correct detections improving from 92.16% to 96.11% over the baseline.

</p>
</details>

<details><summary><b>Privacy-Preserving Portrait Matting</b>
<a href="https://arxiv.org/abs/2104.14222">arxiv:2104.14222</a>
&#x1F4C8; 3 <br>
<p>Jizhizi Li, Sihan Ma, Jing Zhang, Dacheng Tao</p></summary>
<p>

**Abstract:** Recently, there has been an increasing concern about the privacy issue raised by using personally identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable portrait images. To fill the gap, we present P3M-10k in this paper, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting. P3M-10k consists of 10,000 high-resolution face-blurred portrait images along with high-quality alpha mattes. We systematically evaluate both trimap-free and trimap-based matting methods on P3M-10k and find that existing matting methods show different generalization capabilities when following the Privacy-Preserving Training (PPT) setting, i.e., training on face-blurred images and testing on arbitrary images. To devise a better trimap-free portrait matting model, we propose P3M-Net, which leverages the power of a unified framework for both semantic perception and detail matting, and specifically emphasizes the interaction between them and the encoder to facilitate the matting process. Extensive experiments on P3M-10k demonstrate that P3M-Net outperforms the state-of-the-art methods in terms of both objective metrics and subjective visual quality. Besides, it shows good generalization capacity under the PPT setting, confirming the value of P3M-10k for facilitating future research and enabling potential real-world applications. The source code and dataset are available at https://github.com/JizhiziLi/P3M

</p>
</details>

<details><summary><b>Cross-lingual hate speech detection based on multilingual domain-specific word embeddings</b>
<a href="https://arxiv.org/abs/2104.14728">arxiv:2104.14728</a>
&#x1F4C8; 2 <br>
<p>Aymé Arango, Jorge Pérez, Barbara Poblete</p></summary>
<p>

**Abstract:** Automatic hate speech detection in online social networks is an important open problem in Natural Language Processing (NLP). Hate speech is a multidimensional issue, strongly dependant on language and cultural factors. Despite its relevance, research on this topic has been almost exclusively devoted to English. Most supervised learning resources, such as labeled datasets and NLP tools, have been created for this same language. Considering that a large portion of users worldwide speak in languages other than English, there is an important need for creating efficient approaches for multilingual hate speech detection. In this work we propose to address the problem of multilingual hate speech detection from the perspective of transfer learning. Our goal is to determine if knowledge from one particular language can be used to classify other language, and to determine effective ways to achieve this. We propose a hate specific data representation and evaluate its effectiveness against general-purpose universal representations most of which, unlike our proposed model, have been trained on massive amounts of data. We focus on a cross-lingual setting, in which one needs to classify hate speech in one language without having access to any labeled data for that language. We show that the use of our simple yet specific multilingual hate representations improves classification results. We explain this with a qualitative analysis showing that our specific representation is able to capture some common patterns in how hate speech presents itself in different languages.
  Our proposal constitutes, to the best of our knowledge, the first attempt for constructing multilingual specific-task representations. Despite its simplicity, our model outperformed the previous approaches for most of the experimental setups. Our findings can orient future solutions toward the use of domain-specific representations.

</p>
</details>

<details><summary><b>End-to-End Attention-based Image Captioning</b>
<a href="https://arxiv.org/abs/2104.14721">arxiv:2104.14721</a>
&#x1F4C8; 2 <br>
<p>Carola Sundaramoorthy, Lin Ziwen Kelvin, Mahak Sarin, Shubham Gupta</p></summary>
<p>

**Abstract:** In this paper, we address the problem of image captioning specifically for molecular translation where the result would be a predicted chemical notation in InChI format for a given molecular structure. Current approaches mainly follow rule-based or CNN+RNN based methodology. However, they seem to underperform on noisy images and images with small number of distinguishable features. To overcome this, we propose an end-to-end transformer model. When compared to attention-based techniques, our proposed model outperforms on molecular datasets.

</p>
</details>

<details><summary><b>Analytical bounds on the local Lipschitz constants of ReLU networks</b>
<a href="https://arxiv.org/abs/2104.14672">arxiv:2104.14672</a>
&#x1F4C8; 2 <br>
<p>Trevor Avant, Kristi A. Morgansen</p></summary>
<p>

**Abstract:** In this paper, we determine analytical upper bounds on the local Lipschitz constants of feedforward neural networks with ReLU activation functions. We do so by deriving Lipschitz constants and bounds for ReLU, affine-ReLU, and max pooling functions, and combining the results to determine a network-wide bound. Our method uses several insights to obtain tight bounds, such as keeping track of the zero elements of each layer, and analyzing the composition of affine and ReLU functions. Furthermore, we employ a careful computational approach which allows us to apply our method to large networks such as AlexNet and VGG-16. We present several examples using different networks, which show how our local Lipschitz bounds are tighter than the global Lipschitz bounds. We also show how our method can be applied to provide adversarial bounds for classification networks. These results show that our method produces the largest known bounds on minimum adversarial perturbations for large networks such as AlexNet and VGG-16.

</p>
</details>

<details><summary><b>Stochastic Mirror Descent for Low-Rank Tensor Decomposition Under Non-Euclidean Losses</b>
<a href="https://arxiv.org/abs/2104.14562">arxiv:2104.14562</a>
&#x1F4C8; 2 <br>
<p>Wenqiang Pu, Shahana Ibrahim, Xiao Fu, Mingyi Hong</p></summary>
<p>

**Abstract:** This work considers low-rank canonical polyadic decomposition (CPD) under a class of non-Euclidean loss functions that frequently arise in statistical machine learning and signal processing. These loss functions are often used for certain types of tensor data, e.g., count and binary tensors, where the least squares loss is considered unnatural.Compared to the least squares loss, the non-Euclidean losses are generally more challenging to handle. Non-Euclidean CPD has attracted considerable interests and a number of prior works exist. However, pressing computational and theoretical challenges, such as scalability and convergence issues, still remain. This work offers a unified stochastic algorithmic framework for large-scale CPD decomposition under a variety of non-Euclidean loss functions. Our key contribution lies in a tensor fiber sampling strategy-based flexible stochastic mirror descent framework. Leveraging the sampling scheme and the multilinear algebraic structure of low-rank tensors, the proposed lightweight algorithm ensures global convergence to a stationary point under reasonable conditions. Numerical results show that our framework attains promising non-Euclidean CPD performance. The proposed framework also exhibits substantial computational savings compared to state-of-the-art methods.

</p>
</details>

<details><summary><b>MUSE: Multi-faceted Attention for Signed Network Embedding</b>
<a href="https://arxiv.org/abs/2104.14449">arxiv:2104.14449</a>
&#x1F4C8; 2 <br>
<p>Dengcheng Yan, Youwen Zhang, Wei Li, Yiwen Zhang</p></summary>
<p>

**Abstract:** Signed network embedding is an approach to learn low-dimensional representations of nodes in signed networks with both positive and negative links, which facilitates downstream tasks such as link prediction with general data mining frameworks. Due to the distinct properties and significant added value of negative links, existing signed network embedding methods usually design dedicated methods based on social theories such as balance theory and status theory. However, existing signed network embedding methods ignore the characteristics of multiple facets of each node and mix them up in one single representation, which limits the ability to capture the fine-grained attentions between node pairs. In this paper, we propose MUSE, a MUlti-faceted attention-based Signed network Embedding framework to tackle this problem. Specifically, a joint intra- and inter-facet attention mechanism is introduced to aggregate fine-grained information from neighbor nodes. Moreover, balance theory is also utilized to guide information aggregation from multi-order balanced and unbalanced neighbors. Experimental results on four real-world signed network datasets demonstrate the effectiveness of our proposed framework.

</p>
</details>

<details><summary><b>Learning Robust Variational Information Bottleneck with Reference</b>
<a href="https://arxiv.org/abs/2104.14379">arxiv:2104.14379</a>
&#x1F4C8; 2 <br>
<p>Weizhu Qian, Bowei Chen, Xiaowei Huang</p></summary>
<p>

**Abstract:** We propose a new approach to train a variational information bottleneck (VIB) that improves its robustness to adversarial perturbations. Unlike the traditional methods where the hard labels are usually used for the classification task, we refine the categorical class information in the training phase with soft labels which are obtained from a pre-trained reference neural network and can reflect the likelihood of the original class labels. We also relax the Gaussian posterior assumption in the VIB implementation by using the mutual information neural estimation. Extensive experiments have been performed with the MNIST and CIFAR-10 datasets, and the results show that our proposed approach significantly outperforms the benchmarked models.

</p>
</details>

<details><summary><b>From Distributed Machine Learning to Federated Learning: A Survey</b>
<a href="https://arxiv.org/abs/2104.14362">arxiv:2104.14362</a>
&#x1F4C8; 2 <br>
<p>Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong, Dejing Dou</p></summary>
<p>

**Abstract:** In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of laws or regulations, the distributed data and computing resources cannot be directly shared among different regions or organizations for machine learning tasks. Federated learning emerges as an efficient approach to exploit distributed data and computing resources, so as to collaboratively train machine learning models, while obeying the laws and regulations and ensuring data security and data privacy. In this paper, we provide a comprehensive survey of existing works for federated learning. We propose a functional architecture of federated learning systems and a taxonomy of related techniques. Furthermore, we present the distributed training, data communication, and security of FL systems. Finally, we analyze their limitations and propose future research directions.

</p>
</details>

<details><summary><b>Hand Gesture Recognition Based on a Nonconvex Regularization</b>
<a href="https://arxiv.org/abs/2104.14349">arxiv:2104.14349</a>
&#x1F4C8; 2 <br>
<p>Jing Qin, Joshua Ashley, Biyun Xie</p></summary>
<p>

**Abstract:** Recognition of hand gestures is one of the most fundamental tasks in human-robot interaction. Sparse representation based methods have been widely used due to their efficiency and low requirements on the training data. Recently, nonconvex regularization techniques including the $\ell_{1-2}$ regularization have been proposed in the image processing community to promote sparsity while achieving efficient performance. In this paper, we propose a vision-based hand gesture recognition model based on the $\ell_{1-2}$ regularization, which is solved by the alternating direction method of multipliers (ADMM). Numerical experiments on binary and gray-scale data sets have shown the effectiveness of this method in identifying hand gestures.

</p>
</details>

<details><summary><b>Differentiable model-based adaptive optics for two-photon microscopy</b>
<a href="https://arxiv.org/abs/2104.14308">arxiv:2104.14308</a>
&#x1F4C8; 2 <br>
<p>Ivan Vishniakou, Johannes D. Seelig</p></summary>
<p>

**Abstract:** Aberrations limit scanning fluorescence microscopy when imaging in scattering materials such as biological tissue. Model-based approaches for adaptive optics take advantage of a computational model of the optical setup. Such models can be combined with the optimization techniques of machine learning frameworks to find aberration corrections, as was demonstrated for focusing a laser beam through aberrations onto a camera [arXiv:2007.13400]. Here, we extend this approach to two-photon scanning microscopy. The developed sensorless technique finds corrections for aberrations in scattering samples and will be useful for a range of imaging application, for example in brain tissue.

</p>
</details>

<details><summary><b>End-to-End Speech Recognition from Federated Acoustic Models</b>
<a href="https://arxiv.org/abs/2104.14297">arxiv:2104.14297</a>
&#x1F4C8; 2 <br>
<p>Yan Gao, Titouan Parcollet, Salah Zaiem, Javier Fernandez-Marques, Pedro P. B. de Gusmao, Daniel J. Beutel, Nicholas D. Lane</p></summary>
<p>

**Abstract:** Training Automatic Speech Recognition (ASR) models under federated learning (FL) settings has attracted a lot of attention recently. However, the FL scenarios often presented in the literature are artificial and fail to capture the complexity of real FL systems. In this paper, we construct a challenging and realistic ASR federated experimental setup consisting of clients with heterogeneous data distributions using the French and Italian sets of the CommonVoice dataset, a large heterogeneous dataset containing thousands of different speakers, acoustic environments and noises. We present the first empirical study on attention-based sequence-to-sequence End-to-End (E2E) ASR model with three aggregation weighting strategies -- standard FedAvg, loss-based aggregation and a novel word error rate (WER)-based aggregation, compared in two realistic FL scenarios: cross-silo with 10 clients and cross-device with 2K and 4K clients. Our analysis on E2E ASR from heterogeneous and realistic federated acoustic models provides the foundations for future research and development of realistic FL-based ASR applications.

</p>
</details>

<details><summary><b>Meta-learning using privileged information for dynamics</b>
<a href="https://arxiv.org/abs/2104.14290">arxiv:2104.14290</a>
&#x1F4C8; 2 <br>
<p>Ben Day, Alexander Norcliffe, Jacob Moss, Pietro Liò</p></summary>
<p>

**Abstract:** Neural ODE Processes approach the problem of meta-learning for dynamics using a latent variable model, which permits a flexible aggregation of contextual information. This flexibility is inherited from the Neural Process framework and allows the model to aggregate sets of context observations of arbitrary size into a fixed-length representation. In the physical sciences, we often have access to structured knowledge in addition to raw observations of a system, such as the value of a conserved quantity or a description of an understood component. Taking advantage of the aggregation flexibility, we extend the Neural ODE Process model to use additional information within the Learning Using Privileged Information setting, and we validate our extension with experiments showing improved accuracy and calibration on simulated dynamics tasks.

</p>
</details>

<details><summary><b>Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy</b>
<a href="https://arxiv.org/abs/2104.14281">arxiv:2104.14281</a>
&#x1F4C8; 2 <br>
<p>Yongzhen Wang, Xiaozhong Liu, Katy Börner, Jun Lin, Yingnan Ju, Changlong Sun, Luo Si</p></summary>
<p>

**Abstract:** Objective: Ubiquitous internet access is reshaping the way we live, but it is accompanied by unprecedented challenges in preventing chronic diseases that are usually planted by long exposure to unhealthy lifestyles. This paper proposes leveraging online shopping behaviors as a proxy for personal lifestyle choices to improve chronic disease prevention literacy, targeted for times when e-commerce user experience has been assimilated into most people's everyday lives.
  Methods: Longitudinal query logs and purchase records from 15 million online shoppers were accessed, constructing a broad spectrum of lifestyle features covering assorted product categories and buyer personas. Using the lifestyle-related information preceding their first purchases of specific prescription drugs, we could determine associations between online shoppers' past lifestyle choices and whether they suffered from a particular chronic disease or not.
  Results: Novel lifestyle risk factors were discovered in two exemplars -- depression and diabetes, most of which showed cognitive congruence with existing healthcare knowledge. Further, such empirical findings could be adopted to locate online shoppers at high risk of these chronic diseases with decent accuracy (i.e., [area under the receiver operating characteristic curve] AUC=0.68 for depression and AUC=0.70 for diabetes), closely matching the performance of screening surveys benchmarked against medical diagnosis.
  Conclusions: Mining online shopping behaviors can point medical experts to a series of lifestyle issues associated with chronic diseases that are less explored to date. Hopefully, unobtrusive chronic disease surveillance via e-commerce sites can grant consenting individuals a privilege to be connected more readily with the medical profession and sophistication.

</p>
</details>

<details><summary><b>InsertionNet -- A Scalable Solution for Insertion</b>
<a href="https://arxiv.org/abs/2104.14223">arxiv:2104.14223</a>
&#x1F4C8; 2 <br>
<p>Oren Spector, Dotan Di Castro</p></summary>
<p>

**Abstract:** Complicated assembly processes can be described as a sequence of two main activities: grasping and insertion. While general grasping solutions are common in industry, insertion is still only applicable to small subsets of problems, mainly ones involving simple shapes in fixed locations and in which the variations are not taken into consideration. Recently, RL approaches with prior knowledge (e.g., LfD or residual policy) have been adopted. However, these approaches might be problematic in contact-rich tasks since interaction might endanger the robot and its equipment. In this paper, we tackled this challenge by formulating the problem as a regression problem. By combining visual and force inputs, we demonstrate that our method can scale to 16 different insertion tasks in less than 10 minutes. The resulting policies are robust to changes in the socket position, orientation or peg color, as well as to small differences in peg shape. Finally, we demonstrate an end-to-end solution for 2 complex assembly tasks with multi-insertion objectives when the assembly board is randomly placed on a table.

</p>
</details>

<details><summary><b>Fine-grained Generalization Analysis of Vector-valued Learning</b>
<a href="https://arxiv.org/abs/2104.14173">arxiv:2104.14173</a>
&#x1F4C8; 2 <br>
<p>Liang Wu, Antoine Ledent, Yunwen Lei, Marius Kloft</p></summary>
<p>

**Abstract:** Many fundamental machine learning tasks can be formulated as a problem of learning with vector-valued functions, where we learn multiple scalar-valued functions together. Although there is some generalization analysis on different specific algorithms under the empirical risk minimization principle, a unifying analysis of vector-valued learning under a regularization framework is still lacking. In this paper, we initiate the generalization analysis of regularized vector-valued learning algorithms by presenting bounds with a mild dependency on the output dimension and a fast rate on the sample size. Our discussions relax the existing assumptions on the restrictive constraint of hypothesis spaces, smoothness of loss functions and low-noise condition. To understand the interaction between optimization and learning, we further use our results to derive the first generalization bounds for stochastic gradient descent with vector-valued functions. We apply our general results to multi-class classification and multi-label classification, which yield the first bounds with a logarithmic dependency on the output dimension for extreme multi-label classification with the Frobenius regularization. As a byproduct, we derive a Rademacher complexity bound for loss function classes defined in terms of a general strongly convex function.

</p>
</details>

<details><summary><b>Radar-based Automotive Localization using Landmarks in a Multimodal Sensor Graph-based Approach</b>
<a href="https://arxiv.org/abs/2104.14156">arxiv:2104.14156</a>
&#x1F4C8; 2 <br>
<p>Stefan Jürgens, Niklas Koch, Marc-Michael Meinecke</p></summary>
<p>

**Abstract:** Highly automated driving functions currently often rely on a-priori knowledge from maps for planning and prediction in complex scenarios like cities. This makes map-relative localization an essential skill. In this paper, we address the problem of localization with automotive-grade radars, using a real-time graph-based SLAM approach. The system uses landmarks and odometry information as an abstraction layer. This way, besides radars, all kind of different sensor modalities including cameras and lidars can contribute. A single, semantic landmark map is used and maintained for all sensors. We implemented our approach using C++ and thoroughly tested it on data obtained with our test vehicles, comprising cars and trucks. Test scenarios include inner cities and industrial areas like container terminals. The experiments presented in this paper suggest that the approach is able to provide a precise and stable pose in structured environments, using radar data alone. The fusion of additional sensor information from cameras or lidars further boost performance, providing reliable semantic information needed for automated mapping.

</p>
</details>

<details><summary><b>RECKONition: a NLP-based system for Industrial Accidents at Work Prevention</b>
<a href="https://arxiv.org/abs/2104.14150">arxiv:2104.14150</a>
&#x1F4C8; 2 <br>
<p>Patrizia Agnello, Silvia M. Ansaldi, Emilia Lenzi, Alessio Mongelluzzo, Manuel Roveri</p></summary>
<p>

**Abstract:** Extracting patterns and useful information from Natural Language datasets is a challenging task, especially when dealing with data written in a language different from English, like Italian. Machine and Deep Learning, together with Natural Language Processing (NLP) techniques have widely spread and improved lately, providing a plethora of useful methods to address both Supervised and Unsupervised problems on textual information. We propose RECKONition, a NLP-based system for Industrial Accidents at Work Prevention. RECKONition, which is meant to provide Natural Language Understanding, Clustering and Inference, is the result of a joint partnership with the Italian National Institute for Insurance against Accidents at Work (INAIL). The obtained results showed the ability to process textual data written in Italian describing industrial accidents dynamics and consequences.

</p>
</details>

<details><summary><b>Generalization Guarantees for Neural Architecture Search with Train-Validation Split</b>
<a href="https://arxiv.org/abs/2104.14132">arxiv:2104.14132</a>
&#x1F4C8; 2 <br>
<p>Samet Oymak, Mingchen Li, Mahdi Soltanolkotabi</p></summary>
<p>

**Abstract:** Neural Architecture Search (NAS) is a popular method for automatically designing optimized architectures for high-performance deep learning. In this approach, it is common to use bilevel optimization where one optimizes the model weights over the training data (lower-level problem) and various hyperparameters such as the configuration of the architecture over the validation data (upper-level problem). This paper explores the statistical aspects of such problems with train-validation splits. In practice, the lower-level problem is often overparameterized and can easily achieve zero loss. Thus, a-priori it seems impossible to distinguish the right hyperparameters based on training loss alone which motivates a better understanding of the role of train-validation split. To this aim this work establishes the following results. (1) We show that refined properties of the validation loss such as risk and hyper-gradients are indicative of those of the true test loss. This reveals that the upper-level problem helps select the most generalizable model and prevent overfitting with a near-minimal validation sample size. Importantly, this is established for continuous search spaces which are highly relevant for popular differentiable search schemes. (2) We establish generalization bounds for NAS problems with an emphasis on an activation search problem. When optimized with gradient-descent, we show that the train-validation procedure returns the best (model, architecture) pair even if all architectures can perfectly fit the training data to achieve zero error. (3) Finally, we highlight rigorous connections between NAS, multiple kernel learning, and low-rank matrix learning. The latter leads to novel algorithmic insights where the solution of the upper problem can be accurately learned via efficient spectral methods to achieve near-minimal risk.

</p>
</details>

<details><summary><b>REGRAD: A Large-Scale Relational Grasp Dataset for Safe and Object-Specific Robotic Grasping in Clutter</b>
<a href="https://arxiv.org/abs/2104.14118">arxiv:2104.14118</a>
&#x1F4C8; 2 <br>
<p>Hanbo Zhang, Deyu Yang, Han Wang, Binglei Zhao, Xuguang Lan, Jishiyu Ding, Nanning Zheng</p></summary>
<p>

**Abstract:** Despite the impressive progress achieved in robotic grasping, robots are not skilled in sophisticated tasks (e.g. search and grasp a specified target in clutter). Such tasks involve not only grasping but the comprehensive perception of the world (e.g. the object relationships). Recently, encouraging results demonstrate that it is possible to understand high-level concepts by learning. However, such algorithms are usually data-intensive, and the lack of data severely limits their performance. In this paper, we present a new dataset named REGRAD for the learning of relationships among objects and grasps. We collect the annotations of object poses, segmentations, grasps, and relationships for the target-driven relational grasping tasks. Our dataset is collected in both forms of 2D images and 3D point clouds. Moreover, since all the data are generated automatically, it is free to import new objects for data generation. We also released a real-world validation dataset to evaluate the sim-to-real performance of models trained on REGRAD. Finally, we conducted a series of experiments, showing that the models trained on REGRAD could generalize well to the realistic scenarios, in terms of both relationship and grasp detection. Our dataset and code could be found at: https://github.com/poisonwine/REGRAD

</p>
</details>

<details><summary><b>An Automated Approach for Timely Diagnosis and Prognosis of Coronavirus Disease</b>
<a href="https://arxiv.org/abs/2104.14116">arxiv:2104.14116</a>
&#x1F4C8; 2 <br>
<p>Abbas Raza Ali, Marcin Budka</p></summary>
<p>

**Abstract:** Since the outbreak of Coronavirus Disease 2019 (COVID-19), most of the impacted patients have been diagnosed with high fever, dry cough, and soar throat leading to severe pneumonia. Hence, to date, the diagnosis of COVID-19 from lung imaging is proved to be a major evidence for early diagnosis of the disease. Although nucleic acid detection using real-time reverse-transcriptase polymerase chain reaction (rRT-PCR) remains a gold standard for the detection of COVID-19, the proposed approach focuses on the automated diagnosis and prognosis of the disease from a non-contrast chest computed tomography (CT)scan for timely diagnosis and triage of the patient. The prognosis covers the quantification and assessment of the disease to help hospitals with the management and planning of crucial resources, such as medical staff, ventilators and intensive care units (ICUs) capacity. The approach utilises deep learning techniques for automated quantification of the severity of COVID-19 disease via measuring the area of multiple rounded ground-glass opacities (GGO) and consolidations in the periphery (CP) of the lungs and accumulating them to form a severity score. The severity of the disease can be correlated with the medicines prescribed during the triage to assess the effectiveness of the treatment. The proposed approach shows promising results where the classification model achieved 93% accuracy on hold-out data.

</p>
</details>

<details><summary><b>A Gradient-based Deep Neural Network Model for Simulating Multiphase Flow in Porous Media</b>
<a href="https://arxiv.org/abs/2105.02652">arxiv:2105.02652</a>
&#x1F4C8; 1 <br>
<p>Bicheng Yan, Dylan Robert Harp, Rajesh J. Pawar</p></summary>
<p>

**Abstract:** Simulation of multiphase flow in porous media is crucial for the effective management of subsurface energy and environment related activities. The numerical simulators used for modeling such processes rely on spatial and temporal discretization of the governing partial-differential equations (PDEs) into algebraic systems via numerical methods. These simulators usually require dedicated software development and maintenance, and suffer low efficiency from a runtime and memory standpoint. Therefore, developing cost-effective, data-driven models can become a practical choice since deep learning approaches are considered to be universal approximations. In this paper, we describe a gradient-based deep neural network (GDNN) constrained by the physics related to multiphase flow in porous media. We tackle the nonlinearity of flow in porous media induced by rock heterogeneity, fluid properties and fluid-rock interactions by decomposing the nonlinear PDEs into a dictionary of elementary differential operators. We use a combination of operators to handle rock spatial heterogeneity and fluid flow by advection. Since the augmented differential operators are inherently related to the physics of fluid flow, we treat them as first principles prior knowledge to regularize the GDNN training. We use the example of pressure management at geologic CO2 storage sites, where CO2 is injected in saline aquifers and brine is produced, and apply GDNN to construct a predictive model that is trained from physics-based simulation data and emulates the physics process. We demonstrate that GDNN can effectively predict the nonlinear patterns of subsurface responses including the temporal-spatial evolution of the pressure and saturation plumes. GDNN has great potential to tackle challenging problems that are governed by highly nonlinear physics and enables development of data-driven models with higher fidelity.

</p>
</details>

<details><summary><b>A lightweight deep learning based cloud detection method for Sentinel-2A imagery fusing multi-scale spectral and spatial features</b>
<a href="https://arxiv.org/abs/2105.00967">arxiv:2105.00967</a>
&#x1F4C8; 1 <br>
<p>Jun Li, Zhaocong Wu, Zhongwen Hu, Canliang Jian, Shaojie Luo, Lichao Mou, Xiao Xiang Zhu, Matthieu Molinier</p></summary>
<p>

**Abstract:** Clouds are a very important factor in the availability of optical remote sensing images. Recently, deep learning-based cloud detection methods have surpassed classical methods based on rules and physical models of clouds. However, most of these deep models are very large which limits their applicability and explainability, while other models do not make use of the full spectral information in multi-spectral images such as Sentinel-2. In this paper, we propose a lightweight network for cloud detection, fusing multi-scale spectral and spatial features (CDFM3SF) and tailored for processing all spectral bands in Sentinel- 2A images. The proposed method consists of an encoder and a decoder. In the encoder, three input branches are designed to handle spectral bands at their native resolution and extract multiscale spectral features. Three novel components are designed: a mixed depth-wise separable convolution (MDSC) and a shared and dilated residual block (SDRB) to extract multi-scale spatial features, and a concatenation and sum (CS) operation to fuse multi-scale spectral and spatial features with little calculation and no additional parameters. The decoder of CD-FM3SF outputs three cloud masks at the same resolution as input bands to enhance the supervision information of small, middle and large clouds. To validate the performance of the proposed method, we manually labeled 36 Sentinel-2A scenes evenly distributed over mainland China. The experiment results demonstrate that CD-FM3SF outperforms traditional cloud detection methods and state-of-theart deep learning-based methods in both accuracy and speed.

</p>
</details>

<details><summary><b>Complex-valued Convolutional Neural Networks for Enhanced Radar Signal Denoising and Interference Mitigation</b>
<a href="https://arxiv.org/abs/2105.00929">arxiv:2105.00929</a>
&#x1F4C8; 1 <br>
<p>Alexander Fuchs, Johanna Rock, Mate Toth, Paul Meissner, Franz Pernkopf</p></summary>
<p>

**Abstract:** Autonomous driving highly depends on capable sensors to perceive the environment and to deliver reliable information to the vehicles' control systems. To increase its robustness, a diversified set of sensors is used, including radar sensors. Radar is a vital contribution of sensory information, providing high resolution range as well as velocity measurements. The increased use of radar sensors in road traffic introduces new challenges. As the so far unregulated frequency band becomes increasingly crowded, radar sensors suffer from mutual interference between multiple radar sensors. This interference must be mitigated in order to ensure a high and consistent detection sensitivity. In this paper, we propose the use of Complex-Valued Convolutional Neural Networks (CVCNNs) to address the issue of mutual interference between radar sensors. We extend previously developed methods to the complex domain in order to process radar data according to its physical characteristics. This not only increases data efficiency, but also improves the conservation of phase information during filtering, which is crucial for further processing, such as angle estimation. Our experiments show, that the use of CVCNNs increases data efficiency, speeds up network training and substantially improves the conservation of phase information during interference removal.

</p>
</details>

<details><summary><b>The Zero Resource Speech Challenge 2021: Spoken language modelling</b>
<a href="https://arxiv.org/abs/2104.14700">arxiv:2104.14700</a>
&#x1F4C8; 1 <br>
<p>Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Eugene Kharitonov, Emmanuel Dupoux</p></summary>
<p>

**Abstract:** We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.

</p>
</details>

<details><summary><b>Physically Feasible Vehicle Trajectory Prediction</b>
<a href="https://arxiv.org/abs/2104.14679">arxiv:2104.14679</a>
&#x1F4C8; 1 <br>
<p>Harshayu Girase, Jerrick Hoang, Sai Yalamanchi, Micol Marchetti-Bowick</p></summary>
<p>

**Abstract:** Predicting the future motion of actors in a traffic scene is a crucial part of any autonomous driving system. Recent research in this area has focused on trajectory prediction approaches that optimize standard trajectory error metrics. In this work, we describe three important properties -- physical realism guarantees, system maintainability, and sample efficiency -- which we believe are equally important for developing a self-driving system that can operate safely and practically in the real world. Furthermore, we introduce PTNet (PathTrackingNet), a novel approach for vehicle trajectory prediction that is a hybrid of the classical pure pursuit path tracking algorithm and modern graph-based neural networks. By combining a structured robotics technique with a flexible learning approach, we are able to produce a system that not only achieves the same level of performance as other state-of-the-art methods on traditional trajectory error metrics, but also provides strong guarantees about the physical realism of the predicted trajectories while requiring half the amount of data. We believe focusing on this new class of hybrid approaches is an useful direction for developing and maintaining a safety-critical autonomous driving system.

</p>
</details>

<details><summary><b>SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics</b>
<a href="https://arxiv.org/abs/2104.14671">arxiv:2104.14671</a>
&#x1F4C8; 1 <br>
<p>Toufique Ahmed, Noah Rose Ledesma, Premkumar Devanbu</p></summary>
<p>

**Abstract:** Beginning programmers struggle with the complex grammar of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Students could be helped, and instructors' time saved, by automated repair suggestions when dealing with syntax errors. Large samples of student errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help students fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SYNFIX, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SYNFIX's architecture in detail, and provide a detailed evaluation. We have built SYNFIX into a free, open-source version of Visual Studio Code; we make all our source code and models freely available.

</p>
</details>

<details><summary><b>Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance Learning and Radiomics</b>
<a href="https://arxiv.org/abs/2104.14655">arxiv:2104.14655</a>
&#x1F4C8; 1 <br>
<p>Junhua Chen, Haiyan Zeng, Chong Zhang, Zhenwei Shi, Andre Dekker, Leonard Wee, Inigo Bermejo</p></summary>
<p>

**Abstract:** Early diagnosis of lung cancer is a key intervention for the treatment of lung cancer computer aided diagnosis (CAD) can play a crucial role. However, most published CAD methods treat lung cancer diagnosis as a lung nodule classification problem, which does not reflect clinical practice, where clinicians diagnose a patient based on a set of images of nodules, instead of one specific nodule. Besides, the low interpretability of the output provided by these methods presents an important barrier for their adoption. In this article, we treat lung cancer diagnosis as a multiple instance learning (MIL) problem in order to better reflect the diagnosis process in the clinical setting and for the higher interpretability of the output. We chose radiomics as the source of input features and deep attention-based MIL as the classification algorithm.The attention mechanism provides higher interpretability by estimating the importance of each instance in the set for the final diagnosis.In order to improve the model's performance in a small imbalanced dataset, we introduce a new bag simulation method for MIL.The results show that our method can achieve a mean accuracy of 0.807 with a standard error of the mean (SEM) of 0.069, a recall of 0.870 (SEM 0.061), a positive predictive value of 0.928 (SEM 0.078), a negative predictive value of 0.591 (SEM 0.155) and an area under the curve (AUC) of 0.842 (SEM 0.074), outperforming other MIL methods.Additional experiments show that the proposed oversampling strategy significantly improves the model's performance. In addition, our experiments show that our method provides an indication of the importance of each nodule in determining the diagnosis, which combined with the well-defined radiomic features, make the results more interpretable and acceptable for doctors and patients.

</p>
</details>

<details><summary><b>Turing Completeness and Sid Meier's Civilization</b>
<a href="https://arxiv.org/abs/2104.14647">arxiv:2104.14647</a>
&#x1F4C8; 1 <br>
<p>Adrian de Wynter</p></summary>
<p>

**Abstract:** We prove that three strategy video games from the Sid Meier's Civilization series: Sid Meier's Civilization: Beyond Earth, Sid Meier's Civilization V, and Sid Meier's Civilization VI, are Turing complete. We achieve this by building three universal Turing machines-one for each game-using only the elements present in the games, and using their internal rules and mechanics as the transition function. The existence of such machines imply that under the assumptions made, the games are undecidable. We show constructions of these machines within a running game session, and we provide a sample execution of an algorithm-the three-state Busy Beaver-with one of our machines.

</p>
</details>

<details><summary><b>Brain-inspired computing: We need a master plan</b>
<a href="https://arxiv.org/abs/2104.14517">arxiv:2104.14517</a>
&#x1F4C8; 1 <br>
<p>Adnan Mehonic, Anthony J Kenyon</p></summary>
<p>

**Abstract:** New computing technologies inspired by the brain promise fundamentally different ways to process information with extreme energy efficiency and the ability to handle the avalanche of unstructured and noisy data that we are generating at an ever-increasing rate. To realise this promise requires a brave and coordinated plan to bring together disparate research communities and to provide them with the funding, focus and support needed. We have done this in the past with digital technologies; we are in the process of doing it with quantum technologies; can we now do it for brain-inspired computing?

</p>
</details>

<details><summary><b>Uncertainty Principles in Risk-Aware Statistical Estimation</b>
<a href="https://arxiv.org/abs/2104.14283">arxiv:2104.14283</a>
&#x1F4C8; 1 <br>
<p>Nikolas P. Koumpis, Dionysios S. Kalogerias</p></summary>
<p>

**Abstract:** We present a new uncertainty principle for risk-aware statistical estimation, effectively quantifying the inherent trade-off between mean squared error ($\mse$) and risk, the latter measured by the associated average predictive squared error variance ($\sev$), for every admissible estimator of choice. Our uncertainty principle has a familiar form and resembles fundamental and classical results arising in several other areas, such as the Heisenberg principle in statistical and quantum mechanics, and the Gabor limit (time-scale trade-offs) in harmonic analysis. In particular, we prove that, provided a joint generative model of states and observables, the product between $\mse$ and $\sev$ is bounded from below by a computable model-dependent constant, which is explicitly related to the Pareto frontier of a recently studied $\sev$-constrained minimum $\mse$ (MMSE) estimation problem. Further, we show that the aforementioned constant is inherently connected to an intuitive new and rigorously topologically grounded statistical measure of distribution skewness in multiple dimensions, consistent with Pearson's moment coefficient of skewness for variables on the line. Our results are also illustrated via numerical simulations.

</p>
</details>

<details><summary><b>Hardware-Friendly Synaptic Orders and Timescales in Liquid State Machines for Speech Classification</b>
<a href="https://arxiv.org/abs/2104.14264">arxiv:2104.14264</a>
&#x1F4C8; 1 <br>
<p>Vivek Saraswat, Ajinkya Gorad, Anand Naik, Aakash Patil, Udayan Ganguly</p></summary>
<p>

**Abstract:** Liquid State Machines are brain inspired spiking neural networks (SNNs) with random reservoir connectivity and bio-mimetic neuronal and synaptic models. Reservoir computing networks are proposed as an alternative to deep neural networks to solve temporal classification problems. Previous studies suggest 2nd order (double exponential) synaptic waveform to be crucial for achieving high accuracy for TI-46 spoken digits recognition. The proposal of long-time range (ms) bio-mimetic synaptic waveforms is a challenge to compact and power efficient neuromorphic hardware. In this work, we analyze the role of synaptic orders namely: δ (high output for single time step), 0th (rectangular with a finite pulse width), 1st (exponential fall) and 2nd order (exponential rise and fall) and synaptic timescales on the reservoir output response and on the TI-46 spoken digits classification accuracy under a more comprehensive parameter sweep. We find the optimal operating point to be correlated to an optimal range of spiking activity in the reservoir. Further, the proposed 0th order synapses perform at par with the biologically plausible 2nd order synapses. This is substantial relaxation for circuit designers as synapses are the most abundant components in an in-memory implementation for SNNs. The circuit benefits for both analog and mixed-signal realizations of 0th order synapse are highlighted demonstrating 2-3 orders of savings in area and power consumptions by eliminating Op-Amps and Digital to Analog Converter circuits. This has major implications on a complete neural network implementation with focus on peripheral limitations and algorithmic simplifications to overcome them.

</p>
</details>

<details><summary><b>A block-sparse Tensor Train Format for sample-efficient high-dimensional Polynomial Regression</b>
<a href="https://arxiv.org/abs/2104.14255">arxiv:2104.14255</a>
&#x1F4C8; 1 <br>
<p>Michael Götte, Reinhold Schneider, Philipp Trunschke</p></summary>
<p>

**Abstract:** Low-rank tensors are an established framework for high-dimensional least-squares problems. We propose to extend this framework by including the concept of block-sparsity. In the context of polynomial regression each sparsity pattern corresponds to some subspace of homogeneous multivariate polynomials. This allows us to adapt the ansatz space to align better with known sample complexity results. The resulting method is tested in numerical experiments and demonstrates improved computational resource utilization and sample efficiency.

</p>
</details>

<details><summary><b>Heterogeneous electronic medical record representation for similarity computing</b>
<a href="https://arxiv.org/abs/2104.14229">arxiv:2104.14229</a>
&#x1F4C8; 1 <br>
<p>Hoda Memarzadeh, Nasser Ghadiri, Maryam Lotfi Shahreza, Suresh Pokharel</p></summary>
<p>

**Abstract:** Due to the widespread use of tools and the development of text processing techniques, the size and range of clinical data are not limited to structured data. The rapid growth of recorded information has led to big data platforms in healthcare that could be used to improve patients' primary care and serve various secondary purposes. Patient similarity assessment is one of the secondary tasks in identifying patients who are similar to a given patient, and it helps derive insights from similar patients' records to provide better treatment. This type of assessment is based on calculating the distance between patients. Since representing and calculating the similarity of patients plays an essential role in many secondary uses of electronic records, this article examines a new data representation method for Electronic Medical Records (EMRs) while taking into account the information in clinical narratives for similarity computing. Some previous works are based on structured data types, while other works only use unstructured data. However, a comprehensive representation of the information contained in the EMR requires the effective aggregation of both structured and unstructured data. To address the limitations of previous methods, we propose a method that captures the co-occurrence of different medical events, including signs, symptoms, and diseases extracted via unstructured data and structured data. It integrates data as discriminative features to construct a temporal tree, considering the difference between events that have short-term and long-term impacts. Our results show that considering signs, symptoms, and diseases in every time interval leads to less MSE and more precision compared to baseline representations that do not consider this information or consider them separately from structured data.

</p>
</details>

<details><summary><b>Stable Online Control of Linear Time-Varying Systems</b>
<a href="https://arxiv.org/abs/2104.14134">arxiv:2104.14134</a>
&#x1F4C8; 1 <br>
<p>Guannan Qu, Yuanyuan Shi, Sahin Lale, Anima Anandkumar, Adam Wierman</p></summary>
<p>

**Abstract:** Linear time-varying (LTV) systems are widely used for modeling real-world dynamical systems due to their generality and simplicity. Providing stability guarantees for LTV systems is one of the central problems in control theory. However, existing approaches that guarantee stability typically lead to significantly sub-optimal cumulative control cost in online settings where only current or short-term system information is available. In this work, we propose an efficient online control algorithm, COvariance Constrained Online Linear Quadratic (COCO-LQ) control, that guarantees input-to-state stability for a large class of LTV systems while also minimizing the control cost. The proposed method incorporates a state covariance constraint into the semi-definite programming (SDP) formulation of the LQ optimal controller. We empirically demonstrate the performance of COCO-LQ in both synthetic experiments and a power system frequency control example.

</p>
</details>

<details><summary><b>LIDAR and Position-Aided mmWave Beam Selection with Non-local CNNs and Curriculum Training</b>
<a href="https://arxiv.org/abs/2104.14579">arxiv:2104.14579</a>
&#x1F4C8; 0 <br>
<p>Matteo Zecchin, Mahdi Boloursaz Mashhadi, Mikolaj Jankowski, Deniz Gunduz, Marios Kountouris, David Gesbert</p></summary>
<p>

**Abstract:** Efficient millimeter wave (mmWave) beam selection in vehicle-to-infrastructure (V2I) communication is a crucial yet challenging task due to the narrow mmWave beamwidth and high user mobility. To reduce the search overhead of iterative beam discovery procedures, contextual information from light detection and ranging (LIDAR) sensors mounted on vehicles has been leveraged by data-driven methods to produce useful side information. In this paper, we propose a lightweight neural network (NN) architecture along with the corresponding LIDAR preprocessing, which significantly outperforms previous works. Our solution comprises multiple novelties that improve both the convergence speed and the final accuracy of the model. In particular, we define a novel loss function inspired by the knowledge distillation idea, introduce a curriculum training approach exploiting line-of-sight (LOS)/non-line-of-sight (NLOS) information, and we propose a non-local attention module to improve the performance for the more challenging NLOS cases. Simulation results on benchmark datasets show that, utilizing solely LIDAR data and the receiver position, our NN-based beam selection scheme can achieve 79.9% throughput of an exhaustive beam sweeping approach without any beam search overhead and 95% by searching among as few as 6 beams. In a typical mmWave V2I scenario, our proposed method considerably reduces the beam search time required to achieve a desired throughput, in comparison with the inverse fingerprinting and hierarchical beam selection schemes.

</p>
</details>

<details><summary><b>NURBS-Diff: A differentiable programming module for NURBS</b>
<a href="https://arxiv.org/abs/2104.14547">arxiv:2104.14547</a>
&#x1F4C8; 0 <br>
<p>Anjana Deva Prasad, Aditya Balu, Harshil Shah, Soumik Sarkar, Chinmay Hegde, Adarsh Krishnamurthy</p></summary>
<p>

**Abstract:** Boundary representations (B-reps) using Non-Uniform Rational B-splines (NURBS) are the de facto standard used in CAD, but their utility in deep learning-based approaches is not well researched. We propose a differentiable NURBS module to integrate the NURBS representation of CAD models with deep learning methods. We mathematically define the derivatives of the NURBS curves or surfaces with respect to the input parameters. These derivatives are used to define an approximate Jacobian that can be used to perform the "backward" evaluation used while training deep learning models. We have implemented our NURBS module using GPU-accelerated algorithms and integrated it with PyTorch, a popular deep learning framework. We demonstrate the efficacy of our NURBS module in performing CAD operations such as curve or surface fitting and surface offsetting. Further, we show its utility in deep learning for unsupervised point cloud reconstruction. These examples show that our module performs better for certain deep learning frameworks and can be directly integrated with any deep-learning framework requiring NURBS.

</p>
</details>

<details><summary><b>VIRDOCD: a VIRtual DOCtor to Predict Dengue Fatality</b>
<a href="https://arxiv.org/abs/2104.14282">arxiv:2104.14282</a>
&#x1F4C8; 0 <br>
<p>Amit K Chattopadhyay, Subhagata Chattopadhyay</p></summary>
<p>

**Abstract:** Clinicians make routine diagnosis by scrutinizing patients' medical signs and symptoms, a skill popularly referred to as "Clinical Eye". This skill evolves through trial-and-error and improves with time. The success of the therapeutic regime relies largely on the accuracy of interpretation of such sign-symptoms, analyzing which a clinician assesses the severity of the illness. The present study is an attempt to propose a complementary medical front by mathematically modeling the "Clinical Eye" of a VIRtual DOCtor, using Statistical and Machine Intelligence tools (SMI), to analyze Dengue epidemic infected patients (100 case studies with 11 weighted sign-symptoms). The SMI in VIRDOCD reads medical data and translates these into a vector comprising Multiple Linear Regression (MLR) coefficients to predict infection severity grades of dengue patients that clone the clinician's experience-based assessment. Risk managed through ANOVA, the dengue severity grade prediction accuracy from VIRDOCD is found higher (ca 75%) than conventional clinical practice (ca 71.4%, mean accuracy profile assessed by a team of 10 senior consultants). Free of human errors and capable of deciphering even minute differences from almost identical symptoms (to the Clinical Eye), VIRDOCD is uniquely individualized in its decision-making ability. The algorithm has been validated against Random Forest classification (RF, ca 63%), another regression-based classifier similar to MLR that can be trained through supervised learning. We find that MLR-based VIRDOCD is superior to RF in predicting the grade of Dengue morbidity. VIRDOCD can be further extended to analyze other epidemic infections, such as COVID-19.

</p>
</details>

<details><summary><b>A Rigid Registration Method in TEVAR</b>
<a href="https://arxiv.org/abs/2104.14273">arxiv:2104.14273</a>
&#x1F4C8; 0 <br>
<p>Meng Li, Changyan Lin, Heng Wu, Jiasong Li, Hongshuai Cao</p></summary>
<p>

**Abstract:** Since the mapping relationship between definitized intra-interventional X-ray and undefined pre-interventional Computed Tomography(CT) is uncertain, auxiliary positioning devices or body markers, such as medical implants, are commonly used to determine this relationship. However, such approaches can not be widely used in clinical due to the complex realities. To determine the mapping relationship, and achieve a initializtion post estimation of human body without auxiliary equipment or markers, proposed method applies image segmentation and deep feature matching to directly match the X-ray and CT images. As a result, the well-trained network can directly predict the spatial correspondence between arbitrary X-ray and CT. The experimental results show that when combining our approach with the conventional approach, the achieved accuracy and speed can meet the basic clinical intervention needs, and it provides a new direction for intra-interventional registration.

</p>
</details>


[Next Page](2021/2021-04/2021-04-28.md)
