## Summary for 2021-05-11, created on 2021-12-21


<details><summary><b>Diffusion Models Beat GANs on Image Synthesis</b>
<a href="https://arxiv.org/abs/2105.05233">arxiv:2105.05233</a>
&#x1F4C8; 360 <br>
<p>Prafulla Dhariwal, Alex Nichol</p></summary>
<p>

**Abstract:** We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion

</p>
</details>

<details><summary><b>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2105.04906">arxiv:2105.04906</a>
&#x1F4C8; 300 <br>
<p>Adrien Bardes, Jean Ponce, Yann LeCun</p></summary>
<p>

**Abstract:** Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.

</p>
</details>

<details><summary><b>Visual Perspective Taking for Opponent Behavior Modeling</b>
<a href="https://arxiv.org/abs/2105.05145">arxiv:2105.05145</a>
&#x1F4C8; 45 <br>
<p>Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson</p></summary>
<p>

**Abstract:** In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others' plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our approach in the context of visual hide-and-seek - a game that represents a cognitive milestone in human development. Unlike traditional visual predictive model that generates new frames from immediate past frames, our agent can directly predict to multiple future timestamps (25s), extrapolating by 175% beyond the training horizon. We suggest that visual behavior modeling and perspective taking skills will play a critical role in the ability of physical robots to fully integrate into real-world multi-agent activities. Our website is at http://www.cs.columbia.edu/~bchen/vpttob/.

</p>
</details>

<details><summary><b>Including Signed Languages in Natural Language Processing</b>
<a href="https://arxiv.org/abs/2105.05222">arxiv:2105.05222</a>
&#x1F4C8; 37 <br>
<p>Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe Alikhani</p></summary>
<p>

**Abstract:** Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.

</p>
</details>

<details><summary><b>Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity</b>
<a href="https://arxiv.org/abs/2105.04854">arxiv:2105.04854</a>
&#x1F4C8; 21 <br>
<p>Ryan Henderson, Djork-Arné Clevert, Floriane Montanari</p></summary>
<p>

**Abstract:** Rationalizing which parts of a molecule drive the predictions of a molecular graph convolutional neural network (GCNN) can be difficult. To help, we propose two simple regularization techniques to apply during the training of GCNNs: Batch Representation Orthonormalization (BRO) and Gini regularization. BRO, inspired by molecular orbital theory, encourages graph convolution operations to generate orthonormal node embeddings. Gini regularization is applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions. We show that Gini and BRO regularization can improve the accuracy of state-of-the-art GCNN attribution methods on artificial benchmark datasets. In a real-world setting, we demonstrate that medicinal chemists significantly prefer explanations extracted from regularized models. While we only study these regularizers in the context of GCNNs, both can be applied to other types of neural networks

</p>
</details>

<details><summary><b>BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?</b>
<a href="https://arxiv.org/abs/2105.04949">arxiv:2105.04949</a>
&#x1F4C8; 15 <br>
<p>Asahi Ushio, Luis Espinosa-Anke, Steven Schockaert, Jose Camacho-Collados</p></summary>
<p>

**Abstract:** Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as "eye is to seeing what ear is to hearing", sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.

</p>
</details>

<details><summary><b>Addressing "Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for BookCorpus</b>
<a href="https://arxiv.org/abs/2105.05241">arxiv:2105.05241</a>
&#x1F4C8; 10 <br>
<p>Jack Bandy, Nicholas Vincent</p></summary>
<p>

**Abstract:** Recent literature has underscored the importance of dataset documentation work for machine learning, and part of this work involves addressing "documentation debt" for datasets that have been used widely but documented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text dataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, even though little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a preliminary datasheet that provides key context and information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence that (1) BookCorpus likely violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, including problematic content, potential skews in religious representation, and lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus adds to growing literature that urges more careful and systematic documentation for machine learning datasets.

</p>
</details>

<details><summary><b>Counterfactual Explanations for Neural Recommenders</b>
<a href="https://arxiv.org/abs/2105.05008">arxiv:2105.05008</a>
&#x1F4C8; 10 <br>
<p>Khanh Hiep Tran, Azin Ghazimatin, Rishiraj Saha Roy</p></summary>
<p>

**Abstract:** Understanding why specific items are recommended to users can significantly increase their trust and satisfaction in the system. While neural recommenders have become the state-of-the-art in recent years, the complexity of deep models still makes the generation of tangible explanations for end users a challenging problem. Existing methods are usually based on attention distributions over a variety of features, which are still questionable regarding their suitability as explanations, and rather unwieldy to grasp for an end user. Counterfactual explanations based on a small set of the user's own actions have been shown to be an acceptable solution to the tangibility problem. However, current work on such counterfactuals cannot be readily applied to neural models. In this work, we propose ACCENT, the first general framework for finding counterfactual explanations for neural recommenders. It extends recently-proposed influence functions for identifying training points most relevant to a recommendation, from a single to a pair of items, while deducing a counterfactual set in an iterative process. We use ACCENT to generate counterfactual explanations for two popular neural models, Neural Collaborative Filtering (NCF) and Relational Collaborative Filtering (RCF), and demonstrate its feasibility on a sample of the popular MovieLens 100K dataset.

</p>
</details>

<details><summary><b>Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.04962">arxiv:2105.04962</a>
&#x1F4C8; 9 <br>
<p>Julen Urain, Anqi Li, Puze Liu, Carlo D'Eramo, Jan Peters</p></summary>
<p>

**Abstract:** Reactive motion generation problems are usually solved by computing actions as a sum of policies. However, these policies are independent of each other and thus, they can have conflicting behaviors when summing their contributions together. We introduce Composable Energy Policies (CEP), a novel framework for modular reactive motion generation. CEP computes the control action by optimization over the product of a set of stochastic policies. This product of policies will provide a high probability to those actions that satisfy all the components and low probability to the others. Optimizing over the product of the policies avoids the detrimental effect of conflicting behaviors between policies choosing an action that satisfies all the objectives. Besides, we show that CEP naturally adapts to the Reinforcement Learning problem allowing us to integrate, in a hierarchical fashion, any distribution as prior, from multimodal distributions to non-smooth distributions and learn a new policy given them.

</p>
</details>

<details><summary><b>Transfer-Meta Framework for Cross-domain Recommendation to Cold-Start Users</b>
<a href="https://arxiv.org/abs/2105.04785">arxiv:2105.04785</a>
&#x1F4C8; 9 <br>
<p>Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang, Leyu Lin, Qing He</p></summary>
<p>

**Abstract:** Cold-start problems are enormous challenges in practical recommender systems. One promising solution for this problem is cross-domain recommendation (CDR) which leverages rich information from an auxiliary (source) domain to improve the performance of recommender system in the target domain. In these CDR approaches, the family of Embedding and Mapping methods for CDR (EMCDR) is very effective, which explicitly learn a mapping function from source embeddings to target embeddings with overlapping users. However, these approaches suffer from one serious problem: the mapping function is only learned on limited overlapping users, and the function would be biased to the limited overlapping users, which leads to unsatisfying generalization ability and degrades the performance on cold-start users in the target domain. With the advantage of meta learning which has good generalization ability to novel tasks, we propose a transfer-meta framework for CDR (TMCDR) which has a transfer stage and a meta stage. In the transfer (pre-training) stage, a source model and a target model are trained on source and target domains, respectively. In the meta stage, a task-oriented meta network is learned to implicitly transform the user embedding in the source domain to the target feature space. In addition, the TMCDR is a general framework that can be applied upon various base models, e.g., MF, BPR, CML. By utilizing data from Amazon and Douban, we conduct extensive experiments on 6 cross-domain tasks to demonstrate the superior performance and compatibility of TMCDR.

</p>
</details>

<details><summary><b>The Summary Loop: Learning to Write Abstractive Summaries Without Examples</b>
<a href="https://arxiv.org/abs/2105.05361">arxiv:2105.05361</a>
&#x1F4C8; 8 <br>
<p>Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst</p></summary>
<p>

**Abstract:** This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.

</p>
</details>

<details><summary><b>Leveraging Sparse Linear Layers for Debuggable Deep Networks</b>
<a href="https://arxiv.org/abs/2105.04857">arxiv:2105.04857</a>
&#x1F4C8; 8 <br>
<p>Eric Wong, Shibani Santurkar, Aleksander Mądry</p></summary>
<p>

**Abstract:** We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at https://github.com/madrylab/debuggabledeepnetworks.

</p>
</details>

<details><summary><b>GANs for Medical Image Synthesis: An Empirical Study</b>
<a href="https://arxiv.org/abs/2105.05318">arxiv:2105.05318</a>
&#x1F4C8; 6 <br>
<p>Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images.
  Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of a medical datasets.

</p>
</details>

<details><summary><b>ReflectNet -- A Generative Adversarial Method for Single Image Reflection Suppression</b>
<a href="https://arxiv.org/abs/2105.05216">arxiv:2105.05216</a>
&#x1F4C8; 5 <br>
<p>Andreea Birhala, Ionut Mironica</p></summary>
<p>

**Abstract:** Taking pictures through glass windows almost always produces undesired reflections that degrade the quality of the photo. The ill-posed nature of the reflection removal problem reached the attention of many researchers for more than decades. The main challenge of this problem is the lack of real training data and the necessity of generating realistic synthetic data. In this paper, we proposed a single image reflection removal method based on context understanding modules and adversarial training to efficiently restore the transmission layer without reflection. We also propose a complex data generation model in order to create a large training set with various type of reflections. Our proposed reflection removal method outperforms state-of-the-art methods in terms of PSNR and SSIM on the SIR benchmark dataset.

</p>
</details>

<details><summary><b>AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition</b>
<a href="https://arxiv.org/abs/2105.05165">arxiv:2105.05165</a>
&#x1F4C8; 5 <br>
<p>Rameswar Panda, Chun-Fu Chen, Quanfu Fan, Ximeng Sun, Kate Saenko, Aude Oliva, Rogerio Feris</p></summary>
<p>

**Abstract:** Multi-modal learning, which focuses on utilizing various modalities to improve the performance of a model, is widely used in video recognition. While traditional multi-modal learning offers excellent recognition results, its computational expense limits its impact for many real-world applications. In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment conditioned on the input for efficient video recognition. Specifically, given a video segment, a multi-modal policy network is used to decide what modalities should be used for processing by the recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our proposed adaptive approach yields 35%-55% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the input, while also achieving consistent improvements in accuracy over the state-of-the-art methods.

</p>
</details>

<details><summary><b>FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Convergence Analysis</b>
<a href="https://arxiv.org/abs/2105.05001">arxiv:2105.05001</a>
&#x1F4C8; 5 <br>
<p>Baihe Huang, Xiaoxiao Li, Zhao Song, Xin Yang</p></summary>
<p>

**Abstract:** Federated Learning (FL) is an emerging learning scheme that allows different distributed clients to train deep neural networks together without data sharing. Neural networks have become popular due to their unprecedented success. To the best of our knowledge, the theoretical guarantees of FL concerning neural networks with explicit forms and multi-step updates are unexplored. Nevertheless, training analysis of neural networks in FL is non-trivial for two reasons: first, the objective loss function we are optimizing is non-smooth and non-convex, and second, we are even not updating in the gradient direction. Existing convergence results for gradient descent-based methods heavily rely on the fact that the gradient direction is used for updating. This paper presents a new class of convergence analysis for FL, Federated Learning Neural Tangent Kernel (FL-NTK), which corresponds to overparamterized ReLU neural networks trained by gradient descent in FL and is inspired by the analysis in Neural Tangent Kernel (NTK). Theoretically, FL-NTK converges to a global-optimal solution at a linear rate with properly tuned learning parameters. Furthermore, with proper distributional assumptions, FL-NTK can also achieve good generalization.

</p>
</details>

<details><summary><b>Graph-based Neural Architecture Search with Operation Embeddings</b>
<a href="https://arxiv.org/abs/2105.04885">arxiv:2105.04885</a>
&#x1F4C8; 5 <br>
<p>Michail Chatzianastasis, George Dasoulas, Georgios Siolas, Michalis Vazirgiannis</p></summary>
<p>

**Abstract:** Neural Architecture Search (NAS) has recently gained increased attention, as a class of approaches that automatically searches in an input space of network architectures. A crucial part of the NAS pipeline is the encoding of the architecture that consists of the applied computational blocks, namely the operations and the links between them. Most of the existing approaches either fail to capture the structural properties of the architectures or use hand-engineered vector to encode the operator information. In this paper, we propose the replacement of fixed operator encoding with learnable representations in the optimization process. This approach, which effectively captures the relations of different operations, leads to smoother and more accurate representations of the architectures and consequently to improved performance of the end task. Our extensive evaluation in ENAS benchmark demonstrates the effectiveness of the proposed operation embeddings to the generation of highly accurate models, achieving state-of-the-art performance. Finally, our method produces top-performing architectures that share similar operation and graph patterns, highlighting a strong correlation between the structural properties of the architecture and its performance.

</p>
</details>

<details><summary><b>Learning Implicit Temporal Alignment for Few-shot Video Classification</b>
<a href="https://arxiv.org/abs/2105.04823">arxiv:2105.04823</a>
&#x1F4C8; 5 <br>
<p>Songyang Zhang, Jiale Zhou, Xuming He</p></summary>
<p>

**Abstract:** Few-shot video classification aims to learn new video categories with only a few labeled examples, alleviating the burden of costly annotation in real-world applications. However, it is particularly challenging to learn a class-invariant spatial-temporal representation in such a setting. To address this, we propose a novel matching-based few-shot learning strategy for video sequences in this work. Our main idea is to introduce an implicit temporal alignment for a video pair, capable of estimating the similarity between them in an accurate and robust manner. Moreover, we design an effective context encoding module to incorporate spatial and feature channel context, resulting in better modeling of intra-class variations. To train our model, we develop a multi-task loss for learning video matching, leading to video features with better generalization. Extensive experimental results on two challenging benchmarks, show that our method outperforms the prior arts with a sizable margin on SomethingSomething-V2 and competitive results on Kinetics.

</p>
</details>

<details><summary><b>CCN GAC Workshop: Issues with learning in biological recurrent neural networks</b>
<a href="https://arxiv.org/abs/2105.05382">arxiv:2105.05382</a>
&#x1F4C8; 4 <br>
<p>Luke Y. Prince, Ellen Boven, Roy Henha Eyono, Arna Ghosh, Joe Pemberton, Franz Scherr, Claudia Clopath, Rui Ponte Costa, Wolfgang Maass, Blake A. Richards, Cristina Savin, Katharina Anna Wilmes</p></summary>
<p>

**Abstract:** This perspective piece came about through the Generative Adversarial Collaboration (GAC) series of workshops organized by the Computational Cognitive Neuroscience (CCN) conference in 2020. We brought together a number of experts from the field of theoretical neuroscience to debate emerging issues in our understanding of how learning is implemented in biological recurrent neural networks. Here, we will give a brief review of the common assumptions about biological learning and the corresponding findings from experimental neuroscience and contrast them with the efficiency of gradient-based learning in recurrent neural networks commonly used in artificial intelligence. We will then outline the key issues discussed in the workshop: synaptic plasticity, neural circuits, theory-experiment divide, and objective functions. Finally, we conclude with recommendations for both theoretical and experimental neuroscientists when designing new studies that could help to bring clarity to these issues.

</p>
</details>

<details><summary><b>Return-based Scaling: Yet Another Normalisation Trick for Deep RL</b>
<a href="https://arxiv.org/abs/2105.05347">arxiv:2105.05347</a>
&#x1F4C8; 4 <br>
<p>Tom Schaul, Georg Ostrovski, Iurii Kemaev, Diana Borsa</p></summary>
<p>

**Abstract:** Scaling issues are mundane yet irritating for practitioners of reinforcement learning. Error scales vary across domains, tasks, and stages of learning; sometimes by many orders of magnitude. This can be detrimental to learning speed and stability, create interference between learning tasks, and necessitate substantial tuning. We revisit this topic for agents based on temporal-difference learning, sketch out some desiderata and investigate scenarios where simple fixes fall short. The mechanism we propose requires neither tuning, clipping, nor adaptation. We validate its effectiveness and robustness on the suite of Atari games. Our scaling method turns out to be particularly helpful at mitigating interference, when training a shared neural network on multiple targets that differ in reward scale or discounting.

</p>
</details>

<details><summary><b>Exploring a Handwriting Programming Language for Educational Robots</b>
<a href="https://arxiv.org/abs/2105.04963">arxiv:2105.04963</a>
&#x1F4C8; 4 <br>
<p>Laila El-Hamamsy, Vaios Papaspyros, Taavet Kangur, Laura Mathex, Christian Giang, Melissa Skweres, Barbara Bruno, Francesco Mondada</p></summary>
<p>

**Abstract:** Recently, introducing computer science and educational robots in compulsory education has received increasing attention. However, the use of screens in classrooms is often met with resistance, especially in primary school. To address this issue, this study presents the development of a handwriting-based programming language for educational robots. Aiming to align better with existing classroom practices, it allows students to program a robot by drawing symbols with ordinary pens and paper. Regular smartphones are leveraged to process the hand-drawn instructions using computer vision and machine learning algorithms, and send the commands to the robot for execution. To align with the local computer science curriculum, an appropriate playground and scaffolded learning tasks were designed. The system was evaluated in a preliminary test with eight teachers, developers and educational researchers. While the participants pointed out that some technical aspects could be improved, they also acknowledged the potential of the approach to make computer science education in primary school more accessible.

</p>
</details>

<details><summary><b>Could you give me a hint? Generating inference graphs for defeasible reasoning</b>
<a href="https://arxiv.org/abs/2105.05418">arxiv:2105.05418</a>
&#x1F4C8; 3 <br>
<p>Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, Eduard Hovy</p></summary>
<p>

**Abstract:** Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from another NLP task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 20% by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning. (A dataset of 230,000 influence graphs for each defeasible query is located at: https://tinyurl.com/defeasiblegraphs.)

</p>
</details>

<details><summary><b>Spectral Normalisation for Deep Reinforcement Learning: an Optimisation Perspective</b>
<a href="https://arxiv.org/abs/2105.05246">arxiv:2105.05246</a>
&#x1F4C8; 3 <br>
<p>Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath, Lucian Busoniu, Razvan Pascanu</p></summary>
<p>

**Abstract:** Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated \rainbow{} agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning.

</p>
</details>

<details><summary><b>Global Convergence of Three-layer Neural Networks in the Mean Field Regime</b>
<a href="https://arxiv.org/abs/2105.05228">arxiv:2105.05228</a>
&#x1F4C8; 3 <br>
<p>Huy Tuan Pham, Phan-Minh Nguyen</p></summary>
<p>

**Abstract:** In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers.
  In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a \textit{neuronal embedding}, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which -- unlike previous works on two-layer networks -- does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at \textit{any} finite training time (not necessarily at convergence) via an algebraic topology argument.

</p>
</details>

<details><summary><b>Doing Natural Language Processing in A Natural Way: An NLP toolkit based on object-oriented knowledge base and multi-level grammar base</b>
<a href="https://arxiv.org/abs/2105.05227">arxiv:2105.05227</a>
&#x1F4C8; 3 <br>
<p>Yu Guo</p></summary>
<p>

**Abstract:** We introduce an NLP toolkit based on object-oriented knowledge base and multi-level grammar base. This toolkit focuses on semantic parsing, it also has abilities to discover new knowledge and grammar automatically, new discovered knowledge and grammar will be identified by human, and will be used to update the knowledge base and grammar base. This process can be iterated many times to improve the toolkit continuously.

</p>
</details>

<details><summary><b>Pruning of Deep Spiking Neural Networks through Gradient Rewiring</b>
<a href="https://arxiv.org/abs/2105.04916">arxiv:2105.04916</a>
&#x1F4C8; 3 <br>
<p>Yanqi Chen, Zhaofei Yu, Wei Fang, Tiejun Huang, Yonghong Tian</p></summary>
<p>

**Abstract:** Spiking Neural Networks (SNNs) have been attached great importance due to their biological plausibility and high energy-efficiency on neuromorphic chips. As these chips are usually resource-constrained, the compression of SNNs is thus crucial along the road of practical use of SNNs. Most existing methods directly apply pruning approaches in artificial neural networks (ANNs) to SNNs, which ignore the difference between ANNs and SNNs, thus limiting the performance of the pruned SNNs. Besides, these methods are only suitable for shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination in the neural system, we propose gradient rewiring (Grad R), a joint learning algorithm of connectivity and weight for SNNs, that enables us to seamlessly optimize network structure without retraining. Our key innovation is to redefine the gradient to a new synaptic parameter, allowing better exploration of network structures by taking full advantage of the competition between pruning and regrowth of connections. The experimental results show that the proposed method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented 0.73% connectivity, which reveals remarkable structure refining capability in SNNs. Our work suggests that there exists extremely high redundancy in deep SNNs. Our codes are available at https://github.com/Yanqi-Chen/Gradient-Rewiring.

</p>
</details>

<details><summary><b>Benchmarking down-scaled (not so large) pre-trained language models</b>
<a href="https://arxiv.org/abs/2105.04876">arxiv:2105.04876</a>
&#x1F4C8; 3 <br>
<p>M. Aßenmacher, P. Schulze, C. Heumann</p></summary>
<p>

**Abstract:** Large Transformer-based language models are pre-trained on corpora of varying sizes, for a different number of steps and with different batch sizes. At the same time, more fundamental components, such as the pre-training objective or architectural hyperparameters, are modified. In total, it is therefore difficult to ascribe changes in performance to specific factors. Since searching the hyperparameter space over the full systems is too costly, we pre-train down-scaled versions of several popular Transformer-based architectures on a common pre-training corpus and benchmark them on a subset of the GLUE tasks (Wang et al., 2018). Specifically, we systematically compare three pre-training objectives for different shape parameters and model sizes, while also varying the number of pre-training steps and the batch size. In our experiments MLM + NSP (BERT-style) consistently outperforms MLM (RoBERTa-style) as well as the standard LM objective. Furthermore, we find that additional compute should be mainly allocated to an increased model size, while training for more steps is inefficient. Based on these observations, as a final step we attempt to scale up several systems using compound scaling (Tan and Le, 2019) adapted to Transformer-based language models.

</p>
</details>

<details><summary><b>Rationalization through Concepts</b>
<a href="https://arxiv.org/abs/2105.04837">arxiv:2105.04837</a>
&#x1F4C8; 3 <br>
<p>Diego Antognini, Boi Faltings</p></summary>
<p>

**Abstract:** Automated predictions require explanations to be interpretable by humans. One type of explanation is a rationale, i.e., a selection of input features such as relevant text snippets from which the model computes the outcome. However, a single overall selection does not provide a complete explanation, e.g., weighing several aspects for decisions. To this end, we present a novel self-interpretable model called ConRAT. Inspired by how human explanations for high-level decisions are often based on key concepts, ConRAT extracts a set of text snippets as concepts and infers which ones are described in the document. Then, it explains the outcome with a linear aggregation of concepts. Two regularizers drive ConRAT to build interpretable concepts. In addition, we propose two techniques to boost the rationale and predictive performance further. Experiments on both single- and multi-aspect sentiment classification tasks show that ConRAT is the first to generate concepts that align with human rationalization while using only the overall label. Further, it outperforms state-of-the-art methods trained on each aspect label independently.

</p>
</details>

<details><summary><b>Prediction of soft proton intensities in the near-Earth space using machine learning</b>
<a href="https://arxiv.org/abs/2105.15108">arxiv:2105.15108</a>
&#x1F4C8; 2 <br>
<p>Elena A. Kronberg, Tanveer Hannan, Jens Huthmacher, Marcus Münzer, Florian Peste, Ziyang Zhou, Max Berrendorf, Evgeniy Faerman, Fabio Gastaldello, Simona Ghizzardi, Philippe Escoubet, Stein Haaland, Artem Smirnov, Nithin Sivadas, Robert C. Allen, Andrea Tiengo, Raluca Ilie</p></summary>
<p>

**Abstract:** The spatial distribution of energetic protons contributes towards the understanding of magnetospheric dynamics. Based upon 17 years of the Cluster/RAPID observations, we have derived machine learning-based models to predict the proton intensities at energies from 28 to 1,885 keV in the 3D terrestrial magnetosphere at radial distances between 6 and 22 RE. We used the satellite location and indices for solar, solar wind and geomagnetic activity as predictors. The results demonstrate that the neural network (multi-layer perceptron regressor) outperforms baseline models based on the k-Nearest Neighbors and historical binning on average by ~80% and ~33\%, respectively. The average correlation between the observed and predicted data is about 56%, which is reasonable in light of the complex dynamics of fast-moving energetic protons in the magnetosphere. In addition to a quantitative analysis of the prediction results, we also investigate parameter importance in our model. The most decisive parameters for predicting proton intensities are related to the location: ZGSE direction and the radial distance. Among the activity indices, the solar wind dynamic pressure is the most important. The results have a direct practical application, for instance, for assessing the contamination particle background in the X-Ray telescopes for X-ray astronomy orbiting above the radiation belts. To foster reproducible research and to enable the community to build upon our work we publish our complete code, the data, as well as weights of trained models. Further description can be found in the GitHub project at https://github.com/Tanveer81/deep_horizon.

</p>
</details>

<details><summary><b>Frequent Pattern Mining in Continuous-time Temporal Networks</b>
<a href="https://arxiv.org/abs/2105.06399">arxiv:2105.06399</a>
&#x1F4C8; 2 <br>
<p>Ali Jazayeri, Christopher C. Yang</p></summary>
<p>

**Abstract:** Networks are used as highly expressive tools in different disciplines. In recent years, the analysis and mining of temporal networks have attracted substantial attention. Frequent pattern mining is considered an essential task in the network science literature. In addition to the numerous applications, the investigation of frequent pattern mining in networks directly impacts other analytical approaches, such as clustering, quasi-clique and clique mining, and link prediction. In nearly all the algorithms proposed for frequent pattern mining in temporal networks, the networks are represented as sequences of static networks. Then, the inter- or intra-network patterns are mined. This type of representation imposes a computation-expressiveness trade-off to the mining problem. In this paper, we propose a novel representation that can preserve the temporal aspects of the network losslessly. Then, we introduce the concept of constrained interval graphs (CIGs). Next, we develop a series of algorithms for mining the complete set of frequent temporal patterns in a temporal network data set. We also consider four different definitions of isomorphism to allow noise tolerance in temporal data collection. Implementing the algorithm for three real-world data sets proves the practicality of the proposed algorithm and its capability to discover unknown patterns in various settings.

</p>
</details>

<details><summary><b>Unsupervised Representation Learning from Pathology Images with Multi-directional Contrastive Predictive Coding</b>
<a href="https://arxiv.org/abs/2105.05345">arxiv:2105.05345</a>
&#x1F4C8; 2 <br>
<p>Jacob Carse, Frank Carey, Stephen McKenna</p></summary>
<p>

**Abstract:** Digital pathology tasks have benefited greatly from modern deep learning algorithms. However, their need for large quantities of annotated data has been identified as a key challenge. This need for data can be countered by using unsupervised learning in situations where data are abundant but access to annotations is limited. Feature representations learned from unannotated data using contrastive predictive coding (CPC) have been shown to enable classifiers to obtain state of the art performance from relatively small amounts of annotated computer vision data. We present a modification to the CPC framework for use with digital pathology patches. This is achieved by introducing an alternative mask for building the latent context and using a multi-directional PixelCNN autoregressor. To demonstrate our proposed method we learn feature representations from the Patch Camelyon histology dataset. We show that our proposed modification can yield improved deep classification of histology patches.

</p>
</details>

<details><summary><b>Neuro-Symbolic Artificial Intelligence: Current Trends</b>
<a href="https://arxiv.org/abs/2105.05330">arxiv:2105.05330</a>
&#x1F4C8; 2 <br>
<p>Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, Pascal Hitzler</p></summary>
<p>

**Abstract:** Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods with methods that are based on artificial neural networks -- has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic.

</p>
</details>

<details><summary><b>Comparing interpretability and explainability for feature selection</b>
<a href="https://arxiv.org/abs/2105.05328">arxiv:2105.05328</a>
&#x1F4C8; 2 <br>
<p>Jack Dunn, Luca Mingardi, Ying Daisy Zhuo</p></summary>
<p>

**Abstract:** A common approach for feature selection is to examine the variable importance scores for a machine learning model, as a way to understand which features are the most relevant for making predictions. Given the significance of feature selection, it is crucial for the calculated importance scores to reflect reality. Falsely overestimating the importance of irrelevant features can lead to false discoveries, while underestimating importance of relevant features may lead us to discard important features, resulting in poor model performance. Additionally, black-box models like XGBoost provide state-of-the art predictive performance, but cannot be easily understood by humans, and thus we rely on variable importance scores or methods for explainability like SHAP to offer insight into their behavior.
  In this paper, we investigate the performance of variable importance as a feature selection method across various black-box and interpretable machine learning methods. We compare the ability of CART, Optimal Trees, XGBoost and SHAP to correctly identify the relevant subset of variables across a number of experiments. The results show that regardless of whether we use the native variable importance method or SHAP, XGBoost fails to clearly distinguish between relevant and irrelevant features. On the other hand, the interpretable methods are able to correctly and efficiently identify irrelevant features, and thus offer significantly better performance for feature selection.

</p>
</details>

<details><summary><b>Intelligent interactive technologies for mental health and well-being</b>
<a href="https://arxiv.org/abs/2105.05306">arxiv:2105.05306</a>
&#x1F4C8; 2 <br>
<p>Mladjan Jovanovic, Aleksandar Jevremovic, Milica Pejovic-Milovancevic</p></summary>
<p>

**Abstract:** Mental healthcare has seen numerous benefits from interactive technologies and artificial intelligence. Various interventions have successfully used intelligent technologies to automate the assessment and evaluation of psychological treatments and mental well-being and functioning. These technologies include different types of robots, video games, and conversational agents. The paper critically analyzes existing solutions with the outlooks for their future. In particular, we: i)give an overview of the technology for mental health, ii) critically analyze the technology against the proposed criteria, and iii) provide the design outlooks for these technologies.

</p>
</details>

<details><summary><b>Development of a Multi-Task Learning V-Net for Pulmonary Lobar Segmentation on Computed Tomography and Application to Diseased Lungs</b>
<a href="https://arxiv.org/abs/2105.05204">arxiv:2105.05204</a>
&#x1F4C8; 2 <br>
<p>Marc Boubnovski Martell, Mitchell Chen, Kristofer Linton-Reid, Joram M. Posma, Susan J Copley, Eric O. Aboagye</p></summary>
<p>

**Abstract:** Automated lobar segmentation allows regional evaluation of lung disease and is important for diagnosis and therapy planning. Advanced statistical workflows permitting such evaluation is a needed area within respiratory medicine; their adoption remains slow, with poor workflow accuracy. Diseased lung regions often produce high-density zones on CT images, limiting an algorithm's execution to specify damaged lobes due to oblique or lacking fissures. This impact motivated developing an improved machine learning method to segment lung lobes that utilises tracheobronchial tree information to enhance segmentation accuracy through the algorithm's spatial familiarity to define lobar extent more accurately. The method undertakes parallel segmentation of lobes and auxiliary tissues simultaneously by employing multi-task learning (MTL) in conjunction with V-Net-attention, a popular convolutional neural network in the imaging realm. In keeping with the model's adeptness for better generalisation, high performance was retained in an external dataset of patients with four distinct diseases: severe lung cancer, COVID-19 pneumonitis, collapsed lungs and Chronic Obstructive Pulmonary Disease (COPD), even though the training data included none of these cases. The benefit of our external validation test is specifically relevant since our choice includes those patients who have diagnosed lung disease with associated radiological abnormalities. To ensure equal rank is given to all segmentations in the main task we report the following performance (Dice score) on a per-segment basis: normal lungs 0.97, COPD 0.94, lung cancer 0.94, COVID-19 pneumonitis 0.94 and collapsed lung 0.92, all at p<0.05. Even segmenting lobes with large deformations on CT images, the model maintained high accuracy. The approach can be readily adopted in the clinical setting as a robust tool for radiologists.

</p>
</details>

<details><summary><b>Performance Comparison of Different Machine Learning Algorithms on the Prediction of Wind Turbine Power Generation</b>
<a href="https://arxiv.org/abs/2105.05197">arxiv:2105.05197</a>
&#x1F4C8; 2 <br>
<p>Onder Eyecioglu, Batuhan Hangun, Korhan Kayisli, Mehmet Yesilbudak</p></summary>
<p>

**Abstract:** Over the past decade, wind energy has gained more attention in the world. However, owing to its indirectness and volatility properties, wind power penetration has increased the difficulty and complexity in dispatching and planning of electric power systems. Therefore, it is needed to make the high-precision wind power prediction in order to balance the electrical power. For this purpose, in this study, the prediction performance of linear regression, k-nearest neighbor regression and decision tree regression algorithms is compared in detail. k-nearest neighbor regression algorithm provides lower coefficient of determination values, while decision tree regression algorithm produces lower mean absolute error values. In addition, the meteorological parameters of wind speed, wind direction, barometric pressure and air temperature are evaluated in terms of their importance on the wind power parameter. The biggest importance factor is achieved by wind speed parameter. In consequence, many useful assessments are made for wind power predictions.

</p>
</details>

<details><summary><b>Factoring Multidimensional Data to Create a Sophisticated Bayes Classifier</b>
<a href="https://arxiv.org/abs/2105.05181">arxiv:2105.05181</a>
&#x1F4C8; 2 <br>
<p>Anthony LaTorre</p></summary>
<p>

**Abstract:** In this paper we derive an explicit formula for calculating the marginal likelihood of a given factorization of a categorical dataset. Since the marginal likelihood is proportional to the posterior probability of the factorization, these likelihoods can be used to order all possible factorizations and select the "best" way to factor the overall distribution from which the dataset is drawn. The best factorization can then be used to construct a Bayes classifier which benefits from factoring out mutually independent sets of variables.

</p>
</details>

<details><summary><b>A Twin Neural Model for Uplift</b>
<a href="https://arxiv.org/abs/2105.05146">arxiv:2105.05146</a>
&#x1F4C8; 2 <br>
<p>Mouloud Belbahri, Olivier Gandouet, Alejandro Murua, Vahid Partovi Nia</p></summary>
<p>

**Abstract:** Uplift is a particular case of conditional treatment effect modeling. Such models deal with cause-and-effect inference for a specific factor, such as a marketing intervention or a medical treatment. In practice, these models are built on individual data from randomized clinical trials where the goal is to partition the participants into heterogeneous groups depending on the uplift. Most existing approaches are adaptations of random forests for the uplift case. Several split criteria have been proposed in the literature, all relying on maximizing heterogeneity. However, in practice, these approaches are prone to overfitting. In this work, we bring a new vision to uplift modeling. We propose a new loss function defined by leveraging a connection with the Bayesian interpretation of the relative risk. Our solution is developed for a specific twin neural network architecture allowing to jointly optimize the marginal probabilities of success for treated and control individuals. We show that this model is a generalization of the uplift logistic interaction model. We modify the stochastic gradient descent algorithm to allow for structured sparse solutions. This helps training our uplift models to a great extent. We show our proposed method is competitive with the state-of-the-art in simulation setting and on real data from large scale randomized experiments.

</p>
</details>

<details><summary><b>kdehumor at semeval-2020 task 7: a neural network model for detecting funniness in dataset humicroedit</b>
<a href="https://arxiv.org/abs/2105.05135">arxiv:2105.05135</a>
&#x1F4C8; 2 <br>
<p>Rida Miraj, Masaki Aono</p></summary>
<p>

**Abstract:** This paper describes our contribution to SemEval-2020 Task 7: Assessing Humor in Edited News Headlines. Here we present a method based on a deep neural network. In recent years, quite some attention has been devoted to humor production and perception. Our team KdeHumor employs recurrent neural network models including Bi-Directional LSTMs (BiLSTMs). Moreover, we utilize the state-of-the-art pre-trained sentence embedding techniques. We analyze the performance of our method and demonstrate the contribution of each component of our architecture.

</p>
</details>

<details><summary><b>Analysis of One-Hidden-Layer Neural Networks via the Resolvent Method</b>
<a href="https://arxiv.org/abs/2105.05115">arxiv:2105.05115</a>
&#x1F4C8; 2 <br>
<p>Vanessa Piccolo, Dominik Schröder</p></summary>
<p>

**Abstract:** In this work, we investigate the asymptotic spectral density of the random feature matrix $M = Y Y^\ast$ with $Y = f(WX)$ generated by a single-hidden-layer neural network, where $W$ and $X$ are random rectangular matrices with i.i.d. centred entries and $f$ is a non-linear smooth function which is applied entry-wise. We prove that the Stieltjes transform of the limiting spectral distribution approximately satisfies a quartic self-consistent equation, which is exactly the equation obtained by [Pennington, Worah] and [Benigni, Péché] with the moment method. We extend the previous results to the case of additive bias $Y=f(WX+B)$ with $B$ being an independent rank-one Gaussian random matrix, closer modelling the neural network infrastructures encountered in practice. Our key finding is that in the case of additive bias it is impossible to choose an activation function preserving the layer-to-layer singular value distribution, in sharp contrast to the bias-free case where a simple integral constraint is sufficient to achieve isospectrality. To obtain the asymptotics for the empirical spectral density we follow the resolvent method from random matrix theory via the cumulant expansion. We find that this approach is more robust and less combinatorial than the moment method and expect that it will apply also for models where the combinatorics of the former become intractable. The resolvent method has been widely employed, but compared to previous works, it is applied here to non-linear random matrices.

</p>
</details>

<details><summary><b>Integrating extracted information from bert and multiple embedding methods with the deep neural network for humour detection</b>
<a href="https://arxiv.org/abs/2105.05112">arxiv:2105.05112</a>
&#x1F4C8; 2 <br>
<p>Rida Miraj, Masaki Aono</p></summary>
<p>

**Abstract:** Humour detection from sentences has been an interesting and challenging task in the last few years. In attempts to highlight humour detection, most research was conducted using traditional approaches of embedding, e.g., Word2Vec or Glove. Recently BERT sentence embedding has also been used for this task. In this paper, we propose a framework for humour detection in short texts taken from news headlines. Our proposed framework (IBEN) attempts to extract information from written text via the use of different layers of BERT. After several trials, weights were assigned to different layers of the BERT model. The extracted information was then sent to a Bi-GRU neural network as an embedding matrix. We utilized the properties of some external embedding models. A multi-kernel convolution in our neural network was also employed to extract higher-level sentence representations. This framework performed very well on the task of humour detection.

</p>
</details>

<details><summary><b>Joint Text and Label Generation for Spoken Language Understanding</b>
<a href="https://arxiv.org/abs/2105.05052">arxiv:2105.05052</a>
&#x1F4C8; 2 <br>
<p>Yang Li, Ben Athiwaratkun, Cicero Nogueira dos Santos, Bing Xiang</p></summary>
<p>

**Abstract:** Generalization is a central problem in machine learning, especially when data is limited. Using prior information to enforce constraints is the principled way of encouraging generalization. In this work, we propose to leverage the prior information embedded in pretrained language models (LM) to improve generalization for intent classification and slot labeling tasks with limited training data. Specifically, we extract prior knowledge from pretrained LM in the form of synthetic data, which encode the prior implicitly. We fine-tune the LM to generate an augmented language, which contains not only text but also encodes both intent labels and slot labels. The generated synthetic data can be used to train a classifier later. Since the generated data may contain noise, we rephrase the learning from generated data as learning with noisy labels. We then utilize the mixout regularization for the classifier and prove its effectiveness to resist label noise in generated data. Empirically, our method demonstrates superior performance and outperforms the baseline by a large margin.

</p>
</details>

<details><summary><b>BikNN: Anomaly Estimation in Bilateral Domains with k-Nearest Neighbors</b>
<a href="https://arxiv.org/abs/2105.05037">arxiv:2105.05037</a>
&#x1F4C8; 2 <br>
<p>Zhongping Ji</p></summary>
<p>

**Abstract:** In this paper, a novel framework for anomaly estimation is proposed. The basic idea behind our method is to reduce the data into a two-dimensional space and then rank each data point in the reduced space. We attempt to estimate the degree of anomaly in both spatial and density domains. Specifically, we transform the data points into a density space and measure the distances in density domain between each point and its k-Nearest Neighbors in spatial domain. Then, an anomaly coordinate system is built by collecting two unilateral anomalies from k-nearest neighbors of each point. Further more, we introduce two schemes to model their correlation and combine them to get the final anomaly score. Experiments performed on the synthetic and real world datasets demonstrate that the proposed method performs well and achieve highest average performance. We also show that the proposed method can provide visualization and classification of the anomalies in a simple manner. Due to the complexity of the anomaly, none of the existing methods can perform best on all benchmark datasets. Our method takes into account both the spatial domain and the density domain and can be adapted to different datasets by adjusting a few parameters manually.

</p>
</details>

<details><summary><b>Gradient flow encoding with distance optimization adaptive step size</b>
<a href="https://arxiv.org/abs/2105.05031">arxiv:2105.05031</a>
&#x1F4C8; 2 <br>
<p>Kyriakos Flouris, Anna Volokitin, Gustav Bredell, Ender Konukoglu</p></summary>
<p>

**Abstract:** The autoencoder model uses an encoder to map data samples to a lower dimensional latent space and then a decoder to map the latent space representations back to the data space. Implicitly, it relies on the encoder to approximate the inverse of the decoder network, so that samples can be mapped to and back from the latent space faithfully. This approximation may lead to sub-optimal latent space representations. In this work, we investigate a decoder-only method that uses gradient flow to encode data samples in the latent space. The gradient flow is defined based on a given decoder and aims to find the optimal latent space representation for any given sample through optimisation, eliminating the need of an approximate inversion through an encoder. Implementing gradient flow through ordinary differential equations (ODE), we leverage the adjoint method to train a given decoder. We further show empirically that the costly integrals in the adjoint method may not be entirely necessary. Additionally, we propose a $2^{nd}$ order ODE variant to the method, which approximates Nesterov's accelerated gradient descent, with faster convergence per iteration. Commonly used ODE solvers can be quite sensitive to the integration step-size depending on the stiffness of the ODE. To overcome the sensitivity for gradient flow encoding, we use an adaptive solver that prioritises minimising loss at each integration step. We assess the proposed method in comparison to the autoencoding model. In our experiments, GFE showed a much higher data-efficiency than the autoencoding model, which can be crucial for data scarce applications.

</p>
</details>

<details><summary><b>More Powerful Conditional Selective Inference for Generalized Lasso by Parametric Programming</b>
<a href="https://arxiv.org/abs/2105.04920">arxiv:2105.04920</a>
&#x1F4C8; 2 <br>
<p>Vo Nguyen Le Duy, Ichiro Takeuchi</p></summary>
<p>

**Abstract:** Conditional selective inference (SI) has been studied intensively as a new statistical inference framework for data-driven hypotheses. The basic concept of conditional SI is to make the inference conditional on the selection event, which enables an exact and valid statistical inference to be conducted even when the hypothesis is selected based on the data. Conditional SI has mainly been studied in the context of model selection, such as vanilla lasso or generalized lasso. The main limitation of existing approaches is the low statistical power owing to over-conditioning, which is required for computational tractability. In this study, we propose a more powerful and general conditional SI method for a class of problems that can be converted into quadratic parametric programming, which includes generalized lasso. The key concept is to compute the continuum path of the optimal solution in the direction of the selected test statistic and to identify the subset of the data space that corresponds to the model selection event by following the solution path. The proposed parametric programming-based method not only avoids the aforementioned major drawback of over-conditioning, but also improves the performance and practicality of SI in various respects. We conducted several experiments to demonstrate the effectiveness and efficiency of our proposed method.

</p>
</details>

<details><summary><b>Improving the Transient Times for Distributed Stochastic Gradient Methods</b>
<a href="https://arxiv.org/abs/2105.04851">arxiv:2105.04851</a>
&#x1F4C8; 2 <br>
<p>Kun Huang, Shi Pu</p></summary>
<p>

**Abstract:** We consider the distributed optimization problem where $n$ agents each possessing a local cost function, collaboratively minimize the average of the $n$ cost functions over a connected network. Assuming stochastic gradient information is available, we study a distributed stochastic gradient algorithm, called exact diffusion with adaptive stepsizes (EDAS) adapted from the Exact Diffusion method and NIDS and perform a non-asymptotic convergence analysis. We not only show that EDAS asymptotically achieves the same network independent convergence rate as centralized stochastic gradient descent (SGD) for minimizing strongly convex and smooth objective functions, but also characterize the transient time needed for the algorithm to approach the asymptotic convergence rate, which behaves as $K_T=\mathcal{O}\left(\frac{n}{1-λ_2}\right)$, where $1-λ_2$ stands for the spectral gap of the mixing matrix. To the best of our knowledge, EDAS achieves the shortest transient time when the average of the $n$ cost functions is strongly convex and each cost function is smooth. Numerical simulations further corroborate and strengthen the obtained theoretical results.

</p>
</details>

<details><summary><b>Spectral risk-based learning using unbounded losses</b>
<a href="https://arxiv.org/abs/2105.04816">arxiv:2105.04816</a>
&#x1F4C8; 2 <br>
<p>Matthew J. Holland, El Mehdi Haress</p></summary>
<p>

**Abstract:** In this work, we consider the setting of learning problems under a wide class of spectral risk (or "L-risk") functions, where a Lipschitz-continuous spectral density is used to flexibly assign weight to extreme loss values. We obtain excess risk guarantees for a derivative-free learning procedure under unbounded heavy-tailed loss distributions, and propose a computationally efficient implementation which empirically outperforms traditional risk minimizers in terms of balancing spectral risk and misclassification error.

</p>
</details>

<details><summary><b>Deep scattering network for speech emotion recognition</b>
<a href="https://arxiv.org/abs/2105.04806">arxiv:2105.04806</a>
&#x1F4C8; 2 <br>
<p>Premjeet Singh, Goutam Saha, Md Sahidullah</p></summary>
<p>

**Abstract:** This paper introduces scattering transform for speech emotion recognition (SER). Scattering transform generates feature representations which remain stable to deformations and shifting in time and frequency without much loss of information. In speech, the emotion cues are spread across time and localised in frequency. The time and frequency invariance characteristic of scattering coefficients provides a representation robust against emotion irrelevant variations e.g., different speakers, language, gender etc. while preserving the variations caused by emotion cues. Hence, such a representation captures the emotion information more efficiently from speech. We perform experiments to compare scattering coefficients with standard mel-frequency cepstral coefficients (MFCCs) over different databases. It is observed that frequency scattering performs better than time-domain scattering and MFCCs. We also investigate layer-wise scattering coefficients to analyse the importance of time shift and deformation stable scalogram and modulation spectrum coefficients for SER. We observe that layer-wise coefficients taken independently also perform better than MFCCs.

</p>
</details>

<details><summary><b>Deep learning in physics: a study of dielectric quasi-cubic particles in a uniform electric field</b>
<a href="https://arxiv.org/abs/2105.09866">arxiv:2105.09866</a>
&#x1F4C8; 1 <br>
<p>Zhe Wang, Claude Guet</p></summary>
<p>

**Abstract:** Solving physics problems for which we know the equations, boundary conditions and symmetries can be done by deep learning. The constraints can be either imposed as terms in a loss function or used to formulate a neural ansatz. In the present case study, we calculate the induced field inside and outside a dielectric cube placed in a uniform electric field, wherein the dielectric mismatch at edges and corners of the cube makes accurate calculations numerically challenging. The electric potential is expressed as an ansatz incorporating neural networks with known leading order behaviors and symmetries and the Laplace's equation is then solved with boundary conditions at the dielectric interface by minimizing a loss function. The loss function ensures that both Laplace's equation and boundary conditions are satisfied everywhere inside a large solution domain. We study how the electric potential inside and outside a quasi-cubic particle evolves through a sequence of shapes from a sphere to a cube. The neural network being differentiable, it is straightforward to calculate the electric field over the whole domain, the induced surface charge distribution and the polarizability. The neural network being retentive, one can efficiently follow how the field changes upon particle's shape or dielectric constant by iterating from any previously converged solution. The present work's objective is two-fold, first to show how an a priori knowledge can be incorporated into neural networks to achieve efficient learning and second to apply the method and study how the induced field and polarizability change when a dielectric particle progressively changes its shape from a sphere to a cube.

</p>
</details>

<details><summary><b>Machine Assistance for Credit Card Approval? Random Wheel can Recommend and Explain</b>
<a href="https://arxiv.org/abs/2105.06255">arxiv:2105.06255</a>
&#x1F4C8; 1 <br>
<p>Anupam Khan, Soumya K. Ghosh</p></summary>
<p>

**Abstract:** Approval of credit card application is one of the censorious business decision the bankers are usually taking regularly. The growing number of new card applications and the enormous outstanding amount of credit card bills during the recent pandemic make this even more challenging nowadays. Some of the previous studies suggest the usage of machine intelligence for automating the approval process to mitigate this challenge. However, the effectiveness of such automation may depend on the richness of the training dataset and model efficiency. We have recently developed a novel classifier named random wheel which provides a more interpretable output. In this work, we have used an enhanced version of random wheel to facilitate a trustworthy recommendation for credit card approval process. It not only produces more accurate and precise recommendation but also provides an interpretable confidence measure. Besides, it explains the machine recommendation for each credit card application as well. The availability of recommendation confidence and explanation could bring more trust in the machine provided intelligence which in turn can enhance the efficiency of the credit card approval process.

</p>
</details>

<details><summary><b>Variants on Block Design Based Gradient Codes for Adversarial Stragglers</b>
<a href="https://arxiv.org/abs/2105.05231">arxiv:2105.05231</a>
&#x1F4C8; 1 <br>
<p>Animesh Sakorikar, Lele Wang</p></summary>
<p>

**Abstract:** Gradient coding is a coding theoretic framework to provide robustness against slow or unresponsive machines, known as stragglers, in distributed machine learning applications. Recently, Kadhe et al. proposed a gradient code based on a combinatorial design, called balanced incomplete block design (BIBD), which is shown to outperform many existing gradient codes in worst-case adversarial straggling scenarios. However, parameters for which such BIBD constructions exist are very limited. In this paper, we aim to overcome such limitations and construct gradient codes which exist for a wide range of parameters while retaining the superior performance of BIBD gradient codes. Two such constructions are proposed, one based on a probabilistic construction that relax the stringent BIBD gradient code constraints, and the other based on taking the Kronecker product of existing gradient codes. Theoretical error bounds for worst-case adversarial straggling scenarios are derived. Simulations show that the proposed constructions can outperform existing gradient codes with similar redundancy per data piece.

</p>
</details>

<details><summary><b>On the Renyi Differential Privacy of the Shuffle Model</b>
<a href="https://arxiv.org/abs/2105.05180">arxiv:2105.05180</a>
&#x1F4C8; 1 <br>
<p>Antonious M. Girgis, Deepesh Data, Suhas Diggavi, Ananda Theertha Suresh, Peter Kairouz</p></summary>
<p>

**Abstract:** The central question studied in this paper is Renyi Differential Privacy (RDP) guarantees for general discrete local mechanisms in the shuffle privacy model. In the shuffle model, each of the $n$ clients randomizes its response using a local differentially private (LDP) mechanism and the untrusted server only receives a random permutation (shuffle) of the client responses without association to each client. The principal result in this paper is the first non-trivial RDP guarantee for general discrete local randomization mechanisms in the shuffled privacy model, and we develop new analysis techniques for deriving our results which could be of independent interest. In applications, such an RDP guarantee is most useful when we use it for composing several private interactions. We numerically demonstrate that, for important regimes, with composition our bound yields an improvement in privacy guarantee by a factor of $8\times$ over the state-of-the-art approximate Differential Privacy (DP) guarantee (with standard composition) for shuffled models. Moreover, combining with Poisson subsampling, our result leads to at least $10\times$ improvement over subsampled approximate DP with standard composition.

</p>
</details>

<details><summary><b>U-Net-Based Surrogate Model For Evaluation of Microfluidic Channels</b>
<a href="https://arxiv.org/abs/2105.05173">arxiv:2105.05173</a>
&#x1F4C8; 1 <br>
<p>Quang Tuyen Le, Pao-Hsiung Chiu, Chin Chun Ooi</p></summary>
<p>

**Abstract:** Microfluidics have shown great promise in multiple applications, especially in biomedical diagnostics and separations. While the flow properties of these microfluidic devices can be solved by numerical methods such as computational fluid dynamics (CFD), the process of mesh generation and setting up a numerical solver requires some domain familiarity, while more intuitive commercial programs such as Fluent and StarCCM can be expensive. Hence, in this work, we demonstrated the use of a U-Net convolutional neural network as a surrogate model for predicting the velocity and pressure fields that would result for a particular set of microfluidic filter designs. The surrogate model is fast, easy to set-up and can be used to predict and assess the flow velocity and pressure fields across the domain for new designs of interest via the input of a geometry-encoding matrix. In addition, we demonstrate that the same methodology can also be used to train a network to predict pressure based on velocity data, and propose that this can be an alternative to numerical algorithms for calculating pressure based on velocity measurements from particle-image velocimetry measurements. Critically, in both applications, we demonstrate prediction test errors of less than 1%, suggesting that this is indeed a viable method.

</p>
</details>

<details><summary><b>Towards a Model for LSH</b>
<a href="https://arxiv.org/abs/2105.05130">arxiv:2105.05130</a>
&#x1F4C8; 1 <br>
<p>Li Wang</p></summary>
<p>

**Abstract:** As data volumes continue to grow, clustering and outlier detection algorithms are becoming increasingly time-consuming. Classical index structures for neighbor search are no longer sustainable due to the "curse of dimensionality". Instead, approximated index structures offer a good opportunity to significantly accelerate the neighbor search for clustering and outlier detection and to have the lowest possible error rate in the results of the algorithms. Locality-sensitive hashing is one of those. We indicate directions to model the properties of LSH.

</p>
</details>

<details><summary><b>Weighted Hierarchical Sparse Representation for Hyperspectral Target Detection</b>
<a href="https://arxiv.org/abs/2105.04990">arxiv:2105.04990</a>
&#x1F4C8; 1 <br>
<p>Chenlu Wei, Zhiyu Jiang, Yuan Yuan</p></summary>
<p>

**Abstract:** Hyperspectral target detection has been widely studied in the field of remote sensing. However, background dictionary building issue and the correlation analysis of target and background dictionary issue have not been well studied. To tackle these issues, a \emph{Weighted Hierarchical Sparse Representation} for hyperspectral target detection is proposed. The main contributions of this work are listed as follows. 1) Considering the insufficient representation of the traditional background dictionary building by dual concentric window structure, a hierarchical background dictionary is built considering the local and global spectral information simultaneously. 2) To reduce the impureness impact of background dictionary, target scores from target dictionary and background dictionary are weighted considered according to the dictionary quality. Three hyperspectral target detection data sets are utilized to verify the effectiveness of the proposed method. And the experimental results show a better performance when compared with the state-of-the-arts.

</p>
</details>

<details><summary><b>Task-Related Self-Supervised Learning for Remote Sensing Image Change Detection</b>
<a href="https://arxiv.org/abs/2105.04951">arxiv:2105.04951</a>
&#x1F4C8; 1 <br>
<p>Zhinan Cai, Zhiyu Jiang, Yuan Yuan</p></summary>
<p>

**Abstract:** Change detection for remote sensing images is widely applied for urban change detection, disaster assessment and other fields. However, most of the existing CNN-based change detection methods still suffer from the problem of inadequate pseudo-changes suppression and insufficient feature representation. In this work, an unsupervised change detection method based on Task-related Self-supervised Learning Change Detection network with smooth mechanism(TSLCD) is proposed to eliminate it. The main contributions include: (1) the task-related self-supervised learning module is introduced to extract spatial features more effectively. (2) a hard-sample-mining loss function is applied to pay more attention to the hard-to-classify samples. (3) a smooth mechanism is utilized to remove some of pseudo-changes and noise. Experiments on four remote sensing change detection datasets reveal that the proposed TSLCD method achieves the state-of-the-art for change detection task.

</p>
</details>

<details><summary><b>A Hybrid Decomposition-based Multi-objective Evolutionary Algorithm for the Multi-Point Dynamic Aggregation Problem</b>
<a href="https://arxiv.org/abs/2105.04934">arxiv:2105.04934</a>
&#x1F4C8; 1 <br>
<p>Guanqiang Gao, Bin Xin, Yi Mei, Shuxin Ding, Juan Li</p></summary>
<p>

**Abstract:** An emerging optimisation problem from the real-world applications, named the multi-point dynamic aggregation (MPDA) problem, has become one of the active research topics of the multi-robot system. This paper focuses on a multi-objective MPDA problem which is to design an execution plan of the robots to minimise the number of robots and the maximal completion time of all the tasks. The strongly-coupled relationships among robots and tasks, the redundancy of the MPDA encoding, and the variable-size decision space of the MO-MPDA problem posed extra challenges for addressing the problem effectively. To address the above issues, we develop a hybrid decomposition-based multi-objective evolutionary algorithm (HDMOEA) using $ \varepsilon $-constraint method. It selects the maximal completion time of all tasks as the main objective, and converted the other objective into constraints. HDMOEA decomposes a MO-MPDA problem into a series of scalar constrained optimization subproblems by assigning each subproblem with an upper bound robot number. All the subproblems are optimized simultaneously with the transferring knowledge from other subproblems. Besides, we develop a hybrid population initialisation mechanism to enhance the quality of initial solutions, and a reproduction mechanism to transmit effective information and tackle the encoding redundancy. Experimental results show that the proposed HDMOEA method significantly outperforms the state-of-the-art methods in terms of several most-used metrics.

</p>
</details>

<details><summary><b>Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review</b>
<a href="https://arxiv.org/abs/2105.04881">arxiv:2105.04881</a>
&#x1F4C8; 1 <br>
<p>Afshin Shoeibi, Marjane Khodatars, Mahboobeh Jafari, Parisa Moridian, Mitra Rezaei, Roohallah Alizadehsani, Fahime Khozeimeh, Juan Manuel Gorriz, Jónathan Heras, Maryam Panahiazar, Saeid Nahavandi, U. Rajendra Acharya</p></summary>
<p>

**Abstract:** Multiple Sclerosis (MS) is a type of brain disease which causes visual, sensory, and motor problems for people with a detrimental effect on the functioning of the nervous system. In order to diagnose MS, multiple screening methods have been proposed so far; among them, magnetic resonance imaging (MRI) has received considerable attention among physicians. MRI modalities provide physicians with fundamental information about the structure and function of the brain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS using MRI is time-consuming, tedious, and prone to manual errors. Hence, computer aided diagnosis systems (CADS) based on artificial intelligence (AI) methods have been proposed in recent years for accurate diagnosis of MS using MRI neuroimaging modalities. In the AI field, automated MS diagnosis is being conducted using (i) conventional machine learning and (ii) deep learning (DL) techniques. The conventional machine learning approach is based on feature extraction and selection by trial and error. In DL, these steps are performed by the DL model itself. In this paper, a complete review of automated MS diagnosis methods performed using DL techniques with MRI neuroimaging modalities are discussed. Also, each work is thoroughly reviewed and discussed. Finally, the most important challenges and future directions in the automated MS diagnosis using DL techniques coupled with MRI modalities are presented in detail.

</p>
</details>

<details><summary><b>Transitioning to human interaction with AI systems: New challenges and opportunities for HCI professionals to enable human-centered AI</b>
<a href="https://arxiv.org/abs/2105.05424">arxiv:2105.05424</a>
&#x1F4C8; 0 <br>
<p>Wei Xu, Marvin J. Dainoff, Liezhong Ge, Zaifeng Gao</p></summary>
<p>

**Abstract:** While AI has benefited humans, it may also harm humans if not appropriately developed. The focus of HCI work is transiting from conventional human interaction with non-AI computing systems to interaction with AI systems. We conducted a high-level literature review and a holistic analysis of current work in developing AI systems from an HCI perspective. Our review and analysis highlight the new changes introduced by AI technology and the new challenges that HCI professionals face when applying the human-centered AI (HCAI) approach in the development of AI systems. We also identified seven main issues in human interaction with AI systems, which HCI professionals did not encounter when developing non-AI computing systems. To further enable the implementation of the HCAI approach, we identified new HCI opportunities tied to specific HCAI-driven design goals to guide HCI professionals in addressing these new issues. Finally, our assessment of current HCI methods shows the limitations of these methods in support of developing AI systems. We propose alternative methods that can help overcome these limitations and effectively help HCI professionals apply the HCAI approach to the development of AI systems. We also offer strategic recommendations for HCI professionals to effectively influence the development of AI systems with the HCAI approach, eventually developing HCAI systems.

</p>
</details>

<details><summary><b>Homogeneous vector bundles and $G$-equivariant convolutional neural networks</b>
<a href="https://arxiv.org/abs/2105.05400">arxiv:2105.05400</a>
&#x1F4C8; 0 <br>
<p>Jimmy Aronsson</p></summary>
<p>

**Abstract:** $G$-equivariant convolutional neural networks (GCNNs) is a geometric deep learning model for data defined on a homogeneous $G$-space $\mathcal{M}$. GCNNs are designed to respect the global symmetry in $\mathcal{M}$, thereby facilitating learning. In this paper, we analyze GCNNs on homogeneous spaces $\mathcal{M} = G/K$ in the case of unimodular Lie groups $G$ and compact subgroups $K \leq G$. We demonstrate that homogeneous vector bundles is the natural setting for GCNNs. We also use reproducing kernel Hilbert spaces to obtain a precise criterion for expressing $G$-equivariant layers as convolutional layers. This criterion is then rephrased as a bandwidth criterion, leading to even stronger results for some groups.

</p>
</details>

<details><summary><b>Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective</b>
<a href="https://arxiv.org/abs/2105.05381">arxiv:2105.05381</a>
&#x1F4C8; 0 <br>
<p>Shahbaz Rezaei, Zubair Shafiq, Xin Liu</p></summary>
<p>

**Abstract:** Deep ensemble learning has been shown to improve accuracy by training multiple neural networks and fusing their outputs. Ensemble learning has also been used to defend against membership inference attacks that undermine privacy. In this paper, we empirically demonstrate a trade-off between these two goals, namely accuracy and privacy (in terms of membership inference attacks), in deep ensembles. Using a wide range of datasets and model architectures, we show that the effectiveness of membership inference attacks also increases when ensembling improves accuracy. To better understand this trade-off, we study the impact of various factors such as prediction confidence and agreement between models that constitute the ensemble. Finally, we evaluate defenses against membership inference attacks based on regularization and differential privacy. We show that while these defenses can mitigate the effectiveness of the membership inference attack, they simultaneously degrade ensemble accuracy. We illustrate similar trade-off in more advanced and state-of-the-art ensembling techniques, such as snapshot ensembles and diversified ensemble networks. The source code is available in supplementary materials.

</p>
</details>

<details><summary><b>Two novel feature selection algorithms based on crowding distance</b>
<a href="https://arxiv.org/abs/2105.05212">arxiv:2105.05212</a>
&#x1F4C8; 0 <br>
<p>Abdesslem Layeb</p></summary>
<p>

**Abstract:** In this paper, two novel algorithms for features selection are proposed. The first one is a filter method while the second is wrapper method. Both the proposed algorithms use the crowding distance used in the multiobjective optimization as a metric in order to sort the features. The less crowded features have great effects on the target attribute (class). The experimental results have shown the effectiveness and the robustness of the proposed algorithms.

</p>
</details>

<details><summary><b>Segmentation of Anatomical Layers and Artifacts in Intravascular Polarization Sensitive Optical Coherence Tomography Using Attending Physician and Boundary Cardinality Losses</b>
<a href="https://arxiv.org/abs/2105.05137">arxiv:2105.05137</a>
&#x1F4C8; 0 <br>
<p>Mohammad Haft-Javaherian, Martin Villiger, Kenichiro Otsuka, Joost Daemen, Peter Libby, Polina Golland, Brett E. Bouma</p></summary>
<p>

**Abstract:** Intravascular ultrasound and optical coherence tomography are widely available for characterizing coronary stenoses and provide critical vessel parameters to optimize percutaneous intervention. Intravascular polarization-sensitive optical coherence tomography (PS-OCT) simultaneously provides high-resolution cross-sectional images of vascular structures while also revealing preponderant tissue components such as collagen and smooth muscle and thereby enhances plaque characterization. Automated interpretation of these features promises to facilitate the objective clinical investigation of the natural history and significance of coronary atheromas. Here, we propose a convolutional neural network model, optimized using a new multi-term loss function, to classify the lumen, intima, and media layers in addition to the guidewire and plaque shadows. We demonstrate that our multi-class classification model outperforms state-of-the-art methods in detecting the coronary anatomical layers. Furthermore, the proposed model segments two classes of common imaging artifacts and detects the anatomical layers within the thickened vessel wall regions that were excluded from analysis by other studies. The source code and the trained model are publicly available at https://github.com/mhaft/OCTseg

</p>
</details>

<details><summary><b>ANDREAS: Artificial intelligence traiNing scheDuler foR accElerAted resource clusterS</b>
<a href="https://arxiv.org/abs/2105.05080">arxiv:2105.05080</a>
&#x1F4C8; 0 <br>
<p>Federica Filippini, Danilo Ardagna, Marco Lattuada, Edoardo Amaldi, Michele Ciavotta, Maciek Riedl, Katarzyna Materka, Paweł Skrzypek, Fabrizio Magugliani, Marco Cicala</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) and Deep Learning (DL) algorithms are currently applied to a wide range of products and solutions. DL training jobs are highly resource demanding and they experience great benefits when exploiting AI accelerators (e.g., GPUs). However, the effective management of GPU-powered clusters comes with great challenges. Among these, efficient scheduling and resource allocation solutions are crucial to maximize performance and minimize Data Centers operational costs. In this paper we propose ANDREAS, an advanced scheduling solution that tackles these problems jointly, aiming at optimizing DL training runtime workloads and their energy consumption in accelerated clusters. Experiments based on simulation demostrate that we can achieve a cost reduction between 30 and 62% on average with respect to first-principle methods while the validation on a real cluster shows a worst case deviation below 13% between actual and predicted costs, proving the effectiveness of ANDREAS solution in practical scenarios.

</p>
</details>

<details><summary><b>Surrogate assisted active subspace and active subspace assisted surrogate -- A new paradigm for high dimensional structural reliability analysis</b>
<a href="https://arxiv.org/abs/2105.04979">arxiv:2105.04979</a>
&#x1F4C8; 0 <br>
<p>Navaneeth N., Souvik Chakraborty</p></summary>
<p>

**Abstract:** Performing reliability analysis on complex systems is often computationally expensive. In particular, when dealing with systems having high input dimensionality, reliability estimation becomes a daunting task. A popular approach to overcome the problem associated with time-consuming and expensive evaluations is building a surrogate model. However, these computationally efficient models often suffer from the curse of dimensionality. Hence, training a surrogate model for high-dimensional problems is not straightforward. Henceforth, this paper presents a framework for solving high-dimensional reliability analysis problems. The basic premise is to train the surrogate model on a low-dimensional manifold, discovered using the active subspace algorithm. However, learning the low-dimensional manifold using active subspace is non-trivial as it requires information on the gradient of the response variable. To address this issue, we propose using sparse learning algorithms in conjunction with the active subspace algorithm; the resulting algorithm is referred to as the sparse active subspace (SAS) algorithm. We project the high-dimensional inputs onto the identified low-dimensional manifold identified using SAS. A high-fidelity surrogate model is used to map the inputs on the low-dimensional manifolds to the output response. We illustrate the efficacy of the proposed framework by using three benchmark reliability analysis problems from the literature. The results obtained indicate the accuracy and efficiency of the proposed approach compared to already established reliability analysis methods in the literature.

</p>
</details>

<details><summary><b>On Characterizing GAN Convergence Through Proximal Duality Gap</b>
<a href="https://arxiv.org/abs/2105.04801">arxiv:2105.04801</a>
&#x1F4C8; 0 <br>
<p>Sahil Sidheekh, Aroof Aimen, Narayanan C. Krishnan</p></summary>
<p>

**Abstract:** Despite the accomplishments of Generative Adversarial Networks (GANs) in modeling data distributions, training them remains a challenging task. A contributing factor to this difficulty is the non-intuitive nature of the GAN loss curves, which necessitates a subjective evaluation of the generated output to infer training progress. Recently, motivated by game theory, duality gap has been proposed as a domain agnostic measure to monitor GAN training. However, it is restricted to the setting when the GAN converges to a Nash equilibrium. But GANs need not always converge to a Nash equilibrium to model the data distribution. In this work, we extend the notion of duality gap to proximal duality gap that is applicable to the general context of training GANs where Nash equilibria may not exist. We show theoretically that the proximal duality gap is capable of monitoring the convergence of GANs to a wider spectrum of equilibria that subsumes Nash equilibria. We also theoretically establish the relationship between the proximal duality gap and the divergence between the real and generated data distributions for different GAN formulations. Our results provide new insights into the nature of GAN convergence. Finally, we validate experimentally the usefulness of proximal duality gap for monitoring and influencing GAN training.

</p>
</details>


[Next Page](2021/2021-05/2021-05-10.md)
