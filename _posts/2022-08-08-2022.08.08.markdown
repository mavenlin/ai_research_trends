Prev: [2022.08.07]({{ '/2022/08/07/2022.08.07.html' | relative_url }})  Next: [2022.08.09]({{ '/2022/08/09/2022.08.09.html' | relative_url }})
{% raw %}
## Summary for 2022-08-08, created on 2022-08-12


<details><summary><b>Object Detection with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2208.04511">arxiv:2208.04511</a>
&#x1F4C8; 200 <br>
<p>Manoosh Samiei, Ruofeng Li</p></summary>
<p>

**Abstract:** Object localization has been a crucial task in computer vision field. Methods of localizing objects in an image have been proposed based on the features of the attended pixels. Recently researchers have proposed methods to formulate object localization as a dynamic decision process, which can be solved by a reinforcement learning approach. In this project, we implement a novel active object localization algorithm based on deep reinforcement learning. We compare two different action settings for this MDP: a hierarchical method and a dynamic method. We further perform some ablation studies on the performance of the models by investigating different hyperparameters and various architecture changes.

</p>
</details>

<details><summary><b>NeuralVDB: High-resolution Sparse Volume Representation using Hierarchical Neural Networks</b>
<a href="https://arxiv.org/abs/2208.04448">arxiv:2208.04448</a>
&#x1F4C8; 38 <br>
<p>Doyub Kim, Minjae Lee, Ken Museth</p></summary>
<p>

**Abstract:** We introduce NeuralVDB, which improves on an existing industry standard for efficient storage of sparse volumetric data, denoted VDB, by leveraging recent advancements in machine learning. Our novel hybrid data structure can reduce the memory footprints of VDB volumes by orders of magnitude, while maintaining its flexibility and only incurring a small (user-controlled) compression errors. Specifically, NeuralVDB replaces the lower nodes of a shallow and wide VDB tree structure with multiple hierarchy neural networks that separately encode topology and value information by means of neural classifiers and regressors respectively. This approach has proven to maximize the compression ratio while maintaining the spatial adaptivity offered by the higher-level VDB data structure. For sparse signed distance fields and density volumes, we have observed compression ratios on the order of $10\times$ to more than $100\times$ from already compressed VDB inputs, with little to no visual artifacts. We also demonstrate how its application to animated sparse volumes can both accelerate training and generate temporally coherent neural networks.

</p>
</details>

<details><summary><b>Visual-Inertial Multi-Instance Dynamic SLAM with Object-level Relocalisation</b>
<a href="https://arxiv.org/abs/2208.04274">arxiv:2208.04274</a>
&#x1F4C8; 38 <br>
<p>Yifei Ren, Binbin Xu, Christopher L. Choi, Stefan Leutenegger</p></summary>
<p>

**Abstract:** In this paper, we present a tightly-coupled visual-inertial object-level multi-instance dynamic SLAM system. Even in extremely dynamic scenes, it can robustly optimise for the camera pose, velocity, IMU biases and build a dense 3D reconstruction object-level map of the environment. Our system can robustly track and reconstruct the geometries of arbitrary objects, their semantics and motion by incrementally fusing associated colour, depth, semantic, and foreground object probabilities into each object model thanks to its robust sensor and object tracking. In addition, when an object is lost or moved outside the camera field of view, our system can reliably recover its pose upon re-observation. We demonstrate the robustness and accuracy of our method by quantitatively and qualitatively testing it in real-world data sequences.

</p>
</details>

<details><summary><b>Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</b>
<a href="https://arxiv.org/abs/2208.04202">arxiv:2208.04202</a>
&#x1F4C8; 29 <br>
<p>Ting Chen, Ruixiang Zhang, Geoffrey Hinton</p></summary>
<p>

**Abstract:** We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.

</p>
</details>

<details><summary><b>Uncertain Bayesian Networks: Learning from Incomplete Data</b>
<a href="https://arxiv.org/abs/2208.04221">arxiv:2208.04221</a>
&#x1F4C8; 8 <br>
<p>Conrad D. Hougen, Lance M. Kaplan, Federico Cerutti, Alfred O. Hero III</p></summary>
<p>

**Abstract:** When the historical data are limited, the conditional probabilities associated with the nodes of Bayesian networks are uncertain and can be empirically estimated. Second order estimation methods provide a framework for both estimating the probabilities and quantifying the uncertainty in these estimates. We refer to these cases as uncer tain or second-order Bayesian networks. When such data are complete, i.e., all variable values are observed for each instantiation, the conditional probabilities are known to be Dirichlet-distributed. This paper improves the current state-of-the-art approaches for handling uncertain Bayesian networks by enabling them to learn distributions for their parameters, i.e., conditional probabilities, with incomplete data. We extensively evaluate various methods to learn the posterior of the parameters through the desired and empirically derived strength of confidence bounds for various queries.

</p>
</details>

<details><summary><b>Object Detection Using Sim2Real Domain Randomization for Robotic Applications</b>
<a href="https://arxiv.org/abs/2208.04171">arxiv:2208.04171</a>
&#x1F4C8; 8 <br>
<p>Dániel Horváth, Gábor Erdős, Zoltán Istenes, Tomáš Horváth, Sándor Földi</p></summary>
<p>

**Abstract:** Robots working in unstructured environments must be capable of sensing and interpreting their surroundings. One of the main obstacles of deep learning based models in the field of robotics is the lack of domain-specific labeled data for different industrial applications. In this paper, we propose a sim2real transfer learning method based on domain randomization for object detection with which labeled synthetic datasets of arbitrary size and object types can be automatically generated. Subsequently, a state-of-the-art convolutional neural network, YOLOv4, is trained to detect the different types of industrial objects. With the proposed domain randomization method, we could shrink the reality gap to a satisfactory level, achieving 86.32% and 97.38% mAP50 scores respectively in the case of zero-shot and one-shot transfers, on our manually annotated dataset containing 190 real images. On a GeForce RTX 2080 Ti GPU, the data generation process takes less than 0.5s per image and the training lasts around 12h which makes it convenient for industrial use. Our solution matches industrial needs as it can reliably differentiate similar classes of objects by using only 1 real image for training. To our best knowledge, this is the only work thus far satisfying these constraints.

</p>
</details>

<details><summary><b>Aerial Monocular 3D Object Detection</b>
<a href="https://arxiv.org/abs/2208.03974">arxiv:2208.03974</a>
&#x1F4C8; 8 <br>
<p>Yue Hu, Shaoheng Fang, Weidi Xie, Siheng Chen</p></summary>
<p>

**Abstract:** Drones equipped with cameras can significantly enhance human ability to perceive the world because of their remarkable maneuverability in 3D space. Ironically, object detection for drones has always been conducted in the 2D image space, which fundamentally limits their ability to understand 3D scenes. Furthermore, existing 3D object detection methods developed for autonomous driving cannot be directly applied to drones due to the lack of deformation modeling, which is essential for the distant aerial perspective with sensitive distortion and small objects. To fill the gap, this work proposes a dual-view detection system named DVDET to achieve aerial monocular object detection in both the 2D image space and the 3D physical space. To address the severe view deformation issue, we propose a novel trainable geo-deformable transformation module that can properly warp information from the drone's perspective to the BEV. Compared to the monocular methods for cars, our transformation includes a learnable deformable network for explicitly revising the severe deviation. To address the dataset challenge, we propose a new large-scale simulation dataset named AM3D-Sim, generated by the co-simulation of AirSIM and CARLA, and a new real-world aerial dataset named AM3D-Real, collected by DJI Matrice 300 RTK, in both datasets, high-quality annotations for 3D object detection are provided. Extensive experiments show that i) aerial monocular 3D object detection is feasible; ii) the model pre-trained on the simulation dataset benefits real-world performance, and iii) DVDET also benefits monocular 3D object detection for cars. To encourage more researchers to investigate this area, we will release the dataset and related code in https://sjtu-magic.github.io/dataset/AM3D/.

</p>
</details>

<details><summary><b>A Theoretical View on Sparsely Activated Networks</b>
<a href="https://arxiv.org/abs/2208.04461">arxiv:2208.04461</a>
&#x1F4C8; 7 <br>
<p>Cenk Baykal, Nishanth Dikkala, Rina Panigrahy, Cyrus Rashtchian, Xin Wang</p></summary>
<p>

**Abstract:** Deep and wide neural networks successfully fit very complex functions today, but dense models are starting to be prohibitively expensive for inference. To mitigate this, one promising direction is networks that activate a sparse subgraph of the network. The subgraph is chosen by a data-dependent routing function, enforcing a fixed mapping of inputs to subnetworks (e.g., the Mixture of Experts (MoE) paradigm in Switch Transformers). However, prior work is largely empirical, and while existing routing functions work well in practice, they do not lead to theoretical guarantees on approximation ability. We aim to provide a theoretical explanation for the power of sparse networks. As our first contribution, we present a formal model of data-dependent sparse networks that captures salient aspects of popular architectures. We then introduce a routing function based on locality sensitive hashing (LSH) that enables us to reason about how well sparse networks approximate target functions. After representing LSH-based sparse networks with our model, we prove that sparse networks can match the approximation power of dense networks on Lipschitz functions. Applying LSH on the input vectors means that the experts interpolate the target function in different subregions of the input space. To support our theory, we define various datasets based on Lipschitz target functions, and we show that sparse networks give a favorable trade-off between number of active units and approximation quality.

</p>
</details>

<details><summary><b>Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing</b>
<a href="https://arxiv.org/abs/2208.04369">arxiv:2208.04369</a>
&#x1F4C8; 7 <br>
<p>Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai</p></summary>
<p>

**Abstract:** We present a weight similarity measure method that can quantify the weight similarity of non-convex neural networks. To understand the weight similarity of different trained models, we propose to extract the feature representation from the weights of neural networks. We first normalize the weights of neural networks by introducing a chain normalization rule, which is used for weight representation learning and weight similarity measure. We extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural networks. With the chain normalization rule and the new statistical inference, we study the weight similarity measure on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), and find that the weights of an identical neural network optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a similar local solution in a metric space. The weight similarity measure provides more insight into the local solutions of neural networks. Experiments on several datasets consistently validate the hypothesis of weight similarity measure.

</p>
</details>

<details><summary><b>Gaze Estimation Approach Using Deep Differential Residual Network</b>
<a href="https://arxiv.org/abs/2208.04298">arxiv:2208.04298</a>
&#x1F4C8; 7 <br>
<p>Longzhao Huang, Yujie Li, Xu Wang, Haoyu Wang, Ahmed Bouridane, Ahmad Chaddad</p></summary>
<p>

**Abstract:** Gaze estimation, which is a method to determine where a person is looking at given the person's full face, is a valuable clue for understanding human intention. Similarly to other domains of computer vision, deep learning (DL) methods have gained recognition in the gaze estimation domain. However, there are still gaze calibration problems in the gaze estimation domain, thus preventing existing methods from further improving the performances. An effective solution is to directly predict the difference information of two human eyes, such as the differential network (Diff-Nn). However, this solution results in a loss of accuracy when using only one inference image. We propose a differential residual model (DRNet) combined with a new loss function to make use of the difference information of two eye images. We treat the difference information as auxiliary information. We assess the proposed model (DRNet) mainly using two public datasets (1) MpiiGaze and (2) Eyediap. Considering only the eye features, DRNet outperforms the state-of-the-art gaze estimation methods with $angular-error$ of 4.57 and 6.14 using MpiiGaze and Eyediap datasets, respectively. Furthermore, the experimental results also demonstrate that DRNet is extremely robust to noise images.

</p>
</details>

<details><summary><b>On Rademacher Complexity-based Generalization Bounds for Deep Learning</b>
<a href="https://arxiv.org/abs/2208.04284">arxiv:2208.04284</a>
&#x1F4C8; 7 <br>
<p>Lan V. Truong</p></summary>
<p>

**Abstract:** In this paper, we develop some novel bounds for the Rademacher complexity and the generalization error in deep learning with i.i.d. and Markov datasets. The new Rademacher complexity and generalization bounds are tight up to $O(1/\sqrt{n})$ where $n$ is the size of the training set. They can be exponentially decayed in the depth $L$ for some neural network structures. The development of Talagrand's contraction lemmas for high-dimensional mappings between function spaces and deep neural networks for general activation functions is a key technical contribution to this work.

</p>
</details>

<details><summary><b>TGAVC: Improving Autoencoder Voice Conversion with Text-Guided and Adversarial Training</b>
<a href="https://arxiv.org/abs/2208.04035">arxiv:2208.04035</a>
&#x1F4C8; 7 <br>
<p>Huaizhen Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Zhen Zeng, Edward Xiao, Jing Xiao</p></summary>
<p>

**Abstract:** Non-parallel many-to-many voice conversion remains an interesting but challenging speech processing task. Recently, AutoVC, a conditional autoencoder based method, achieved excellent conversion results by disentangling the speaker identity and the speech content using information-constraining bottlenecks. However, due to the pure autoencoder training method, it is difficult to evaluate the separation effect of content and speaker identity. In this paper, a novel voice conversion framework, named $\boldsymbol T$ext $\boldsymbol G$uided $\boldsymbol A$utoVC(TGAVC), is proposed to more effectively separate content and timbre from speech, where an expected content embedding produced based on the text transcriptions is designed to guide the extraction of voice content. In addition, the adversarial training is applied to eliminate the speaker identity information in the estimated content embedding extracted from speech. Under the guidance of the expected content embedding and the adversarial training, the content encoder is trained to extract speaker-independent content embedding from speech. Experiments on AIShell-3 dataset show that the proposed model outperforms AutoVC in terms of naturalness and similarity of converted speech.

</p>
</details>

<details><summary><b>Abutting Grating Illusion: Cognitive Challenge to Neural Network Models</b>
<a href="https://arxiv.org/abs/2208.03958">arxiv:2208.03958</a>
&#x1F4C8; 7 <br>
<p>Jinyu Fan, Yi Zeng</p></summary>
<p>

**Abstract:** Even the state-of-the-art deep learning models lack fundamental abilities compared to humans. Multiple comparison paradigms have been proposed to explore the distinctions between humans and deep learning. While most comparisons rely on corruptions inspired by mathematical transformations, very few have bases on human cognitive phenomena. In this study, we propose a novel corruption method based on the abutting grating illusion, which is a visual phenomenon widely discovered in both human and a wide range of animal species. The corruption method destroys the gradient-defined boundaries and generates the perception of illusory contours using line gratings abutting each other. We applied the method on MNIST, high resolution MNIST, and silhouette object images. Various deep learning models are tested on the corruption, including models trained from scratch and 109 models pretrained with ImageNet or various data augmentation techniques. Our results show that abutting grating corruption is challenging even for state-of-the-art deep learning models because most models are randomly guessing. We also discovered that the DeepAugment technique can greatly improve robustness against abutting grating illusion. Visualisation of early layers indicates that better performing models exhibit stronger end-stopping property, which is consistent with neuroscience discoveries. To validate the corruption method, 24 human subjects are involved to classify samples of corrupted datasets.

</p>
</details>

<details><summary><b>Training Overparametrized Neural Networks in Sublinear Time</b>
<a href="https://arxiv.org/abs/2208.04508">arxiv:2208.04508</a>
&#x1F4C8; 6 <br>
<p>Hang Hu, Zhao Song, Omri Weinstein, Danyang Zhuo</p></summary>
<p>

**Abstract:** The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of AI. Despite the popularity and low cost-per-iteration of traditional Backpropagation via gradient decent, SGD has prohibitive convergence rate in non-convex settings, both in theory and practice.
  To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-α} n d + n^3$ amortized time in the same overparametrized regime, where $α\in (0.01,1)$ is some fixed constant. This method relies on a new and alternative view of neural networks, as a set of binary search trees, where each iteration corresponds to modifying a small subset of the nodes in the tree. We believe this view would have further applications in the design and analysis of DNNs.

</p>
</details>

<details><summary><b>Second Order Ensemble Langevin Method for Sampling and Inverse Problems</b>
<a href="https://arxiv.org/abs/2208.04506">arxiv:2208.04506</a>
&#x1F4C8; 6 <br>
<p>Ziming Liu, Andrew M. Stuart, Yixuan Wang</p></summary>
<p>

**Abstract:** We propose a sampling method based on an ensemble approximation of second order Langevin dynamics. The log target density is appended with a quadratic term in an auxiliary momentum variable and damped-driven Hamiltonian dynamics introduced; the resulting stochastic differential equation is invariant to the Gibbs measure, with marginal on the position coordinates given by the target. A preconditioner based on covariance under the law of the dynamics does not change this invariance property, and is introduced to accelerate convergence to the Gibbs measure. The resulting mean-field dynamics may be approximated by an ensemble method; this results in a gradient-free and affine-invariant stochastic dynamical system. Numerical results demonstrate its potential as the basis for a numerical sampler in Bayesian inverse problems.

</p>
</details>

<details><summary><b>Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation</b>
<a href="https://arxiv.org/abs/2208.04435">arxiv:2208.04435</a>
&#x1F4C8; 6 <br>
<p>Mou-Cheng Xu, Yukun Zhou, Chen Jin, Marius de Groot, Daniel C. Alexander, Neil P. Oxtoby, Yipeng Hu, Joseph Jacob</p></summary>
<p>

**Abstract:** This paper concerns pseudo labelling in segmentation. Our contribution is fourfold. Firstly, we present a new formulation of pseudo-labelling as an Expectation-Maximization (EM) algorithm for clear statistical interpretation. Secondly, we propose a semi-supervised medical image segmentation method purely based on the original pseudo labelling, namely SegPL. We demonstrate SegPL is a competitive approach against state-of-the-art consistency regularisation based methods on semi-supervised segmentation on a 2D multi-class MRI brain tumour segmentation task and a 3D binary CT lung vessel segmentation task. The simplicity of SegPL allows less computational cost comparing to prior methods. Thirdly, we demonstrate that the effectiveness of SegPL may originate from its robustness against out-of-distribution noises and adversarial attacks. Lastly, under the EM framework, we introduce a probabilistic generalisation of SegPL via variational inference, which learns a dynamic threshold for pseudo labelling during the training. We show that SegPL with variational inference can perform uncertainty estimation on par with the gold-standard method Deep Ensemble.

</p>
</details>

<details><summary><b>Exploiting Shape Cues for Weakly Supervised Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2208.04286">arxiv:2208.04286</a>
&#x1F4C8; 6 <br>
<p>Sungpil Kho, Pilhyeon Lee, Wonyoung Lee, Minsong Ki, Hyeran Byun</p></summary>
<p>

**Abstract:** Weakly supervised semantic segmentation (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation maps (CAMs) and use such masks to supervise segmentation networks. However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.

</p>
</details>

<details><summary><b>Self-Supervised Contrastive Representation Learning for 3D Mesh Segmentation</b>
<a href="https://arxiv.org/abs/2208.04278">arxiv:2208.04278</a>
&#x1F4C8; 6 <br>
<p>Ayaan Haque, Hankyu Moon, Heng Hao, Sima Didari, Jae Oh Woo, Patrick Bangert</p></summary>
<p>

**Abstract:** 3D deep learning is a growing field of interest due to the vast amount of information stored in 3D formats. Triangular meshes are an efficient representation for irregular, non-uniform 3D objects. However, meshes are often challenging to annotate due to their high geometrical complexity. Specifically, creating segmentation masks for meshes is tedious and time-consuming. Therefore, it is desirable to train segmentation networks with limited-labeled data. Self-supervised learning (SSL), a form of unsupervised representation learning, is a growing alternative to fully-supervised learning which can decrease the burden of supervision for training. We propose SSL-MeshCNN, a self-supervised contrastive learning method for pre-training CNNs for mesh segmentation. We take inspiration from traditional contrastive learning frameworks to design a novel contrastive learning algorithm specifically for meshes. Our preliminary experiments show promising results in reducing the heavy labeled data requirement needed for mesh segmentation by at least 33%.

</p>
</details>

<details><summary><b>Deep Machine Learning Reconstructing Lattice Topology with Strong Thermal Fluctuations</b>
<a href="https://arxiv.org/abs/2208.04119">arxiv:2208.04119</a>
&#x1F4C8; 6 <br>
<p>Xiao-Han Wang, Pei Shi, Bin Xi, Jie Hu, Shi-Ju Ran</p></summary>
<p>

**Abstract:** Applying artificial intelligence to scientific problems (namely AI for science) is currently under hot debate. However, the scientific problems differ much from the conventional ones with images, texts, and etc., where new challenges emerges with the unbalanced scientific data and complicated effects from the physical setups. In this work, we demonstrate the validity of the deep convolutional neural network (CNN) on reconstructing the lattice topology (i.e., spin connectivities) in the presence of strong thermal fluctuations and unbalanced data. Taking the kinetic Ising model with Glauber dynamics as an example, the CNN maps the time-dependent local magnetic momenta (a single-node feature) evolved from a specific initial configuration (dubbed as an evolution instance) to the probabilities of the presences of the possible couplings. Our scheme distinguishes from the previous ones that might require the knowledge on the node dynamics, the responses from perturbations, or the evaluations of statistic quantities such as correlations or transfer entropy from many evolution instances. The fine tuning avoids the "barren plateau" caused by the strong thermal fluctuations at high temperatures. Accurate reconstructions can be made where the thermal fluctuations dominate over the correlations and consequently the statistic methods in general fail. Meanwhile, we unveil the generalization of CNN on dealing with the instances evolved from the unlearnt initial spin configurations and those with the unlearnt lattices. We raise an open question on the learning with unbalanced data in the nearly "double-exponentially" large sample space.

</p>
</details>

<details><summary><b>INSPIRED2: An Improved Dataset for Sociable Conversational Recommendation</b>
<a href="https://arxiv.org/abs/2208.04104">arxiv:2208.04104</a>
&#x1F4C8; 6 <br>
<p>Ahtsham Manzoor, Dietmar Jannach</p></summary>
<p>

**Abstract:** Conversational recommender systems (CRS) that interact with users in natural language utilize recommendation dialogs collected with the help of paired humans, where one plays the role of a seeker and the other as a recommender. These recommendation dialogs include items and entities to disclose seekers' preferences in natural language. However, in order to precisely model the seekers' preferences and respond consistently, mainly CRS rely on explicitly annotated items and entities that appear in the dialog, and usually leverage the domain knowledge. In this work, we investigate INSPIRED, a dataset consisting of recommendation dialogs for the sociable conversational recommendation, where items and entities were explicitly annotated using automatic keyword or pattern matching techniques. To this end, we found a large number of cases where items and entities were either wrongly annotated or missing annotations at all. The question however remains to what extent automatic techniques for annotations are effective. Moreover, it is unclear what is the relative impact of poor and improved annotations on the overall effectiveness of a CRS in terms of the consistency and quality of responses. In this regard, first, we manually fixed the annotations and removed the noise in the INSPIRED dataset. Second, we evaluate the performance of several benchmark CRS using both versions of the dataset. Our analyses suggest that with the improved version of the dataset, i.e., INSPIRED2, various benchmark CRS outperformed and that dialogs are rich in knowledge concepts compared to when the original version is used. We release our improved dataset (INSPIRED2) publicly at https://github.com/ahtsham58/INSPIRED2.

</p>
</details>

<details><summary><b>Image Quality Assessment with Gradient Siamese Network</b>
<a href="https://arxiv.org/abs/2208.04081">arxiv:2208.04081</a>
&#x1F4C8; 6 <br>
<p>Heng Cong, Lingzhi Fu, Rongyu Zhang, Yusheng Zhang, Hao Wang, Jiarong He, Jin Gao</p></summary>
<p>

**Abstract:** In this work, we introduce Gradient Siamese Network (GSN) for image quality assessment. The proposed method is skilled in capturing the gradient features between distorted images and reference images in full-reference image quality assessment(IQA) task. We utilize Central Differential Convolution to obtain both semantic features and detail difference hidden in image pair. Furthermore, spatial attention guides the network to concentrate on regions related to image detail. For the low-level, mid-level and high-level features extracted by the network, we innovatively design a multi-level fusion method to improve the efficiency of feature utilization. In addition to the common mean square error supervision, we further consider the relative distance among batch samples and successfully apply KL divergence loss to the image quality assessment task. We experimented the proposed algorithm GSN on several publicly available datasets and proved its superior performance. Our network won the second place in NTIRE 2022 Perceptual Image Quality Assessment Challenge track 1 Full-Reference.

</p>
</details>

<details><summary><b>Speaker-adaptive Lip Reading with User-dependent Padding</b>
<a href="https://arxiv.org/abs/2208.04498">arxiv:2208.04498</a>
&#x1F4C8; 5 <br>
<p>Minsu Kim, Hyunjun Kim, Yong Man Ro</p></summary>
<p>

**Abstract:** Lip reading aims to predict speech based on lip movements alone. As it focuses on visual information to model the speech, its performance is inherently sensitive to personal lip appearances and movements. This makes the lip reading models show degraded performance when they are applied to unseen speakers due to the mismatch between training and testing conditions. Speaker adaptation technique aims to reduce this mismatch between train and test speakers, thus guiding a trained model to focus on modeling the speech content without being intervened by the speaker variations. In contrast to the efforts made in audio-based speech recognition for decades, the speaker adaptation methods have not well been studied in lip reading. In this paper, to remedy the performance degradation of lip reading model on unseen speakers, we propose a speaker-adaptive lip reading method, namely user-dependent padding. The user-dependent padding is a speaker-specific input that can participate in the visual feature extraction stage of a pre-trained lip reading model. Therefore, the lip appearances and movements information of different speakers can be considered during the visual feature encoding, adaptively for individual speakers. Moreover, the proposed method does not need 1) any additional layers, 2) to modify the learned weights of the pre-trained model, and 3) the speaker label of train data used during pre-train. It can directly adapt to unseen speakers by learning the user-dependent padding only, in a supervised or unsupervised manner. Finally, to alleviate the speaker information insufficiency in public lip reading databases, we label the speaker of a well-known audio-visual database, LRW, and design an unseen-speaker lip reading scenario named LRW-ID.

</p>
</details>

<details><summary><b>Deep Maxout Network Gaussian Process</b>
<a href="https://arxiv.org/abs/2208.04468">arxiv:2208.04468</a>
&#x1F4C8; 5 <br>
<p>Libin Liang, Ye Tian, Ge Cheng</p></summary>
<p>

**Abstract:** Study of neural networks with infinite width is important for better understanding of the neural network in practical application. In this work, we derive the equivalence of the deep, infinite-width maxout network and the Gaussian process (GP) and characterize the maxout kernel with a compositional structure. Moreover, we build up the connection between our deep maxout network kernel and deep neural network kernels. We also give an efficient numerical implementation of our kernel which can be adapted to any maxout rank. Numerical results show that doing Bayesian inference based on the deep maxout network kernel can lead to competitive results compared with their finite-width counterparts and deep neural network kernels. This enlightens us that the maxout activation may also be incorporated into other infinite-width neural network structures such as the convolutional neural network (CNN).

</p>
</details>

<details><summary><b>A Linear Programming Approach for Resource-Aware Information-Theoretic Tree Abstractions</b>
<a href="https://arxiv.org/abs/2208.04220">arxiv:2208.04220</a>
&#x1F4C8; 5 <br>
<p>Daniel T. Larsson, Dipankar Maity, Panagiotis Tsiotras</p></summary>
<p>

**Abstract:** In this chapter, an integer linear programming formulation for the problem of obtaining task-relevant, multi-resolution, environment abstractions for resource-constrained autonomous agents is presented. The formulation leverages concepts from information-theoretic signal compression, specifically, the information bottleneck (IB) method, to pose an abstraction problem as an optimal encoder search over the space of multi-resolution trees. The abstractions emerge in a task-relevant manner as a function of agent information-processing constraints. We detail our formulation, and show how hierarchical tree structures, signal encoders, and information-theoretic methods for signal compression can be unified under a common theme. A discussion delineating the benefits and drawbacks of our formulation is presented, as well as a detailed explanation how our approach can be interpreted within the context of generating abstractions for resource-constrained autonomous systems. It is shown that the resulting information-theoretic abstraction problem over the space of multi-resolution trees can be formulated as a integer linear programming (ILP) problem. We demonstrate the approach on a number of examples, and provide a discussion detailing the differences of the proposed framework compared to existing methods. Lastly, we consider a linear program relaxation of the ILP problem, thereby demonstrating that multi-resolution information-theoretic tree abstractions can be obtained by solving a convex program.

</p>
</details>

<details><summary><b>SsaA: A Self-supervised auto-Annotation System for Online Visual Inspection and Manufacturing Automation</b>
<a href="https://arxiv.org/abs/2208.04173">arxiv:2208.04173</a>
&#x1F4C8; 5 <br>
<p>Jiawei Li, Bolin Jiang, Yan Liu, Chengxiao Luo, Naiqi Li, Bin Chen</p></summary>
<p>

**Abstract:** Recent trends in cloud computing technology effectively boosted the application of visual inspection. However, most of the available systems work in a human-in-the-loop manner and can not provide long-term support to the online application. To make a step forward, this paper outlines an automatic annotation system called SsaA, working in a self-supervised learning manner, for continuously making the online visual inspection in the manufacturing automation scenarios. Benefit from the self-supervised learning, SsaA is effective to establish a visual inspection application for the whole life-cycle of manufacturing. In the early stage, with only the anomaly-free data, the unsupervised algorithms are adopted to process the pretext task and generate coarse labels for the following data. Then supervised algorithms are trained for the downstream task. With user-friendly web-based interfaces, SsaA is very convenient to integrate and deploy both of the unsupervised and supervised algorithms. So far, the SsaA system has been adopted for some real-life industrial applications.

</p>
</details>

<details><summary><b>Towards Semantic Communications: Deep Learning-Based Image Semantic Coding</b>
<a href="https://arxiv.org/abs/2208.04094">arxiv:2208.04094</a>
&#x1F4C8; 5 <br>
<p>Danlan Huang, Feifei Gao, Xiaoming Tao, Qiyuan Du, Jianhua Lu</p></summary>
<p>

**Abstract:** Semantic communications has received growing interest since it can remarkably reduce the amount of data to be transmitted without missing critical information. Most existing works explore the semantic encoding and transmission for text and apply techniques in Natural Language Processing (NLP) to interpret the meaning of the text. In this paper, we conceive the semantic communications for image data that is much more richer in semantics and bandwidth sensitive. We propose an reinforcement learning based adaptive semantic coding (RL-ASC) approach that encodes images beyond pixel level. Firstly, we define the semantic concept of image data that includes the category, spatial arrangement, and visual feature as the representation unit, and propose a convolutional semantic encoder to extract semantic concepts. Secondly, we propose the image reconstruction criterion that evolves from the traditional pixel similarity to semantic similarity and perceptual performance. Thirdly, we design a novel RL-based semantic bit allocation model, whose reward is the increase in rate-semantic-perceptual performance after encoding a certain semantic concept with adaptive quantization level. Thus, the task-related information is preserved and reconstructed properly while less important data is discarded. Finally, we propose the Generative Adversarial Nets (GANs) based semantic decoder that fuses both locally and globally features via an attention module. Experimental results demonstrate that the proposed RL-ASC is noise robust and could reconstruct visually pleasant and semantic consistent image, and saves times of bit cost compared to standard codecs and other deep learning-based image codecs.

</p>
</details>

<details><summary><b>Sampling Based On Natural Image Statistics Improves Local Surrogate Explainers</b>
<a href="https://arxiv.org/abs/2208.03961">arxiv:2208.03961</a>
&#x1F4C8; 5 <br>
<p>Ricardo Kleinlein, Alexander Hepburn, Raúl Santos-Rodríguez, Fernando Fernández-Martínez</p></summary>
<p>

**Abstract:** Many problems in computer vision have recently been tackled using models whose predictions cannot be easily interpreted, most commonly deep neural networks. Surrogate explainers are a popular post-hoc interpretability method to further understand how a model arrives at a particular prediction. By training a simple, more interpretable model to locally approximate the decision boundary of a non-interpretable system, we can estimate the relative importance of the input features on the prediction. Focusing on images, surrogate explainers, e.g., LIME, generate a local neighbourhood around a query image by sampling in an interpretable domain. However, these interpretable domains have traditionally been derived exclusively from the intrinsic features of the query image, not taking into consideration the manifold of the data the non-interpretable model has been exposed to in training (or more generally, the manifold of real images). This leads to suboptimal surrogates trained on potentially low probability images. We address this limitation by aligning the local neighbourhood on which the surrogate is trained with the original training data distribution, even when this distribution is not accessible. We propose two approaches to do so, namely (1) altering the method for sampling the local neighbourhood and (2) using perceptual metrics to convey some of the properties of the distribution of natural images.

</p>
</details>

<details><summary><b>VectorFlow: Combining Images and Vectors for Traffic Occupancy and Flow Prediction</b>
<a href="https://arxiv.org/abs/2208.04530">arxiv:2208.04530</a>
&#x1F4C8; 4 <br>
<p>Xin Huang, Xiaoyu Tian, Junru Gu, Qiao Sun, Hang Zhao</p></summary>
<p>

**Abstract:** Predicting future behaviors of road agents is a key task in autonomous driving. While existing models have demonstrated great success in predicting marginal agent future behaviors, it remains a challenge to efficiently predict consistent joint behaviors of multiple agents. Recently, the occupancy flow fields representation was proposed to represent joint future states of road agents through a combination of occupancy grid and flow, which supports efficient and consistent joint predictions. In this work, we propose a novel occupancy flow fields predictor to produce accurate occupancy and flow predictions, by combining the power of an image encoder that learns features from a rasterized traffic image and a vector encoder that captures information of continuous agent trajectories and map states. The two encoded features are fused by multiple attention modules before generating final predictions. Our simple but effective model ranks 3rd place on the Waymo Open Dataset Occupancy and Flow Prediction Challenge, and achieves the best performance in the occluded occupancy and flow prediction task.

</p>
</details>

<details><summary><b>Multiple Instance Neural Networks Based on Sparse Attention for Cancer Detection using T-cell Receptor Sequences</b>
<a href="https://arxiv.org/abs/2208.04524">arxiv:2208.04524</a>
&#x1F4C8; 4 <br>
<p>Younghoon Kim, Tao Wang, Danyi Xiong, Xinlei Wang, Seongoh Park</p></summary>
<p>

**Abstract:** Early detection of cancers has been much explored due to its paramount importance in biomedical fields. Among different types of data used to answer this biological question, studies based on T cell receptors (TCRs) are under recent spotlight due to the growing appreciation of the roles of the host immunity system in tumor biology. However, the one-to-many correspondence between a patient and multiple TCR sequences hinders researchers from simply adopting classical statistical/machine learning methods. There were recent attempts to model this type of data in the context of multiple instance learning (MIL).
  Despite the novel application of MIL to cancer detection using TCR sequences and the demonstrated adequate performance in several tumor types, there is still room for improvement, especially for certain cancer types. Furthermore, explainable neural network models are not fully investigated for this application.
  In this article, we propose multiple instance neural networks based on sparse attention (MINN-SA) to enhance the performance in cancer detection and explainability. The sparse attention structure drops out uninformative instances in each bag, achieving both interpretability and better predictive performance in combination with the skip connection.
  Our experiments show that MINN-SA yields the highest area under the ROC curve (AUC) scores on average measured across 10 different types of cancers, compared to existing MIL approaches. Moreover, we observe from the estimated attentions that MINN-SA can identify the TCRs that are specific for tumor antigens in the same T cell repertoire.

</p>
</details>

<details><summary><b>Optimal scheduling of entropy regulariser for continuous-time linear-quadratic reinforcement learning</b>
<a href="https://arxiv.org/abs/2208.04466">arxiv:2208.04466</a>
&#x1F4C8; 4 <br>
<p>Lukasz Szpruch, Tanut Treetanthiploet, Yufei Zhang</p></summary>
<p>

**Abstract:** This work uses the entropy-regularised relaxed stochastic control perspective as a principled framework for designing reinforcement learning (RL) algorithms. Herein agent interacts with the environment by generating noisy controls distributed according to the optimal relaxed policy. The noisy policies on the one hand, explore the space and hence facilitate learning but, on the other hand, introduce bias by assigning a positive probability to non-optimal actions. This exploration-exploitation trade-off is determined by the strength of entropy regularisation. We study algorithms resulting from two entropy regularisation formulations: the exploratory control approach, where entropy is added to the cost objective, and the proximal policy update approach, where entropy penalises the divergence of policies between two consecutive episodes. We analyse the finite horizon continuous-time linear-quadratic (LQ) RL problem for which both algorithms yield a Gaussian relaxed policy. We quantify the precise difference between the value functions of a Gaussian policy and its noisy evaluation and show that the execution noise must be independent across time. By tuning the frequency of sampling from relaxed policies and the parameter governing the strength of entropy regularisation, we prove that the regret, for both learning algorithms, is of the order $\mathcal{O}(\sqrt{N}) $ (up to a logarithmic factor)
  over $N$ episodes, matching the best known result from the literature.

</p>
</details>

<details><summary><b>Denoising Induction Motor Sounds Using an Autoencoder</b>
<a href="https://arxiv.org/abs/2208.04462">arxiv:2208.04462</a>
&#x1F4C8; 4 <br>
<p>Thanh Tran, Sebastian Bader, Jan Lundgren</p></summary>
<p>

**Abstract:** Denoising is the process of removing noise from sound signals while improving the quality and adequacy of the sound signals. Denoising sound has many applications in speech processing, sound events classification, and machine failure detection systems. This paper describes a method for creating an autoencoder to map noisy machine sounds to clean sounds for denoising purposes. There are several types of noise in sounds, for example, environmental noise and generated frequency-dependent noise from signal processing methods. Noise generated by environmental activities is environmental noise. In the factory, environmental noise can be created by vehicles, drilling, people working or talking in the survey area, wind, and flowing water. Those noises appear as spikes in the sound record. In the scope of this paper, we demonstrate the removal of generated noise with Gaussian distribution and the environmental noise with a specific example of the water sink faucet noise from the induction motor sounds. The proposed method was trained and verified on 49 normal function sounds and 197 horizontal misalignment fault sounds from the Machinery Fault Database (MAFAULDA). The mean square error (MSE) was used as the assessment criteria to evaluate the similarity between denoised sounds using the proposed autoencoder and the original sounds in the test set. The MSE is below or equal to 0.14 when denoise both types of noises on 15 testing sounds of the normal function category. The MSE is below or equal to 0.15 when denoising 60 testing sounds on the horizontal misalignment fault category. The low MSE shows that both the generated Gaussian noise and the environmental noise were almost removed from the original sounds with the proposed trained autoencoder.

</p>
</details>

<details><summary><b>Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts</b>
<a href="https://arxiv.org/abs/2208.04417">arxiv:2208.04417</a>
&#x1F4C8; 4 <br>
<p>Babak Hemmatian, Lav R. Varshney</p></summary>
<p>

**Abstract:** Recent work demonstrates a bias in the GPT-3 model towards generating violent text completions when prompted about Muslims, compared with Christians and Hindus. Two pre-registered replication attempts, one exact and one approximate, found only the weakest bias in the more recent Instruct Series version of GPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent completions were observed. Additional pre-registered experiments, however, showed that using common names associated with the religions in prompts yields a highly significant increase in violent completions, also revealing a stronger second-order bias against Muslims. Names of Muslim celebrities from non-violent domains resulted in relatively fewer violent completions, suggesting that access to individualized information can steer the model away from using stereotypes. Nonetheless, content analysis revealed religion-specific violent themes containing highly offensive ideas regardless of prompt format. Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.

</p>
</details>

<details><summary><b>Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey</b>
<a href="https://arxiv.org/abs/2208.04415">arxiv:2208.04415</a>
&#x1F4C8; 4 <br>
<p>Ayush Kumar, Parth Nagarkar, Prabhav Nalhe, Sanjeev Vijayakumar</p></summary>
<p>

**Abstract:** With the future striving toward data-centric decision-making, seamless access to databases is of utmost importance. There is extensive research on creating an efficient text-to-sql (TEXT2SQL) model to access data from the database. Using a Natural language is one of the best interfaces that can bridge the gap between the data and results by accessing the database efficiently, especially for non-technical users. It will open the doors and create tremendous interest among users who are well versed in technical skills or not very skilled in query languages. Even if numerous deep learning-based algorithms are proposed or studied, there still is very challenging to have a generic model to solve the data query issues using natural language in a real-work scenario. The reason is the use of different datasets in different studies, which comes with its limitations and assumptions. At the same time, we do lack a thorough understanding of these proposed models and their limitations with the specific dataset it is trained on. In this paper, we try to present a holistic overview of 24 recent neural network models studied in the last couple of years, including their architectures involving convolutional neural networks, recurrent neural networks, pointer networks, reinforcement learning, generative models, etc. We also give an overview of the 11 datasets that are widely used to train the models for TEXT2SQL technologies. We also discuss the future application possibilities of TEXT2SQL technologies for seamless data queries.

</p>
</details>

<details><summary><b>Boosting neural video codecs by exploiting hierarchical redundancy</b>
<a href="https://arxiv.org/abs/2208.04303">arxiv:2208.04303</a>
&#x1F4C8; 4 <br>
<p>Reza Pourreza, Hoang Le, Amir Said, Guillaume Sautiere, Auke Wiggers</p></summary>
<p>

**Abstract:** In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e., similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate savings in RGB and YUV420 colorspaces measured on the UVG dataset.

</p>
</details>

<details><summary><b>Snowpack Estimation in Key Mountainous Water Basins from Openly-Available, Multimodal Data Sources</b>
<a href="https://arxiv.org/abs/2208.04246">arxiv:2208.04246</a>
&#x1F4C8; 4 <br>
<p>Malachy Moran, Kayla Woputz, Derrick Hee, Manuela Girotto, Paolo D'Odorico, Ritwik Gupta, Daniel Feldman, Puya Vahabi, Alberto Todeschini, Colorado J Reed</p></summary>
<p>

**Abstract:** Accurately estimating the snowpack in key mountainous basins is critical for water resource managers to make decisions that impact local and global economies, wildlife, and public policy. Currently, this estimation requires multiple LiDAR-equipped plane flights or in situ measurements, both of which are expensive, sparse, and biased towards accessible regions. In this paper, we demonstrate that fusing spatial and temporal information from multiple, openly-available satellite and weather data sources enables estimation of snowpack in key mountainous regions. Our multisource model outperforms single-source estimation by 5.0 inches RMSE, as well as outperforms sparse in situ measurements by 1.2 inches RMSE.

</p>
</details>

<details><summary><b>SKDCGN: Source-free Knowledge Distillation of Counterfactual Generative Networks using cGANs</b>
<a href="https://arxiv.org/abs/2208.04226">arxiv:2208.04226</a>
&#x1F4C8; 4 <br>
<p>Sameer Ambekar, Ankit Ankit, Diego van der Mast, Mark Alence, Matteo Tafuro, Christos Athanasiadis</p></summary>
<p>

**Abstract:** With the usage of appropriate inductive biases, Counterfactual Generative Networks (CGNs) can generate novel images from random combinations of shape, texture, and background manifolds. These images can be utilized to train an invariant classifier, avoiding the wide spread problem of deep architectures learning spurious correlations rather than meaningful ones. As a consequence, out-of-domain robustness is improved. However, the CGN architecture comprises multiple over parameterized networks, namely BigGAN and U2-Net. Training these networks requires appropriate background knowledge and extensive computation. Since one does not always have access to the precise training details, nor do they always possess the necessary knowledge of counterfactuals, our work addresses the following question: Can we use the knowledge embedded in pre-trained CGNs to train a lower-capacity model, assuming black-box access (i.e., only access to the pretrained CGN model) to the components of the architecture? In this direction, we propose a novel work named SKDCGN that attempts knowledge transfer using Knowledge Distillation (KD). In our proposed architecture, each independent mechanism (shape, texture, background) is represented by a student 'TinyGAN' that learns from the pretrained teacher 'BigGAN'. We demonstrate the efficacy of the proposed method using state-of-the-art datasets such as ImageNet, and MNIST by using KD and appropriate loss functions. Moreover, as an additional contribution, our paper conducts a thorough study on the composition mechanism of the CGNs, to gain a better understanding of how each mechanism influences the classification accuracy of an invariant classifier. Code available at: https://github.com/ambekarsameer96/SKDCGN

</p>
</details>

<details><summary><b>Vision-Based Activity Recognition in Children with Autism-Related Behaviors</b>
<a href="https://arxiv.org/abs/2208.04206">arxiv:2208.04206</a>
&#x1F4C8; 4 <br>
<p>Pengbo Wei, David Ahmedt-Aristizabal, Harshala Gammulle, Simon Denman, Mohammad Ali Armin</p></summary>
<p>

**Abstract:** Advances in machine learning and contactless sensors have enabled the understanding complex human behaviors in a healthcare setting. In particular, several deep learning systems have been introduced to enable comprehensive analysis of neuro-developmental conditions such as Autism Spectrum Disorder (ASD). This condition affects children from their early developmental stages onwards, and diagnosis relies entirely on observing the child's behavior and detecting behavioral cues. However, the diagnosis process is time-consuming as it requires long-term behavior observation, and the scarce availability of specialists. We demonstrate the effect of a region-based computer vision system to help clinicians and parents analyze a child's behavior. For this purpose, we adopt and enhance a dataset for analyzing autism-related actions using videos of children captured in uncontrolled environments (e.g. videos collected with consumer-grade cameras, in varied environments). The data is pre-processed by detecting the target child in the video to reduce the impact of background noise. Motivated by the effectiveness of temporal convolutional models, we propose both light-weight and conventional models capable of extracting action features from video frames and classifying autism-related behaviors by analyzing the relationships between frames in a video. Through extensive evaluations on the feature extraction and learning strategies, we demonstrate that the best performance is achieved with an Inflated 3D Convnet and Multi-Stage Temporal Convolutional Networks, achieving a 0.83 Weighted F1-score for classification of the three autism-related actions, outperforming existing methods. We also propose a light-weight solution by employing the ESNet backbone within the same system, achieving competitive results of 0.71 Weighted F1-score, and enabling potential deployment on embedded systems.

</p>
</details>

<details><summary><b>Efficient Neural Net Approaches in Metal Casting Defect Detection</b>
<a href="https://arxiv.org/abs/2208.04150">arxiv:2208.04150</a>
&#x1F4C8; 4 <br>
<p>Rohit Lal, Bharath Kumar Bolla, Sabeesh Ethiraj</p></summary>
<p>

**Abstract:** One of the most pressing challenges prevalent in the steel manufacturing industry is the identification of surface defects. Early identification of casting defects can help boost performance, including streamlining production processes. Though, deep learning models have helped bridge this gap and automate most of these processes, there is a dire need to come up with lightweight models that can be deployed easily with faster inference times. This research proposes a lightweight architecture that is efficient in terms of accuracy and inference time compared with sophisticated pre-trained CNN architectures like MobileNet, Inception, and ResNet, including vision transformers. Methodologies to minimize computational requirements such as depth-wise separable convolution and global average pooling (GAP) layer, including techniques that improve architectural efficiencies and augmentations, have been experimented. Our results indicate that a custom model of 590K parameters with depth-wise separable convolutions outperformed pretrained architectures such as Resnet and Vision transformers in terms of accuracy (81.87%) and comfortably outdid architectures such as Resnet, Inception, and Vision transformers in terms of faster inference times (12 ms). Blurpool fared outperformed other techniques, with an accuracy of 83.98%. Augmentations had a paradoxical effect on the model performance. No direct correlation between depth-wise and 3x3 convolutions on inference time, they, however, they played a direct role in improving model efficiency by enabling the networks to go deeper and by decreasing the number of trainable parameters. Our work sheds light on the fact that custom networks with efficient architectures and faster inference times can be built without the need of relying on pre-trained architectures.

</p>
</details>

<details><summary><b>A review on longitudinal data analysis with random forest in precision medicine</b>
<a href="https://arxiv.org/abs/2208.04112">arxiv:2208.04112</a>
&#x1F4C8; 4 <br>
<p>Jianchang Hu, Silke Szymczak</p></summary>
<p>

**Abstract:** Precision medicine provides customized treatments to patients based on their characteristics and is a promising approach to improving treatment efficiency. Large scale omics data are useful for patient characterization, but often their measurements change over time, leading to longitudinal data. Random forest is one of the state-of-the-art machine learning methods for building prediction models, and can play a crucial role in precision medicine. In this paper, we review extensions of the standard random forest method for the purpose of longitudinal data analysis. Extension methods are categorized according to the data structures for which they are designed. We consider both univariate and multivariate responses and further categorize the repeated measurements according to whether the time effect is relevant. Information of available software implementations of the reviewed extensions is also given. We conclude with discussions on the limitations of our review and some future research directions.

</p>
</details>

<details><summary><b>Template-based Abstractive Microblog Opinion Summarisation</b>
<a href="https://arxiv.org/abs/2208.04083">arxiv:2208.04083</a>
&#x1F4C8; 4 <br>
<p>Iman Munire Bilal, Bo Wang, Adam Tsakalidis, Dong Nguyen, Rob Procter, Maria Liakata</p></summary>
<p>

**Abstract:** We introduce the task of microblog opinion summarisation (MOS) and share a dataset of 3100 gold-standard opinion summaries to facilitate research in this domain. The dataset contains summaries of tweets spanning a 2-year period and covers more topics than any other public Twitter summarisation dataset. Summaries are abstractive in nature and have been created by journalists skilled in summarising news articles following a template separating factual information (main story) from author opinions. Our method differs from previous work on generating gold-standard summaries from social media, which usually involves selecting representative posts and thus favours extractive summarisation models. To showcase the dataset's utility and challenges, we benchmark a range of abstractive and extractive state-of-the-art summarisation models and achieve good performance, with the former outperforming the latter. We also show that fine-tuning is necessary to improve performance and investigate the benefits of using different sample sizes.

</p>
</details>

<details><summary><b>A Map of Diverse Synthetic Stable Roommates Instances</b>
<a href="https://arxiv.org/abs/2208.04041">arxiv:2208.04041</a>
&#x1F4C8; 4 <br>
<p>Niclas Boehmer, Klaus Heeger, Stanisław Szufa</p></summary>
<p>

**Abstract:** Focusing on Stable Roommates (SR) instances, we contribute to the toolbox for conducting experiments for stable matching problems. We introduce a polynomial-time computable pseudometric to measure the similarity of SR instances, analyze its properties, and use it to create a map of SR instances. This map visualizes 460 synthetic SR instances (each sampled from one of ten different statistical cultures) as follows: Each instance is a point in the plane, and two points are close on the map if the corresponding SR instances are similar to each other. Subsequently, we conduct several exemplary experiments and depict their results on the map, illustrating the map's usefulness as a non-aggregate visualization tool, the diversity of our generated dataset, and the need to use instances sampled from different statistical cultures. Lastly, to demonstrate that our framework can also be used for other matching problems under preference, we create and analyze a map of Stable Marriage instances.

</p>
</details>

<details><summary><b>Sparse Attentive Memory Network for Click-through Rate Prediction with Long Sequences</b>
<a href="https://arxiv.org/abs/2208.04022">arxiv:2208.04022</a>
&#x1F4C8; 4 <br>
<p>Qianying Lin, Wen-Ji Zhou, Yanshi Wang, Qing Da, Qing-Guo Chen, Bing Wang</p></summary>
<p>

**Abstract:** Sequential recommendation predicts users' next behaviors with their historical interactions. Recommending with longer sequences improves recommendation accuracy and increases the degree of personalization. As sequences get longer, existing works have not yet addressed the following two main challenges. Firstly, modeling long-range intra-sequence dependency is difficult with increasing sequence lengths. Secondly, it requires efficient memory and computational speeds. In this paper, we propose a Sparse Attentive Memory (SAM) network for long sequential user behavior modeling. SAM supports efficient training and real-time inference for user behavior sequences with lengths on the scale of thousands. In SAM, we model the target item as the query and the long sequence as the knowledge database, where the former continuously elicits relevant information from the latter. SAM simultaneously models target-sequence dependencies and long-range intra-sequence dependencies with O(L) complexity and O(1) number of sequential updates, which can only be achieved by the self-attention mechanism with O(L^2) complexity. Extensive empirical results demonstrate that our proposed solution is effective not only in long user behavior modeling but also on short sequences modeling. Implemented on sequences of length 1000, SAM is successfully deployed on one of the largest international E-commerce platforms. This inference time is within 30ms, with a substantial 7.30% click-through rate improvement for the online A/B test. To the best of our knowledge, it is the first end-to-end long user sequence modeling framework that models intra-sequence and target-sequence dependencies with the aforementioned degree of efficiency and successfully deployed on a large-scale real-time industrial recommender system.

</p>
</details>

<details><summary><b>CSSAM:Code Search via Attention Matching of Code Semantics and Structures</b>
<a href="https://arxiv.org/abs/2208.03922">arxiv:2208.03922</a>
&#x1F4C8; 4 <br>
<p>Yi Hu, Bo Cai, Yaoxiang Yu</p></summary>
<p>

**Abstract:** Despite the continuous efforts in improving both the effectiveness and efficiency of code search, two issues remained unsolved. First, programming languages have inherent strong structural linkages, and feature mining of code as text form would omit the structural information contained inside it. Second, there is a potential semantic relationship between code and query, it is challenging to align code and text across sequences so that vectors are spatially consistent during similarity matching. To tackle both issues, in this paper, a code search model named CSSAM (Code Semantics and Structures Attention Matching) is proposed. By introducing semantic and structural matching mechanisms, CSSAM effectively extracts and fuses multidimensional code features. Specifically, the cross and residual layer was developed to facilitate high-latitude spatial alignment of code and query at the token level. By leveraging the residual interaction, a matching module is designed to preserve more code semantics and descriptive features, that enhances the adhesion between the code and its corresponding query text. Besides, to improve the model's comprehension of the code's inherent structure, a code representation structure named CSRG (Code Semantic Representation Graph) is proposed for jointly representing abstract syntax tree nodes and the data flow of the codes. According to the experimental results on two publicly available datasets containing 540k and 330k code segments, CSSAM significantly outperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and NDCG@50 on both datasets respectively. Moreover, the ablation study is conducted to quantitatively measure the impact of each key component of CSSAM on the efficiency and effectiveness of code search, which offers the insights into the improvement of advanced code search solutions.

</p>
</details>

<details><summary><b>FourCastNet: Accelerating Global High-Resolution Weather Forecasting using Adaptive Fourier Neural Operators</b>
<a href="https://arxiv.org/abs/2208.05419">arxiv:2208.05419</a>
&#x1F4C8; 3 <br>
<p>Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, Animashree Anandkumar</p></summary>
<p>

**Abstract:** Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy due to high computational cost and strict time-to-solution limits. We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCast-Net is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9%of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072GPUs is 67.4minutes, resulting in an 80,000times faster time-to-solution relative to state-of-the-art NWP, in inference. FourCastNet produces accurate instantaneous weather predictions for a week in advance, enables enormous ensembles that better capture weather extremes, and supports higher global forecast resolutions.

</p>
</details>

<details><summary><b>Fast Offline Policy Optimization for Large Scale Recommendation</b>
<a href="https://arxiv.org/abs/2208.05327">arxiv:2208.05327</a>
&#x1F4C8; 3 <br>
<p>Otmane Sakhi, David Rohde, Alexandre Gilotte</p></summary>
<p>

**Abstract:** Personalised interactive systems such as recommender systems require selecting relevant items dependent on context. Production systems need to identify the items rapidly from very large catalogues which can be efficiently solved using maximum inner product search technology. Offline optimisation of maximum inner product search can be achieved by a relaxation of the discrete problem resulting in policy learning or reinforce style learning algorithms. Unfortunately this relaxation step requires computing a sum over the entire catalogue making the complexity of the evaluation of the gradient (and hence each stochastic gradient descent iterations) linear in the catalogue size. This calculation is untenable in many real world examples such as large catalogue recommender systems severely limiting the usefulness of this method in practice. In this paper we show how it is possible to produce an excellent approximation of these policy learning algorithms that scale logarithmically with the catalogue size. Our contribution is based upon combining three novel ideas: a new Monte Carlo estimate of the gradient of a policy, the self normalised importance sampling estimator and the use of fast maximum inner product search at training time. Extensive experiments show our algorithm is an order of magnitude faster than naive approaches yet produces equally good policies.

</p>
</details>

<details><summary><b>Synthetic Aperture Radar Image Change Detection via Layer Attention-Based Noise-Tolerant Network</b>
<a href="https://arxiv.org/abs/2208.04481">arxiv:2208.04481</a>
&#x1F4C8; 3 <br>
<p>Desen Meng, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li</p></summary>
<p>

**Abstract:** Recently, change detection methods for synthetic aperture radar (SAR) images based on convolutional neural networks (CNN) have gained increasing research attention. However, existing CNN-based methods neglect the interactions among multilayer convolutions, and errors involved in the preclassification restrict the network optimization. To this end, we proposed a layer attention-based noise-tolerant network, termed LANTNet. In particular, we design a layer attention module that adaptively weights the feature of different convolution layers. In addition, we design a noise-tolerant loss function that effectively suppresses the impact of noisy labels. Therefore, the model is insensitive to noisy labels in the preclassification results. The experimental results on three SAR datasets show that the proposed LANTNet performs better compared to several state-of-the-art methods. The source codes are available at https://github.com/summitgao/LANTNet

</p>
</details>

<details><summary><b>Learning-Based Client Selection for Federated Learning Services Over Wireless Networks with Constrained Monetary Budgets</b>
<a href="https://arxiv.org/abs/2208.04322">arxiv:2208.04322</a>
&#x1F4C8; 3 <br>
<p>Zhipeng Cheng, Xuwei Fan, Minghui Liwang, Ning Chen, Xianbin Wang</p></summary>
<p>

**Abstract:** We investigate a data quality-aware dynamic client selection problem for multiple federated learning (FL) services in a wireless network, where each client has dynamic datasets for the simultaneous training of multiple FL services and each FL service demander has to pay for the clients with constrained monetary budgets. The problem is formalized as a non-cooperative Markov game over the training rounds. A multi-agent hybrid deep reinforcement learning-based algorithm is proposed to optimize the joint client selection and payment actions, while avoiding action conflicts. Simulation results indicate that our proposed algorithm can significantly improve the training performance.

</p>
</details>

<details><summary><b>Inflating 2D Convolution Weights for Efficient Generation of 3D Medical Images</b>
<a href="https://arxiv.org/abs/2208.03934">arxiv:2208.03934</a>
&#x1F4C8; 3 <br>
<p>Yanbin Liu, Girish Dwivedi, Farid Boussaid, Frank Sanfilippo, Makoto Yamada, Mohammed Bennamoun</p></summary>
<p>

**Abstract:** The generation of three-dimensional (3D) medical images can have great application potential since it takes into account the 3D anatomical structure. There are two problems, however, that prevent effective training of a 3D medical generative model: (1) 3D medical images are very expensive to acquire and annotate, resulting in an insufficient number of training images, (2) a large number of parameters are involved in 3D convolution. To address both problems, we propose a novel GAN model called 3D Split&Shuffle-GAN. In order to address the 3D data scarcity issue, we first pre-train a two-dimensional (2D) GAN model using abundant image slices and inflate the 2D convolution weights to improve initialization of the 3D GAN. Novel 3D network architectures are proposed for both the generator and discriminator of the GAN model to significantly reduce the number of parameters while maintaining the quality of image generation. A number of weight inflation strategies and parameter-efficient 3D architectures are investigated. Experiments on both heart (Stanford AIMI Coronary Calcium) and brain (Alzheimer's Disease Neuroimaging Initiative) datasets demonstrate that the proposed approach leads to improved 3D images generation quality with significantly fewer parameters.

</p>
</details>

<details><summary><b>A Frequency-aware Software Cache for Large Recommendation System Embeddings</b>
<a href="https://arxiv.org/abs/2208.05321">arxiv:2208.05321</a>
&#x1F4C8; 2 <br>
<p>Jiarui Fang, Geng Zhang, Jiatong Han, Shenggui Li, Zhengda Bian, Yongbin Li, Jin Liu, Yang You</p></summary>
<p>

**Abstract:** Deep learning recommendation models (DLRMs) have been widely applied in Internet companies. The embedding tables of DLRMs are too large to fit on GPU memory entirely. We propose a GPU-based software cache approaches to dynamically manage the embedding table in the CPU and GPU memory space by leveraging the id's frequency statistics of the target dataset. Our proposed software cache is efficient in training entire DLRMs on GPU in a synchronized update manner. It is also scaled to multiple GPUs in combination with the widely used hybrid parallel training approaches. Evaluating our prototype system shows that we can keep only 1.5% of the embedding parameters in the GPU to obtain a decent end-to-end training speed.

</p>
</details>

<details><summary><b>Learning from imperfect training data using a robust loss function: application to brain image segmentation</b>
<a href="https://arxiv.org/abs/2208.04941">arxiv:2208.04941</a>
&#x1F4C8; 2 <br>
<p>Haleh Akrami, Wenhui Cui, Anand A Joshi, Richard M. Leahy</p></summary>
<p>

**Abstract:** Segmentation is one of the most important tasks in MRI medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, head segmentation is commonly used for measuring and visualizing the brain's anatomical structures and is also a necessary step for other applications such as current-source reconstruction in electroencephalography and magnetoencephalography (EEG/MEG). Here we propose a deep learning framework that can segment brain, skull, and extra-cranial tissue using only T1-weighted MRI as input. In addition, we describe a robust method for training the model in the presence of noisy labels.

</p>
</details>

<details><summary><b>Using Large Context for Kidney Multi-Structure Segmentation from CTA Images</b>
<a href="https://arxiv.org/abs/2208.04525">arxiv:2208.04525</a>
&#x1F4C8; 2 <br>
<p>Weiwei Cao, Yuzhu Cao</p></summary>
<p>

**Abstract:** Accurate and automated segmentation of multi-structure (i.e., kidneys, renal tu-mors, arteries, and veins) from 3D CTA is one of the most important tasks for surgery-based renal cancer treatment (e.g., laparoscopic partial nephrectomy). This paper briefly presents the main technique details of the multi-structure seg-mentation method in MICCAI 2022 KIPA challenge. The main contribution of this paper is that we design the 3D UNet with the large context information cap-turing capability. Our method ranked eighth on the MICCAI 2022 KIPA chal-lenge open testing dataset with a mean position of 8.2. Our code and trained models are publicly available at https://github.com/fengjiejiejiejie/kipa22_nnunet.

</p>
</details>

<details><summary><b>A Time-to-first-spike Coding and Conversion Aware Training for Energy-Efficient Deep Spiking Neural Network Processor Design</b>
<a href="https://arxiv.org/abs/2208.04494">arxiv:2208.04494</a>
&#x1F4C8; 2 <br>
<p>Dongwoo Lew, Kyungchul Lee, Jongsun Park</p></summary>
<p>

**Abstract:** In this paper, we present an energy-efficient SNN architecture, which can seamlessly run deep spiking neural networks (SNNs) with improved accuracy. First, we propose a conversion aware training (CAT) to reduce ANN-to-SNN conversion loss without hardware implementation overhead. In the proposed CAT, the activation function developed for simulating SNN during ANN training, is efficiently exploited to reduce the data representation error after conversion. Based on the CAT technique, we also present a time-to-first-spike coding that allows lightweight logarithmic computation by utilizing spike time information. The SNN processor design that supports the proposed techniques has been implemented using 28nm CMOS process. The processor achieves the top-1 accuracies of 91.7%, 67.9% and 57.4% with inference energy of 486.7uJ, 503.6uJ, and 1426uJ to process CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, when running VGG-16 with 5bit logarithmic weights.

</p>
</details>

<details><summary><b>A Visual Analytics System for Improving Attention-based Traffic Forecasting Models</b>
<a href="https://arxiv.org/abs/2208.04350">arxiv:2208.04350</a>
&#x1F4C8; 2 <br>
<p>Seungmin Jin, Hyunwook Lee, Cheonbok Park, Hyeshin Chu, Yunwon Tae, Jaegul Choo, Sungahn Ko</p></summary>
<p>

**Abstract:** With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.

</p>
</details>

<details><summary><b>A Multi-label Continual Learning Framework to Scale Deep Learning Approaches for Packaging Equipment Monitoring</b>
<a href="https://arxiv.org/abs/2208.04227">arxiv:2208.04227</a>
&#x1F4C8; 2 <br>
<p>Davide Dalle Pezze, Denis Deronjic, Chiara Masiero, Diego Tosato, Alessandro Beghi, Gian Antonio Susto</p></summary>
<p>

**Abstract:** Continual Learning aims to learn from a stream of tasks, being able to remember at the same time both new and old tasks. While many approaches were proposed for single-class classification, multi-label classification in the continual scenario remains a challenging problem. For the first time, we study multi-label classification in the Domain Incremental Learning scenario. Moreover, we propose an efficient approach that has a logarithmic complexity with regard to the number of tasks, and can be applied also in the Class Incremental Learning scenario. We validate our approach on a real-world multi-label Alarm Forecasting problem from the packaging industry. For the sake of reproducibility, the dataset and the code used for the experiments are publicly available.

</p>
</details>

<details><summary><b>Three-Dimensional Coherent Diffractive Imaging of Isolated Faceted Nanostructures</b>
<a href="https://arxiv.org/abs/2208.04044">arxiv:2208.04044</a>
&#x1F4C8; 2 <br>
<p>Alessandro Colombo, Simon Dold, Patrice Kolb, Nils Bernhardt, Patrick Behrens, Jonathan Correa, Stefan Düsterer, Benjamin Erk, Linos Hecht, Andrea Heilrath, Robert Irsig, Norman Iwe, Jakob Jordan, Björn Kruse, Bruno Langbehn, Bastian Manschwetus, Franklin Martinez, Karl-Heinz Meiwes-Broer, Kevin Oldenburg, Christopher Passow, Christian Peltz, Mario Sauppe, Fabian Seel, Rico Mayro P. Tanyag, Rolf Treusch</p></summary>
<p>

**Abstract:** The structure and dynamics of isolated nanosamples in free flight can be directly visualized via single-shot coherent diffractive imaging using the intense and short pulses of X-ray free-electron lasers. Wide-angle scattering images even encode three-dimensional morphological information of the samples, but the retrieval of this information remains a challenge. Up to now, effective three-dimensional morphology reconstructions from single shots were only achieved via fitting with highly constrained models, requiring a priori knowledge about possible geometrical shapes. Here we present a much more generic imaging approach. Relying on a model that allows for any sample morphology described by a convex polyhedron, we reconstruct wide-angle diffraction patterns from individual silver nanoparticles. In addition to known structural motives with high symmetries, we retrieve imperfect shapes and agglomerates which were not accessible previously. Our results open new routes towards true 3D structure determination of single nanoparticles and, ultimately, 3D movies of ultrafast nanoscale dynamics.

</p>
</details>

<details><summary><b>Channel Estimation under Hardware Impairments: Bayesian Methods versus Deep Learning</b>
<a href="https://arxiv.org/abs/2208.04033">arxiv:2208.04033</a>
&#x1F4C8; 2 <br>
<p>Özlem Tugfe Demir, Emil Björnson</p></summary>
<p>

**Abstract:** This paper considers the impact of general hardware impairments in a multiple-antenna base station and user equipments on the uplink performance. First, the effective channels are analytically derived for distortion-aware receivers when using finite-sized signal constellations. Next, a deep feedforward neural network is designed and trained to estimate the effective channels. Its performance is compared with state-of-the-art distortion-aware and unaware Bayesian linear minimum mean-squared error (LMMSE) estimators. The proposed deep learning approach improves the estimation quality by exploiting impairment characteristics, while LMMSE methods treat distortion as noise.

</p>
</details>

<details><summary><b>Stain-Adaptive Self-Supervised Learning for Histopathology Image Analysis</b>
<a href="https://arxiv.org/abs/2208.04017">arxiv:2208.04017</a>
&#x1F4C8; 2 <br>
<p>Hai-Li Ye, Da-Han Wang</p></summary>
<p>

**Abstract:** It is commonly recognized that color variations caused by differences in stains is a critical issue for histopathology image analysis. Existing methods adopt color matching, stain separation, stain transfer or the combination of them to alleviate the stain variation problem. In this paper, we propose a novel Stain-Adaptive Self-Supervised Learning(SASSL) method for histopathology image analysis. Our SASSL integrates a domain-adversarial training module into the SSL framework to learn distinctive features that are robust to both various transformations and stain variations. The proposed SASSL is regarded as a general method for domain-invariant feature extraction which can be flexibly combined with arbitrary downstream histopathology image analysis modules (e.g. nuclei/tissue segmentation) by fine-tuning the features for specific downstream tasks. We conducted experiments on publicly available pathological image analysis datasets including the PANDA, BreastPathQ, and CAMELYON16 datasets, achieving the state-of-the-art performance. Experimental results demonstrate that the proposed method can robustly improve the feature extraction ability of the model, and achieve stable performance improvement in downstream tasks.

</p>
</details>

<details><summary><b>Ensembled Autoencoder Regularization for Multi-Structure Segmentation for Kidney Cancer Treatment</b>
<a href="https://arxiv.org/abs/2208.04007">arxiv:2208.04007</a>
&#x1F4C8; 2 <br>
<p>David Jozef Hresko, Marek Kurej, Jakub Gazda, Peter Drotar</p></summary>
<p>

**Abstract:** The kidney cancer is one of the most common cancer types. The treatment frequently include surgical intervention. However, surgery is in this case particularly challenging due to regional anatomical relations. Organ delineation can significantly improve surgical planning and execution. In this contribution, we propose ensemble of two fully convolutional networks for segmentation of kidney, tumor, veins and arteries. While SegResNet architecture achieved better performance on tumor, the nnU-Net provided more precise segmentation for kidneys, arteries and veins. So in our proposed approach we combine these two networks, and further boost the performance by mixup augmentation.

</p>
</details>

<details><summary><b>Towards lifelong learning of Recurrent Neural Networks for control design</b>
<a href="https://arxiv.org/abs/2208.03980">arxiv:2208.03980</a>
&#x1F4C8; 2 <br>
<p>Fabio Bonassi, Jing Xie, Marcello Farina, Riccardo Scattolini</p></summary>
<p>

**Abstract:** This paper proposes a method for lifelong learning of Recurrent Neural Networks, such as NNARX, ESN, LSTM, and GRU, to be used as plant models in control system synthesis. The problem is significant because in many practical applications it is required to adapt the model when new information is available and/or the system undergoes changes, without the need to store an increasing amount of data as time proceeds. Indeed, in this context, many problems arise, such as the well known Catastrophic Forgetting and Capacity Saturation ones. We propose an adaptation algorithm inspired by Moving Horizon Estimators, deriving conditions for its convergence. The described method is applied to a simulated chemical plant, already adopted as a challenging benchmark in the existing literature. The main results achieved are discussed.

</p>
</details>

<details><summary><b>All-optical image classification through unknown random diffusers using a single-pixel diffractive network</b>
<a href="https://arxiv.org/abs/2208.03968">arxiv:2208.03968</a>
&#x1F4C8; 2 <br>
<p>Yi Luo, Bijie Bai, Yuhang Li, Ege Cetintas, Aydogan Ozcan</p></summary>
<p>

**Abstract:** Classification of an object behind a random and unknown scattering medium sets a challenging task for computational imaging and machine vision fields. Recent deep learning-based approaches demonstrated the classification of objects using diffuser-distorted patterns collected by an image sensor. These methods demand relatively large-scale computing using deep neural networks running on digital computers. Here, we present an all-optical processor to directly classify unknown objects through unknown, random phase diffusers using broadband illumination detected with a single pixel. A set of transmissive diffractive layers, optimized using deep learning, forms a physical network that all-optically maps the spatial information of an input object behind a random diffuser into the power spectrum of the output light detected through a single pixel at the output plane of the diffractive network. We numerically demonstrated the accuracy of this framework using broadband radiation to classify unknown handwritten digits through random new diffusers, never used during the training phase, and achieved a blind testing accuracy of 88.53%. This single-pixel all-optical object classification system through random diffusers is based on passive diffractive layers that process broadband input light and can operate at any part of the electromagnetic spectrum by simply scaling the diffractive features proportional to the wavelength range of interest. These results have various potential applications in, e.g., biomedical imaging, security, robotics, and autonomous driving.

</p>
</details>

<details><summary><b>NPB-REC: Non-parametric Assessment of Uncertainty in Deep-learning-based MRI Reconstruction from Undersampled Data</b>
<a href="https://arxiv.org/abs/2208.03966">arxiv:2208.03966</a>
&#x1F4C8; 2 <br>
<p>Samah Khawaled, Moti Freiman</p></summary>
<p>

**Abstract:** Uncertainty quantification in deep-learning (DL) based image reconstruction models is critical for reliable clinical decision making based on the reconstructed images. We introduce "NPB-REC", a non-parametric fully Bayesian framework for uncertainty assessment in MRI reconstruction from undersampled "k-space" data. We use Stochastic gradient Langevin dynamics (SGLD) during the training phase to characterize the posterior distribution of the network weights. We demonstrated the added-value of our approach on the multi-coil brain MRI dataset, from the fastmri challenge, in comparison to the baseline E2E-VarNet with and without inference-time dropout. Our experiments show that NPB-REC outperforms the baseline by means of reconstruction accuracy (PSNR and SSIM of $34.55$, $0.908$ vs. $33.08$, $0.897$, $p<0.01$) in high acceleration rates ($R=8$). This is also measured in regions of clinical annotations. More significantly, it provides a more accurate estimate of the uncertainty that correlates with the reconstruction error, compared to the Monte-Carlo inference time Dropout method (Pearson correlation coefficient of $R=0.94$ vs. $R=0.91$). The proposed approach has the potential to facilitate safe utilization of DL based methods for MRI reconstruction from undersampled data. Code and trained models are available in \url{https://github.com/samahkh/NPB-REC}.

</p>
</details>

<details><summary><b>A high-resolution dynamical view on momentum methods for over-parameterized neural networks</b>
<a href="https://arxiv.org/abs/2208.03941">arxiv:2208.03941</a>
&#x1F4C8; 2 <br>
<p>Xin Liu, Wei Tao, Jun Wang, Zhisong Pan</p></summary>
<p>

**Abstract:** In this paper, we present the convergence analysis of momentum methods in training a two-layer over-parameterized ReLU neural network, where the number of parameters is significantly larger than that of training instances. Existing works on momentum methods show that the heavy-ball method (HB) and Nesterov's accelerated method (NAG) share the same limiting ordinary differential equation (ODE), which leads to identical convergence rate. From a high-resolution dynamical view, we show that HB differs from NAG in terms of the convergence rate. In addition, our findings provide tighter upper bounds on convergence for the high-resolution ODEs of HB and NAG.

</p>
</details>

<details><summary><b>Learning to Improve Code Efficiency</b>
<a href="https://arxiv.org/abs/2208.05297">arxiv:2208.05297</a>
&#x1F4C8; 1 <br>
<p>Binghong Chen, Daniel Tarlow, Kevin Swersky, Martin Maas, Pablo Heiber, Ashish Naik, Milad Hashemi, Parthasarathy Ranganathan</p></summary>
<p>

**Abstract:** Improvements in the performance of computing systems, driven by Moore's Law, have transformed society. As such hardware-driven gains slow down, it becomes even more important for software developers to focus on performance and efficiency during development. While several studies have demonstrated the potential from such improved code efficiency (e.g., 2x better generational improvements compared to hardware), unlocking these gains in practice has been challenging. Reasoning about algorithmic complexity and the interaction of coding patterns on hardware can be challenging for the average programmer, especially when combined with pragmatic constraints around development velocity and multi-person development.
  This paper seeks to address this problem. We analyze a large competitive programming dataset from the Google Code Jam competition and find that efficient code is indeed rare, with a 2x runtime difference between the median and the 90th percentile of solutions. We propose using machine learning to automatically provide prescriptive feedback in the form of hints, to guide programmers towards writing high-performance code. To automatically learn these hints from the dataset, we propose a novel discrete variational auto-encoder, where each discrete latent variable represents a different learned category of code-edit that increases performance. We show that this method represents the multi-modal space of code efficiency edits better than a sequence-to-sequence baseline and generates a distribution of more efficient solutions.

</p>
</details>

<details><summary><b>Attention Hijacking in Trojan Transformers</b>
<a href="https://arxiv.org/abs/2208.04946">arxiv:2208.04946</a>
&#x1F4C8; 1 <br>
<p>Weimin Lyu, Songzhu Zheng, Tengfei Ma, Haibin Ling, Chao Chen</p></summary>
<p>

**Abstract:** Trojan attacks pose a severe threat to AI systems. Recent works on Transformer models received explosive popularity and the self-attentions are now indisputable. This raises a central question: Can we reveal the Trojans through attention mechanisms in BERTs and ViTs? In this paper, we investigate the attention hijacking pattern in Trojan AIs, \ie, the trigger token ``kidnaps'' the attention weights when a specific trigger is present. We observe the consistent attention hijacking pattern in Trojan Transformers from both Natural Language Processing (NLP) and Computer Vision (CV) domains. This intriguing property helps us to understand the Trojan mechanism in BERTs and ViTs. We also propose an Attention-Hijacking Trojan Detector (AHTD) to discriminate the Trojan AIs from the clean ones.

</p>
</details>

<details><summary><b>Clustering Optimisation Method for Highly Connected Biological Data</b>
<a href="https://arxiv.org/abs/2208.04720">arxiv:2208.04720</a>
&#x1F4C8; 1 <br>
<p>Richard Tjörnhammar</p></summary>
<p>

**Abstract:** Currently, data-driven discovery in biological sciences resides in finding segmentation strategies in multivariate data that produce sensible descriptions of the data. Clustering is but one of several approaches and sometimes falls short because of difficulties in assessing reasonable cutoffs, the number of clusters that need to be formed or that an approach fails to preserve topological properties of the original system in its clustered form. In this work, we show how a simple metric for connectivity clustering evaluation leads to an optimised segmentation of biological data.
  The novelty of the work resides in the creation of a simple optimisation method for clustering crowded data. The resulting clustering approach only relies on metrics derived from the inherent properties of the clustering. The new method facilitates knowledge for optimised clustering, which is easy to implement.
  We discuss how the clustering optimisation strategy corresponds to the viable information content yielded by the final segmentation. We further elaborate on how the clustering results, in the optimal solution, corresponds to prior knowledge of three different data sets.

</p>
</details>

<details><summary><b>Learning to Learn to Predict Performance Regressions in Production at Meta</b>
<a href="https://arxiv.org/abs/2208.04351">arxiv:2208.04351</a>
&#x1F4C8; 1 <br>
<p>Moritz Beller, Hongyu Li, Vivek Nair, Vijayaraghavan Murali, Imad Ahmad, Jürgen Cito, Drew Carlson, Ari Aye, Wes Dyer</p></summary>
<p>

**Abstract:** Catching and attributing code change-induced performance regressions in production is hard; predicting them beforehand, even harder. A primer on automatically learning to predict performance regressions in software, this article gives an account of the experiences we gained when researching and deploying an ML-based regression prediction pipeline at Meta. In this paper, we report on a comparative study with four ML models of increasing complexity, from (1) code-opaque, over (2) Bag of Words, (3) off-the-shelve Transformer-based, to (4) a bespoke Transformer-based model, coined SuperPerforator. Our investigation shows the inherent difficulty of the performance prediction problem, which is characterized by a large imbalance of benign onto regressing changes. Our results also call into question the general applicability of Transformer-based architectures for performance prediction: an off-the-shelve CodeBERT-based approach had surprisingly poor performance; our highly customized SuperPerforator architecture initially achieved prediction performance that was just on par with simpler Bag of Words models, and only outperformed them for down-stream use cases. This ability of SuperPerforator to transfer to an application with few learning examples afforded an opportunity to deploy it in practice at Meta: it can act as a pre-filter to sort out changes that are unlikely to introduce a regression, truncating the space of changes to search a regression in by up to 43%, a 45x improvement over a random baseline. To gain further insight into SuperPerforator, we explored it via a series of experiments computing counterfactual explanations. These highlight which parts of a code change the model deems important, thereby validating the learned black-box model.

</p>
</details>

<details><summary><b>Is this Change the Answer to that Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness</b>
<a href="https://arxiv.org/abs/2208.04125">arxiv:2208.04125</a>
&#x1F4C8; 1 <br>
<p>Haoye Tian, Xunzhu Tang, Andrew Habib, Shangwen Wang, Kui Liu, Xin Xia, Jacques Klein, Tegawendé F. Bissyandé</p></summary>
<p>

**Abstract:** In this work, we propose a novel perspective to the problem of patch correctness assessment: a correct patch implements changes that "answer" to a problem posed by buggy behaviour. Concretely, we turn the patch correctness assessment into a Question Answering problem. To tackle this problem, our intuition is that natural language processing can provide the necessary representations and models for assessing the semantic correlation between a bug (question) and a patch (answer). Specifically, we consider as inputs the bug reports as well as the natural language description of the generated patches. Our approach, Quatrain, first considers state of the art commit message generation models to produce the relevant inputs associated to each generated patch. Then we leverage a neural network architecture to learn the semantic correlation between bug reports and commit messages. Experiments on a large dataset of 9135 patches generated for three bug datasets (Defects4j, Bugs.jar and Bears) show that Quatrain can achieve an AUC of 0.886 on predicting patch correctness, and recalling 93% correct patches while filtering out 62% incorrect patches. Our experimental results further demonstrate the influence of inputs quality on prediction performance. We further perform experiments to highlight that the model indeed learns the relationship between bug reports and code change descriptions for the prediction. Finally, we compare against prior work and discuss the benefits of our approach.

</p>
</details>

<details><summary><b>Optimistic Optimisation of Composite Objective with Exponentiated Update</b>
<a href="https://arxiv.org/abs/2208.04065">arxiv:2208.04065</a>
&#x1F4C8; 1 <br>
<p>Weijia Shao, Fikret Sivrikaya, Sahin Albayrak</p></summary>
<p>

**Abstract:** This paper proposes a new family of algorithms for the online optimisation of composite objectives. The algorithms can be interpreted as the combination of the exponentiated gradient and $p$-norm algorithm. Combined with algorithmic ideas of adaptivity and optimism, the proposed algorithms achieve a sequence-dependent regret upper bound, matching the best-known bounds for sparse target decision variables. Furthermore, the algorithms have efficient implementations for popular composite objectives and constraints and can be converted to stochastic optimisation algorithms with the optimal accelerated rate for smooth objectives.

</p>
</details>

<details><summary><b>Solving the Online Assignment Problem with Machine Learned Advice</b>
<a href="https://arxiv.org/abs/2208.04016">arxiv:2208.04016</a>
&#x1F4C8; 1 <br>
<p>Clarence Gabriel R. Kasilag, Pollux M. Rey, Jhoirene B. Clemente</p></summary>
<p>

**Abstract:** The online assignment problem plays an important role in operational research and computer science which is why immense attention has been given to improving its solution quality. Due to the incomplete information about the input, it is difficult for online algorithms to produce the optimal solution. The quality of the solution of an online algorithm is measured using a competitive ratio. No online deterministic algorithm can achieve a competitive ratio better than (2n-1). It has been shown that advice in online computation improves the lower bound of the competitive ratio of online problems. Advice in online computation can be interpreted as additional information for the online algorithm to compensate for the lack of information about the whole input sequence. In this study, we investigate how introducing machine-learned advice could improve the competitive ratio for this problem. We provide an online algorithm for the online assignment problem by simulating a machine learning algorithm that predicts the whole input in advance. We utilize an optimal offline algorithm to provide a matching solution from the predicted input. Furthermore, we investigate how the prediction error of machine learning affects the competitive ratio of the online algorithm. We utilize a benchmark data set to perform our empirical analysis. We show that as the Machine Learning prediction error increases, the solution quality decreases. Moreover, the magnitude of error is directly proportional to the size of the input. This result is analogous to the competitive ratio of the best deterministic algorithm for the online assignment problem which is dependent also on the parameter n.

</p>
</details>

<details><summary><b>Generating physically-consistent high-resolution climate data with hard-constrained neural networks</b>
<a href="https://arxiv.org/abs/2208.05424">arxiv:2208.05424</a>
&#x1F4C8; 0 <br>
<p>Paula Harder, Qidong Yang, Venkatesh Ramesh, Prasanna Sattigeri, Alex Hernandez-Garcia, Campbell Watson, Daniela Szwarcman, David Rolnick</p></summary>
<p>

**Abstract:** The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and therefore often predict quantities at a coarse spatial resolution. Statistical downscaling can provide an efficient method of upsampling low-resolution data. In this field, deep learning has been applied successfully, often using methods from the super-resolution domain in computer vision. Despite often achieving visually compelling results, such models often violate conservation laws when predicting physical variables. In order to conserve important physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep downscaling model while also increasing their performance according to traditional metrics. We introduce two ways of constraining the network: A renormalization layer added to the end of the neural network and a successive approach that scales with increasing upsampling factors. We show the applicability of our methods across different popular architectures and upsampling factors using ERA5 reanalysis data.

</p>
</details>

<details><summary><b>Bridging the gap between target-based and cell-based drug discovery with a graph generative multi-task model</b>
<a href="https://arxiv.org/abs/2208.04944">arxiv:2208.04944</a>
&#x1F4C8; 0 <br>
<p>Fan Hu, Dongqi Wang, Huazhen Huang, Yishen Hu, Peng Yin</p></summary>
<p>

**Abstract:** Drug discovery is vitally important for protecting human against disease. Target-based screening is one of the most popular methods to develop new drugs in the past several decades. This method efficiently screens candidate drugs inhibiting target protein in vitro, but it often fails due to inadequate activity of the selected drugs in vivo. Accurate computational methods are needed to bridge this gap. Here, we propose a novel graph multi task deep learning model to identify compounds carrying both target inhibitory and cell active (MATIC) properties. On a carefully curated SARS-CoV-2 dataset, the proposed MATIC model shows advantages comparing with traditional method in screening effective compounds in vivo. Next, we explored the model interpretability and found that the learned features for target inhibition (in vitro) or cell active (in vivo) tasks are different with molecular property correlations and atom functional attentions. Based on these findings, we utilized a monte carlo based reinforcement learning generative model to generate novel multi-property compounds with both in vitro and in vivo efficacy, thus bridging the gap between target-based and cell-based drug discovery.

</p>
</details>

<details><summary><b>Adversarial robustness of $β-$VAE through the lens of local geometry</b>
<a href="https://arxiv.org/abs/2208.03923">arxiv:2208.03923</a>
&#x1F4C8; 0 <br>
<p>Asif Khan, Amos Storkey</p></summary>
<p>

**Abstract:** Variational autoencoders (VAEs) are susceptible to adversarial attacks. An adversary can find a small perturbation in the input sample to change its latent encoding non-smoothly, thereby compromising the reconstruction. A known reason for such vulnerability is the latent space distortions arising from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in the inputs leads to a significant change in the latent space encodings. This paper demonstrates that the sensitivity around a data point is due to a directional bias of a stochastic pullback metric tensor induced by the encoder network. The pullback metric tensor measures the infinitesimal volume change from input to latent space. Thus, it can be viewed as a lens to analyse the effect of small changes in the input leading to distortions in the latent space. We propose robustness evaluation scores using the eigenspectrum of a pullback metric. Moreover, we empirically show that the scores correlate with the robustness parameter $β$ of the $β-$VAE.

</p>
</details>


{% endraw %}
Prev: [2022.08.07]({{ '/2022/08/07/2022.08.07.html' | relative_url }})  Next: [2022.08.09]({{ '/2022/08/09/2022.08.09.html' | relative_url }})