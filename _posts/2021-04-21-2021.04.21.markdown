## Summary for 2021-04-21, created on 2021-12-22


<details><summary><b>MLDS: A Dataset for Weight-Space Analysis of Neural Networks</b>
<a href="https://arxiv.org/abs/2104.10555">arxiv:2104.10555</a>
&#x1F4C8; 66 <br>
<p>John Clemens</p></summary>
<p>

**Abstract:** Neural networks are powerful models that solve a variety of complex real-world problems. However, the stochastic nature of training and large number of parameters in a typical neural model makes them difficult to evaluate via inspection. Research shows this opacity can hide latent undesirable behavior, be it from poorly representative training data or via malicious intent to subvert the behavior of the network, and that this behavior is difficult to detect via traditional indirect evaluation criteria such as loss. Therefore, it is time to explore direct ways to evaluate a trained neural model via its structure and weights. In this paper we present MLDS, a new dataset consisting of thousands of trained neural networks with carefully controlled parameters and generated via a global volunteer-based distributed computing platform. This dataset enables new insights into both model-to-model and model-to-training-data relationships. We use this dataset to show clustering of models in weight-space with identical training data and meaningful divergence in weight-space with even a small change to the training data, suggesting that weight-space analysis is a viable and effective alternative to loss for evaluating neural networks.

</p>
</details>

<details><summary><b>Deep Learning for Click-Through Rate Estimation</b>
<a href="https://arxiv.org/abs/2104.10584">arxiv:2104.10584</a>
&#x1F4C8; 45 <br>
<p>Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, Xiuqiang He</p></summary>
<p>

**Abstract:** Click-through rate (CTR) estimation plays as a core function module in various personalized online services, including online advertising, recommender systems, and web search etc. From 2015, the success of deep learning started to benefit CTR estimation performance and now deep CTR models have been widely applied in many industrial platforms. In this survey, we provide a comprehensive review of deep learning models for CTR estimation tasks. First, we take a review of the transfer from shallow to deep CTR models and explain why going deep is a necessary trend of development. Second, we concentrate on explicit feature interaction learning modules of deep CTR models. Then, as an important perspective on large platforms with abundant user histories, deep behavior models are discussed. Moreover, the recently emerged automated methods for deep CTR architecture design are presented. Finally, we summarize the survey and discuss the future prospects of this field.

</p>
</details>

<details><summary><b>Programmable 3D snapshot microscopy with Fourier convolutional networks</b>
<a href="https://arxiv.org/abs/2104.10611">arxiv:2104.10611</a>
&#x1F4C8; 19 <br>
<p>Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Misha B. Ahrens, Kaspar Podgorski, Srinivas C. Turaga</p></summary>
<p>

**Abstract:** 3D snapshot microscopy enables fast volumetric imaging by capturing a 3D volume in a single 2D camera image, and has found a variety of biological applications such as whole brain imaging of fast neural activity in larval zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is both sample- and task-dependent, with no general solution known. Highly programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure. We perform such optimization with deep learning, using a differentiable wave-optics simulation of light propagation through a programmable microscope and a neural network to reconstruct volumes from the microscope image. We introduce a class of global kernel Fourier convolutional neural networks which can efficiently decode information from multiple depths in the volume, globally encoded across a 3D snapshot image. We show that our proposed networks succeed in large field of view volume reconstruction and microscope parameter optimization where traditional networks fail. We also show that our networks outperform the state-of-the-art learned reconstruction algorithms for lensless computational photography.

</p>
</details>

<details><summary><b>Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models</b>
<a href="https://arxiv.org/abs/2104.10558">arxiv:2104.10558</a>
&#x1F4C8; 18 <br>
<p>Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright, Rowan McAllister, Joseph E. Gonzalez, Sergey Levine</p></summary>
<p>

**Abstract:** Humans have a remarkable ability to make decisions by accurately reasoning about future events, including the future behaviors and states of mind of other agents. Consider driving a car through a busy intersection: it is necessary to reason about the physics of the vehicle, the intentions of other drivers, and their beliefs about your own intentions. If you signal a turn, another driver might yield to you, or if you enter the passing lane, another driver might decelerate to give you room to merge in front. Competent drivers must plan how they can safely react to a variety of potential future behaviors of other agents before they make their next move. This requires contingency planning: explicitly planning a set of conditional actions that depend on the stochastic outcome of future events. In this work, we develop a general-purpose contingency planner that is learned end-to-end using high-dimensional scene observations and low-dimensional behavioral observations. We use a conditional autoregressive flow model to create a compact contingency planning space, and show how this model can tractably learn contingencies from behavioral observations. We developed a closed-loop control benchmark of realistic multi-agent scenarios in a driving simulator (CARLA), on which we compare our method to various noncontingent methods that reason about multi-agent future behavior, including several state-of-the-art deep learning-based planning approaches. We illustrate that these noncontingent planning methods fundamentally fail on this benchmark, and find that our deep contingency planning method achieves significantly superior performance. Code to run our benchmark and reproduce our results is available at https://sites.google.com/view/contingency-planning

</p>
</details>

<details><summary><b>Lossless Compression with Latent Variable Models</b>
<a href="https://arxiv.org/abs/2104.10544">arxiv:2104.10544</a>
&#x1F4C8; 17 <br>
<p>James Townsend</p></summary>
<p>

**Abstract:** We develop a simple and elegant method for lossless compression using latent variable models, which we call 'bits back with asymmetric numeral systems' (BB-ANS). The method involves interleaving encode and decode steps, and achieves an optimal rate when compressing batches of data. We demonstrate it firstly on the MNIST test set, showing that state-of-the-art lossless compression is possible using a small variational autoencoder (VAE) model. We then make use of a novel empirical insight, that fully convolutional generative models, trained on small images, are able to generalize to images of arbitrary size, and extend BB-ANS to hierarchical latent variable models, enabling state-of-the-art lossless compression of full-size colour images from the ImageNet dataset. We describe 'Craystack', a modular software framework which we have developed for rapid prototyping of compression using deep generative models.

</p>
</details>

<details><summary><b>Using CNNs for AD classification based on spatial correlation of BOLD signals during the observation</b>
<a href="https://arxiv.org/abs/2104.10596">arxiv:2104.10596</a>
&#x1F4C8; 15 <br>
<p>Nazanin Beheshti, Lennart Johnsson</p></summary>
<p>

**Abstract:** Resting state functional magnetic resonance images (fMRI) are commonly used for classification of patients as having Alzheimer's disease (AD), mild cognitive impairment (MCI), or being cognitive normal (CN). Most methods use time-series correlation of voxels signals during the observation period as a basis for the classification. In this paper we show that Convolutional Neural Network (CNN) classification based on spatial correlation of time-averaged signals yield a classification accuracy of up to 82% (sensitivity 86%, specificity 80%)for a data set with 429 subjects (246 cognitive normal and 183 Alzheimer patients). For the spatial correlation of time-averaged signal values we use voxel subdomains around center points of the 90 regions AAL atlas. We form the subdomains as sets of voxels along a Hilbert curve of a bounding box in which the brain is embedded with the AAL regions center points serving as subdomain seeds. The matrix resulting from the spatial correlation of the 90 arrays formed by the subdomain segments of the Hilbert curve yields a symmetric 90x90 matrix that is used for the classification based on two different CNN networks, a 4-layer CNN network with 3x3 filters and with 4, 8, 16, and 32 output channels respectively, and a 2-layer CNN network with 3x3 filters and with 4 and 8 output channels respectively. The results of the two networks are reported and compared.

</p>
</details>

<details><summary><b>Invertible Denoising Network: A Light Solution for Real Noise Removal</b>
<a href="https://arxiv.org/abs/2104.10546">arxiv:2104.10546</a>
&#x1F4C8; 14 <br>
<p>Yang Liu, Zhenyue Qin, Saeed Anwar, Pan Ji, Dongwoo Kim, Sabrina Caldwell, Tom Gedeon</p></summary>
<p>

**Abstract:** Invertible networks have various benefits for image denoising since they are lightweight, information-lossless, and memory-saving during back-propagation. However, applying invertible models to remove noise is challenging because the input is noisy, and the reversed output is clean, following two different distributions. We propose an invertible denoising network, InvDN, to address this challenge. InvDN transforms the noisy input into a low-resolution clean image and a latent representation containing noise. To discard noise and restore the clean image, InvDN replaces the noisy latent representation with another one sampled from a prior distribution during reversion. The denoising performance of InvDN is better than all the existing competitive models, achieving a new state-of-the-art result for the SIDD dataset while enjoying less run time. Moreover, the size of InvDN is far smaller, only having 4.2% of the number of parameters compared to the most recently proposed DANet. Further, via manipulating the noisy latent representation, InvDN is also able to generate noise more similar to the original one. Our code is available at: https://github.com/Yang-Liu1082/InvDN.git.

</p>
</details>

<details><summary><b>A Two-Stage Attentive Network for Single Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2104.10488">arxiv:2104.10488</a>
&#x1F4C8; 9 <br>
<p>Jiqing Zhang, Chengjiang Long, Yuxin Wang, Haiyin Piao, Haiyang Mei, Xin Yang, Baocai Yin</p></summary>
<p>

**Abstract:** Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and contribute remarkable progress. However, most of the existing CNNs-based SISR methods do not adequately explore contextual information in the feature extraction stage and pay little attention to the final high-resolution (HR) image reconstruction step, hence hindering the desired SR performance. To address the above two issues, in this paper, we propose a two-stage attentive network (TSAN) for accurate SISR in a coarse-to-fine manner. Specifically, we design a novel multi-context attentive block (MCAB) to make the network focus on more informative contextual features. Moreover, we present an essential refined attention block (RAB) which could explore useful cues in HR space for reconstructing fine-detailed HR image. Extensive evaluations on four benchmark datasets demonstrate the efficacy of our proposed TSAN in terms of quantitative metrics and visual effects. Code is available at https://github.com/Jee-King/TSAN.

</p>
</details>

<details><summary><b>Online GANs for Automatic Performance Testing</b>
<a href="https://arxiv.org/abs/2104.11069">arxiv:2104.11069</a>
&#x1F4C8; 8 <br>
<p>Ivan Porres, Hergys Rexha, Sébastien Lafond</p></summary>
<p>

**Abstract:** In this paper we present a novel algorithm for automatic performance testing that uses an online variant of the Generative Adversarial Network (GAN) to optimize the test generation process. The objective of the proposed approach is to generate, for a given test budget, a test suite containing a high number of tests revealing performance defects. This is achieved using a GAN to generate the tests and predict their outcome. This GAN is trained online while generating and executing the tests. The proposed approach does not require a prior training set or model of the system under test. We provide an initial evaluation the algorithm using an example test system, and compare the obtained results with other possible approaches.
  We consider that the presented algorithm serves as a proof of concept and we hope that it can spark a research discussion on the application of GANs to test generation.

</p>
</details>

<details><summary><b>A Short Survey of Pre-trained Language Models for Conversational AI-A NewAge in NLP</b>
<a href="https://arxiv.org/abs/2104.10810">arxiv:2104.10810</a>
&#x1F4C8; 8 <br>
<p>Munazza Zaib, Quan Z. Sheng, Wei Emma Zhang</p></summary>
<p>

**Abstract:** Building a dialogue system that can communicate naturally with humans is a challenging yet interesting problem of agent-based computing. The rapid growth in this area is usually hindered by the long-standing problem of data scarcity as these systems are expected to learn syntax, grammar, decision making, and reasoning from insufficient amounts of task-specific dataset. The recently introduced pre-trained language models have the potential to address the issue of data scarcity and bring considerable advantages by generating contextualized word embeddings. These models are considered counterpart of ImageNet in NLP and have demonstrated to capture different facets of language such as hierarchical relations, long-term dependency, and sentiment. In this short survey paper, we discuss the recent progress made in the field of pre-trained language models. We also deliberate that how the strengths of these language models can be leveraged in designing more engaging and more eloquent conversational agents. This paper, therefore, intends to establish whether these pre-trained models can overcome the challenges pertinent to dialogue systems, and how their architecture could be exploited in order to overcome these challenges. Open challenges in the field of dialogue systems have also been deliberated.

</p>
</details>

<details><summary><b>Causal-TGAN: Generating Tabular Data Using Causal Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2104.10680">arxiv:2104.10680</a>
&#x1F4C8; 8 <br>
<p>Bingyang Wen, Luis Oliveros Colon, K. P. Subbalakshmi, R. Chandramouli</p></summary>
<p>

**Abstract:** Synthetic data generation becomes prevalent as a solution to privacy leakage and data shortage. Generative models are designed to generate a realistic synthetic dataset, which can precisely express the data distribution for the real dataset. The generative adversarial networks (GAN), which gain great success in the computer vision fields, are doubtlessly used for synthetic data generation. Though there are prior works that have demonstrated great progress, most of them learn the correlations in the data distributions rather than the true processes in which the datasets are naturally generated. Correlation is not reliable for it is a statistical technique that only tells linear dependencies and is easily affected by the dataset's bias. Causality, which encodes all underlying factors of how the real data be naturally generated, is more reliable than correlation. In this work, we propose a causal model named Causal Tabular Generative Neural Network (Causal-TGAN) to generate synthetic tabular data using the tabular data's causal information. Extensive experiments on both simulated datasets and real datasets demonstrate the better performance of our method when given the true causal graph and a comparable performance when using the estimated causal graph.

</p>
</details>

<details><summary><b>User-oriented Fairness in Recommendation</b>
<a href="https://arxiv.org/abs/2104.10671">arxiv:2104.10671</a>
&#x1F4C8; 8 <br>
<p>Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang</p></summary>
<p>

**Abstract:** As a highly data-driven application, recommender systems could be affected by data bias, resulting in unfair results for different data groups, which could be a reason that affects the system performance. Therefore, it is important to identify and solve the unfairness issues in recommendation scenarios. In this paper, we address the unfairness problem in recommender systems from the user perspective. We group users into advantaged and disadvantaged groups according to their level of activity, and conduct experiments to show that current recommender systems will behave unfairly between two groups of users. Specifically, the advantaged users (active) who only account for a small proportion in data enjoy much higher recommendation quality than those disadvantaged users (inactive). Such bias can also affect the overall performance since the disadvantaged users are the majority. To solve this problem, we provide a re-ranking approach to mitigate this unfairness problem by adding constraints over evaluation metrics. The experiments we conducted on several real-world datasets with various recommendation algorithms show that our approach can not only improve group fairness of users in recommender systems, but also achieve better overall recommendation performance.

</p>
</details>

<details><summary><b>MetricOpt: Learning to Optimize Black-Box Evaluation Metrics</b>
<a href="https://arxiv.org/abs/2104.10631">arxiv:2104.10631</a>
&#x1F4C8; 8 <br>
<p>Chen Huang, Shuangfei Zhai, Pengsheng Guo, Josh Susskind</p></summary>
<p>

**Abstract:** We study the problem of directly optimizing arbitrary non-differentiable task evaluation metrics such as misclassification rate and recall. Our method, named MetricOpt, operates in a black-box setting where the computational details of the target metric are unknown. We achieve this by learning a differentiable value function, which maps compact task-specific model parameters to metric observations. The learned value function is easily pluggable into existing optimizers like SGD and Adam, and is effective for rapidly finetuning a pre-trained model. This leads to consistent improvements since the value function provides effective metric supervision during finetuning, and helps to correct the potential bias of loss-only supervision. MetricOpt achieves state-of-the-art performance on a variety of metrics for (image) classification, image retrieval and object detection. Solid benefits are found over competing methods, which often involve complex loss design or adaptation. MetricOpt also generalizes well to new tasks and model architectures.

</p>
</details>

<details><summary><b>Recurrent Feedback Improves Recognition of Partially Occluded Objects</b>
<a href="https://arxiv.org/abs/2104.10615">arxiv:2104.10615</a>
&#x1F4C8; 8 <br>
<p>Markus Roland Ernst, Jochen Triesch, Thomas Burwick</p></summary>
<p>

**Abstract:** Recurrent connectivity in the visual cortex is believed to aid object recognition for challenging conditions such as occlusion. Here we investigate if and how artificial neural networks also benefit from recurrence. We compare architectures composed of bottom-up, lateral and top-down connections and evaluate their performance using two novel stereoscopic occluded object datasets. We find that classification accuracy is significantly higher for recurrent models when compared to feedforward models of matched parametric complexity. Additionally we show that for challenging stimuli, the recurrent feedback is able to correctly revise the initial feedforward guess.

</p>
</details>

<details><summary><b>How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies</b>
<a href="https://arxiv.org/abs/2104.10513">arxiv:2104.10513</a>
&#x1F4C8; 8 <br>
<p>Soroosh Tayebi Arasteh, Mehrpad Monajem, Vincent Christlein, Philipp Heinrich, Anguelos Nicolaou, Hamidreza Naderi Boldaji, Mahshad Lotfinia, Stefan Evert</p></summary>
<p>

**Abstract:** Twitter sentiment analysis, which often focuses on predicting the polarity of tweets, has attracted increasing attention over the last years, in particular with the rise of deep learning (DL). In this paper, we propose a new task: predicting the predominant sentiment among (first-order) replies to a given tweet. Therefore, we created RETWEET, a large dataset of tweets and replies manually annotated with sentiment labels. As a strong baseline, we propose a two-stage DL-based method: first, we create automatically labeled training data by applying a standard sentiment classifier to tweet replies and aggregating its predictions for each original tweet; our rationale is that individual errors made by the classifier are likely to cancel out in the aggregation step. Second, we use the automatically labeled data for supervised training of a neural network to predict reply sentiment from the original tweets. The resulting classifier is evaluated on the new RETWEET dataset, showing promising results, especially considering that it has been trained without any manually labeled data. Both the dataset and the baseline implementation are publicly available.

</p>
</details>

<details><summary><b>Stateless Neural Meta-Learning using Second-Order Gradients</b>
<a href="https://arxiv.org/abs/2104.10527">arxiv:2104.10527</a>
&#x1F4C8; 7 <br>
<p>Mike Huisman, Aske Plaat, Jan N. van Rijn</p></summary>
<p>

**Abstract:** Deep learning typically requires large data sets and much compute power for each new problem that is learned. Meta-learning can be used to learn a good prior that facilitates quick learning, thereby relaxing these requirements so that new tasks can be learned quicker; two popular approaches are MAML and the meta-learner LSTM. In this work, we compare the two and formally show that the meta-learner LSTM subsumes MAML. Combining this insight with recent empirical findings, we construct a new algorithm (dubbed TURTLE) which is simpler than the meta-learner LSTM yet more expressive than MAML. TURTLE outperforms both techniques at few-shot sine wave regression and image classification on miniImageNet and CUB without any additional hyperparameter tuning, at a computational cost that is comparable with second-order MAML. The key to TURTLE's success lies in the use of second-order gradients, which also significantly increases the performance of the meta-learner LSTM by 1-6% accuracy.

</p>
</details>

<details><summary><b>Hierarchical Convolutional Neural Network with Feature Preservation and Autotuned Thresholding for Crack Detection</b>
<a href="https://arxiv.org/abs/2104.10511">arxiv:2104.10511</a>
&#x1F4C8; 7 <br>
<p>Qiuchen Zhu, Tran Hiep Dinh, Manh Duong Phung, Quang Phuc Ha</p></summary>
<p>

**Abstract:** Drone imagery is increasingly used in automated inspection for infrastructure surface defects, especially in hazardous or unreachable environments. In machine vision, the key to crack detection rests with robust and accurate algorithms for image processing. To this end, this paper proposes a deep learning approach using hierarchical convolutional neural networks with feature preservation (HCNNFP) and an intercontrast iterative thresholding algorithm for image binarization. First, a set of branch networks is proposed, wherein the output of previous convolutional blocks is half-sizedly concatenated to the current ones to reduce the obscuration in the down-sampling stage taking into account the overall information loss. Next, to extract the feature map generated from the enhanced HCNN, a binary contrast-based autotuned thresholding (CBAT) approach is developed at the post-processing step, where patterns of interest are clustered within the probability map of the identified features. The proposed technique is then applied to identify surface cracks on the surface of roads, bridges or pavements. An extensive comparison with existing techniques is conducted on various datasets and subject to a number of evaluation criteria including the average F-measure (AF\b{eta}) introduced here for dynamic quantification of the performance. Experiments on crack images, including those captured by unmanned aerial vehicles inspecting a monorail bridge. The proposed technique outperforms the existing methods on various tested datasets especially for GAPs dataset with an increase of about 1.4% in terms of AF\b{eta} while the mean percentage error drops by 2.2%. Such performance demonstrates the merits of the proposed HCNNFP architecture for surface defect inspection.

</p>
</details>

<details><summary><b>A Survey on Federated Learning and its Applications for Accelerating Industrial Internet of Things</b>
<a href="https://arxiv.org/abs/2104.10501">arxiv:2104.10501</a>
&#x1F4C8; 7 <br>
<p>Jiehan Zhou, Shouhua Zhang, Qinghua Lu, Wenbin Dai, Min Chen, Xin Liu, Susanna Pirttikangas, Yang Shi, Weishan Zhang, Enrique Herrera-Viedma</p></summary>
<p>

**Abstract:** Federated learning (FL) brings collaborative intelligence into industries without centralized training data to accelerate the process of Industry 4.0 on the edge computing level. FL solves the dilemma in which enterprises wish to make the use of data intelligence with security concerns. To accelerate industrial Internet of things with the further leverage of FL, existing achievements on FL are developed from three aspects: 1) define terminologies and elaborate a general framework of FL for accommodating various scenarios; 2) discuss the state-of-the-art of FL on fundamental researches including data partitioning, privacy preservation, model optimization, local model transportation, personalization, motivation mechanism, platform & tools, and benchmark; 3) discuss the impacts of FL from the economic perspective. To attract more attention from industrial academia and practice, a FL-transformed manufacturing paradigm is presented, and future research directions of FL are given and possible immediate applications in Industry 4.0 domain are also proposed.

</p>
</details>

<details><summary><b>SSLM: Self-Supervised Learning for Medical Diagnosis from MR Video</b>
<a href="https://arxiv.org/abs/2104.10481">arxiv:2104.10481</a>
&#x1F4C8; 7 <br>
<p>Siladittya Manna, Saumik Bhattacharya, Umapada Pal</p></summary>
<p>

**Abstract:** In medical image analysis, the cost of acquiring high-quality data and their annotation by experts is a barrier in many medical applications. Most of the techniques used are based on supervised learning framework and need a large amount of annotated data to achieve satisfactory performance. As an alternative, in this paper, we propose a self-supervised learning approach to learn the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful spatial context-invariant representations. The downstream task in our paper is a class imbalanced multi-label classification. Different experiments show that the features learnt by the pretext model provide explainable performance in the downstream task. Moreover, the efficiency and reliability of the proposed pretext model in learning representations of minority classes without applying any strategy towards imbalance in the dataset can be seen from the results. To the best of our knowledge, this work is the first work of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in class imbalanced multi-label classification tasks on MR video.
  The code for evaluation of the proposed work is available at https://github.com/sadimanna/sslm

</p>
</details>

<details><summary><b>Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations</b>
<a href="https://arxiv.org/abs/2104.10824">arxiv:2104.10824</a>
&#x1F4C8; 6 <br>
<p>Kaidong Li, Mohammad I. Fathan, Krushi Patel, Tianxiao Zhang, Cuncong Zhong, Ajay Bansal, Amit Rastogi, Jean S. Wang, Guanghui Wang</p></summary>
<p>

**Abstract:** Colorectal cancer (CRC) is one of the most common types of cancer with a high mortality rate. Colonoscopy is the preferred procedure for CRC screening and has proven to be effective in reducing CRC mortality. Thus, a reliable computer-aided polyp detection and classification system can significantly increase the effectiveness of colonoscopy. In this paper, we create an endoscopic dataset collected from various sources and annotate the ground truth of polyp location and classification results with the help of experienced gastroenterologists. The dataset can serve as a benchmark platform to train and evaluate the machine learning models for polyp classification. We have also compared the performance of eight state-of-the-art deep learning-based object detection models. The results demonstrate that deep CNN models are promising in CRC screening. This work can serve as a baseline for future research in polyp detection and classification.

</p>
</details>

<details><summary><b>Dataset Inference: Ownership Resolution in Machine Learning</b>
<a href="https://arxiv.org/abs/2104.10706">arxiv:2104.10706</a>
&#x1F4C8; 6 <br>
<p>Pratyush Maini, Mohammad Yaghini, Nicolas Papernot</p></summary>
<p>

**Abstract:** With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $dataset$ $inference$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.

</p>
</details>

<details><summary><b>Robust Kernel-based Distribution Regression</b>
<a href="https://arxiv.org/abs/2104.10637">arxiv:2104.10637</a>
&#x1F4C8; 6 <br>
<p>Zhan Yu, Daniel W. C. Ho, Ding-Xuan Zhou</p></summary>
<p>

**Abstract:** Regularization schemes for regression have been widely studied in learning theory and inverse problems. In this paper, we study distribution regression (DR) which involves two stages of sampling, and aims at regressing from probability measures to real-valued responses over a reproducing kernel Hilbert space (RKHS). Recently, theoretical analysis on DR has been carried out via kernel ridge regression and several learning behaviors have been observed. However, the topic has not been explored and understood beyond the least square based DR. By introducing a robust loss function $l_σ$ for two-stage sampling problems, we present a novel robust distribution regression (RDR) scheme. With a windowing function $V$ and a scaling parameter $σ$ which can be appropriately chosen, $l_σ$ can include a wide range of popular used loss functions that enrich the theme of DR. Moreover, the loss $l_σ$ is not necessarily convex, hence largely improving the former regression class (least square) in the literature of DR. The learning rates under different regularity ranges of the regression function $f_ρ$ are comprehensively studied and derived via integral operator techniques. The scaling parameter $σ$ is shown to be crucial in providing robustness and satisfactory learning rates of RDR.

</p>
</details>

<details><summary><b>SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer</b>
<a href="https://arxiv.org/abs/2104.10567">arxiv:2104.10567</a>
&#x1F4C8; 6 <br>
<p>Yueming Lyu, Jing Dong, Bo Peng, Wei Wang, Tieniu Tan</p></summary>
<p>

**Abstract:** In recent years, virtual makeup applications have become more and more popular. However, it is still challenging to propose a robust makeup transfer method in the real-world environment. Current makeup transfer methods mostly work well on good-conditioned clean makeup images, but transferring makeup that exhibits shadow and occlusion is not satisfying. To alleviate it, we propose a novel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN (SOGAN). Given the source and the reference faces, we first fit a 3D face model and then disentangle the faces into shape and texture. In the texture branch, we map the texture to the UV space and design a UV texture generator to transfer the makeup. Since human faces are symmetrical in the UV space, we can conveniently remove the undesired shadow and occlusion from the reference image by carefully designing a Flip Attention Module (FAM). After obtaining cleaner makeup features from the reference image, a Makeup Transfer Module (MTM) is introduced to perform accurate makeup transfer. The qualitative and quantitative experiments demonstrate that our SOGAN not only achieves superior results in shadow and occlusion situations but also performs well in large pose and expression variations.

</p>
</details>

<details><summary><b>Improving BERT Pretraining with Syntactic Supervision</b>
<a href="https://arxiv.org/abs/2104.10516">arxiv:2104.10516</a>
&#x1F4C8; 6 <br>
<p>Giorgos Tziafas, Konstantinos Kogkalidis, Gijs Wijnholds, Michael Moortgat</p></summary>
<p>

**Abstract:** Bidirectional masked Transformers have become the core theme in the current NLP landscape. Despite their impressive benchmarks, a recurring theme in recent research has been to question such models' capacity for syntactic generalization. In this work, we seek to address this question by adding a supervised, token-level supertagging objective to standard unsupervised pretraining, enabling the explicit incorporation of syntactic biases into the network's training dynamics. Our approach is straightforward to implement, induces a marginal computational overhead and is general enough to adapt to a variety of settings. We apply our methodology on Lassy Large, an automatically annotated corpus of written Dutch. Our experiments suggest that our syntax-aware model performs on par with established baselines, despite Lassy Large being one order of magnitude smaller than commonly used corpora.

</p>
</details>

<details><summary><b>Frequency Domain Loss Function for Deep Exposure Correction of Dark Images</b>
<a href="https://arxiv.org/abs/2104.10856">arxiv:2104.10856</a>
&#x1F4C8; 5 <br>
<p>Ojasvi Yadav, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic</p></summary>
<p>

**Abstract:** We address the problem of exposure correction of dark, blurry and noisy images captured in low-light conditions in the wild. Classical image-denoising filters work well in the frequency space but are constrained by several factors such as the correct choice of thresholds, frequency estimates etc. On the other hand, traditional deep networks are trained end-to-end in the RGB space by formulating this task as an image-translation problem. However, that is done without any explicit constraints on the inherent noise of the dark images and thus produce noisy and blurry outputs. To this end we propose a DCT/FFT based multi-scale loss function, which when combined with traditional losses, trains a network to translate the important features for visually pleasing output. Our loss function is end-to-end differentiable, scale-agnostic, and generic; i.e., it can be applied to both RAW and JPEG images in most existing frameworks without additional overhead. Using this loss function, we report significant improvements over the state-of-the-art using quantitative metrics and subjective tests.

</p>
</details>

<details><summary><b>Accurate and fast matrix factorization for low-rank learning</b>
<a href="https://arxiv.org/abs/2104.10785">arxiv:2104.10785</a>
&#x1F4C8; 5 <br>
<p>Reza Godaz, Reza Monsefi, Faezeh Toutounian, Reshad Hosseini</p></summary>
<p>

**Abstract:** In this paper, we tackle two important problems in low-rank learning, which are partial singular value decomposition and numerical rank estimation of huge matrices. By using the concepts of Krylov subspaces such as Golub-Kahan bidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose two methods for solving these problems in a fast and accurate way. Our experiments show the advantages of the proposed methods compared to the traditional and randomized singular value decomposition methods. The proposed methods are appropriate for applications involving huge matrices where the accuracy of the desired singular values and also all of their corresponding singular vectors are essential. As a real application, we evaluate the performance of our methods on the problem of Riemannian similarity learning between two various image datasets of MNIST and USPS.

</p>
</details>

<details><summary><b>Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud</b>
<a href="https://arxiv.org/abs/2104.10622">arxiv:2104.10622</a>
&#x1F4C8; 5 <br>
<p>Chenlei Lv, Weisi Lin, Baoquan Zhao</p></summary>
<p>

**Abstract:** Mesh reconstruction from a 3D point cloud is an important topic in the fields of computer graphic, computer vision, and multimedia analysis. In this paper, we propose a voxel structure-based mesh reconstruction framework. It provides the intrinsic metric to improve the accuracy of local region detection. Based on the detected local regions, an initial reconstructed mesh can be obtained. With the mesh optimization in our framework, the initial reconstructed mesh is optimized into an isotropic one with the important geometric features such as external and internal edges. The experimental results indicate that our framework shows great advantages over peer ones in terms of mesh quality, geometric feature keeping, and processing speed.

</p>
</details>

<details><summary><b>Covert Channel Attack to Federated Learning Systems</b>
<a href="https://arxiv.org/abs/2104.10561">arxiv:2104.10561</a>
&#x1F4C8; 5 <br>
<p>Gabriele Costa, Fabio Pinelli, Simone Soderi, Gabriele Tolomei</p></summary>
<p>

**Abstract:** Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.

</p>
</details>

<details><summary><b>On Sampling-Based Training Criteria for Neural Language Modeling</b>
<a href="https://arxiv.org/abs/2104.10507">arxiv:2104.10507</a>
&#x1F4C8; 5 <br>
<p>Yingbo Gao, David Thulke, Alexander Gerstenberger, Khoa Viet Tran, Ralf Schlüter, Hermann Ney</p></summary>
<p>

**Abstract:** As the vocabulary size of modern word-based language models becomes ever larger, many sampling-based training criteria are proposed and investigated. The essence of these sampling methods is that the softmax-related traversal over the entire vocabulary can be simplified, giving speedups compared to the baseline. A problem we notice about the current landscape of such sampling methods is the lack of a systematic comparison and some myths about preferring one over another. In this work, we consider Monte Carlo sampling, importance sampling, a novel method we call compensated partial summation, and noise contrastive estimation. Linking back to the three traditional criteria, namely mean squared error, binary cross-entropy, and cross-entropy, we derive the theoretical solutions to the training problems. Contrary to some common belief, we show that all these sampling methods can perform equally well, as long as we correct for the intended class posterior probabilities. Experimental results in language modeling and automatic speech recognition on Switchboard and LibriSpeech support our claim, with all sampling-based methods showing similar perplexities and word error rates while giving the expected speedups.

</p>
</details>

<details><summary><b>Reinforcement Learning for Traffic Signal Control: Comparison with Commercial Systems</b>
<a href="https://arxiv.org/abs/2104.10455">arxiv:2104.10455</a>
&#x1F4C8; 5 <br>
<p>Alvaro Cabrejas-Egea, Raymond Zhang, Neil Walton</p></summary>
<p>

**Abstract:** Recently, Intelligent Transportation Systems are leveraging the power of increased sensory coverage and computing power to deliver data-intensive solutions achieving higher levels of performance than traditional systems. Within Traffic Signal Control (TSC), this has allowed the emergence of Machine Learning (ML) based systems. Among this group, Reinforcement Learning (RL) approaches have performed particularly well. Given the lack of industry standards in ML for TSC, literature exploring RL often lacks comparison against commercially available systems and straightforward formulations of how the agents operate. Here we attempt to bridge that gap. We propose three different architectures for TSC RL agents and compare them against the currently used commercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code for them. The agents use variations of Deep Q-Learning and Actor Critic, using states and rewards based on queue lengths. Their performance is compared in across different map scenarios with variable demand, assessing them in terms of the global delay and average queue length. We find that the RL-based systems can significantly and consistently achieve lower delays when compared with existing commercial systems.

</p>
</details>

<details><summary><b>Brittle Features May Help Anomaly Detection</b>
<a href="https://arxiv.org/abs/2104.10453">arxiv:2104.10453</a>
&#x1F4C8; 5 <br>
<p>Kimberly T. Mai, Toby Davies, Lewis D. Griffin</p></summary>
<p>

**Abstract:** One-class anomaly detection is challenging. A representation that clearly distinguishes anomalies from normal data is ideal, but arriving at this representation is difficult since only normal data is available at training time. We examine the performance of representations, transferred from auxiliary tasks, for anomaly detection. Our results suggest that the choice of representation is more important than the anomaly detector used with these representations, although knowledge distillation can work better than using the representations directly. In addition, separability between anomalies and normal data is important but not the sole factor for a good representation, as anomaly detection performance is also correlated with more adversarially brittle features in the representation space. Finally, we show our configuration can detect 96.4% of anomalies in a genuine X-ray security dataset, outperforming previous results.

</p>
</details>

<details><summary><b>Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?</b>
<a href="https://arxiv.org/abs/2104.10441">arxiv:2104.10441</a>
&#x1F4C8; 5 <br>
<p>Tim Isbister, Fredrik Carlsson, Magnus Sahlgren</p></summary>
<p>

**Abstract:** Most work in NLP makes the assumption that it is desirable to develop solutions in the native language in question. There is consequently a strong trend towards building native language models even for low-resource languages. This paper questions this development, and explores the idea of simply translating the data into English, thereby enabling the use of pretrained, and large-scale, English language models. We demonstrate empirically that a large English language model coupled with modern machine translation outperforms native language models in most Scandinavian languages. The exception to this is Finnish, which we assume is due to inferior translation quality. Our results suggest that machine translation is a mature technology, which raises a serious counter-argument for training native language models for low-resource languages. This paper therefore strives to make a provocative but important point. As English language models are improving at an unprecedented pace, which in turn improves machine translation, it is from an empirical and environmental stand-point more effective to translate data from low-resource languages into English, than to build language models for such languages.

</p>
</details>

<details><summary><b>TITAN: T Cell Receptor Specificity Prediction with Bimodal Attention Networks</b>
<a href="https://arxiv.org/abs/2105.03323">arxiv:2105.03323</a>
&#x1F4C8; 4 <br>
<p>Anna Weber, Jannis Born, María Rodríguez Martínez</p></summary>
<p>

**Abstract:** Motivation: The activity of the adaptive immune system is governed by T-cells and their specific T-cell receptors (TCR), which selectively recognize foreign antigens. Recent advances in experimental techniques have enabled sequencing of TCRs and their antigenic targets (epitopes), allowing to research the missing link between TCR sequence and epitope binding specificity. Scarcity of data and a large sequence space make this task challenging, and to date only models limited to a small set of epitopes have achieved good performance. Here, we establish a k-nearest-neighbor (K-NN) classifier as a strong baseline and then propose TITAN (Tcr epITope bimodal Attention Networks), a bimodal neural network that explicitly encodes both TCR sequences and epitopes to enable the independent study of generalization capabilities to unseen TCRs and/or epitopes. Results: By encoding epitopes at the atomic level with SMILES sequences, we leverage transfer learning and data augmentation to enrich the input data space and boost performance. TITAN achieves high performance in the prediction of specificity of unseen TCRs (ROC-AUC 0.87 in 10-fold CV) and surpasses the results of the current state-of-the-art (ImRex) by a large margin. Notably, our Levenshtein-distance-based K-NN classifier also exhibits competitive performance on unseen TCRs. While the generalization to unseen epitopes remains challenging, we report two major breakthroughs. First, by dissecting the attention heatmaps, we demonstrate that the sparsity of available epitope data favors an implicit treatment of epitopes as classes. This may be a general problem that limits unseen epitope performance for sufficiently complex models. Second, we show that TITAN nevertheless exhibits significantly improved performance on unseen epitopes and is capable of focusing attention on chemically meaningful molecular structures.

</p>
</details>

<details><summary><b>Multi-task Learning with Attention for End-to-end Autonomous Driving</b>
<a href="https://arxiv.org/abs/2104.10753">arxiv:2104.10753</a>
&#x1F4C8; 4 <br>
<p>Keishi Ishihara, Anssi Kanervisto, Jun Miura, Ville Hautamäki</p></summary>
<p>

**Abstract:** Autonomous driving systems need to handle complex scenarios such as lane following, avoiding collisions, taking turns, and responding to traffic signals. In recent years, approaches based on end-to-end behavioral cloning have demonstrated remarkable performance in point-to-point navigational scenarios, using a realistic simulator and standard benchmarks. Offline imitation learning is readily available, as it does not require expensive hand annotation or interaction with the target environment, but it is difficult to obtain a reliable system. In addition, existing methods have not specifically addressed the learning of reaction for traffic lights, which are a rare occurrence in the training datasets. Inspired by the previous work on multi-task learning and attention modeling, we propose a novel multi-task attention-aware network in the conditional imitation learning (CIL) framework. This does not only improve the success rate of standard benchmarks, but also the ability to react to traffic lights, which we show with standard benchmarks.

</p>
</details>

<details><summary><b>AdaptiFont: Increasing Individuals' Reading Speed with a Generative Font Model and Bayesian Optimization</b>
<a href="https://arxiv.org/abs/2104.10741">arxiv:2104.10741</a>
&#x1F4C8; 4 <br>
<p>Florian Kadner, Yannik Keller, Constantin A. Rothkopf</p></summary>
<p>

**Abstract:** Digital text has become one of the primary ways of exchanging knowledge, but text needs to be rendered to a screen to be read. We present AdaptiFont, a human-in-the-loop system that is aimed at interactively increasing readability of text displayed on a monitor. To this end, we first learn a generative font space with non-negative matrix factorization from a set of classic fonts. In this space we generate new true-type-fonts through active learning, render texts with the new font, and measure individual users' reading speed. Bayesian optimization sequentially generates new fonts on the fly to progressively increase individuals' reading speed. The results of a user study show that this adaptive font generation system finds regions in the font space corresponding to high reading speeds, that these fonts significantly increase participants' reading speed, and that the found fonts are significantly different across individual readers.

</p>
</details>

<details><summary><b>Learning Fine-grained Fact-Article Correspondence in Legal Cases</b>
<a href="https://arxiv.org/abs/2104.10726">arxiv:2104.10726</a>
&#x1F4C8; 4 <br>
<p>Jidong Ge, Yunyun huang, Xiaoyu Shen, Chuanyi Li, Wei Hu</p></summary>
<p>

**Abstract:** Automatically recommending relevant law articles to a given legal case has attracted much attention as it can greatly release human labor from searching over the large database of laws. However, current researches only support coarse-grained recommendation where all relevant articles are predicted as a whole without explaining which specific fact each article is relevant with. Since one case can be formed of many supporting facts, traversing over them to verify the correctness of recommendation results can be time-consuming. We believe that learning fine-grained correspondence between each single fact and law articles is crucial for an accurate and trustworthy AI system. With this motivation, we perform a pioneering study and create a corpus with manually annotated fact-article correspondences. We treat the learning as a text matching task and propose a multi-level matching network to address it. To help the model better digest the content of law articles, we parse articles in form of premise-conclusion pairs with random forest. Experiments show that the parsed form yielded better performance and the resulting model surpassed other popular text matching baselines. Furthermore, we compare with previous researches and find that establishing the fine-grained fact-article correspondences can improve the recommendation accuracy by a large margin. Our best system reaches an F1 score of 96.3%, making it of great potential for practical use. It can also significantly boost the downstream task of legal decision prediction, increasing the F1 score by up to 12.7%.

</p>
</details>

<details><summary><b>Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings</b>
<a href="https://arxiv.org/abs/2104.10715">arxiv:2104.10715</a>
&#x1F4C8; 4 <br>
<p>Utkarsh Sarawgi, Rishab Khincha, Wazeer Zulfikar, Satrajit Ghosh, Pattie Maes</p></summary>
<p>

**Abstract:** Reliability of machine learning (ML) systems is crucial in safety-critical applications such as healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of ML systems in deployment. Sequential and parallel ensemble techniques have shown improved performance of ML systems in multi-modal settings by leveraging the feature sets together. We propose an uncertainty-aware boosting technique for multi-modal ensembling in order to focus on the data points with higher associated uncertainty estimates, rather than the ones with higher loss values. We evaluate this method on healthcare tasks related to Dementia and Parkinson's disease which involve real-world multi-modal speech and text data, wherein our method shows an improved performance. Additional analysis suggests that introducing uncertainty-awareness into the boosted ensembles decreases the overall entropy of the system, making it more robust to heteroscedasticity in the data, as well as better calibrating each of the modalities along with high quality prediction intervals. We open-source our entire codebase at https://github.com/usarawgi911/Uncertainty-aware-boosting

</p>
</details>

<details><summary><b>Photothermal-SR-Net: A Customized Deep Unfolding Neural Network for Photothermal Super Resolution Imaging</b>
<a href="https://arxiv.org/abs/2104.10563">arxiv:2104.10563</a>
&#x1F4C8; 4 <br>
<p>Samim Ahmadi, Linh Kästner, Jan Christian Hauffen, Peter Jung, Mathias Ziegler</p></summary>
<p>

**Abstract:** This paper presents deep unfolding neural networks to handle inverse problems in photothermal radiometry enabling super resolution (SR) imaging. Photothermal imaging is a well-known technique in active thermography for nondestructive inspection of defects in materials such as metals or composites. A grand challenge of active thermography is to overcome the spatial resolution limitation imposed by heat diffusion in order to accurately resolve each defect. The photothermal SR approach enables to extract high-frequency spatial components based on the deconvolution with the thermal point spread function. However, stable deconvolution can only be achieved by using the sparse structure of defect patterns, which often requires tedious, hand-crafted tuning of hyperparameters and results in computationally intensive algorithms. On this account, Photothermal-SR-Net is proposed in this paper, which performs deconvolution by deep unfolding considering the underlying physics. This enables to super resolve 2D thermal images for nondestructive testing with a substantially improved convergence rate. Since defects appear sparsely in materials, Photothermal-SR-Net applies trained block-sparsity thresholding to the acquired thermal images in each convolutional layer. The performance of the proposed approach is evaluated and discussed using various deep unfolding and thresholding approaches applied to 2D thermal images. Subsequently, studies are conducted on how to increase the reconstruction quality and the computational performance of Photothermal-SR-Net is evaluated. Thereby, it was found that the computing time for creating high-resolution images could be significantly reduced without decreasing the reconstruction quality by using pixel binning as a preprocessing step.

</p>
</details>

<details><summary><b>Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs</b>
<a href="https://arxiv.org/abs/2104.10553">arxiv:2104.10553</a>
&#x1F4C8; 4 <br>
<p>Luyang Luo, Hao Chen, Yongjie Xiao, Yanning Zhou, Xi Wang, Varut Vardhanabhuti, Mingxiang Wu, Pheng-Ann Heng</p></summary>
<p>

**Abstract:** Deep learning has demonstrated radiograph screening performances that are comparable or superior to radiologists. However, recent studies show that deep models for thoracic disease classification usually show degraded performance when applied to external data. Such phenomena can be categorized into shortcut learning, where the deep models learn unintended decision rules that can fit the identically distributed training and test set but fail to generalize to other distributions. A natural way to alleviate this defect is explicitly indicating the lesions and focusing the model on learning the intended features. In this paper, we conduct extensive retrospective experiments to compare a popular thoracic disease classification model, CheXNet, and a thoracic lesion detection model, CheXDet. We first showed that the two models achieved similar image-level classification performance on the internal test set with no significant differences under many scenarios. Meanwhile, we found incorporating external training data even led to performance degradation for CheXNet. Then, we compared the models' internal performance on the lesion localization task and showed that CheXDet achieved significantly better performance than CheXNet even when given 80% less training data. By further visualizing the models' decision-making regions, we revealed that CheXNet learned patterns other than the target lesions, demonstrating its shortcut learning defect. Moreover, CheXDet achieved significantly better external performance than CheXNet on both the image-level classification task and the lesion localization task. Our findings suggest improving annotation granularity for training deep learning systems as a promising way to elevate future deep learning-based diagnosis systems for clinical usage.

</p>
</details>

<details><summary><b>A Lightweight Concept Drift Detection and Adaptation Framework for IoT Data Streams</b>
<a href="https://arxiv.org/abs/2104.10529">arxiv:2104.10529</a>
&#x1F4C8; 4 <br>
<p>Li Yang, Abdallah Shami</p></summary>
<p>

**Abstract:** In recent years, with the increasing popularity of "Smart Technology", the number of Internet of Things (IoT) devices and systems have surged significantly. Various IoT services and functionalities are based on the analytics of IoT streaming data. However, IoT data analytics faces concept drift challenges due to the dynamic nature of IoT systems and the ever-changing patterns of IoT data streams. In this article, we propose an adaptive IoT streaming data analytics framework for anomaly detection use cases based on optimized LightGBM and concept drift adaptation. A novel drift adaptation method named Optimized Adaptive and Sliding Windowing (OASW) is proposed to adapt to the pattern changes of online IoT data streams. Experiments on two public datasets show the high accuracy and efficiency of our proposed adaptive LightGBM model compared against other state-of-the-art approaches. The proposed adaptive LightGBM model can perform continuous learning and drift adaptation on IoT data streams without human intervention.

</p>
</details>

<details><summary><b>Eye Know You: Metric Learning for End-to-end Biometric Authentication Using Eye Movements from a Longitudinal Dataset</b>
<a href="https://arxiv.org/abs/2104.10489">arxiv:2104.10489</a>
&#x1F4C8; 4 <br>
<p>Dillon Lohr, Henry Griffith, Oleg V Komogortsev</p></summary>
<p>

**Abstract:** While numerous studies have explored eye movement biometrics since the modality's inception in 2004, the permanence of eye movements remains largely unexplored as most studies utilize datasets collected within a short time frame. This paper presents a convolutional neural network for authenticating users using their eye movements. The network is trained with an established metric learning loss function, multi-similarity loss, which seeks to form a well-clustered embedding space and directly enables the enrollment and authentication of out-of-sample users. Performance measures are computed on GazeBase, a task-diverse and publicly-available dataset collected over a 37-month period. This study includes an exhaustive analysis of the effects of training on various tasks and downsampling from 1000 Hz to several lower sampling rates. Our results reveal that reasonable authentication accuracy may be achieved even during a low-cognitive-load task or at low sampling rates. Moreover, we find that eye movements are quite resilient against template aging after 3 years.

</p>
</details>

<details><summary><b>Improving the Accuracy of Early Exits in Multi-Exit Architectures via Curriculum Learning</b>
<a href="https://arxiv.org/abs/2104.10461">arxiv:2104.10461</a>
&#x1F4C8; 4 <br>
<p>Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis</p></summary>
<p>

**Abstract:** Deploying deep learning services for time-sensitive and resource-constrained settings such as IoT using edge computing systems is a challenging task that requires dynamic adjustment of inference time. Multi-exit architectures allow deep neural networks to terminate their execution early in order to adhere to tight deadlines at the cost of accuracy. To mitigate this cost, in this paper we introduce a novel method called Multi-Exit Curriculum Learning that utilizes curriculum learning, a training strategy for neural networks that imitates human learning by sorting the training samples based on their difficulty and gradually introducing them to the network. Experiments on CIFAR-10 and CIFAR-100 datasets and various configurations of multi-exit architectures show that our method consistently improves the accuracy of early exits compared to the standard training approach.

</p>
</details>

<details><summary><b>Jacobian Regularization for Mitigating Universal Adversarial Perturbations</b>
<a href="https://arxiv.org/abs/2104.10459">arxiv:2104.10459</a>
&#x1F4C8; 4 <br>
<p>Kenneth T. Co, David Martinez Rego, Emil C. Lupu</p></summary>
<p>

**Abstract:** Universal Adversarial Perturbations (UAPs) are input perturbations that can fool a neural network on large sets of data. They are a class of attacks that represents a significant threat as they facilitate realistic, practical, and low-cost attacks on neural networks. In this work, we derive upper bounds for the effectiveness of UAPs based on norms of data-dependent Jacobians. We empirically verify that Jacobian regularization greatly increases model robustness to UAPs by up to four times whilst maintaining clean performance. Our theoretical analysis also allows us to formulate a metric for the strength of shared adversarial perturbations between pairs of inputs. We apply this metric to benchmark datasets and show that it is highly correlated with the actual observed robustness. This suggests that realistic and practical universal attacks can be reliably mitigated without sacrificing clean accuracy, which shows promise for the robustness of machine learning systems.

</p>
</details>

<details><summary><b>Text Summarization of Czech News Articles Using Named Entities</b>
<a href="https://arxiv.org/abs/2104.10454">arxiv:2104.10454</a>
&#x1F4C8; 4 <br>
<p>Petr Marek, Štěpán Müller, Jakub Konrád, Petr Lorenc, Jan Pichl, Jan Šedivý</p></summary>
<p>

**Abstract:** The foundation for the research of summarization in the Czech language was laid by the work of Straka et al. (2018). They published the SumeCzech, a large Czech news-based summarization dataset, and proposed several baseline approaches. However, it is clear from the achieved results that there is a large space for improvement. In our work, we focus on the impact of named entities on the summarization of Czech news articles. First, we annotate SumeCzech with named entities. We propose a new metric ROUGE_NE that measures the overlap of named entities between the true and generated summaries, and we show that it is still challenging for summarization systems to reach a high score in it. We propose an extractive summarization approach Named Entity Density that selects a sentence with the highest ratio between a number of entities and the length of the sentence as the summary of the article. The experiments show that the proposed approach reached results close to the solid baseline in the domain of news articles selecting the first sentence. Moreover, we demonstrate that the selected sentence reflects the style of reports concisely identifying to whom, when, where, and what happened. We propose that such a summary is beneficial in combination with the first sentence of an article in voice applications presenting news articles. We propose two abstractive summarization approaches based on Seq2Seq architecture. The first approach uses the tokens of the article. The second approach has access to the named entity annotations. The experiments show that both approaches exceed state-of-the-art results previously reported by Straka et al. (2018), with the latter achieving slightly better results on SumeCzech's out-of-domain testing set.

</p>
</details>

<details><summary><b>Revisiting Document Representations for Large-Scale Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2104.10355">arxiv:2104.10355</a>
&#x1F4C8; 4 <br>
<p>Jihyung Kil, Wei-Lun Chao</p></summary>
<p>

**Abstract:** Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.

</p>
</details>

<details><summary><b>Automating Visual Blockage Classification of Culverts with Deep Learning</b>
<a href="https://arxiv.org/abs/2105.03232">arxiv:2105.03232</a>
&#x1F4C8; 3 <br>
<p>Umair Iqbal, Johan Barthelemy, Wanqing Li, Pascal Perez</p></summary>
<p>

**Abstract:** Blockage of culverts by transported debris materials is reported as main contributor in originating urban flash floods. Conventional modelling approaches had no success in addressing the problem largely because of unavailability of peak floods hydraulic data and highly non-linear behaviour of debris at culvert. This article explores a new dimension to investigate the issue by proposing the use of Intelligent Video Analytic (IVA) algorithms for extracting blockage related information. Potential of using existing Convolutional Neural Network (CNN) algorithms (i.e., DarkNet53, DenseNet121, InceptionResNetV2, InceptionV3, MobileNet, ResNet50, VGG16, EfficientNetB3, NASNet) is investigated over a custom collected blockage dataset (i.e., Images of Culvert Openings and Blockage (ICOB)) to predict the blockage in a given image. Models were evaluated based on their performance on test dataset (i.e., accuracy, loss, precision, recall, F1-score, Jaccard-Index), Floating Point Operations Per Second (FLOPs) and response times to process a single test instance. From the results, NASNet was reported most efficient in classifying the blockage with the accuracy of 85\%; however, EfficientNetB3 was recommended for the hardware implementation because of its improved response time with accuracy comparable to NASNet (i.e., 83\%). False Negative (FN) instances, False Positive (FP) instances and CNN layers activation suggested that background noise and oversimplified labelling criteria were two contributing factors in degraded performance of existing CNN algorithms.

</p>
</details>

<details><summary><b>Rapid Detection of Aircrafts in Satellite Imagery based on Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2104.11677">arxiv:2104.11677</a>
&#x1F4C8; 3 <br>
<p>Arsalan Tahir, Muhammad Adil, Arslan Ali</p></summary>
<p>

**Abstract:** Object detection is one of the fundamental objectives in Applied Computer Vision. In some of the applications, object detection becomes very challenging such as in the case of satellite image processing. Satellite image processing has remained the focus of researchers in domains of Precision Agriculture, Climate Change, Disaster Management, etc. Therefore, object detection in satellite imagery is one of the most researched problems in this domain. This paper focuses on aircraft detection. in satellite imagery using deep learning techniques. In this paper, we used YOLO deep learning framework for aircraft detection. This method uses satellite images collected by different sources as learning for the model to perform detection. Object detection in satellite images is mostly complex because objects have many variations, types, poses, sizes, complex and dense background. YOLO has some limitations for small size objects (less than$\sim$32 pixels per object), therefore we upsample the prediction grid to reduce the coarseness of the model and to accurately detect the densely clustered objects. The improved model shows good accuracy and performance on different unknown images having small, rotating, and dense objects to meet the requirements in real-time.

</p>
</details>

<details><summary><b>XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision Trees</b>
<a href="https://arxiv.org/abs/2104.10818">arxiv:2104.10818</a>
&#x1F4C8; 3 <br>
<p>Aaron M. Roth, Jing Liang, Dinesh Manocha</p></summary>
<p>

**Abstract:** We present a novel sensor-based learning navigation algorithm to compute a collision-free trajectory for a robot in dense and dynamic environments with moving obstacles or targets. Our approach uses deep reinforcement learning-based expert policy that is trained using a sim2real paradigm. In order to increase the reliability and handle the failure cases of the expert policy, we combine with a policy extraction technique to transform the resulting policy into a decision tree format. The resulting decision tree has properties which we use to analyze and modify the policy and improve performance on navigation metrics including smoothness, frequency of oscillation, frequency of immobilization, and obstruction of target. We are able to modify the policy to address these imperfections without retraining, combining the learning power of deep learning with the control of domain-specific algorithms. We highlight the benefits of our algorithm in simulated environments and navigating a Clearpath Jackal robot among moving pedestrians. (Videos at this url: https://gamma.umd.edu/researchdirections/xrl/navviper)

</p>
</details>

<details><summary><b>NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Methods and Results</b>
<a href="https://arxiv.org/abs/2104.10781">arxiv:2104.10781</a>
&#x1F4C8; 3 <br>
<p>Ren Yang, Radu Timofte, Jing Liu, Yi Xu, Xinjian Zhang, Minyi Zhao, Shuigeng Zhou, Kelvin C. K. Chan, Shangchen Zhou, Xiangyu Xu, Chen Change Loy, Xin Li, Fanglong Liu, He Zheng, Lielin Jiang, Qi Zhang, Dongliang He, Fu Li, Qingqing Dang, Yibin Huang, Matteo Maggioni, Zhongqian Fu, Shuai Xiao, Cheng li, Thomas Tanay</p></summary>
<p>

**Abstract:** This paper reviews the first NTIRE challenge on quality enhancement of compressed video, with a focus on the proposed methods and results. In this challenge, the new Large-scale Diverse Video (LDV) dataset is employed. The challenge has three tracks. Tracks 1 and 2 aim at enhancing the videos compressed by HEVC at a fixed QP, while Track 3 is designed for enhancing the videos compressed by x265 at a fixed bit-rate. Besides, the quality enhancement of Tracks 1 and 3 targets at improving the fidelity (PSNR), and Track 2 targets at enhancing the perceptual quality. The three tracks totally attract 482 registrations. In the test phase, 12 teams, 8 teams and 11 teams submitted the final results of Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of video quality enhancement. The homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh

</p>
</details>

<details><summary><b>Discovering Classification Rules for Interpretable Learning with Linear Programming</b>
<a href="https://arxiv.org/abs/2104.10751">arxiv:2104.10751</a>
&#x1F4C8; 3 <br>
<p>M. Hakan Akyüz, Ş. İlker Birbil</p></summary>
<p>

**Abstract:** Rules embody a set of if-then statements which include one or more conditions to classify a subset of samples in a dataset. In various applications such classification rules are considered to be interpretable by the decision makers. We introduce two new algorithms for interpretability and learning. Both algorithms take advantage of linear programming, and hence, they are scalable to large data sets. The first algorithm extracts rules for interpretation of trained models that are based on tree/rule ensembles. The second algorithm generates a set of classification rules through a column generation approach. The proposed algorithms return a set of rules along with their optimal weights indicating the importance of each rule for classification. Moreover, our algorithms allow assigning cost coefficients, which could relate to different attributes of the rules, such as; rule lengths, estimator weights, number of false negatives, and so on. Thus, the decision makers can adjust these coefficients to divert the training process and obtain a set of rules that are more appealing for their needs. We have tested the performances of both algorithms on a collection of datasets and presented a case study to elaborate on optimal rule weights. Our results show that a good compromise between interpretability and accuracy can be obtained by the proposed algorithms.

</p>
</details>

<details><summary><b>PocketNet: A Smaller Neural Network for Medical Image Analysis</b>
<a href="https://arxiv.org/abs/2104.10745">arxiv:2104.10745</a>
&#x1F4C8; 3 <br>
<p>Adrian Celaya, Jonas A. Actor, Rajarajeswari Muthusivarajan, Evan Gates, Caroline Chung, Dawid Schellingerhout, Beatrice Riviere, David Fuentes</p></summary>
<p>

**Abstract:** Medical imaging deep learning models are often large and complex, requiring specialized hardware to train and evaluate these models. To address such issues, we propose the PocketNet paradigm to reduce the size of deep learning models by throttling the growth of the number of channels in convolutional neural networks. We demonstrate that, for a range of segmentation and classification tasks, PocketNet architectures produce results comparable to that of conventional neural networks while reducing the number of parameters by multiple orders of magnitude, using up to 90% less GPU memory, and speeding up training times by up to 40%, thereby allowing such models to be trained and deployed in resource-constrained settings.

</p>
</details>

<details><summary><b>Link Prediction on N-ary Relational Data Based on Relatedness Evaluation</b>
<a href="https://arxiv.org/abs/2104.10424">arxiv:2104.10424</a>
&#x1F4C8; 3 <br>
<p>Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng</p></summary>
<p>

**Abstract:** With the overwhelming popularity of Knowledge Graphs (KGs), researchers have poured attention to link prediction to fill in missing facts for a long time. However, they mainly focus on link prediction on binary relational data, where facts are usually represented as triples in the form of (head entity, relation, tail entity). In practice, n-ary relational facts are also ubiquitous. When encountering such facts, existing studies usually decompose them into triples by introducing a multitude of auxiliary virtual entities and additional triples. These conversions result in the complexity of carrying out link prediction on n-ary relational data. It has even proven that they may cause loss of structure information. To overcome these problems, in this paper, we represent each n-ary relational fact as a set of its role and role-value pairs. We then propose a method called NaLP to conduct link prediction on n-ary relational data, which explicitly models the relatedness of all the role and role-value pairs in an n-ary relational fact. We further extend NaLP by introducing type constraints of roles and role-values without any external type-specific supervision, and proposing a more reasonable negative sampling mechanism. Experimental results validate the effectiveness and merits of the proposed methods.

</p>
</details>

<details><summary><b>Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification</b>
<a href="https://arxiv.org/abs/2104.10401">arxiv:2104.10401</a>
&#x1F4C8; 3 <br>
<p>Sangrok Lee, Taekang Woo, Sang Hun Lee</p></summary>
<p>

**Abstract:** Vehicle re-identification (Re-ID) distinguishes between the same vehicle and other vehicles in images. It is challenging due to significant intra-instance differences between identical vehicles from different views and subtle inter-instance differences of similar vehicles. Researchers have tried to address this problem by extracting features robust to variations of viewpoints and environments. More recently, they tried to improve performance by using additional metadata such as key points, orientation, and temporal information. Although these attempts have been relatively successful, they all require expensive annotations. Therefore, this paper proposes a novel deep neural network called a multi-attention-based soft partition (MUSP) network to solve this problem. This network does not use metadata and only uses multiple soft attentions to identify a specific vehicle area. This function was performed by metadata in previous studies. Experiments verified that MUSP achieved state-of-the-art (SOTA) performance for the VehicleID dataset without any additional annotations and was comparable to VeRi-776 and VERI-Wild.

</p>
</details>

<details><summary><b>Learning future terrorist targets through temporal meta-graphs</b>
<a href="https://arxiv.org/abs/2104.10398">arxiv:2104.10398</a>
&#x1F4C8; 3 <br>
<p>Gian Maria Campedelli, Mihovil Bartulovic, Kathleen M. Carley</p></summary>
<p>

**Abstract:** In the last 20 years, terrorism has led to hundreds of thousands of deaths and massive economic, political, and humanitarian crises in several regions of the world. Using real-world data on attacks occurred in Afghanistan and Iraq from 2001 to 2018, we propose the use of temporal meta-graphs and deep learning to forecast future terrorist targets. Focusing on three event dimensions, i.e., employed weapons, deployed tactics and chosen targets, meta-graphs map the connections among temporally close attacks, capturing their operational similarities and dependencies. From these temporal meta-graphs, we derive 2-day-based time series that measure the centrality of each feature within each dimension over time. Formulating the problem in the context of the strategic behavior of terrorist actors, these multivariate temporal sequences are then utilized to learn what target types are at the highest risk of being chosen. The paper makes two contributions. First, it demonstrates that engineering the feature space via temporal meta-graphs produces richer knowledge than shallow time-series that only rely on frequency of feature occurrences. Second, the performed experiments reveal that bi-directional LSTM networks achieve superior forecasting performance compared to other algorithms, calling for future research aiming at fully discovering the potential of artificial intelligence to counter terrorist violence.

</p>
</details>

<details><summary><b>Dual Head Adversarial Training</b>
<a href="https://arxiv.org/abs/2104.10377">arxiv:2104.10377</a>
&#x1F4C8; 3 <br>
<p>Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are known to be vulnerable to adversarial examples/attacks, raising concerns about their reliability in safety-critical applications. A number of defense methods have been proposed to train robust DNNs resistant to adversarial attacks, among which adversarial training has so far demonstrated the most promising results. However, recent studies have shown that there exists an inherent tradeoff between accuracy and robustness in adversarially-trained DNNs. In this paper, we propose a novel technique Dual Head Adversarial Training (DH-AT) to further improve the robustness of existing adversarial training methods. Different from existing improved variants of adversarial training, DH-AT modifies both the architecture of the network and the training strategy to seek more robustness. Specifically, DH-AT first attaches a second network head (or branch) to one intermediate layer of the network, then uses a lightweight convolutional neural network (CNN) to aggregate the outputs of the two heads. The training strategy is also adapted to reflect the relative importance of the two heads. We empirically show, on multiple benchmark datasets, that DH-AT can bring notable robustness improvements to existing adversarial training methods. Compared with TRADES, one state-of-the-art adversarial training method, our DH-AT can improve the robustness by 3.4% against PGD40 and 2.3% against AutoAttack, and also improve the clean accuracy by 1.8%.

</p>
</details>

<details><summary><b>3KG: Contrastive Learning of 12-Lead Electrocardiograms using Physiologically-Inspired Augmentations</b>
<a href="https://arxiv.org/abs/2106.04452">arxiv:2106.04452</a>
&#x1F4C8; 2 <br>
<p>Bryan Gopal, Ryan W. Han, Gautham Raghupathi, Andrew Y. Ng, Geoffrey H. Tison, Pranav Rajpurkar</p></summary>
<p>

**Abstract:** We propose 3KG, a physiologically-inspired contrastive learning approach that generates views using 3D augmentations of the 12-lead electrocardiogram. We evaluate representation quality by fine-tuning a linear layer for the downstream task of 23-class diagnosis on the PhysioNet 2020 challenge training data and find that 3KG achieves a $9.1\%$ increase in mean AUC over the best self-supervised baseline when trained on $1\%$ of labeled data. Our empirical analysis shows that combining spatial and temporal augmentations produces the strongest representations. In addition, we investigate the effect of this physiologically-inspired pretraining on downstream performance on different disease subgroups and find that 3KG makes the greatest gains for conduction and rhythm abnormalities. Our method allows for flexibility in incorporating other self-supervised strategies and highlights the potential for similar modality-specific augmentations for other biomedical signals.

</p>
</details>

<details><summary><b>Applications of Artificial Intelligence, Machine Learning and related techniques for Computer Networking Systems</b>
<a href="https://arxiv.org/abs/2105.15103">arxiv:2105.15103</a>
&#x1F4C8; 2 <br>
<p>Krishna M. Sivalingam</p></summary>
<p>

**Abstract:** This article presents a primer/overview of applications of Artificial Intelligence and Machine Learning (AI/ML) techniques to address problems in the domain of computer networking. In particular, the techniques have been used to support efficient and accurate traffic prediction, traffic classification, anomaly detection, network management, network security, network resource allocation and optimization, network scheduling algorithms, fault diagnosis and many more such applications. The article first summarizes some of the key networking concepts and a few representative machine learning techniques and algorithms. The article then presents details regarding the availability of data sets for networking applications and machine learning software and toolkits for processing these data sets. Highlights of some of the standards activities, pursued by ITU-T and ETSI, which are related to AI/ML for networking, are also presented. Finally, the article discusses a small set of representative networking problems where AI/ML techniques have been successfully applied.

</p>
</details>

<details><summary><b>Mini-batch graphs for robust image classification</b>
<a href="https://arxiv.org/abs/2105.03237">arxiv:2105.03237</a>
&#x1F4C8; 2 <br>
<p>Arnab Kumar Mondal, Vineet Jain, Kaleem Siddiqi</p></summary>
<p>

**Abstract:** Current deep learning models for classification tasks in computer vision are trained using mini-batches. In the present article, we take advantage of the relationships between samples in a mini-batch, using graph neural networks to aggregate information from similar images. This helps mitigate the adverse effects of alterations to the input images on classification performance. Diverse experiments on image-based object and scene classification show that this approach not only improves a classifier's performance but also increases its robustness to image perturbations and adversarial attacks. Further, we also show that mini-batch graph neural networks can help to alleviate the problem of mode collapse in Generative Adversarial Networks.

</p>
</details>

<details><summary><b>Understanding and Accelerating EM Algorithm's Convergence by Fair Competition Principle and Rate-Verisimilitude Function</b>
<a href="https://arxiv.org/abs/2104.12592">arxiv:2104.12592</a>
&#x1F4C8; 2 <br>
<p>Chenguang Lu</p></summary>
<p>

**Abstract:** Why can the Expectation-Maximization (EM) algorithm for mixture models converge? Why can different initial parameters cause various convergence difficulties? The Q-L synchronization theory explains that the observed data log-likelihood L and the complete data log-likelihood Q are positively correlated; we can achieve maximum L by maximizing Q. According to this theory, the Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to eliminate locally maximal Q for avoiding L's local convergence. However, this paper proves that in some cases, Q may and should decrease for L to increase; slow or local convergence exists only because of small samples and unfair competition. This paper uses marriage competition to explain different convergence difficulties and proposes the Fair Competition Principle (FCP) with an initialization map for improving initializations. It uses the rate-verisimilitude function, extended from the rate-distortion function, to explain the convergence of the EM and improved EM algorithms. This convergence proof adopts variational and iterative methods that Shannon et al. used for analyzing rate-distortion functions. The initialization map can vastly save both algorithms' running times for binary Gaussian mixtures. The FCP and the initialization map are useful for complicated mixtures but not sufficient; we need further studies for specific methods.

</p>
</details>

<details><summary><b>Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis</b>
<a href="https://arxiv.org/abs/2104.10851">arxiv:2104.10851</a>
&#x1F4C8; 2 <br>
<p>Alexander Hadjiivanov</p></summary>
<p>

**Abstract:** Most classical (non-spiking) neural network models disregard internal neuron dynamics and treat neurons as simple input integrators. However, biological neurons have an internal state governed by complex dynamics that plays a crucial role in learning, adaptation and the overall network activity and behaviour. This paper presents the Membrane Potential and Activation Threshold Homeostasis (MPATH) neuron model, which combines several biologically inspired mechanisms to efficiently simulate internal neuron dynamics with a single parameter analogous to the membrane time constant in biological neurons. The model allows neurons to maintain a form of dynamic equilibrium by automatically regulating their activity when presented with fluctuating input. One consequence of the MPATH model is that it imbues neurons with a sense of time without recurrent connections, paving the way for modelling processes that depend on temporal aspects of neuron activity. Experiments demonstrate the model's ability to adapt to and continually learn from its input.

</p>
</details>

<details><summary><b>Conditional Selective Inference for Robust Regression and Outlier Detection using Piecewise-Linear Homotopy Continuation</b>
<a href="https://arxiv.org/abs/2104.10840">arxiv:2104.10840</a>
&#x1F4C8; 2 <br>
<p>Toshiaki Tsukurimichi, Yu Inatsu, Vo Nguyen Le Duy, Ichiro Takeuchi</p></summary>
<p>

**Abstract:** In practical data analysis under noisy environment, it is common to first use robust methods to identify outliers, and then to conduct further analysis after removing the outliers. In this paper, we consider statistical inference of the model estimated after outliers are removed, which can be interpreted as a selective inference (SI) problem. To use conditional SI framework, it is necessary to characterize the events of how the robust method identifies outliers. Unfortunately, the existing methods cannot be directly used here because they are applicable to the case where the selection events can be represented by linear/quadratic constraints. In this paper, we propose a conditional SI method for popular robust regressions by using homotopy method. We show that the proposed conditional SI method is applicable to a wide class of robust regression and outlier detection methods and has good empirical performance on both synthetic data and real data experiments.

</p>
</details>

<details><summary><b>Sharp Global Guarantees for Nonconvex Low-Rank Matrix Recovery in the Overparameterized Regime</b>
<a href="https://arxiv.org/abs/2104.10790">arxiv:2104.10790</a>
&#x1F4C8; 2 <br>
<p>Richard Y. Zhang</p></summary>
<p>

**Abstract:** We prove that it is possible for nonconvex low-rank matrix recovery to contain no spurious local minima when the rank of the unknown ground truth $r^{\star}<r$ is strictly less than the search rank $r$, and yet for the claim to be false when $r^{\star}=r$. Under the restricted isometry property (RIP), we prove, for the general overparameterized regime with $r^{\star}\le r$, that an RIP constant of $δ<1/(1+\sqrt{r^{\star}/r})$ is sufficient for the inexistence of spurious local minima, and that $δ<1/(1+1/\sqrt{r-r^{\star}+1})$ is necessary due to existence of counterexamples. Without an explicit control over $r^{\star}\le r$, an RIP constant of $δ<1/2$ is both necessary and sufficient for the exact recovery of a rank-$r$ ground truth. But if the ground truth is known a priori to have $r^{\star}=1$, then the sharp RIP threshold for exact recovery is improved to $δ<1/(1+1/\sqrt{r})$.

</p>
</details>

<details><summary><b>NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset and Study</b>
<a href="https://arxiv.org/abs/2104.10782">arxiv:2104.10782</a>
&#x1F4C8; 2 <br>
<p>Ren Yang, Radu Timofte</p></summary>
<p>

**Abstract:** This paper introduces a novel dataset for video enhancement and studies the state-of-the-art methods of the NTIRE 2021 challenge on quality enhancement of compressed video. The challenge is the first NTIRE challenge in this direction, with three competitions, hundreds of participants and tens of proposed solutions. Our newly collected Large-scale Diverse Video (LDV) dataset is employed in the challenge. In our study, we analyze the proposed methods of the challenge and several methods in previous works on the proposed LDV dataset. We find that the NTIRE 2021 challenge advances the state-of-the-art of quality enhancement on compressed video. The proposed LDV dataset is publicly available at the homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh

</p>
</details>

<details><summary><b>Skeleton Clustering: Dimension-Free Density-based Clustering</b>
<a href="https://arxiv.org/abs/2104.10770">arxiv:2104.10770</a>
&#x1F4C8; 2 <br>
<p>Zeyu Wei, Yen-Chi Chen</p></summary>
<p>

**Abstract:** We introduce a density-based clustering method called skeleton clustering that can detect clusters in multivariate and even high-dimensional data with irregular shapes. To bypass the curse of dimensionality, we propose surrogate density measures that are less dependent on the dimension but have intuitive geometric interpretations. The clustering framework constructs a concise representation of the given data as an intermediate step and can be thought of as a combination of prototype methods, density-based clustering, and hierarchical clustering. We show by theoretical analysis and empirical studies that the skeleton clustering leads to reliable clusters in multivariate and high-dimensional scenarios.

</p>
</details>

<details><summary><b>Automatic model training under restrictive time constraints</b>
<a href="https://arxiv.org/abs/2104.10746">arxiv:2104.10746</a>
&#x1F4C8; 2 <br>
<p>Lukas Cironis, Jan Palczewski, Georgios Aivaliotis</p></summary>
<p>

**Abstract:** We develop a hyperparameter optimisation algorithm, Automated Budget Constrained Training (AutoBCT), which balances the quality of a model with the computational cost required to tune it. The relationship between hyperparameters, model quality and computational cost must be learnt and this learning is incorporated directly into the optimisation problem. At each training epoch, the algorithm decides whether to terminate or continue training, and, in the latter case, what values of hyperparameters to use. This decision weighs optimally potential improvements in the quality with the additional training time and the uncertainty about the learnt quantities. The performance of our algorithm is verified on a number of machine learning problems encompassing random forests and neural networks. Our approach is rooted in the theory of Markov decision processes with partial information and we develop a numerical method to compute the value function and an optimal strategy.

</p>
</details>

<details><summary><b>Robustness of ML-Enhanced IDS to Stealthy Adversaries</b>
<a href="https://arxiv.org/abs/2104.10742">arxiv:2104.10742</a>
&#x1F4C8; 2 <br>
<p>Vance Wong, John Emanuello</p></summary>
<p>

**Abstract:** Intrusion Detection Systems (IDS) enhanced with Machine Learning (ML) have demonstrated the capacity to efficiently build a prototype of "normal" cyber behaviors in order to detect cyber threats' activity with greater accuracy than traditional rule-based IDS. Because these are largely black boxes, their acceptance requires proof of robustness to stealthy adversaries. Since it is impossible to build a baseline from activity completely clean of that of malicious cyber actors (outside of controlled experiments), the training data for deployed models will be poisoned with examples of activity that analysts would want to be alerted about. We train an autoencoder-based anomaly detection system on network activity with various proportions of malicious activity mixed in and demonstrate that they are robust to this sort of poisoning.

</p>
</details>

<details><summary><b>Mixture Models for the Analysis, Edition, and Synthesis of Continuous Time Series</b>
<a href="https://arxiv.org/abs/2104.10731">arxiv:2104.10731</a>
&#x1F4C8; 2 <br>
<p>Sylvain Calinon</p></summary>
<p>

**Abstract:** This chapter presents an overview of techniques used for the analysis, edition, and synthesis of time series, with a particular emphasis on motion data. The use of mixture models allows the decomposition of time signals as a superposition of basis functions. It provides a compact representation that aims at keeping the essential characteristics of the signals. Various types of basis functions have been proposed, with developments originating from different fields of research, including computer graphics, human motion science, robotics, control, and neuroscience. Examples of applications with radial, Bernstein and Fourier basis functions will be presented, with associated source codes to get familiar with these techniques.

</p>
</details>

<details><summary><b>Deep limits and cut-off phenomena for neural networks</b>
<a href="https://arxiv.org/abs/2104.10727">arxiv:2104.10727</a>
&#x1F4C8; 2 <br>
<p>Benny Avelin, Anders Karlsson</p></summary>
<p>

**Abstract:** We consider dynamical and geometrical aspects of deep learning. For many standard choices of layer maps we display semi-invariant metrics which quantify differences between data or decision functions. This allows us, when considering random layer maps and using non-commutative ergodic theorems, to deduce that certain limits exist when letting the number of layers tend to infinity. We also examine the random initialization of standard networks where we observe a surprising cut-off phenomenon in terms of the number of layers, the depth of the network. This could be a relevant parameter when choosing an appropriate number of layers for a given learning task, or for selecting a good initialization procedure. More generally, we hope that the notions and results in this paper can provide a framework, in particular a geometric one, for a part of the theoretical understanding of deep neural networks.

</p>
</details>

<details><summary><b>A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection</b>
<a href="https://arxiv.org/abs/2104.10719">arxiv:2104.10719</a>
&#x1F4C8; 2 <br>
<p>Biswadeep Chakraborty, Xueyuan She, Saibal Mukhopadhyay</p></summary>
<p>

**Abstract:** This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on Convolutional SNN using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being 150X energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.

</p>
</details>

<details><summary><b>Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose Estimation</b>
<a href="https://arxiv.org/abs/2104.10414">arxiv:2104.10414</a>
&#x1F4C8; 2 <br>
<p>Zhong-Qiu Zhao, Yao Gao, Yuchen Ge, Weidong Tian</p></summary>
<p>

**Abstract:** Although deep convolution neural networks (DCNN) have achieved excellent performance in human pose estimation, these networks often have a large number of parameters and computations, leading to the slow inference speed. For this issue, an effective solution is knowledge distillation, which transfers knowledge from a large pre-trained network (teacher) to a small network (student). However, there are some defects in the existing approaches: (I) Only a single teacher is adopted, neglecting the potential that a student can learn from multiple teachers. (II) The human segmentation mask can be regarded as additional prior information to restrict the location of keypoints, which is never utilized. (III) A student with a small number of parameters cannot fully imitate heatmaps provided by datasets and teachers. (IV) There exists noise in heatmaps generated by teachers, which causes model degradation. To overcome these defects, we propose an orderly dual-teacher knowledge distillation (ODKD) framework, which consists of two teachers with different capabilities. Specifically, the weaker one (primary teacher, PT) is used to teach keypoints information, the stronger one (senior teacher, ST) is utilized to transfer segmentation and keypoints information by adding the human segmentation mask. Taking dual-teacher together, an orderly learning strategy is proposed to promote knowledge absorbability. Moreover, we employ a binarization operation which further improves the learning ability of the student and reduces noise in heatmaps. Experimental results on COCO and OCHuman keypoints datasets show that our proposed ODKD can improve the performance of different lightweight models by a large margin, and HRNet-W16 equipped with ODKD achieves state-of-the-art performance for lightweight human pose estimation.

</p>
</details>

<details><summary><b>Federated Traffic Synthesizing and Classification Using Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2104.10400">arxiv:2104.10400</a>
&#x1F4C8; 2 <br>
<p>Chenxin Xu, Rong Xia, Yong Xiao, Yingyu Li, Guangming Shi, Kwang-cheng Chen</p></summary>
<p>

**Abstract:** With the fast growing demand on new services and applications as well as the increasing awareness of data protection, traditional centralized traffic classification approaches are facing unprecedented challenges. This paper introduces a novel framework, Federated Generative Adversarial Networks and Automatic Classification (FGAN-AC), which integrates decentralized data synthesizing with traffic classification. FGAN-AC is able to synthesize and classify multiple types of service data traffic from decentralized local datasets without requiring a large volume of manually labeled dataset or causing any data leakage. Two types of data synthesizing approaches have been proposed and compared: computation-efficient FGAN (FGAN-\uppercase\expandafter{\romannumeral1}) and communication-efficient FGAN (FGAN-\uppercase\expandafter{\romannumeral2}). The former only implements a single CNN model for processing each local dataset and the later only requires coordination of intermediate model training parameters. An automatic data classification and model updating framework has been proposed to automatically identify unknown traffic from the synthesized data samples and create new pseudo-labels for model training. Numerical results show that our proposed framework has the ability to synthesize highly mixed service data traffic and can significantly improve the traffic classification performance compared to existing solutions.

</p>
</details>

<details><summary><b>Towards Corruption-Agnostic Robust Domain Adaptation</b>
<a href="https://arxiv.org/abs/2104.10376">arxiv:2104.10376</a>
&#x1F4C8; 2 <br>
<p>Yifan Xu, Kekai Sheng, Weiming Dong, Baoyuan Wu, Changsheng Xu, Bao-Gang Hu</p></summary>
<p>

**Abstract:** Big progress has been achieved in domain adaptation in decades. Existing works are always based on an ideal assumption that testing target domain are i.i.d. with training target domains. However, due to unpredictable corruptions (e.g., noise and blur) in real data like web images, domain adaptation methods are increasingly required to be corruption robust on target domains. In this paper, we investigate a new task, Corruption-agnostic Robust Domain Adaptation (CRDA): to be accurate on original data and robust against unavailable-for-training corruptions on target domains. This task is non-trivial due to large domain discrepancy and unsupervised target domains. We observe that simple combinations of popular methods of domain adaptation and corruption robustness have sub-optimal CRDA results. We propose a new approach based on two technical insights into CRDA: 1) an easy-to-plug module called Domain Discrepancy Generator (DDG) that generates samples that enlarge domain discrepancy to mimic unpredictable corruptions; 2) a simple but effective teacher-student scheme with contrastive loss to enhance the constraints on target domains. Experiments verify that DDG keeps or even improves performance on original data and achieves better corruption robustness that baselines.

</p>
</details>

<details><summary><b>Machine-Learning Assisted Optimization Strategies for Phase Change Materials Embedded within Electronic Packages</b>
<a href="https://arxiv.org/abs/2104.14433">arxiv:2104.14433</a>
&#x1F4C8; 1 <br>
<p>Meghavin Bhatasana, Amy Marconnet</p></summary>
<p>

**Abstract:** Leveraging the latent heat of phase change materials (PCMs) can reduce the peak temperatures and transient variations in temperature in electronic devices. But as the power levels increase, the thermal conduction pathway from the heat source to the heat sink limits the effectiveness of these systems. In this work, we evaluate embedding the PCM within the silicon device layer of an electronic device to minimize the thermal resistance between the source and the PCM to minimize this thermal resistance and enhance the thermal performance of the device. The geometry and material properties of the embedded PCM regions are optimized using a combination of parametric and machine learning algorithms. For a fixed geometry, considering commercially available materials, Solder 174 significantly outperforms other organic and metallic PCMs. Also with a fixed geometry, the optimal melting points to minimize the peak temperature is higher than the optimal melting point to minimize the amplitude of the transient temperature oscillation, and both optima increase with increasing heater power. Extending beyond conventional optimization strategies, genetic algorithms and particle swarm optimization with and without neural network surrogate models are used to enable optimization of many geometric and material properties. For the test case evaluated, the optimized geometries and properties are similar between all ML-assisted algorithms, but the computational time depends on the technique. Ultimately, the optimized design with embedded phase change materials reduces the maximum temperature rise by 19% and the fluctuations by up to 88% compared to devices without PCM.

</p>
</details>

<details><summary><b>Assessing Validity of Static Analysis Warnings using Ensemble Learning</b>
<a href="https://arxiv.org/abs/2104.11593">arxiv:2104.11593</a>
&#x1F4C8; 1 <br>
<p>Anshul Tanwar, Hariharan Manikandan, Krishna Sundaresan, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi</p></summary>
<p>

**Abstract:** Static Analysis (SA) tools are used to identify potential weaknesses in code and fix them in advance, while the code is being developed. In legacy codebases with high complexity, these rules-based static analysis tools generally report a lot of false warnings along with the actual ones. Though the SA tools uncover many hidden bugs, they are lost in the volume of fake warnings reported. The developers expend large hours of time and effort in identifying the true warnings. Other than impacting the developer productivity, true bugs are also missed out due to this challenge. To address this problem, we propose a Machine Learning (ML)-based learning process that uses source codes, historic commit data, and classifier-ensembles to prioritize the True warnings from the given list of warnings. This tool is integrated into the development workflow to filter out the false warnings and prioritize actual bugs. We evaluated our approach on the networking C codes, from a large data pool of static analysis warnings reported by the tools. Time-to-time these warnings are addressed by the developers, labelling them as authentic bugs or fake alerts. The ML model is trained with full supervision over the code features. Our results confirm that applying deep learning over the traditional static analysis reports is an assuring approach for drastically reducing the false positive rates.

</p>
</details>

<details><summary><b>Aedes-AI: Neural Network Models of Mosquito Abundance</b>
<a href="https://arxiv.org/abs/2104.10771">arxiv:2104.10771</a>
&#x1F4C8; 1 <br>
<p>Adrienne C. Kinney, Sean Current, Joceline Lega</p></summary>
<p>

**Abstract:** We present artificial neural networks as a feasible replacement for a mechanistic model of mosquito abundance. We develop a feed-forward neural network, a long short-term memory recurrent neural network, and a gated recurrent unit network. We evaluate the networks in their ability to replicate the spatiotemporal features of mosquito populations predicted by the mechanistic model, and discuss how augmenting the training data with time series that emphasize specific dynamical behaviors affects model performance. We conclude with an outlook on how such equation-free models may facilitate vector control or the estimation of disease risk at arbitrary spatial scales.

</p>
</details>

<details><summary><b>Robust Testing and Estimation under Manipulation Attacks</b>
<a href="https://arxiv.org/abs/2104.10740">arxiv:2104.10740</a>
&#x1F4C8; 1 <br>
<p>Jayadev Acharya, Ziteng Sun, Huanyu Zhang</p></summary>
<p>

**Abstract:** We study robust testing and estimation of discrete distributions in the strong contamination model. We consider both the "centralized setting" and the "distributed setting with information constraints" including communication and local privacy (LDP) constraints. Our technique relates the strength of manipulation attacks to the earth-mover distance using Hamming distance as the metric between messages(samples) from the users. In the centralized setting, we provide optimal error bounds for both learning and testing. Our lower bounds under local information constraints build on the recent lower bound methods in distributed inference. In the communication constrained setting, we develop novel algorithms based on random hashing and an $\ell_1/\ell_1$ isometry.

</p>
</details>

<details><summary><b>Multi-Class Micro-CT Image Segmentation Using Sparse Regularized Deep Networks</b>
<a href="https://arxiv.org/abs/2104.10705">arxiv:2104.10705</a>
&#x1F4C8; 1 <br>
<p>Amirsaeed Yazdani, Yung-Chen Sun, Nicholas B. Stephens, Timothy Ryan, Vishal Monga</p></summary>
<p>

**Abstract:** It is common in anthropology and paleontology to address questions about extant and extinct species through the quantification of osteological features observable in micro-computed tomographic (micro-CT) scans. In cases where remains were buried, the grey values present in these scans may be classified as belonging to air, dirt, or bone. While various intensity-based methods have been proposed to segment scans into these classes, it is often the case that intensity values for dirt and bone are nearly indistinguishable. In these instances, scientists resort to laborious manual segmentation, which does not scale well in practice when a large number of scans are to be analyzed. Here we present a new domain-enriched network for three-class image segmentation, which utilizes the domain knowledge of experts familiar with manually segmenting bone and dirt structures. More precisely, our novel structure consists of two components: 1) a representation network trained on special samples based on newly designed custom loss terms, which extracts discriminative bone and dirt features, 2) and a segmentation network that leverages these extracted discriminative features. These two parts are jointly trained in order to optimize the segmentation performance. A comparison of our network to that of the current state-of-the-art U-NETs demonstrates the benefits of our proposal, particularly when the number of labeled training images are limited, which is invariably the case for micro-CT segmentation.

</p>
</details>

<details><summary><b>Scaling of neural-network quantum states for time evolution</b>
<a href="https://arxiv.org/abs/2104.10696">arxiv:2104.10696</a>
&#x1F4C8; 1 <br>
<p>Sheng-Hsuan Lin, Frank Pollmann</p></summary>
<p>

**Abstract:** Simulating quantum many-body dynamics on classical computers is a challenging problem due to the exponential growth of the Hilbert space. Artificial neural networks have recently been introduced as a new tool to approximate quantum-many body states. We benchmark the variational power of the restricted Boltzmann machine quantum states and different shallow and deep neural autoregressive quantum states to simulate global quench dynamics of a non-integrable quantum Ising chain. We find that the number of parameters required to represent the quantum state at a given accuracy increases exponentially in time. The growth rate is only slightly affected by the network architecture over a wide range of different design choices: shallow and deep networks, small and large filter sizes, dilated and normal convolutions, with and without shortcut connections.

</p>
</details>

<details><summary><b>Towards Causal Models for Adversary Distractions</b>
<a href="https://arxiv.org/abs/2104.10575">arxiv:2104.10575</a>
&#x1F4C8; 1 <br>
<p>Ron Alford, Andy Applebaum</p></summary>
<p>

**Abstract:** Automated adversary emulation is becoming an indispensable tool of network security operators in testing and evaluating their cyber defenses. At the same time, it has exposed how quickly adversaries can propagate through the network. While research has greatly progressed on quality decoy generation to fool human adversaries, we may need different strategies to slow computer agents. In this paper, we show that decoy generation can slow an automated agent's decision process, but that the degree to which it is inhibited is greatly dependent on the types of objects used. This points to the need to explicitly evaluate decoy generation and placement strategies against fast moving, automated adversaries.

</p>
</details>

<details><summary><b>Tackling Variabilities in Autonomous Driving</b>
<a href="https://arxiv.org/abs/2104.10415">arxiv:2104.10415</a>
&#x1F4C8; 1 <br>
<p>Yuqiong Qi, Yang Hu, Haibin Wu, Shen Li, Haiyu Mao, Xiaochun Ye, Dongrui Fan, Ninghui Sun</p></summary>
<p>

**Abstract:** The state-of-the-art driving automation system demands extreme computational resources to meet rigorous accuracy and latency requirements. Though emerging driving automation computing platforms are based on ASIC to provide better performance and power guarantee, building such an accelerator-based computing platform for driving automation still present challenges. First, the workloads mix and performance requirements exposed to driving automation system present significant variability. Second, with more cameras/sensors integrated in a future fully autonomous driving vehicle, a heterogeneous multi-accelerator architecture substrate is needed that requires a design space exploration for a new form of parallelism. In this work, we aim to extensively explore the above system design challenges and these challenges motivate us to propose a comprehensive framework that synergistically handles the heterogeneous hardware accelerator design principles, system design criteria, and task scheduling mechanism. Specifically, we propose a novel heterogeneous multi-core AI accelerator (HMAI) to provide the hardware substrate for the driving automation tasks with variability. We also define system design criteria to better utilize hardware resources and achieve increased throughput while satisfying the performance and energy restrictions. Finally, we propose a deep reinforcement learning (RL)-based task scheduling mechanism FlexAI, to resolve task mapping issue. Experimental results show that with FlexAI scheduling, basically 100% tasks in each driving route can be processed by HMAI within their required period to ensure safety, and FlexAI can also maximally reduce the breaking distance up to 96% as compared to typical heuristics and guided random-search-based algorithms.

</p>
</details>

<details><summary><b>Model-aided Deep Reinforcement Learning for Sample-efficient UAV Trajectory Design in IoT Networks</b>
<a href="https://arxiv.org/abs/2104.10403">arxiv:2104.10403</a>
&#x1F4C8; 1 <br>
<p>Omid Esrafilian, Harald Bayerlein, David Gesbert</p></summary>
<p>

**Abstract:** Deep Reinforcement Learning (DRL) is gaining attention as a potential approach to design trajectories for autonomous unmanned aerial vehicles (UAV) used as flying access points in the context of cellular or Internet of Things (IoT) connectivity. DRL solutions offer the advantage of on-the-go learning hence relying on very little prior contextual information. A corresponding drawback however lies in the need for many learning episodes which severely restricts the applicability of such approach in real-world time- and energy-constrained missions. Here, we propose a model-aided deep Q-learning approach that, in contrast to previous work, considerably reduces the need for extensive training data samples, while still achieving the overarching goal of DRL, i.e to guide a battery-limited UAV on an efficient data harvesting trajectory, without prior knowledge of wireless channel characteristics and limited knowledge of wireless node locations. The key idea consists in using a small subset of nodes as anchors (i.e. with known location) and learning a model of the propagation environment while implicitly estimating the positions of regular nodes. Interaction with the model allows us to train a deep Q-network (DQN) to approximate the optimal UAV control policy. We show that in comparison with standard DRL approaches, the proposed model-aided approach requires at least one order of magnitude less training data samples to reach identical data collection performance, hence offering a first step towards making DRL a viable solution to the problem.

</p>
</details>

<details><summary><b>Wireless Sensing With Deep Spectrogram Network and Primitive Based Autoregressive Hybrid Channel Model</b>
<a href="https://arxiv.org/abs/2104.10378">arxiv:2104.10378</a>
&#x1F4C8; 1 <br>
<p>Guoliang Li, Shuai Wang, Jie Li, Rui Wang, Xiaohui Peng, Tony Xiao Han</p></summary>
<p>

**Abstract:** Human motion recognition (HMR) based on wireless sensing is a low-cost technique for scene understanding. Current HMR systems adopt support vector machines (SVMs) and convolutional neural networks (CNNs) to classify radar signals. However, whether a deeper learning model could improve the system performance is currently not known. On the other hand, training a machine learning model requires a large dataset, but data gathering from experiment is cost-expensive and time-consuming. Although wireless channel models can be adopted for dataset generation, current channel models are mostly designed for communication rather than sensing. To address the above problems, this paper proposes a deep spectrogram network (DSN) by leveraging the residual mapping technique to enhance the HMR performance. Furthermore, a primitive based autoregressive hybrid (PBAH) channel model is developed, which facilitates efficient training and testing dataset generation for HMR in a virtual environment. Experimental results demonstrate that the proposed PBAH channel model matches the actual experimental data very well and the proposed DSN achieves significantly smaller recognition error than that of CNN.

</p>
</details>

<details><summary><b>Neuromorphic Algorithm-hardware Codesign for Temporal Pattern Learning</b>
<a href="https://arxiv.org/abs/2104.10712">arxiv:2104.10712</a>
&#x1F4C8; 0 <br>
<p>Haowen Fang, Brady Taylor, Ziru Li, Zaidao Mei, Hai Li, Qinru Qiu</p></summary>
<p>

**Abstract:** Neuromorphic computing and spiking neural networks (SNN) mimic the behavior of biological systems and have drawn interest for their potential to perform cognitive tasks with high energy efficiency. However, some factors such as temporal dynamics and spike timings prove critical for information processing but are often ignored by existing works, limiting the performance and applications of neuromorphic computing. On one hand, due to the lack of effective SNN training algorithms, it is difficult to utilize the temporal neural dynamics. Many existing algorithms still treat neuron activation statistically. On the other hand, utilizing temporal neural dynamics also poses challenges to hardware design. Synapses exhibit temporal dynamics, serving as memory units that hold historical information, but are often simplified as a connection with weight. Most current models integrate synaptic activations in some storage medium to represent membrane potential and institute a hard reset of membrane potential after the neuron emits a spike. This is done for its simplicity in hardware, requiring only a "clear" signal to wipe the storage medium, but destroys temporal information stored in the neuron.
  In this work, we derive an efficient training algorithm for Leaky Integrate and Fire neurons, which is capable of training a SNN to learn complex spatial temporal patterns. We achieved competitive accuracy on two complex datasets. We also demonstrate the advantage of our model by a novel temporal pattern association task. Codesigned with this algorithm, we have developed a CMOS circuit implementation for a memristor-based network of neuron and synapses which retains critical neural dynamics with reduced complexity. This circuit implementation of the neuron model is simulated to demonstrate its ability to react to temporal spiking patterns with an adaptive threshold.

</p>
</details>

<details><summary><b>Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations</b>
<a href="https://arxiv.org/abs/2104.10586">arxiv:2104.10586</a>
&#x1F4C8; 0 <br>
<p>Kaidi Xu, Chenan Wang, Hao Cheng, Bhavya Kailkhura, Xue Lin, Ryan Goldhahn</p></summary>
<p>

**Abstract:** To tackle the susceptibility of deep neural networks to examples, the adversarial training has been proposed which provides a notion of robust through an inner maximization problem presenting the first-order embedded within the outer minimization of the training loss. To generalize the adversarial robustness over different perturbation types, the adversarial training method has been augmented with the improved inner maximization presenting a union of multiple perturbations e.g., various $\ell_p$ norm-bounded perturbations.

</p>
</details>

<details><summary><b>Sparse-shot Learning with Exclusive Cross-Entropy for Extremely Many Localisations</b>
<a href="https://arxiv.org/abs/2104.10425">arxiv:2104.10425</a>
&#x1F4C8; 0 <br>
<p>Andreas Panteli, Jonas Teuwen, Hugo Horlings, Efstratios Gavves</p></summary>
<p>

**Abstract:** Object localisation, in the context of regular images, often depicts objects like people or cars. In these images, there is typically a relatively small number of objects per class, which usually is manageable to annotate. However, outside the setting of regular images, we are often confronted with a different situation. In computational pathology, digitised tissue sections are extremely large images, whose dimensions quickly exceed 250'000x250'000 pixels, where relevant objects, such as tumour cells or lymphocytes can quickly number in the millions. Annotating them all is practically impossible and annotating sparsely a few, out of many more, is the only possibility. Unfortunately, learning from sparse annotations, or sparse-shot learning, clashes with standard supervised learning because what is not annotated is treated as a negative. However, assigning negative labels to what are true positives leads to confusion in the gradients and biased learning. To this end, we present exclusive cross-entropy, which slows down the biased learning by examining the second-order loss derivatives in order to drop the loss terms corresponding to likely biased terms. Experiments on nine datasets and two different localisation tasks, detection with YOLLO and segmentation with Unet, show that we obtain considerable improvements compared to cross-entropy or focal loss, while often reaching the best possible performance for the model with only 10-40% of annotations.

</p>
</details>


[Next Page]({{ '/2021/04/20/2021.04.20.html' | relative_url }})
