Prev: [2022.03.02]({{ '/2022/03/02/2022.03.02.html' | relative_url }})  Next: [2022.03.04]({{ '/2022/03/04/2022.03.04.html' | relative_url }})
{% raw %}
## Summary for 2022-03-03, created on 2022-03-13


<details><summary><b>Understanding Failure Modes of Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2203.01881">arxiv:2203.01881</a>
&#x1F4C8; 540 <br>
<p>Neha Mukund Kalibhat, Kanika Narang, Liang Tan, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</p></summary>
<p>

**Abstract:** Self-supervised learning methods have shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure models and interpreting the learned representations of these models. In this paper, we tackle these issues and study the representation space of self-supervised models by understanding the underlying reasons for misclassifications in a downstream task. Over several state-of-the-art self-supervised models including SimCLR, SwaV, MoCo V2 and BYOL, we observe that representations of correctly classified samples have few discriminative features with highly deviated values compared to other features. This is in a clear contrast with representations of misclassified samples. We also observe that noisy features in the representation space often correspond to spurious attributes in images making the models less interpretable. Building on these observations, we propose a sample-wise Self-Supervised Representation Quality Score (or, Q-Score) that, without access to any label information, is able to predict if a given sample is likely to be misclassified in the downstream task, achieving an AUPRC of up to 0.90. Q-Score can also be used as a regularization to remedy low-quality representations leading to 3.26% relative improvement in accuracy of SimCLR on ImageNet-100. Moreover, we show that Q-Score regularization increases representation sparsity, thus reducing noise and improving interpretability through gradient heatmaps.

</p>
</details>

<details><summary><b>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</b>
<a href="https://arxiv.org/abs/2203.01913">arxiv:2203.01913</a>
&#x1F4C8; 107 <br>
<p>Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, Phillip Isola</p></summary>
<p>

**Abstract:** Thin, reflective objects such as forks and whisks are common in our daily lives, but they are particularly challenging for robot perception because it is hard to reconstruct them using commodity RGB-D cameras or multi-view stereo techniques. While traditional pipelines struggle with objects like these, Neural Radiance Fields (NeRFs) have recently been shown to be remarkably effective for performing view synthesis on objects with thin structures or reflective materials. In this paper we explore the use of NeRF as a new source of supervision for robust robot vision systems. In particular, we demonstrate that a NeRF representation of a scene can be used to train dense object descriptors. We use an optimized NeRF to extract dense correspondences between multiple views of an object, and then use these correspondences as training data for learning a view-invariant representation of the object. NeRF's usage of a density field allows us to reformulate the correspondence problem with a novel distribution-of-depths formulation, as opposed to the conventional approach of using a depth map. Dense correspondence models supervised with our method significantly outperform off-the-shelf learned descriptors by 106% (PCK@3px metric, more than doubling performance) and outperform our baseline supervised with multi-view stereo by 29%. Furthermore, we demonstrate the learned dense descriptors enable robots to perform accurate 6-degree of freedom (6-DoF) pick and place of thin and reflective objects.

</p>
</details>

<details><summary><b>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</b>
<a href="https://arxiv.org/abs/2203.02053">arxiv:2203.02053</a>
&#x1F4C8; 92 <br>
<p>Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, James Zou</p></summary>
<p>

**Abstract:** We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization, contrastive learning keeps the different modalities separate by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model's downstream zero-shot classification performance and fairness. Our code and data are available at https://modalitygap.readthedocs.io/

</p>
</details>

<details><summary><b>Random Quantum Neural Networks (RQNN) for Noisy Image Recognition</b>
<a href="https://arxiv.org/abs/2203.01764">arxiv:2203.01764</a>
&#x1F4C8; 54 <br>
<p>Debanjan Konar, Erol Gelenbe, Soham Bhandary, Aditya Das Sarma, Attila Cangi</p></summary>
<p>

**Abstract:** Classical Random Neural Networks (RNNs) have demonstrated effective applications in decision making, signal processing, and image recognition tasks. However, their implementation has been limited to deterministic digital systems that output probability distributions in lieu of stochastic behaviors of random spiking signals. We introduce the novel class of supervised Random Quantum Neural Networks (RQNNs) with a robust training strategy to better exploit the random nature of the spiking RNN. The proposed RQNN employs hybrid classical-quantum algorithms with superposition state and amplitude encoding features, inspired by quantum information theory and the brain's spatial-temporal stochastic spiking property of neuron information encoding. We have extensively validated our proposed RQNN model, relying on hybrid classical-quantum algorithms via the PennyLane Quantum simulator with a limited number of \emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST datasets demonstrate that the proposed RQNN model achieves an average classification accuracy of $94.9\%$. Additionally, the experimental findings illustrate the proposed RQNN's effectiveness and resilience in noisy settings, with enhanced image classification accuracy when compared to the classical counterparts (RNNs), classical Spiking Neural Networks (SNNs), and the classical convolutional neural network (AlexNet). Furthermore, the RQNN can deal with noise, which is useful for various applications, including computer vision in NISQ devices. The PyTorch code (https://github.com/darthsimpus/RQN) is made available on GitHub to reproduce the results reported in this manuscript.

</p>
</details>

<details><summary><b>Autoregressive Image Generation using Residual Quantization</b>
<a href="https://arxiv.org/abs/2203.01941">arxiv:2203.01941</a>
&#x1F4C8; 46 <br>
<p>Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han</p></summary>
<p>

**Abstract:** For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256$\times$256 image as 8$\times$8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images.

</p>
</details>

<details><summary><b>Graph Neural Networks for Multimodal Single-Cell Data Integration</b>
<a href="https://arxiv.org/abs/2203.01884">arxiv:2203.01884</a>
&#x1F4C8; 45 <br>
<p>Hongzhi Wen, Jiayuan Ding, Wei Jin, Yuying Xie, Jiliang Tang</p></summary>
<p>

**Abstract:** Recent advances in multimodal single-cell technologies have enabled simultaneous acquisitions of multiple omics data from the same cell, providing deeper insights into cellular states and dynamics. However, it is challenging to learn the joint representations from the multimodal data, model the relationship between modalities, and, more importantly, incorporate the vast amount of single-modality datasets into the downstream analyses. To address these challenges and correspondingly facilitate multimodal single-cell data analyses, three key tasks have been introduced: $\textit{modality prediction}$, $\textit{modality matching}$ and $\textit{joint embedding}$. In this work, we present a general Graph Neural Network framework $\textit{scMoGNN}$ to tackle these three tasks and show that $\textit{scMoGNN}$ demonstrates superior results in all three tasks compared with the state-of-the-art and conventional approaches. Our method is an official winner in the overall ranking of $\textit{modality prediction}$ from $\href{https://openproblems.bio/neurips_2021/}{\textit{NeurIPS 2021 Competition}}$.

</p>
</details>

<details><summary><b>Quantity over Quality: Training an AV Motion Planner with Large Scale Commodity Vision Data</b>
<a href="https://arxiv.org/abs/2203.01681">arxiv:2203.01681</a>
&#x1F4C8; 43 <br>
<p>Lukas Platinsky, Tayyab Naseer, Hui Chen, Ben Haines, Haoyue Zhu, Hugo Grimmett, Luca Del Pero</p></summary>
<p>

**Abstract:** With the Autonomous Vehicle (AV) industry shifting towards Autonomy 2.0, the performance of self-driving systems starts to rely heavily on large quantities of expert driving demonstrations. However, collecting this demonstration data typically involves expensive HD sensor suites (LiDAR + RADAR + cameras), which quickly becomes financially infeasible at the scales required. This motivates the use of commodity vision sensors for data collection, which are an order of magnitude cheaper than the HD sensor suites, but offer lower fidelity. If it were possible to leverage these for training an AV motion planner, observing the `long tail' of driving events would become a financially viable strategy. As our main contribution we show it is possible to train a high-performance motion planner using commodity vision data which outperforms planners trained on HD-sensor data for a fraction of the cost. We do this by comparing the autonomy system performance when training on these two different sensor configurations, and showing that we can compensate for the lower sensor fidelity by means of increased quantity: a planner trained on 100h of commodity vision data outperforms one with 25h of expensive HD data. We also share the technical challenges we had to tackle to make this work. To the best of our knowledge, we are the first to demonstrate that this is possible using real-world data.

</p>
</details>

<details><summary><b>Evolving symbolic density functionals</b>
<a href="https://arxiv.org/abs/2203.02540">arxiv:2203.02540</a>
&#x1F4C8; 23 <br>
<p>He Ma, Arunachalam Narayanaswamy, Patrick Riley, Li Li</p></summary>
<p>

**Abstract:** Systematic development of accurate density functionals has been a decades-long challenge for scientists. Despite the emerging application of machine learning (ML) in approximating functionals, the resulting ML functionals usually contain more than tens of thousands parameters, which makes a huge gap in the formulation with the conventional human-designed symbolic functionals. We propose a new framework, Symbolic Functional Evolutionary Search (SyFES), that automatically constructs accurate functionals in the symbolic form, which is more explainable to humans, cheaper to evaluate, and easier to integrate to existing density functional theory codes than other ML functionals. We first show that without prior knowledge, SyFES reconstructed a known functional from scratch. We then demonstrate that evolving from an existing functional $ω$B97M-V, SyFES found a new functional, GAS22 (Google Accelerated Science 22), that performs better on main-group chemistry. Our framework opens a new direction in leveraging computing power for the systematic development of symbolic density functionals.

</p>
</details>

<details><summary><b>Playable Environments: Video Manipulation in Space and Time</b>
<a href="https://arxiv.org/abs/2203.01914">arxiv:2203.01914</a>
&#x1F4C8; 22 <br>
<p>Willi Menapace, Aliaksandr Siarohin, Christian Theobalt, Vladislav Golyanik, Sergey Tulyakov, Stéphane Lathuilière, Elisa Ricci</p></summary>
<p>

**Abstract:** We present Playable Environments - a new representation for interactive video generation and manipulation in space and time. With a single image at inference time, our novel framework allows the user to move objects in 3D while generating a video by providing a sequence of desired actions. The actions are learnt in an unsupervised manner. The camera can be controlled to get the desired viewpoint. Our method builds an environment state for each frame, which can be manipulated by our proposed action module and decoded back to the image space with volumetric rendering. To support diverse appearances of objects, we extend neural radiance fields with style-based modulation. Our method trains on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations. To set a challenging benchmark, we introduce two large scale video datasets with significant camera movements. As evidenced by our experiments, playable environments enable several creative applications not attainable by prior video synthesis works, including playable 3D video generation, stylization and manipulation. Further details, code and examples are available at https://willi-menapace.github.io/playable-environments-website

</p>
</details>

<details><summary><b>Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction</b>
<a href="https://arxiv.org/abs/2203.01703">arxiv:2203.01703</a>
&#x1F4C8; 20 <br>
<p>Dominik J. E. Waibel, Scott Atwell, Matthias Meier, Carsten Marr, Bastian Rieck</p></summary>
<p>

**Abstract:** Reconstructing 3D objects from 2D images is both challenging for our brains and machine learning algorithms. To support this spatial reasoning task, contextual information about the overall shape of an object is critical. However, such information is not captured by established loss terms (e.g. Dice loss). We propose to complement geometrical shape information by including multi-scale topological features, such as connected components, cycles, and voids, in the reconstruction loss. Our method calculates topological features from 3D volumetric data based on cubical complexes and uses an optimal transport distance to guide the reconstruction process. This topology-aware loss is fully differentiable, computationally efficient, and can be added to any neural network. We demonstrate the utility of our loss by incorporating it into SHAPR, a model for predicting the 3D cell shape of individual cells based on 2D microscopy images. Using a hybrid loss that leverages both geometrical and topological information of single objects to assess their shape, we find that topological information substantially improves the quality of reconstructions, thus highlighting its ability to extract more relevant features from image datasets.

</p>
</details>

<details><summary><b>Vision-Language Intelligence: Tasks, Representation Learning, and Large Models</b>
<a href="https://arxiv.org/abs/2203.01922">arxiv:2203.01922</a>
&#x1F4C8; 17 <br>
<p>Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, PengChuan Zhang, Lei Zhang</p></summary>
<p>

**Abstract:** This paper presents a comprehensive survey of vision-language (VL) intelligence from the perspective of time. This survey is inspired by the remarkable progress in both computer vision and natural language processing, and recent trends shifting from single modality processing to multiple modality comprehension. We summarize the development in this field into three time periods, namely task-specific methods, vision-language pre-training (VLP) methods, and larger models empowered by large-scale weakly-labeled data. We first take some common VL tasks as examples to introduce the development of task-specific methods. Then we focus on VLP methods and comprehensively review key components of the model structures and training methods. After that, we show how recent work utilizes large-scale raw image-text data to learn language-aligned visual representations that generalize better on zero or few shot learning tasks. Finally, we discuss some potential future trends towards modality cooperation, unified representation, and knowledge incorporation. We believe that this review will be of help for researchers and practitioners of AI and ML, especially those interested in computer vision and natural language processing.

</p>
</details>

<details><summary><b>Audio-Visual Object Classification for Human-Robot Collaboration</b>
<a href="https://arxiv.org/abs/2203.01977">arxiv:2203.01977</a>
&#x1F4C8; 14 <br>
<p>A. Xompero, Y. L. Pang, T. Patten, A. Prabhakar, B. Calli, A. Cavallaro</p></summary>
<p>

**Abstract:** Human-robot collaboration requires the contactless estimation of the physical properties of containers manipulated by a person, for example while pouring content in a cup or moving a food box. Acoustic and visual signals can be used to estimate the physical properties of such objects, which may vary substantially in shape, material and size, and also be occluded by the hands of the person. To facilitate comparisons and stimulate progress in solving this problem, we present the CORSMAL challenge and a dataset to assess the performance of the algorithms through a set of well-defined performance scores. The tasks of the challenge are the estimation of the mass, capacity, and dimensions of the object (container), and the classification of the type and amount of its content. A novel feature of the challenge is our real-to-simulation framework for visualising and assessing the impact of estimation errors in human-to-robot handovers.

</p>
</details>

<details><summary><b>Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining</b>
<a href="https://arxiv.org/abs/2203.01969">arxiv:2203.01969</a>
&#x1F4C8; 9 <br>
<p>Benjamin Billot, Magdamo Colin, Sean E. Arnold, Sudeshna Das, Juan. E. Iglesias</p></summary>
<p>

**Abstract:** Retrospective analysis of brain MRI scans acquired in the clinic has the potential to enable neuroimaging studies with sample sizes much larger than those found in research datasets. However, analysing such clinical images "in the wild" is challenging, since subjects are scanned with highly variable protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent advances in convolutional neural networks (CNNs) and domain randomisation for image segmentation, best represented by the publicly available method SynthSeg, may enable morphometry of clinical MRI at scale. In this work, we first evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000 scans acquired at Massachusetts General Hospital. We show that SynthSeg is generally robust, but frequently falters on scans with low signal-to-noise ratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that greatly mitigates these problems using a hierarchy of conditional segmentation and denoising CNNs. We show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and state-of-the-art segmentation denoising methods. Finally, we apply our approach to a proof-of-concept volumetric study of ageing, where it closely replicates atrophy patterns observed in research studies conducted on high-quality, 1mm, T1-weighted scans. The code and trained model are publicly available at https://github.com/BBillot/SynthSeg.

</p>
</details>

<details><summary><b>Weakly Supervised Object Localization as Domain Adaption</b>
<a href="https://arxiv.org/abs/2203.01714">arxiv:2203.01714</a>
&#x1F4C8; 9 <br>
<p>Lei Zhu, Qi She, Qian Chen, Yunfei You, Boyu Wang, Yanye Lu</p></summary>
<p>

**Abstract:** Weakly supervised object localization (WSOL) focuses on localizing objects only with the supervision of image-level classification masks. Most previous WSOL methods follow the classification activation map (CAM) that localizes objects based on the classification structure with the multi-instance learning (MIL) mechanism. However, the MIL mechanism makes CAM only activate discriminative object parts rather than the whole object, weakening its performance for localizing objects. To avoid this problem, this work provides a novel perspective that models WSOL as a domain adaption (DA) task, where the score estimator trained on the source/image domain is tested on the target/pixel domain to locate objects. Under this perspective, a DA-WSOL pipeline is designed to better engage DA approaches into WSOL to enhance localization performance. It utilizes a proposed target sampling strategy to select different types of target samples. Based on these types of target samples, domain adaption localization (DAL) loss is elaborated. It aligns the feature distribution between the two domains by DA and makes the estimator perceive target domain cues by Universum regularization. Experiments show that our pipeline outperforms SOTA methods on multi benchmarks. Code are released at \url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.

</p>
</details>

<details><summary><b>Constrained unsupervised anomaly segmentation</b>
<a href="https://arxiv.org/abs/2203.01671">arxiv:2203.01671</a>
&#x1F4C8; 9 <br>
<p>Julio Silva-Rodríguez, Valery Naranjo, Jose Dolz</p></summary>
<p>

**Abstract:** Current unsupervised anomaly localization approaches rely on generative models to learn the distribution of normal images, which is later used to identify potential anomalous regions derived from errors on the reconstructed images. However, a main limitation of nearly all prior literature is the need of employing anomalous images to set a class-specific threshold to locate the anomalies. This limits their usability in realistic scenarios, where only normal data is typically accessible. Despite this major drawback, only a handful of works have addressed this limitation, by integrating supervision on attention maps during training. In this work, we propose a novel formulation that does not require accessing images with abnormalities to define the threshold. Furthermore, and in contrast to very recent work, the proposed constraint is formulated in a more principled manner, leveraging well-known knowledge in constrained optimization. In particular, the equality constraint on the attention maps in prior work is replaced by an inequality constraint, which allows more flexibility. In addition, to address the limitations of penalty-based functions we employ an extension of the popular log-barrier methods to handle the constraint. Last, we propose an alternative regularization term that maximizes the Shannon entropy of the attention maps, reducing the amount of hyperparameters of the proposed model. Comprehensive experiments on two publicly available datasets on brain lesion segmentation demonstrate that the proposed approach substantially outperforms relevant literature, establishing new state-of-the-art results for unsupervised lesion segmentation, and without the need to access anomalous images.

</p>
</details>

<details><summary><b>DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations</b>
<a href="https://arxiv.org/abs/2203.02013">arxiv:2203.02013</a>
&#x1F4C8; 8 <br>
<p>Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, Louis-Philippe Morency</p></summary>
<p>

**Abstract:** The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at https://github.com/lvyiwei1/DIME.

</p>
</details>

<details><summary><b>NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification</b>
<a href="https://arxiv.org/abs/2203.01921">arxiv:2203.01921</a>
&#x1F4C8; 8 <br>
<p>Shreyas Fadnavis, Jens Sjölund, Anders Eklund, Eleftherios Garyfallidis</p></summary>
<p>

**Abstract:** Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue micro-architecture, which can, in turn, be used to reconstruct tissue microstructure and white matter pathways. The accuracy of such tasks is hampered by the low signal-to-noise ratio in dMRI. Today, the noise is characterized mainly by visual inspection of residual maps and estimated standard deviation. However, it is hard to estimate the impact of noise on downstream tasks based only on such qualitative assessments. To address this issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for quantitative image quality analysis in the absence of a ground truth reference image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the uncertainty of microstructural measures. Specifically, NUQ uses the maximum mean discrepancy metric to compute a pooled quality score by comparing samples drawn from the posterior distribution of the microstructure measures. We show that NUQ allows a fine-grained analysis of noise, capturing details that are visually imperceptible. We perform qualitative and quantitative comparisons on real datasets, showing that NUQ generates consistent scores across different denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics and controls, we quantify the substantial impact of denoising on group differences.

</p>
</details>

<details><summary><b>A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions</b>
<a href="https://arxiv.org/abs/2203.03724">arxiv:2203.03724</a>
&#x1F4C8; 7 <br>
<p>Francois St-Hilaire, Dung Do Vu, Antoine Frau, Nathan Burns, Farid Faraji, Joseph Potochny, Stephane Robert, Arnaud Roussel, Selene Zheng, Taylor Glazier, Junfel Vincent Romano, Robert Belfer, Muhammad Shayan, Ariella Smofsky, Tommy Delarosbil, Seulmin Ahn, Simon Eden-Walker, Kritika Sony, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan, Adela Matajova, Victor Chen, Hossein Sahraei, Robert Larson</p></summary>
<p>

**Abstract:** Despite artificial intelligence (AI) having transformed major aspects of our society, less than a fraction of its potential has been explored, let alone deployed, for education. AI-powered learning can provide millions of learners with a highly personalized, active and practical learning experience, which is key to successful learning. This is especially relevant in the context of online learning platforms. In this paper, we present the results of a comparative head-to-head study on learning outcomes for two popular online learning platforms (n=199 participants): A MOOC platform following a traditional model delivering content using lecture videos and multiple-choice quizzes, and the Korbit learning platform providing a highly personalized, active and practical learning experience. We observe a huge and statistically significant increase in the learning outcomes, with students on the Korbit platform providing full feedback resulting in higher course completion rates and achieving learning gains 2 to 2.5 times higher than both students on the MOOC platform and students in a control group who don't receive personalized feedback on the Korbit platform. The results demonstrate the tremendous impact that can be achieved with a personalized, active learning AI-powered system. Making this technology and learning experience available to millions of learners around the world will represent a significant leap forward towards the democratization of education.

</p>
</details>

<details><summary><b>An Efficient Subpopulation-based Membership Inference Attack</b>
<a href="https://arxiv.org/abs/2203.02080">arxiv:2203.02080</a>
&#x1F4C8; 7 <br>
<p>Shahbaz Rezaei, Xin Liu</p></summary>
<p>

**Abstract:** Membership inference attacks allow a malicious entity to predict whether a sample is used during training of a victim model or not. State-of-the-art membership inference attacks have shown to achieve good accuracy which poses a great privacy threat. However, majority of SOTA attacks require training dozens to hundreds of shadow models to accurately infer membership. This huge computation cost raises questions about practicality of these attacks on deep models. In this paper, we introduce a fundamentally different MI attack approach which obviates the need to train hundreds of shadow models. Simply put, we compare the victim model output on the target sample versus the samples from the same subpopulation (i.e., semantically similar samples), instead of comparing it with the output of hundreds of shadow models. The intuition is that the model response should not be significantly different between the target sample and its subpopulation if it was not a training sample. In cases where subpopulation samples are not available to the attacker, we show that training only a single generative model can fulfill the requirement. Hence, we achieve the state-of-the-art membership inference accuracy while significantly reducing the training computation cost.

</p>
</details>

<details><summary><b>Label-Free Explainability for Unsupervised Models</b>
<a href="https://arxiv.org/abs/2203.01928">arxiv:2203.01928</a>
&#x1F4C8; 7 <br>
<p>Jonathan Crabbé, Mihaela van der Schaar</p></summary>
<p>

**Abstract:** Unsupervised black-box models are challenging to interpret. Indeed, most existing explainability methods require labels to select which component(s) of the black-box's output to interpret. In the absence of labels, black-box outputs often are representation vectors whose components do not correspond to any meaningful quantity. Hence, choosing which component(s) to interpret in a label-free unsupervised/self-supervised setting is an important, yet unsolved problem. To bridge this gap in the literature, we introduce two crucial extensions of post-hoc explanation techniques: (1) label-free feature importance and (2) label-free example importance that respectively highlight influential features and training examples for a black-box to construct representations at inference time. We demonstrate that our extensions can be successfully implemented as simple wrappers around many existing feature and example importance methods. We illustrate the utility of our label-free explainability paradigm through a qualitative and quantitative comparison of representation spaces learned by various autoencoders trained on distinct unsupervised tasks.

</p>
</details>

<details><summary><b>Socially Aware Robot Crowd Navigation with Interaction Graphs and Human Trajectory Prediction</b>
<a href="https://arxiv.org/abs/2203.01821">arxiv:2203.01821</a>
&#x1F4C8; 7 <br>
<p>Shuijing Liu, Peixin Chang, Zhe Huang, Neeloy Chakraborty, Weihang Liang, Junyi Geng, Katherine Driggs-Campbell</p></summary>
<p>

**Abstract:** We study the problem of safe and socially aware robot navigation in dense and interactive human crowds. Previous works use simplified methods to model the personal spaces of pedestrians and ignore the social compliance of the robot behaviors. In this paper, we provide a more accurate representation of personal zones of walking pedestrians with their future trajectories. The predicted personal zones are incorporated into a reinforcement learning framework to prevent the robot from intruding into the personal zones. To learn socially aware navigation policies, we propose a novel recurrent graph neural network with attention mechanisms to capture the interactions among agents through space and time. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in the simulator to a real-world TurtleBot 2i.

</p>
</details>

<details><summary><b>Intensity Image-based LiDAR Fiducial Marker System</b>
<a href="https://arxiv.org/abs/2203.01816">arxiv:2203.01816</a>
&#x1F4C8; 7 <br>
<p>Yibo Liu, Hunter Schofield, Jinjun Shan</p></summary>
<p>

**Abstract:** The fiducial marker system for LiDAR is crucial for the robotic application but it is still rare to date. In this paper, an Intensity Image-based LiDAR Fiducial Marker (IILFM) system is developed. This system only requires an unstructured point cloud with intensity as the input and it has no restriction on marker placement and shape. A marker detection method that locates the predefined 3D fiducials in the point cloud through the intensity image is introduced. Then, an approach that utilizes the detected 3D fiducials to estimate the LiDAR 6-DOF pose that describes the transmission from the world coordinate system to the LiDAR coordinate system is developed. Moreover, all these processes run in real-time (approx 40 Hz on Livox Mid-40 and approx 143 Hz on VLP-16). Qualitative and quantitative experiments are conducted to demonstrate that the proposed system has similar convenience and accuracy as the conventional visual fiducial marker system. The codes and results are available at: https://github.com/York-SDCNLab/IILFM.

</p>
</details>

<details><summary><b>NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields</b>
<a href="https://arxiv.org/abs/2203.01762">arxiv:2203.01762</a>
&#x1F4C8; 7 <br>
<p>Shanyan Guan, Huayu Deng, Yunbo Wang, Xiaokang Yang</p></summary>
<p>

**Abstract:** Deep learning has shown great potential for modeling the physical dynamics of complex particle systems such as fluids (in Lagrangian descriptions). Existing approaches, however, require the supervision of consecutive particle properties, including positions and velocities. In this paper, we consider a partially observable scenario known as fluid dynamics grounding, that is, inferring the state transitions and interactions within the fluid particle systems from sequential visual observations of the fluid surface. We propose a differentiable two-stage network named NeuroFluid. Our approach consists of (i) a particle-driven neural renderer, which involves fluid physical properties into the volume rendering function, and (ii) a particle transition model optimized to reduce the differences between the rendered and the observed images. NeuroFluid provides the first solution to unsupervised learning of particle-based fluid dynamics by training these two models jointly. It is shown to reasonably estimate the underlying physics of fluids with different initial shapes, viscosity, and densities. It is a potential alternative approach to understanding complex fluid mechanics, such as turbulence, that are difficult to model using traditional methods of mathematical physics.

</p>
</details>

<details><summary><b>Automated clustering of COVID-19 anti-vaccine discourse on Twitter</b>
<a href="https://arxiv.org/abs/2203.01549">arxiv:2203.01549</a>
&#x1F4C8; 7 <br>
<p>Ignacio Ojea Quintana, Marc Cheong, Mark Alfano, Ritsaart Reimann, Colin Klein</p></summary>
<p>

**Abstract:** Attitudes about vaccination have become more polarized; it is common to see vaccine disinformation and fringe conspiracy theories online. An observational study of Twitter vaccine discourse is found in Ojea Quintana et al. (2021): the authors analyzed approximately six months' of Twitter discourse -- 1.3 million original tweets and 18 million retweets between December 2019 and June 2020, ranging from before to after the establishment of Covid-19 as a pandemic. This work expands upon Ojea Quintana et al. (2021) with two main contributions from data science. First, based on the authors' initial network clustering and qualitative analysis techniques, we are able to clearly demarcate and visualize the language patterns used in discourse by Antivaxxers (anti-vaccination campaigners and vaccine deniers) versus other clusters (collectively, Others). Second, using the characteristics of Antivaxxers' tweets, we develop text classifiers to determine the likelihood a given user is employing anti-vaccination language, ultimately contributing to an early-warning mechanism to improve the health of our epistemic environment and bolster (and not hinder) public health initiatives.

</p>
</details>

<details><summary><b>Anomaly Detection-Inspired Few-Shot Medical Image Segmentation Through Self-Supervision With Supervoxels</b>
<a href="https://arxiv.org/abs/2203.02048">arxiv:2203.02048</a>
&#x1F4C8; 6 <br>
<p>Stine Hansen, Srishti Gautam, Robert Jenssen, Michael Kampffmeyer</p></summary>
<p>

**Abstract:** Recent work has shown that label-efficient few-shot learning through self-supervision can achieve promising medical image segmentation results. However, few-shot segmentation models typically rely on prototype representations of the semantic classes, resulting in a loss of local information that can degrade performance. This is particularly problematic for the typically large and highly heterogeneous background class in medical image segmentation problems. Previous works have attempted to address this issue by learning additional prototypes for each class, but since the prototypes are based on a limited number of slices, we argue that this ad-hoc solution is insufficient to capture the background properties. Motivated by this, and the observation that the foreground class (e.g., one organ) is relatively homogeneous, we propose a novel anomaly detection-inspired approach to few-shot medical image segmentation in which we refrain from modeling the background explicitly. Instead, we rely solely on a single foreground prototype to compute anomaly scores for all query pixels. The segmentation is then performed by thresholding these anomaly scores using a learned threshold. Assisted by a novel self-supervision task that exploits the 3D structure of medical images through supervoxels, our proposed anomaly detection-inspired few-shot medical image segmentation model outperforms previous state-of-the-art approaches on two representative MRI datasets for the tasks of abdominal organ segmentation and cardiac segmentation.

</p>
</details>

<details><summary><b>Interventions, Where and How? Experimental Design for Causal Models at Scale</b>
<a href="https://arxiv.org/abs/2203.02016">arxiv:2203.02016</a>
&#x1F4C8; 6 <br>
<p>Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, Yarin Gal, Stefan Bauer</p></summary>
<p>

**Abstract:** Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability which introduces uncertainties in estimating the underlying structural causal model (SCM). Incorporating these uncertainties and selecting optimal experiments (interventions) to perform can help to identify the true SCM faster. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. In this paper, we incorporate recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, which allows for active causal discovery of nonlinear, large SCMs, while selecting both the target and the value to intervene with. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the in-silico single-cell gene regulatory network dataset, DREAM.

</p>
</details>

<details><summary><b>CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation</b>
<a href="https://arxiv.org/abs/2203.01929">arxiv:2203.01929</a>
&#x1F4C8; 6 <br>
<p>Muhammad Zubair Irshad, Thomas Kollar, Michael Laskey, Kevin Stone, Zsolt Kira</p></summary>
<p>

**Abstract:** This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instance-level pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple one-stage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this per-pixel representation, our approach can reconstruct in real-time (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute improvement in mAP for 6D pose for novel real-world object instances.

</p>
</details>

<details><summary><b>Local Constraint-Based Causal Discovery under Selection Bias</b>
<a href="https://arxiv.org/abs/2203.01848">arxiv:2203.01848</a>
&#x1F4C8; 6 <br>
<p>Philip Versteeg, Cheng Zhang, Joris M. Mooij</p></summary>
<p>

**Abstract:** We consider the problem of discovering causal relations from independence constraints selection bias in addition to confounding is present. While the seminal FCI algorithm is sound and complete in this setup, no criterion for the causal interpretation of its output under selection bias is presently known. We focus instead on local patterns of independence relations, where we find no sound method for only three variable that can include background knowledge. Y-Structure patterns are shown to be sound in predicting causal relations from data under selection bias, where cycles may be present. We introduce a finite-sample scoring rule for Y-Structures that is shown to successfully predict causal relations in simulation experiments that include selection mechanisms. On real-world microarray data, we show that a Y-Structure variant performs well across different datasets, potentially circumventing spurious correlations due to selection bias.

</p>
</details>

<details><summary><b>Uniform Approximations for Randomized Hadamard Transforms with Applications</b>
<a href="https://arxiv.org/abs/2203.01599">arxiv:2203.01599</a>
&#x1F4C8; 6 <br>
<p>Yeshwanth Cherapanamjeri, Jelani Nelson</p></summary>
<p>

**Abstract:** Randomized Hadamard Transforms (RHTs) have emerged as a computationally efficient alternative to the use of dense unstructured random matrices across a range of domains in computer science and machine learning. For several applications such as dimensionality reduction and compressed sensing, the theoretical guarantees for methods based on RHTs are comparable to approaches using dense random matrices with i.i.d.\ entries. However, several such applications are in the low-dimensional regime where the number of rows sampled from the matrix is rather small. Prior arguments are not applicable to the high-dimensional regime often found in machine learning applications like kernel approximation. Given an ensemble of RHTs with Gaussian diagonals, $\{M^i\}_{i = 1}^m$, and any $1$-Lipschitz function, $f: \mathbb{R} \to \mathbb{R}$, we prove that the average of $f$ over the entries of $\{M^i v\}_{i = 1}^m$ converges to its expectation uniformly over $\| v \| \leq 1$ at a rate comparable to that obtained from using truly Gaussian matrices. We use our inequality to then derive improved guarantees for two applications in the high-dimensional regime: 1) kernel approximation and 2) distance estimation. For kernel approximation, we prove the first \emph{uniform} approximation guarantees for random features constructed through RHTs lending theoretical justification to their empirical success while for distance estimation, our convergence result implies data structures with improved runtime guarantees over previous work by the authors. We believe our general inequality is likely to find use in other applications.

</p>
</details>

<details><summary><b>Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking</b>
<a href="https://arxiv.org/abs/2203.01552">arxiv:2203.01552</a>
&#x1F4C8; 6 <br>
<p>Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, Juneyoung Park</p></summary>
<p>

**Abstract:** Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem. To elaborate, we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states. Then, the dialogue states can be recovered by inversely applying the summary generation rules. We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings. Our method also exhibits vast speedup during both training and inference as it can generate all states at once. Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.

</p>
</details>

<details><summary><b>Private High-Dimensional Hypothesis Testing</b>
<a href="https://arxiv.org/abs/2203.01537">arxiv:2203.01537</a>
&#x1F4C8; 6 <br>
<p>Shyam Narayanan</p></summary>
<p>

**Abstract:** We provide improved differentially private algorithms for identity testing of high-dimensional distributions. Specifically, for $d$-dimensional Gaussian distributions with known covariance $Σ$, we can test whether the distribution comes from $\mathcal{N}(μ^*, Σ)$ for some fixed $μ^*$ or from some $\mathcal{N}(μ, Σ)$ with total variation distance at least $α$ from $\mathcal{N}(μ^*, Σ)$ with $(\varepsilon, 0)$-differential privacy, using only \[\tilde{O}\left(\frac{d^{1/2}}{α^2} + \frac{d^{1/3}}{α^{4/3} \cdot \varepsilon^{2/3}} + \frac{1}{α\cdot \varepsilon}\right)\]
samples if the algorithm is allowed to be computationally inefficient, and only \[\tilde{O}\left(\frac{d^{1/2}}{α^2} + \frac{d^{1/4}}{α\cdot \varepsilon}\right)\] samples for a computationally efficient algorithm. We also provide a matching lower bound showing that our computationally inefficient algorithm has optimal sample complexity. We also extend our algorithms to various related problems, including mean testing of Gaussians with bounded but unknown covariance, uniformity testing of product distributions over $\{\pm 1\}^d$, and tolerant testing.
  Our results improve over the previous best work of Canonne, Kamath, McMillan, Ullman, and Zakynthinou \cite{CanonneKMUZ20} for both computationally efficient and inefficient algorithms, and even our computationally efficient algorithm matches the optimal \emph{non-private} sample complexity of $O\left(\frac{\sqrt{d}}{α^2}\right)$ in many standard parameter settings. In addition, our results show that, surprisingly, private identity testing of $d$-dimensional Gaussians can be done with fewer samples than private identity testing of discrete distributions over a domain of size $d$ \cite{AcharyaSZ18}, which refutes a conjectured lower bound of Canonne et al. \cite{CanonneKMUZ20}.

</p>
</details>

<details><summary><b>Learning Incrementally to Segment Multiple Organs in a CT Image</b>
<a href="https://arxiv.org/abs/2203.02100">arxiv:2203.02100</a>
&#x1F4C8; 5 <br>
<p>Pengbo Liu, Xia Wang, Mengsi Fan, Hongli Pan, Minmin Yin, Xiaohong Zhu, Dandan Du, Xiaoying Zhao, Li Xiao, Lian Ding, Xingwang Wu, S. Kevin Zhou</p></summary>
<p>

**Abstract:** There exists a large number of datasets for organ segmentation, which are partially annotated and sequentially constructed. A typical dataset is constructed at a certain time by curating medical images and annotating the organs of interest. In other words, new datasets with annotations of new organ categories are built over time. To unleash the potential behind these partially labeled, sequentially-constructed datasets, we propose to incrementally learn a multi-organ segmentation model. In each incremental learning (IL) stage, we lose the access to previous data and annotations, whose knowledge is assumingly captured by the current model, and gain the access to a new dataset with annotations of new organ categories, from which we learn to update the organ segmentation model to include the new organs. While IL is notorious for its `catastrophic forgetting' weakness in the context of natural image analysis, we experimentally discover that such a weakness mostly disappears for CT multi-organ segmentation. To further stabilize the model performance across the IL stages, we introduce a light memory module and some loss functions to restrain the representation of different categories in feature space, aggregating feature representation of the same class and separating feature representation of different classes. Extensive experiments on five open-sourced datasets are conducted to illustrate the effectiveness of our method.

</p>
</details>

<details><summary><b>WPNAS: Neural Architecture Search by jointly using Weight Sharing and Predictor</b>
<a href="https://arxiv.org/abs/2203.02086">arxiv:2203.02086</a>
&#x1F4C8; 5 <br>
<p>Ke Lin, Yong A, Zhuoxin Gan, Yingying Jiang</p></summary>
<p>

**Abstract:** Weight sharing based and predictor based methods are two major types of fast neural architecture search methods. In this paper, we propose to jointly use weight sharing and predictor in a unified framework. First, we construct a SuperNet in a weight-sharing way and probabilisticly sample architectures from the SuperNet. To increase the correctness of the evaluation of architectures, besides direct evaluation using the inherited weights, we further apply a few-shot predictor to assess the architecture on the other hand. The final evaluation of the architecture is the combination of direct evaluation, the prediction from the predictor and the cost of the architecture. We regard the evaluation as a reward and apply a self-critical policy gradient approach to update the architecture probabilities. To further reduce the side effects of weight sharing, we propose a weakly weight sharing method by introducing another HyperNet. We conduct experiments on datasets including CIFAR-10, CIFAR-100 and ImageNet under NATS-Bench, DARTS and MobileNet search space. The proposed WPNAS method achieves state-of-the-art performance on these datasets.

</p>
</details>

<details><summary><b>User-Level Membership Inference Attack against Metric Embedding Learning</b>
<a href="https://arxiv.org/abs/2203.02077">arxiv:2203.02077</a>
&#x1F4C8; 5 <br>
<p>Guoyao Li, Shahbaz Rezaei, Xin Liu</p></summary>
<p>

**Abstract:** Membership inference (MI) determines if a sample was part of a victim model training set. Recent development of MI attacks focus on record-level membership inference which limits their application in many real-world scenarios. For example, in the person re-identification task, the attacker (or investigator) is interested in determining if a user's images have been used during training or not. However, the exact training images might not be accessible to the attacker. In this paper, we develop a user-level MI attack where the goal is to find if any sample from the target user has been used during training even when no exact training sample is available to the attacker. We focus on metric embedding learning due to its dominance in person re-identification, where user-level MI attack is more sensible. We conduct an extensive evaluation on several datasets and show that our approach achieves high accuracy on user-level MI task.

</p>
</details>

<details><summary><b>Why adversarial training can hurt robust accuracy</b>
<a href="https://arxiv.org/abs/2203.02006">arxiv:2203.02006</a>
&#x1F4C8; 5 <br>
<p>Jacob Clarysse, Julia Hörmann, Fanny Yang</p></summary>
<p>

**Abstract:** Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite may be true -- Even though adversarial training helps when enough data is available, it may hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Our proof provides explanatory insights that may also transfer to feature learning models. Further, we observe in experiments on standard image datasets that the same behavior occurs for perceptible attacks that effectively reduce class information such as mask attacks and object corruptions.

</p>
</details>

<details><summary><b>Region-of-Interest Based Neural Video Compression</b>
<a href="https://arxiv.org/abs/2203.01978">arxiv:2203.01978</a>
&#x1F4C8; 5 <br>
<p>Yura Perugachi-Diaz, Guillaume Sautière, Davide Abati, Yang Yang, Amirhossein Habibian, Taco S Cohen</p></summary>
<p>

**Abstract:** Humans do not perceive all parts of a scene with the same resolution, but rather focus on few regions of interest (ROIs). Traditional Object-Based codecs take advantage of this biological intuition, and are capable of non-uniform allocation of bits in favor of salient regions, at the expense of increased distortion the remaining areas: such a strategy allows a boost in perceptual quality under low rate constraints. Recently, several neural codecs have been introduced for video compression, yet they operate uniformly over all spatial locations, lacking the capability of ROI-based processing. In this paper, we introduce two models for ROI-based neural video coding. First, we propose an implicit model that is fed with a binary ROI mask and it is trained by de-emphasizing the distortion of the background. Secondly, we design an explicit latent scaling method, that allows control over the quantization binwidth for different spatial regions of latent variables, conditioned on the ROI mask. By extensive experiments, we show that our methods outperform all our baselines in terms of Rate-Distortion (R-D) performance in the ROI. Moreover, they can generalize to different datasets and to any arbitrary ROI at inference time. Finally, they do not require expensive pixel-level annotations during training, as synthetic ROI masks can be used with little to no degradation in performance. To the best of our knowledge, our proposals are the first solutions that integrate ROI-based capabilities into neural video compression models.

</p>
</details>

<details><summary><b>A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer's disease using structural MRI images</b>
<a href="https://arxiv.org/abs/2203.01944">arxiv:2203.01944</a>
&#x1F4C8; 5 <br>
<p>Mona Ashtari-Majlan, Abbas Seifi, Mohammad Mahdi Dehshibi</p></summary>
<p>

**Abstract:** Early diagnosis of Alzheimer's disease and its prodromal stage, also known as mild cognitive impairment (MCI), is critical since some patients with progressive MCI will develop the disease. We propose a multi-stream deep convolutional neural network fed with patch-based imaging data to classify stable MCI and progressive MCI. First, we compare MRI images of Alzheimer's disease with cognitively normal subjects to identify distinct anatomical landmarks using a multivariate statistical test. These landmarks are then used to extract patches that are fed into the proposed multi-stream convolutional neural network to classify MRI images. Next, we train the architecture in a separate scenario using samples from Alzheimer's disease images, which are anatomically similar to the progressive MCI ones and cognitively normal images to compensate for the lack of progressive MCI training data. Finally, we transfer the trained model weights to the proposed architecture in order to fine-tune the model using progressive MCI and stable MCI data. Experimental results on the ADNI-1 dataset indicate that our method outperforms existing methods for MCI classification, with an F1-score of 85.96%.

</p>
</details>

<details><summary><b>Color Space-based HoVer-Net for Nuclei Instance Segmentation and Classification</b>
<a href="https://arxiv.org/abs/2203.01940">arxiv:2203.01940</a>
&#x1F4C8; 5 <br>
<p>Hussam Azzuni, Muhammad Ridzuan, Min Xu, Mohammad Yaqub</p></summary>
<p>

**Abstract:** Nuclei segmentation and classification is the first and most crucial step that is utilized for many different microscopy medical analysis applications. However, it suffers from many issues such as the segmentation of small objects, imbalance, and fine-grained differences between types of nuclei. In this paper, multiple different contributions were done tackling these problems present. Firstly, the recently released "ConvNeXt" was used as the encoder for HoVer-Net model since it leverages the key components of transformers that make them perform well. Secondly, to enhance the visual differences between nuclei, a multi-channel color space-based approach is used to aid the model in extracting distinguishing features. Thirdly, Unified Focal loss (UFL) was used to tackle the background-foreground imbalance. Finally, Sharpness-Aware Minimization (SAM) was used to ensure generalizability of the model. Overall, we were able to outperform the current state-of-the-art (SOTA), HoVer-Net, on the preliminary test set of the CoNiC Challenge 2022 by 12.489% mPQ+.

</p>
</details>

<details><summary><b>Label-Only Model Inversion Attacks via Boundary Repulsion</b>
<a href="https://arxiv.org/abs/2203.01925">arxiv:2203.01925</a>
&#x1F4C8; 5 <br>
<p>Mostafa Kahla, Si Chen, Hoang Anh Just, Ruoxi Jia</p></summary>
<p>

**Abstract:** Recent studies show that the state-of-the-art deep neural networks are vulnerable to model inversion attacks, in which access to a model is abused to reconstruct private training data of any given target class. Existing attacks rely on having access to either the complete target model (whitebox) or the model's soft-labels (blackbox). However, no prior work has been done in the harder but more practical scenario, in which the attacker only has access to the model's predicted label, without a confidence measure. In this paper, we introduce an algorithm, Boundary-Repelling Model Inversion (BREP-MI), to invert private training data using only the target model's predicted labels. The key idea of our algorithm is to evaluate the model's predicted labels over a sphere and then estimate the direction to reach the target class's centroid. Using the example of face recognition, we show that the images reconstructed by BREP-MI successfully reproduce the semantics of the private training data for various datasets and target model architectures. We compare BREP-MI with the state-of-the-art whitebox and blackbox model inversion attacks and the results show that despite assuming less knowledge about the target model, BREP-MI outperforms the blackbox attack and achieves comparable results to the whitebox attack.

</p>
</details>

<details><summary><b>Sparse Bayesian Optimization</b>
<a href="https://arxiv.org/abs/2203.01900">arxiv:2203.01900</a>
&#x1F4C8; 5 <br>
<p>Sulin Liu, Qing Feng, David Eriksson, Benjamin Letham, Eytan Bakshy</p></summary>
<p>

**Abstract:** Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often requires taking the interpretability and simplicity of the configurations into consideration, a setting that has not been previously studied in the BO literature. To make BO applicable in this setting, we present several regularization-based approaches that allow us to discover sparse and more interpretable configurations. We propose a novel differentiable relaxation based on homotopy continuation that makes it possible to target sparsity by working directly with $L_0$ regularization. We identify failure modes for regularized BO and develop a hyperparameter-free method, sparsity exploring Bayesian optimization (SEBO) that seeks to simultaneously maximize a target objective and sparsity. SEBO and methods based on fixed regularization are evaluated on synthetic and real-world problems, and we show that we are able to efficiently optimize for sparsity.

</p>
</details>

<details><summary><b>Query Processing on Tensor Computation Runtimes</b>
<a href="https://arxiv.org/abs/2203.01877">arxiv:2203.01877</a>
&#x1F4C8; 5 <br>
<p>Dong He, Supun Nakandala, Dalitso Banda, Rathijit Sen, Karla Saur, Kwanghyun Park, Carlo Curino, Jesús Camacho-Rodríguez, Konstantinos Karanasos, Matteo Interlandi</p></summary>
<p>

**Abstract:** The huge demand for computation in artificial intelligence (AI) is driving unparalleled investments in new hardware and software systems for AI. This leads to an explosion in the number of specialized hardware devices, which are now part of the offerings of major cloud providers. Meanwhile, by hiding the low-level complexity through a tensor-based interface, tensor computation runtimes (TCRs) such as PyTorch allow data scientists to efficiently exploit the exciting capabilities offered by the new hardware. In this paper, we explore how databases can ride the wave of innovation happening in the AI space. Specifically, we present Tensor Query Processor (TQP): a SQL query processor leveraging the tensor interface of TCRs. TQP is able to efficiently run the full TPC-H benchmark by implementing novel algorithms for executing relational operators on the specialized tensor routines provided by TCRs. Meanwhile, TQP can target various hardware while only requiring a fraction of the usual development effort. Experiments show that TQP can improve query execution time by up to 20x over CPU-only systems, and up to 5x over specialized GPU solutions. Finally, TQP can accelerate queries mixing ML predictions and SQL end-to-end, and deliver up to 5x speedup over CPU baselines.

</p>
</details>

<details><summary><b>Robustness and Adaptation to Hidden Factors of Variation</b>
<a href="https://arxiv.org/abs/2203.01864">arxiv:2203.01864</a>
&#x1F4C8; 5 <br>
<p>William Paul, Philippe Burlina</p></summary>
<p>

**Abstract:** We tackle here a specific, still not widely addressed aspect, of AI robustness, which consists of seeking invariance / insensitivity of model performance to hidden factors of variations in the data. Towards this end, we employ a two step strategy that a) does unsupervised discovery, via generative models, of sensitive factors that cause models to under-perform, and b) intervenes models to make their performance invariant to these sensitive factors' influence. We consider 3 separate interventions for robustness, including: data augmentation, semantic consistency, and adversarial alignment. We evaluate our method using metrics that measure trade offs between invariance (insensitivity) and overall performance (utility) and show the benefits of our method for 3 settings (unsupervised, semi-supervised and generalization).

</p>
</details>

<details><summary><b>Generative Modeling for Low Dimensional Speech Attributes with Neural Spline Flows</b>
<a href="https://arxiv.org/abs/2203.01786">arxiv:2203.01786</a>
&#x1F4C8; 5 <br>
<p>Kevin J. Shih, Rafael Valle, Rohan Badlani, João Felipe Santos, Bryan Catanzaro</p></summary>
<p>

**Abstract:** Despite recent advances in generative modeling for text-to-speech synthesis, these models do not yet have the same fine-grained adjustability of pitch-conditioned deterministic models such as FastPitch and FastSpeech2. Pitch information is not only low-dimensional, but also discontinuous, making it particularly difficult to model in a generative setting. Our work explores several techniques for handling the aforementioned issues in the context of Normalizing Flow models. We also find this problem to be very well suited for Neural Spline flows, which is a highly expressive alternative to the more common affine-coupling mechanism in Normalizing Flows.

</p>
</details>

<details><summary><b>On Learning Contrastive Representations for Learning with Noisy Labels</b>
<a href="https://arxiv.org/abs/2203.01785">arxiv:2203.01785</a>
&#x1F4C8; 5 <br>
<p>Li Yi, Sheng Liu, Qi She, A. Ian McLeod, Boyu Wang</p></summary>
<p>

**Abstract:** Deep neural networks are able to memorize noisy labels easily with a softmax cross-entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.

</p>
</details>

<details><summary><b>Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology</b>
<a href="https://arxiv.org/abs/2203.01726">arxiv:2203.01726</a>
&#x1F4C8; 5 <br>
<p>S. Kyathanahally, T. Hardeman, M. Reyes, E. Merz, T. Bulas, F. Pomati, M. Baity-Jesi</p></summary>
<p>

**Abstract:** Monitoring biodiversity is paramount to manage and protect natural resources, particularly in times of global change. Collecting images of organisms over large temporal or spatial scales is a promising practice to monitor and study biodiversity change of natural ecosystems, providing large amounts of data with minimal interference with the environment. Deep learning models are currently used to automate classification of organisms into taxonomic units. However, imprecision in these classifiers introduce a measurement noise that is difficult to control and can significantly hinder the analysis and interpretation of data. In our study, we show that this limitation can be overcome by ensembles of Data-efficient image Transformers (DeiTs), which significantly outperform the previous state of the art (SOTA). We validate our results on a large number of ecological imaging datasets of diverse origin, and organisms of study ranging from plankton to insects, birds, dog breeds, animals in the wild, and corals. On all the data sets we test, we achieve a new SOTA, with a reduction of the error with respect to the previous SOTA ranging from 18.48% to 87.50%, depending on the data set, and often achieving performances very close to perfect classification. The main reason why ensembles of DeiTs perform better is not due to the single-model performance of DeiTs, but rather to the fact that predictions by independent models have a smaller overlap, and this maximizes the profit gained by ensembling. This positions DeiT ensembles as the best candidate for image classification in biodiversity monitoring.

</p>
</details>

<details><summary><b>Selective Residual M-Net for Real Image Denoising</b>
<a href="https://arxiv.org/abs/2203.01645">arxiv:2203.01645</a>
&#x1F4C8; 5 <br>
<p>Chi-Mao Fan, Tsung-Jung Liu, Kuan-Hsien Liu</p></summary>
<p>

**Abstract:** Image restoration is a low-level vision task which is to restore degraded images to noise-free images. With the success of deep neural networks, the convolutional neural networks surpass the traditional restoration methods and become the mainstream in the computer vision area. To advance the performanceof denoising algorithms, we propose a blind real image denoising network (SRMNet) by employing a hierarchical architecture improved from U-Net. Specifically, we use a selective kernel with residual block on the hierarchical structure called M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet has competitive performance results on two synthetic and two real-world noisy datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/TentativeGitHub/SRMNet.

</p>
</details>

<details><summary><b>Early Time-Series Classification Algorithms: An Empirical Comparison</b>
<a href="https://arxiv.org/abs/2203.01628">arxiv:2203.01628</a>
&#x1F4C8; 5 <br>
<p>Charilaos Akasiadis, Evgenios Kladis, Evangelos Michelioudakis, Elias Alevizos, Alexander Artikis</p></summary>
<p>

**Abstract:** Early Time-Series Classification (ETSC) is the task of predicting the class of incoming time-series by observing as few measurements as possible. Such methods can be employed to obtain classification forecasts in many time-critical applications. However, available techniques are not equally suitable for every problem, since differentiations in the data characteristics can impact algorithm performance in terms of earliness, accuracy, F1-score, and training time. We evaluate six existing ETSC algorithms on publicly available data, as well as on two newly introduced datasets originating from the life sciences and maritime domains. Our goal is to provide a framework for the evaluation and comparison of ETSC algorithms and to obtain intuition on how such approaches perform on real-life applications. The presented framework may also serve as a benchmark for new related techniques.

</p>
</details>

<details><summary><b>Nanopublication-Based Semantic Publishing and Reviewing: A Field Study with Formalization Papers</b>
<a href="https://arxiv.org/abs/2203.01608">arxiv:2203.01608</a>
&#x1F4C8; 5 <br>
<p>Cristina-Iulia Bucur, Tobias Kuhn, Davide Ceolin, Jacco van Ossenbruggen</p></summary>
<p>

**Abstract:** With the rapidly increasing amount of scientific literature,it is getting continuously more difficult for researchers in different disciplines to be updated with the recent findings in their field of study.Processing scientific articles in an automated fashion has been proposed as a solution to this problem,but the accuracy of such processing remains very poor for extraction tasks beyond the basic ones.Few approaches have tried to change how we publish scientific results in the first place,by making articles machine-interpretable by expressing them with formal semantics from the start.In the work presented here,we set out to demonstrate that we can formally publish high-level scientific claims in formal logic,and publish the results in a special issue of an existing journal.We use the concept and technology of nanopublications for this endeavor,and represent not just the submissions and final papers in this RDF-based format,but also the whole process in between,including reviews,responses,and decisions.We do this by performing a field study with what we call formalization papers,which contribute a novel formalization of a previously published claim.We received 15 submissions from 18 authors,who then went through the whole publication process leading to the publication of their contributions in the special issue.Our evaluation shows the technical and practical feasibility of our approach.The participating authors mostly showed high levels of interest and confidence,and mostly experienced the process as not very difficult,despite the technical nature of the current user interfaces.We believe that these results indicate that it is possible to publish scientific results from different fields with machine-interpretable semantics from the start,which in turn opens countless possibilities to radically improve in the future the effectiveness and efficiency of the scientific endeavor as a whole.

</p>
</details>

<details><summary><b>A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism</b>
<a href="https://arxiv.org/abs/2203.01594">arxiv:2203.01594</a>
&#x1F4C8; 5 <br>
<p>Rashid Khan, M Shujah Islam, Khadija Kanwal, Mansoor Iqbal, Md. Imran Hossain, Zhongfu Ye</p></summary>
<p>

**Abstract:** Image captioning is a fast-growing research field of computer vision and natural language processing that involves creating text explanations for images. This study aims to develop a system that uses a pre-trained convolutional neural network (CNN) to extract features from an image, integrates the features with an attention mechanism, and creates captions using a recurrent neural network (RNN). To encode an image into a feature vector as graphical attributes, we employed multiple pre-trained convolutional neural networks. Following that, a language model known as GRU is chosen as the decoder to construct the descriptive sentence. In order to increase performance, we merge the Bahdanau attention model with GRU to allow learning to be focused on a specific portion of the image. On the MSCOCO dataset, the experimental results achieve competitive performance against state-of-the-art approaches.

</p>
</details>

<details><summary><b>DareFightingICE Competition: A Fighting Game Sound Design and AI Competition</b>
<a href="https://arxiv.org/abs/2203.01556">arxiv:2203.01556</a>
&#x1F4C8; 5 <br>
<p>Ibrahim Khan, Thai Van Nguyen, Xincheng Dai, Ruck Thawonmas</p></summary>
<p>

**Abstract:** This paper presents a new competition -- at the 2022 IEEE Conference on Games (CoG) -- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG.

</p>
</details>

<details><summary><b>ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor Search on ARM</b>
<a href="https://arxiv.org/abs/2203.02505">arxiv:2203.02505</a>
&#x1F4C8; 4 <br>
<p>Yusuke Matsui, Yoshiki Imaizumi, Naoya Miyamoto, Naoki Yoshifuji</p></summary>
<p>

**Abstract:** We accelerate the 4-bit product quantization (PQ) on the ARM architecture. Notably, the drastic performance of the conventional 4-bit PQ strongly relies on x64-specific SIMD register, such as AVX2; hence, we cannot yet achieve such good performance on ARM. To fill this gap, we first bundle two 128-bit registers as one 256-bit component. We then apply shuffle operations for each using the ARM-specific NEON instruction. By making this simple but critical modification, we achieve a dramatic speedup for the 4-bit PQ on an ARM architecture. Experiments show that the proposed method consistently achieves a 10x improvement over the naive PQ with the same accuracy.

</p>
</details>

<details><summary><b>FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis</b>
<a href="https://arxiv.org/abs/2203.02110">arxiv:2203.02110</a>
&#x1F4C8; 4 <br>
<p>Yawen Wu, Dewen Zeng, Xiaowei Xu, Yiyu Shi, Jingtong Hu</p></summary>
<p>

**Abstract:** Many works have shown that deep learning-based medical image classification models can exhibit bias toward certain demographic attributes like race, gender, and age. Existing bias mitigation methods primarily focus on learning debiased models, which may not necessarily guarantee all sensitive information can be removed and usually comes with considerable accuracy degradation on both privileged and unprivileged groups. To tackle this issue, we propose a method, FairPrune, that achieves fairness by pruning. Conventionally, pruning is used to reduce the model size for efficient inference. However, we show that pruning can also be a powerful tool to achieve fairness. Our observation is that during pruning, each parameter in the model has different importance for different groups' accuracy. By pruning the parameters based on this importance difference, we can reduce the accuracy difference between the privileged group and the unprivileged group to improve fairness without a large accuracy drop. To this end, we use the second derivative of the parameters of a pre-trained model to quantify the importance of each parameter with respect to the model accuracy for each group. Experiments on two skin lesion diagnosis datasets over multiple sensitive attributes demonstrate that our method can greatly improve fairness while keeping the average accuracy of both groups as high as possible.

</p>
</details>

<details><summary><b>Universal Segmentation of 33 Anatomies</b>
<a href="https://arxiv.org/abs/2203.02098">arxiv:2203.02098</a>
&#x1F4C8; 4 <br>
<p>Pengbo Liu, Yang Deng, Ce Wang, Yuan Hui, Qian Li, Jun Li, Shiwei Luo, Mengke Sun, Quan Quan, Shuxin Yang, You Hao, Honghu Xiao, Chunpeng Zhao, Xinbao Wu, S. Kevin Zhou</p></summary>
<p>

**Abstract:** In the paper, we present an approach for learning a single model that universally segments 33 anatomical structures, including vertebrae, pelvic bones, and abdominal organs. Our model building has to address the following challenges. Firstly, while it is ideal to learn such a model from a large-scale, fully-annotated dataset, it is practically hard to curate such a dataset. Thus, we resort to learn from a union of multiple datasets, with each dataset containing the images that are partially labeled. Secondly, along the line of partial labelling, we contribute an open-source, large-scale vertebra segmentation dataset for the benefit of spine analysis community, CTSpine1K, boasting over 1,000 3D volumes and over 11K annotated vertebrae. Thirdly, in a 3D medical image segmentation task, due to the limitation of GPU memory, we always train a model using cropped patches as inputs instead a whole 3D volume, which limits the amount of contextual information to be learned. To this, we propose a cross-patch transformer module to fuse more information in adjacent patches, which enlarges the aggregated receptive field for improved segmentation performance. This is especially important for segmenting, say, the elongated spine. Based on 7 partially labeled datasets that collectively contain about 2,800 3D volumes, we successfully learn such a universal model. Finally, we evaluate the universal model on multiple open-source datasets, proving that our model has a good generalization performance and can potentially serve as a solid foundation for downstream tasks.

</p>
</details>

<details><summary><b>X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback</b>
<a href="https://arxiv.org/abs/2203.02072">arxiv:2203.02072</a>
&#x1F4C8; 4 <br>
<p>Jensen Gao, Siddharth Reddy, Glen Berseth, Nicholas Hardy, Nikhilesh Natraj, Karunesh Ganguly, Anca D. Dragan, Sergey Levine</p></summary>
<p>

**Abstract:** We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze or neural activity measured by a brain implant. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, a large-scale observational study on handwriting samples from 60 users, and a pilot study with one participant using an electrocorticography-based brain-computer interface. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.

</p>
</details>

<details><summary><b>Counting Molecules: Python based scheme for automated enumeration and categorization of molecules in scanning tunneling microscopy images</b>
<a href="https://arxiv.org/abs/2203.01998">arxiv:2203.01998</a>
&#x1F4C8; 4 <br>
<p>Jack Hellerstedt, Aleš Cahlík, Martin Švec, Oleksandr Stetsovych, Tyler Hennen</p></summary>
<p>

**Abstract:** Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly progressing to offer unprecedented spatial resolution of a diverse array of chemical species. In particular, they are employed to characterize on-surface chemical reactions by directly examining precursors and products. Chiral effects and self-assembled structures can also be investigated. This open source, modular, python based scheme automates the categorization of a variety of molecules present in medium sized (10$\times$10 to 100$\times$100 nm) scanned probe images.

</p>
</details>

<details><summary><b>Semantic-guided Image Virtual Attribute Learning for Noisy Multi-label Chest X-ray Classification</b>
<a href="https://arxiv.org/abs/2203.01937">arxiv:2203.01937</a>
&#x1F4C8; 4 <br>
<p>Yuanhong Chen, Fengbei Liu, Yu Tian, Yuyuan Liu, Gustavo Carneiro</p></summary>
<p>

**Abstract:** Deep learning methods have shown outstanding classification accuracy in medical image analysis problems, which is largely attributed to the availability of large datasets manually annotated with clean labels. However, such manual annotation can be expensive to obtain for large datasets, so we may rely on machine-generated noisy labels. Many Chest X-ray (CXR) classifiers are modelled from datasets with machine-generated labels, but their training procedure is in general not robust to the presence of noisy-label samples and can overfit those samples to produce sub-optimal solutions. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. To address such noisy multi-label CXR learning problem, we propose a new learning method based on estimating image virtual attributes using semantic information from the label to assist in the identification and correction of noisy multi-labels from training samples. Our experiments on diverse noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness across all datasets.

</p>
</details>

<details><summary><b>ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images</b>
<a href="https://arxiv.org/abs/2203.01883">arxiv:2203.01883</a>
&#x1F4C8; 4 <br>
<p>Mohammad Rahimzadeh, Mahmoud Reza Mohammadi</p></summary>
<p>

**Abstract:** Optical coherence tomography (OCT) imaging is a well-known technology for visualizing retinal layers and helps ophthalmologists to detect possible diseases. Accurate and early diagnosis of common retinal diseases can prevent the patients from suffering critical damages to their vision. Computer-aided diagnosis (CAD) systems can significantly assist ophthalmologists in improving their examinations. This paper presents a new enhanced deep ensemble convolutional neural network for detecting retinal diseases from OCT images. Our model generates rich and multi-resolution features by employing the learning architectures of two robust convolutional models. Spatial resolution is a critical factor in medical images, especially the OCT images that contain tiny essential points. To empower our model, we apply a new post-architecture model to our ensemble model for enhancing spatial resolution learning without increasing computational costs. The introduced post-architecture model can be deployed to any feature extraction model to improve the utilization of the feature map's spatial values. We have collected two open-source datasets for our experiments to make our models capable of detecting six crucial retinal diseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen alongside the normal cases. Our experiments on two datasets and comparing our model with some other well-known deep convolutional neural networks have proven that our architecture can increase the classification accuracy up to 5%. We hope that our proposed methods create the next step of CAD systems development and help future researches. The code of this paper is shared at https://github.com/mr7495/OCT-classification.

</p>
</details>

<details><summary><b>DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy</b>
<a href="https://arxiv.org/abs/2203.01882">arxiv:2203.01882</a>
&#x1F4C8; 4 <br>
<p>Juan P. Vigueras-Guillén, Jeroen van Rooij, Bart T. H. van Dooren, Hans G. Lemij, Esma Islamaj, Lucas J. van Vliet, Koenraad A. Vermeer</p></summary>
<p>

**Abstract:** To estimate the corneal endothelial parameters from specular microscopy images depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new deep learning methodology that includes a novel attention mechanism named feedback non-local attention (fNLA). Our approach first infers the cell edges, then selects the cells that are well detected, and finally applies a postprocessing method to correct mistakes and provide the binary segmentation from which the corneal parameters are estimated (cell density [ECD], coefficient of variation [CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which contained guttae. Manual segmentation was performed in all images. We compared the results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with fNLA provided the best performance, with a mean absolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6 times smaller than the error obtained by Topcon's built-in software. Our approach handled the cells affected by guttae remarkably well, detecting cell edges occluded by small guttae while discarding areas covered by large guttae. fNLA made use of the local information, providing sharper edges in guttae areas and better results in the selection of well-detected cells. Overall, the proposed method obtained reliable and accurate estimations in extremely challenging specular images with guttae, being the first method in the literature to solve this problem adequately. Code is available in our GitHub.

</p>
</details>

<details><summary><b>Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2203.01855">arxiv:2203.01855</a>
&#x1F4C8; 4 <br>
<p>Michael S. Lee, Henny Admoni, Reid Simmons</p></summary>
<p>

**Abstract:** To collaborate well with robots, we must be able to understand their decision making. Humans naturally infer other agents' beliefs and desires by reasoning about their observable behavior in a way that resembles inverse reinforcement learning (IRL). Thus, robots can convey their beliefs and desires by providing demonstrations that are informative for a human's IRL. An informative demonstration is one that differs strongly from the learner's expectations of what the robot will do given their current understanding of the robot's decision making. However, standard IRL does not model the learner's existing expectations, and thus cannot do this counterfactual reasoning. We propose to incorporate the learner's current understanding of the robot's decision making into our model of human IRL, so that our robot can select demonstrations that maximize the human's understanding. We also propose a novel measure for estimating the difficulty for a human to predict instances of a robot's behavior in unseen environments. A user study finds that our test difficulty measure correlates well with human performance and confidence. Interestingly, considering human beliefs and counterfactuals when selecting demonstrations decreases human performance on easy tests, but increases performance on difficult tests, providing insight on how to best utilize such models.

</p>
</details>

<details><summary><b>On Practical Reinforcement Learning: Provable Robustness, Scalability, and Statistical Efficiency</b>
<a href="https://arxiv.org/abs/2203.01758">arxiv:2203.01758</a>
&#x1F4C8; 4 <br>
<p>Thanh Nguyen-Tang</p></summary>
<p>

**Abstract:** This thesis rigorously studies fundamental reinforcement learning (RL) methods in modern practical considerations, including robust RL, distributional RL, and offline RL with neural function approximation. The thesis first prepares the readers with an overall overview of RL and key technical background in statistics and optimization. In each of the settings, the thesis motivates the problems to be studied, reviews the current literature, provides computationally efficient algorithms with provable efficiency guarantees, and concludes with future research directions. The thesis makes fundamental contributions to the three settings above, both algorithmically, theoretically, and empirically, while staying relevant to practical considerations.

</p>
</details>

<details><summary><b>Accelerated SGD for Non-Strongly-Convex Least Squares</b>
<a href="https://arxiv.org/abs/2203.01744">arxiv:2203.01744</a>
&#x1F4C8; 4 <br>
<p>Aditya Varre, Nicolas Flammarion</p></summary>
<p>

**Abstract:** We consider stochastic approximation for the least squares regression problem in the non-strongly convex setting. We present the first practical algorithm that achieves the optimal prediction error rates in terms of dependence on the noise of the problem, as $O(d/t)$ while accelerating the forgetting of the initial conditions to $O(d/t^2)$. Our new algorithm is based on a simple modification of the accelerated gradient descent. We provide convergence results for both the averaged and the last iterate of the algorithm. In order to describe the tightness of these new bounds, we present a matching lower bound in the noiseless setting and thus show the optimality of our algorithm.

</p>
</details>

<details><summary><b>Machine learning model to project the impact of Ukraine crisis</b>
<a href="https://arxiv.org/abs/2203.01738">arxiv:2203.01738</a>
&#x1F4C8; 4 <br>
<p>Javad T. Firouzjaee, Pouriya Khaliliyan</p></summary>
<p>

**Abstract:** Russia's attack on Ukraine on Thursday 24 February 2022 hitched financial markets and the increased geopolitical crisis. In this paper, we select some main economic indexes, such as Gold, Oil (WTI), NDAQ, and known currency which are involved in this crisis and try to find the quantitative effect of this war on them. To quantify the war effect, we use the correlation feature and the relationships between these economic indices, create datasets, and compare the results of forecasts with real data. To study war effects, we use Machine Learning Linear Regression. We carry on empirical experiments and perform on these economic indices datasets to evaluate and predict this war tolls and its effects on main economics indexes.

</p>
</details>

<details><summary><b>Detecting High-Quality GAN-Generated Face Images using Neural Networks</b>
<a href="https://arxiv.org/abs/2203.01716">arxiv:2203.01716</a>
&#x1F4C8; 4 <br>
<p>Ehsan Nowroozi, Mauro Conti, Yassine Mekdad</p></summary>
<p>

**Abstract:** In the past decades, the excessive use of the last-generation GAN (Generative Adversarial Networks) models in computer vision has enabled the creation of artificial face images that are visually indistinguishable from genuine ones. These images are particularly used in adversarial settings to create fake social media accounts and other fake online profiles. Such malicious activities can negatively impact the trustworthiness of users identities. On the other hand, the recent development of GAN models may create high-quality face images without evidence of spatial artifacts. Therefore, reassembling uniform color channel correlations is a challenging research problem. To face these challenges, we need to develop efficient tools able to differentiate between fake and authentic face images. In this chapter, we propose a new strategy to differentiate GAN-generated images from authentic images by leveraging spectral band discrepancies, focusing on artificial face image synthesis. In particular, we enable the digital preservation of face images using the Cross-band co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these techniques and feed them to a Convolutional Neural Networks (CNN) architecture to identify the real from artificial faces. Additionally, we show that the performance boost is particularly significant and achieves more than 92% in different post-processing environments. Finally, we provide several research observations demonstrating that this strategy improves a comparable detection method based only on intra-band spatial co-occurrences.

</p>
</details>

<details><summary><b>Translational Lung Imaging Analysis Through Disentangled Representations</b>
<a href="https://arxiv.org/abs/2203.01668">arxiv:2203.01668</a>
&#x1F4C8; 4 <br>
<p>Pedro M. Gordaliza, Juan José Vaquero, Arrate Muñoz-Barrutia</p></summary>
<p>

**Abstract:** The development of new treatments often requires clinical trials with translational animal models using (pre)-clinical imaging to characterize inter-species pathological processes. Deep Learning (DL) models are commonly used to automate retrieving relevant information from the images. Nevertheless, they typically suffer from low generability and explainability as a product of their entangled design, resulting in a specific DL model per animal model. Consequently, it is not possible to take advantage of the high capacity of DL to discover statistical relationships from inter-species images.
  To alleviate this problem, in this work, we present a model capable of extracting disentangled information from images of different animal models and the mechanisms that generate the images. Our method is located at the intersection between deep generative models, disentanglement and causal representation learning. It is optimized from images of pathological lung infected by Tuberculosis and is able: a) from an input slice, infer its position in a volume, the animal model to which it belongs, the damage present and even more, generate a mask covering the whole lung (similar overlap measures to the nnU-Net), b) generate realistic lung images by setting the above variables and c) generate counterfactual images, namely, healthy versions of a damaged input slice.

</p>
</details>

<details><summary><b>Continuous Relaxation For The Multivariate Non-Central Hypergeometric Distribution</b>
<a href="https://arxiv.org/abs/2203.01629">arxiv:2203.01629</a>
&#x1F4C8; 4 <br>
<p>Thomas M. Sutter, Laura Manduchi, Alain Ryser, Julia E. Vogt</p></summary>
<p>

**Abstract:** Partitioning a set of elements into a given number of groups of a priori unknown sizes is an important task in many applications. Due to hard constraints, it is a non-differentiable problem which prohibits its direct use in modern machine learning frameworks. Hence, previous works mostly fall back on suboptimal heuristics or simplified assumptions. The multivariate hypergeometric distribution offers a probabilistic formulation of how to distribute a given number of samples across multiple groups. Unfortunately, as a discrete probability distribution, it neither is differentiable. In this work, we propose a continuous relaxation for the multivariate non-central hypergeometric distribution. We introduce an efficient and numerically stable sampling procedure. This enables reparameterized gradients for the hypergeometric distribution and its integration into automatic differentiation frameworks. We highlight the applicability and usability of the proposed formulation on two different common machine learning tasks.

</p>
</details>

<details><summary><b>Data Augmentation as Feature Manipulation: a story of desert cows and grass cows</b>
<a href="https://arxiv.org/abs/2203.01572">arxiv:2203.01572</a>
&#x1F4C8; 4 <br>
<p>Ruoqi Shen, Sébastien Bubeck, Suriya Gunasekar</p></summary>
<p>

**Abstract:** Data augmentation is a cornerstone of the machine learning pipeline, yet its theoretical underpinnings remain unclear. Is it merely a way to artificially augment the data set size? Or is it about encouraging the model to satisfy certain invariance? In this work we consider another angle, and we study the effect of data augmentation on the dynamic of the learning process. We find that data augmentation can alter the relative importance of various features, effectively making certain informative but hard to learn features more likely to be captured in the learning process. Importantly, we show that this effect is more pronounced for non-linear models, such as neural networks. Our main contribution is a detailed analysis of data augmentation on the learning dynamic for a two layer convolutional neural network in the recently proposed multi-view model by Allen-Zhu and Li [2020]. We complement this analysis with further experimental evidence that data augmentation can be viewed as a form of feature manipulation.

</p>
</details>

<details><summary><b>Rethinking the role of normalization and residual blocks for spiking neural networks</b>
<a href="https://arxiv.org/abs/2203.01544">arxiv:2203.01544</a>
&#x1F4C8; 4 <br>
<p>Shin-ichi Ikegawa, Ryuji Saiin, Yoshihide Sawada, Naotake Natori</p></summary>
<p>

**Abstract:** Biologically inspired spiking neural networks (SNNs) are widely used to realize ultralow-power energy consumption. However, deep SNNs are not easy to train due to the excessive firing of spiking neurons in the hidden layers. To tackle this problem, we propose a novel but simple normalization technique called postsynaptic potential normalization. This normalization removes the subtraction term from the standard normalization and uses the second raw moment instead of the variance as the division term. The spike firing can be controlled, enabling the training to proceed appropriating, by conducting this simple normalization to the postsynaptic potential. The experimental results show that SNNs with our normalization outperformed other models using other normalizations. Furthermore, through the pre-activation residual blocks, the proposed model can train with more than 100 layers without other special techniques dedicated to SNNs.

</p>
</details>

<details><summary><b>QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition</b>
<a href="https://arxiv.org/abs/2203.01543">arxiv:2203.01543</a>
&#x1F4C8; 4 <br>
<p>Andy T. Liu, Wei Xiao, Henghui Zhu, Dejiao Zhang, Shang-Wen Li, Andrew Arnold</p></summary>
<p>

**Abstract:** Recently, prompt-based learning for pre-trained language models has succeeded in few-shot Named Entity Recognition (NER) by exploiting prompts as task guidance to increase label efficiency. However, previous prompt-based methods for few-shot NER have limitations such as a higher computational complexity, poor zero-shot ability, requiring manual prompt engineering, or lack of prompt robustness. In this work, we address these shortcomings by proposing a new prompt-based learning NER method with Question Answering (QA), called QaNER. Our approach includes 1) a refined strategy for converting NER problems into the QA formulation; 2) NER prompt generation for QA models; 3) prompt-based tuning with QA models on a few annotated NER examples; 4) zero-shot NER by prompting the QA model. Comparing the proposed approach with previous methods, QaNER is faster at inference, insensitive to the prompt quality, and robust to hyper-parameters, as well as demonstrating significantly better low-resource performance and zero-shot capability.

</p>
</details>

<details><summary><b>Self-supervised Transparent Liquid Segmentation for Robotic Pouring</b>
<a href="https://arxiv.org/abs/2203.01538">arxiv:2203.01538</a>
&#x1F4C8; 4 <br>
<p>Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held</p></summary>
<p>

**Abstract:** Liquid state estimation is important for robotics tasks such as pouring; however, estimating the state of transparent liquids is a challenging problem. We propose a novel segmentation pipeline that can segment transparent liquids such as water from a static, RGB image without requiring any manual annotations or heating of the liquid for training. Instead, we use a generative model that is capable of translating images of colored liquids into synthetically generated transparent liquid images, trained only on an unpaired dataset of colored and transparent liquid images. Segmentation labels of colored liquids are obtained automatically using background subtraction. Our experiments show that we are able to accurately predict a segmentation mask for transparent liquids without requiring any manual annotations. We demonstrate the utility of transparent liquid segmentation in a robotic pouring task that controls pouring by perceiving the liquid height in a transparent cup. Accompanying video and supplementary materials can be found

</p>
</details>

<details><summary><b>SIERRA: A Modular Framework for Research Automation</b>
<a href="https://arxiv.org/abs/2203.04748">arxiv:2203.04748</a>
&#x1F4C8; 3 <br>
<p>John Harwell, London Lowmanstone, Maria Gini</p></summary>
<p>

**Abstract:** Modern intelligent systems researchers employ the scientific method: they form hypotheses about system behavior, and then run experiments using one or more independent variables to test their hypotheses. We present SIERRA, a novel framework structured around that idea for accelerating research developments and improving reproducibility of results. SIERRA makes it easy to quickly specify the independent variable(s) for an experiment, generate experimental inputs, automatically run the experiment, and process the results to generate deliverables such as graphs and videos. SIERRA provides reproducible automation independent of the execution environment (HPC hardware, real robots, etc.) and targeted platform (arbitrary simulator or real robots), enabling exact experiment replication (up to the limit of the execution environment and platform). It employs a deeply modular approach that allows easy customization and extension of automation for the needs of individual researchers, thereby eliminating manual experiment configuration and result processing via throw-away scripts.

</p>
</details>

<details><summary><b>MixCL: Pixel label matters to contrastive learning</b>
<a href="https://arxiv.org/abs/2203.02114">arxiv:2203.02114</a>
&#x1F4C8; 3 <br>
<p>Jun Li, Quan Quan, S. Kevin Zhou</p></summary>
<p>

**Abstract:** Contrastive learning and self-supervised techniques have gained prevalence in computer vision for the past few years. It is essential for medical image analysis, which is often notorious for its lack of annotations. Most existing self-supervised methods applied in natural imaging tasks focus on designing proxy tasks for unlabeled data. For example, contrastive learning is often based on the fact that an image and its transformed version share the same identity. However, pixel annotations contain much valuable information for medical image segmentation, which is largely ignored in contrastive learning. In this work, we propose a novel pre-training framework called Mixed Contrastive Learning (MixCL) that leverages both image identities and pixel labels for better modeling by maintaining identity consistency, label consistency, and reconstruction consistency together. Consequently, thus pre-trained model has more robust representations that characterize medical images. Extensive experiments demonstrate the effectiveness of the proposed method, improving the baseline by 5.28% and 14.12% in Dice coefficient when 5% labeled data of Spleen and 15% of BTVC are used in fine-tuning, respectively.

</p>
</details>

<details><summary><b>Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision</b>
<a href="https://arxiv.org/abs/2203.02106">arxiv:2203.02106</a>
&#x1F4C8; 3 <br>
<p>Xiangde Luo, Minhao Hu, Wenjun Liao, Shuwei Zhai, Tao Song, Guotai Wang, Shaoting Zhang</p></summary>
<p>

**Abstract:** Medical image segmentation plays an irreplaceable role in computer-assisted diagnosis, treatment planning, and following-up. Collecting and annotating a large-scale dataset is crucial to training a powerful segmentation model, but producing high-quality segmentation masks is an expensive and time-consuming procedure. Recently, weakly-supervised learning that uses sparse annotations (points, scribbles, bounding boxes) for network training has achieved encouraging performance and shown the potential for annotation cost reduction. However, due to the limited supervision signal of sparse annotations, it is still challenging to employ them for networks training directly. In this work, we propose a simple yet efficient scribble-supervised image segmentation method and apply it to cardiac MRI segmentation. Specifically, we employ a dual-branch network with one encoder and two slightly different decoders for image segmentation and dynamically mix the two decoders' predictions to generate pseudo labels for auxiliary supervision. By combining the scribble supervision and auxiliary pseudo labels supervision, the dual-branch network can efficiently learn from scribble annotations end-to-end. Experiments on the public ACDC dataset show that our method performs better than current scribble-supervised segmentation methods and also outperforms several semi-supervised segmentation methods.

</p>
</details>

<details><summary><b>Enhanced physics-constrained deep neural networks for modeling vanadium redox flow battery</b>
<a href="https://arxiv.org/abs/2203.01985">arxiv:2203.01985</a>
&#x1F4C8; 3 <br>
<p>QiZhi He, Yucheng Fu, Panos Stinis, Alexandre Tartakovsky</p></summary>
<p>

**Abstract:** Numerical modeling and simulation have become indispensable tools for advancing a comprehensive understanding of the underlying mechanisms and cost-effective process optimization and control of flow batteries. In this study, we propose an enhanced version of the physics-constrained deep neural network (PCDNN) approach [1] to provide high-accuracy voltage predictions in the vanadium redox flow batteries (VRFBs). The purpose of the PCDNN approach is to enforce the physics-based zero-dimensional (0D) VRFB model in a neural network to assure model generalization for various battery operation conditions. Limited by the simplifications of the 0D model, the PCDNN cannot capture sharp voltage changes in the extreme SOC regions. To improve the accuracy of voltage prediction at extreme ranges, we introduce a second (enhanced) DNN to mitigate the prediction errors carried from the 0D model itself and call the resulting approach enhanced PCDNN (ePCDNN). By comparing the model prediction with experimental data, we demonstrate that the ePCDNN approach can accurately capture the voltage response throughout the charge--discharge cycle, including the tail region of the voltage discharge curve. Compared to the standard PCDNN, the prediction accuracy of the ePCDNN is significantly improved. The loss function for training the ePCDNN is designed to be flexible by adjusting the weights of the physics-constrained DNN and the enhanced DNN. This allows the ePCDNN framework to be transferable to battery systems with variable physical model fidelity.

</p>
</details>

<details><summary><b>Investigating the limited performance of a deep-learning-based SPECT denoising approach: An observer-study-based characterization</b>
<a href="https://arxiv.org/abs/2203.01918">arxiv:2203.01918</a>
&#x1F4C8; 3 <br>
<p>Zitong Yu, Md Ashequr Rahman, Abhinav K. Jha</p></summary>
<p>

**Abstract:** Multiple objective assessment of image-quality-based studies have reported that several deep-learning-based denoising methods show limited performance on signal-detection tasks. Our goal was to investigate the reasons for this limited performance. To achieve this goal, we conducted a task-based characterization of a DL-based denoising approach for individual signal properties. We conducted this study in the context of evaluating a DL-based approach for denoising SPECT images. The training data consisted of signals of different sizes and shapes within a clustered-lumpy background, imaged with a 2D parallel-hole-collimator SPECT system. The projections were generated at normal and 20% low count level, both of which were reconstructed using an OSEM algorithm. A CNN-based denoiser was trained to process the low-count images. The performance of this CNN was characterized for five different signal sizes and four different SBR by designing each evaluation as an SKE/BKS signal-detection task. Performance on this task was evaluated using an anthropomorphic CHO. As in previous studies, we observed that the DL-based denoising method did not improve performance on signal-detection tasks. Evaluation using the idea of observer-study-based characterization demonstrated that the DL-based denoising approach did not improve performance on the signal-detection task for any of the signal types. Overall, these results provide new insights on the performance of the DL-based denoising approach as a function of signal size and contrast. More generally, the observer study-based characterization provides a mechanism to evaluate the sensitivity of the method to specific object properties and may be explored as analogous to characterizations such as modulation transfer function for linear systems. Finally, this work underscores the need for objective task-based evaluation of DL-based denoising approaches.

</p>
</details>

<details><summary><b>Computer Vision Aided Blockage Prediction in Real-World Millimeter Wave Deployments</b>
<a href="https://arxiv.org/abs/2203.01907">arxiv:2203.01907</a>
&#x1F4C8; 3 <br>
<p>Gouranga Charan, Ahmed Alkhateeb</p></summary>
<p>

**Abstract:** This paper provides the first real-world evaluation of using visual (RGB camera) data and machine learning for proactively predicting millimeter wave (mmWave) dynamic link blockages before they happen. Proactively predicting line-of-sight (LOS) link blockages enables mmWave/sub-THz networks to make proactive network management decisions, such as proactive beam switching and hand-off) before a link failure happens. This can significantly enhance the network reliability and latency while efficiently utilizing the wireless resources. To evaluate this gain in reality, this paper (i) develops a computer vision based solution that processes the visual data captured by a camera installed at the infrastructure node and (ii) studies the feasibility of the proposed solution based on the large-scale real-world dataset, DeepSense 6G, that comprises multi-modal sensing and communication data. Based on the adopted real-world dataset, the developed solution achieves $\approx 90\%$ accuracy in predicting blockages happening within the future $0.1$s and $\approx 80\%$ for blockages happening within $1$s, which highlights a promising solution for mmWave/sub-THz communication networks.

</p>
</details>

<details><summary><b>Quantum Reinforcement Learning via Policy Iteration</b>
<a href="https://arxiv.org/abs/2203.01889">arxiv:2203.01889</a>
&#x1F4C8; 3 <br>
<p>El Amine Cherrat, Iordanis Kerenidis, Anupam Prakash</p></summary>
<p>

**Abstract:** Quantum computing has shown the potential to substantially speed up machine learning applications, in particular for supervised and unsupervised learning. Reinforcement learning, on the other hand, has become essential for solving many decision making problems and policy iteration methods remain the foundation of such approaches. In this paper, we provide a general framework for performing quantum reinforcement learning via policy iteration. We validate our framework by designing and analyzing: \emph{quantum policy evaluation} methods for infinite horizon discounted problems by building quantum states that approximately encode the value function of a policy $π$; and \emph{quantum policy improvement} methods by post-processing measurement outcomes on these quantum states. Last, we study the theoretical and experimental performance of our quantum algorithms on two environments from OpenAI's Gym.

</p>
</details>

<details><summary><b>KamNet: An Integrated Spatiotemporal Deep Neural Network for Rare Event Search in KamLAND-Zen</b>
<a href="https://arxiv.org/abs/2203.01870">arxiv:2203.01870</a>
&#x1F4C8; 3 <br>
<p>A. Li, Z. Fu, L. Winslow, C. Grant, H. Song, H. Ozaki, I. Shimizu, A. Takeuchi</p></summary>
<p>

**Abstract:** Rare event searches allow us to search for new physics at energy scales inaccessible with other means by leveraging specialized large-mass detectors. Machine learning provides a new tool to maximize the information provided by these detectors. The information is sparse, which forces these algorithms to start from the lowest level data and exploit all symmetries in the detector to produce results. In this work we present KamNet which harnesses breakthroughs in geometric deep learning and spatiotemporal data analysis to maximize the physics reach of KamLAND-Zen, a kiloton scale spherical liquid scintillator detector searching for neutrinoless double beta decay ($0νββ$). Using a simplified background model for KamLAND we show that KamNet outperforms a conventional CNN on benchmarking MC simulations with an increasing level of robustness. Using simulated data, we then demonstrate KamNet's ability to increase KamLAND-Zen's sensitivity to $0νββ$ and $0νββ$ to excited states. A key component of this work is the addition of an attention mechanism to elucidate the underlying physics KamNet is using for the background rejection.

</p>
</details>

<details><summary><b>The world seems different in a social context: a neural network analysis of human experimental data</b>
<a href="https://arxiv.org/abs/2203.01862">arxiv:2203.01862</a>
&#x1F4C8; 3 <br>
<p>Maria Tsfasman, Anja Philippsen, Carlo Mazzola, Serge Thill, Alessandra Sciutti, Yukie Nagai</p></summary>
<p>

**Abstract:** Human perception and behavior are affected by the situational context, in particular during social interactions. A recent study demonstrated that humans perceive visual stimuli differently depending on whether they do the task by themselves or together with a robot. Specifically, it was found that the central tendency effect is stronger in social than in non-social task settings. The particular nature of such behavioral changes induced by social interaction, and their underlying cognitive processes in the human brain are, however, still not well understood. In this paper, we address this question by training an artificial neural network inspired by the predictive coding theory on the above behavioral data set. Using this computational model, we investigate whether the change in behavior that was caused by the situational context in the human experiment could be explained by continuous modifications of a parameter expressing how strongly sensory and prior information affect perception. We demonstrate that it is possible to replicate human behavioral data in both individual and social task settings by modifying the precision of prior and sensory signals, indicating that social and non-social task settings might in fact exist on a continuum. At the same time an analysis of the neural activation traces of the trained networks provides evidence that information is coded in fundamentally different ways in the network in the individual and in the social conditions. Our results emphasize the importance of computational replications of behavioral data for generating hypotheses on the underlying cognitive mechanisms of shared perception and may provide inspiration for follow-up studies in the field of neuroscience.

</p>
</details>

<details><summary><b>Identification in Tree-shaped Linear Structural Causal Models</b>
<a href="https://arxiv.org/abs/2203.01852">arxiv:2203.01852</a>
&#x1F4C8; 3 <br>
<p>Benito van der Zander, Marcel Wienöbst, Markus Bläser, Maciej Liśkiewicz</p></summary>
<p>

**Abstract:** Linear structural equation models represent direct causal effects as directed edges and confounding factors as bidirected edges. An open problem is to identify the causal parameters from correlations between the nodes. We investigate models, whose directed component forms a tree, and show that there, besides classical instrumental variables, missing cycles of bidirected edges can be used to identify the model. They can yield systems of quadratic equations that we explicitly solve to obtain one or two solutions for the causal parameters of adjacent directed edges. We show how multiple missing cycles can be combined to obtain a unique solution. This results in an algorithm that can identify instances that previously required approaches based on Gröbner bases, which have doubly-exponential time complexity in the number of structural parameters.

</p>
</details>

<details><summary><b>T-Cal: An optimal test for the calibration of predictive models</b>
<a href="https://arxiv.org/abs/2203.01850">arxiv:2203.01850</a>
&#x1F4C8; 3 <br>
<p>Donghwan Lee, Xinmeng Huang, Hamed Hassani, Edgar Dobriban</p></summary>
<p>

**Abstract:** The prediction accuracy of machine learning methods is steadily increasing, but the calibration of their uncertainty predictions poses a significant challenge. Numerous works focus on obtaining well-calibrated predictive models, but less is known about reliably assessing model calibration. This limits our ability to know when algorithms for improving calibration have a real effect, and when their improvements are merely artifacts due to random noise in finite datasets. In this work, we consider detecting mis-calibration of predictive models using a finite validation dataset as a hypothesis testing problem. The null hypothesis is that the predictive model is calibrated, while the alternative hypothesis is that the deviation from calibration is sufficiently large.
  We find that detecting mis-calibration is only possible when the conditional probabilities of the classes are sufficiently smooth functions of the predictions. When the conditional class probabilities are Hölder continuous, we propose T-Cal, a minimax optimal test for calibration based on a debiased plug-in estimator of the $\ell_2$-Expected Calibration Error (ECE). We further propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We verify our theoretical findings with a broad range of experiments, including with several popular deep neural net architectures and several standard post-hoc calibration methods. T-Cal is a practical general-purpose tool, which -- combined with classical tests for discrete-valued predictors -- can be used to test the calibration of virtually any probabilistic classification method.

</p>
</details>

<details><summary><b>Reinforcement Learning in Possibly Nonstationary Environments</b>
<a href="https://arxiv.org/abs/2203.01707">arxiv:2203.01707</a>
&#x1F4C8; 3 <br>
<p>Mengbing Li, Chengchun Shi, Zhenke Wu, Piotr Fryzlewicz</p></summary>
<p>

**Abstract:** We consider reinforcement learning (RL) methods in offline nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal policy based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimisation in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, and a real data example from the 2018 Intern Health Study. A Python implementation of the proposed procedure is available at https://github.com/limengbinggz/CUSUM-RL

</p>
</details>

<details><summary><b>Fail-Safe Generative Adversarial Imitation Learning</b>
<a href="https://arxiv.org/abs/2203.01696">arxiv:2203.01696</a>
&#x1F4C8; 3 <br>
<p>Philipp Geiger, Christoph-Nikolas Straehle</p></summary>
<p>

**Abstract:** For flexible yet safe imitation learning (IL), we propose a modular approach that uses a generative imitator policy with a safety layer, has an overall explicit density/gradient, can therefore be end-to-end trained using generative adversarial IL (GAIL), and comes with theoretical worst-case safety/robustness guarantees. The safety layer's exact density comes from using a countable non-injective gluing of piecewise differentiable injections and the change-of-variables formula. The safe set (into which the safety layer maps) is inferred by sampling actions and their potential future fail-safe fallback continuations, together with Lipschitz continuity and convexity arguments. We also provide theoretical bounds showing the advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (quadratic error). In an experiment on challenging real-world driver interaction data, we empirically demonstrate tractability, safety and imitation performance of our approach.

</p>
</details>

<details><summary><b>Joint Probability Estimation Using Tensor Decomposition and Dictionaries</b>
<a href="https://arxiv.org/abs/2203.01667">arxiv:2203.01667</a>
&#x1F4C8; 3 <br>
<p>Shaan ul Haque, Ajit Rajwade, Karthik S. Gurumoorthy</p></summary>
<p>

**Abstract:** In this work, we study non-parametric estimation of joint probabilities of a given set of discrete and continuous random variables from their (empirically estimated) 2D marginals, under the assumption that the joint probability could be decomposed and approximated by a mixture of product densities/mass functions. The problem of estimating the joint probability density function (PDF) using semi-parametric techniques such as Gaussian Mixture Models (GMMs) is widely studied. However such techniques yield poor results when the underlying densities are mixtures of various other families of distributions such as Laplacian or generalized Gaussian, uniform, Cauchy, etc. Further, GMMs are not the best choice to estimate joint distributions which are hybrid in nature, i.e., some random variables are discrete while others are continuous. We present a novel approach for estimating the PDF using ideas from dictionary representations in signal processing coupled with low rank tensor decompositions. To the best our knowledge, this is the first work on estimating joint PDFs employing dictionaries alongside tensor decompositions. We create a dictionary of various families of distributions by inspecting the data, and use it to approximate each decomposed factor of the product in the mixture. Our approach can naturally handle hybrid $N$-dimensional distributions. We test our approach on a variety of synthetic and real datasets to demonstrate its effectiveness in terms of better classification rates and lower error rates, when compared to state of the art estimators.

</p>
</details>

<details><summary><b>$β$-DARTS: Beta-Decay Regularization for Differentiable Architecture Search</b>
<a href="https://arxiv.org/abs/2203.01665">arxiv:2203.01665</a>
&#x1F4C8; 3 <br>
<p>Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, Wanli Ouyang</p></summary>
<p>

**Abstract:** Neural Architecture Search~(NAS) has attracted increasingly more attention in recent years because of its capability to design deep neural networks automatically. Among them, differential NAS approaches such as DARTS, have gained popularity for the search efficiency. However, they suffer from two main issues, the weak robustness to the performance collapse and the poor generalization ability of the searched architectures. To solve these two problems, a simple-but-efficient regularization method, termed as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process. Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. Furthermore, we provide in-depth theoretical analysis on how it works and why it works. Experimental results on NAS-Bench-201 show that our proposed method can help to stabilize the searching process and makes the searched network more transferable across different datasets. In addition, our search scheme shows an outstanding property of being less dependent on training time and data. Comprehensive experiments on a variety of search spaces and datasets validate the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models</b>
<a href="https://arxiv.org/abs/2203.01584">arxiv:2203.01584</a>
&#x1F4C8; 3 <br>
<p>Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, Kui Ren</p></summary>
<p>

**Abstract:** Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related features, e.g., gender and ethnicity. The key advantage is that FAAP does not modify deployed models in terms of parameters and structures. To achieve this, we design a discriminator to distinguish fairness-related attributes based on latent representations from deployed models. Meanwhile, a perturbation generator is trained against the discriminator, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model parameters), which shows the transferability of FAAP, foreseeing the potential of black-box adaptation.

</p>
</details>

<details><summary><b>Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings</b>
<a href="https://arxiv.org/abs/2203.01570">arxiv:2203.01570</a>
&#x1F4C8; 3 <br>
<p>Dongsheng Wang, Dandan Guo, He Zhao, Huangjie Zheng, Korawat Tanwisuth, Bo Chen, Mingyuan Zhou</p></summary>
<p>

**Abstract:** A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions. It is focused on capturing the word co-occurrences in a document and hence often suffers from poor performance in analyzing short documents. In addition, its parameter estimation often relies on approximate posterior inference that is either not scalable or suffers from large approximation error. This paper introduces a new topic-modeling framework where each document is viewed as a set of word embedding vectors and each topic is modeled as an embedding vector in the same embedding space. Embedding the words and topics in the same vector space, we define a method to measure the semantic difference between the embedding vectors of the words of a document and these of the topics, and optimize the topic embeddings to minimize the expected difference over all documents. Experiments on text analysis demonstrate that the proposed method, which is amenable to mini-batch stochastic gradient descent based optimization and hence scalable to big corpora, provides competitive performance in discovering more coherent and diverse topics and extracting better document representations.

</p>
</details>

<details><summary><b>Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work</b>
<a href="https://arxiv.org/abs/2203.01536">arxiv:2203.01536</a>
&#x1F4C8; 3 <br>
<p>Khawar Islam</p></summary>
<p>

**Abstract:** Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey

</p>
</details>

<details><summary><b>An Open Challenge for Inductive Link Prediction on Knowledge Graphs</b>
<a href="https://arxiv.org/abs/2203.01520">arxiv:2203.01520</a>
&#x1F4C8; 3 <br>
<p>Mikhail Galkin, Max Berrendorf, Charles Tapley Hoyt</p></summary>
<p>

**Abstract:** An emerging trend in representation learning over knowledge graphs (KGs) moves beyond transductive link prediction tasks over a fixed set of known entities in favor of inductive tasks that imply training on one graph and performing inference over a new graph with unseen entities. In inductive setups, node features are often not available and training shallow entity embedding matrices is meaningless as they cannot be used at inference time with unseen entities. Despite the growing interest, there are not enough benchmarks for evaluating inductive representation learning methods. In this work, we introduce ILPC 2022, a novel open challenge on KG inductive link prediction. To this end, we constructed two new datasets based on Wikidata with various sizes of training and inference graphs that are much larger than existing inductive benchmarks. We also provide two strong baselines leveraging recently proposed inductive methods. We hope this challenge helps to streamline community efforts in the inductive graph representation learning area. ILPC 2022 follows best practices on evaluation fairness and reproducibility, and is available at https://github.com/pykeen/ilpc2022.

</p>
</details>

<details><summary><b>Bilateral Deep Reinforcement Learning Approach for Better-than-human Car Following Model</b>
<a href="https://arxiv.org/abs/2203.04749">arxiv:2203.04749</a>
&#x1F4C8; 2 <br>
<p>Tianyu Shi, Yifei Ai, Omar ElSamadisy, Baher Abdulhai</p></summary>
<p>

**Abstract:** In the coming years and decades, autonomous vehicles (AVs) will become increasingly prevalent, offering new opportunities for safer and more convenient travel and potentially smarter traffic control methods exploiting automation and connectivity. Car following is a prime function in autonomous driving. Car following based on reinforcement learning has received attention in recent years with the goal of learning and achieving performance levels comparable to humans. However, most existing RL methods model car following as a unilateral problem, sensing only the vehicle ahead. Recent literature, however, Wang and Horn [16] has shown that bilateral car following that considers the vehicle ahead and the vehicle behind exhibits better system stability. In this paper we hypothesize that this bilateral car following can be learned using RL, while learning other goals such as efficiency maximisation, jerk minimization, and safety rewards leading to a learned model that outperforms human driving.
  We propose and introduce a Deep Reinforcement Learning (DRL) framework for car following control by integrating bilateral information into both state and reward function based on the bilateral control model (BCM) for car following control. Furthermore, we use a decentralized multi-agent reinforcement learning framework to generate the corresponding control action for each agent. Our simulation results demonstrate that our learned policy is better than the human driving policy in terms of (a) inter-vehicle headways, (b) average speed, (c) jerk, (d) Time to Collision (TTC) and (e) string stability.

</p>
</details>

<details><summary><b>Attention-based Region of Interest (ROI) Detection for Speech Emotion Recognition</b>
<a href="https://arxiv.org/abs/2203.03428">arxiv:2203.03428</a>
&#x1F4C8; 2 <br>
<p>Jay Desai, Houwei Cao, Ravi Shah</p></summary>
<p>

**Abstract:** Automatic emotion recognition for real-life appli-cations is a challenging task. Human emotion expressions aresubtle, and can be conveyed by a combination of several emo-tions. In most existing emotion recognition studies, each audioutterance/video clip is labelled/classified in its entirety. However,utterance/clip-level labelling and classification can be too coarseto capture the subtle intra-utterance/clip temporal dynamics. Forexample, an utterance/video clip usually contains only a fewemotion-salient regions and many emotionless regions. In thisstudy, we propose to use attention mechanism in deep recurrentneural networks to detection the Regions-of-Interest (ROI) thatare more emotionally salient in human emotional speech/video,and further estimate the temporal emotion dynamics by aggre-gating those emotionally salient regions-of-interest. We comparethe ROI from audio and video and analyse them. We comparethe performance of the proposed attention networks with thestate-of-the-art LSTM models on multi-class classification task ofrecognizing six basic human emotions, and the proposed attentionmodels exhibit significantly better performance. Furthermore, theattention weight distribution can be used to interpret how anutterance can be expressed as a mixture of possible emotions.

</p>
</details>

<details><summary><b>3D endoscopic depth estimation using 3D surface-aware constraints</b>
<a href="https://arxiv.org/abs/2203.02131">arxiv:2203.02131</a>
&#x1F4C8; 2 <br>
<p>Shang Zhao, Ce Wang, Qiyuan Wang, Yanzhe Liu, S Kevin Zhou</p></summary>
<p>

**Abstract:** Robotic-assisted surgery allows surgeons to conduct precise surgical operations with stereo vision and flexible motor control. However, the lack of 3D spatial perception limits situational awareness during procedures and hinders mastering surgical skills in the narrow abdominal space. Depth estimation, as a representative perception task, is typically defined as an image reconstruction problem. In this work, we show that depth estimation can be reformed from a 3D surface perspective. We propose a loss function for depth estimation that integrates the surface-aware constraints, leading to a faster and better convergence with the valid information from spatial information. In addition, camera parameters are incorporated into the training pipeline to increase the control and transparency of the depth estimation. We also integrate a specularity removal module to recover more buried image information. Quantitative experimental results on endoscopic datasets and user studies with medical professionals demonstrate the effectiveness of our method.

</p>
</details>

<details><summary><b>Adversarial Patterns: Building Robust Android Malware Classifiers</b>
<a href="https://arxiv.org/abs/2203.02121">arxiv:2203.02121</a>
&#x1F4C8; 2 <br>
<p>Dipkamal Bhusal, Nidhi Rastogi</p></summary>
<p>

**Abstract:** Deep learning-based classifiers have substantially improved recognition of malware samples. However, these classifiers can be vulnerable to adversarial input perturbations. Any vulnerability in malware classifiers poses significant threats to the platforms they defend. Therefore, to create stronger defense models against malware, we must understand the patterns in input perturbations caused by an adversary. This survey paper presents a comprehensive study on adversarial machine learning for android malware classifiers. We first present an extensive background in building a machine learning classifier for android malware, covering both image-based and text-based feature extraction approaches. Then, we examine the pattern and advancements in the state-of-the-art research in evasion attacks and defenses. Finally, we present guidelines for designing robust malware classifiers and enlist research directions for the future.

</p>
</details>

<details><summary><b>Towards Benchmarking and Evaluating Deepfake Detection</b>
<a href="https://arxiv.org/abs/2203.02115">arxiv:2203.02115</a>
&#x1F4C8; 2 <br>
<p>Chenhao Lin, Jingyi Deng, Pengbin Hu, Chao Shen, Qian Wang, Qi Li</p></summary>
<p>

**Abstract:** Deepfake detection automatically recognizes the manipulated medias through the analysis of the difference between manipulated and non-altered videos. It is natural to ask which are the top performers among the existing deepfake detection approaches to identify promising research directions and provide practical guidance. Unfortunately, it's difficult to conduct a sound benchmarking comparison of existing detection approaches using the results in the literature because evaluation conditions are inconsistent across studies. Our objective is to establish a comprehensive and consistent benchmark, to develop a repeatable evaluation procedure, and to measure the performance of a range of detection approaches so that the results can be compared soundly. A challenging dataset consisting of the manipulated samples generated by more than 13 different methods has been collected, and 11 popular detection approaches (9 algorithms) from the existing literature have been implemented and evaluated with 6 fair-minded and practical evaluation metrics. Finally, 92 models have been trained and 644 experiments have been performed for the evaluation. The results along with the shared data and evaluation methodology constitute a benchmark for comparing deepfake detection approaches and measuring progress.

</p>
</details>

<details><summary><b>Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations</b>
<a href="https://arxiv.org/abs/2203.02107">arxiv:2203.02107</a>
&#x1F4C8; 2 <br>
<p>Hao Shen, Weikang Wan, He Wang</p></summary>
<p>

**Abstract:** Generalizable object manipulation skills are critical for intelligent and multi-functional robots to work in real-world complex scenes. Despite the recent progress in reinforcement learning, it is still very challenging to learn a generalizable manipulation policy that can handle a category of geometrically diverse articulated objects. In this work, we tackle this category-level object manipulation policy learning problem via imitation learning in a task-agnostic manner, where we assume no handcrafted dense rewards but only a terminal reward. Given this novel and challenging generalizable policy learning problem, we identify several key issues that can fail the previous imitation learning algorithms and hinder the generalization to unseen instances. We then propose several general but critical techniques, including generative adversarial self-imitation learning from demonstrations, progressive growing of discriminator, and instance-balancing for expert buffer, that accurately pinpoints and tackles these issues and can benefit category-level manipulation policy learning regardless of the tasks. Our experiments on ManiSkill benchmarks demonstrate a remarkable improvement on all tasks and our ablation studies further validate the contribution of each proposed technique.

</p>
</details>

<details><summary><b>Dynamic Backdoors with Global Average Pooling</b>
<a href="https://arxiv.org/abs/2203.02079">arxiv:2203.02079</a>
&#x1F4C8; 2 <br>
<p>Stefanos Koffas, Stjepan Picek, Mauro Conti</p></summary>
<p>

**Abstract:** Outsourced training and machine learning as a service have resulted in novel attack vectors like backdoor attacks. Such attacks embed a secret functionality in a neural network activated when the trigger is added to its input. In most works in the literature, the trigger is static, both in terms of location and pattern. The effectiveness of various detection mechanisms depends on this property. It was recently shown that countermeasures in image classification, like Neural Cleanse and ABS, could be bypassed with dynamic triggers that are effective regardless of their pattern and location. Still, such backdoors are demanding as they require a large percentage of poisoned training data. In this work, we are the first to show that dynamic backdoor attacks could happen due to a global average pooling layer without increasing the percentage of the poisoned training data. Nevertheless, our experiments in sound classification, text sentiment analysis, and image classification show this to be very difficult in practice.

</p>
</details>

<details><summary><b>Learning Time-optimized Path Tracking with or without Sensory Feedback</b>
<a href="https://arxiv.org/abs/2203.01968">arxiv:2203.01968</a>
&#x1F4C8; 2 <br>
<p>Jonas C. Kiemel, Torsten Kröger</p></summary>
<p>

**Abstract:** In this paper, we present a learning-based approach that allows a robot to quickly follow a reference path defined in joint space without exceeding limits on the position, velocity, acceleration and jerk of each robot joint. Contrary to offline methods for time-optimal path parameterization, the reference path can be changed during motion execution. In addition, our approach can utilize sensory feedback, for instance, to follow a reference path with a bipedal robot without losing balance. With our method, the robot is controlled by a neural network that is trained via reinforcement learning using data generated by a physics simulator. From a mathematical perspective, the problem of tracking a reference path in a time-optimized manner is formalized as a Markov decision process. Each state includes a fixed number of waypoints specifying the next part of the reference path. The action space is designed in such a way that all resulting motions comply with the specified kinematic joint limits. The reward function finally reflects the trade-off between the execution time, the deviation from the desired reference path and optional additional objectives like balancing. We evaluate our approach with and without additional objectives and show that time-optimized path tracking can be successfully learned for both industrial and humanoid robots. In addition, we demonstrate that networks trained in simulation can be successfully transferred to a real Kuka robot.

</p>
</details>

<details><summary><b>Bayesian Spillover Graphs for Dynamic Networks</b>
<a href="https://arxiv.org/abs/2203.01912">arxiv:2203.01912</a>
&#x1F4C8; 2 <br>
<p>Grace Deng, David S. Matteson</p></summary>
<p>

**Abstract:** We present Bayesian Spillover Graphs (BSG), a novel method for learning temporal relationships, identifying critical nodes, and quantifying uncertainty for multi-horizon spillover effects in a dynamic system. BSG leverages both an interpretable framework via forecast error variance decompositions (FEVD) and comprehensive uncertainty quantification via Bayesian time series models to contextualize temporal relationships in terms of systemic risk and prediction variability. Forecast horizon hyperparameter $h$ allows for learning both short-term and equilibrium state network behaviors. Experiments for identifying source and sink nodes under various graph and error specifications show significant performance gains against state-of-the-art Bayesian Networks and deep-learning baselines. Applications to real-world systems also showcase BSG as an exploratory analysis tool for uncovering indirect spillovers and quantifying risk.

</p>
</details>

<details><summary><b>Reconstruction of univariate functions from directional persistence diagrams</b>
<a href="https://arxiv.org/abs/2203.01894">arxiv:2203.01894</a>
&#x1F4C8; 2 <br>
<p>Aina Ferrà, Carles Casacuberta, Oriol Pujol</p></summary>
<p>

**Abstract:** We describe a method for approximating a single-variable function $f$ using persistence diagrams of sublevel sets of $f$ from height functions in different directions. We provide algorithms for the piecewise linear case and for the smooth case. Three directions suffice to locate all local maxima and minima of a piecewise linear continuous function from its collection of directional persistence diagrams, while five directions are needed in the case of smooth functions with non-degenerate critical points.
  Our approximation of functions by means of persistence diagrams is motivated by a study of importance attribution in machine learning, where one seeks to reduce the number of critical points of signal functions without a significant loss of information for a neural network classifier.

</p>
</details>

<details><summary><b>Topological data analysis of truncated contagion maps</b>
<a href="https://arxiv.org/abs/2203.01720">arxiv:2203.01720</a>
&#x1F4C8; 2 <br>
<p>Florian Klimm</p></summary>
<p>

**Abstract:** The investigation of dynamical processes on networks has been one focus for the study of contagion processes. It has been demonstrated that contagions can be used to obtain information about the embedding of nodes in a Euclidean space. Specifically, one can use the activation times of threshold contagions to construct contagion maps as a manifold-learning approach. One drawback of contagion maps is their high computational cost. Here, we demonstrate that a truncation of the threshold contagions may considerably speed up the construction of contagion maps. Finally, we show that contagion maps may be used to find an insightful low-dimensional embedding for single-cell RNA-sequencing data in the form of cell-similarity networks and so reveal biological manifolds. Overall, our work makes the use of contagion maps as manifold-learning approaches on empirical network data more viable.

</p>
</details>

<details><summary><b>Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation</b>
<a href="https://arxiv.org/abs/2203.01677">arxiv:2203.01677</a>
&#x1F4C8; 2 <br>
<p>KiYoon Yoo, Jangho Kim, Jiho Jang, Nojun Kwak</p></summary>
<p>

**Abstract:** Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest AUC on 29 out of 30 dataset-attack-model combinations. Source code is available in https://github.com/anoymous92874838/text-adv-detection.

</p>
</details>

<details><summary><b>Improving X-ray Diagnostics through Eye-Tracking and XR</b>
<a href="https://arxiv.org/abs/2203.01643">arxiv:2203.01643</a>
&#x1F4C8; 2 <br>
<p>Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, João Madeiras Pereira, Joaquim Jorge</p></summary>
<p>

**Abstract:** There is a growing need to assist radiologists in performing X-ray readings and diagnoses fast, comfortably, and effectively. As radiologists strive to maximize productivity, it is essential to consider the impact of reading rooms in interpreting complex examinations and ensure that higher volume and reporting speeds do not compromise patient outcomes. Virtual Reality (VR) is a disruptive technology for clinical practice in assessing X-ray images. We argue that conjugating eye-tracking with VR devices and Machine Learning may overcome obstacles posed by inadequate ergonomic postures and poor room conditions that often cause erroneous diagnostics when professionals examine digital images.

</p>
</details>

<details><summary><b>Kernel Density Estimation by Genetic Algorithm</b>
<a href="https://arxiv.org/abs/2203.01535">arxiv:2203.01535</a>
&#x1F4C8; 2 <br>
<p>Kiheiji Nishida</p></summary>
<p>

**Abstract:** This study proposes a data condensation method for multivariate kernel density estimation by genetic algorithm. First, our proposed algorithm generates multiple subsamples of a given size with replacement from the original sample. The subsamples and their constituting data points are regarded as $\it{chromosome}$ and $\it{gene}$, respectively, in the terminology of genetic algorithm. Second, each pair of subsamples breeds two new subsamples, where each data point faces either $\it{crossover}$, $\it{mutation}$, or $\it{reproduction}$ with a certain probability. The dominant subsamples in terms of fitness values are inherited by the next generation. This process is repeated generation by generation and brings the sparse representation of kernel density estimator in its completion. We confirmed from simulation studies that the resulting estimator can perform better than other well-known density estimators.

</p>
</details>

<details><summary><b>Exploration of Various Deep Learning Models for Increased Accuracy in Automatic Polyp Detection</b>
<a href="https://arxiv.org/abs/2203.04093">arxiv:2203.04093</a>
&#x1F4C8; 1 <br>
<p>Ariel E. Isidro, Arnel C. Fajardo, Alexander A. Hernandez</p></summary>
<p>

**Abstract:** This paper is created to explore deep learning models and algorithms that results in highest accuracy in detecting polyp on colonoscopy images. Previous studies implemented deep learning using convolution neural network (CNN) algorithm in detecting polyp and non-polyp. Other studies used dropout, and data augmentation algorithm but mostly not checking the overfitting, thus, include more than four-layer modelss. Rulei Yu et.al from the Institute of Software, Chinese Academy of Sciences said that transfer learning is better talking about performance or improving the previous used algorithm. Most especially in applying the transfer learning in feature extraction. Series of experiments were conducted with only a minimum of 4 CNN layers applying previous used models and identified the model that produce the highest percentage accuracy of 98% among the other models that apply transfer learning. Further studies could use different optimizer to a different CNN modelsto increase accuracy.

</p>
</details>

<details><summary><b>Distributionally Robust Bayesian Optimization with $φ$-divergences</b>
<a href="https://arxiv.org/abs/2203.02128">arxiv:2203.02128</a>
&#x1F4C8; 1 <br>
<p>Hisham Husain, Vu Nguyen, Anton van den Hengel</p></summary>
<p>

**Abstract:** The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $φ$-divergences, which subsumes many popular choices, such as the $χ^2$-divergence, Total Variation, and the extant Kullback-Leibler (KL) divergence. We show that the DRO-BO problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. We then show experimentally that our method surpasses existing methods, attesting to the theoretical results

</p>
</details>

<details><summary><b>Abuse and Fraud Detection in Streaming Services Using Heuristic-Aware Machine Learning</b>
<a href="https://arxiv.org/abs/2203.02124">arxiv:2203.02124</a>
&#x1F4C8; 1 <br>
<p>Soheil Esmaeilzadeh, Negin Salajegheh, Amir Ziai, Jeff Boote</p></summary>
<p>

**Abstract:** This work presents a fraud and abuse detection framework for streaming services by modeling user streaming behavior. The goal is to discover anomalous and suspicious incidents and scale the investigation efforts by creating models that characterize the user behavior. We study the use of semi-supervised as well as supervised approaches for anomaly detection. In the semi-supervised approach, by leveraging only a set of authenticated anomaly-free data samples, we show the use of one-class classification algorithms as well as autoencoder deep neural networks for anomaly detection. In the supervised anomaly detection task, we present a so-called heuristic-aware data labeling strategy for creating labeled data samples. We carry out binary classification as well as multi-class multi-label classification tasks for not only detecting the anomalous samples but also identifying the underlying anomaly behavior(s) associated with each one. Finally, using a systematic feature importance study we provide insights into the underlying set of features that characterize different streaming fraud categories. To the best of our knowledge, this is the first paper to use machine learning methods for fraud and abuse detection in real-world scale streaming services.

</p>
</details>

<details><summary><b>In the Service of Online Order: Tackling Cyber-Bullying with Machine Learning and Affect Analysis</b>
<a href="https://arxiv.org/abs/2203.02116">arxiv:2203.02116</a>
&#x1F4C8; 1 <br>
<p>Michal Ptaszynski, Pawel Dybala, Tatsuaki Matsuba, Fumito Masui, Rafal Rzepka, Kenji Araki, Yoshio Momouchi</p></summary>
<p>

**Abstract:** One of the burning problems lately in Japan has been cyber-bullying, or slandering and bullying people online. The problem has been especially noticed on unofficial Web sites of Japanese schools. Volunteers consisting of school personnel and PTA (Parent-Teacher Association) members have started Online Patrol to spot malicious contents within Web forums and blogs. In practise, Online Patrol assumes reading through the whole Web contents, which is a task difficult to perform manually. With this paper we introduce a research intended to help PTA members perform Online Patrol more efficiently. We aim to develop a set of tools that can automatically detect malicious entries and report them to PTA members. First, we collected cyber-bullying data from unofficial school Web sites. Then we performed analysis of this data in two ways. Firstly, we analysed the entries with a multifaceted affect analysis system in order to find distinctive features for cyber-bullying and apply them to a machine learning classifier. Secondly, we applied a SVM based machine learning method to train a classifier for detection of cyber-bullying. The system was able to classify cyber-bullying entries with 88.2% of balanced F-score.

</p>
</details>

<details><summary><b>Teaching Robots to Span the Space of Functional Expressive Motion</b>
<a href="https://arxiv.org/abs/2203.02091">arxiv:2203.02091</a>
&#x1F4C8; 1 <br>
<p>Arjun Sripathy, Andreea Bobu, Zhongyu Li, Koushil Sreenath, Daniel S. Brown, Anca D. Dragan</p></summary>
<p>

**Abstract:** Our goal is to enable robots to perform functional tasks in emotive ways, be it in response to their users' emotional states, or expressive of their confidence levels. Prior work has proposed learning independent cost functions from user feedback for each target emotion, so that the robot may optimize it alongside task and environment specific objectives for any situation it encounters. However, this approach is inefficient when modeling multiple emotions and unable to generalize to new ones. In this work, we leverage the fact that emotions are not independent of each other: they are related through a latent space of Valence-Arousal-Dominance (VAD). Our key idea is to learn a model for how trajectories map onto VAD with user labels. Considering the distance between a trajectory's mapping and a target VAD allows this single model to represent cost functions for all emotions. As a result 1) all user feedback can contribute to learning about every emotion; 2) the robot can generate trajectories for any emotion in the space instead of only a few predefined ones; and 3) the robot can respond emotively to user-generated natural language by mapping it to a target VAD. We introduce a method that interactively learns to map trajectories to this latent space and test it in simulation and in a user study. In experiments, we use a simple vacuum robot as well as the Cassie biped.

</p>
</details>

<details><summary><b>Bayesian community detection for networks with covariates</b>
<a href="https://arxiv.org/abs/2203.02090">arxiv:2203.02090</a>
&#x1F4C8; 1 <br>
<p>Luyi Shen, Arash Amini, Nathaniel Josephs, Lizhen Lin</p></summary>
<p>

**Abstract:** The increasing prevalence of network data in a vast variety of fields and the need to extract useful information out of them have spurred fast developments in related models and algorithms. Among the various learning tasks with network data, community detection, the discovery of node clusters or "communities," has arguably received the most attention in the scientific community. In many real-world applications, the network data often come with additional information in the form of node or edge covariates that should ideally be leveraged for inference. In this paper, we add to a limited literature on community detection for networks with covariates by proposing a Bayesian stochastic block model with a covariate-dependent random partition prior. Under our prior, the covariates are explicitly expressed in specifying the prior distribution on the cluster membership. Our model has the flexibility of modeling uncertainties of all the parameter estimates including the community membership. Importantly, and unlike the majority of existing methods, our model has the ability to learn the number of the communities via posterior inference without having to assume it to be known. Our model can be applied to community detection in both dense and sparse networks, with both categorical and continuous covariates, and our MCMC algorithm is very efficient with good mixing properties. We demonstrate the superior performance of our model over existing models in a comprehensive simulation study and an application to two real datasets.

</p>
</details>

<details><summary><b>Interpretable Latent Variables in Deep State Space Models</b>
<a href="https://arxiv.org/abs/2203.02057">arxiv:2203.02057</a>
&#x1F4C8; 1 <br>
<p>Haoxuan Wu, David S. Matteson, Martin T. Wells</p></summary>
<p>

**Abstract:** We introduce a new version of deep state-space models (DSSMs) that combines a recurrent neural network with a state-space framework to forecast time series data. The model estimates the observed series as functions of latent variables that evolve non-linearly through time. Due to the complexity and non-linearity inherent in DSSMs, previous works on DSSMs typically produced latent variables that are very difficult to interpret. Our paper focus on producing interpretable latent parameters with two key modifications. First, we simplify the predictive decoder by restricting the response variables to be a linear transformation of the latent variables plus some noise. Second, we utilize shrinkage priors on the latent variables to reduce redundancy and improve robustness. These changes make the latent variables much easier to understand and allow us to interpret the resulting latent variables as random effects in a linear mixed model. We show through two public benchmark datasets the resulting model improves forecasting performances.

</p>
</details>

<details><summary><b>Symmetry Structured Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2203.02056">arxiv:2203.02056</a>
&#x1F4C8; 1 <br>
<p>Kehelwala Dewage Gayan Maduranga, Vasily Zadorozhnyy, Qiang Ye</p></summary>
<p>

**Abstract:** We consider Convolutional Neural Networks (CNNs) with 2D structured features that are symmetric in the spatial dimensions. Such networks arise in modeling pairwise relationships for a sequential recommendation problem, as well as secondary structure inference problems of RNA and protein sequences. We develop a CNN architecture that generates and preserves the symmetry structure in the network's convolutional layers. We present parameterizations for the convolutional kernels that produce update rules to maintain symmetry throughout the training. We apply this architecture to the sequential recommendation problem, the RNA secondary structure inference problem, and the protein contact map prediction problem, showing that the symmetric structured networks produce improved results using fewer numbers of machine parameters.

</p>
</details>

<details><summary><b>Automatic Detection and Segmentation of Postoperative Cerebellar Damage Based on Normalization</b>
<a href="https://arxiv.org/abs/2203.02042">arxiv:2203.02042</a>
&#x1F4C8; 1 <br>
<p>Silu Zhang, Stuart McAfee, Zoltan Patay, Matthew Scoggins</p></summary>
<p>

**Abstract:** Surgical resection is a common procedure in the treatment of pediatric posterior fossa tumors. However, surgical damage is often unavoidable and its association with postoperative complications is not well understood. A reliable localization and measure of cerebellar damage is fundamental to study the relationship between the damaged cerebellar regions and postoperative neurological outcomes. Existing cerebellum normalization methods are not reliable on postoperative scans, therefore current approaches to measure surgical damage rely on manual labelling. In this work, we develop a robust algorithm to automatically detect and measure cerebellum damage due to surgery using postoperative 3D T1 magnetic resonance imaging. In our proposed approach, normal brain tissues are first segmented using a Bayesian algorithm customized for postoperative scans. Next, the cerebellum is isolated by nonlinear registration of a whole brain template to the native space. The isolated cerebellum is then normalized into the spatially unbiased atlas (SUIT) space using anatomical information derived from the previous step. Finally, the damage is detected in the atlas space by comparing the normalized cerebellum and the SUIT template. We evaluated our damage detection tool on postoperative scans of 153 patients diagnosed with medulloblastoma based on inspection by human expects. We also designed a simulation to test the proposed approach without human intervention. Our results show that the proposed approach has superior performance on various scenarios.

</p>
</details>

<details><summary><b>Nonlinear predictive models computation in ADPCM schemes</b>
<a href="https://arxiv.org/abs/2203.02020">arxiv:2203.02020</a>
&#x1F4C8; 1 <br>
<p>Marcos Faundez-Zanuy</p></summary>
<p>

**Abstract:** Recently several papers have been published on nonlinear prediction applied to speech coding. At ICASSP98 we presented a system based on an ADPCM scheme with a nonlinear predictor based on a neural net. The most critical parameter was the training procedure in order to achieve good generalization capability and robustness against mismatch between training and testing conditions. In this paper, we propose several new approaches that improve the performance of the original system in up to 1.2dB of SEGSNR (using bayesian regularization). The variance of the SEGSNR between frames is also minimized, so the new scheme produces a more stable quality of the output.

</p>
</details>

<details><summary><b>On consistency of constrained spectral clustering under representation-aware stochastic block model</b>
<a href="https://arxiv.org/abs/2203.02005">arxiv:2203.02005</a>
&#x1F4C8; 1 <br>
<p>Shubham Gupta, Ambedkar Dukkipati</p></summary>
<p>

**Abstract:** Spectral clustering is widely used in practice due to its flexibility, computational efficiency, and well-understood theoretical performance guarantees. Recently, spectral clustering has been studied to find balanced clusters under population-level constraints. These constraints are specified by additional information available in the form of auxiliary categorical node attributes. In this paper, we consider a scenario where these attributes may not be observable, but manifest as latent features of an auxiliary graph. Motivated by this, we study constrained spectral clustering with the aim of finding balanced clusters in a given \textit{similarity graph} $\mathcal{G}$, such that each individual is adequately represented with respect to an auxiliary graph $\mathcal{R}$ (we refer to this as representation graph). We propose an individual-level balancing constraint that formalizes this idea. Our work leads to an interesting stochastic block model that not only plants the given partitions in $\mathcal{G}$ but also plants the auxiliary information encoded in the representation graph $\mathcal{R}$. We develop unnormalized and normalized variants of spectral clustering in this setting. These algorithms use $\mathcal{R}$ to find clusters in $\mathcal{G}$ that approximately satisfy the proposed constraint. We also establish the first statistical consistency result for constrained spectral clustering under individual-level constraints for graphs sampled from the above-mentioned variant of the stochastic block model. Our experimental results corroborate our theoretical findings.

</p>
</details>

<details><summary><b>Unfolding-Aided Bootstrapped Phase Retrieval in Optical Imaging</b>
<a href="https://arxiv.org/abs/2203.01695">arxiv:2203.01695</a>
&#x1F4C8; 1 <br>
<p>Samuel Pinilla, Kumar Vijay Mishra, Igor Shevkunov, Mojtaba Soltanalian, Vladimir Katkovnik, Karen Egiazarian</p></summary>
<p>

**Abstract:** Phase retrieval in optical imaging refers to the recovery of a complex signal from phaseless data acquired in the form of its diffraction patterns. These patterns are acquired through a system with a coherent light source that employs a diffractive optical element (DOE) to modulate the scene resulting in coded diffraction patterns at the sensor. Recently, the hybrid approach of model-driven network or deep unfolding has emerged as an effective alternative because it allows for bounding the complexity of phase retrieval algorithms while also retaining their efficacy. Additionally, such hybrid approaches have shown promise in improving the design of DOEs that follow theoretical uniqueness conditions. There are opportunities to exploit novel experimental setups and resolve even more complex DOE phase retrieval applications. This paper presents an overview of algorithms and applications of deep unfolding for bootstrapped - regardless of near, middle, and far zones - phase retrieval.

</p>
</details>

<details><summary><b>Risk-aware Stochastic Shortest Path</b>
<a href="https://arxiv.org/abs/2203.01640">arxiv:2203.01640</a>
&#x1F4C8; 1 <br>
<p>Tobias Meggendorfer</p></summary>
<p>

**Abstract:** We treat the problem of risk-aware control for stochastic shortest path (SSP) on Markov decision processes (MDP). Typically, expectation is considered for SSP, which however is oblivious to the incurred risk. We present an alternative view, instead optimizing conditional value-at-risk (CVaR), an established risk measure. We treat both Markov chains as well as MDP and introduce, through novel insights, two algorithms, based on linear programming and value iteration, respectively. Both algorithms offer precise and provably correct solutions. Evaluation of our prototype implementation shows that risk-aware control is feasible on several moderately sized models.

</p>
</details>

<details><summary><b>A shallow physics-informed neural network for solving partial differential equations on surfaces</b>
<a href="https://arxiv.org/abs/2203.01581">arxiv:2203.01581</a>
&#x1F4C8; 1 <br>
<p>Wei-Fan Hu, Yi-Jun Shih, Te-Sheng Lin, Ming-Chih Lai</p></summary>
<p>

**Abstract:** In this paper, we introduce a mesh-free physics-informed neural network for solving partial differential equations on surfaces. Based on the idea of embedding techniques, we write the underlying surface differential equations using conventional Cartesian differential operators. With the aid of level set function, the surface geometrical quantities, such as the normal and mean curvature of the surface, can be computed directly and used in our surface differential expressions. So instead of imposing the normal extension constraints used in literature, we take the whole Cartesian differential expressions into account in our loss function. Meanwhile, we adopt a completely shallow (one hidden layer) network so the present model is easy to implement and train. We perform a series of numerical experiments on both stationary and time-dependent partial differential equations on complicated surface geometries. The result shows that, with just a few hundred trainable parameters, our network model is able to achieve high predictive accuracy.

</p>
</details>

<details><summary><b>Automated Single-Label Patent Classification using Ensemble Classifiers</b>
<a href="https://arxiv.org/abs/2203.03552">arxiv:2203.03552</a>
&#x1F4C8; 0 <br>
<p>Eleni Kamateri, Vasileios Stamatis, Konstantinos Diamantaras, Michail Salampasis</p></summary>
<p>

**Abstract:** Many thousands of patent applications arrive at patent offices around the world every day. One important subtask when a patent application is submitted is to assign one or more classification codes from the complex and hierarchical patent classification schemes that will enable routing of the patent application to a patent examiner who is knowledgeable about the specific technical field. This task is typically undertaken by patent professionals, however due to the large number of applications and the potential complexity of an invention, they are usually overwhelmed. Therefore, there is a need for this code assignment manual task to be supported or even fully automated by classification systems that will classify patent applications, hopefully with an accuracy close to patent professionals. Like in many other text analysis problems, in the last years, this intellectually demanding task has been studied using word embeddings and deep learning techniques. In this paper we shortly review these research efforts and experiment with similar deep learning techniques using different feature representations on automatic patent classification in the level of sub-classes. On top of that, we present an innovative method of ensemble classifiers trained with different parts of the patent document. To the best of our knowledge, this is the first time that an ensemble method was proposed for the patent classification problem. Our first results are quite promising showing that an ensemble architecture of classifiers significantly outperforms current state-of-the-art techniques using the same classifiers as standalone solutions.

</p>
</details>

<details><summary><b>Detecting Offensive Language on Social Networks: An End-to-end Detection Method based on Graph Attention Networks</b>
<a href="https://arxiv.org/abs/2203.02123">arxiv:2203.02123</a>
&#x1F4C8; 0 <br>
<p>Zhenxiong Miao, Xingshu Chen, Haizhou Wang, Rui Tang, Zhou Yang, Wenyi Tang</p></summary>
<p>

**Abstract:** The pervasiveness of offensive language on the social network has caused adverse effects on society, such as abusive behavior online. It is urgent to detect offensive language and curb its spread. Existing research shows that methods with community structure features effectively improve the performance of offensive language detection. However, the existing models deal with community structure independently, which seriously affects the effectiveness of detection models. In this paper, we propose an end-to-end method based on community structure and text features for offensive language detection (CT-OLD). Specifically, the community structure features are directly captured by the graph attention network layer, and the text embeddings are taken from the last hidden layer of BERT. Attention mechanisms and position encoding are used to fuse these features. Meanwhile, we add user opinion to the community structure for representing user features. The user opinion is represented by user historical behavior information, which outperforms that represented by text information. Besides the above point, the distribution of users and tweets is unbalanced in the popular datasets, which limits the generalization ability of the model. To address this issue, we construct and release a dataset with reasonable user distribution. Our method outperforms baselines with the F1 score of 89.94%. The results show that the end-to-end model effectively learns the potential information of community structure and text, and user historical behavior information is more suitable for user opinion representation.

</p>
</details>

<details><summary><b>GraspARL: Dynamic Grasping via Adversarial Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2203.02119">arxiv:2203.02119</a>
&#x1F4C8; 0 <br>
<p>Tianhao Wu, Fangwei Zhong, Yiran Geng, Hongchen Wang, Yongjian Zhu, Yizhou Wang, Hao Dong</p></summary>
<p>

**Abstract:** Grasping moving objects, such as goods on a belt or living animals, is an important but challenging task in robotics. Conventional approaches rely on a set of manually defined object motion patterns for training, resulting in poor generalization to unseen object trajectories. In this work, we introduce an adversarial reinforcement learning framework for dynamic grasping, namely GraspARL. To be specific. we formulate the dynamic grasping problem as a 'move-and-grasp' game, where the robot is to pick up the object on the mover and the adversarial mover is to find a path to escape it. Hence, the two agents play a min-max game and are trained by reinforcement learning. In this way, the mover can auto-generate diverse moving trajectories while training. And the robot trained with the adversarial trajectories can generalize to various motion patterns. Empirical results on the simulator and real-world scenario demonstrate the effectiveness of each and good generalization of our method.

</p>
</details>

<details><summary><b>Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers</b>
<a href="https://arxiv.org/abs/2203.01859">arxiv:2203.01859</a>
&#x1F4C8; 0 <br>
<p>Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Marios Kountouris, David Gesbert</p></summary>
<p>

**Abstract:** Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training criterion produces predictive distributions that are able to concurrently counteract the detrimental effects of model misspecification and outliers.

</p>
</details>


{% endraw %}
Prev: [2022.03.02]({{ '/2022/03/02/2022.03.02.html' | relative_url }})  Next: [2022.03.04]({{ '/2022/03/04/2022.03.04.html' | relative_url }})