Prev: [2022.04.30]({{ '/2022/04/30/2022.04.30.html' | relative_url }})  Next: [2022.05.02]({{ '/2022/05/02/2022.05.02.html' | relative_url }})
{% raw %}
## Summary for 2022-05-01, created on 2022-05-05


<details><summary><b>Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions</b>
<a href="https://arxiv.org/abs/2205.00415">arxiv:2205.00415</a>
&#x1F4C8; 27 <br>
<p>Mihir Parmar, Swaroop Mishra, Mor Geva, Chitta Baral</p></summary>
<p>

**Abstract:** In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.

</p>
</details>

<details><summary><b>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</b>
<a href="https://arxiv.org/abs/2205.00445">arxiv:2205.00445</a>
&#x1F4C8; 16 <br>
<p>Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz</p></summary>
<p>

**Abstract:** Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.

</p>
</details>

<details><summary><b>A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness</b>
<a href="https://arxiv.org/abs/2205.00403">arxiv:2205.00403</a>
&#x1F4C8; 12 <br>
<p>Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper Snoek, Dustin Tran, Balaji Lakshminarayanan</p></summary>
<p>

**Abstract:** Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at https://github.com/google/uncertainty-baselines

</p>
</details>

<details><summary><b>Shape Change and Control of Pressure-based Soft Agents</b>
<a href="https://arxiv.org/abs/2205.00467">arxiv:2205.00467</a>
&#x1F4C8; 7 <br>
<p>Federico Pigozzi</p></summary>
<p>

**Abstract:** Biological agents possess bodies that are mostly of soft tissues. Researchers have resorted to soft bodies to investigate Artificial Life (ALife)-related questions; similarly, a new era of soft-bodied robots has just begun. Nevertheless, because of their infinite degrees of freedom, soft bodies pose unique challenges in terms of simulation, control, and optimization. Here we propose a novel soft-bodied agents formalism, namely Pressure-based Soft Agents (PSAs): they are bodies of gas enveloped by a chain of springs and masses, with pressure pushing on the masses from inside the body. Pressure endows the agents with structure, while springs and masses simulate softness and allow the agents to assume a large gamut of shapes. Actuation takes place by changing the length of springs or modulating global pressure. We optimize the controller of PSAs for a locomotion task on hilly terrain and an escape task from a cage; the latter is particularly suitable for soft-bodied agents, as it requires the agent to contort itself to squeeze through a small aperture. Our results suggest that PSAs are indeed effective at those tasks and that controlling pressure is fundamental for shape-changing. Looking forward, we envision PSAs to play a role in the modeling of soft-bodied agents, including soft robots and biological cells. Videos of evolved agents are available at https://pressuresoftagents.github.io.

</p>
</details>

<details><summary><b>A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction</b>
<a href="https://arxiv.org/abs/2205.01094">arxiv:2205.01094</a>
&#x1F4C8; 5 <br>
<p>Yong Xie, Dakuo Wang, Pin-Yu Chen, Jinjun Xiong, Sijia Liu, Sanmi Koyejo</p></summary>
<p>

**Abstract:** More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather real-time information and sentiment to predict stock price movements. Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability is underexplored. In this paper, we experiment with a variety of adversarial attack configurations to fool three stock prediction victim models. We address the task of adversarial generation by solving combinatorial optimization problems with semantics and budget constraints. Our results show that the proposed attack method can achieve consistent success rates and cause significant monetary loss in trading simulation by simply concatenating a perturbed but semantically similar tweet.

</p>
</details>

<details><summary><b>MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries</b>
<a href="https://arxiv.org/abs/2205.00613">arxiv:2205.00613</a>
&#x1F4C8; 4 <br>
<p>Tianyuan Zhang, Xuanyao Chen, Yue Wang, Yilun Wang, Hang Zhao</p></summary>
<p>

**Abstract:** Accurate and consistent 3D tracking from multiple cameras is a key component in a vision-based autonomous driving system. It involves modeling 3D dynamic objects in complex scenes across multiple cameras. This problem is inherently challenging due to depth estimation, visual occlusions, appearance ambiguity, etc. Moreover, objects are not consistently associated across time and cameras. To address that, we propose an end-to-end \textbf{MU}lti-camera \textbf{TR}acking framework called MUTR3D. In contrast to prior works, MUTR3D does not explicitly rely on the spatial and appearance similarity of objects. Instead, our method introduces \textit{3D track query} to model spatial and appearance coherent track for each object that appears in multiple cameras and multiple frames. We use camera transformations to link 3D trackers with their observations in 2D images. Each tracker is further refined according to the features that are obtained from camera images. MUTR3D uses a set-to-set loss to measure the difference between the predicted tracking results and the ground truths. Therefore, it does not require any post-processing such as non-maximum suppression and/or bounding box association. MUTR3D outperforms state-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code is available at: \url{https://github.com/a1600012888/MUTR3D}.

</p>
</details>

<details><summary><b>Forecasting Market Changes using Variational Inference</b>
<a href="https://arxiv.org/abs/2205.00605">arxiv:2205.00605</a>
&#x1F4C8; 4 <br>
<p>Udai Nagpal, Krishan Nagpal</p></summary>
<p>

**Abstract:** Though various approaches have been considered, forecasting near-term market changes of equities and similar market data remains quite difficult. In this paper we introduce an approach to forecast near-term market changes for equity indices as well as portfolios using variational inference (VI). VI is a machine learning approach which uses optimization techniques to estimate complex probability densities. In the proposed approach, clusters of explanatory variables are identified and market changes are forecast based on cluster-specific linear regression. Apart from the expected value of changes, the proposed approach can also be used to obtain the distribution of possible outcomes, which can be used to estimate confidence levels of forecasts and risk measures such as VaR (Value at Risk) for the portfolio. Another advantage of the proposed approach is the clear model interpretation, as clusters of explanatory variables (or market regimes) are identified for which the future changes follow similar relationships. Knowledge about such clusters can provide useful insights about portfolio performance and identify the relative importance of variables in different market regimes. Illustrative examples of equity and bond indices are considered to demonstrate forecasts of the proposed approach during Covid-related volatility in early 2020 and subsequent benign market conditions. For the portfolios considered, it is shown that the proposed approach provides useful forecasts in both normal and volatile markets even with only a few explanatory variables. Additionally the predicted estimate and distribution adapt quickly to changing market conditions and thus may also be useful in obtaining better real-time estimates of risk measures such as VaR compared to traditional approaches.

</p>
</details>

<details><summary><b>Experimental quantum pattern recognition in IBMQ and diamond NVs</b>
<a href="https://arxiv.org/abs/2205.00561">arxiv:2205.00561</a>
&#x1F4C8; 4 <br>
<p>Sreetama Das, Jingfu Zhang, Stefano Martina, Dieter Suter, Filippo Caruso</p></summary>
<p>

**Abstract:** One of the most promising applications of quantum computing is the processing of graphical data like images. Here, we investigate the possibility of realizing a quantum pattern recognition protocol based on swap test, and use the IBMQ noisy intermediate-scale quantum (NISQ) devices to verify the idea. We find that with a two-qubit protocol, swap test can efficiently detect the similarity between two patterns with good fidelity, though for three or more qubits the noise in the real devices becomes detrimental. To mitigate this noise effect, we resort to destructive swap test, which shows an improved performance for three-qubit states. Due to limited cloud access to larger IBMQ processors, we take a segment-wise approach to apply the destructive swap test on higher dimensional images. In this case, we define an average overlap measure which shows faithfulness to distinguish between two very different or very similar patterns when simulated on real IBMQ processors. As test images, we use binary images with simple patterns, greyscale MNIST numbers and MNIST fashion images, as well as binary images of human blood vessel obtained from magnetic resonance imaging (MRI). We also present an experimental set up for applying destructive swap test using the nitrogen vacancy centre (NVs) in diamond. Our experimental data show high fidelity for single qubit states. Lastly, we propose a protocol inspired from quantum associative memory, which works in an analogous way to supervised learning for performing quantum pattern recognition using destructive swap test.

</p>
</details>

<details><summary><b>The Multivariate Community Hawkes Model for Dependent Relational Events in Continuous-time Networks</b>
<a href="https://arxiv.org/abs/2205.00639">arxiv:2205.00639</a>
&#x1F4C8; 3 <br>
<p>Hadeel Soliman, Lingfei Zhao, Zhipeng Huang, Subhadeep Paul, Kevin S. Xu</p></summary>
<p>

**Abstract:** The stochastic block model (SBM) is one of the most widely used generative models for network data. Many continuous-time dynamic network models are built upon the same assumption as the SBM: edges or events between all pairs of nodes are conditionally independent given the block or community memberships, which prevents them from reproducing higher-order motifs such as triangles that are commonly observed in real networks. We propose the multivariate community Hawkes (MULCH) model, an extremely flexible community-based model for continuous-time networks that introduces dependence between node pairs using structured multivariate Hawkes processes. We fit the model using a spectral clustering and likelihood-based local refinement procedure. We find that our proposed MULCH model is far more accurate than existing models both for predictive and generative tasks.

</p>
</details>

<details><summary><b>Enhancing Adversarial Training with Feature Separability</b>
<a href="https://arxiv.org/abs/2205.00637">arxiv:2205.00637</a>
&#x1F4C8; 3 <br>
<p>Yaxin Li, Xiaorui Liu, Han Xu, Wentao Wang, Jiliang Tang</p></summary>
<p>

**Abstract:** Deep Neural Network (DNN) are vulnerable to adversarial attacks. As a countermeasure, adversarial training aims to achieve robustness based on the min-max optimization problem and it has shown to be one of the most effective defense strategies. However, in this work, we found that compared with natural training, adversarial training fails to learn better feature representations for either clean or adversarial samples, which can be one reason why adversarial training tends to have severe overfitting issues and less satisfied generalize performance. Specifically, we observe two major shortcomings of the features learned by existing adversarial training methods:(1) low intra-class feature similarity; and (2) conservative inter-classes feature variance. To overcome these shortcomings, we introduce a new concept of adversarial training graph (ATG) with which the proposed adversarial training with feature separability (ATFS) enables to coherently boost the intra-class feature similarity and increase inter-class feature variance. Through comprehensive experiments, we demonstrate that the proposed ATFS framework significantly improves both clean and robust performance.

</p>
</details>

<details><summary><b>Design equivariant neural networks for 3D point cloud</b>
<a href="https://arxiv.org/abs/2205.00630">arxiv:2205.00630</a>
&#x1F4C8; 3 <br>
<p>Thuan N. A. Trang, Thieu N. Vo, Khuong D. Nguyen</p></summary>
<p>

**Abstract:** This work seeks to improve the generalization and robustness of existing neural networks for 3D point clouds by inducing group equivariance under general group transformations. The main challenge when designing equivariant models for point clouds is how to trade-off the performance of the model and the complexity. Existing equivariant models are either too complicate to implement or very high complexity. The main aim of this study is to build a general procedure to introduce group equivariant property to SOTA models for 3D point clouds. The group equivariant models built form our procedure are simple to implement, less complexity in comparison with the existing ones, and they preserve the strengths of the original SOTA backbone. From the results of the experiments on object classification, it is shown that our methods are superior to other group equivariant models in performance and complexity. Moreover, our method also helps to improve the mIoU of semantic segmentation models. Overall, by using a combination of only-finite-rotation equivariance and augmentation, our models can outperform existing full $SO(3)$-equivariance models with much cheaper complexity and GPU memory. The proposed procedure is general and forms a fundamental approach to group equivariant neural networks. We believe that it can be easily adapted to other SOTA models in the future.

</p>
</details>

<details><summary><b>Semantically Informed Slang Interpretation</b>
<a href="https://arxiv.org/abs/2205.00616">arxiv:2205.00616</a>
&#x1F4C8; 3 <br>
<p>Zhewei Sun, Richard Zemel, Yang Xu</p></summary>
<p>

**Abstract:** Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret. Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang. We perform rigorous evaluation on two large-scale online slang dictionaries and show that our approach not only achieves state-of-the-art accuracy for slang interpretation in English, but also does so in zero-shot and few-shot scenarios where training data is sparse. Furthermore, we show how the same framework can be applied to enhancing machine translation of slang from English to other languages. Our work creates opportunities for the automated interpretation and translation of informal language.

</p>
</details>

<details><summary><b>Ridgeless Regression with Random Features</b>
<a href="https://arxiv.org/abs/2205.00477">arxiv:2205.00477</a>
&#x1F4C8; 3 <br>
<p>Jian Li, Yong Liu, Yingying Zhang</p></summary>
<p>

**Abstract:** Recent theoretical studies illustrated that kernel ridgeless regression can guarantee good generalization ability without an explicit regularization. In this paper, we investigate the statistical properties of ridgeless regression with random features and stochastic gradient descent. We explore the effect of factors in the stochastic gradient and random features, respectively. Specifically, random features error exhibits the double-descent curve. Motivated by the theoretical findings, we propose a tunable kernel algorithm that optimizes the spectral density of kernel during training. Our work bridges the interpolation theory and practical algorithm.

</p>
</details>

<details><summary><b>ETMS@IITKGP at SemEval-2022 Task 10: Structured Sentiment Analysis Using A Generative Approach</b>
<a href="https://arxiv.org/abs/2205.00440">arxiv:2205.00440</a>
&#x1F4C8; 3 <br>
<p>Raghav R, Adarsh Vemali, Rajdeep Mukherjee</p></summary>
<p>

**Abstract:** Structured Sentiment Analysis (SSA) deals with extracting opinion tuples in a text, where each tuple (h, e, t, p) consists of h, the holder, who expresses a sentiment polarity p towards a target t through a sentiment expression e. While prior works explore graph-based or sequence labeling-based approaches for the task, we in this paper present a novel unified generative method to solve SSA, a SemEval2022 shared task. We leverage a BART-based encoder-decoder architecture and suitably modify it to generate, given a sentence, a sequence of opinion tuples. Each generated tuple consists of seven integers respectively representing the indices corresponding to the start and end positions of the holder, target, and expression spans, followed by the sentiment polarity class associated between the target and the sentiment expression. We perform rigorous experiments for both Monolingual and Cross-lingual subtasks, and achieve competitive Sentiment F1 scores on the leaderboard in both settings.

</p>
</details>

<details><summary><b>Conditional $β$-VAE for De Novo Molecular Generation</b>
<a href="https://arxiv.org/abs/2205.01592">arxiv:2205.01592</a>
&#x1F4C8; 2 <br>
<p>Ryan J Richards, Austen M Groener</p></summary>
<p>

**Abstract:** Deep learning has significantly advanced and accelerated de novo molecular generation. Generative networks, namely Variational Autoencoders (VAEs) can not only randomly generate new molecules, but also alter molecular structures to optimize specific chemical properties which are pivotal for drug-discovery. While VAEs have been proposed and researched in the past for pharmaceutical applications, they possess deficiencies which limit their ability to both optimize properties and decode syntactically valid molecules. We present a recurrent, conditional $β$-VAE which disentangles the latent space to enhance post hoc molecule optimization. We create a mutual information driven training protocol and data augmentations to both increase molecular validity and promote longer sequence generation. We demonstrate the efficacy of our framework on the ZINC-250k dataset, achieving SOTA unconstrained optimization results on the penalized LogP (pLogP) and QED scores, while also matching current SOTA results for validity, novelty and uniqueness scores for random generation. We match the current SOTA on QED for top-3 molecules at 0.948, while setting a new SOTA for pLogP optimization at 104.29, 90.12, 69.68 and demonstrating improved results on the constrained optimization task.

</p>
</details>

<details><summary><b>Adversarial Plannning</b>
<a href="https://arxiv.org/abs/2205.00566">arxiv:2205.00566</a>
&#x1F4C8; 2 <br>
<p>Valentin Vie, Ryan Sheatsley, Sophia Beyda, Sushrut Shringarputale, Kevin Chan, Trent Jaeger, Patrick McDaniel</p></summary>
<p>

**Abstract:** Planning algorithms are used in computational systems to direct autonomous behavior. In a canonical application, for example, planning for autonomous vehicles is used to automate the static or continuous planning towards performance, resource management, or functional goals (e.g., arriving at the destination, managing fuel fuel consumption). Existing planning algorithms assume non-adversarial settings; a least-cost plan is developed based on available environmental information (i.e., the input instance). Yet, it is unclear how such algorithms will perform in the face of adversaries attempting to thwart the planner. In this paper, we explore the security of planning algorithms used in cyber- and cyber-physical systems. We present two $\textit{adversarial planning}$ algorithms-one static and one adaptive-that perturb input planning instances to maximize cost (often substantially so). We evaluate the performance of the algorithms against two dominant planning algorithms used in commercial applications (D* Lite and Fast Downward) and show both are vulnerable to extremely limited adversarial action. Here, experiments show that an adversary is able to increase plan costs in 66.9% of instances by only removing a single action from the actions space (D* Lite) and render 70% of instances from an international planning competition unsolvable by removing only three actions (Fast Forward). Finally, we show that finding an optimal perturbation in any search-based planning system is NP-hard.

</p>
</details>

<details><summary><b>Deep Learning with Logical Constraints</b>
<a href="https://arxiv.org/abs/2205.00523">arxiv:2205.00523</a>
&#x1F4C8; 2 <br>
<p>Eleonora Giunchiglia, Mihaela Catalina Stoian, Thomas Lukasiewicz</p></summary>
<p>

**Abstract:** In recent years, there has been an increasing interest in exploiting logically specified background knowledge in order to obtain neural models (i) with a better performance, (ii) able to learn from less data, and/or (iii) guaranteed to be compliant with the background knowledge itself, e.g., for safety-critical applications. In this survey, we retrace such works and categorize them based on (i) the logical language that they use to express the background knowledge and (ii) the goals that they achieve.

</p>
</details>

<details><summary><b>Domain Adaptation meets Individual Fairness. And they get along</b>
<a href="https://arxiv.org/abs/2205.00504">arxiv:2205.00504</a>
&#x1F4C8; 2 <br>
<p>Debarghya Mukherjee, Felix Petersen, Mikhail Yurochkin, Yuekai Sun</p></summary>
<p>

**Abstract:** Many instances of algorithmic bias are caused by distributional shifts. For example, machine learning (ML) models often perform worse on demographic groups that are underrepresented in the training data. In this paper, we leverage this connection between algorithmic fairness and distribution shifts to show that algorithmic fairness interventions can help ML models overcome distribution shifts, and that domain adaptation methods (for overcoming distribution shifts) can mitigate algorithmic biases. In particular, we show that (i) enforcing suitable notions of individual fairness (IF) can improve the out-of-distribution accuracy of ML models, and that (ii) it is possible to adapt representation alignment methods for domain adaptation to enforce (individual) fairness. The former is unexpected because IF interventions were not developed with distribution shifts in mind. The latter is also unexpected because representation alignment is not a common approach in the IF literature.

</p>
</details>

<details><summary><b>Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation</b>
<a href="https://arxiv.org/abs/2205.00501">arxiv:2205.00501</a>
&#x1F4C8; 2 <br>
<p>Nitesh Goyal, Ian Kivlichan, Rachel Rosen, Lucy Vasserman</p></summary>
<p>

**Abstract:** Machine learning models are commonly used to detect toxicity in online conversations. These models are trained on datasets annotated by human raters. We explore how raters' self-described identities impact how they annotate toxicity in online comments. We first define the concept of specialized rater pools: rater pools formed based on raters' self-described identities, rather than at random. We formed three such rater pools for this study--specialized rater pools of raters from the U.S. who identify as African American, LGBTQ, and those who identify as neither. Each of these rater pools annotated the same set of comments, which contains many references to these identity groups. We found that rater identity is a statistically significant factor in how raters will annotate toxicity for identity-related annotations. Using preliminary content analysis, we examined the comments with the most disagreement between rater pools and found nuanced differences in the toxicity annotations. Next, we trained models on the annotations from each of the different rater pools, and compared the scores of these models on comments from several test sets. Finally, we discuss how using raters that self-identify with the subjects of comments can create more inclusive machine learning models, and provide more nuanced ratings than those by random raters.

</p>
</details>

<details><summary><b>Molecular Identification from AFM images using the IUPAC Nomenclature and Attribute Multimodal Recurrent Neural Networks</b>
<a href="https://arxiv.org/abs/2205.00449">arxiv:2205.00449</a>
&#x1F4C8; 2 <br>
<p>Jaime Carracedo-Cosme, Carlos Romero-Muñiz, Pablo Pou, Rubén Pérez</p></summary>
<p>

**Abstract:** Despite being the main tool to visualize molecules at the atomic scale, AFM with CO-functionalized metal tips is unable to chemically identify the observed molecules. Here we present a strategy to address this challenging task using deep learning techniques. Instead of identifying a finite number of molecules following a traditional classification approach, we define the molecular identification as an image captioning problem. We design an architecture, composed of two multimodal recurrent neural networks, capable of identifying the structure and composition of an unknown molecule using a 3D-AFM image stack as input. The neural network is trained to provide the name of each molecule according to the IUPAC nomenclature rules. To train and test this algorithm we use the novel QUAM-AFM dataset, which contains almost 700,000 molecules and 165 million AFM images. The accuracy of the predictions is remarkable, achieving a high score quantified by the cumulative BLEU 4-gram, a common metric in language recognition studies.

</p>
</details>

<details><summary><b>Drone Flocking Optimization using NSGA-II and Principal Component Analysis</b>
<a href="https://arxiv.org/abs/2205.00432">arxiv:2205.00432</a>
&#x1F4C8; 2 <br>
<p>Jagdish Chand Bansal, Nikhil Sethi, Ogbonnaya Anicho, Atulya Nagar</p></summary>
<p>

**Abstract:** Individual agents in natural systems like flocks of birds or schools of fish display a remarkable ability to coordinate and communicate in local groups and execute a variety of tasks efficiently. Emulating such natural systems into drone swarms to solve problems in defence, agriculture, industry automation and humanitarian relief is an emerging technology. However, flocking of aerial robots while maintaining multiple objectives, like collision avoidance, high speed etc. is still a challenge. In this paper, optimized flocking of drones in a confined environment with multiple conflicting objectives is proposed. The considered objectives are collision avoidance (with each other and the wall), speed, correlation, and communication (connected and disconnected agents). Principal Component Analysis (PCA) is applied for dimensionality reduction, and understanding the collective dynamics of the swarm. The control model is characterised by 12 parameters which are then optimized using a multi-objective solver (NSGA-II). The obtained results are reported and compared with that of the CMA-ES algorithm. The study is particularly useful as the proposed optimizer outputs a Pareto Front representing different types of swarms which can applied to different scenarios in the real world.

</p>
</details>

<details><summary><b>Community detection in multiplex networks based on orthogonal nonnegative matrix tri-factorization</b>
<a href="https://arxiv.org/abs/2205.00626">arxiv:2205.00626</a>
&#x1F4C8; 1 <br>
<p>Meiby Ortiz-Bouza, Selin Aviyente</p></summary>
<p>

**Abstract:** Networks provide a powerful tool to model complex systems where the different entities in the system are presented by nodes and their interactions by edges. Recently, there has been a growing interest in multiplex networks as they can represent the interactions between a pair of nodes through multiple types of links, each reflecting a distinct type of interaction. One of the important tools in understanding network topology is community detection. Although there are numerous works on community detection in single layer networks, existing work on multiplex community detection mostly focuses on learning a common community structure across layers without taking the heterogeneity of the different layers into account. In this paper, we introduce a new multiplex community detection approach that can identify communities that are common across layers as well as those that are unique to each layer. The proposed algorithm employs Orthogonal Nonnegative Matrix Tri-Factorization to model each layer's adjacency matrix as the sum of two low-rank matrix factorizations, corresponding to the common and private communities, respectively. The proposed algorithm is evaluated on both synthetic and real multiplex networks and compared to state-of-the-art techniques.

</p>
</details>

<details><summary><b>Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs</b>
<a href="https://arxiv.org/abs/2205.00484">arxiv:2205.00484</a>
&#x1F4C8; 1 <br>
<p>Songlin Yang, Wei Liu, Kewei Tu</p></summary>
<p>

**Abstract:** Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models, both of which can be represented as factor graph grammars (FGGs), a powerful formalism capable of describing a wide range of models. Recent research found it beneficial to use large state spaces for HMMs and PCFGs. However, inference with large state spaces is computationally demanding, especially for PCFGs. To tackle this challenge, we leverage tensor rank decomposition (aka.\ CPD) to decrease inference computational complexities for a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of an FGG and then construct a new FGG defined in the rank space. Inference with the new FGG produces the same result but has a lower time complexity when the rank size is smaller than the state size. We conduct experiments on HMM language modeling and unsupervised PCFG parsing, showing better performance than previous work. Our code is publicly available at \url{https://github.com/VPeterV/RankSpace-Models}.

</p>
</details>

<details><summary><b>Boost decoding performance of finite geometry LDPC codes with deep learning tactics</b>
<a href="https://arxiv.org/abs/2205.00481">arxiv:2205.00481</a>
&#x1F4C8; 1 <br>
<p>Guangwen Li, Xiao Yu</p></summary>
<p>

**Abstract:** It was known a standard min-sum decoder can be unrolled as a neural network after weighting each edges. We adopt the similar decoding framework to seek a low-complexity and high-performance decoder for a class of finite geometry LDPC codes in short and moderate block lengths. It is elaborated on how to generate high-quality training data effectively, and the strong link is illustrated between training loss and the bit error rate of a neural decoder after tracing the evolution curves. Considering there exists a potential conflict between the neural networks and the error-correction decoders in terms of their objectives, the necessity of restraining the number of trainable parameters to ensure training convergence or reduce decoding complexity is highlighted. Consequently, for the referred LDPC codes, their rigorous algebraic structure promotes the feasibility of cutting down the number of trainable parameters even to only one, whereas incurring marginal performance loss in the simulation.

</p>
</details>

<details><summary><b>None Class Ranking Loss for Document-Level Relation Extraction</b>
<a href="https://arxiv.org/abs/2205.00476">arxiv:2205.00476</a>
&#x1F4C8; 1 <br>
<p>Yang Zhou, Wee Sun Lee</p></summary>
<p>

**Abstract:** Document-level relation extraction (RE) aims at extracting relations among entities expressed across multiple sentences, which can be viewed as a multi-label classification problem. In a typical document, most entity pairs do not express any pre-defined relation and are labeled as "none" or "no relation". For good document-level RE performance, it is crucial to distinguish such none class instances (entity pairs) from those of pre-defined classes (relations). However, most existing methods only estimate the probability of pre-defined relations independently without considering the probability of "no relation". This ignores the context of entity pairs and the label correlations between the none class and pre-defined classes, leading to sub-optimal predictions. To address this problem, we propose a new multi-label loss that encourages large margins of label confidence scores between each pre-defined class and the none class, which enables captured label correlations and context-dependent thresholding for label prediction. To gain further robustness against positive-negative imbalance and mislabeled data that could appear in real-world RE datasets, we propose a margin regularization and a margin shifting technique. Experimental results demonstrate that our method significantly outperforms existing multi-label losses for document-level RE and works well in other multi-label tasks such as emotion classification when none class instances are available for training.

</p>
</details>

<details><summary><b>An Analysis of the Features Considerable for NFT Recommendations</b>
<a href="https://arxiv.org/abs/2205.00456">arxiv:2205.00456</a>
&#x1F4C8; 1 <br>
<p>Dinuka Piyadigama, Guhanathan Poravi</p></summary>
<p>

**Abstract:** This research explores the methods that NFTs can be recommended to people who interact with NFT-marketplaces to explore NFTs of preference and similarity to what they have been searching for. While exploring past methods that can be adopted for recommendations, the use of NFT traits for recommendations has been explored. The outcome of the research highlights the necessity of using multiple Recommender Systems to present the user with the best possible NFTs when interacting with decentralized systems.

</p>
</details>

<details><summary><b>TinyLight: Adaptive Traffic Signal Control on Devices with Extremely Limited Resources</b>
<a href="https://arxiv.org/abs/2205.00427">arxiv:2205.00427</a>
&#x1F4C8; 1 <br>
<p>Dong Xing, Qian Zheng, Qianhui Liu, Gang Pan</p></summary>
<p>

**Abstract:** Recent advances in deep reinforcement learning (DRL) have largely promoted the performance of adaptive traffic signal control (ATSC). Nevertheless, regarding the implementation, most works are cumbersome in terms of storage and computation. This hinders their deployment on scenarios where resources are limited. In this work, we propose TinyLight, the first DRL-based ATSC model that is designed for devices with extremely limited resources. TinyLight first constructs a super-graph to associate a rich set of candidate features with a group of light-weighted network blocks. Then, to diminish the model's resource consumption, we ablate edges in the super-graph automatically with a novel entropy-minimized objective function. This enables TinyLight to work on a standalone microcontroller with merely 2KB RAM and 32KB ROM. We evaluate TinyLight on multiple road networks with real-world traffic demands. Experiments show that even with extremely limited resources, TinyLight still achieves competitive performance. The source code and appendix of this work can be found at \url{https://bit.ly/38hH8t8}.

</p>
</details>

<details><summary><b>Re-defining Radiology Quality Assurance (QA) -- Artificial Intelligence (AI)-Based QA by Restricted Investigation of Unequal Scores (AQUARIUS)</b>
<a href="https://arxiv.org/abs/2205.00629">arxiv:2205.00629</a>
&#x1F4C8; 0 <br>
<p>Axel Wismuller, Larry Stockmaster, Ali Vosoughi</p></summary>
<p>

**Abstract:** There is an urgent need for streamlining radiology Quality Assurance (QA) programs to make them better and faster. Here, we present a novel approach, Artificial Intelligence (AI)-Based QUality Assurance by Restricted Investigation of Unequal Scores (AQUARIUS), for re-defining radiology QA, which reduces human effort by up to several orders of magnitude over existing approaches. AQUARIUS typically includes automatic comparison of AI-based image analysis with natural language processing (NLP) on radiology reports. Only the usually small subset of cases with discordant reads is subsequently reviewed by human experts. To demonstrate the clinical applicability of AQUARIUS, we performed a clinical QA study on Intracranial Hemorrhage (ICH) detection in 1936 head CT scans from a large academic hospital. Immediately following image acquisition, scans were automatically analyzed for ICH using a commercially available software (Aidoc, Tel Aviv, Israel). Cases rated positive for ICH by AI (ICH-AI+) were automatically flagged in radiologists' reading worklists, where flagging was randomly switched off with probability 50\%. Using AQUARIUS with NLP on final radiology reports and targeted expert neuroradiology review of only 29 discordantly classified cases reduced the human QA effort by 98.5\%, where we found a total of six non-reported true ICH+ cases, with radiologists' missed ICH detection rates of 0.52\% and 2.5\% for flagged and non-flagged cases, respectively. We conclude that AQUARIUS, by combining AI-based image analysis with NLP-based pre-selection of cases for targeted human expert review, can efficiently identify missed findings in radiology studies and significantly expedite radiology QA programs in a hybrid human-machine interoperability approach.

</p>
</details>

<details><summary><b>Physics-aware Reduced-order Modeling of Transonic Flow via $β$-Variational Autoencoder</b>
<a href="https://arxiv.org/abs/2205.00608">arxiv:2205.00608</a>
&#x1F4C8; 0 <br>
<p>Yu-Eop Kang, Sunwoong Yang, Kwanjung Yee</p></summary>
<p>

**Abstract:** Autoencoder-based reduced-order modeling has recently attracted significant attention, owing to the ability to capture underlying nonlinear features. However, its uninterpretable latent variables (LVs) severely undermine the applicability to various physical problems. This study proposes physics-aware reduced-order modeling using a $β$-variational autoencoder to address this issue. The presented approach can quantify the rank and independence of LVs, which is validated both quantitatively and qualitatively using various techniques. Accordingly, LVs containing interpretable physical features were successfully identified. It was also verified that these "physics-aware" LVs correspond to the physical parameters that are the generating factors of the dataset, i.e., the Mach number and angle of attack in this study. Moreover, the effects of these physics-aware LVs on the accuracy of reduced-order modeling were investigated, which verified the potential of this method to alleviate the computational cost of the offline stage by excluding physics-unaware LVs.

</p>
</details>

<details><summary><b>Using a novel fractional-order gradient method for CNN back-propagation</b>
<a href="https://arxiv.org/abs/2205.00581">arxiv:2205.00581</a>
&#x1F4C8; 0 <br>
<p>Mundher Mohammed Taresh, Ningbo Zhu, Talal Ahmed Ali Ali, Mohammed Alghaili, Weihua Guo</p></summary>
<p>

**Abstract:** Computer-aided diagnosis tools have experienced rapid growth and development in recent years. Among all, deep learning is the most sophisticated and popular tool. In this paper, researchers propose a novel deep learning model and apply it to COVID-19 diagnosis. Our model uses the tool of fractional calculus, which has the potential to improve the performance of gradient methods. To this end, the researcher proposes a fractional-order gradient method for the back-propagation of convolutional neural networks based on the Caputo definition. However, if only the first term of the infinite series of the Caputo definition is used to approximate the fractional-order derivative, the length of the memory is truncated. Therefore, the fractional-order gradient (FGD) method with a fixed memory step and an adjustable number of terms is used to update the weights of the layers. Experiments were performed on the COVIDx dataset to demonstrate fast convergence, good accuracy, and the ability to bypass the local optimal point. We also compared the performance of the developed fractional-order neural networks and Integer-order neural networks. The results confirmed the effectiveness of our proposed model in the diagnosis of COVID-19.

</p>
</details>

<details><summary><b>Thermodynamically Consistent Machine-Learned Internal State Variable Approach for Data-Driven Modeling of Path-Dependent Materials</b>
<a href="https://arxiv.org/abs/2205.00578">arxiv:2205.00578</a>
&#x1F4C8; 0 <br>
<p>Xiaolong He, Jiun-Shyan Chen</p></summary>
<p>

**Abstract:** Characterization and modeling of path-dependent behaviors of complex materials by phenomenological models remains challenging due to difficulties in formulating mathematical expressions and internal state variables (ISVs) governing path-dependent behaviors. Data-driven machine learning models, such as deep neural networks and recurrent neural networks (RNNs), have become viable alternatives. However, pure black-box data-driven models mapping inputs to outputs without considering the underlying physics suffer from unstable and inaccurate generalization performance. This study proposes a machine-learned physics-informed data-driven constitutive modeling approach for path-dependent materials based on the measurable material states. The proposed data-driven constitutive model is designed with the consideration of universal thermodynamics principles, where the ISVs essential to the material path-dependency are inferred automatically from the hidden state of RNNs. The RNN describing the evolution of the data-driven machine-learned ISVs follows the thermodynamics second law. To enhance the robustness and accuracy of RNN models, stochasticity is introduced to model training. The effects of the number of RNN history steps, the internal state dimension, the model complexity, and the strain increment on model performances have been investigated. The effectiveness of the proposed method is evaluated by modeling soil material behaviors under cyclic shear loading using experimental stress-strain data.

</p>
</details>

<details><summary><b>Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection</b>
<a href="https://arxiv.org/abs/2205.00525">arxiv:2205.00525</a>
&#x1F4C8; 0 <br>
<p>Akshat Goel, Denise Gorse</p></summary>
<p>

**Abstract:** While deep learning models have seen recent high uptake in the geosciences, and are appealing in their ability to learn from minimally processed input data, as black box models they do not provide an easy means to understand how a decision is reached, which in safety-critical tasks especially can be problematical. An alternative route is to use simpler, more transparent white box models, in which task-specific feature construction replaces the more opaque feature discovery process performed automatically within deep learning models. Using data from the Groningen Gas Field in the Netherlands, we build on an existing logistic regression model by the addition of four further features discovered using elastic net driven data mining within the catch22 time series analysis package. We then evaluate the performance of the augmented logistic regression model relative to a deep (CNN) model, pre-trained on the Groningen data, on progressively increasing noise-to-signal ratios. We discover that, for each ratio, our logistic regression model correctly detects every earthquake, while the deep model fails to detect nearly 20 % of seismic events, thus justifying at least a degree of caution in the application of deep models, especially to data with higher noise-to-signal ratios.

</p>
</details>

<details><summary><b>Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition</b>
<a href="https://arxiv.org/abs/2205.00506">arxiv:2205.00506</a>
&#x1F4C8; 0 <br>
<p>Yang Zhou, Zhanhao He, Keyu Lu, Guanhong Wang, Gaoang Wang</p></summary>
<p>

**Abstract:** Video-based action recognition is one of the most popular topics in computer vision. With recent advances of selfsupervised video representation learning approaches, action recognition usually follows a two-stage training framework, i.e., self-supervised pre-training on large-scale unlabeled sets and transfer learning on a downstream labeled set. However, catastrophic forgetting of the pre-trained knowledge becomes the main issue in the downstream transfer learning of action recognition, resulting in a sub-optimal solution. In this paper, to alleviate the above issue, we propose a novel transfer learning approach that combines self-distillation in fine-tuning to preserve knowledge from the pre-trained model learned from the large-scale dataset. Specifically, we fix the encoder from the last epoch as the teacher model to guide the training of the encoder from the current epoch in the transfer learning. With such a simple yet effective learning strategy, we outperform state-of-the-art methods on widely used UCF101 and HMDB51 datasets in action recognition task.

</p>
</details>

<details><summary><b>Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction</b>
<a href="https://arxiv.org/abs/2205.00463">arxiv:2205.00463</a>
&#x1F4C8; 0 <br>
<p>Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang</p></summary>
<p>

**Abstract:** Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's exposure to X-ray radiation. In recent years, supervised deep learning has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational (TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.

</p>
</details>

<details><summary><b>Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation</b>
<a href="https://arxiv.org/abs/2205.00459">arxiv:2205.00459</a>
&#x1F4C8; 0 <br>
<p>Qingyan Meng, Mingqing Xiao, Shen Yan, Yisen Wang, Zhouchen Lin, Zhi-Quan Luo</p></summary>
<p>

**Abstract:** Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.

</p>
</details>

<details><summary><b>Adaptive Online Optimization with Predictions: Static and Dynamic Environments</b>
<a href="https://arxiv.org/abs/2205.00446">arxiv:2205.00446</a>
&#x1F4C8; 0 <br>
<p>Pedro Zattoni Scroccaro, Arman Sharifi Kolarijani, Peyman Mohajerin Esfahani</p></summary>
<p>

**Abstract:** In the past few years, Online Convex Optimization (OCO) has received notable attention in the control literature thanks to its flexible real-time nature and powerful performance guarantees. In this paper, we propose new step-size rules and OCO algorithms that simultaneously exploit gradient predictions, function predictions and dynamics, features particularly pertinent to control applications. The proposed algorithms enjoy static and dynamic regret bounds in terms of the dynamics of the reference action sequence, gradient prediction error and function prediction error, which are generalizations of known regularity measures from the literature. We present results for both convex and strongly convex costs. We validate the performance of the proposed algorithms in a trajectory tracking case study, as well as portfolio optimization using real-world datasets.

</p>
</details>

<details><summary><b>Analysis of Diffractive Neural Networks for Seeing Through Random Diffusers</b>
<a href="https://arxiv.org/abs/2205.00428">arxiv:2205.00428</a>
&#x1F4C8; 0 <br>
<p>Yuhang Li, Yi Luo, Bijie Bai, Aydogan Ozcan</p></summary>
<p>

**Abstract:** Imaging through diffusive media is a challenging problem, where the existing solutions heavily rely on digital computers to reconstruct distorted images. We provide a detailed analysis of a computer-free, all-optical imaging method for seeing through random, unknown phase diffusers using diffractive neural networks, covering different deep learning-based training strategies. By analyzing various diffractive networks designed to image through random diffusers with different correlation lengths, a trade-off between the image reconstruction fidelity and distortion reduction capability of the diffractive network was observed. During its training, random diffusers with a range of correlation lengths were used to improve the diffractive network's generalization performance. Increasing the number of random diffusers used in each epoch reduced the overfitting of the diffractive network's imaging performance to known diffusers. We also demonstrated that the use of additional diffractive layers improved the generalization capability to see through new, random diffusers. Finally, we introduced deliberate misalignments in training to 'vaccinate' the network against random layer-to-layer shifts that might arise due to the imperfect assembly of the diffractive networks. These analyses provide a comprehensive guide in designing diffractive networks to see through random diffusers, which might profoundly impact many fields, such as biomedical imaging, atmospheric physics, and autonomous driving.

</p>
</details>


{% endraw %}
Prev: [2022.04.30]({{ '/2022/04/30/2022.04.30.html' | relative_url }})  Next: [2022.05.02]({{ '/2022/05/02/2022.05.02.html' | relative_url }})