## Summary for 2021-05-12, created on 2021-12-21


<details><summary><b>Kernel Thinning</b>
<a href="https://arxiv.org/abs/2105.05842">arxiv:2105.05842</a>
&#x1F4C8; 71 <br>
<p>Raaz Dwivedi, Lester Mackey</p></summary>
<p>

**Abstract:** We introduce kernel thinning, a new procedure for compressing a distribution $\mathbb{P}$ more effectively than i.i.d.\ sampling or standard thinning. Given a suitable reproducing kernel $\mathbf{k}$ and $\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\mathbb{P}$ into a $\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. With high probability, the maximum discrepancy in integration error is $\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$ for compactly supported $\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an equal-sized i.i.d.\ sample from $\mathbb{P}$ suffers $Ω(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\mathbb{R}^d$ and a wide range of common kernels. We use our results to derive explicit non-asymptotic maximum mean discrepancy bounds for Gaussian, Matérn, and B-spline kernels and present two vignettes illustrating the practical benefits of kernel thinning over i.i.d.\ sampling and standard Markov chain Monte Carlo thinning, in dimensions $d=2$ through $100$.

</p>
</details>

<details><summary><b>When Does Contrastive Visual Representation Learning Work?</b>
<a href="https://arxiv.org/abs/2105.05837">arxiv:2105.05837</a>
&#x1F4C8; 49 <br>
<p>Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, Serge Belongie</p></summary>
<p>

**Abstract:** Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on fine-grained visual classification tasks.

</p>
</details>

<details><summary><b>Segmenter: Transformer for Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2105.05633">arxiv:2105.05633</a>
&#x1F4C8; 27 <br>
<p>Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid</p></summary>
<p>

**Abstract:** Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.

</p>
</details>

<details><summary><b>Directional GAN: A Novel Conditioning Strategy for Generative Networks</b>
<a href="https://arxiv.org/abs/2105.05712">arxiv:2105.05712</a>
&#x1F4C8; 22 <br>
<p>Shradha Agrawal, Shankar Venkitachalam, Dhanya Raghu, Deepak Pai</p></summary>
<p>

**Abstract:** Image content is a predominant factor in marketing campaigns, websites and banners. Today, marketers and designers spend considerable time and money in generating such professional quality content. We take a step towards simplifying this process using Generative Adversarial Networks (GANs). We propose a simple and novel conditioning strategy which allows generation of images conditioned on given semantic attributes using a generator trained for an unconditional image generation task. Our approach is based on modifying latent vectors, using directional vectors of relevant semantic attributes in latent space. Our method is designed to work with both discrete (binary and multi-class) and continuous image attributes. We show the applicability of our proposed approach, named Directional GAN, on multiple public datasets, with an average accuracy of 86.4% across different attributes.

</p>
</details>

<details><summary><b>How Reliable are Model Diagnostics?</b>
<a href="https://arxiv.org/abs/2105.05641">arxiv:2105.05641</a>
&#x1F4C8; 20 <br>
<p>Vamsi Aribandi, Yi Tay, Donald Metzler</p></summary>
<p>

**Abstract:** In the pursuit of a deeper understanding of a model's behaviour, there is recent impetus for developing suites of probes aimed at diagnosing models beyond simple metrics like accuracy or BLEU. This paper takes a step back and asks an important and timely question: how reliable are these diagnostics in providing insight into models and training setups? We critically examine three recent diagnostic tests for pre-trained language models, and find that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed. Based on our empirical findings, we also formulate recommendations for practitioners and researchers.

</p>
</details>

<details><summary><b>Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2105.05537">arxiv:2105.05537</a>
&#x1F4C8; 15 <br>
<p>Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang</p></summary>
<p>

**Abstract:** In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of the convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by 4x, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at https://github.com/HuCaoFighting/Swin-Unet.

</p>
</details>

<details><summary><b>Unbiased Monte Carlo Cluster Updates with Autoregressive Neural Networks</b>
<a href="https://arxiv.org/abs/2105.05650">arxiv:2105.05650</a>
&#x1F4C8; 13 <br>
<p>Dian Wu, Riccardo Rossi, Giuseppe Carleo</p></summary>
<p>

**Abstract:** Efficient sampling of complex high-dimensional probability distributions is a central task in computational science. Machine learning methods like autoregressive neural networks, used with Markov chain Monte Carlo sampling, provide good approximations to such distributions, but suffer from either intrinsic bias or high variance. In this Letter, we propose a way to make this approximation unbiased and with low variance. Our method uses physical symmetries and variable-size cluster updates which utilize the structure of autoregressive factorization. We test our method for first- and second-order phase transitions of classical spin systems, showing its viability for critical systems and in the presence of metastable states.

</p>
</details>

<details><summary><b>Automating Data Science: Prospects and Challenges</b>
<a href="https://arxiv.org/abs/2105.05699">arxiv:2105.05699</a>
&#x1F4C8; 11 <br>
<p>Tijl De Bie, Luc De Raedt, José Hernández-Orallo, Holger H. Hoos, Padhraic Smyth, Christopher K. I. Williams</p></summary>
<p>

**Abstract:** Given the complexity of typical data science projects and the associated demand for human expertise, automation has the potential to transform the data science process.
  Key insights:
  * Automation in data science aims to facilitate and transform the work of data scientists, not to replace them.
  * Important parts of data science are already being automated, especially in the modeling stages, where techniques such as automated machine learning (AutoML) are gaining traction.
  * Other aspects are harder to automate, not only because of technological challenges, but because open-ended and context-dependent tasks require human interaction.

</p>
</details>

<details><summary><b>Deep Graphics Encoder for Real-Time Video Makeup Synthesis from Example</b>
<a href="https://arxiv.org/abs/2105.06407">arxiv:2105.06407</a>
&#x1F4C8; 10 <br>
<p>Robin Kips, Ruowei Jiang, Sileye Ba, Edmund Phung, Parham Aarabi, Pietro Gori, Matthieu Perrot, Isabelle Bloch</p></summary>
<p>

**Abstract:** While makeup virtual-try-on is now widespread, parametrizing a computer graphics rendering engine for synthesizing images of a given cosmetics product remains a challenging task. In this paper, we introduce an inverse computer graphics method for automatic makeup synthesis from a reference image, by learning a model that maps an example portrait image with makeup to the space of rendering parameters. This method can be used by artists to automatically create realistic virtual cosmetics image samples, or by consumers, to virtually try-on a makeup extracted from their favorite reference image.

</p>
</details>

<details><summary><b>Semi-Supervised Variational Reasoning for Medical Dialogue Generation</b>
<a href="https://arxiv.org/abs/2105.06071">arxiv:2105.06071</a>
&#x1F4C8; 10 <br>
<p>Dongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma, Maarten de Rijke</p></summary>
<p>

**Abstract:** Medical dialogue generation aims to provide automatic and accurate responses to assist physicians to obtain diagnosis and treatment suggestions in an efficient manner. In medical dialogues two key characteristics are relevant for response generation: patient states (such as symptoms, medication) and physician actions (such as diagnosis, treatments). In medical scenarios large-scale human annotations are usually not available, due to the high costs and privacy requirements. Hence, current approaches to medical dialogue generation typically do not explicitly account for patient states and physician actions, and focus on implicit representation instead. We propose an end-to-end variational reasoning approach to medical dialogue generation. To be able to deal with a limited amount of labeled data, we introduce both patient state and physician action as latent variables with categorical priors for explicit patient state tracking and physician policy learning, respectively. We propose a variational Bayesian generative approach to approximate posterior distributions over patient states and physician actions. We use an efficient stochastic gradient variational Bayes estimator to optimize the derived evidence lower bound, where a 2-stage collapsed inference method is proposed to reduce the bias during model training. A physician policy network composed of an action-classifier and two reasoning detectors is proposed for augmented reasoning ability. We conduct experiments on three datasets collected from medical platforms. Our experimental results show that the proposed method outperforms state-of-the-art baselines in terms of objective and subjective evaluation metrics. Our experiments also indicate that our proposed semi-supervised reasoning method achieves a comparable performance as state-of-the-art fully supervised learning baselines for physician policy learning.

</p>
</details>

<details><summary><b>Discrete representations in neural models of spoken language</b>
<a href="https://arxiv.org/abs/2105.05582">arxiv:2105.05582</a>
&#x1F4C8; 10 <br>
<p>Bertrand Higy, Lieke Gelderloos, Afra Alishahi, Grzegorz Chrupała</p></summary>
<p>

**Abstract:** The distributed and continuous representations used by neural networks are at odds with representations employed in linguistics, which are typically symbolic. Vector quantization has been proposed as a way to induce discrete neural representations that are closer in nature to their linguistic counterparts. However, it is not clear which metrics are the best-suited to analyze such discrete representations. We compare the merits of four commonly used metrics in the context of weakly supervised models of spoken language. We compare the results they show when applied to two different models, while systematically studying the effect of the placement and size of the discretization layer. We find that different evaluation regimes can give inconsistent results. While we can attribute them to the properties of the different metrics in most cases, one point of concern remains: the use of minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit inventories, unlike metrics applied to complete utterances. Furthermore, while in general vector quantization induces representations that correlate with units posited in linguistics, the strength of this correlation is only moderate.

</p>
</details>

<details><summary><b>Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level</b>
<a href="https://arxiv.org/abs/2105.06020">arxiv:2105.06020</a>
&#x1F4C8; 9 <br>
<p>Ruiqi Zhong, Dhruba Ghosh, Dan Klein, Jacob Steinhardt</p></summary>
<p>

**Abstract:** Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-of-distribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%. We also find that finetuning noise increases with model size and that instance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium correlates with improvement from BERT-Medium to BERT-Large. Our findings suggest that instance-level predictions provide a rich source of information; we therefore, recommend that researchers supplement model weights with model predictions.

</p>
</details>

<details><summary><b>Learning to Generate Novel Scene Compositions from Single Images and Videos</b>
<a href="https://arxiv.org/abs/2105.05847">arxiv:2105.05847</a>
&#x1F4C8; 9 <br>
<p>Vadim Sushko, Juergen Gall, Anna Khoreva</p></summary>
<p>

**Abstract:** Training GANs in low-data regimes remains a challenge, as overfitting often leads to memorization or training divergence. In this work, we introduce One-Shot GAN that can learn to generate samples from a training set as little as one image or one video. We propose a two-branch discriminator, with content and layout branches designed to judge the internal content separately from the scene layout realism. This allows synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single-image GAN models, One-Shot GAN achieves higher diversity and quality of synthesis. It is also not restricted to the single image setting, successfully learning in the introduced setting of a single video.

</p>
</details>

<details><summary><b>On risk-based active learning for structural health monitoring</b>
<a href="https://arxiv.org/abs/2105.05622">arxiv:2105.05622</a>
&#x1F4C8; 8 <br>
<p>A. J. Hughes, L. A. Bull, P. Gardner, R. J. Barthorpe, N. Dervilis, K. Worden</p></summary>
<p>

**Abstract:** A primary motivation for the development and implementation of structural health monitoring systems, is the prospect of gaining the ability to make informed decisions regarding the operation and maintenance of structures and infrastructure. Unfortunately, descriptive labels for measured data corresponding to health-state information for the structure of interest are seldom available prior to the implementation of a monitoring system. This issue limits the applicability of the traditional supervised and unsupervised approaches to machine learning in the development of statistical classifiers for decision-supporting SHM systems.
  The current paper presents a risk-based formulation of active learning, in which the querying of class-label information is guided by the expected value of said information for each incipient data point. When applied to structural health monitoring, the querying of class labels can be mapped onto the inspection of a structure of interest in order to determine its health state. In the current paper, the risk-based active learning process is explained and visualised via a representative numerical example and subsequently applied to the Z24 Bridge benchmark. The results of the case studies indicate that a decision-maker's performance can be improved via the risk-based active learning of a statistical classifier, such that the decision process itself is taken into account.

</p>
</details>

<details><summary><b>Playing Codenames with Language Graphs and Word Embeddings</b>
<a href="https://arxiv.org/abs/2105.05885">arxiv:2105.05885</a>
&#x1F4C8; 6 <br>
<p>Divya Koyyalagunta, Anna Sun, Rachel Lea Draelos, Cynthia Rudin</p></summary>
<p>

**Abstract:** Although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. Word games are not as constrained as games like chess or poker. Instead, word game strategy is defined by the players' understanding of the way words relate to each other. The word game Codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. We propose an algorithm that can generate Codenames clues from the language graph BabelNet or from any of several embedding methods - word2vec, GloVe, fastText or BERT. We introduce a new scoring function that measures the quality of clues, and we propose a weighting term called DETECT that incorporates dictionary-based word representations and document frequency to improve clue selection. We develop BabelNet-Word Selection Framework (BabelNet-WSF) to improve BabelNet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for Codenames. Extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8% improvement in precision@2 in some cases. Overall, this work advances the formal study of word games and approaches for common sense language understanding.

</p>
</details>

<details><summary><b>Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization</b>
<a href="https://arxiv.org/abs/2105.05612">arxiv:2105.05612</a>
&#x1F4C8; 6 <br>
<p>Damien Teney, Ehsan Abbasnejad, Simon Lucey, Anton van den Hengel</p></summary>
<p>

**Abstract:** Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn. We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more complex predictive patterns. OOD generalization fundamentally requires information beyond i.i.d. examples, such as multiple training environments, counterfactual examples, or other side information. Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA results in visual recognition on biased data and generalization across visual domains. The method - the first to evade the simplicity bias - highlights the need for a better understanding and control of inductive biases in deep learning.

</p>
</details>

<details><summary><b>Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding</b>
<a href="https://arxiv.org/abs/2105.05596">arxiv:2105.05596</a>
&#x1F4C8; 6 <br>
<p>Zhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yuejia Xiang, Ningyu Zhang, Yefeng Zheng</p></summary>
<p>

**Abstract:** Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent entities, relations, and others) between two KGs. The existing methods can be divided into the embedding-based models, and the conventional reasoning and lexical matching based systems. The former compute the similarity of entities via their cross-KG embeddings, but they usually rely on an ideal supervised learning setting for good performance and lack appropriate reasoning to avoid logically wrong mappings; while the latter address the reasoning issue but are poor at utilizing the KG graph structures and the entity contexts. In this study, we aim at combining the above two solutions and thus propose an iterative framework named PRASE which is based on probabilistic reasoning and semantic embedding. It learns the KG embeddings via entity mappings from a probabilistic reasoning system named PARIS, and feeds the resultant entity mappings and embeddings back into PARIS for augmentation. The PRASE framework is compatible with different embedding-based models, and our experiments on multiple datasets have demonstrated its state-of-the-art performance.

</p>
</details>

<details><summary><b>Multiscale Invertible Generative Networks for High-Dimensional Bayesian Inference</b>
<a href="https://arxiv.org/abs/2105.05489">arxiv:2105.05489</a>
&#x1F4C8; 6 <br>
<p>Shumao Zhang, Pengchuan Zhang, Thomas Y. Hou</p></summary>
<p>

**Abstract:** We propose a Multiscale Invertible Generative Network (MsIGN) and associated training algorithm that leverages multiscale structure to solve high-dimensional Bayesian inference. To address the curse of dimensionality, MsIGN exploits the low-dimensional nature of the posterior, and generates samples from coarse to fine scale (low to high dimension) by iteratively upsampling and refining samples. MsIGN is trained in a multi-stage manner to minimize the Jeffreys divergence, which avoids mode dropping in high-dimensional cases. On two high-dimensional Bayesian inverse problems, we show superior performance of MsIGN over previous approaches in posterior approximation and multiple mode capture. On the natural image synthesis task, MsIGN achieves superior performance in bits-per-dimension over baseline models and yields great interpret-ability of its neurons in intermediate layers.

</p>
</details>

<details><summary><b>Compatibility-aware Heterogeneous Visual Search</b>
<a href="https://arxiv.org/abs/2105.06047">arxiv:2105.06047</a>
&#x1F4C8; 5 <br>
<p>Rahul Duggal, Hao Zhou, Shuo Yang, Yuanjun Xiong, Wei Xia, Zhuowen Tu, Stefano Soatto</p></summary>
<p>

**Abstract:** We tackle the problem of visual search under resource constraints. Existing systems use the same embedding model to compute representations (embeddings) for the query and gallery images. Such systems inherently face a hard accuracy-efficiency trade-off: the embedding model needs to be large enough to ensure high accuracy, yet small enough to enable query-embedding computation on resource-constrained platforms. This trade-off could be mitigated if gallery embeddings are generated from a large model and query embeddings are extracted using a compact model. The key to building such a system is to ensure representation compatibility between the query and gallery models. In this paper, we address two forms of compatibility: One enforced by modifying the parameters of each model that computes the embeddings. The other by modifying the architectures that compute the embeddings, leading to compatibility-aware neural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval tasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to ordinary (homogeneous) visual search using the largest embedding model (paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while maintaining accuracy within 0.3% and 1.6% of the paragon on DeepFashion2 and IJB-C respectively.

</p>
</details>

<details><summary><b>Exploring the Similarity of Representations in Model-Agnostic Meta-Learning</b>
<a href="https://arxiv.org/abs/2105.05757">arxiv:2105.05757</a>
&#x1F4C8; 5 <br>
<p>Thomas Goerttler, Klaus Obermayer</p></summary>
<p>

**Abstract:** In past years model-agnostic meta-learning (MAML) has been one of the most promising approaches in meta-learning. It can be applied to different kinds of problems, e.g., reinforcement learning, but also shows good results on few-shot learning tasks. Besides their tremendous success in these tasks, it has still not been fully revealed yet, why it works so well. Recent work proposes that MAML rather reuses features than rapidly learns. In this paper, we want to inspire a deeper understanding of this question by analyzing MAML's representation. We apply representation similarity analysis (RSA), a well-established method in neuroscience, to the few-shot learning instantiation of MAML. Although some part of our analysis supports their general results that feature reuse is predominant, we also reveal arguments against their conclusion. The similarity-increase of layers closer to the input layers arises from the learning task itself and not from the model. In addition, the representations after inner gradient steps make a broader change to the representation than the changes during meta-training.

</p>
</details>

<details><summary><b>Encoding Explanatory Knowledge for Zero-shot Science Question Answering</b>
<a href="https://arxiv.org/abs/2105.05737">arxiv:2105.05737</a>
&#x1F4C8; 5 <br>
<p>Zili Zhou, Marco Valentino, Donal Landers, Andre Freitas</p></summary>
<p>

**Abstract:** This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge Transfer), a novel method for the automatic transfer of explanatory knowledge through neural encoding mechanisms. We demonstrate that N-XKT is able to improve accuracy and generalization on science Question Answering (QA). Specifically, by leveraging facts from background explanatory knowledge corpora, the N-XKT model shows a clear improvement on zero-shot QA. Furthermore, we show that N-XKT can be fine-tuned on a target QA dataset, enabling faster convergence and more accurate results. A systematic analysis is conducted to quantitatively analyze the performance of the N-XKT model and the impact of different categories of knowledge on the zero-shot generalization task.

</p>
</details>

<details><summary><b>Disentangling Sampling and Labeling Bias for Learning in Large-Output Spaces</b>
<a href="https://arxiv.org/abs/2105.05736">arxiv:2105.05736</a>
&#x1F4C8; 5 <br>
<p>Ankit Singh Rawat, Aditya Krishna Menon, Wittawat Jitkrittum, Sadeep Jayasumana, Felix X. Yu, Sashank Reddi, Sanjiv Kumar</p></summary>
<p>

**Abstract:** Negative sampling schemes enable efficient training given a large number of classes, by offering a means to approximate a computationally expensive loss function that takes all labels into account. In this paper, we present a new connection between these schemes and loss modification techniques for countering label imbalance. We show that different negative sampling schemes implicitly trade-off performance on dominant versus rare labels. Further, we provide a unified means to explicitly tackle both sampling bias, arising from working with a subset of all labels, and labeling bias, which is inherent to the data due to label imbalance. We empirically verify our findings on long-tail classification and retrieval benchmarks.

</p>
</details>

<details><summary><b>TopoTxR: A Topological Biomarker for Predicting Treatment Response in Breast Cancer</b>
<a href="https://arxiv.org/abs/2105.06049">arxiv:2105.06049</a>
&#x1F4C8; 4 <br>
<p>Fan Wang, Saarthak Kapse, Steven Liu, Prateek Prasanna, Chao Chen</p></summary>
<p>

**Abstract:** Characterization of breast parenchyma on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a challenging task owing to the complexity of underlying tissue structures. Current quantitative approaches, including radiomics and deep learning models, do not explicitly capture the complex and subtle parenchymal structures, such as fibroglandular tissue. In this paper, we propose a novel method to direct a neural network's attention to a dedicated set of voxels surrounding biologically relevant tissue structures. By extracting multi-dimensional topological structures with high saliency, we build a topology-derived biomarker, TopoTxR. We demonstrate the efficacy of TopoTxR in predicting response to neoadjuvant chemotherapy in breast cancer. Our qualitative and quantitative results suggest differential topological behavior of breast tissue on treatment-naïve imaging, in patients who respond favorably to therapy versus those who do not.

</p>
</details>

<details><summary><b>Improving Code Autocompletion with Transfer Learning</b>
<a href="https://arxiv.org/abs/2105.05991">arxiv:2105.05991</a>
&#x1F4C8; 4 <br>
<p>Wen Zhou, Seohyun Kim, Vijayaraghavan Murali, Gareth Ari Aye</p></summary>
<p>

**Abstract:** Software language models have achieved promising results predicting code completion usages, and several industry studies have described successful IDE integrations. Recently, accuracy in autocompletion prediction improved 12.8% from training on a real-world dataset collected from programmers' IDE activity. But what if limited examples of IDE autocompletion in the target programming language are available for model training? In this paper, we investigate the efficacy of pretraining autocompletion models on non-IDE, non-autocompletion, and different-language example code sequences. We find that these unsupervised pretrainings improve model accuracy by over 50% on very small fine-tuning datasets and over 10% on 50k labeled examples. We confirm the real-world impact of these pretrainings in an online setting through A/B testing on thousands of IDE autocompletion users, finding that pretraining is responsible for increases of up to 6.63% autocompletion usage.

</p>
</details>

<details><summary><b>MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2105.05912">arxiv:2105.05912</a>
&#x1F4C8; 4 <br>
<p>Ahmad Rashid, Vasileios Lioutas, Mehdi Rezagholizadeh</p></summary>
<p>

**Abstract:** The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present, MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-Large.

</p>
</details>

<details><summary><b>Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms</b>
<a href="https://arxiv.org/abs/2105.05791">arxiv:2105.05791</a>
&#x1F4C8; 4 <br>
<p>Ryoto Ishizuka, Ryo Nishikimi, Kazuyoshi Yoshii</p></summary>
<p>

**Abstract:** This paper describes an automatic drum transcription (ADT) method that directly estimates a tatum-level drum score from a music signal, in contrast to most conventional ADT methods that estimate the frame-level onset probabilities of drums. To estimate a tatum-level score, we propose a deep transcription model that consists of a frame-level encoder for extracting the latent features from a music signal and a tatum-level decoder for estimating a drum score from the latent features pooled at the tatum level. To capture the global repetitive structure of drum scores, which is difficult to learn with a recurrent neural network (RNN), we introduce a self-attention mechanism with tatum-synchronous positional encoding into the decoder. To mitigate the difficulty of training the self-attention-based model from an insufficient amount of paired data and improve the musical naturalness of the estimated scores, we propose a regularized training method that uses a global structure-aware masked language (score) model with a self-attention mechanism pretrained from an extensive collection of drum scores. Experimental results showed that the proposed regularized model outperformed the conventional RNN-based model in terms of the tatum-level error rate and the frame-level F-measure, even when only a limited amount of paired data was available so that the non-regularized model underperformed the RNN-based model.

</p>
</details>

<details><summary><b>The Greedy and Recursive Search for Morphological Productivity</b>
<a href="https://arxiv.org/abs/2105.05790">arxiv:2105.05790</a>
&#x1F4C8; 4 <br>
<p>Caleb Belth, Sarah Payne, Deniz Beser, Jordan Kodner, Charles Yang</p></summary>
<p>

**Abstract:** As children acquire the knowledge of their language's morphology, they invariably discover the productive processes that can generalize to new words. Morphological learning is made challenging by the fact that even fully productive rules have exceptions, as in the well-known case of English past tense verbs, which features the -ed rule against the irregular verbs. The Tolerance Principle is a recent proposal that provides a precise threshold of exceptions that a productive rule can withstand. Its empirical application so far, however, requires the researcher to fully specify rules defined over a set of words. We propose a greedy search model that automatically hypothesizes rules and evaluates their productivity over a vocabulary. When the search for broader productivity fails, the model recursively subdivides the vocabulary and continues the search for productivity over narrower rules. Trained on psychologically realistic data from child-directed input, our model displays developmental patterns observed in child morphology acquisition, including the notoriously complex case of German noun pluralization. It also produces responses to nonce words that, despite receiving only a fraction of the training data, are more similar to those of human subjects than current neural network models' responses are.

</p>
</details>

<details><summary><b>Looking at CTR Prediction Again: Is Attention All You Need?</b>
<a href="https://arxiv.org/abs/2105.05563">arxiv:2105.05563</a>
&#x1F4C8; 4 <br>
<p>Yuan Cheng, Yanbo Xue</p></summary>
<p>

**Abstract:** Click-through rate (CTR) prediction is a critical problem in web search, recommendation systems and online advertisement displaying. Learning good feature interactions is essential to reflect user's preferences to items. Many CTR prediction models based on deep learning have been proposed, but researchers usually only pay attention to whether state-of-the-art performance is achieved, and ignore whether the entire framework is reasonable. In this work, we use the discrete choice model in economics to redefine the CTR prediction problem, and propose a general neural network framework built on self-attention mechanism. It is found that most existing CTR prediction models align with our proposed general framework. We also examine the expressive power and model complexity of our proposed framework, along with potential extensions to some existing models. And finally we demonstrate and verify our insights through some experimental results on public datasets.

</p>
</details>

<details><summary><b>Learning Uncertainty with Artificial Neural Networks for Improved Remaining Time Prediction of Business Processes</b>
<a href="https://arxiv.org/abs/2105.05559">arxiv:2105.05559</a>
&#x1F4C8; 4 <br>
<p>Hans Weytjens, Jochen De Weerdt</p></summary>
<p>

**Abstract:** Artificial neural networks will always make a prediction, even when completely uncertain and regardless of the consequences. This obliviousness of uncertainty is a major obstacle towards their adoption in practice. Techniques exist, however, to estimate the two major types of uncertainty: model uncertainty and observation noise in the data. Bayesian neural networks are theoretically well-founded models that can learn the model uncertainty of their predictions. Minor modifications to these models and their loss functions allow learning the observation noise for individual samples as well. This paper is the first to apply these techniques to predictive process monitoring. We found that they contribute towards more accurate predictions and work quickly. However, their main benefit resides with the uncertainty estimates themselves that allow the separation of higher-quality from lower-quality predictions and the building of confidence intervals. This leads to many interesting applications, enables an earlier adoption of prediction systems with smaller datasets and fosters a better cooperation with humans.

</p>
</details>

<details><summary><b>Winograd Algorithm for AdderNet</b>
<a href="https://arxiv.org/abs/2105.05530">arxiv:2105.05530</a>
&#x1F4C8; 4 <br>
<p>Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu, Yunhe Wang</p></summary>
<p>

**Abstract:** Adder neural network (AdderNet) is a new kind of deep model that replaces the original massive multiplications in convolutions by additions while preserving the high performance. Since the hardware complexity of additions is much lower than that of multiplications, the overall energy consumption is thus reduced significantly. To further optimize the hardware overhead of using AdderNet, this paper studies the winograd algorithm, which is a widely used fast algorithm for accelerating convolution and saving the computational costs. Unfortunately, the conventional Winograd algorithm cannot be directly applied to AdderNets since the distributive law in multiplication is not valid for the l1-norm. Therefore, we replace the element-wise multiplication in the Winograd equation by additions and then develop a new set of transform matrixes that can enhance the representation ability of output features to maintain the performance. Moreover, we propose the l2-to-l1 training strategy to mitigate the negative impacts caused by formal inconsistency. Experimental results on both FPGA and benchmarks show that the new method can further reduce the energy consumption without affecting the accuracy of the original AdderNet.

</p>
</details>

<details><summary><b>Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction</b>
<a href="https://arxiv.org/abs/2105.05498">arxiv:2105.05498</a>
&#x1F4C8; 4 <br>
<p>Gyubok Lee, Seongjun Yang, Edward Choi</p></summary>
<p>

**Abstract:** Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.

</p>
</details>

<details><summary><b>Ensemble Making Few-Shot Learning Stronger</b>
<a href="https://arxiv.org/abs/2105.11904">arxiv:2105.11904</a>
&#x1F4C8; 3 <br>
<p>Qing Lin, Yongbin Liu, Wen Wen, Zhihua Tao</p></summary>
<p>

**Abstract:** Few-shot learning has been proposed and rapidly emerging as a viable means for completing various tasks. Many few-shot models have been widely used for relation learning tasks. However, each of these models has a shortage of capturing a certain aspect of semantic features, for example, CNN on long-range dependencies part, Transformer on local features. It is difficult for a single model to adapt to various relation learning, which results in the high variance problem. Ensemble strategy could be competitive on improving the accuracy of few-shot relation extraction and mitigating high variance risks. This paper explores an ensemble approach to reduce the variance and introduces fine-tuning and feature attention strategies to calibrate relation-level features. Results on several few-shot relation learning tasks show that our model significantly outperforms the previous state-of-the-art models.

</p>
</details>

<details><summary><b>Multilingual Offensive Language Identification for Low-resource Languages</b>
<a href="https://arxiv.org/abs/2105.05996">arxiv:2105.05996</a>
&#x1F4C8; 3 <br>
<p>Tharindu Ranasinghe, Marcos Zampieri</p></summary>
<p>

**Abstract:** Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbullying, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of available English datasets by applying cross-lingual contextual word embeddings and transfer learning to make predictions in low-resource languages. We project predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi, Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in TRAC-2 shared task, 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in OffensEval 2020, 0.8568 F1 macro for Hindi in HASOC 2019 shared task and 0.7513 F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) showing that our approach compares favourably to the best systems submitted to recent shared tasks on these three languages. Additionally, we report competitive performance on Arabic, and Turkish using the training and development sets of OffensEval 2020 shared task. The results for all languages confirm the robustness of cross-lingual contextual embeddings and transfer learning for this task.

</p>
</details>

<details><summary><b>Removing Blocking Artifacts in Video Streams Using Event Cameras</b>
<a href="https://arxiv.org/abs/2105.05973">arxiv:2105.05973</a>
&#x1F4C8; 3 <br>
<p>Henry H. Chopp, Srutarshi Banerjee, Oliver Cossairt, Aggelos K. Katsaggelos</p></summary>
<p>

**Abstract:** In this paper, we propose EveRestNet, a convolutional neural network designed to remove blocking artifacts in videostreams using events from neuromorphic sensors. We first degrade the video frame using a quadtree structure to produce the blocking artifacts to simulate transmitting a video under a heavily constrained bandwidth. Events from the neuromorphic sensor are also simulated, but are transmitted in full. Using the distorted frames and the event stream, EveRestNet is able to improve the image quality.

</p>
</details>

<details><summary><b>Deep Snapshot HDR Reconstruction Based on the Polarization Camera</b>
<a href="https://arxiv.org/abs/2105.05824">arxiv:2105.05824</a>
&#x1F4C8; 3 <br>
<p>Juiwen Ting, Xuesong Wu, Kangkang Hu, Hong Zhang</p></summary>
<p>

**Abstract:** The recent development of the on-chip micro-polarizer technology has made it possible to acquire four spatially aligned and temporally synchronized polarization images with the same ease of operation as a conventional camera. In this paper, we investigate the use of this sensor technology in high-dynamic-range (HDR) imaging. Specifically, observing that natural light can be attenuated differently by varying the orientation of the polarization filter, we treat the multiple images captured by the polarization camera as a set captured under different exposure times. In our approach, we first study the relationship among polarizer orientation, degree and angle of polarization of light to the exposure time of a pixel in the polarization image. Subsequently, we propose a deep snapshot HDR reconstruction framework to recover an HDR image using the polarization images. A polarized HDR dataset is created to train and evaluate our approach. We demonstrate that our approach performs favorably against state-of-the-art HDR reconstruction algorithms.

</p>
</details>

<details><summary><b>Thematic recommendations on knowledge graphs using multilayer networks</b>
<a href="https://arxiv.org/abs/2105.05733">arxiv:2105.05733</a>
&#x1F4C8; 3 <br>
<p>Mariano Beguerisse-Díaz, Dimitrios Korkinof, Till Hoffmann</p></summary>
<p>

**Abstract:** We present a framework to generate and evaluate thematic recommendations based on multilayer network representations of knowledge graphs (KGs). In this representation, each layer encodes a different type of relationship in the KG, and directed interlayer couplings connect the same entity in different roles. The relative importance of different types of connections is captured by an intuitive salience matrix that can be estimated from data, tuned to incorporate domain knowledge, address different use cases, or respect business logic.
  We apply an adaptation of the personalised PageRank algorithm to multilayer models of KGs to generate item-item recommendations. These recommendations reflect the knowledge we hold about the content and are suitable for thematic and/or cold-start recommendation settings. Evaluating thematic recommendations from user data presents unique challenges that we address by developing a method to evaluate recommendations relying on user-item ratings, yet respecting their thematic nature. We also show that the salience matrix can be estimated from user data. We demonstrate the utility of our methods by significantly improving consumption metrics in an AB test where collaborative filtering delivered subpar performance. We also apply our approach to movie recommendation using publicly-available data to ensure the reproducibility of our results. We demonstrate that our approach outperforms existing thematic recommendation methods and is even competitive with collaborative filtering approaches.

</p>
</details>

<details><summary><b>Priberam at MESINESP Multi-label Classification of Medical Texts Task</b>
<a href="https://arxiv.org/abs/2105.05614">arxiv:2105.05614</a>
&#x1F4C8; 3 <br>
<p>Ruben Cardoso, Zita Marinho, Afonso Mendes, Sebastião Miranda</p></summary>
<p>

**Abstract:** Medical articles provide current state of the art treatments and diagnostics to many medical practitioners and professionals. Existing public databases such as MEDLINE contain over 27 million articles, making it difficult to extract relevant content without the use of efficient search engines. Information retrieval tools are crucial in order to navigate and provide meaningful recommendations for articles and treatments. Classifying these articles into broader medical topics can improve the retrieval of related articles. The set of medical labels considered for the MESINESP task is on the order of several thousands of labels (DeCS codes), which falls under the extreme multi-label classification problem. The heterogeneous and highly hierarchical structure of medical topics makes the task of manually classifying articles extremely laborious and costly. It is, therefore, crucial to automate the process of classification. Typical machine learning algorithms become computationally demanding with such a large number of labels and achieving better recall on such datasets becomes an unsolved problem.
  This work presents Priberam's participation at the BioASQ task Mesinesp. We address the large multi-label classification problem through the use of four different models: a Support Vector Machine (SVM), a customised search engine (Priberam Search), a BERT based classifier, and a SVM-rank ensemble of all the previous models. Results demonstrate that all three individual models perform well and the best performance is achieved by their ensemble, granting Priberam the 6th place in the present challenge and making it the 2nd best team.

</p>
</details>

<details><summary><b>"Alexa, what do you do for fun?" Characterizing playful requests with virtual assistants</b>
<a href="https://arxiv.org/abs/2105.05571">arxiv:2105.05571</a>
&#x1F4C8; 3 <br>
<p>Chen Shani, Alexander Libov, Sofia Tolmach, Liane Lewin-Eytan, Yoelle Maarek, Dafna Shahaf</p></summary>
<p>

**Abstract:** Virtual assistants such as Amazon's Alexa, Apple's Siri, Google Home, and Microsoft's Cortana, are becoming ubiquitous in our daily lives and successfully help users in various daily tasks, such as making phone calls or playing music. Yet, they still struggle with playful utterances, which are not meant to be interpreted literally. Examples include jokes or absurd requests or questions such as, "Are you afraid of the dark?", "Who let the dogs out?", or "Order a zillion gummy bears". Today, virtual assistants often return irrelevant answers to such utterances, except for hard-coded ones addressed by canned replies.
  To address the challenge of automatically detecting playful utterances, we first characterize the different types of playful human-virtual assistant interaction. We introduce a taxonomy of playful requests rooted in theories of humor and refined by analyzing real-world traffic from Alexa. We then focus on one node, personification, where users refer to the virtual assistant as a person ("What do you do for fun?"). Our conjecture is that understanding such utterances will improve user experience with virtual assistants. We conducted a Wizard-of-Oz user study and showed that endowing virtual assistant s with the ability to identify humorous opportunities indeed has the potential to increase user satisfaction. We hope this work will contribute to the understanding of the landscape of the problem and inspire novel ideas and techniques towards the vision of giving virtual assistants a sense of humor.

</p>
</details>

<details><summary><b>Some Pragmatic Prevention's Guidelines regarding SARS-CoV-2 and COVID-19 in Latin-America inspired by mixed Machine Learning Techniques and Artificial Mathematical Intelligence. Case Study: Colombia</b>
<a href="https://arxiv.org/abs/2105.12213">arxiv:2105.12213</a>
&#x1F4C8; 2 <br>
<p>Danny A. J. Gomez-Ramirez, Yoe A. Herrera-Jaramillo, Johana C. Ortega-Giraldo, Alex M. Ardila-Garcia</p></summary>
<p>

**Abstract:** We use an enhanced methodology combining specific forms of AI techniques, opinion mining and artificial mathematical intelligence (AMI), with public data on the spread of the coronavirus SARS-CoV-2 and the incidence of COVID-19 disease in Colombia during the first three months since the first reported positive case. The results obtained, together with conceptual tools coming from the global taxonomy of fundamental cognitive mechanisms emerging in AMI and with suitable contextual information from Colombian public health and mainstream social media, allowed us to stating specific preventive guidelines for a better restructuring of initial safe and stable life conditions in Colombia, and in an extended manner in similar Latin American Countries. More specifically, we describe three major guidelines: 1) regular creative visualization and effective planning, 2) the continuous use of constructive linguistic frameworks, and 3) frequent and moderate use of kinesthetic routines. They should be understood as effective tools from a cognitive and behavioural perspective, rather than from a biological one. Even more, the first two guidelines should be acknowledged in integral cooperation with the third one regarding the global effect of COVID-19 in human beings as a whole, this includes the mind and body.

</p>
</details>

<details><summary><b>Good and Bad Optimization Models: Insights from Rockafellians</b>
<a href="https://arxiv.org/abs/2105.06073">arxiv:2105.06073</a>
&#x1F4C8; 2 <br>
<p>Johannes O. Royset</p></summary>
<p>

**Abstract:** A basic requirement for a mathematical model is often that its solution (output) shouldn't change much if the model's parameters (input) are perturbed. This is important because the exact values of parameters may not be known and one would like to avoid being mislead by an output obtained using incorrect values. Thus, it's rarely enough to address an application by formulating a model, solving the resulting optimization problem and presenting the solution as the answer. One would need to confirm that the model is suitable, i.e., "good," and this can, at least in part, be achieved by considering a family of optimization problems constructed by perturbing parameters of concern. The resulting sensitivity analysis uncovers troubling situations with unstable solutions, which we referred to as "bad" models, and indicates better model formulations. Embedding an actual problem of interest within a family of problems is also a primary path to optimality conditions as well as computationally attractive, alternative problems, which under ideal circumstances, and when properly tuned, may even furnish the minimum value of the actual problem. The tuning of these alternative problems turns out to be intimately tied to finding multipliers in optimality conditions and thus emerges as a main component of several optimization algorithms. In fact, the tuning amounts to solving certain dual optimization problems. In this tutorial, we'll discuss the opportunities and insights afforded by this broad perspective.

</p>
</details>

<details><summary><b>Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective</b>
<a href="https://arxiv.org/abs/2105.06018">arxiv:2105.06018</a>
&#x1F4C8; 2 <br>
<p>Bin Liu</p></summary>
<p>

**Abstract:** This paper is concerned with multi-modal data fusion (MMDF) under unexpected modality failures in nonlinear non-Gaussian dynamic processes. An efficient framework to tackle this problem is proposed. In particular, a notion termed modality "\emph{usefulness}", which takes a value of 1 or 0, is used for indicating whether the observation of this modality is useful or not. For $n$ modalities involved, $2^n$ combinations of their "\emph{usefulness}" values exist. Each combination defines one hypothetical model of the true data generative process. Then the problem of concern is formalized as a task of nonlinear non-Gaussian state filtering under model uncertainty, which is addressed by a dynamic model averaging (DMA) based particle filter (PF) algorithm. This DMA algorithm employs $2^n$ models, while all models share the same state-transition function and a unique set of particle values. That makes its computational complexity only slightly larger than a single model based PF algorithm, especially for scenarios in which $n$ is small. Experimental results show that the proposed solution outperforms remarkably state-of-the-art methods. Code and data are available at https://github.com/robinlau1981/fusion.

</p>
</details>

<details><summary><b>Spelling Correction with Denoising Transformer</b>
<a href="https://arxiv.org/abs/2105.05977">arxiv:2105.05977</a>
&#x1F4C8; 2 <br>
<p>Alex Kuznetsov, Hector Urdiales</p></summary>
<p>

**Abstract:** We present a novel method of performing spelling correction on short input strings, such as search queries or individual words. At its core lies a procedure for generating artificial typos which closely follow the error patterns manifested by humans. This procedure is used to train the production spelling correction model based on a transformer architecture. This model is currently served in the HubSpot product search. We show that our approach to typo generation is superior to the widespread practice of adding noise, which ignores human patterns. We also demonstrate how our approach may be extended to resource-scarce settings and train spelling correction models for Arabic, Greek, Russian, and Setswana languages, without using any labeled data.

</p>
</details>

<details><summary><b>Analysing The Impact Of Linguistic Features On Cross-Lingual Transfer</b>
<a href="https://arxiv.org/abs/2105.05975">arxiv:2105.05975</a>
&#x1F4C8; 2 <br>
<p>Błażej Dolicki, Gerasimos Spanakis</p></summary>
<p>

**Abstract:** There is an increasing amount of evidence that in cases with little or no data in a target language, training on a different language can yield surprisingly good results. However, currently there are no established guidelines for choosing the training (source) language. In attempt to solve this issue we thoroughly analyze a state-of-the-art multilingual model and try to determine what impacts good transfer between languages. As opposed to the majority of multilingual NLP literature, we don't only train on English, but on a group of almost 30 languages. We show that looking at particular syntactic features is 2-4 times more helpful in predicting the performance than an aggregated syntactic similarity. We find out that the importance of syntactic features strongly differs depending on the downstream task - no single feature is a good performance predictor for all NLP tasks. As a result, one should not expect that for a target language $L_1$ there is a single language $L_2$ that is the best choice for any NLP task (for instance, for Bulgarian, the best source language is French on POS tagging, Russian on NER and Thai on NLI). We discuss the most important linguistic features affecting the transfer quality using statistical and machine learning methods.

</p>
</details>

<details><summary><b>Dynamical Isometry: The Missing Ingredient for Neural Network Pruning</b>
<a href="https://arxiv.org/abs/2105.05916">arxiv:2105.05916</a>
&#x1F4C8; 2 <br>
<p>Huan Wang, Can Qin, Yue Bai, Yun Fu</p></summary>
<p>

**Abstract:** Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly. Unfortunately, the reason behind it remains elusive up to date. This paper is meant to explain it through the lens of dynamical isometry [42]. Specifically, we examine neural network pruning from an unusual perspective: pruning as initialization for finetuning, and ask whether the inherited weights serve as a good initialization for the finetuning? The insights from dynamical isometry suggest a negative answer. Despite its critical role, this issue has not been well-recognized by the community so far. In this paper, we will show the understanding of this problem is very important -- on top of explaining the aforementioned mystery about the larger finetuning rate, it also unveils the mystery about the value of pruning [5, 30]. Besides a clearer theoretical understanding of pruning, resolving the problem can also bring us considerable performance benefits in practice.

</p>
</details>

<details><summary><b>The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs</b>
<a href="https://arxiv.org/abs/2105.05911">arxiv:2105.05911</a>
&#x1F4C8; 2 <br>
<p>Christopher Morris, Matthias Fey, Nils M. Kriege</p></summary>
<p>

**Abstract:** In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, emerged as a powerful tool for (supervised) machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine learning setting. We discuss the theoretical background, show how to use it for supervised graph- and node classification, discuss recent extensions, and its connection to neural architectures. Moreover, we give an overview of current applications and future directions to stimulate research.

</p>
</details>

<details><summary><b>Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning</b>
<a href="https://arxiv.org/abs/2105.05883">arxiv:2105.05883</a>
&#x1F4C8; 2 <br>
<p>Yann Fraboni, Richard Vidal, Laetitia Kameni, Marco Lorenzi</p></summary>
<p>

**Abstract:** This work addresses the problem of optimizing communications between server and clients in federated learning (FL). Current sampling approaches in FL are either biased, or non optimal in terms of server-clients communications and training stability. To overcome this issue, we introduce \textit{clustered sampling} for clients selection. We prove that clustered sampling leads to better clients representatitivity and to reduced variance of the clients stochastic aggregation weights in FL. Compatibly with our theory, we provide two different clustering approaches enabling clients aggregation based on 1) sample size, and 2) models similarity. Through a series of experiments in non-iid and unbalanced scenarios, we demonstrate that model aggregation through clustered sampling consistently leads to better training convergence and variability when compared to standard sampling approaches. Our approach does not require any additional operation on the clients side, and can be seamlessly integrated in standard FL implementations. Finally, clustered sampling is compatible with existing methods and technologies for privacy enhancement, and for communication reduction through model compression.

</p>
</details>

<details><summary><b>Out of the Box: Embodied Navigation in the Real World</b>
<a href="https://arxiv.org/abs/2105.05873">arxiv:2105.05873</a>
&#x1F4C8; 2 <br>
<p>Roberto Bigazzi, Federico Landi, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara</p></summary>
<p>

**Abstract:** The research field of Embodied AI has witnessed substantial progress in visual navigation and exploration thanks to powerful simulating platforms and the availability of 3D data of indoor and photorealistic environments. These two factors have opened the doors to a new generation of intelligent agents capable of achieving nearly perfect PointGoal Navigation. However, such architectures are commonly trained with millions, if not billions, of frames and tested in simulation. Together with great enthusiasm, these results yield a question: how many researchers will effectively benefit from these advances? In this work, we detail how to transfer the knowledge acquired in simulation into the real world. To that end, we describe the architectural discrepancies that damage the Sim2Real adaptation ability of models trained on the Habitat simulator and propose a novel solution tailored towards the deployment in real-world scenarios. We then deploy our models on a LoCoBot, a Low-Cost Robot equipped with a single Intel RealSense camera. Different from previous work, our testing scene is unavailable to the agent in simulation. The environment is also inaccessible to the agent beforehand, so it cannot count on scene-specific semantic priors. In this way, we reproduce a setting in which a research group (potentially from other fields) needs to employ the agent visual navigation capabilities as-a-Service. Our experiments indicate that it is possible to achieve satisfying results when deploying the obtained model in the real world. Our code and models are available at https://github.com/aimagelab/LoCoNav.

</p>
</details>

<details><summary><b>20-fold Accelerated 7T fMRI Using Referenceless Self-Supervised Deep Learning Reconstruction</b>
<a href="https://arxiv.org/abs/2105.05827">arxiv:2105.05827</a>
&#x1F4C8; 2 <br>
<p>Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller, Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, Kâmil Uğurbil, Mehmet Akçakaya</p></summary>
<p>

**Abstract:** High spatial and temporal resolution across the whole brain is essential to accurately resolve neural activities in fMRI. Therefore, accelerated imaging techniques target improved coverage with high spatio-temporal resolution. Simultaneous multi-slice (SMS) imaging combined with in-plane acceleration are used in large studies that involve ultrahigh field fMRI, such as the Human Connectome Project. However, for even higher acceleration rates, these methods cannot be reliably utilized due to aliasing and noise artifacts. Deep learning (DL) reconstruction techniques have recently gained substantial interest for improving highly-accelerated MRI. Supervised learning of DL reconstructions generally requires fully-sampled training datasets, which is not available for high-resolution fMRI studies. To tackle this challenge, self-supervised learning has been proposed for training of DL reconstruction with only undersampled datasets, showing similar performance to supervised learning. In this study, we utilize a self-supervised physics-guided DL reconstruction on a 5-fold SMS and 4-fold in-plane accelerated 7T fMRI data. Our results show that our self-supervised DL reconstruction produce high-quality images at this 20-fold acceleration, substantially improving on existing methods, while showing similar functional precision and temporal effects in the subsequent analysis compared to a standard 10-fold accelerated acquisition.

</p>
</details>

<details><summary><b>Building a Question and Answer System for News Domain</b>
<a href="https://arxiv.org/abs/2105.05744">arxiv:2105.05744</a>
&#x1F4C8; 2 <br>
<p>Sandipan Basu, Aravind Gaddala, Pooja Chetan, Garima Tiwari, Narayana Darapaneni, Sadwik Parvathaneni, Anwesh Reddy Paduri</p></summary>
<p>

**Abstract:** This project attempts to build a Question- Answering system in the News Domain, where Passages will be News articles, and anyone can ask a Question against it. We have built a span-based model using an Attention mechanism, where the model predicts the answer to a question as to the position of the start and end tokens in a paragraph. For training our model, we have used the Stanford Question and Answer (SQuAD 2.0) dataset[1]. To do well on SQuAD 2.0, systems must not only answer questions when possible but also determine when no answer is supported by the paragraph and abstain from answering. Our model architecture comprises three layers- Embedding Layer, RNN Layer, and the Attention Layer. For the Embedding layer, we used GloVe and the Universal Sentence Encoder. For the RNN Layer, we built variations of the RNN Layer including bi-LSTM and Stacked LSTM and we built an Attention Layer using a Context to Question Attention and also improvised on the innovative Bidirectional Attention Layer. Our best performing model which uses GloVe Embedding combined with Bi-LSTM and Context to Question Attention achieved an F1 Score and EM of 33.095 and 33.094 respectively. We also leveraged transfer learning and built a Transformer based model using BERT. The BERT-based model achieved an F1 Score and EM of 57.513 and 49.769 respectively. We concluded that the BERT model is superior in all aspects of answering various types of questions.

</p>
</details>

<details><summary><b>Early prediction of respiratory failure in the intensive care unit</b>
<a href="https://arxiv.org/abs/2105.05728">arxiv:2105.05728</a>
&#x1F4C8; 2 <br>
<p>Matthias Hüser, Martin Faltys, Xinrui Lyu, Chris Barber, Stephanie L. Hyland, Tobias M. Merz, Gunnar Rätsch</p></summary>
<p>

**Abstract:** The development of respiratory failure is common among patients in intensive care units (ICU). Large data quantities from ICU patient monitoring systems make timely and comprehensive analysis by clinicians difficult but are ideal for automatic processing by machine learning algorithms. Early prediction of respiratory system failure could alert clinicians to patients at risk of respiratory failure and allow for early patient reassessment and treatment adjustment. We propose an early warning system that predicts moderate/severe respiratory failure up to 8 hours in advance. Our system was trained on HiRID-II, a data-set containing more than 60,000 admissions to a tertiary care ICU. An alarm is typically triggered several hours before the beginning of respiratory failure. Our system outperforms a clinical baseline mimicking traditional clinical decision-making based on pulse-oximetric oxygen saturation and the fraction of inspired oxygen. To provide model introspection and diagnostics, we developed an easy-to-use web browser-based system to explore model input data and predictions visually.

</p>
</details>

<details><summary><b>An Efficient Learning Framework For Federated XGBoost Using Secret Sharing And Distributed Optimization</b>
<a href="https://arxiv.org/abs/2105.05717">arxiv:2105.05717</a>
&#x1F4C8; 2 <br>
<p>Lunchen Xie, Jiaqi Liu, Songtao Lu, Tsung-hui Chang, Qingjiang Shi</p></summary>
<p>

**Abstract:** XGBoost is one of the most widely used machine learning models in the industry due to its superior learning accuracy and efficiency. Targeting at data isolation issues in the big data problems, it is crucial to deploy a secure and efficient federated XGBoost (FedXGB) model. Existing FedXGB models either have data leakage issues or are only applicable to the two-party setting with heavy communication and computation overheads. In this paper, a lossless multi-party federated XGB learning framework is proposed with a security guarantee, which reshapes the XGBoost's split criterion calculation process under a secret sharing setting and solves the leaf weight calculation problem by leveraging distributed optimization. Remarkably, a thorough analysis of model security is provided as well, and multiple numerical results showcase the superiority of the proposed FedXGB compared with the state-of-the-art models on benchmark datasets.

</p>
</details>

<details><summary><b>Acting upon Imagination: when to trust imagined trajectories in model based reinforcement learning</b>
<a href="https://arxiv.org/abs/2105.05716">arxiv:2105.05716</a>
&#x1F4C8; 2 <br>
<p>Adrian Remonda, Eduardo Veas, Granit Luzhnica</p></summary>
<p>

**Abstract:** Model based reinforcement learning (MBRL) uses an imperfect model of the world to imagine trajectories of future states and plan the best actions to maximize a reward function. These trajectories are imperfect and MBRL attempts to overcome this by relying on model predictive control (MPC) to continuously re-imagine trajectories from scratch. Such re-generation of imagined trajectories carries the major computational cost and increasing complexity in tasks with longer receding horizon. This paper aims to investigate how far in the future the imagined trajectories can be relied upon while still maintaining acceptable reward. Firstly, an error analysis is presented for systematic skipping recalculations for varying number of consecutive steps.% in several challenging benchmark control tasks. Secondly, we propose two methods offering when to trust and act upon imagined trajectories, looking at recent errors with respect to expectations, or comparing the confidence in an action imagined against its execution. Thirdly, we evaluate the effects of acting upon imagination while training the model of the world. Results show that acting upon imagination can reduce calculations by at least 20% and up to 80%, depending on the environment, while retaining acceptable reward.

</p>
</details>

<details><summary><b>Look-Ahead Screening Rules for the Lasso</b>
<a href="https://arxiv.org/abs/2105.05648">arxiv:2105.05648</a>
&#x1F4C8; 2 <br>
<p>Johan Larsson</p></summary>
<p>

**Abstract:** The lasso is a popular method to induce shrinkage and sparsity in the solution vector (coefficients) of regression problems, particularly when there are many predictors relative to the number of observations. Solving the lasso in this high-dimensional setting can, however, be computationally demanding. Fortunately, this demand can be alleviated via the use of screening rules that discard predictors prior to fitting the model, leading to a reduced problem to be solved. In this paper, we present a new screening strategy: look-ahead screening. Our method uses safe screening rules to find a range of penalty values for which a given predictor cannot enter the model, thereby screening predictors along the remainder of the path. In experiments we show that these look-ahead screening rules outperform the active warm-start version of the Gap Safe rules.

</p>
</details>

<details><summary><b>Cross-Modal and Multimodal Data Analysis Based on Functional Mapping of Spectral Descriptors and Manifold Regularization</b>
<a href="https://arxiv.org/abs/2105.05631">arxiv:2105.05631</a>
&#x1F4C8; 2 <br>
<p>Maysam Behmanesh, Peyman Adibi, Jocelyn Chanussot, Sayyed Mohammad Saeed Ehsani</p></summary>
<p>

**Abstract:** Multimodal manifold modeling methods extend the spectral geometry-aware data analysis to learning from several related and complementary modalities. Most of these methods work based on two major assumptions: 1) there are the same number of homogeneous data samples in each modality, and 2) at least partial correspondences between modalities are given in advance as prior knowledge. This work proposes two new multimodal modeling methods. The first method establishes a general analyzing framework to deal with the multimodal information problem for heterogeneous data without any specific prior knowledge. For this purpose, first, we identify the localities of each manifold by extracting local descriptors via spectral graph wavelet signatures (SGWS). Then, we propose a manifold regularization framework based on the functional mapping between SGWS descriptors (FMBSD) for finding the pointwise correspondences. The second method is a manifold regularized multimodal classification based on pointwise correspondences (M$^2$CPC) used for the problem of multiclass classification of multimodal heterogeneous, which the correspondences between modalities are determined based on the FMBSD method. The experimental results of evaluating the FMBSD method on three common cross-modal retrieval datasets and evaluating the (M$^2$CPC) method on three benchmark multimodal multiclass classification datasets indicate their effectiveness and superiority over state-of-the-art methods.

</p>
</details>

<details><summary><b>NLP for Climate Policy: Creating a Knowledge Platform for Holistic and Effective Climate Action</b>
<a href="https://arxiv.org/abs/2105.05621">arxiv:2105.05621</a>
&#x1F4C8; 2 <br>
<p>Pradip Swarnakar, Ashutosh Modi</p></summary>
<p>

**Abstract:** Climate change is a burning issue of our time, with the Sustainable Development Goal (SDG) 13 of the United Nations demanding global climate action. Realizing the urgency, in 2015 in Paris, world leaders signed an agreement committing to taking voluntary action to reduce carbon emissions. However, the scale, magnitude, and climate action processes vary globally, especially between developed and developing countries. Therefore, from parliament to social media, the debates and discussions on climate change gather data from wide-ranging sources essential to the policy design and implementation. The downside is that we do not currently have the mechanisms to pool the worldwide dispersed knowledge emerging from the structured and unstructured data sources.
  The paper thematically discusses how NLP techniques could be employed in climate policy research and contribute to society's good at large. In particular, we exemplify symbiosis of NLP and Climate Policy Research via four methodologies. The first one deals with the major topics related to climate policy using automated content analysis. We investigate the opinions (sentiments) of major actors' narratives towards climate policy in the second methodology. The third technique explores the climate actors' beliefs towards pro or anti-climate orientation. Finally, we discuss developing a Climate Knowledge Graph.
  The present theme paper further argues that creating a knowledge platform would help in the formulation of a holistic climate policy and effective climate action. Such a knowledge platform would integrate the policy actors' varied opinions from different social sectors like government, business, civil society, and the scientific community. The research outcome will add value to effective climate action because policymakers can make informed decisions by looking at the diverse public opinion on a comprehensive platform.

</p>
</details>

<details><summary><b>Priberam Labs at the NTCIR-15 SHINRA2020-ML: Classification Task</b>
<a href="https://arxiv.org/abs/2105.05605">arxiv:2105.05605</a>
&#x1F4C8; 2 <br>
<p>Ruben Cardoso, Afonso Mendes, Andre Lamurias</p></summary>
<p>

**Abstract:** Wikipedia is an online encyclopedia available in 285 languages. It composes an extremely relevant Knowledge Base (KB), which could be leveraged by automatic systems for several purposes. However, the structure and organisation of such information are not prone to automatic parsing and understanding and it is, therefore, necessary to structure this knowledge. The goal of the current SHINRA2020-ML task is to leverage Wikipedia pages in order to categorise their corresponding entities across 268 hierarchical categories, belonging to the Extended Named Entity (ENE) ontology. In this work, we propose three distinct models based on the contextualised embeddings yielded by Multilingual BERT. We explore the performances of a linear layer with and without explicit usage of the ontology's hierarchy, and a Gated Recurrent Units (GRU) layer. We also test several pooling strategies to leverage BERT's embeddings and selection criteria based on the labels' scores. We were able to achieve good performance across a large variety of languages, including those not seen during the fine-tuning process (zero-shot languages).

</p>
</details>

<details><summary><b>OutFlip: Generating Out-of-Domain Samples for Unknown Intent Detection with Natural Language Attack</b>
<a href="https://arxiv.org/abs/2105.05601">arxiv:2105.05601</a>
&#x1F4C8; 2 <br>
<p>DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, Dong Ryeol Shin</p></summary>
<p>

**Abstract:** Out-of-domain (OOD) input detection is vital in a task-oriented dialogue system since the acceptance of unsupported inputs could lead to an incorrect response of the system. This paper proposes OutFlip, a method to generate out-of-domain samples using only in-domain training dataset automatically. A white-box natural language attack method HotFlip is revised to generate out-of-domain samples instead of adversarial examples. Our evaluation results showed that integrating OutFlip-generated out-of-domain samples into the training dataset could significantly improve an intent classification model's out-of-domain detection performance.

</p>
</details>

<details><summary><b>StutterNet: Stuttering Detection Using Time Delay Neural Network</b>
<a href="https://arxiv.org/abs/2105.05599">arxiv:2105.05599</a>
&#x1F4C8; 2 <br>
<p>Shakeel A. Sheikh, Md Sahidullah, Fabrice Hirsch, Slim Ouni</p></summary>
<p>

**Abstract:** This paper introduces StutterNet, a novel deep learning based stuttering detection capable of detecting and identifying various types of disfluencies. Most of the existing work in this domain uses automatic speech recognition (ASR) combined with language models for stuttering detection. Compared to the existing work, which depends on the ASR module, our method relies solely on the acoustic signal. We use a time-delay neural network (TDNN) suitable for capturing contextual aspects of the disfluent utterances. We evaluate our system on the UCLASS stuttering dataset consisting of more than 100 speakers. Our method achieves promising results and outperforms the state-of-the-art residual neural network based method. The number of trainable parameters of the proposed method is also substantially less due to the parameter sharing scheme of TDNN.

</p>
</details>

<details><summary><b>Structural risk minimization for quantum linear classifiers</b>
<a href="https://arxiv.org/abs/2105.05566">arxiv:2105.05566</a>
&#x1F4C8; 2 <br>
<p>Casper Gyurik, Dyon van Vreumingen, Vedran Dunjko</p></summary>
<p>

**Abstract:** Quantum machine learning (QML) models based on parameterized quantum circuits are often highlighted as candidates for quantum computing's near-term "killer application". However, the understanding of the empirical and generalization performance of these models is still in its infancy. In this paper, we study how to balance between training accuracy and generalization performance (also called structural risk minimization) for two prominent QML models introduced by Havlíček et al. (Nature, 2019), and Schuld and Killoran (PRL, 2019). Firstly, using relationships to well-understood classical models, we prove that two model parameters -- i.e., the dimension of the sum of the images and the Frobenius norm of the observables used by the model -- closely control the models' complexity and therefore its generalization performance. Secondly, using ideas inspired by process tomography, we prove that these model parameters also closely control the models' ability to capture correlations in sets of training examples. In summary, our results give rise to new options for structural risk minimization for QML models.

</p>
</details>

<details><summary><b>Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time</b>
<a href="https://arxiv.org/abs/2105.05555">arxiv:2105.05555</a>
&#x1F4C8; 2 <br>
<p>Yu Cheng, Honghao Lin</p></summary>
<p>

**Abstract:** We study the problem of learning Bayesian networks where an $ε$-fraction of the samples are adversarially corrupted. We focus on the fully-observable case where the underlying graph structure is known. In this work, we present the first nearly-linear time algorithm for this problem with a dimension-independent error guarantee. Previous robust algorithms with comparable error guarantees are slower by at least a factor of $(d/ε)$, where $d$ is the number of variables in the Bayesian network and $ε$ is the fraction of corrupted samples.
  Our algorithm and analysis are considerably simpler than those in previous work. We achieve this by establishing a direct connection between robust learning of Bayesian networks and robust mean estimation. As a subroutine in our algorithm, we develop a robust mean estimation algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples, which may be of independent interest.

</p>
</details>

<details><summary><b>Evaluating Gender Bias in Natural Language Inference</b>
<a href="https://arxiv.org/abs/2105.05541">arxiv:2105.05541</a>
&#x1F4C8; 2 <br>
<p>Shanya Sharma, Manan Dey, Koustuv Sinha</p></summary>
<p>

**Abstract:** Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of gender bias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task that involves pairing gender-neutral premises against a gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are significantly prone to gender-induced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.

</p>
</details>

<details><summary><b>Cyclically Equivariant Neural Decoders for Cyclic Codes</b>
<a href="https://arxiv.org/abs/2105.05540">arxiv:2105.05540</a>
&#x1F4C8; 2 <br>
<p>Xiangyu Chen, Min Ye</p></summary>
<p>

**Abstract:** Neural decoders were introduced as a generalization of the classic Belief Propagation (BP) decoding algorithms, where the Trellis graph in the BP algorithm is viewed as a neural network, and the weights in the Trellis graph are optimized by training the neural network. In this work, we propose a novel neural decoder for cyclic codes by exploiting their cyclically invariant property. More precisely, we impose a shift invariant structure on the weights of our neural decoder so that any cyclic shift of inputs results in the same cyclic shift of outputs. Extensive simulations with BCH codes and punctured Reed-Muller (RM) codes show that our new decoder consistently outperforms previous neural decoders when decoding cyclic codes. Finally, we propose a list decoding procedure that can significantly reduce the decoding error probability for BCH codes and punctured RM codes. For certain high-rate codes, the gap between our list decoder and the Maximum Likelihood decoder is less than $0.1$dB. Code available at https://github.com/cyclicallyneuraldecoder/CyclicallyEquivariantNeuralDecoders

</p>
</details>

<details><summary><b>Interpretable performance analysis towards offline reinforcement learning: A dataset perspective</b>
<a href="https://arxiv.org/abs/2105.05473">arxiv:2105.05473</a>
&#x1F4C8; 2 <br>
<p>Chenyang Xi, Bo Tang, Jiajun Shen, Xinfu Liu, Feiyu Xiong, Xueying Li</p></summary>
<p>

**Abstract:** Offline reinforcement learning (RL) has increasingly become the focus of the artificial intelligent research due to its wide real-world applications where the collection of data may be difficult, time-consuming, or costly. In this paper, we first propose a two-fold taxonomy for existing offline RL algorithms from the perspective of exploration and exploitation tendency. Secondly, we derive the explicit expression of the upper bound of extrapolation error and explore the correlation between the performance of different types of algorithms and the distribution of actions under states. Specifically, we relax the strict assumption on the sufficiently large amount of state-action tuples. Accordingly, we provably explain why batch constrained Q-learning (BCQ) performs better than other existing techniques. Thirdly, after identifying the weakness of BCQ on dataset of low mean episode returns, we propose a modified variant based on top return selection mechanism, which is proved to be able to gain state-of-the-art performance on various datasets. Lastly, we create a benchmark platform on the Atari domain, entitled RL easy go (RLEG), at an estimated cost of more than 0.3 million dollars. We make it open-source for fair and comprehensive competitions between offline RL algorithms with complete datasets and checkpoints being provided.

</p>
</details>

<details><summary><b>High-Performance FPGA-based Accelerator for Bayesian Neural Networks</b>
<a href="https://arxiv.org/abs/2105.09163">arxiv:2105.09163</a>
&#x1F4C8; 1 <br>
<p>Hongxiang Fan, Martin Ferianc, Miguel Rodrigues, Hongyu Zhou, Xinyu Niu, Wayne Luk</p></summary>
<p>

**Abstract:** Neural networks (NNs) have demonstrated their potential in a wide range of applications such as image recognition, decision making or recommendation systems. However, standard NNs are unable to capture their model uncertainty which is crucial for many safety-critical applications including healthcare and autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to express uncertainty in their prediction via a mathematical grounding. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their expensive computational cost and limited hardware performance. This work proposes a novel FPGA-based hardware architecture to accelerate BNNs inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN accelerators, the proposed accelerator can achieve up to 4 times higher energy efficiency and 9 times better compute efficiency. Considering partial Bayesian inference, an automatic framework is proposed, which explores the trade-off between hardware and algorithmic performance. Extensive experiments are conducted to demonstrate that our proposed framework can effectively find the optimal points in the design space.

</p>
</details>

<details><summary><b>A one-armed CNN for exoplanet detection from light curves</b>
<a href="https://arxiv.org/abs/2105.06292">arxiv:2105.06292</a>
&#x1F4C8; 1 <br>
<p>Koko Visser, Bas Bosma, Eric Postma</p></summary>
<p>

**Abstract:** We propose Genesis, a one-armed simplified Convolutional Neural Network (CNN)for exoplanet detection, and compare it to the more complex, two-armed CNN called Astronet. Furthermore, we examine how Monte Carlo cross-validation affects the estimation of the exoplanet detection performance. Finally, we increase the input resolution twofold to assess its effect on performance. The experiments reveal that (i)the reduced complexity of Genesis, i.e., a more than 95% reduction in the number of free parameters, incurs a small performance cost of about 0.5% compared to Astronet, (ii) Monte Carlo cross-validation provides a more realistic performance estimate that is almost 0.7% below the original estimate, and (iii) the twofold increase in input resolution decreases the average performance by about 0.5%. We conclude by arguing that further exploration of shallower CNN architectures may be beneficial in order to improve the generalizability of CNN-based exoplanet detection across surveys.

</p>
</details>

<details><summary><b>Model Pruning Based on Quantified Similarity of Feature Maps</b>
<a href="https://arxiv.org/abs/2105.06052">arxiv:2105.06052</a>
&#x1F4C8; 1 <br>
<p>Zidu Wang, Xuexin Liu, Long Huang, Yunqing Chen, Yufei Zhang, Zhikang Lin, Rui Wang</p></summary>
<p>

**Abstract:** A high-accuracy CNN is often accompanied by huge parameters, which are usually stored in the high-dimensional tensors. However, there are few methods can figure out the redundant information of the parameters stored in the high-dimensional tensors, which leads to the lack of theoretical guidance for the compression of CNNs. In this paper, we propose a novel theory to find redundant information in three dimensional tensors, namely Quantified Similarity of Feature Maps (QSFM), and use this theory to prune convolutional neural networks to enhance the inference speed. Our method belongs to filter pruning, which can be implemented without using any special libraries. We perform our method not only on common convolution layers but also on special convolution layers, such as depthwise separable convolution layers. The experiments prove that QSFM can find the redundant information in the neural network effectively. Without any fine-tuning operation, QSFM can compress ResNet-56 on CIFAR-10 significantly (48.27% FLOPs and 57.90% parameters reduction) with only a loss of 0.54% in the top-1 accuracy. QSFM also prunes ResNet-56, VGG-16 and MobileNetV2 with fine-tuning operation, which also shows excellent results.

</p>
</details>

<details><summary><b>Joint Community Detection and Rotational Synchronization via Semidefinite Programming</b>
<a href="https://arxiv.org/abs/2105.06031">arxiv:2105.06031</a>
&#x1F4C8; 1 <br>
<p>Yifeng Fan, Yuehaw Khoo, Zhizhen Zhao</p></summary>
<p>

**Abstract:** In the presence of heterogeneous data, where randomly rotated objects fall into multiple underlying categories, it is challenging to simultaneously classify them into clusters and synchronize them based on pairwise relations. This gives rise to the joint problem of community detection and synchronization. We propose a series of semidefinite relaxations, and prove their exact recovery when extending the celebrated stochastic block model to this new setting where both rotations and cluster identities are to be determined. Numerical experiments demonstrate the efficacy of our proposed algorithms and confirm our theoretical result which indicates a sharp phase transition for exact recovery.

</p>
</details>

<details><summary><b>An Open-Source Multi-Goal Reinforcement Learning Environment for Robotic Manipulation with Pybullet</b>
<a href="https://arxiv.org/abs/2105.05985">arxiv:2105.05985</a>
&#x1F4C8; 1 <br>
<p>Xintong Yang, Ze Ji, Jing Wu, Yu-Kun Lai</p></summary>
<p>

**Abstract:** This work re-implements the OpenAI Gym multi-goal robotic manipulation environment, originally based on the commercial Mujoco engine, onto the open-source Pybullet engine. By comparing the performances of the Hindsight Experience Replay-aided Deep Deterministic Policy Gradient agent on both environments, we demonstrate our successful re-implementation of the original environment. Besides, we provide users with new APIs to access a joint control mode, image observations and goals with customisable camera and a built-in on-hand camera. We further design a set of multi-step, multi-goal, long-horizon and sparse reward robotic manipulation tasks, aiming to inspire new goal-conditioned reinforcement learning algorithms for such challenges. We use a simple, human-prior-based curriculum learning method to benchmark the multi-step manipulation tasks. Discussions about future research opportunities regarding this kind of tasks are also provided.

</p>
</details>

<details><summary><b>An Open-Source Tool for Classification Models in Resource-Constrained Hardware</b>
<a href="https://arxiv.org/abs/2105.05983">arxiv:2105.05983</a>
&#x1F4C8; 1 <br>
<p>Lucas Tsutsui da Silva, Vinicius M. A. Souza, Gustavo E. A. P. A. Batista</p></summary>
<p>

**Abstract:** Applications that need to sense, measure, and gather real-time information from the environment frequently face three main restrictions: power consumption, cost, and lack of infrastructure. Most of the challenges imposed by these limitations can be better addressed by embedding Machine Learning (ML) classifiers in the hardware that senses the environment, creating smart sensors able to interpret the low-level data stream. However, for this approach to be cost-effective, we need highly efficient classifiers suitable to execute in unresourceful hardware, such as low-power microcontrollers. In this paper, we present an open-source tool named EmbML - Embedded Machine Learning that implements a pipeline to develop classifiers for resource-constrained hardware. We describe its implementation details and provide a comprehensive analysis of its classifiers considering accuracy, classification time, and memory usage. Moreover, we compare the performance of its classifiers with classifiers produced by related tools to demonstrate that our tool provides a diverse set of classification algorithms that are both compact and accurate. Finally, we validate EmbML classifiers in a practical application of a smart sensor and trap for disease vector mosquitoes.

</p>
</details>

<details><summary><b>DONet: Dual-Octave Network for Fast MR Image Reconstruction</b>
<a href="https://arxiv.org/abs/2105.05980">arxiv:2105.05980</a>
&#x1F4C8; 1 <br>
<p>Chun-Mei Feng, Zhanyuan Yang, Huazhu Fu, Yong Xu, Jian Yang, Ling Shao</p></summary>
<p>

**Abstract:** Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration has long been the subject of research. This is commonly achieved by obtaining multiple undersampled images, simultaneously, through parallel imaging. In this paper, we propose the Dual-Octave Network (DONet), which is capable of learning multi-scale spatial-frequency features from both the real and imaginary components of MR data, for fast parallel MR image reconstruction. More specifically, our DONet consists of a series of Dual-Octave convolutions (Dual-OctConv), which are connected in a dense manner for better reuse of features. In each Dual-OctConv, the input feature maps and convolutional kernels are first split into two components (ie, real and imaginary), and then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intra-group information updating and inter-group information exchange to aggregate the contextual information across different groups. Our framework provides three appealing benefits: (i) It encourages information interaction and fusion between the real and imaginary components at various spatial frequencies to achieve richer representational capacity. (ii) The dense connections between the real and imaginary groups in each Dual-OctConv make the propagation of features more efficient by feature reuse. (iii) DONet enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. Extensive experiments on two popular datasets (ie, clinical knee and fastMRI), under different undersampling patterns and acceleration factors, demonstrate the superiority of our model in accelerated parallel MR image reconstruction.

</p>
</details>

<details><summary><b>A new perspective on low-rank optimization</b>
<a href="https://arxiv.org/abs/2105.05947">arxiv:2105.05947</a>
&#x1F4C8; 1 <br>
<p>Dimitris Bertsimas, Ryan Cory-Wright, Jean Pauphilet</p></summary>
<p>

**Abstract:** A key question in many low-rank problems throughout optimization, machine learning, and statistics is to characterize the convex hulls of simple low-rank sets and judiciously apply these convex hulls to obtain strong yet computationally tractable convex relaxations. We invoke the matrix perspective function - the matrix analog of the perspective function-and characterize explicitly the convex hull of epigraphs of convex quadratic, matrix exponential, and matrix power functions under low-rank constraints. Further, we exploit these characterizations to develop strong relaxations for a variety of low-rank problems including reduced rank regression, non-negative matrix factorization, and factor analysis. We establish that these relaxations can be modeled via semidefinite and matrix power cone constraints, and thus optimized over tractably. The proposed approach parallels and generalizes the perspective reformulation technique in mixed-integer optimization, and leads to new relaxations for a broad class of problems.

</p>
</details>

<details><summary><b>Attention-based Neural Beamforming Layers for Multi-channel Speech Recognition</b>
<a href="https://arxiv.org/abs/2105.05920">arxiv:2105.05920</a>
&#x1F4C8; 1 <br>
<p>Bhargav Pulugundla, Yang Gao, Brian King, Gokce Keskin, Harish Mallidi, Minhua Wu, Jasha Droppo, Roland Maas</p></summary>
<p>

**Abstract:** Attention-based beamformers have recently been shown to be effective for multi-channel speech recognition. However, they are less capable at capturing local information. In this work, we propose a 2D Conv-Attention module which combines convolution neural networks with attention for beamforming. We apply self- and cross-attention to explicitly model the correlations within and between the input channels. The end-to-end 2D Conv-Attention model is compared with a multi-head self-attention and superdirective-based neural beamformers. We train and evaluate on an in-house multi-channel dataset. The results show a relative improvement of 3.8% in WER by the proposed model over the baseline neural beamformer.

</p>
</details>

<details><summary><b>Better than BERT but Worse than Baseline</b>
<a href="https://arxiv.org/abs/2105.05915">arxiv:2105.05915</a>
&#x1F4C8; 1 <br>
<p>Boxiang Liu, Jiaji Huang, Xingyu Cai, Kenneth Church</p></summary>
<p>

**Abstract:** This paper compares BERT-SQuAD and Ab3P on the Abbreviation Definition Identification (ADI) task. ADI inputs a text and outputs short forms (abbreviations/acronyms) and long forms (expansions). BERT with reranking improves over BERT without reranking but fails to reach the Ab3P rule-based baseline. What is BERT missing? Reranking introduces two new features: charmatch and freq. The first feature identifies opportunities to take advantage of character constraints in acronyms and the second feature identifies opportunities to take advantage of frequency constraints across documents.

</p>
</details>

<details><summary><b>Unsupervised Acute Intracranial Hemorrhage Segmentation with Mixture Models</b>
<a href="https://arxiv.org/abs/2105.05891">arxiv:2105.05891</a>
&#x1F4C8; 1 <br>
<p>Kimmo Kärkkäinen, Shayan Fazeli, Majid Sarrafzadeh</p></summary>
<p>

**Abstract:** Intracranial hemorrhage occurs when blood vessels rupture or leak within the brain tissue or elsewhere inside the skull. It can be caused by physical trauma or by various medical conditions and in many cases leads to death. The treatment must be started as soon as possible, and therefore the hemorrhage should be diagnosed accurately and quickly. The diagnosis is usually performed by a radiologist who analyses a Computed Tomography (CT) scan containing a large number of cross-sectional images throughout the brain. Analysing each image manually can be very time-consuming, but automated techniques can help speed up the process. While much of the recent research has focused on solving this problem by using supervised machine learning algorithms, publicly-available training data remains scarce due to privacy concerns. This problem can be alleviated by unsupervised algorithms. In this paper, we propose a fully-unsupervised algorithm which is based on the mixture models. Our algorithm utilizes the fact that the properties of hemorrhage and healthy tissues follow different distributions, and therefore an appropriate formulation of these distributions allows us to separate them through an Expectation-Maximization process. In addition, our algorithm is able to adaptively determine the number of clusters such that all the hemorrhage regions can be found without including noisy voxels. We demonstrate the results of our algorithm on publicly-available datasets that contain all different hemorrhage types in various sizes and intensities, and our results are compared to earlier unsupervised and supervised algorithms. The results show that our algorithm can outperform the other algorithms with most hemorrhage types.

</p>
</details>

<details><summary><b>The Federated Tumor Segmentation (FeTS) Challenge</b>
<a href="https://arxiv.org/abs/2105.05874">arxiv:2105.05874</a>
&#x1F4C8; 1 <br>
<p>Sarthak Pati, Ujjwal Baid, Maximilian Zenk, Brandon Edwards, Micah Sheller, G. Anthony Reina, Patrick Foley, Alexey Gruzdev, Jason Martin, Shadi Albarqouni, Yong Chen, Russell Taki Shinohara, Annika Reinke, David Zimmerer, John B. Freymann, Justin S. Kirby, Christos Davatzikos, Rivka R. Colen, Aikaterini Kotrotsou, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Hassan Fathallah-Shaykh, Roland Wiest, Andras Jakab</p></summary>
<p>

**Abstract:** This manuscript describes the first challenge on Federated Learning, namely the Federated Tumor Segmentation (FeTS) challenge 2021. International challenges have become the standard for validation of biomedical image analysis methods. However, the actual performance of participating (even the winning) algorithms on "real-world" clinical data often remains unclear, as the data included in challenges are usually acquired in very controlled settings at few institutions. The seemingly obvious solution of just collecting increasingly more data from more institutions in such challenges does not scale well due to privacy and ownership hurdles. Towards alleviating these concerns, we are proposing the FeTS challenge 2021 to cater towards both the development and the evaluation of models for the segmentation of intrinsically heterogeneous (in appearance, shape, and histology) brain tumors, namely gliomas. Specifically, the FeTS 2021 challenge uses clinically acquired, multi-institutional magnetic resonance imaging (MRI) scans from the BraTS 2020 challenge, as well as from various remote independent institutions included in the collaborative network of a real-world federation (https://www.fets.ai/). The goals of the FeTS challenge are directly represented by the two included tasks: 1) the identification of the optimal weight aggregation approach towards the training of a consensus model that has gained knowledge via federated learning from multiple geographically distinct institutions, while their data are always retained within each institution, and 2) the federated evaluation of the generalizability of brain tumor segmentation models "in the wild", i.e. on data from institutional distributions that were not part of the training datasets.

</p>
</details>

<details><summary><b>How to Design Robust Algorithms using Noisy Comparison Oracle</b>
<a href="https://arxiv.org/abs/2105.05782">arxiv:2105.05782</a>
&#x1F4C8; 1 <br>
<p>Raghavendra Addanki, Sainyam Galhotra, Barna Saha</p></summary>
<p>

**Abstract:** Metric based comparison operations such as finding maximum, nearest and farthest neighbor are fundamental to studying various clustering techniques such as $k$-center clustering and agglomerative hierarchical clustering. These techniques crucially rely on accurate estimation of pairwise distance between records. However, computing exact features of the records, and their pairwise distances is often challenging, and sometimes not possible. We circumvent this challenge by leveraging weak supervision in the form of a comparison oracle that compares the relative distance between the queried points such as `Is point u closer to v or w closer to x?'.
  However, it is possible that some queries are easier to answer than others using a comparison oracle. We capture this by introducing two different noise models called adversarial and probabilistic noise. In this paper, we study various problems that include finding maximum, nearest/farthest neighbor search under these noise models. Building upon the techniques we develop for these comparison operations, we give robust algorithms for k-center clustering and agglomerative hierarchical clustering. We prove that our algorithms achieve good approximation guarantees with a high probability and analyze their query complexity. We evaluate the effectiveness and efficiency of our techniques empirically on various real-world datasets.

</p>
</details>

<details><summary><b>Machine learning moment closure models for the radiative transfer equation I: directly learning a gradient based closure</b>
<a href="https://arxiv.org/abs/2105.05690">arxiv:2105.05690</a>
&#x1F4C8; 1 <br>
<p>Juntao Huang, Yingda Cheng, Andrew J. Christlieb, Luke F. Roberts</p></summary>
<p>

**Abstract:** In this paper, we take a data-driven approach and apply machine learning to the moment closure problem for radiative transfer equation in slab geometry. Instead of learning the unclosed high order moment, we propose to directly learn the gradient of the high order moment using neural networks. This new approach is consistent with the exact closure we derive for the free streaming limit and also provides a natural output normalization. A variety of benchmark tests, including the variable scattering problem, the Gaussian source problem and the two material problem, show both good accuracy and generalizability of our machine learning closure model.

</p>
</details>

<details><summary><b>An Appraisal Transition System for Event-driven Emotions in Agent-based Player Experience Testing</b>
<a href="https://arxiv.org/abs/2105.05589">arxiv:2105.05589</a>
&#x1F4C8; 1 <br>
<p>Saba Gholizadeh Ansari, I. S. W. B. Prasetya, Mehdi Dastani, Frank Dignum, Gabriele Keller</p></summary>
<p>

**Abstract:** Player experience (PX) evaluation has become a field of interest in the game industry. Several manual PX techniques have been introduced to assist developers to understand and evaluate the experience of players in computer games. However, automated testing of player experience still needs to be addressed. An automated player experience testing framework would allow designers to evaluate the PX requirements in the early development stages without the necessity of participating human players. In this paper, we propose an automated player experience testing approach by suggesting a formal model of event-based emotions. In particular, we discuss an event-based transition system to formalize relevant emotions using Ortony, Clore, & Collins (OCC) theory of emotions. A working prototype of the model is integrated on top of Aplib, a tactical agent programming library, to create intelligent PX test agents, capable of appraising emotions in a 3D game case study. The results are graphically shown e.g. as heat maps. Emotion visualization of the test agent would ultimately help game designers in creating content that evokes a certain experience in players.

</p>
</details>

<details><summary><b>A Survey on Reinforcement Learning-Aided Caching in Mobile Edge Networks</b>
<a href="https://arxiv.org/abs/2105.05564">arxiv:2105.05564</a>
&#x1F4C8; 1 <br>
<p>Nikolaos Nomikos, Spyros Zoupanos, Themistoklis Charalambous, Ioannis Krikidis, Athina Petropulu</p></summary>
<p>

**Abstract:** Mobile networks are experiencing tremendous increase in data volume and user density. An efficient technique to alleviate this issue is to bring the data closer to the users by exploiting the caches of edge network nodes, such as fixed or mobile access points and even user devices. Meanwhile, the fusion of machine learning and wireless networks offers a viable way for network optimization as opposed to traditional optimization approaches which incur high complexity, or fail to provide optimal solutions. Among the various machine learning categories, reinforcement learning operates in an online and autonomous manner without relying on large sets of historical data for training. In this survey, reinforcement learning-aided mobile edge caching is presented, aiming at highlighting the achieved network gains over conventional caching approaches. Taking into account the heterogeneity of sixth generation (6G) networks in various wireless settings, such as fixed, vehicular and flying networks, learning-aided edge caching is presented, departing from traditional architectures. Furthermore, a categorization according to the desirable performance metric, such as spectral, energy and caching efficiency, average delay, and backhaul and fronthaul offloading is provided. Finally, several open issues are discussed, targeting to stimulate further interest in this important research field.

</p>
</details>

<details><summary><b>AVA: Adversarial Vignetting Attack against Visual Recognition</b>
<a href="https://arxiv.org/abs/2105.05558">arxiv:2105.05558</a>
&#x1F4C8; 1 <br>
<p>Binyu Tian, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Xiaohong Li, Yang Liu</p></summary>
<p>

**Abstract:** Vignetting is an inherited imaging phenomenon within almost all optical systems, showing as a radial intensity darkening toward the corners of an image. Since it is a common effect for photography and usually appears as a slight intensity variation, people usually regard it as a part of a photo and would not even want to post-process it. Due to this natural advantage, in this work, we study vignetting from a new viewpoint, i.e., adversarial vignetting attack (AVA), which aims to embed intentionally misleading information into vignetting and produce a natural adversarial example without noise patterns. This example can fool the state-of-the-art deep convolutional neural networks (CNNs) but is imperceptible to humans. To this end, we first propose the radial-isotropic adversarial vignetting attack (RI-AVA) based on the physical model of vignetting, where the physical parameters (e.g., illumination factor and focal length) are tuned through the guidance of target CNN models. To achieve higher transferability across different CNNs, we further propose radial-anisotropic adversarial vignetting attack (RA-AVA) by allowing the effective regions of vignetting to be radial-anisotropic and shape-free. Moreover, we propose the geometry-aware level-set optimization method to solve the adversarial vignetting regions and physical parameters jointly. We validate the proposed methods on three popular datasets, i.e., DEV, CIFAR10, and Tiny ImageNet, by attacking four CNNs, e.g., ResNet50, EfficientNet-B0, DenseNet121, and MobileNet-V2, demonstrating the advantages of our methods over baseline methods on both transferability and image quality.

</p>
</details>

<details><summary><b>Discrete-time Contraction-based Control of Nonlinear Systems with Parametric Uncertainties using Neural Networks</b>
<a href="https://arxiv.org/abs/2105.05432">arxiv:2105.05432</a>
&#x1F4C8; 1 <br>
<p>Lai Wei, Ryan McCloy, Jie Bao</p></summary>
<p>

**Abstract:** Flexible manufacturing in the process industry requires control systems to achieve time-varying setpoints (e.g., product specifications) based on market demand. Contraction theory provides a useful framework for reference-independent system analysis and tracking control for nonlinear systems. However, determination of the control contraction metrics and control laws can be very difficult for general nonlinear systems. This work develops an approach to discrete-time contraction analysis and control using neural networks. The methodology involves training a neural network to learn a contraction metric and feedback gain. The resulting contraction-based controller embeds the trained neural network and is capable of achieving efficient tracking of time-varying references, with a full range of model uncertainty, without the need for controller structure redesign. This is a robust approach that can deal with bounded parametric uncertainties in the process model, which are commonly encountered in industrial (chemical) processes. Simulation examples are provided to illustrate the above approach.

</p>
</details>

<details><summary><b>TabLeX: A Benchmark Dataset for Structure and Content Information Extraction from Scientific Tables</b>
<a href="https://arxiv.org/abs/2105.06400">arxiv:2105.06400</a>
&#x1F4C8; 0 <br>
<p>Harsh Desai, Pratik Kayal, Mayank Singh</p></summary>
<p>

**Abstract:** Information Extraction (IE) from the tables present in scientific articles is challenging due to complicated tabular representations and complex embedded text. This paper presents TabLeX, a large-scale benchmark dataset comprising table images generated from scientific articles. TabLeX consists of two subsets, one for table structure extraction and the other for table content extraction. Each table image is accompanied by its corresponding LATEX source code. To facilitate the development of robust table IE tools, TabLeX contains images in different aspect ratios and in a variety of fonts. Our analysis sheds light on the shortcomings of current state-of-the-art table extraction models and shows that they fail on even simple table images. Towards the end, we experiment with a transformer-based existing baseline to report performance scores. In contrast to the static benchmarks, we plan to augment this dataset with more complex and diverse tables at regular intervals.

</p>
</details>

<details><summary><b>Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches</b>
<a href="https://arxiv.org/abs/2105.06295">arxiv:2105.06295</a>
&#x1F4C8; 0 <br>
<p>Albara Ah Ramli, Huanle Zhang, Jiahui Hou, Rex Liu, Xin Liu, Alina Nicorici, Daniel Aranki, Corey Owens, Poonam Prasad, Craig McDonald, Erik Henricson</p></summary>
<p>

**Abstract:** Differences in gait patterns of children with Duchenne muscular dystrophy (DMD) and typically developing (TD) peers are visible to the eye, but quantification of those differences outside of the gait laboratory has been elusive. We measured vertical, mediolateral, and anteroposterior acceleration using a waist-worn iPhone accelerometer during ambulation across a typical range of velocities. Six TD and six DMD children from 3-15 years of age underwent seven walking/running tasks, including five 25m walk/run tests at a slow walk to running speeds, a 6-minute walk test (6MWT), and a 100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features (CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate between DMD and TD control children using extracted features and raw data. Extracted CFs showed reduced step length and a greater mediolateral component of total power (TP) consistent with shorter strides and Trendelenberg-like gait commonly observed in DMD. AI methods using CFs and raw data varied ineffectiveness at differentiating between DMD and TD controls at different speeds, with an accuracy of some methods exceeding 91%. We demonstrate that by using AI tools with accelerometer data from a consumer-level smartphone, we can identify DMD gait disturbance in toddlers to early teens.

</p>
</details>

<details><summary><b>House Price Prediction using Satellite Imagery</b>
<a href="https://arxiv.org/abs/2105.06060">arxiv:2105.06060</a>
&#x1F4C8; 0 <br>
<p>Sina Jandaghi Semnani, Hoormazd Rezaei</p></summary>
<p>

**Abstract:** In this paper we show how using satellite images can improve the accuracy of housing price estimation models. Using Los Angeles County's property assessment dataset, by transferring learning from an Inception-v3 model pretrained on ImageNet, we could achieve an improvement of ~10% in R-squared score compared to two baseline models that only use non-image features of the house.

</p>
</details>

<details><summary><b>Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings</b>
<a href="https://arxiv.org/abs/2105.06029">arxiv:2105.06029</a>
&#x1F4C8; 0 <br>
<p>Ming Yin, Yu-Xiang Wang</p></summary>
<p>

**Abstract:** This work studies the statistical limits of uniform convergence for offline policy evaluation (OPE) problems with model-based methods (for episodic MDP) and provides a unified framework towards optimal learning for several well-motivated offline tasks. Uniform OPE $\sup_Π|Q^π-\hat{Q}^π|<ε$ is a stronger measure than the point-wise OPE and ensures offline learning when $Π$ contains all policies (the global class). In this paper, we establish an $Ω(H^2 S/d_mε^2)$ lower bound (over model-based family) for the global uniform OPE and our main result establishes an upper bound of $\tilde{O}(H^2/d_mε^2)$ for the \emph{local} uniform convergence that applies to all \emph{near-empirically optimal} policies for the MDPs with \emph{stationary} transition. Here $d_m$ is the minimal marginal state-action probability. Critically, the highlight in achieving the optimal rate $\tilde{O}(H^2/d_mε^2)$ is our design of \emph{singleton absorbing MDP}, which is a new sharp analysis tool that works with the model-based approach. We generalize such a model-based framework to the new settings: offline task-agnostic and the offline reward-free with optimal complexity $\tilde{O}(H^2\log(K)/d_mε^2)$ ($K$ is the number of tasks) and $\tilde{O}(H^2S/d_mε^2)$ respectively. These results provide a unified solution for simultaneously solving different offline RL problems.

</p>
</details>

<details><summary><b>Optimal transport with some directed distances</b>
<a href="https://arxiv.org/abs/2105.05989">arxiv:2105.05989</a>
&#x1F4C8; 0 <br>
<p>Wolfgang Stummer</p></summary>
<p>

**Abstract:** We present a toolkit of directed distances between quantile functions. By employing this, we solve some new optimal transport (OT) problems which e.g. considerably flexibilize some prominent OTs expressed through Wasserstein distances.

</p>
</details>

<details><summary><b>Efficient Algorithms for Estimating the Parameters of Mixed Linear Regression Models</b>
<a href="https://arxiv.org/abs/2105.05953">arxiv:2105.05953</a>
&#x1F4C8; 0 <br>
<p>Babak Barazandeh, Ali Ghafelebashi, Meisam Razaviyayn, Ram Sriharsha</p></summary>
<p>

**Abstract:** Mixed linear regression (MLR) model is among the most exemplary statistical tools for modeling non-linear distributions using a mixture of linear models. When the additive noise in MLR model is Gaussian, Expectation-Maximization (EM) algorithm is a widely-used algorithm for maximum likelihood estimation of MLR parameters. However, when noise is non-Gaussian, the steps of EM algorithm may not have closed-form update rules, which makes EM algorithm impractical. In this work, we study the maximum likelihood estimation of the parameters of MLR model when the additive noise has non-Gaussian distribution. In particular, we consider the case that noise has Laplacian distribution and we first show that unlike the the Gaussian case, the resulting sub-problems of EM algorithm in this case does not have closed-form update rule, thus preventing us from using EM in this case. To overcome this issue, we propose a new algorithm based on combining the alternating direction method of multipliers (ADMM) with EM algorithm idea. Our numerical experiments show that our method outperforms the EM algorithm in statistical accuracy and computational time in non-Gaussian noise case.

</p>
</details>

<details><summary><b>SimNet: Accurate and High-Performance Computer Architecture Simulation using Machine Learning</b>
<a href="https://arxiv.org/abs/2105.05821">arxiv:2105.05821</a>
&#x1F4C8; 0 <br>
<p>Lingda Li, Santosh Pandey, Thomas Flynn, Hang Liu, Noel Wheeler, Adolfy Hoisie</p></summary>
<p>

**Abstract:** While discrete-event simulators are essential tools for architecture research, design, and development, their practicality is limited by an extremely long time-to-solution for realistic applications under investigation. This work describes a concerted effort, where machine learning (ML) is used to accelerate discrete-event simulation. First, an ML-based instruction latency prediction framework that accounts for both static instruction properties and dynamic processor states is constructed. Then, a GPU-accelerated parallel simulator is implemented based on the proposed instruction latency predictor, and its simulation accuracy and throughput are validated and evaluated against a state-of-the-art simulator. Leveraging modern GPUs, the ML-based simulator outperforms traditional simulators significantly.

</p>
</details>

<details><summary><b>Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</b>
<a href="https://arxiv.org/abs/2105.05720">arxiv:2105.05720</a>
&#x1F4C8; 0 <br>
<p>Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi</p></summary>
<p>

**Abstract:** Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone.
  Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.

</p>
</details>

<details><summary><b>An efficient projection neural network for $\ell_1$-regularized logistic regression</b>
<a href="https://arxiv.org/abs/2105.05449">arxiv:2105.05449</a>
&#x1F4C8; 0 <br>
<p>Majid Mohammadi, Amir Ahooye Atashin, Damian A. Tamburri</p></summary>
<p>

**Abstract:** $\ell_1$ regularization has been used for logistic regression to circumvent the overfitting and use the estimated sparse coefficient for feature selection. However, the challenge of such a regularization is that the $\ell_1$ norm is not differentiable, making the standard algorithms for convex optimization not applicable to this problem. This paper presents a simple projection neural network for $\ell_1$-regularized logistics regression. In contrast to many available solvers in the literature, the proposed neural network does not require any extra auxiliary variable nor any smooth approximation, and its complexity is almost identical to that of the gradient descent for logistic regression without $\ell_1$ regularization, thanks to the projection operator. We also investigate the convergence of the proposed neural network by using the Lyapunov theory and show that it converges to a solution of the problem with any arbitrary initial value. The proposed neural solution significantly outperforms state-of-the-art methods with respect to the execution time and is competitive in terms of accuracy and AUROC.

</p>
</details>


[Next Page](2021/2021-05/2021-05-11.md)
