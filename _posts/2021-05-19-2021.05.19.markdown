## Summary for 2021-05-19, created on 2021-12-21


<details><summary><b>E(n) Equivariant Normalizing Flows</b>
<a href="https://arxiv.org/abs/2105.09016">arxiv:2105.09016</a>
&#x1F4C8; 48 <br>
<p>Victor Garcia Satorras, Emiel Hoogeboom, Fabian B. Fuchs, Ingmar Posner, Max Welling</p></summary>
<p>

**Abstract:** This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.

</p>
</details>

<details><summary><b>Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2105.09501">arxiv:2105.09501</a>
&#x1F4C8; 41 <br>
<p>Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li</p></summary>
<p>

**Abstract:** Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 outperforms existing best unified model and achieves competitive or even better performance than the pre-trained and fine-tuned model mBART on tens of WMT's translation directions. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual Transformer baseline. Code, data and trained models are available at https://github.com/PANXiao1994/mRASP2.

</p>
</details>

<details><summary><b>Image to Image Translation : Generating maps from satellite images</b>
<a href="https://arxiv.org/abs/2105.09253">arxiv:2105.09253</a>
&#x1F4C8; 31 <br>
<p>Vaishali Ingale, Rishabh Singh, Pragati Patwal</p></summary>
<p>

**Abstract:** Generation of maps from satellite images is conventionally done by a range of tools. Maps became an important part of life whose conversion from satellite images may be a bit expensive but Generative models can pander to this challenge. These models aims at finding the patterns between the input and output image. Image to image translation is employed to convert satellite image to corresponding map. Different techniques for image to image translations like Generative adversarial network, Conditional adversarial networks and Co-Variational Auto encoders are used to generate the corresponding human-readable maps for that region, which takes a satellite image at a given zoom level as its input. We are training our model on Conditional Generative Adversarial Network which comprises of Generator model which which generates fake images while the discriminator tries to classify the image as real or fake and both these models are trained synchronously in adversarial manner where both try to fool each other and result in enhancing model performance.

</p>
</details>

<details><summary><b>Stratified Data Integration</b>
<a href="https://arxiv.org/abs/2105.09432">arxiv:2105.09432</a>
&#x1F4C8; 23 <br>
<p>Fausto Giunchiglia, Alessio Zamboni, Mayukh Bagchi, Simone Bocca</p></summary>
<p>

**Abstract:** We propose a novel approach to the problem of semantic heterogeneity where data are organized into a set of stratified and independent representation layers, namely: conceptual(where a set of unique alinguistic identifiers are connected inside a graph codifying their meaning), language(where sets of synonyms, possibly from multiple languages, annotate concepts), knowledge(in the form of a graph where nodes are entity types and links are properties), and data(in the form of a graph of entities populating the previous knowledge graph). This allows us to state the problem of semantic heterogeneity as a problem of Representation Diversity where the different types of heterogeneity, viz. Conceptual, Language, Knowledge, and Data, are uniformly dealt within each single layer, independently from the others. In this paper we describe the proposed stratified representation of data and the process by which data are first transformed into the target representation, then suitably integrated and then, finally, presented to the user in her preferred format. The proposed framework has been evaluated in various pilot case studies and in a number of industrial data integration problems.

</p>
</details>

<details><summary><b>The State of AI Ethics Report (January 2021)</b>
<a href="https://arxiv.org/abs/2105.09059">arxiv:2105.09059</a>
&#x1F4C8; 19 <br>
<p>Abhishek Gupta, Alexandrine Royer, Connor Wright, Falaah Arif Khan, Victoria Heath, Erick Galinkin, Ryan Khurana, Marianna Bergamaschi Ganapini, Muriam Fancy, Masa Sweidan, Mo Akif, Renjie Butalid</p></summary>
<p>

**Abstract:** The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics captures the most relevant developments in AI Ethics since October 2020. It aims to help anyone, from machine learning experts to human rights activists and policymakers, quickly digest and understand the field's ever-changing developments. Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, including: algorithmic injustice, discrimination, ethical AI, labor impacts, misinformation, privacy, risk and security, social media, and more.
  In addition, The State of AI Ethics includes exclusive content written by world-class AI Ethics experts from universities, research institutes, consulting firms, and governments. Unique to this report is "The Abuse and Misogynoir Playbook," written by Dr. Katlyn Tuner (Research Scientist, Space Enabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program in Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics; Lead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant Professor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The piece (and accompanying infographic), is a deep-dive into the historical and systematic silencing, erasure, and revision of Black women's contributions to knowledge and scholarship in the United Stations, and globally. Exposing and countering this Playbook has become increasingly important following the firing of AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.
  This report should be used not only as a point of reference and insight on the latest thinking in the field of AI Ethics, but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of AI on the world.

</p>
</details>

<details><summary><b>Tool- and Domain-Agnostic Parameterization of Style Transfer Effects Leveraging Pretrained Perceptual Metrics</b>
<a href="https://arxiv.org/abs/2105.09207">arxiv:2105.09207</a>
&#x1F4C8; 17 <br>
<p>Hiromu Yakura, Yuki Koyama, Masataka Goto</p></summary>
<p>

**Abstract:** Current deep learning techniques for style transfer would not be optimal for design support since their "one-shot" transfer does not fit exploratory design processes. To overcome this gap, we propose parametric transcription, which transcribes an end-to-end style transfer effect into parameter values of specific transformations available in an existing content editing tool. With this approach, users can imitate the style of a reference sample in the tool that they are familiar with and thus can easily continue further exploration by manipulating the parameters. To enable this, we introduce a framework that utilizes an existing pretrained model for style transfer to calculate a perceptual style distance to the reference sample and uses black-box optimization to find the parameters that minimize this distance. Our experiments with various third-party tools, such as Instagram and Blender, show that our framework can effectively leverage deep learning techniques for computational design support.

</p>
</details>

<details><summary><b>Compositional Processing Emerges in Neural Networks Solving Math Problems</b>
<a href="https://arxiv.org/abs/2105.08961">arxiv:2105.08961</a>
&#x1F4C8; 14 <br>
<p>Jacob Russin, Roland Fernandez, Hamid Palangi, Eric Rosen, Nebojsa Jojic, Paul Smolensky, Jianfeng Gao</p></summary>
<p>

**Abstract:** A longstanding question in cognitive science concerns the learning mechanisms underlying compositionality in human cognition. Humans can infer the structured relationships (e.g., grammatical rules) implicit in their sensory observations (e.g., auditory speech), and use this knowledge to guide the composition of simpler meanings into complex wholes. Recent progress in artificial neural networks has shown that when large models are trained on enough linguistic data, grammatical structure emerges in their representations. We extend this work to the domain of mathematical reasoning, where it is possible to formulate precise hypotheses about how meanings (e.g., the quantities corresponding to numerals) should be composed according to structured rules (e.g., order of operations). Our work shows that neural networks are not only able to infer something about the structured relationships implicit in their training data, but can also deploy this knowledge to guide the composition of individual meanings into composite wholes.

</p>
</details>

<details><summary><b>MLBiNet: A Cross-Sentence Collective Event Detection Network</b>
<a href="https://arxiv.org/abs/2105.09458">arxiv:2105.09458</a>
&#x1F4C8; 11 <br>
<p>Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, Huajun Chen</p></summary>
<p>

**Abstract:** We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.

</p>
</details>

<details><summary><b>Correlated Input-Dependent Label Noise in Large-Scale Image Classification</b>
<a href="https://arxiv.org/abs/2105.10305">arxiv:2105.10305</a>
&#x1F4C8; 10 <br>
<p>Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, Jesse Berent</p></summary>
<p>

**Abstract:** Large scale image classification datasets often contain noisy labels. We take a principled probabilistic approach to modelling input-dependent, also known as heteroscedastic, label noise in these datasets. We place a multivariate Normal distributed latent variable on the final hidden layer of a neural network classifier. The covariance matrix of this latent variable, models the aleatoric uncertainty due to label noise. We demonstrate that the learned covariance structure captures known sources of label noise between semantically similar and co-occurring classes. Compared to standard neural network training and other baselines, we show significantly improved accuracy on Imagenet ILSVRC 2012 79.3% (+2.6%), Imagenet-21k 47.0% (+1.1%) and JFT 64.7% (+1.6%). We set a new state-of-the-art result on WebVision 1.0 with 76.6% top-1 accuracy. These datasets range from over 1M to over 300M training examples and from 1k classes to more than 21k classes. Our method is simple to use, and we provide an implementation that is a drop-in replacement for the final fully-connected layer in a deep classifier.

</p>
</details>

<details><summary><b>DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons</b>
<a href="https://arxiv.org/abs/2105.09352">arxiv:2105.09352</a>
&#x1F4C8; 10 <br>
<p>Dawn Drain, Colin B. Clement, Guillermo Serrato, Neel Sundaresan</p></summary>
<p>

**Abstract:** The joint task of bug localization and program repair is an integral part of the software development process. In this work we present DeepDebug, an approach to automated debugging using large, pretrained transformers. We begin by training a bug-creation model on reversed commit data for the purpose of generating synthetic bugs. We apply these synthetic bugs toward two ends. First, we directly train a backtranslation model on all functions from 200K repositories. Next, we focus on 10K repositories for which we can execute tests, and create buggy versions of all functions in those repositories that are covered by passing tests. This provides us with rich debugging information such as stack traces and print statements, which we use to finetune our model which was pretrained on raw source code. Finally, we strengthen all our models by expanding the context window beyond the buggy function itself, and adding a skeleton consisting of that function's parent class, imports, signatures, docstrings, and method bodies, in order of priority. On the QuixBugs benchmark, we increase the total number of fixes found by over 50%, while also decreasing the false positive rate from 35% to 5% and decreasing the timeout from six hours to one minute. On our own benchmark of executable tests, our model fixes 68% of all bugs on its first attempt without using traces, and after adding traces it fixes 75% on first attempt. We will open-source our framework and validation set for evaluating on executable tests.

</p>
</details>

<details><summary><b>Online Selection of Diverse Committees</b>
<a href="https://arxiv.org/abs/2105.09295">arxiv:2105.09295</a>
&#x1F4C8; 10 <br>
<p>Virginie Do, Jamal Atif, Jérôme Lang, Nicolas Usunier</p></summary>
<p>

**Abstract:** Citizens' assemblies need to represent subpopulations according to their proportions in the general population. These large committees are often constructed in an online fashion by contacting people, asking for the demographic features of the volunteers, and deciding to include them or not. This raises a trade-off between the number of people contacted (and the incurring cost) and the representativeness of the committee. We study three methods, theoretically and experimentally: a greedy algorithm that includes volunteers as long as proportionality is not violated; a non-adaptive method that includes a volunteer with a probability depending only on their features, assuming that the joint feature distribution in the volunteer pool is known; and a reinforcement learning based approach when this distribution is not known a priori but learnt online.

</p>
</details>

<details><summary><b>XCycles Backprojection Acoustic Super-Resolution</b>
<a href="https://arxiv.org/abs/2105.09128">arxiv:2105.09128</a>
&#x1F4C8; 10 <br>
<p>Feras Almasri, Jurgen Vandendriessche, Laurent Segers, Bruno da Silva, An Braeken, Kris Steenhaut, Abdellah Touhafi, Olivier Debeir</p></summary>
<p>

**Abstract:** The computer vision community has paid much attention to the development of visible image super-resolution (SR) using deep neural networks (DNNs) and has achieved impressive results. The advancement of non-visible light sensors, such as acoustic imaging sensors, has attracted much attention, as they allow people to visualize the intensity of sound waves beyond the visible spectrum. However, because of the limitations imposed on acquiring acoustic data, new methods for improving the resolution of the acoustic images are necessary. At this time, there is no acoustic imaging dataset designed for the SR problem. This work proposed a novel backprojection model architecture for the acoustic image super-resolution problem, together with Acoustic Map Imaging VUB-ULB Dataset (AMIVU). The dataset provides large simulated and real captured images at different resolutions. The proposed XCycles BackProjection model (XCBP), in contrast to the feedforward model approach, fully uses the iterative correction procedure in each cycle to reconstruct the residual error correction for the encoded features in both low- and high-resolution space. The proposed approach was evaluated on the dataset and showed high outperformance compared to the classical interpolation operators and to the recent feedforward state-of-the-art models. It also contributed to a drastically reduced sub-sampling error produced during the data acquisition.

</p>
</details>

<details><summary><b>Guaranteeing Maximin Shares: Some Agents Left Behind</b>
<a href="https://arxiv.org/abs/2105.09383">arxiv:2105.09383</a>
&#x1F4C8; 9 <br>
<p>Hadi Hosseini, Andrew Searns</p></summary>
<p>

**Abstract:** The maximin share (MMS) guarantee is a desirable fairness notion for allocating indivisible goods. While MMS allocations do not always exist, several approximation techniques have been developed to ensure that all agents receive a fraction of their maximin share. We focus on an alternative approximation notion, based on the population of agents, that seeks to guarantee MMS for a fraction of agents. We show that no optimal approximation algorithm can satisfy more than a constant number of agents, and discuss the existence and computation of MMS for all but one agent and its relation to approximate MMS guarantees. We then prove the existence of allocations that guarantee MMS for $\frac{2}{3}$ of agents, and devise a polynomial time algorithm that achieves this bound for up to nine agents. A key implication of our result is the existence of allocations that guarantee $\text{MMS}^{\lceil{3n/2}\rceil}$, i.e., the value that agents receive by partitioning the goods into $\lceil{\frac{3}{2}n}\rceil$ bundles, improving the best known guarantee of $\text{MMS}^{2n-2}$. Finally, we provide empirical experiments using synthetic data.

</p>
</details>

<details><summary><b>Robo-Advising: Enhancing Investment with Inverse Optimization and Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.09264">arxiv:2105.09264</a>
&#x1F4C8; 9 <br>
<p>Haoran Wang, Shi Yu</p></summary>
<p>

**Abstract:** Machine Learning (ML) has been embraced as a powerful tool by the financial industry, with notable applications spreading in various domains including investment management. In this work, we propose a full-cycle data-driven investment robo-advising framework, consisting of two ML agents. The first agent, an inverse portfolio optimization agent, infers an investor's risk preference and expected return directly from historical allocation data using online inverse optimization. The second agent, a deep reinforcement learning (RL) agent, aggregates the inferred sequence of expected returns to formulate a new multi-period mean-variance portfolio optimization problem that can be solved using deep RL approaches. The proposed investment pipeline is applied on real market data from April 1, 2016 to February 1, 2021 and has shown to consistently outperform the S&P 500 benchmark portfolio that represents the aggregate market optimal allocation. The outperformance may be attributed to the the multi-period planning (versus single-period planning) and the data-driven RL approach (versus classical estimation approach).

</p>
</details>

<details><summary><b>Joint Calibrationless Reconstruction and Segmentation of Parallel MRI</b>
<a href="https://arxiv.org/abs/2105.09220">arxiv:2105.09220</a>
&#x1F4C8; 9 <br>
<p>Aniket Pramanik, Xiaodong Wu, Mathews Jacob</p></summary>
<p>

**Abstract:** The volume estimation of brain regions from MRI data is a key problem in many clinical applications, where the acquisition of data at high spatial resolution is desirable. While parallel MRI and constrained image reconstruction algorithms can accelerate the scans, image reconstruction artifacts are inevitable, especially at high acceleration factors. We introduce a novel image domain deep-learning framework for calibrationless parallel MRI reconstruction, coupled with a segmentation network to improve image quality and to reduce the vulnerability of current segmentation algorithms to image artifacts resulting from acceleration. The combination of the proposed image domain deep calibrationless approach with the segmentation algorithm offers improved image quality, while increasing the accuracy of the segmentations. The novel architecture with an encoder shared between the reconstruction and segmentation tasks is seen to reduce the need for segmented training datasets. In particular, the proposed few-shot training strategy requires only 10% of segmented datasets to offer good performance.

</p>
</details>

<details><summary><b>Learn Fine-grained Adaptive Loss for Multiple Anatomical Landmark Detection in Medical Images</b>
<a href="https://arxiv.org/abs/2105.09124">arxiv:2105.09124</a>
&#x1F4C8; 9 <br>
<p>Guang-Quan Zhou, Juzheng Miao, Xin Yang, Rui Li, En-Ze Huo, Wenlong Shi, Yuhao Huang, Jikuan Qian, Chaoyu Chen, Dong Ni</p></summary>
<p>

**Abstract:** Automatic and accurate detection of anatomical landmarks is an essential operation in medical image analysis with a multitude of applications. Recent deep learning methods have improved results by directly encoding the appearance of the captured anatomy with the likelihood maps (i.e., heatmaps). However, most current solutions overlook another essence of heatmap regression, the objective metric for regressing target heatmaps and rely on hand-crafted heuristics to set the target precision, thus being usually cumbersome and task-specific. In this paper, we propose a novel learning-to-learn framework for landmark detection to optimize the neural network and the target precision simultaneously. The pivot of this work is to leverage the reinforcement learning (RL) framework to search objective metrics for regressing multiple heatmaps dynamically during the training process, thus avoiding setting problem-specific target precision. We also introduce an early-stop strategy for active termination of the RL agent's interaction that adapts the optimal precision for separate targets considering exploration-exploitation tradeoffs. This approach shows better stability in training and improved localization accuracy in inference. Extensive experimental results on two different applications of landmark localization: 1) our in-house prenatal ultrasound (US) dataset and 2) the publicly available dataset of cephalometric X-Ray landmark detection, demonstrate the effectiveness of our proposed method. Our proposed framework is general and shows the potential to improve the efficiency of anatomical landmark detection.

</p>
</details>

<details><summary><b>Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead</b>
<a href="https://arxiv.org/abs/2105.09121">arxiv:2105.09121</a>
&#x1F4C8; 9 <br>
<p>Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis</p></summary>
<p>

**Abstract:** Deploying deep learning models in time-critical applications with limited computational resources, for instance in edge computing systems and IoT networks, is a challenging task that often relies on dynamic inference methods such as early exiting. In this paper, we introduce a novel architecture for early exiting based on the vision transformer architecture, as well as a fine-tuning strategy that significantly increase the accuracy of early exit branches compared to conventional approaches while introducing less overhead. Through extensive experiments on image and audio classification as well as audiovisual crowd counting, we show that our method works for both classification and regression problems, and in both single- and multi-modal settings. Additionally, we introduce a novel method for integrating audio and visual modalities within early exits in audiovisual data analysis, that can lead to a more fine-grained dynamic inference.

</p>
</details>

<details><summary><b>DeepCAD: A Deep Generative Network for Computer-Aided Design Models</b>
<a href="https://arxiv.org/abs/2105.09492">arxiv:2105.09492</a>
&#x1F4C8; 8 <br>
<p>Rundi Wu, Chang Xiao, Changxi Zheng</p></summary>
<p>

**Abstract:** Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.

</p>
</details>

<details><summary><b>Do We Really Need to Learn Representations from In-domain Data for Outlier Detection?</b>
<a href="https://arxiv.org/abs/2105.09270">arxiv:2105.09270</a>
&#x1F4C8; 8 <br>
<p>Zhisheng Xiao, Qing Yan, Yali Amit</p></summary>
<p>

**Abstract:** Unsupervised outlier detection, which predicts if a test sample is an outlier or not using only the information from unlabelled inlier data, is an important but challenging task. Recently, methods based on the two-stage framework achieve state-of-the-art performance on this task. The framework leverages self-supervised representation learning algorithms to train a feature extractor on inlier data, and applies a simple outlier detector in the feature space. In this paper, we explore the possibility of avoiding the high cost of training a distinct representation for each outlier detection task, and instead using a single pre-trained network as the universal feature extractor regardless of the source of in-domain data. In particular, we replace the task-specific feature extractor by one network pre-trained on ImageNet with a self-supervised loss. In experiments, we demonstrate competitive or better performance on a variety of outlier detection benchmarks compared with previous two-stage methods, suggesting that learning representations from in-domain data may be unnecessary for outlier detection.

</p>
</details>

<details><summary><b>Copyright in Generative Deep Learning</b>
<a href="https://arxiv.org/abs/2105.09266">arxiv:2105.09266</a>
&#x1F4C8; 8 <br>
<p>Giorgio Franceschelli, Mirco Musolesi</p></summary>
<p>

**Abstract:** Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of generative deep learning for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States of America and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of generative deep learning. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers.

</p>
</details>

<details><summary><b>Boosting Variational Inference With Locally Adaptive Step-Sizes</b>
<a href="https://arxiv.org/abs/2105.09240">arxiv:2105.09240</a>
&#x1F4C8; 8 <br>
<p>Gideon Dresdner, Saurav Shekhar, Fabian Pedregosa, Francesco Locatello, Gunnar Rätsch</p></summary>
<p>

**Abstract:** Variational Inference makes a trade-off between the capacity of the variational family and the tractability of finding an approximate posterior distribution. Instead, Boosting Variational Inference allows practitioners to obtain increasingly good posterior approximations by spending more compute. The main obstacle to widespread adoption of Boosting Variational Inference is the amount of resources necessary to improve over a strong Variational Inference baseline. In our work, we trace this limitation back to the global curvature of the KL-divergence. We characterize how the global curvature impacts time and memory consumption, address the problem with the notion of local curvature, and provide a novel approximate backtracking algorithm for estimating local curvature. We give new theoretical convergence rates for our algorithms and provide experimental validation on synthetic and real-world datasets.

</p>
</details>

<details><summary><b>The State of AI Ethics Report (Volume 4)</b>
<a href="https://arxiv.org/abs/2105.09060">arxiv:2105.09060</a>
&#x1F4C8; 8 <br>
<p>Abhishek Gupta, Alexandrine Royer, Connor Wright, Victoria Heath, Muriam Fancy, Marianna Bergamaschi Ganapini, Shannon Egan, Masa Sweidan, Mo Akif, Renjie Butalid</p></summary>
<p>

**Abstract:** The 4th edition of the Montreal AI Ethics Institute's The State of AI Ethics captures the most relevant developments in the field of AI Ethics since January 2021. This report aims to help anyone, from machine learning experts to human rights activists and policymakers, quickly digest and understand the ever-changing developments in the field. Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, with a particular focus on four key themes: Ethical AI, Fairness & Justice, Humans & Tech, and Privacy.
  In addition, The State of AI Ethics includes exclusive content written by world-class AI Ethics experts from universities, research institutes, consulting firms, and governments. Opening the report is a long-form piece by Edward Higgs (Professor of History, University of Essex) titled "AI and the Face: A Historian's View." In it, Higgs examines the unscientific history of facial analysis and how AI might be repeating some of those mistakes at scale. The report also features chapter introductions by Alexa Hagerty (Anthropologist, University of Cambridge), Marianna Ganapini (Faculty Director, Montreal AI Ethics Institute), Deborah G. Johnson (Emeritus Professor, Engineering and Society, University of Virginia), and Soraj Hongladarom (Professor of Philosophy and Director, Center for Science, Technology and Society, Chulalongkorn University in Bangkok).
  This report should be used not only as a point of reference and insight on the latest thinking in the field of AI Ethics, but should also be used as a tool for introspection as we aim to foster a more nuanced conversation regarding the impacts of AI on the world.

</p>
</details>

<details><summary><b>Medical Image Segmentation Using Squeeze-and-Expansion Transformers</b>
<a href="https://arxiv.org/abs/2105.09511">arxiv:2105.09511</a>
&#x1F4C8; 7 <br>
<p>Shaohua Li, Xiuchao Sui, Xiangde Luo, Xinxing Xu, Yong Liu, Rick Goh</p></summary>
<p>

**Abstract:** Medical image segmentation is important for computer-aided diagnosis. Good segmentation demands the model to see the big picture and fine details simultaneously, i.e., to learn image features that incorporate large context while keep high spatial resolutions. To approach this goal, the most widely used methods -- U-Net and variants, extract and fuse multi-scale features. However, the fused features still have small "effective receptive fields" with a focus on local image cues, limiting their performance. In this work, we propose Segtran, an alternative segmentation framework based on transformers, which have unlimited "effective receptive fields" even at high feature resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer: a squeezed attention block regularizes the self attention of transformers, and an expansion block learns diversified representations. Additionally, we propose a new positional encoding scheme for transformers, imposing a continuity inductive bias for images. Experiments were performed on 2D and 3D medical image segmentation tasks: optic disc/cup segmentation in fundus images (REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain tumor segmentation in MRI scans (BraTS'19 challenge). Compared with representative existing methods, Segtran consistently achieved the highest segmentation accuracy, and exhibited good cross-domain generalization capabilities. The source code of Segtran is released at https://github.com/askerlee/segtran.

</p>
</details>

<details><summary><b>Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection</b>
<a href="https://arxiv.org/abs/2105.09452">arxiv:2105.09452</a>
&#x1F4C8; 7 <br>
<p>Lucas N. Alegre, Ana L. C. Bazzan, Bruno C. da Silva</p></summary>
<p>

**Abstract:** Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP a context. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, or a priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that (i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and (ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.

</p>
</details>

<details><summary><b>Superpixel-based Domain-Knowledge Infusion in Computer Vision</b>
<a href="https://arxiv.org/abs/2105.09448">arxiv:2105.09448</a>
&#x1F4C8; 7 <br>
<p>Gunjan Chhablani, Abheesht Sharma, Harshit Pandey, Tirtharaj Dash</p></summary>
<p>

**Abstract:** Superpixels are higher-order perceptual groups of pixels in an image, often carrying much more information than raw pixels. There is an inherent relational structure to the relationship among different superpixels of an image. This relational information can convey some form of domain information about the image, e.g. relationship between superpixels representing two eyes in a cat image. Our interest in this paper is to construct computer vision models, specifically those based on Deep Neural Networks (DNNs) to incorporate these superpixels information. We propose a methodology to construct a hybrid model that leverages (a) Convolutional Neural Network (CNN) to deal with spatial information in an image, and (b) Graph Neural Network (GNN) to deal with relational superpixel information in the image. The proposed deep model is learned using a generic hybrid loss function that we call a `hybrid' loss. We evaluate the predictive performance of our proposed hybrid vision model on four popular image classification datasets: MNIST, FMNIST, CIFAR-10 and CIFAR-100. Moreover, we evaluate our method on three real-world classification tasks: COVID-19 X-Ray Detection, LFW Face Recognition, and SOCOFing Fingerprint Identification. The results demonstrate that the relational superpixel information provided via a GNN could improve the performance of standard CNN-based vision systems.

</p>
</details>

<details><summary><b>Multiply Robust Causal Mediation Analysis with Continuous Treatments</b>
<a href="https://arxiv.org/abs/2105.09254">arxiv:2105.09254</a>
&#x1F4C8; 7 <br>
<p>AmirEmad Ghassami, Numair Sani, Yizhen Xu, Ilya Shpitser</p></summary>
<p>

**Abstract:** In many applications, researchers are interested in the direct and indirect causal effects of an intervention on an outcome of interest. Mediation analysis offers a rigorous framework for the identification and estimation of such causal quantities. In the case of binary treatment, efficient estimators for the direct and indirect effects are derived by Tchetgen Tchetgen and Shpitser (2012). These estimators are based on influence functions and possess desirable multiple robustness properties. However, they are not readily applicable when treatments are continuous, which is the case in several settings, such as drug dosage in medical applications. In this work, we extend the influence function-based estimator of Tchetgen Tchetgen and Shpitser (2012) to deal with continuous treatments by utilizing a kernel smoothing approach. We first demonstrate that our proposed estimator preserves the multiple robustness property of the estimator in Tchetgen Tchetgen and Shpitser (2012). Then we show that under certain mild regularity conditions, our estimator is asymptotically normal. Our estimation scheme allows for high-dimensional nuisance parameters that can be estimated at slower rates than the target parameter. Additionally, we utilize cross-fitting, which allows for weaker smoothness requirements for the nuisance functions.

</p>
</details>

<details><summary><b>Explainable Tsetlin Machine framework for fake news detection with credibility score assessment</b>
<a href="https://arxiv.org/abs/2105.09114">arxiv:2105.09114</a>
&#x1F4C8; 7 <br>
<p>Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao</p></summary>
<p>

**Abstract:** The proliferation of fake news, i.e., news intentionally spread for misinformation, poses a threat to individuals and society. Despite various fact-checking websites such as PolitiFact, robust detection techniques are required to deal with the increase in fake news. Several deep learning models show promising results for fake news classification, however, their black-box nature makes it difficult to explain their classification decisions and quality-assure the models. We here address this problem by proposing a novel interpretable fake news detection framework based on the recently introduced Tsetlin Machine (TM). In brief, we utilize the conjunctive clauses of the TM to capture lexical and semantic properties of both true and fake news text. Further, we use the clause ensembles to calculate the credibility of fake news. For evaluation, we conduct experiments on two publicly available datasets, PolitiFact and GossipCop, and demonstrate that the TM framework significantly outperforms previously published baselines by at least $5\%$ in terms of accuracy, with the added benefit of an interpretable logic-based representation. Further, our approach provides higher F1-score than BERT and XLNet, however, we obtain slightly lower accuracy. We finally present a case study on our model's explainability, demonstrating how it decomposes into meaningful words and their negations.

</p>
</details>

<details><summary><b>Guided Facial Skin Color Correction</b>
<a href="https://arxiv.org/abs/2105.09034">arxiv:2105.09034</a>
&#x1F4C8; 7 <br>
<p>Keiichiro Shirai, Tatsuya Baba, Shunsuke Ono, Masahiro Okuda, Yusuke Tatesumi, Paul Perrotin</p></summary>
<p>

**Abstract:** This paper proposes an automatic image correction method for portrait photographs, which promotes consistency of facial skin color by suppressing skin color changes due to background colors. In portrait photographs, skin color is often distorted due to the lighting environment (e.g., light reflected from a colored background wall and over-exposure by a camera strobe), and if the photo is artificially combined with another background color, this color change is emphasized, resulting in an unnatural synthesized result. In our framework, after roughly extracting the face region and rectifying the skin color distribution in a color space, we perform color and brightness correction around the face in the original image to achieve a proper color balance of the facial image, which is not affected by luminance and background colors. Unlike conventional algorithms for color correction, our final result is attained by a color correction process with a guide image. In particular, our guided image filtering for the color correction does not require a perfectly-aligned guide image required in the original guide image filtering method proposed by He et al. Experimental results show that our method generates more natural results than conventional methods on not only headshot photographs but also natural scene photographs. We also show automatic yearbook style photo generation as an another application.

</p>
</details>

<details><summary><b>Classifying concepts via visual properties</b>
<a href="https://arxiv.org/abs/2105.09422">arxiv:2105.09422</a>
&#x1F4C8; 6 <br>
<p>Fausto Giunchiglia, Mayukh Bagchi</p></summary>
<p>

**Abstract:** We assume that substances in the world are represented by two types of concepts, namely substance concepts and classification concepts, the former instrumental to (visual) perception, the latter to (language based) classification. Based on this distinction, we introduce a general methodology for building lexico-semantic hierarchies of substance concepts, where nodes are annotated with the media, e.g.,videos or photos, from which substance concepts are extracted, and are associated with the corresponding classification concepts. The methodology is based on Ranganathan's original faceted approach, contextualized to the problem of classifying substance concepts. The key novelty is that the hierarchy is built exploiting the visual properties of substance concepts, while the linguistically defined properties of classification concepts are only used to describe substance concepts. The validity of the approach is exemplified by providing some highlights of an ongoing project whose goal is to build a large scale multimedia multilingual concept hierarchy.

</p>
</details>

<details><summary><b>Heterogeneous Contrastive Learning</b>
<a href="https://arxiv.org/abs/2105.09401">arxiv:2105.09401</a>
&#x1F4C8; 6 <br>
<p>Lecheng Zheng, Yada Zhu, Jingrui He, Jinjun Xiong</p></summary>
<p>

**Abstract:** With the advent of big data across multiple high-impact applications, we are often facing the challenge of complex heterogeneity. The newly collected data usually consist of multiple modalities and characterized with multiple labels, thus exhibiting the co-existence of multiple types of heterogeneity. Although state-of-the-art techniques are good at modeling the complex heterogeneity with sufficient label information, such label information can be quite expensive to obtain in real applications, leading to sub-optimal performance using these techniques. Inspired by the capability of contrastive learning to utilize rich unlabeled data for improving performance, in this paper, we propose a unified heterogeneous learning framework, which combines both weighted unsupervised contrastive loss and weighted supervised contrastive loss to model multiple types of heterogeneity. We also provide theoretical analyses showing that the proposed weighted supervised contrastive loss is the lower bound of the mutual information of two samples from the same class and the weighted unsupervised contrastive loss is the lower bound of the mutual information between the hidden representation of two views of the same sample. Experimental results on real-world data sets demonstrate the effectiveness and the efficiency of the proposed method modeling multiple types of heterogeneity.

</p>
</details>

<details><summary><b>Separation of Powers in Federated Learning</b>
<a href="https://arxiv.org/abs/2105.09400">arxiv:2105.09400</a>
&#x1F4C8; 6 <br>
<p>Pau-Chen Cheng, Kevin Eykholt, Zhongshu Gu, Hani Jamjoom, K. R. Jayaram, Enriquillo Valdez, Ashish Verma</p></summary>
<p>

**Abstract:** Federated Learning (FL) enables collaborative training among mutually distrusting parties. Model updates, rather than training data, are concentrated and fused in a central aggregation server. A key security challenge in FL is that an untrustworthy or compromised aggregation process might lead to unforeseeable information leakage. This challenge is especially acute due to recently demonstrated attacks that have reconstructed large fractions of training data from ostensibly "sanitized" model updates.
  In this paper, we introduce TRUDA, a new cross-silo FL system, employing a trustworthy and decentralized aggregation architecture to break down information concentration with regard to a single aggregator. Based on the unique computational properties of model-fusion algorithms, all exchanged model updates in TRUDA are disassembled at the parameter-granularity and re-stitched to random partitions designated for multiple TEE-protected aggregators. Thus, each aggregator only has a fragmentary and shuffled view of model updates and is oblivious to the model architecture. Our new security mechanisms can fundamentally mitigate training reconstruction attacks, while still preserving the final accuracy of trained models and keeping performance overheads low.

</p>
</details>

<details><summary><b>VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation</b>
<a href="https://arxiv.org/abs/2105.09371">arxiv:2105.09371</a>
&#x1F4C8; 6 <br>
<p>Haresh Karnan, Garrett Warnell, Xuesu Xiao, Peter Stone</p></summary>
<p>

**Abstract:** While imitation learning for vision based autonomous mobile robot navigation has recently received a great deal of attention in the research community, existing approaches typically require state action demonstrations that were gathered using the deployment platform. However, what if one cannot easily outfit their platform to record these demonstration signals or worse yet the demonstrator does not have access to the platform at all? Is imitation learning for vision based autonomous navigation even possible in such scenarios? In this work, we hypothesize that the answer is yes and that recent ideas from the Imitation from Observation (IfO) literature can be brought to bear such that a robot can learn to navigate using only ego centric video collected by a demonstrator, even in the presence of viewpoint mismatch. To this end, we introduce a new algorithm, Visual Observation only Imitation Learning for Autonomous navigation (VOILA), that can successfully learn navigation policies from a single video demonstration collected from a physically different agent. We evaluate VOILA in the photorealistic AirSim simulator and show that VOILA not only successfully imitates the expert, but that it also learns navigation policies that can generalize to novel environments. Further, we demonstrate the effectiveness of VOILA in a real world setting by showing that it allows a wheeled Jackal robot to successfully imitate a human walking in an environment using a video recorded using a mobile phone camera.

</p>
</details>

<details><summary><b>Generative Adversarial Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2105.09356">arxiv:2105.09356</a>
&#x1F4C8; 6 <br>
<p>Seyed Saeed Changiz Rezaei, Fred X. Han, Di Niu, Mohammad Salameh, Keith Mills, Shuo Lian, Wei Lu, Shangling Jui</p></summary>
<p>

**Abstract:** Despite the empirical success of neural architecture search (NAS) in deep learning applications, the optimality, reproducibility and cost of NAS schemes remain hard to assess. In this paper, we propose Generative Adversarial NAS (GA-NAS) with theoretically provable convergence guarantees, promoting stability and reproducibility in neural architecture search. Inspired by importance sampling, GA-NAS iteratively fits a generator to previously discovered top architectures, thus increasingly focusing on important parts of a large search space. Furthermore, we propose an efficient adversarial learning approach, where the generator is trained by reinforcement learning based on rewards provided by a discriminator, thus being able to explore the search space without evaluating a large number of architectures. Extensive experiments show that GA-NAS beats the best published results under several cases on three public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search constraints and search spaces. We show that GA-NAS can be used to improve already optimized baselines found by other NAS methods, including EfficientNet and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in their original search space.

</p>
</details>

<details><summary><b>Unsupervised Discriminative Learning of Sounds for Audio Event Classification</b>
<a href="https://arxiv.org/abs/2105.09279">arxiv:2105.09279</a>
&#x1F4C8; 6 <br>
<p>Sascha Hornauer, Ke Li, Stella X. Yu, Shabnam Ghaffarzadegan, Liu Ren</p></summary>
<p>

**Abstract:** Recent progress in network-based audio event classification has shown the benefit of pre-training models on visual data such as ImageNet. While this process allows knowledge transfer across different domains, training a model on large-scale visual datasets is time consuming. On several audio event classification benchmarks, we show a fast and effective alternative that pre-trains the model unsupervised, only on audio data and yet delivers on-par performance with ImageNet pre-training. Furthermore, we show that our discriminative audio learning can be used to transfer knowledge across audio datasets and optionally include ImageNet pre-training.

</p>
</details>

<details><summary><b>Provable Guarantees on the Robustness of Decision Rules to Causal Interventions</b>
<a href="https://arxiv.org/abs/2105.09108">arxiv:2105.09108</a>
&#x1F4C8; 6 <br>
<p>Benjie Wang, Clare Lyle, Marta Kwiatkowska</p></summary>
<p>

**Abstract:** Robustness of decision rules to shifts in the data-generating process is crucial to the successful deployment of decision-making systems. Such shifts can be viewed as interventions on a causal graph, which capture (possibly hypothetical) changes in the data-generating process, whether due to natural reasons or by the action of an adversary. We consider causal Bayesian networks and formally define the interventional robustness problem, a novel model-based notion of robustness for decision functions that measures worst-case performance with respect to a set of interventions that denote changes to parameters and/or causal influences. By relying on a tractable representation of Bayesian networks as arithmetic circuits, we provide efficient algorithms for computing guaranteed upper and lower bounds on the interventional robustness probabilities. Experimental results demonstrate that the methods yield useful and interpretable bounds for a range of practical networks, paving the way towards provably causally robust decision-making systems.

</p>
</details>

<details><summary><b>Mill.jl and JsonGrinder.jl: automated differentiable feature extraction for learning from raw JSON data</b>
<a href="https://arxiv.org/abs/2105.09107">arxiv:2105.09107</a>
&#x1F4C8; 6 <br>
<p>Simon Mandlik, Matej Racinsky, Viliam Lisy, Tomas Pevny</p></summary>
<p>

**Abstract:** Learning from raw data input, thus limiting the need for manual feature engineering, is one of the key components of many successful applications of machine learning methods. While machine learning problems are often formulated on data that naturally translate into a vector representation suitable for classifiers, there are data sources, for example in cybersecurity, that are naturally represented in diverse files with a unifying hierarchical structure, such as XML, JSON, and Protocol Buffers. Converting this data to vector (tensor) representation is generally done by manual feature engineering, which is laborious, lossy, and prone to human bias about the importance of particular features.
  Mill and JsonGrinder is a tandem of libraries, which fully automates the conversion. Starting with an arbitrary set of JSON samples, they create a differentiable machine learning model capable of infer from further JSON samples in their raw form.

</p>
</details>

<details><summary><b>Errors-in-Variables for deep learning: rethinking aleatoric uncertainty</b>
<a href="https://arxiv.org/abs/2105.09095">arxiv:2105.09095</a>
&#x1F4C8; 6 <br>
<p>Jörg Martin, Clemens Elster</p></summary>
<p>

**Abstract:** We present a Bayesian treatment for deep regression using an Errors-in-Variables model which accounts for the uncertainty associated with the input to the employed neural network. It is shown how the treatment can be combined with already existing approaches for uncertainty quantification that are based on variational inference. Our approach yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We illustrate and discuss the approach along various toy and real world examples.

</p>
</details>

<details><summary><b>Local Aggressive Adversarial Attacks on 3D Point Cloud</b>
<a href="https://arxiv.org/abs/2105.09090">arxiv:2105.09090</a>
&#x1F4C8; 6 <br>
<p>Yiming Sun, Feng Chen, Zhiyu Chen, Mingjie Wang</p></summary>
<p>

**Abstract:** Deep neural networks are found to be prone to adversarial examples which could deliberately fool the model to make mistakes. Recently, a few of works expand this task from 2D image to 3D point cloud by using global point cloud optimization. However, the perturbations of global point are not effective for misleading the victim model. First, not all points are important in optimization toward misleading. Abundant points account considerable distortion budget but contribute trivially to attack. Second, the multi-label optimization is suboptimal for adversarial attack, since it consumes extra energy in finding multi-label victim model collapse and causes instance transformation to be dissimilar to any particular instance. Third, the independent adversarial and perceptibility losses, caring misclassification and dissimilarity separately, treat the updating of each point equally without a focus. Therefore, once perceptibility loss approaches its budget threshold, all points would be stock in the surface of hypersphere and attack would be locked in local optimality. Therefore, we propose a local aggressive adversarial attacks (L3A) to solve above issues. Technically, we select a bunch of salient points, the high-score subset of point cloud according to gradient, to perturb. Then a flow of aggressive optimization strategies are developed to reinforce the unperceptive generation of adversarial examples toward misleading victim models. Extensive experiments on PointNet, PointNet++ and DGCNN demonstrate the state-of-the-art performance of our method against existing adversarial attack methods.

</p>
</details>

<details><summary><b>Disentanglement Learning for Variational Autoencoders Applied to Audio-Visual Speech Enhancement</b>
<a href="https://arxiv.org/abs/2105.08970">arxiv:2105.08970</a>
&#x1F4C8; 6 <br>
<p>Guillaume Carbajal, Julius Richter, Timo Gerkmann</p></summary>
<p>

**Abstract:** Recently, the standard variational autoencoder has been successfully used to learn a probabilistic prior over speech signals, which is then used to perform speech enhancement. Variational autoencoders have then been conditioned on a label describing a high-level speech attribute (e.g. speech activity) that allows for a more explicit control of speech generation. However, the label is not guaranteed to be disentangled from the other latent variables, which results in limited performance improvements compared to the standard variational autoencoder. In this work, we propose to use an adversarial training scheme for variational autoencoders to disentangle the label from the other latent variables. At training, we use a discriminator that competes with the encoder of the variational autoencoder. Simultaneously, we also use an additional encoder that estimates the label for the decoder of the variational autoencoder, which proves to be crucial to learn disentanglement. We show the benefit of the proposed disentanglement learning when a voice activity label, estimated from visual data, is used for speech enhancement.

</p>
</details>

<details><summary><b>Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional Network and Airport Situational Awareness Map</b>
<a href="https://arxiv.org/abs/2105.08969">arxiv:2105.08969</a>
&#x1F4C8; 6 <br>
<p>Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim</p></summary>
<p>

**Abstract:** To model and forecast flight delays accurately, it is crucial to harness various vehicle trajectory and contextual sensor data on airport tarmac areas. These heterogeneous sensor data, if modelled correctly, can be used to generate a situational awareness map. Existing techniques apply traditional supervised learning methods onto historical data, contextual features and route information among different airports to predict flight delay are inaccurate and only predict arrival delay but not departure delay, which is essential to airlines. In this paper, we propose a vision-based solution to achieve a high forecasting accuracy, applicable to the airport. Our solution leverages a snapshot of the airport situational awareness map, which contains various trajectories of aircraft and contextual features such as weather and airline schedules. We propose an end-to-end deep learning architecture, TrajCNN, which captures both the spatial and temporal information from the situational awareness map. Additionally, we reveal that the situational awareness map of the airport has a vital impact on estimating flight departure delay. Our proposed framework obtained a good result (around 18 minutes error) for predicting flight departure delay at Los Angeles International Airport.

</p>
</details>

<details><summary><b>Decomposing reverse-mode automatic differentiation</b>
<a href="https://arxiv.org/abs/2105.09469">arxiv:2105.09469</a>
&#x1F4C8; 5 <br>
<p>Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, Alexey Radul</p></summary>
<p>

**Abstract:** We decompose reverse-mode automatic differentiation into (forward-mode) linearization followed by transposition. Doing so isolates the essential difference between forward- and reverse-mode AD, and simplifies their joint implementation. In particular, once forward-mode AD rules are defined for every primitive operation in a source language, only linear primitives require an additional transposition rule in order to arrive at a complete reverse-mode AD implementation. This is how reverse-mode AD is written in JAX and Dex.

</p>
</details>

<details><summary><b>Latent Gaussian Model Boosting</b>
<a href="https://arxiv.org/abs/2105.08966">arxiv:2105.08966</a>
&#x1F4C8; 5 <br>
<p>Fabio Sigrist</p></summary>
<p>

**Abstract:** Latent Gaussian models and boosting are widely used techniques in statistics and machine learning. Tree-boosting shows excellent predictive accuracy on many data sets, but potential drawbacks are that it assumes conditional independence of samples, produces discontinuous predictions for, e.g., spatial data, and it can have difficulty with high-cardinality categorical variables. Latent Gaussian models, such as Gaussian process and grouped random effects models, are flexible prior models that allow for making probabilistic predictions. However, existing latent Gaussian models usually assume either a zero or a linear prior mean function which can be an unrealistic assumption. This article introduces a novel approach that combines boosting and latent Gaussian models in order to remedy the above-mentioned drawbacks and to leverage the advantages of both techniques. We obtain increased predictive accuracy compared to existing approaches in both simulated and real-world data experiments.

</p>
</details>

<details><summary><b>Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection</b>
<a href="https://arxiv.org/abs/2105.09509">arxiv:2105.09509</a>
&#x1F4C8; 4 <br>
<p>Shirong Shen, Tongtong Wu, Guilin Qi, Yuan-Fang Li, Gholamreza Haffari, Sheng Bi</p></summary>
<p>

**Abstract:** Event detection (ED) aims at detecting event trigger words in sentences and classifying them into specific event types. In real-world applications, ED typically does not have sufficient labelled data, thus can be formulated as a few-shot learning problem. To tackle the issue of low sample diversity in few-shot ED, we propose a novel knowledge-based few-shot event detection method which uses a definition-based encoder to introduce external event knowledge as the knowledge prior of event types. Furthermore, as external knowledge typically provides limited and imperfect coverage of event types, we introduce an adaptive knowledge-enhanced Bayesian meta-learning method to dynamically adjust the knowledge prior of event types. Experiments show our method consistently and substantially outperforms a number of baselines by at least 15 absolute F1 points under the same few-shot settings.

</p>
</details>

<details><summary><b>Physics-informed neural networks (PINNs) for fluid mechanics: A review</b>
<a href="https://arxiv.org/abs/2105.09506">arxiv:2105.09506</a>
&#x1F4C8; 4 <br>
<p>Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, George Em Karniadakis</p></summary>
<p>

**Abstract:** Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier-Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implementing them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.

</p>
</details>

<details><summary><b>L1 Regression with Lewis Weights Subsampling</b>
<a href="https://arxiv.org/abs/2105.09433">arxiv:2105.09433</a>
&#x1F4C8; 4 <br>
<p>Aditya Parulekar, Advait Parulekar, Eric Price</p></summary>
<p>

**Abstract:** We consider the problem of finding an approximate solution to $\ell_1$ regression while only observing a small number of labels. Given an $n \times d$ unlabeled data matrix $X$, we must choose a small set of $m \ll n$ rows to observe the labels of, then output an estimate $\widehatβ$ whose error on the original problem is within a $1 + \varepsilon$ factor of optimal. We show that sampling from $X$ according to its Lewis weights and outputting the empirical minimizer succeeds with probability $1-δ$ for $m > O(\frac{1}{\varepsilon^2} d \log \frac{d}{\varepsilon δ})$. This is analogous to the performance of sampling according to leverage scores for $\ell_2$ regression, but with exponentially better dependence on $δ$. We also give a corresponding lower bound of $Ω(\frac{d}{\varepsilon^2} + (d + \frac{1}{\varepsilon^2}) \log\frac{1}δ)$.

</p>
</details>

<details><summary><b>Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder</b>
<a href="https://arxiv.org/abs/2105.09428">arxiv:2105.09428</a>
&#x1F4C8; 4 <br>
<p>Chuhong Lahlou, Ancil Crayton, Caroline Trier, Evan Willett</p></summary>
<p>

**Abstract:** In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to predict risk in value-based care for incorporation into CMS Innovation Center payment and service delivery models. Recently, modern language models have played key roles in a number of health related tasks. This paper presents, to the best of our knowledge, the first application of these models to patient readmission prediction. To facilitate this, we create a dataset of 1.2 million medical history samples derived from the Limited Dataset (LDS) issued by CMS. Moreover, we propose a comprehensive modeling solution centered on a deep learning framework for this data. To demonstrate the framework, we train an attention-based Transformer to learn Medicare semantics in support of performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91 recall on readmission classification. We also introduce a novel data pre-processing pipeline and discuss pertinent deployment considerations surrounding model explainability and bias.

</p>
</details>

<details><summary><b>Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network</b>
<a href="https://arxiv.org/abs/2105.09378">arxiv:2105.09378</a>
&#x1F4C8; 4 <br>
<p>Fasil Gadjimuradov, Thomas Benkert, Marcel Dominik Nickel, Andreas Maier</p></summary>
<p>

**Abstract:** Purpose: To develop an algorithm for robust partial Fourier (PF) reconstruction applicable to diffusion-weighted (DW) images with non-smooth phase variations.
  Methods: Based on an unrolled proximal splitting algorithm, a neural network architecture is derived which alternates between data consistency operations and regularization implemented by recurrent convolutions. In order to exploit correlations, multiple repetitions of the same slice are jointly reconstructed under consideration of permutation-equivariance. The proposed method is trained on DW liver data of 60 volunteers and evaluated on retrospectively and prospectively sub-sampled data of different anatomies and resolutions. In addition, the benefits of using a recurrent network over other unrolling strategies is investigated.
  Results: Conventional PF techniques can be significantly outperformed in terms of quantitative measures as well as perceptual image quality. The proposed method is able to generalize well to brain data with contrasts and resolution not present in the training set. The reduction in echo time (TE) associated with prospective PF-sampling enables DW imaging with higher signal. Also, the TE increase in acquisitions with higher resolution can be compensated for. It can be shown that unrolling by means of a recurrent network produced better results than using a weight-shared network or a cascade of networks.
  Conclusion: This work demonstrates that robust PF reconstruction of DW data is feasible even at strong PF factors in applications with severe phase variations. Since the proposed method does not rely on smoothness priors of the phase but uses learned recurrent convolutions instead, artifacts of conventional PF methods can be avoided.

</p>
</details>

<details><summary><b>Exploring The Limits Of Data Augmentation For Retinal Vessel Segmentation</b>
<a href="https://arxiv.org/abs/2105.09365">arxiv:2105.09365</a>
&#x1F4C8; 4 <br>
<p>Enes Sadi Uysal, M. Şafak Bilici, B. Selin Zaza, M. Yiğit Özgenç, Onur Boyar</p></summary>
<p>

**Abstract:** Retinal Vessel Segmentation is important for the diagnosis of various diseases. The research on retinal vessel segmentation focuses mainly on the improvement of the segmentation model which is usually based on U-Net architecture. In our study, we use the U-Net architecture and we rely on heavy data augmentation in order to achieve better performance. The success of the data augmentation relies on successfully addressing the problem of input images. By analyzing input images and performing the augmentation accordingly we show that the performance of the U-Net model can be increased dramatically. Results are reported using the most widely used retina dataset, DRIVE.

</p>
</details>

<details><summary><b>Methods for Detoxification of Texts for the Russian Language</b>
<a href="https://arxiv.org/abs/2105.09052">arxiv:2105.09052</a>
&#x1F4C8; 4 <br>
<p>Daryna Dementieva, Daniil Moskovskiy, Varvara Logacheva, David Dale, Olga Kozlova, Nikita Semenov, Alexander Panchenko</p></summary>
<p>

**Abstract:** We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models - unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model - and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.

</p>
</details>

<details><summary><b>A Novel lightweight Convolutional Neural Network, ExquisiteNetV2</b>
<a href="https://arxiv.org/abs/2105.09008">arxiv:2105.09008</a>
&#x1F4C8; 4 <br>
<p>Shi-Yao Zhou, Chung-Yen Su</p></summary>
<p>

**Abstract:** In the paper of ExquisiteNetV1, the ability of classification of ExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and better model ExquisiteNetV2. We conduct many experiments to evaluate its performance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known models on 15 credible datasets under the same condition. According to the experimental results, ExquisiteNetV2 gets the highest classification accuracy over half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts of parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing speed.

</p>
</details>

<details><summary><b>Physics Validation of Novel Convolutional 2D Architectures for Speeding Up High Energy Physics Simulations</b>
<a href="https://arxiv.org/abs/2105.08960">arxiv:2105.08960</a>
&#x1F4C8; 4 <br>
<p>Florian Rehm, Sofia Vallecorsa, Kerstin Borras, Dirk Krücker</p></summary>
<p>

**Abstract:** The precise simulation of particle transport through detectors remains a key element for the successful interpretation of high energy physics results. However, Monte Carlo based simulation is extremely demanding in terms of computing resources. This challenge motivates investigations of faster, alternative approaches for replacing the standard Monte Carlo approach.
  We apply Generative Adversarial Networks (GANs), a deep learning technique, to replace the calorimeter detector simulations and speeding up the simulation time by orders of magnitude. We follow a previous approach which used three-dimensional convolutional neural networks and develop new two-dimensional convolutional networks to solve the same 3D image generation problem faster. Additionally, we increased the number of parameters and the neural networks representational power, obtaining a higher accuracy. We compare our best convolutional 2D neural network architecture and evaluate it versus the previous 3D architecture and Geant4 data. Our results demonstrate a high physics accuracy and further consolidate the use of GANs for fast detector simulations.

</p>
</details>

<details><summary><b>Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization</b>
<a href="https://arxiv.org/abs/2105.11908">arxiv:2105.11908</a>
&#x1F4C8; 3 <br>
<p>M. Lautaro Hickmann, Fabian Wurzberger, Megi Hoxhalli, Arne Lochner, Jessica Töllich, Ansgar Scherp</p></summary>
<p>

**Abstract:** Modern multi-document summarization (MDS) methods are based on transformer architectures. They generate state of the art summaries, but lack explainability. We focus on graph-based transformer models for MDS as they gained recent popularity. We aim to improve the explainability of the graph-based MDS by analyzing their attention weights. In a graph-based MDS such as GraphSum, vertices represent the textual units, while the edges form some similarity graph over the units. We compare GraphSum's performance utilizing different textual units, i. e., sentences versus paragraphs, on two news benchmark datasets, namely WikiSum and MultiNews. Our experiments show that paragraph-level representations provide the best summarization performance. Thus, we subsequently focus oAnalysisn analyzing the paragraph-level attention weights of GraphSum's multi-heads and decoding layers in order to improve the explainability of a transformer-based MDS model. As a reference metric, we calculate the ROUGE scores between the input paragraphs and each sentence in the generated summary, which indicate source origin information via text similarity. We observe a high correlation between the attention weights and this reference metric, especially on the the later decoding layers of the transformer architecture. Finally, we investigate if the generated summaries follow a pattern of positional bias by extracting which paragraph provided the most information for each generated summary. Our results show that there is a high correlation between the position in the summary and the source origin.

</p>
</details>

<details><summary><b>A Preference Random Walk Algorithm for Link Prediction through Mutual Influence Nodes in Complex Networks</b>
<a href="https://arxiv.org/abs/2105.09494">arxiv:2105.09494</a>
&#x1F4C8; 3 <br>
<p>Kamal Berahmand, Elahe Nasiri, Saman Forouzandeh, Yuefeng Li</p></summary>
<p>

**Abstract:** Predicting links in complex networks has been one of the essential topics within the realm of data mining and science discovery over the past few years. This problem remains an attempt to identify future, deleted, and redundant links using the existing links in a graph. Local random walk is considered to be one of the most well-known algorithms in the category of quasi-local methods. It traverses the network using the traditional random walk with a limited number of steps, randomly selecting one adjacent node in each step among the nodes which have equal importance. Then this method uses the transition probability between node pairs to calculate the similarity between them. However, in most datasets, this method is not able to perform accurately in scoring remarkably similar nodes. In the present article, an efficient method is proposed for improving local random walk by encouraging random walk to move, in every step, towards the node which has a stronger influence. Therefore, the next node is selected according to the influence of the source node. To do so, using mutual information, the concept of the asymmetric mutual influence of nodes is presented. A comparison between the proposed method and other similarity-based methods (local, quasi-local, and global) has been performed, and results have been reported for 11 real-world networks. It had a higher prediction accuracy compared with other link prediction approaches.

</p>
</details>

<details><summary><b>Localization and Control of Magnetic Suture Needles in Cluttered Surgical Site with Blood and Tissue</b>
<a href="https://arxiv.org/abs/2105.09481">arxiv:2105.09481</a>
&#x1F4C8; 3 <br>
<p>Will Pryor, Yotam Barnoy, Suraj Raval, Xiaolong Liu, Lamar Mair, Daniel Lerner, Onder Erin, Gregory D. Hager, Yancy Diaz-Mercado, Axel Krieger</p></summary>
<p>

**Abstract:** Real-time visual localization of needles is necessary for various surgical applications, including surgical automation and visual feedback. In this study we investigate localization and autonomous robotic control of needles in the context of our magneto-suturing system. Our system holds the potential for surgical manipulation with the benefit of minimal invasiveness and reduced patient side effects. However, the non-linear magnetic fields produce unintuitive forces and demand delicate position-based control that exceeds the capabilities of direct human manipulation. This makes automatic needle localization a necessity. Our localization method combines neural network-based segmentation and classical techniques, and we are able to consistently locate our needle with 0.73 mm RMS error in clean environments and 2.72 mm RMS error in challenging environments with blood and occlusion. The average localization RMS error is 2.16 mm for all environments we used in the experiments. We combine this localization method with our closed-loop feedback control system to demonstrate the further applicability of localization to autonomous control. Our needle is able to follow a running suture path in (1) no blood, no tissue; (2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood, with tissue environments. The tip position tracking error ranges from 2.6 mm to 3.7 mm RMS, opening the door towards autonomous suturing tasks.

</p>
</details>

<details><summary><b>A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver</b>
<a href="https://arxiv.org/abs/2105.09446">arxiv:2105.09446</a>
&#x1F4C8; 3 <br>
<p>Evan Lowe, Levent Guvenç</p></summary>
<p>

**Abstract:** As passenger vehicle technologies have advanced, so have their capabilities to avoid obstacles, especially with developments in tires, suspensions, steering, as well as safety technologies like ABS, ESC, and more recently, ADAS systems. However, environments around passenger vehicles have also become more complex, and dangerous. There have previously been studies that outline driver tendencies and performance capabilities when attempting to avoid obstacles while driving passenger vehicles. Now that autonomous vehicles are being developed with obstacle avoidance capabilities, it is important to target performance that meets or exceeds that of human drivers. This manuscript highlights systems that are crucial for an emergency obstacle avoidance maneuver (EOAM) and identifies the state-of-the-art for each of the related systems, while considering the nuances of traveling at highway speeds. Some of the primary EOAM-related systems/areas that are discussed in this review are: general path planning methods, system hierarchies, decision-making, trajectory generation, and trajectory-tracking control methods. After concluding remarks, suggestions for future work which could lead to an ideal EOAM development, are discussed.

</p>
</details>

<details><summary><b>Surprisingly Popular Voting Recovers Rankings, Surprisingly!</b>
<a href="https://arxiv.org/abs/2105.09386">arxiv:2105.09386</a>
&#x1F4C8; 3 <br>
<p>Hadi Hosseini, Debmalya Mandal, Nisarg Shah, Kevin Shi</p></summary>
<p>

**Abstract:** The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \emph{prediction} of other individuals' votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches.

</p>
</details>

<details><summary><b>Analyzing Machine Learning Approaches for Online Malware Detection in Cloud</b>
<a href="https://arxiv.org/abs/2105.09268">arxiv:2105.09268</a>
&#x1F4C8; 3 <br>
<p>Jeffrey C Kimmell, Mahmoud Abdelsalam, Maanak Gupta</p></summary>
<p>

**Abstract:** The variety of services and functionality offered by various cloud service providers (CSP) have exploded lately. Utilizing such services has created numerous opportunities for enterprises infrastructure to become cloud-based and, in turn, assisted the enterprises to easily and flexibly offer services to their customers. The practice of renting out access to servers to clients for computing and storage purposes is known as Infrastructure as a Service (IaaS). The popularity of IaaS has led to serious and critical concerns with respect to the cyber security and privacy. In particular, malware is often leveraged by malicious entities against cloud services to compromise sensitive data or to obstruct their functionality. In response to this growing menace, malware detection for cloud environments has become a widely researched topic with numerous methods being proposed and deployed. In this paper, we present online malware detection based on process level performance metrics, and analyze the effectiveness of different baseline machine learning models including, Support Vector Classifier (SVC), Random Forest Classifier (RFC), KNearest Neighbor (KNN), Gradient Boosted Classifier (GBC), Gaussian Naive Bayes (GNB) and Convolutional Neural Networks (CNN). Our analysis conclude that neural network models can most accurately detect the impact malware have on the process level features of virtual machines in the cloud, and therefore are best suited to detect them. Our models were trained, validated, and tested by using a dataset of 40,680 malicious and benign samples. The dataset was complied by running different families of malware (collected from VirusTotal) in a live cloud environment and collecting the process level features.

</p>
</details>

<details><summary><b>Adaptive Hypergraph Convolutional Network for No-Reference 360-degree Image Quality Assessment</b>
<a href="https://arxiv.org/abs/2105.09143">arxiv:2105.09143</a>
&#x1F4C8; 3 <br>
<p>Jun Fu, Chen Hou, Wei Zhou, Jiahua Xu, Zhibo Chen</p></summary>
<p>

**Abstract:** In no-reference 360-degree image quality assessment (NR 360IQA), graph convolutional networks (GCNs), which model interactions between viewports through graphs, have achieved impressive performance. However, prevailing GCN-based NR 360IQA methods suffer from three main limitations. First, they only use high-level features of the distorted image to regress the quality score, while the human visual system (HVS) scores the image based on hierarchical features. Second, they simplify complex high-order interactions between viewports in a pairwise fashion through graphs. Third, in the graph construction, they only consider spatial locations of viewports, ignoring its content characteristics. Accordingly, to address these issues, we propose an adaptive hypergraph convolutional network for NR 360IQA, denoted as AHGCN. Specifically, we first design a multi-level viewport descriptor for extracting hierarchical representations from viewports. Then, we model interactions between viewports through hypergraphs, where each hyperedge connects two or more viewports. In the hypergraph construction, we build a location-based hyperedge and a content-based hyperedge for each viewport. Experimental results on two public 360IQA databases demonstrate that our proposed approach has a clear advantage over state-of-the-art full-reference and no-reference IQA models.

</p>
</details>

<details><summary><b>Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?</b>
<a href="https://arxiv.org/abs/2105.09142">arxiv:2105.09142</a>
&#x1F4C8; 3 <br>
<p>Maxime Peyrard, Beatriz Borges, Kristina Gligorić, Robert West</p></summary>
<p>

**Abstract:** The automatic detection of humor poses a grand challenge for natural language processing. Transformer-based systems have recently achieved remarkable results on this task, but they usually (1)~were evaluated in setups where serious vs humorous texts came from entirely different sources, and (2)~focused on benchmarking performance without providing insights into how the models work. We make progress in both respects by training and analyzing transformer-based humor recognition models on a recently introduced dataset consisting of minimal pairs of aligned sentences, one serious, the other humorous. We find that, although our aligned dataset is much harder than previous datasets, transformer-based models recognize the humorous sentence in an aligned pair with high accuracy (78%). In a careful error analysis, we characterize easy vs hard instances. Finally, by analyzing attention weights, we obtain important insights into the mechanisms by which transformers recognize humor. Most remarkably, we find clear evidence that one single attention head learns to recognize the words that make a test sentence humorous, even without access to this information at training time.

</p>
</details>

<details><summary><b>Obstructing Classification via Projection</b>
<a href="https://arxiv.org/abs/2105.09047">arxiv:2105.09047</a>
&#x1F4C8; 3 <br>
<p>Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckman, Jérôme Urhausen, Kevin Verbeek</p></summary>
<p>

**Abstract:** Machine learning and data mining techniques are effective tools to classify large amounts of data. But they tend to preserve any inherent bias in the data, for example, with regards to gender or race. Removing such bias from data or the learned representations is quite challenging. In this paper we study a geometric problem which models a possible approach for bias removal. Our input is a set of points P in Euclidean space R^d and each point is labeled with k binary-valued properties. A priori we assume that it is "easy" to classify the data according to each property. Our goal is to obstruct the classification according to one property by a suitable projection to a lower-dimensional Euclidean space R^m (m < d), while classification according to all other properties remains easy.
  What it means for classification to be easy depends on the classification model used. We first consider classification by linear separability as employed by support vector machines. We use Kirchberger's Theorem to show that, under certain conditions, a simple projection to R^(d-1) suffices to eliminate the linear separability of one of the properties whilst maintaining the linear separability of the other properties. We also study the problem of maximizing the linear "inseparability" of the chosen property. Second, we consider more complex forms of separability and prove a connection between the number of projections required to obstruct classification and the Helly-type properties of such separabilities.

</p>
</details>

<details><summary><b>When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics</b>
<a href="https://arxiv.org/abs/2105.08997">arxiv:2105.08997</a>
&#x1F4C8; 3 <br>
<p>Iuliia Pliushch, Martin Mundt, Nicolas Lupp, Visvanathan Ramesh</p></summary>
<p>

**Abstract:** Although a plethora of architectural variants for deep classification has been introduced over time, recent works have found empirical evidence towards similarities in their training process. It has been hypothesized that neural networks converge not only to similar representations, but also exhibit a notion of empirical agreement on which data instances are learned first. Following in the latter works$'$ footsteps, we define a metric to quantify the relationship between such classification agreement over time, and posit that the agreement phenomenon can be mapped to core statistics of the investigated dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal, ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to be independent of specific architectures, training hyper-parameters or labels, albeit follows an ordering according to image statistics.

</p>
</details>

<details><summary><b>Prototype Guided Federated Learning of Visual Feature Representations</b>
<a href="https://arxiv.org/abs/2105.08982">arxiv:2105.08982</a>
&#x1F4C8; 3 <br>
<p>Umberto Michieli, Mete Ozay</p></summary>
<p>

**Abstract:** Federated Learning (FL) is a framework which enables distributed model training using a large corpus of decentralized training data. Existing methods aggregate models disregarding their internal representations, which are crucial for training models in vision tasks. System and statistical heterogeneity (e.g., highly imbalanced and non-i.i.d. data) further harm model training. To this end, we introduce a method, called FedProto, which computes client deviations using margins of prototypical representations learned on distributed data, and applies them to drive federated optimization via an attention mechanism. In addition, we propose three methods to analyse statistical properties of feature representations learned in FL, in order to elucidate the relationship between accuracy, margins and feature discrepancy of FL models. In experimental analyses, FedProto demonstrates state-of-the-art accuracy and convergence rate across image classification and semantic segmentation benchmarks by enabling maximum margin training of FL models. Moreover, FedProto reduces uncertainty of predictions of FL models compared to the baseline. To our knowledge, this is the first work evaluating FL models in dense prediction tasks, such as semantic segmentation.

</p>
</details>

<details><summary><b>BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer</b>
<a href="https://arxiv.org/abs/2105.08952">arxiv:2105.08952</a>
&#x1F4C8; 3 <br>
<p>Haoping Bai, Meng Cao, Ping Huang, Jiulong Shan</p></summary>
<p>

**Abstract:** As the applications of deep learning models on edge devices increase at an accelerating pace, fast adaptation to various scenarios with varying resource constraints has become a crucial aspect of model deployment. As a result, model optimization strategies with adaptive configuration are becoming increasingly popular. While single-shot quantized neural architecture search enjoys flexibility in both model architecture and quantization policy, the combined search space comes with many challenges, including instability when training the weight-sharing supernet and difficulty in navigating the exponentially growing search space. Existing methods tend to either limit the architecture search space to a small set of options or limit the quantization policy search space to fixed precision policies. To this end, we propose BatchQuant, a robust quantizer formulation that allows fast and stable training of a compact, single-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to train a compact supernet (offering over $10^{76}$ quantized subnets) within substantially fewer GPU hours than previous methods. Our approach, Quantized-for-all (QFA), is the first to seamlessly extend one-shot weight-sharing NAS supernet to support subnets with arbitrary ultra-low bitwidth mixed-precision quantization policies without retraining. QFA opens up new possibilities in joint hardware-aware neural architecture search and quantization. We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under a low complexity constraint ($<20$ MFLOPs). The code and models will be made publicly available at https://github.com/bhpfelix/QFA.

</p>
</details>

<details><summary><b>Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network</b>
<a href="https://arxiv.org/abs/2105.08949">arxiv:2105.08949</a>
&#x1F4C8; 3 <br>
<p>Chun-Mei Feng, Huazhu Fu, Shuhao Yuan, Yong Xu</p></summary>
<p>

**Abstract:** Super-resolution (SR) plays a crucial role in improving the image quality of magnetic resonance imaging (MRI). MRI produces multi-contrast images and can provide a clear display of soft tissues. However, current super-resolution methods only employ a single contrast, or use a simple multi-contrast fusion mechanism, ignoring the rich relations among different contrasts, which are valuable for improving SR. In this work, we propose a multi-stage integration network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the dependencies between multi-contrast images at different stages to guide image SR. In particular, our MINet first learns a hierarchical feature representation from multiple convolutional stages for each of different-contrast image. Subsequently, we introduce a multi-stage integration module to mine the comprehensive relations between the representations of the multi-contrast images. Specifically, the module matches each representation with all other features, which are integrated in terms of their similarities to obtain an enriched representation. Extensive experiments on fastMRI and real-world clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art multi-contrast SR methods in terms of various metrics and 2) our multi-stage integration module is able to excavate complex interactions among multi-contrast features at different stages, leading to improved target-image quality.

</p>
</details>

<details><summary><b>Drone-based AI and 3D Reconstruction for Digital Twin Augmentation</b>
<a href="https://arxiv.org/abs/2106.03797">arxiv:2106.03797</a>
&#x1F4C8; 2 <br>
<p>Alex To, Maican Liu, Muhammad Hazeeq Bin Muhammad Hairul, Joseph G. Davis, Jeannie S. A. Lee, Henrik Hesse, Hoang D. Nguyen</p></summary>
<p>

**Abstract:** Digital Twin is an emerging technology at the forefront of Industry 4.0, with the ultimate goal of combining the physical space and the virtual space. To date, the Digital Twin concept has been applied in many engineering fields, providing useful insights in the areas of engineering design, manufacturing, automation, and construction industry. While the nexus of various technologies opens up new opportunities with Digital Twin, the technology requires a framework to integrate the different technologies, such as the Building Information Model used in the Building and Construction industry. In this work, an Information Fusion framework is proposed to seamlessly fuse heterogeneous components in a Digital Twin framework from the variety of technologies involved. This study aims to augment Digital Twin in buildings with the use of AI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a drone-based Digital Twin augmentation framework with reusable and customisable components. A proof of concept is also developed, and extensive evaluation is conducted for 3D reconstruction and applications of AI for defect detection.

</p>
</details>

<details><summary><b>Dynamic region proposal networks for semantic segmentation in automated glaucoma screening</b>
<a href="https://arxiv.org/abs/2105.11364">arxiv:2105.11364</a>
&#x1F4C8; 2 <br>
<p>Shivam Shah, Nikhil Kasukurthi, Harshit Pande</p></summary>
<p>

**Abstract:** Screening for the diagnosis of glaucoma through a fundus image can be determined by the optic cup to disc diameter ratio (CDR), which requires the segmentation of the cup and disc regions. In this paper, we propose two novel approaches, namely Parameter-Shared Branched Network (PSBN) andWeak Region of Interest Model-based segmentation (WRoIM) to identify disc and cup boundaries. Unlike the previous approaches, the proposed methods are trained end-to-end through a single neural network architecture and use dynamic cropping instead of manual or traditional computer vision-based cropping. We are able to achieve similar performance as that of state-of-the-art approaches with less number of network parameters. Our experiments include comparison with different best known methods on publicly available Drishti-GS1 and RIM-ONE v3 datasets. With $7.8 \times 10^6$ parameters our approach achieves a Dice score of 0.96/0.89 for disc/cup segmentation on Drishti-GS1 data whereas the existing state-of-the-art approach uses $19.8\times 10^6$ parameters to achieve a dice score of 0.97/0.89.

</p>
</details>

<details><summary><b>Speech & Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine</b>
<a href="https://arxiv.org/abs/2105.09406">arxiv:2105.09406</a>
&#x1F4C8; 2 <br>
<p>Behzad Javaheri</p></summary>
<p>

**Abstract:** Herein, we have compared the performance of SVM and MLP in emotion recognition using speech and song channels of the RAVDESS dataset. We have undertaken a journey to extract various audio features, identify optimal scaling strategy and hyperparameter for our models. To increase sample size, we have performed audio data augmentation and addressed data imbalance using SMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of 82 compared to 75%. Following data augmentation, the performance of both algorithms was identical at ~79%, however, overfitting was evident for the SVM. Our final exploration indicated that the performance of both SVM and MLP were similar in which both resulted in lower accuracy for the speech channel compared to the song channel. Our findings suggest that both SVM and MLP are powerful classifiers for emotion recognition in a vocal-dependent manner.

</p>
</details>

<details><summary><b>Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions</b>
<a href="https://arxiv.org/abs/2105.09392">arxiv:2105.09392</a>
&#x1F4C8; 2 <br>
<p>Gengchen Mai, Krzysztof Janowicz, Rui Zhu, Ling Cai, Ni Lao</p></summary>
<p>

**Abstract:** As an important part of Artificial Intelligence (AI), Question Answering (QA) aims at generating answers to questions phrased in natural language. While there has been substantial progress in open-domain question answering, QA systems are still struggling to answer questions which involve geographic entities or concepts and that require spatial operations. In this paper, we discuss the problem of geographic question answering (GeoQA). We first investigate the reasons why geographic questions are difficult to answer by analyzing challenges of geographic questions. We discuss the uniqueness of geographic questions compared to general QA. Then we review existing work on GeoQA and classify them by the types of questions they can address. Based on this survey, we provide a generic classification framework for geographic questions. Finally, we conclude our work by pointing out unique future research directions for GeoQA.

</p>
</details>

<details><summary><b>User Label Leakage from Gradients in Federated Learning</b>
<a href="https://arxiv.org/abs/2105.09369">arxiv:2105.09369</a>
&#x1F4C8; 2 <br>
<p>Aidmar Wainakh, Fabrizio Ventola, Till Müßig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, Max Mühlhäuser</p></summary>
<p>

**Abstract:** Federated learning enables multiple users to build a joint model by sharing their model updates (gradients), while their raw data remains local on their devices. In contrast to the common belief that this provides privacy benefits, we here add to the very recent results on privacy risks when sharing gradients. Specifically, we propose Label Leakage from Gradients (LLG), a novel attack to extract the labels of the users' training data from their shared gradients. The attack exploits the direction and magnitude of gradients to determine the presence or absence of any label. LLG is simple yet effective, capable of leaking potential sensitive information represented by labels, and scales well to arbitrary batch sizes and multiple classes. We empirically and mathematically demonstrate the validity of our attack under different settings. Moreover, empirical results show that LLG successfully extracts labels with high accuracy at the early stages of model training. We also discuss different defense mechanisms against such leakage. Our findings suggest that gradient compression is a practical technique to prevent our attack.

</p>
</details>

<details><summary><b>Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation</b>
<a href="https://arxiv.org/abs/2105.09235">arxiv:2105.09235</a>
&#x1F4C8; 2 <br>
<p>Giovanni Bonetta, Rossella Cancelliere, Ding Liu, Paul Vozila</p></summary>
<p>

**Abstract:** Transformer-based models have demonstrated excellent capabilities of capturing patterns and structures in natural language generation and achieved state-of-the-art results in many tasks. In this paper we present a transformer-based model for multi-turn dialog response generation. Our solution is based on a hybrid approach which augments a transformer-based generative model with a novel retrieval mechanism, which leverages the memorized information in the training data via k-Nearest Neighbor search. Our system is evaluated on two datasets made by customer/assistant dialogs: the Taskmaster-1, released by Google and holding high quality, goal-oriented conversational data and a proprietary dataset collected from a real customer service call center. Both achieve better BLEU scores over strong baselines.

</p>
</details>

<details><summary><b>Attack on practical speaker verification system using universal adversarial perturbations</b>
<a href="https://arxiv.org/abs/2105.09022">arxiv:2105.09022</a>
&#x1F4C8; 2 <br>
<p>Weiyi Zhang, Shuning Zhao, Le Liu, Jianmin Li, Xingliang Cheng, Thomas Fang Zheng, Xiaolin Hu</p></summary>
<p>

**Abstract:** In authentication scenarios, applications of practical speaker verification systems usually require a person to read a dynamic authentication text. Previous studies played an audio adversarial example as a digital signal to perform physical attacks, which would be easily rejected by audio replay detection modules. This work shows that by playing our crafted adversarial perturbation as a separate source when the adversary is speaking, the practical speaker verification system will misjudge the adversary as a target speaker. A two-step algorithm is proposed to optimize the universal adversarial perturbation to be text-independent and has little effect on the authentication text recognition. We also estimated room impulse response (RIR) in the algorithm which allowed the perturbation to be effective after being played over the air. In the physical experiment, we achieved targeted attacks with success rate of 100%, while the word error rate (WER) on speech recognition was only increased by 3.55%. And recorded audios could pass replay detection for the live person speaking.

</p>
</details>

<details><summary><b>Complementary Structure-Learning Neural Networks for Relational Reasoning</b>
<a href="https://arxiv.org/abs/2105.08944">arxiv:2105.08944</a>
&#x1F4C8; 2 <br>
<p>Jacob Russin, Maryam Zolfaghar, Seongmin A. Park, Erie Boorman, Randall C. O'Reilly</p></summary>
<p>

**Abstract:** The neural mechanisms supporting flexible relational inferences, especially in novel situations, are a major focus of current research. In the complementary learning systems framework, pattern separation in the hippocampus allows rapid learning in novel environments, while slower learning in neocortex accumulates small weight changes to extract systematic structure from well-learned environments. In this work, we adapt this framework to a task from a recent fMRI experiment where novel transitive inferences must be made according to implicit relational structure. We show that computational models capturing the basic cognitive properties of these two systems can explain relational transitive inferences in both familiar and novel environments, and reproduce key phenomena observed in the fMRI experiment.

</p>
</details>

<details><summary><b>Investigating Math Word Problems using Pretrained Multilingual Language Models</b>
<a href="https://arxiv.org/abs/2105.08928">arxiv:2105.08928</a>
&#x1F4C8; 2 <br>
<p>Minghuan Tan, Lei Wang, Lingxiao Jiang, Jing Jiang</p></summary>
<p>

**Abstract:** In this paper, we revisit math word problems~(MWPs) from the cross-lingual and multilingual perspective. We construct our MWP solvers over pretrained multilingual language models using sequence-to-sequence model with copy mechanism. We compare how the MWP solvers perform in cross-lingual and multilingual scenarios. To facilitate the comparison of cross-lingual performance, we first adapt the large-scale English dataset MathQA as a counterpart of the Chinese dataset Math23K. Then we extend several English datasets to bilingual datasets through machine translation plus human annotation. Our experiments show that the MWP solvers may not be transferred to a different language even if the target expressions have the same operator set and constants. But for both cross-lingual and multilingual cases, it can be better generalized if problem types exist on both source language and target language.

</p>
</details>

<details><summary><b>Trilevel and Multilevel Optimization using Monotone Operator Theory</b>
<a href="https://arxiv.org/abs/2105.09407">arxiv:2105.09407</a>
&#x1F4C8; 1 <br>
<p>Allahkaram Shafiei, Vyacheslav Kungurtsev, Jakub Marecek</p></summary>
<p>

**Abstract:** We consider rather a general class of multi-level optimization problems, where a convex objective function is to be minimized, subject to constraints to optima of a nested convex optimization problem. As a special case, we consider a trilevel optimization problem, where the objective of the two lower layers consists of a sum of a smooth and a non-smooth term. Based on fixed-point theory and related arguments, we present a natural first-order algorithm and analyze its convergence and rates of convergence in several regimes of parameters.

</p>
</details>

<details><summary><b>From parcel to continental scale -- A first European crop type map based on Sentinel-1 and LUCAS Copernicus in-situ observations</b>
<a href="https://arxiv.org/abs/2105.09261">arxiv:2105.09261</a>
&#x1F4C8; 1 <br>
<p>Raphaël d'Andrimont, Astrid Verhegghen, Guido Lemoine, Pieter Kempeneers, Michele Meroni, Marijn van der Velde</p></summary>
<p>

**Abstract:** Detailed parcel-level crop type mapping for the whole European Union (EU) is necessary for the evaluation of agricultural policies. The Copernicus program, and Sentinel-1 (S1) in particular, offers the opportunity to monitor agricultural land at a continental scale and in a timely manner. However, so far the potential of S1 has not been explored at such a scale. Capitalizing on the unique LUCAS 2018 Copernicus in-situ survey, we present the first continental crop type map at 10-m spatial resolution for the EU based on S1A and S1B Synthetic Aperture Radar observations for the year 2018. Random forest classification algorithms are tuned to detect 19 different crop types. We assess the accuracy of this EU crop map with three approaches. First, the accuracy is assessed with independent LUCAS core in-situ observations over the continent. Second, an accuracy assessment is done specifically for main crop types from farmers declarations from 6 EU member countries or regions totaling >3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are compared to the subnational (NUTS 2) area statistics reported by Eurostat. The overall accuracy for the map is reported as 80.3% when grouping main crop classes and 76% when considering all 19 crop type classes separately. Highest accuracies are obtained for rape and turnip rape with user and produced accuracies higher than 96%. The correlation between the remotely sensed estimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99 (rape and turnip rape). Finally, we discuss how the framework presented here can underpin the operational delivery of in-season high-resolution based crop mapping.

</p>
</details>

<details><summary><b>TarGAN: Target-Aware Generative Adversarial Networks for Multi-modality Medical Image Translation</b>
<a href="https://arxiv.org/abs/2105.08993">arxiv:2105.08993</a>
&#x1F4C8; 1 <br>
<p>Junxiao Chen, Jia Wei, Rui Li</p></summary>
<p>

**Abstract:** Paired multi-modality medical images, can provide complementary information to help physicians make more reasonable decisions than single modality medical images. But they are difficult to generate due to multiple factors in practice (e.g., time, cost, radiation dose). To address these problems, multi-modality medical image translation has aroused increasing research interest recently. However, the existing works mainly focus on translation effect of a whole image instead of a critical target area or Region of Interest (ROI), e.g., organ and so on. This leads to poor-quality translation of the localized target area which becomes blurry, deformed or even with extra unreasonable textures. In this paper, we propose a novel target-aware generative adversarial network called TarGAN, which is a generic multi-modality medical image translation model capable of (1) learning multi-modality medical image translation without relying on paired data, (2) enhancing quality of target area generation with the help of target area labels. The generator of TarGAN jointly learns mapping at two levels simultaneously - whole image translation mapping and target area translation mapping. These two mappings are interrelated through a proposed crossing loss. The experiments on both quantitative measures and qualitative evaluations demonstrate that TarGAN outperforms the state-of-the-art methods in all cases. Subsequent segmentation task is conducted to demonstrate effectiveness of synthetic images generated by TarGAN in a real-world application. Our code is available at https://github.com/2165998/TarGAN.

</p>
</details>

<details><summary><b>VSGM -- Enhance robot task understanding ability through visual semantic graph</b>
<a href="https://arxiv.org/abs/2105.08959">arxiv:2105.08959</a>
&#x1F4C8; 1 <br>
<p>Cheng Yu Tsai, Mu-Chun Su</p></summary>
<p>

**Abstract:** In recent years, developing AI for robotics has raised much attention. The interaction of vision and language of robots is particularly difficult. We consider that giving robots an understanding of visual semantics and language semantics will improve inference ability. In this paper, we propose a novel method-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to obtain better visual image features, improve the robot's visual understanding ability. By providing prior knowledge of the robot and detecting the objects in the image, it predicts the correlation between the attributes of the object and the objects and converts them into a graph-based representation; and mapping the object in the image to be a top-down egocentric map. Finally, the important object features of the current task are extracted by Graph Neural Networks. The method proposed in this paper is verified in the ALFRED (Action Learning From Realistic Environments and Directives) dataset. In this dataset, the robot needs to perform daily indoor household tasks following the required language instructions. After the model is added to the VSGM, the task success rate can be improved by 6~10%.

</p>
</details>

<details><summary><b>Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection</b>
<a href="https://arxiv.org/abs/2105.09464">arxiv:2105.09464</a>
&#x1F4C8; 0 <br>
<p>Yongxiang Gu, Xiaolin Qin, Yuncong Peng, Lu Li</p></summary>
<p>

**Abstract:** As one of the prevalent components, Feature Pyramid Network (FPN) is widely used in the current object detection models to improve the performance of multi-scale detection. However, its interaction is still in a local and lossy manner, thus limiting the representation power. In this paper, to simulate a global view of human vision in object detection and address the inherent defects of interaction mode in FPN, we construct a novel architecture termed Content-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN, which fuses features within a local receptive field, CA-FPN can adaptively aggregate similar features from a global view. It is equipped with a global content extraction module and light linear spatial transformers. The former allows to extract multi-scale context information and the latter can deeply combine the global content extraction module with the vanilla FPN using the linearized attention function, which is designed to reduce model complexity. Furthermore, CA-FPN can be readily plugged into existing FPN-based models. Extensive experiments on the challenging COCO and PASCAL VOC object detection datasets demonstrated that our CA-FPN significantly outperforms competitive FPN-based detectors without bells and whistles. When plugging CA-FPN into Cascade R-CNN framework built upon a standard ResNet-50 backbone, our method can achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous state-of-the-art by 1.5 AP, demonstrating the potentiality of application.

</p>
</details>

<details><summary><b>iTelos -- Purpose Driven Knowledge Graph Generation</b>
<a href="https://arxiv.org/abs/2105.09418">arxiv:2105.09418</a>
&#x1F4C8; 0 <br>
<p>Fausto Giunchiglia, Simone Bocca, Mattia Fumagalli, Mayukh Bagchi, Alessio Zamboni</p></summary>
<p>

**Abstract:** When building a new application we are more and more confronted with the need of reusing and integrating pre-existing knowledge, e.g., ontologies, schemas, data of any kind, from multiple sources. Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is. This difficulty is the cause of high costs, with the further drawback that the resulting application will again be hardly reusable. It is a negative loop which consistently reinforces itself. iTelos is a general purpose methodology aiming at minimizing as much as possible the effects of this loop. iTelos is based on the intuition that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries. This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of an a-priori defined purpose, which is then used to drive a middle-out development process where the application schema and data are continuously aligned.

</p>
</details>

<details><summary><b>TableZa -- A classical Computer Vision approach to Tabular Extraction</b>
<a href="https://arxiv.org/abs/2105.09137">arxiv:2105.09137</a>
&#x1F4C8; 0 <br>
<p>Saumya Banthia, Anantha Sharma, Ravi Mangipudi</p></summary>
<p>

**Abstract:** Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).

</p>
</details>

<details><summary><b>Music Generation using Three-layered LSTM</b>
<a href="https://arxiv.org/abs/2105.09046">arxiv:2105.09046</a>
&#x1F4C8; 0 <br>
<p>Vaishali Ingale, Anush Mohan, Divit Adlakha, Krishan Kumar, Mohit Gupta</p></summary>
<p>

**Abstract:** This paper explores the idea of utilising Long Short-Term Memory neural networks (LSTMNN) for the generation of musical sequences in ABC notation. The proposed approach takes ABC notations from the Nottingham dataset and encodes it to be fed as input for the neural networks. The primary objective is to input the neural networks with an arbitrary note, let the network process and augment a sequence based on the note until a good piece of music is produced. Multiple calibrations have been done to amend the parameters of the network for optimal generation. The output is assessed on the basis of rhythm, harmony, and grammar accuracy.

</p>
</details>


[Next Page]({{ '/2021/05/18/2021.05.18.html' | relative_url }})
