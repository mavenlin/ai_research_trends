## Summary for 2021-04-18, created on 2021-12-22


<details><summary><b>SimCSE: Simple Contrastive Learning of Sentence Embeddings</b>
<a href="https://arxiv.org/abs/2104.08821">arxiv:2104.08821</a>
&#x1F4C8; 136 <br>
<p>Tianyu Gao, Xingcheng Yao, Danqi Chen</p></summary>
<p>

**Abstract:** This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show -- both theoretically and empirically -- that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.

</p>
</details>

<details><summary><b>Off-Policy Risk Assessment in Contextual Bandits</b>
<a href="https://arxiv.org/abs/2104.08977">arxiv:2104.08977</a>
&#x1F4C8; 24 <br>
<p>Audrey Huang, Liu Leqi, Zachary C. Lipton, Kamyar Azizzadenesheli</p></summary>
<p>

**Abstract:** Even when unable to run experiments, practitioners can evaluate prospective policies, using previously logged data. However, while the bandits literature has adopted a diverse set of objectives, most research on off-policy evaluation to date focuses on the expected reward. In this paper, we introduce Lipschitz risk functionals, a broad class of objectives that subsumes conditional value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT risks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework that first estimates a target policy's CDF and then generates plugin estimates for any collection of Lipschitz risks, providing finite sample guarantees that hold simultaneously over the entire class. We instantiate OPRA with both importance sampling and doubly robust estimators. Our primary theoretical contributions are (i) the first uniform concentration inequalities for both CDF estimators in contextual bandits and (ii) error bounds on our Lipschitz risk estimates, which all converge at a rate of $O(1/\sqrt{n})$.

</p>
</details>

<details><summary><b>Many-Speakers Single Channel Speech Separation with Optimal Permutation Training</b>
<a href="https://arxiv.org/abs/2104.08955">arxiv:2104.08955</a>
&#x1F4C8; 21 <br>
<p>Shaked Dovrat, Eliya Nachmani, Lior Wolf</p></summary>
<p>

**Abstract:** Single channel speech separation has experienced great progress in the last few years. However, training neural speech separation for a large number of speakers (e.g., more than 10 speakers) is out of reach for the current methods, which rely on the Permutation Invariant Loss (PIT). In this work, we present a permutation invariant training that employs the Hungarian algorithm in order to train with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in comparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified architecture that can handle the increased number of speakers. Our approach separates up to $20$ speakers and improves the previous results for large $C$ by a wide margin.

</p>
</details>

<details><summary><b>SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts</b>
<a href="https://arxiv.org/abs/2104.08809">arxiv:2104.08809</a>
&#x1F4C8; 21 <br>
<p>Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug Downey, Tom Hope</p></summary>
<p>

**Abstract:** Determining coreference of concept mentions across multiple documents is a fundamental task in natural language understanding. Previous work on cross-document coreference resolution (CDCR) typically considers mentions of events in the news, which seldom involve abstract technical concepts that are prevalent in science and technology. These complex concepts take diverse or ambiguous forms and have many hierarchical levels of granularity (e.g., tasks and subtasks), posing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference clusters and hierarchy between them. We create SciCo, an expert-annotated dataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+ resource. We study strong baseline models that we customize for H-CDCR, and highlight challenges for future work.

</p>
</details>

<details><summary><b>Consistent Accelerated Inference via Confident Adaptive Transformers</b>
<a href="https://arxiv.org/abs/2104.08803">arxiv:2104.08803</a>
&#x1F4C8; 17 <br>
<p>Tal Schuster, Adam Fisch, Tommi Jaakkola, Regina Barzilay</p></summary>
<p>

**Abstract:** We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs -- Confident Adaptive Transformers -- in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.

</p>
</details>

<details><summary><b>Case-based Reasoning for Natural Language Queries over Knowledge Bases</b>
<a href="https://arxiv.org/abs/2104.08762">arxiv:2104.08762</a>
&#x1F4C8; 11 <br>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum</p></summary>
<p>

**Abstract:** It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions -- a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the ComplexWebQuestions dataset, CBR-KBQA outperforms the current state of the art by 11\% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases \emph{without} any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.

</p>
</details>

<details><summary><b>GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation</b>
<a href="https://arxiv.org/abs/2104.08826">arxiv:2104.08826</a>
&#x1F4C8; 9 <br>
<p>Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park</p></summary>
<p>

**Abstract:** Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach.

</p>
</details>

<details><summary><b>Cross-Task Generalization via Natural Language Crowdsourcing Instructions</b>
<a href="https://arxiv.org/abs/2104.08773">arxiv:2104.08773</a>
&#x1F4C8; 9 <br>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi</p></summary>
<p>

**Abstract:** Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. NLP models built with the conventional paradigm, however, often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions and 193k task instances. The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks. These models, however, are far behind supervised task-specific models, indicating significant room for more progress in this direction.

</p>
</details>

<details><summary><b>On Training Sketch Recognizers for New Domains</b>
<a href="https://arxiv.org/abs/2104.08850">arxiv:2104.08850</a>
&#x1F4C8; 8 <br>
<p>Kemal Tugrul Yesilbek, T. Metin Sezgin</p></summary>
<p>

**Abstract:** Sketch recognition algorithms are engineered and evaluated using publicly available datasets contributed by the sketch recognition community over the years. While existing datasets contain sketches of a limited set of generic objects, each new domain inevitably requires collecting new data for training domain specific recognizers. This gives rise to two fundamental concerns: First, will the data collection protocol yield ecologically valid data? Second, will the amount of collected data suffice to train sufficiently accurate classifiers? In this paper, we draw attention to these two concerns. We show that the ecological validity of the data collection protocol and the ability to accommodate small datasets are significant factors impacting recognizer accuracy in realistic scenarios. More specifically, using sketch-based gaming as a use case, we show that deep learning methods, as well as more traditional methods, suffer significantly from dataset shift. Furthermore, we demonstrate that in realistic scenarios where data is scarce and expensive, standard measures taken for adapting deep learners to small datasets fall short of comparing favorably with alternatives. Although transfer learning, and extensive data augmentation help deep learners, they still perform significantly worse compared to standard setups (e.g., SVMs and GBMs with standard feature representations). We pose learning from small datasets as a key problem for the deep sketch recognition field, one which has been ignored in the bulk of the existing literature.

</p>
</details>

<details><summary><b>Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval</b>
<a href="https://arxiv.org/abs/2104.08801">arxiv:2104.08801</a>
&#x1F4C8; 8 <br>
<p>Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy</p></summary>
<p>

**Abstract:** In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA) from source to target domain. While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between the target domain and synthetic data distribution, and reduces model overfitting to the source domain. We run UDA experiments on question generation and passage retrieval from the \textit{Natural Questions} domain to machine learning and biomedical domains. We find that back-training vastly outperforms self-training by a mean improvement of 7.8 BLEU-4 points on generation, and 17.6\% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation dataset- \textit{MLQuestions} containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs.

</p>
</details>

<details><summary><b>Low-Rank Subspaces for Unsupervised Entity Linking</b>
<a href="https://arxiv.org/abs/2104.08737">arxiv:2104.08737</a>
&#x1F4C8; 8 <br>
<p>Akhil Arora, Alberto García-Durán, Robert West</p></summary>
<p>

**Abstract:** Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fact that the entities that are truly mentioned in a document (the "gold entities") tend to form a semantically dense subset of the set of all candidate entities in the document. Geometrically speaking, when representing entities as vectors via some given embedding, the gold entities tend to lie in a low-rank subspace of the full embedding space. Eigenthemes identifies this subspace using the singular value decomposition and scores candidate entities according to their proximity to the subspace. On the empirical front, we introduce multiple strong baselines that compare favorably to (and sometimes even outperform) the existing state of the art. Extensive experiments on benchmark datasets from a variety of real-world domains showcase the effectiveness of our approach.

</p>
</details>

<details><summary><b>The Intrinsic Dimension of Images and Its Impact on Learning</b>
<a href="https://arxiv.org/abs/2104.08894">arxiv:2104.08894</a>
&#x1F4C8; 7 <br>
<p>Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom Goldstein</p></summary>
<p>

**Abstract:** It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found here https://github.com/ppope/dimensions.

</p>
</details>

<details><summary><b>Perspectives on Machine Learning from Psychology's Reproducibility Crisis</b>
<a href="https://arxiv.org/abs/2104.08878">arxiv:2104.08878</a>
&#x1F4C8; 7 <br>
<p>Samuel J. Bell, Onno P. Kampman</p></summary>
<p>

**Abstract:** In the early 2010s, a crisis of reproducibility rocked the field of psychology. Following a period of reflection, the field has responded with radical reform of its scientific practices. More recently, similar questions about the reproducibility of machine learning research have also come to the fore. In this short paper, we present select ideas from psychology's reformation, translating them into relevance for a machine learning audience.

</p>
</details>

<details><summary><b>On the Influence of Masking Policies in Intermediate Pre-training</b>
<a href="https://arxiv.org/abs/2104.08840">arxiv:2104.08840</a>
&#x1F4C8; 7 <br>
<p>Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma, Wen-tau Yih, Xiang Ren, Madian Khabsa</p></summary>
<p>

**Abstract:** Current NLP models are predominantly trained through a two-stage "pre-train then fine-tune" pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it is still unclear (1) in what cases such intermediate pre-training is helpful, (2) whether hand-crafted heuristic objectives are optimal for a given task, and (3) whether a masking policy designed for one task is generalizable beyond that task. In this paper, we perform a large-scale empirical study to investigate the effect of various masking policies in intermediate pre-training with nine selected tasks across three categories. Crucially, we introduce methods to automate the discovery of optimal masking policies via direct supervision or meta-learning. We conclude that the success of intermediate pre-training is dependent on appropriate pre-train corpus, selection of output format (i.e., masked spans or full sentence), and clear understanding of the role that MLM plays for the downstream task. In addition, we find our learned masking policies outperform the heuristic of masking named entities on TriviaQA, and policies learned from one task can positively transfer to other tasks in certain cases, inviting future research in this direction.

</p>
</details>

<details><summary><b>Solving Inefficiency of Self-supervised Representation Learning</b>
<a href="https://arxiv.org/abs/2104.08760">arxiv:2104.08760</a>
&#x1F4C8; 7 <br>
<p>Guangrun Wang, Keze Wang, Guangcong Wang, Philip H. S. Torr, Liang Lin</p></summary>
<p>

**Abstract:** Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model's superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. Codes available at: https://github.com/wanggrun/triplet .

</p>
</details>

<details><summary><b>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</b>
<a href="https://arxiv.org/abs/2104.08758">arxiv:2104.08758</a>
&#x1F4C8; 7 <br>
<p>Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner</p></summary>
<p>

**Abstract:** Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.

</p>
</details>

<details><summary><b>A recipe for annotating grounded clarifications</b>
<a href="https://arxiv.org/abs/2104.08964">arxiv:2104.08964</a>
&#x1F4C8; 6 <br>
<p>Luciana Benotti, Patrick Blackburn</p></summary>
<p>

**Abstract:** In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker's utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.

</p>
</details>

<details><summary><b>Flexible Generation of Natural Language Deductions</b>
<a href="https://arxiv.org/abs/2104.08825">arxiv:2104.08825</a>
&#x1F4C8; 6 <br>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett</p></summary>
<p>

**Abstract:** An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose -- it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe ParaPattern, a method for building models to generate deductive inferences from diverse natural language inputs without direct human supervision. We train BART-based models (Lewis et al., 2020) to generate the result of applying a particular logical operation to one or more premise statements. Crucially, we develop a largely automated pipeline for constructing suitable training examples from Wikipedia. We evaluate our models using out-of-domain sentence compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as targeted perturbation sets. Our results show that our models are substantially more accurate and flexible than baseline systems. ParaPattern achieves 85% validity on examples of the 'substitution' operation from EntailmentBank without the use of any in-domain training data, matching the performance of a model fine-tuned for EntailmentBank. The full source code for our method is publicly available.

</p>
</details>

<details><summary><b>Multilingual Knowledge Graph Completion with Joint Relation and Entity Alignment</b>
<a href="https://arxiv.org/abs/2104.08804">arxiv:2104.08804</a>
&#x1F4C8; 6 <br>
<p>Harkanwar Singh, Prachi Jain,  Mausam, Soumen Chakrabarti</p></summary>
<p>

**Abstract:** Knowledge Graph Completion (KGC) predicts missing facts in an incomplete Knowledge Graph. Almost all of existing KGC research is applicable to only one KG at a time, and in one language only. However, different language speakers may maintain separate KGs in their language and no individual KG is expected to be complete. Moreover, common entities or relations in these KGs have different surface forms and IDs, leading to ID proliferation. Entity alignment (EA) and relation alignment (RA) tasks resolve this by recognizing pairs of entity (relation) IDs in different KGs that represent the same entity (relation). This can further help prediction of missing facts, since knowledge from one KG is likely to benefit completion of another. High confidence predictions may also add valuable information for the alignment tasks. In response, we study the novel task of jointly training multilingual KGC, relation alignment and entity alignment models. We present ALIGNKGC, which uses some seed alignments to jointly optimize all three of KGC, EA and RA losses. A key component of ALIGNKGC is an embedding based soft notion of asymmetric overlap defined on the (subject, object) set signatures of relations this aids in better predicting relations that are equivalent to or implied by other relations. Extensive experiments with DBPedia in five languages establish the benefits of joint training for all tasks, achieving 10-32 MRR improvements of ALIGNKGC over a strong state-of-the-art single-KGC system completion model over each monolingual KG . Further, ALIGNKGC achieves reasonable gains in EA and RA tasks over a vanilla completion model over a KG that combines all facts without alignment, underscoring the value of joint training for these tasks.

</p>
</details>

<details><summary><b>A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble Learning</b>
<a href="https://arxiv.org/abs/2104.08902">arxiv:2104.08902</a>
&#x1F4C8; 5 <br>
<p>Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, Keyan Wang</p></summary>
<p>

**Abstract:** Recently, there has been rapid and significant progress on image dehazing. Many deep learning based methods have shown their superb performance in handling homogeneous dehazing problems. However, we observe that even if a carefully designed convolutional neural network (CNN) can perform well on large-scaled dehazing benchmarks, the network usually fails on the non-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons are mainly in two folds. Firstly, due to its non-homogeneous nature, the non-uniformly distributed haze is harder to be removed than the homogeneous haze. Secondly, the research challenge only provides limited data (there are only 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping from the domain of hazy images to that of clear ones based on very limited data is extremely hard. To this end, we propose a simple but effective approach for non-homogeneous dehazing via ensemble learning. To be specific, we introduce a two-branch neural network to separately deal with the aforementioned problems and then map their distinct features by a learnable fusion tail. We show extensive experimental results to illustrate the effectiveness of our proposed method.

</p>
</details>

<details><summary><b>GooAQ: Open Question Answering with Diverse Answer Types</b>
<a href="https://arxiv.org/abs/2104.08727">arxiv:2104.08727</a>
&#x1F4C8; 5 <br>
<p>Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, Chris Callison-Burch</p></summary>
<p>

**Abstract:** While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types.

</p>
</details>

<details><summary><b>Do We Really Need Gold Samples for Sample Weighting Under Label Noise?</b>
<a href="https://arxiv.org/abs/2104.09045">arxiv:2104.09045</a>
&#x1F4C8; 4 <br>
<p>Aritra Ghosh, Andrew Lan</p></summary>
<p>

**Abstract:** Learning with labels noise has gained significant traction recently due to the sensitivity of deep neural networks under label noise under common loss functions. Losses that are theoretically robust to label noise, however, often makes training difficult. Consequently, several recently proposed methods, such as Meta-Weight-Net (MW-Net), use a small number of unbiased, clean samples to learn a weighting function that downweights samples that are likely to have corrupted labels under the meta-learning framework. However, obtaining such a set of clean samples is not always feasible in practice. In this paper, we analytically show that one can easily train MW-Net without access to clean samples simply by using a loss function that is robust to label noise, such as mean absolute error, as the meta objective to train the weighting network. We experimentally show that our method beats all existing methods that do not use clean samples and performs on-par with methods that use gold samples on benchmark datasets across various noise types and noise rates.

</p>
</details>

<details><summary><b>Reference-based Weak Supervision for Answer Sentence Selection using Web Data</b>
<a href="https://arxiv.org/abs/2104.08943">arxiv:2104.08943</a>
&#x1F4C8; 4 <br>
<p>Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti</p></summary>
<p>

**Abstract:** Answer sentence selection (AS2) modeling requires annotated data, i.e., hand-labeled question-answer pairs. We present a strategy to collect weakly supervised answers for a question based on its reference to improve AS2 modeling. Specifically, we introduce Reference-based Weak Supervision (RWS), a fully automatic large-scale data pipeline that harvests high-quality weakly-supervised answers from abundant Web data requiring only a question-reference pair as input. We study the efficacy and robustness of RWS in the setting of TANDA, a recent state-of-the-art fine-tuning approach specialized for AS2. Our experiments indicate that the produced data consistently bolsters TANDA. We achieve the state of the art in terms of P@1, 90.1%, and MAP, 92.9%, on WikiQA.

</p>
</details>

<details><summary><b>Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings</b>
<a href="https://arxiv.org/abs/2104.08928">arxiv:2104.08928</a>
&#x1F4C8; 4 <br>
<p>Kan Xu, Xuanyi Zhao, Hamsa Bastani, Osbert Bastani</p></summary>
<p>

**Abstract:** Sparse regression has recently been applied to enable transfer learning from very limited data. We study an extension of this approach to unsupervised learning -- in particular, learning word embeddings from unstructured text corpora using low-rank matrix factorization. Intuitively, when transferring word embeddings to a new domain, we expect that the embeddings change for only a small number of words -- e.g., the ones with novel meanings in that domain. We propose a novel group-sparse penalty that exploits this sparsity to perform transfer learning when there is very little text data available in the target domain -- e.g., a single article of text. We prove generalization bounds for our algorithm. Furthermore, we empirically evaluate its effectiveness, both in terms of prediction accuracy in downstream tasks as well as the interpretability of the results.

</p>
</details>

<details><summary><b>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</b>
<a href="https://arxiv.org/abs/2104.08786">arxiv:2104.08786</a>
&#x1F4C8; 4 <br>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp</p></summary>
<p>

**Abstract:** When primed with only a handful of training samples, very large pretrained language models such as GPT-3, have shown competitive results when compared to fully-supervised fine-tuned large pretrained language models. We demonstrate that the order in which the samples are provided can be the difference between near state-of-the-art and random guess performance: Essentially some permutations are "fantastic" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the few-shot setting as it requires additional annotated data. Instead, we use the generative nature of the language models to construct an artificial development set and based on entropy statistics of the candidate permutations from this set we identify performant prompts. Our method improves upon GPT-family models by on average 13% relative across eleven different established text classification tasks.

</p>
</details>

<details><summary><b>Revisiting the Complexity Analysis of Conflict-Based Search: New Computational Techniques and Improved Bounds</b>
<a href="https://arxiv.org/abs/2104.08759">arxiv:2104.08759</a>
&#x1F4C8; 4 <br>
<p>Ofir Gordon, Yuval Filmus, Oren Salzman</p></summary>
<p>

**Abstract:** The problem of Multi-Agent Path Finding (MAPF) calls for finding a set of conflict-free paths for a fleet of agents operating in a given environment. Arguably, the state-of-the-art approach to computing optimal solutions is Conflict-Based Search (CBS). In this work we revisit the complexity analysis of CBS to provide tighter bounds on the algorithm's run-time in the worst-case. Our analysis paves the way to better pinpoint the parameters that govern (in the worst case) the algorithm's computational complexity.
  Our analysis is based on two complementary approaches: In the first approach we bound the run-time using the size of a Multi-valued Decision Diagram (MDD) -- a layered graph which compactly contains all possible single-agent paths between two given vertices for a specific path length.
  In the second approach we express the running time by a novel recurrence relation which bounds the algorithm's complexity. We use generating functions-based analysis in order to tightly bound the recurrence.
  Using these technique we provide several new upper-bounds on CBS's complexity. The results allow us to improve the existing bound on the running time of CBS for many cases. For example, on a set of common benchmarks we improve the upper-bound by a factor of at least $2^{10^{7}}$.

</p>
</details>

<details><summary><b>Can NLI Models Verify QA Systems' Predictions?</b>
<a href="https://arxiv.org/abs/2104.08731">arxiv:2104.08731</a>
&#x1F4C8; 4 <br>
<p>Jifan Chen, Eunsol Choi, Greg Durrett</p></summary>
<p>

**Abstract:** To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just "good enough" in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires the premise (document context) to contain all necessary information to support the hypothesis (proposed answer to the question). We leverage large pre-trained models and recent prior datasets to construct powerful question converter and decontextualization modules, which can reformulate QA instances as premise-hypothesis pairs with very high reliability. Then, by combining standard NLI datasets with NLI examples automatically derived from QA training data, we can train NLI models to judge the correctness of QA models' proposed answers. We show that our NLI approach can generally improve the confidence estimation of a QA model across different domains, evaluated in a selective QA setting. Careful manual analysis over the predictions of our NLI model shows that it can further identify cases where the QA model produces the right answer for the wrong reason, or where the answer cannot be verified as addressing all aspects of the question.

</p>
</details>

<details><summary><b>BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with Assembly Models</b>
<a href="https://arxiv.org/abs/2104.09040">arxiv:2104.09040</a>
&#x1F4C8; 3 <br>
<p>Aadil Islam, Weicheng Ma, Soroush Vosoughi</p></summary>
<p>

**Abstract:** This paper describes a system submitted by team BigGreen to LCP 2021 for predicting the lexical complexity of English words in a given context. We assemble a feature engineering-based model with a deep neural network model founded on BERT. While BERT itself performs competitively, our feature engineering-based model helps in extreme cases, eg. separating instances of easy and neutral difficulty. Our handcrafted features comprise a breadth of lexical, semantic, syntactic, and novel phonological measures. Visualizations of BERT attention maps offer insight into potential features that Transformers models may learn when fine-tuned for lexical complexity prediction. Our ensembled predictions score reasonably well for the single word subtask, and we demonstrate how they can be harnessed to perform well on the multi word expression subtask too.

</p>
</details>

<details><summary><b>Unsupervised Shape Completion via Deep Prior in the Neural Tangent Kernel Perspective</b>
<a href="https://arxiv.org/abs/2104.09023">arxiv:2104.09023</a>
&#x1F4C8; 3 <br>
<p>Lei Chu, Hao Pan, Wenping Wang</p></summary>
<p>

**Abstract:** We present a novel approach for completing and reconstructing 3D shapes from incomplete scanned data by using deep neural networks. Rather than being trained on supervised completion tasks and applied on a testing shape, the network is optimized from scratch on the single testing shape, to fully adapt to the shape and complete the missing data using contextual guidance from the known regions. The ability to complete missing data by an untrained neural network is usually referred to as the deep prior. In this paper, we interpret the deep prior from a neural tangent kernel (NTK) perspective and show that the completed shape patches by the trained CNN are naturally similar to existing patches, as they are proximate in the kernel feature space induced by NTK. The interpretation allows us to design more efficient network structures and learning mechanisms for the shape completion and reconstruction task. Being more aware of structural regularities than both traditional and other unsupervised learning-based reconstruction methods, our approach completes large missing regions with plausible shapes and complements supervised learning-based methods that use database priors by requiring no extra training data set and showing flexible adaptation to a particular shape instance.

</p>
</details>

<details><summary><b>Contrastive Learning Improves Model Robustness Under Label Noise</b>
<a href="https://arxiv.org/abs/2104.08984">arxiv:2104.08984</a>
&#x1F4C8; 3 <br>
<p>Aritra Ghosh, Andrew Lan</p></summary>
<p>

**Abstract:** Deep neural network-based classifiers trained with the categorical cross-entropy (CCE) loss are sensitive to label noise in the training data. One common type of method that can mitigate the impact of label noise can be viewed as supervised robust methods; one can simply replace the CCE loss with a loss that is robust to label noise, or re-weight training samples and down-weight those with higher loss values. Recently, another type of method using semi-supervised learning (SSL) has been proposed, which augments these supervised robust methods to exploit (possibly) noisy samples more effectively. Although supervised robust methods perform well across different data types, they have been shown to be inferior to the SSL methods on image classification tasks under label noise. Therefore, it remains to be seen that whether these supervised robust methods can also perform well if they can utilize the unlabeled samples more effectively. In this paper, we show that by initializing supervised robust methods using representations learned through contrastive learning leads to significantly improved performance under label noise. Surprisingly, even the simplest method (training a classifier with the CCE loss) can outperform the state-of-the-art SSL method by more than 50\% under high label noise when initialized with contrastive learning. Our implementation will be publicly available at {\url{https://github.com/arghosh/noisy_label_pretrain}}.

</p>
</details>

<details><summary><b>On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles</b>
<a href="https://arxiv.org/abs/2104.08962">arxiv:2104.08962</a>
&#x1F4C8; 3 <br>
<p>Rakesh Gosangi, Ravneet Arora, Mohsen Gheisarieha, Debanjan Mahata, Haimin Zhang</p></summary>
<p>

**Abstract:** In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness.

</p>
</details>

<details><summary><b>Attention-based Clinical Note Summarization</b>
<a href="https://arxiv.org/abs/2104.08942">arxiv:2104.08942</a>
&#x1F4C8; 3 <br>
<p>Neel Kanwal, Giuseppe Rizzo</p></summary>
<p>

**Abstract:** The trend of deploying digital systems in numerous industries has induced a hike in recording digital information. The health sector has observed an extensive adoption of digital devices and systems that generate large volumes of personal medical records. Electronic health records contain valuable information for retrospective and prospective analysis that is often not entirely exploited because of the dense information storage. The crude purpose of condensing health records is to select the information that holds most characteristics of the original documents based on reported disease. These summaries may boost diagnosis and extend a doctor's time with the patient during a high workload situation like the COVID-19 pandemic. In this paper, we propose applying a multi-head attention-based mechanism to perform extractive summarization of meaningful phrases in clinical notes. This method finds major sentences for a summary by correlating tokens, segments, and positional embeddings. The model outputs attention scores that are statistically transformed to extract key phrases and can be used to projection on the heat-mapping tool for visual and human use.

</p>
</details>

<details><summary><b>CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP</b>
<a href="https://arxiv.org/abs/2104.08835">arxiv:2104.08835</a>
&#x1F4C8; 3 <br>
<p>Qinyuan Ye, Bill Yuchen Lin, Xiang Ren</p></summary>
<p>

**Abstract:** Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.

</p>
</details>

<details><summary><b>Modeling Ideological Agenda Setting and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity</b>
<a href="https://arxiv.org/abs/2104.08829">arxiv:2104.08829</a>
&#x1F4C8; 3 <br>
<p>Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze</p></summary>
<p>

**Abstract:** The increasing polarization of online political discourse calls for computational tools that are able to automatically detect and monitor ideological divides in social media. Here, we introduce a minimally supervised method that directly leverages the network structure of online discussion forums, specifically Reddit, to detect polarized concepts. We model polarization along the dimensions of agenda setting and framing, drawing upon insights from moral psychology. The architecture we propose combines graph neural networks with structured sparsity and results in representations for concepts and subreddits that capture phenomena such as ideological radicalization and subreddit hijacking. We also create a new dataset of political discourse covering 12 years and more than 600 online groups with different ideologies.

</p>
</details>

<details><summary><b>Schema Curation via Causal Association Rule Mining</b>
<a href="https://arxiv.org/abs/2104.08811">arxiv:2104.08811</a>
&#x1F4C8; 3 <br>
<p>Noah Weber, Anton Belyy, Nils Holzenberger, Rachel Rudinger, Benjamin Van Durme</p></summary>
<p>

**Abstract:** Event schemas are structured knowledge sources defining typical real-world scenarios (e.g., going to an airport). We present a framework for efficient human-in-the-loop construction of a schema library, based on a novel mechanism for schema induction and a well-crafted interface that allows non-experts to "program" complex event structures. Associated with this work we release a machine readable resource (schema library) of 232 detailed event schemas, each of which describe a distinct typical scenario in terms of its relevant sub-event structure (what happens in the scenario), participants (who plays a role in the scenario), fine-grained typing of each participant, and the implied relational constraints between them. Our custom annotation interface, SchemaBlocks, and the event schemas are available online.

</p>
</details>

<details><summary><b>Unsupervised Deep Keyphrase Generation</b>
<a href="https://arxiv.org/abs/2104.08729">arxiv:2104.08729</a>
&#x1F4C8; 3 <br>
<p>Xianjie Shen, Yinghan Wang, Rui Meng, Jingbo Shang</p></summary>
<p>

**Abstract:** Keyphrase generation aims to summarize long documents with a collection of salient phrases. Deep neural models have demonstrated a remarkable success in this task, capable of predicting keyphrases that are even absent from a document. However, such abstractiveness is acquired at the expense of a substantial amount of annotated data. In this paper, we present a novel method for keyphrase generation, AutoKeyGen, without the supervision of any human annotation. Motivated by the observation that an absent keyphrase in one document can appear in other places, in whole or in part, we first construct a phrase bank by pooling all phrases in a corpus. With this phrase bank, we then draw candidate absent keyphrases for each document through a partial matching process. To rank both types of candidates, we combine their lexical- and semantic-level similarities to the input document. Moreover, we utilize these top-ranked candidates as to train a deep generative model for more absent keyphrases. Extensive experiments demonstrate that AutoKeyGen outperforms all unsupervised baselines and can even beat strong supervised methods in certain cases.

</p>
</details>

<details><summary><b>Labels, Information, and Computation: Efficient, Privacy-Preserving Learning Using Sufficient Labels</b>
<a href="https://arxiv.org/abs/2104.09015">arxiv:2104.09015</a>
&#x1F4C8; 2 <br>
<p>Shiyu Duan, Jose C. Principe</p></summary>
<p>

**Abstract:** In supervised learning, obtaining a large set of fully-labeled training data is expensive. We show that we do not always need full label information on every single training example to train a competent classifier. Specifically, inspired by the principle of sufficiency in statistics, we present a statistic (a summary) of the fully-labeled training set that captures almost all the relevant information for classification but at the same time is easier to obtain directly. We call this statistic "sufficiently-labeled data" and prove its sufficiency and efficiency for finding the optimal hidden representations, on which competent classifier heads can be trained using as few as a single randomly-chosen fully-labeled example per class. Sufficiently-labeled data can be obtained from annotators directly without collecting the fully-labeled data first. And we prove that it is easier to directly obtain sufficiently-labeled data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data naturally preserves user privacy by storing relative, instead of absolute, information. Extensive experimental results are provided to support our theory.

</p>
</details>

<details><summary><b>Few-shot Learning for Topic Modeling</b>
<a href="https://arxiv.org/abs/2104.09011">arxiv:2104.09011</a>
&#x1F4C8; 2 <br>
<p>Tomoharu Iwata</p></summary>
<p>

**Abstract:** Topic models have been successfully used for analyzing text documents. However, with existing topic models, many documents are required for training. In this paper, we propose a neural network-based few-shot learning method that can learn a topic model from just a few documents. The neural networks in our model take a small number of documents as inputs, and output topic model priors. The proposed method trains the neural networks such that the expected test likelihood is improved when topic model parameters are estimated by maximizing the posterior probability using the priors based on the EM algorithm. Since each step in the EM algorithm is differentiable, the proposed method can backpropagate the loss through the EM algorithm to train the neural networks. The expected test likelihood is maximized by a stochastic gradient descent method using a set of multiple text corpora with an episodic training framework. In our experiments, we demonstrate that the proposed method achieves better perplexity than existing methods using three real-world text document sets.

</p>
</details>

<details><summary><b>Constraints Satisfiability Driven Reinforcement Learning for Autonomous Cyber Defense</b>
<a href="https://arxiv.org/abs/2104.08994">arxiv:2104.08994</a>
&#x1F4C8; 2 <br>
<p>Ashutosh Dutta, Ehab Al-Shaer, Samrat Chatterjee</p></summary>
<p>

**Abstract:** With the increasing system complexity and attack sophistication, the necessity of autonomous cyber defense becomes vivid for cyber and cyber-physical systems (CPSs). Many existing frameworks in the current state-of-the-art either rely on static models with unrealistic assumptions, or fail to satisfy the system safety and security requirements. In this paper, we present a new hybrid autonomous agent architecture that aims to optimize and verify defense policies of reinforcement learning (RL) by incorporating constraints verification (using satisfiability modulo theory (SMT)) into the agent's decision loop. The incorporation of SMT does not only ensure the satisfiability of safety and security requirements, but also provides constant feedback to steer the RL decision-making toward safe and effective actions. This approach is critically needed for CPSs that exhibit high risk due to safety or security violations. Our evaluation of the presented approach in a simulated CPS environment shows that the agent learns the optimal policy fast and defeats diversified attack strategies in 99\% cases.

</p>
</details>

<details><summary><b>Non-asymptotic model selection in block-diagonal mixture of polynomial experts models</b>
<a href="https://arxiv.org/abs/2104.08959">arxiv:2104.08959</a>
&#x1F4C8; 2 <br>
<p>TrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, Florence Forbes</p></summary>
<p>

**Abstract:** Model selection, via penalized likelihood type criteria, is a standard task in many statistical inference and machine learning problems. Progress has led to deriving criteria with asymptotic consistency results and an increasing emphasis on introducing non-asymptotic criteria. We focus on the problem of modeling non-linear relationships in regression data with potential hidden graph-structured interactions between the high-dimensional predictors, within the mixture of experts modeling framework. In order to deal with such a complex situation, we investigate a block-diagonal localized mixture of polynomial experts (BLoMPE) regression model, which is constructed upon an inverse regression and block-diagonal structures of the Gaussian expert covariance matrices. We introduce a penalized maximum likelihood selection criterion to estimate the unknown conditional density of the regression model. This model selection criterion allows us to handle the challenging problem of inferring the number of mixture components, the degree of polynomial mean functions, and the hidden block-diagonal structures of the covariance matrices, which reduces the number of parameters to be estimated and leads to a trade-off between complexity and sparsity in the model. In particular, we provide a strong theoretical guarantee: a finite-sample oracle inequality satisfied by the penalized maximum likelihood estimator with a Jensen-Kullback-Leibler type loss, to support the introduced non-asymptotic model selection criterion. The penalty shape of this criterion depends on the complexity of the considered random subcollection of BLoMPE models, including the relevant graph structures, the degree of polynomial mean functions, and the number of mixture components.

</p>
</details>

<details><summary><b>Convolutional Neural Networks in Orthodontics: a review</b>
<a href="https://arxiv.org/abs/2104.08886">arxiv:2104.08886</a>
&#x1F4C8; 2 <br>
<p>Szymon Płotka, Tomasz Włodarczyk, Ryszard Szczerba, Przemysław Rokita, Patrycja Bartkowska, Oskar Komisarek, Artur Matthews-Brzozowski, Tomasz Trzciński</p></summary>
<p>

**Abstract:** Convolutional neural networks (CNNs) are used in many areas of computer vision, such as object tracking and recognition, security, military, and biomedical image analysis. This review presents the application of convolutional neural networks in one of the fields of dentistry - orthodontics. Advances in medical imaging technologies and methods allow CNNs to be used in orthodontics to shorten the planning time of orthodontic treatment, including an automatic search of landmarks on cephalometric X-ray images, tooth segmentation on Cone-Beam Computed Tomography (CBCT) images or digital models, and classification of defects on X-Ray panoramic images. In this work, we describe the current methods, the architectures of deep convolutional neural networks used, and their implementations, together with a comparison of the results achieved by them. The promising results and visualizations of the described studies show that the use of methods based on convolutional neural networks allows for the improvement of computer-based orthodontic treatment planning, both by reducing the examination time and, in many cases, by performing the analysis much more accurately than a manual orthodontist does.

</p>
</details>

<details><summary><b>A survey of image labelling for computer vision applications</b>
<a href="https://arxiv.org/abs/2104.08885">arxiv:2104.08885</a>
&#x1F4C8; 2 <br>
<p>Christoph Sager, Christian Janiesch, Patrick Zschech</p></summary>
<p>

**Abstract:** Supervised machine learning methods for image analysis require large amounts of labelled training data to solve computer vision problems. The recent rise of deep learning algorithms for recognising image content has led to the emergence of many ad-hoc labelling tools. With this survey, we capture and systematise the commonalities as well as the distinctions between existing image labelling software. We perform a structured literature review to compile the underlying concepts and features of image labelling software such as annotation expressiveness and degree of automation. We structure the manual labelling task by its organisation of work, user interface design options, and user support techniques to derive a systematisation schema for this survey. Applying it to available software and the body of literature, enabled us to uncover several application archetypes and key domains such as image retrieval or instance identification in healthcare or television.

</p>
</details>

<details><summary><b>Lesion-Inspired Denoising Network: Connecting Medical Image Denoising and Lesion Detection</b>
<a href="https://arxiv.org/abs/2104.08845">arxiv:2104.08845</a>
&#x1F4C8; 2 <br>
<p>Kecheng Chen, Kun Long, Yazhou Ren, Jiayu Sun, Xiaorong Pu</p></summary>
<p>

**Abstract:** Deep learning has achieved notable performance in the denoising task of low-quality medical images and the detection task of lesions, respectively. However, existing low-quality medical image denoising approaches are disconnected from the detection task of lesions. Intuitively, the quality of denoised images will influence the lesion detection accuracy that in turn can be used to affect the denoising performance. To this end, we propose a play-and-plug medical image denoising framework, namely Lesion-Inspired Denoising Network (LIDnet), to collaboratively improve both denoising performance and detection accuracy of denoised medical images. Specifically, we propose to insert the feedback of downstream detection task into existing denoising framework by jointly learning a multi-loss objective. Instead of using perceptual loss calculated on the entire feature map, a novel region-of-interest (ROI) perceptual loss induced by the lesion detection task is proposed to further connect these two tasks. To achieve better optimization for overall framework, we propose a customized collaborative training strategy for LIDnet. On consideration of clinical usability and imaging characteristics, three low-dose CT images datasets are used to evaluate the effectiveness of the proposed LIDnet. Experiments show that, by equipping with LIDnet, both of the denoising and lesion detection performance of baseline methods can be significantly improved.

</p>
</details>

<details><summary><b>Best Practices for Noise-Based Augmentation to Improve the Performance of Emotion Recognition "In the Wild"</b>
<a href="https://arxiv.org/abs/2104.08806">arxiv:2104.08806</a>
&#x1F4C8; 2 <br>
<p>Mimansa Jaiswal, Emily Mower Provost</p></summary>
<p>

**Abstract:** Emotion recognition as a key component of high-stake downstream applications has been shown to be effective, such as classroom engagement or mental health assessments. These systems are generally trained on small datasets collected in single laboratory environments, and hence falter when tested on data that has different noise characteristics. Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, in emotion recognition, noise-based data augmentation may change the underlying label of the original emotional sample. In this work, we generate realistic noisy samples of a well known emotion dataset (IEMOCAP) using multiple categories of environmental and synthetic noise. We evaluate how both human and machine emotion perception changes when noise is introduced. We find that some commonly used augmentation techniques for emotion recognition significantly change human perception, which may lead to unreliable evaluation metrics such as evaluating efficiency of adversarial attack. We also find that the trained state-of-the-art emotion recognition models fail to classify unseen noise-augmented samples, even when trained on noise augmented datasets. This finding demonstrates the brittleness of these systems in real-world conditions. We propose a set of recommendations for noise-based augmentation of emotion datasets and for how to deploy these emotion recognition systems "in the wild".

</p>
</details>

<details><summary><b>Variational Weakly Supervised Sentiment Analysis with Posterior Regularization</b>
<a href="https://arxiv.org/abs/2104.08779">arxiv:2104.08779</a>
&#x1F4C8; 2 <br>
<p>Ziqian Zeng, Yangqiu Song</p></summary>
<p>

**Abstract:** Sentiment analysis is an important task in natural language processing (NLP). Most of existing state-of-the-art methods are under the supervised learning paradigm. However, human annotations can be scarce. Thus, we should leverage more weak supervision for sentiment analysis. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the posterior distribution of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the posterior distributions of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance.

</p>
</details>

<details><summary><b>Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training for Semi-Supervised Text Classification</b>
<a href="https://arxiv.org/abs/2104.08763">arxiv:2104.08763</a>
&#x1F4C8; 2 <br>
<p>Shunsuke Kitada, Hitoshi Iyatomi</p></summary>
<p>

**Abstract:** We propose a new general training technique for attention mechanisms based on virtual adversarial training (VAT). VAT can compute adversarial perturbations from unlabeled data in a semi-supervised setting for the attention mechanisms that have been reported in previous studies to be vulnerable to perturbations. Empirical experiments reveal that our technique (1) provides significantly better prediction performance compared to not only conventional adversarial training-based techniques but also VAT-based techniques in a semi-supervised setting, (2) demonstrates a stronger correlation with the word importance and better agreement with evidence provided by humans, and (3) gains in performance with increasing amounts of unlabeled data.

</p>
</details>

<details><summary><b>DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions of Annotators' Labels</b>
<a href="https://arxiv.org/abs/2104.08755">arxiv:2104.08755</a>
&#x1F4C8; 2 <br>
<p>Zhaohao Zeng, Tetsuya Sakai</p></summary>
<p>

**Abstract:** We introduce a data set called DCH-2, which contains 4,390 real customer-helpdesk dialogues in Chinese and their English translations. DCH-2 also contains dialogue-level annotations and turn-level annotations obtained independently from either 19 or 20 annotators. The data set was built through our effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15 Dialogue Evaluation tasks, to help researchers understand what constitutes an effective customer-helpdesk dialogue, and thereby build efficient and helpful helpdesk systems that are available to customers at all times. In addition, DCH-2 may be utilised for other purposes, for example, as a repository for retrieval-based dialogue systems, or as a parallel corpus for machine translation in the helpdesk domain.

</p>
</details>

<details><summary><b>Mining Latent Structures for Multimedia Recommendation</b>
<a href="https://arxiv.org/abs/2104.09036">arxiv:2104.09036</a>
&#x1F4C8; 1 <br>
<p>Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, Liang Wang</p></summary>
<p>

**Abstract:** Multimedia content is of predominance in the modern Web era. Investigating how users interact with multimodal items is a continuing concern within the rapid development of recommender systems. The majority of previous work focuses on modeling user-item interactions with multimodal features included as side information. However, this scheme is not well-designed for multimedia recommendation. Specifically, only collaborative item-item relationships are implicitly modeled through high-order item-user-item relations. Considering that items are associated with rich contents in multiple modalities, we argue that the latent semantic item-item structures underlying these multimodal contents could be beneficial for learning better item representations and further boosting recommendation. To this end, we propose a LATent sTructure mining method for multImodal reCommEndation, which we term LATTICE for brevity. To be specific, in the proposed LATTICE model, we devise a novel modality-aware structure learning layer, which learns item-item structures for each modality and aggregates multiple modalities to obtain latent item graphs. Based on the learned latent graphs, we perform graph convolutions to explicitly inject high-order item affinities into item representations. These enriched item representations can then be plugged into existing collaborative filtering methods to make more accurate recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over state-of-the-art multimedia recommendation methods and validate the efficacy of mining latent item-item relationships from multimodal features.

</p>
</details>

<details><summary><b>Decentralized Inference with Graph Neural Networks in Wireless Communication Systems</b>
<a href="https://arxiv.org/abs/2104.09027">arxiv:2104.09027</a>
&#x1F4C8; 1 <br>
<p>Mengyuan Lee, Guanding Yu, Huaiyu Dai</p></summary>
<p>

**Abstract:** Graph neural network (GNN) is an efficient neural network model for graph data and is widely used in different fields, including wireless communications. Different from other neural network models, GNN can be implemented in a decentralized manner with information exchanges among neighbors, making it a potentially powerful tool for decentralized control in wireless communication systems. The main bottleneck, however, is wireless channel impairments that deteriorate the prediction robustness of GNN. To overcome this obstacle, we analyze and enhance the robustness of the decentralized GNN in different wireless communication systems in this paper. Specifically, using a GNN binary classifier as an example, we first develop a methodology to verify whether the predictions are robust. Then, we analyze the performance of the decentralized GNN binary classifier in both uncoded and coded wireless communication systems. To remedy imperfect wireless transmission and enhance the prediction robustness, we further propose novel retransmission mechanisms for the above two communication systems, respectively. Through simulations on the synthetic graph data, we validate our analysis, verify the effectiveness of the proposed retransmission mechanisms, and provide some insights for practical implementation.

</p>
</details>

<details><summary><b>Quantum Algorithms for Data Representation and Analysis</b>
<a href="https://arxiv.org/abs/2104.08987">arxiv:2104.08987</a>
&#x1F4C8; 1 <br>
<p>Armando Bellante, Alessandro Luongo, Stefano Zanero</p></summary>
<p>

**Abstract:** We narrow the gap between previous literature on quantum linear algebra and useful data analysis on a quantum computer, providing quantum procedures that speed-up the solution of eigenproblems for data representation in machine learning. The power and practical use of these subroutines is shown through new quantum algorithms, sublinear in the input matrix's size, for principal component analysis, correspondence analysis, and latent semantic analysis. We provide a theoretical analysis of the run-time and prove tight bounds on the randomized algorithms' error. We run experiments on multiple datasets, simulating PCA's dimensionality reduction for image classification with the novel routines. The results show that the run-time parameters that do not depend on the input's size are reasonable and that the error on the computed model is small, allowing for competitive classification performances.

</p>
</details>

<details><summary><b>Functional Protein Structure Annotation Using a Deep Convolutional Generative Adversarial Network</b>
<a href="https://arxiv.org/abs/2104.08969">arxiv:2104.08969</a>
&#x1F4C8; 1 <br>
<p>Ethan Moyer, Jeff Winchell, Isamu Isozaki, Yigit Alparslan, Mali Halac, Edward Kim</p></summary>
<p>

**Abstract:** Identifying novel functional protein structures is at the heart of molecular engineering and molecular biology, requiring an often computationally exhaustive search. We introduce the use of a Deep Convolutional Generative Adversarial Network (DCGAN) to classify protein structures based on their functionality by encoding each sample in a grid object structure using three features in each object: the generic atom type, the position atom type, and its occupancy relative to a given atom. We train DCGAN on 3-dimensional (3D) decoy and native protein structures in order to generate and discriminate 3D protein structures. At the end of our training, loss converges to a local minimum and our DCGAN can annotate functional proteins robustly against adversarial protein samples. In the future we hope to extend the novel structures we found from the generator in our DCGAN with more samples to explore more granular functionality with varying functions. We hope that our effort will advance the field of protein structure prediction.

</p>
</details>

<details><summary><b>A Simulated Experiment to Explore Robotic Dialogue Strategies for People with Dementia</b>
<a href="https://arxiv.org/abs/2104.08940">arxiv:2104.08940</a>
&#x1F4C8; 1 <br>
<p>Fengpei Yuan, Amir Sadovnik, Ran Zhang, Devin Casenhiser, Eun Jin Paek, Si On Yoon, Xiaopeng Zhao</p></summary>
<p>

**Abstract:** People with Alzheimer's disease and related dementias (ADRD) often show the problem of repetitive questioning, which brings a great burden on persons with ADRD (PwDs) and their caregivers. Conversational robots hold promise of coping with this problem and hence alleviating the burdens on caregivers. In this paper, we proposed a partially observable markov decision process (POMDP) model for the PwD-robot interaction in the context of repetitive questioning, and used Q-learning to learn an adaptive conversation strategy (i.e., rate of follow-up question and difficulty of follow-up question) towards PwDs with different cognitive capabilities and different engagement levels. The results indicated that Q-learning was helpful for action selection for the robot. This may be a useful step towards the application of conversational social robots to cope with repetitive questioning in PwDs.

</p>
</details>

<details><summary><b>On the approximation of functions by tanh neural networks</b>
<a href="https://arxiv.org/abs/2104.08938">arxiv:2104.08938</a>
&#x1F4C8; 1 <br>
<p>Tim De Ryck, Samuel Lanthaler, Siddhartha Mishra</p></summary>
<p>

**Abstract:** We derive bounds on the error, in high-order Sobolev norms, incurred in the approximation of Sobolev-regular as well as analytic functions by neural networks with the hyperbolic tangent activation function. These bounds provide explicit estimates on the approximation error with respect to the size of the neural networks. We show that tanh neural networks with only two hidden layers suffice to approximate functions at comparable or better rates than much deeper ReLU neural networks.

</p>
</details>

<details><summary><b>SurvNAM: The machine learning survival model explanation</b>
<a href="https://arxiv.org/abs/2104.08903">arxiv:2104.08903</a>
&#x1F4C8; 1 <br>
<p>Lev V. Utkin, Egor D. Satyukov, Andrei V. Konstantinov</p></summary>
<p>

**Abstract:** A new modification of the Neural Additive Model (NAM) called SurvNAM and its modifications are proposed to explain predictions of the black-box machine learning survival model. The method is based on applying the original NAM to solving the explanation problem in the framework of survival analysis. The basic idea behind SurvNAM is to train the network by means of a specific expected loss function which takes into account peculiarities of the survival model predictions and is based on approximating the black-box model by the extension of the Cox proportional hazards model which uses the well-known Generalized Additive Model (GAM) in place of the simple linear relationship of covariates. The proposed method SurvNAM allows performing the local and global explanation. A set of examples around the explained example is randomly generated for the local explanation. The global explanation uses the whole training dataset. The proposed modifications of SurvNAM are based on using the Lasso-based regularization for functions from GAM and for a special representation of the GAM functions using their weighted linear and non-linear parts, which is implemented as a shortcut connection. A lot of numerical experiments illustrate the SurvNAM efficiency.

</p>
</details>

<details><summary><b>Provably Safe Tolerance Estimation for Robot Arms via Sum-of-Squares Programming</b>
<a href="https://arxiv.org/abs/2104.08896">arxiv:2104.08896</a>
&#x1F4C8; 1 <br>
<p>Weiye Zhao, Suqin He, Changliu Liu</p></summary>
<p>

**Abstract:** Tolerance estimation problems are prevailing in engineering applications. For example, in modern robotics, it remains challenging to efficiently estimate joint tolerance, \ie the maximal allowable deviation from a reference robot state such that safety constraints are still satisfied. This paper presented an efficient algorithm to estimate the joint tolerance using sum-of-squares programming. It is theoretically proved that the algorithm provides a tight lower bound of the joint tolerance. Extensive numerical studies demonstrate that the proposed method is computationally efficient and near optimal. The algorithm is implemented in the JTE toolbox and is available at \url{https://github.com/intelligent-control-lab/Sum-of-Square-Safety-Optimization}.

</p>
</details>

<details><summary><b>End-to-End Interactive Prediction and Planning with Optical Flow Distillation for Autonomous Driving</b>
<a href="https://arxiv.org/abs/2104.08862">arxiv:2104.08862</a>
&#x1F4C8; 1 <br>
<p>Hengli Wang, Peide Cai, Rui Fan, Yuxiang Sun, Ming Liu</p></summary>
<p>

**Abstract:** With the recent advancement of deep learning technology, data-driven approaches for autonomous car prediction and planning have achieved extraordinary performance. Nevertheless, most of these approaches follow a non-interactive prediction and planning paradigm, hypothesizing that a vehicle's behaviors do not affect others. The approaches based on such a non-interactive philosophy typically perform acceptably in sparse traffic scenarios but can easily fail in dense traffic scenarios. Therefore, we propose an end-to-end interactive neural motion planner (INMP) for autonomous driving in this paper. Given a set of past surrounding-view images and a high definition map, our INMP first generates a feature map in bird's-eye-view space, which is then processed to detect other agents and perform interactive prediction and planning jointly. Also, we adopt an optical flow distillation paradigm, which can effectively improve the network performance while still maintaining its real-time inference speed. Extensive experiments on the nuScenes dataset and in the closed-loop Carla simulation environment demonstrate the effectiveness and efficiency of our INMP for the detection, prediction, and planning tasks. Our project page is at sites.google.com/view/inmp-ofd.

</p>
</details>

<details><summary><b>A Rank based Adaptive Mutation in Genetic Algorithm</b>
<a href="https://arxiv.org/abs/2104.08842">arxiv:2104.08842</a>
&#x1F4C8; 1 <br>
<p>Avijit Basak</p></summary>
<p>

**Abstract:** Traditionally Genetic Algorithm has been used for optimization of unimodal and multimodal functions. Earlier researchers worked with constant probabilities of GA control operators like crossover, mutation etc. for tuning the optimization in specific domains. Recent advancements in this field witnessed adaptive approach in probability determination. In Adaptive mutation primarily poor individuals are utilized to explore state space, so mutation probability is usually generated proportionally to the difference between fitness of best chromosome and itself (fMAX - f). However, this approach is susceptible to nature of fitness distribution during optimization. This paper presents an alternate approach of mutation probability generation using chromosome rank to avoid any susceptibility to fitness distribution. Experiments are done to compare results of simple genetic algorithm (SGA) with constant mutation probability and adaptive approaches within a limited resource constraint for unimodal, multimodal functions and Travelling Salesman Problem (TSP). Measurements are done for average best fitness, number of generations evolved and percentage of global optimum achievements out of several trials. The results demonstrate that the rank-based adaptive mutation approach is superior to fitness-based adaptive approach as well as SGA in a multimodal problem space.

</p>
</details>

<details><summary><b>CNN aided Weighted Interpolation for Channel Estimation in Vehicular Communications</b>
<a href="https://arxiv.org/abs/2104.08813">arxiv:2104.08813</a>
&#x1F4C8; 1 <br>
<p>Abdul Karim Gizzini, Marwa Chafii, Ahmad Nimr, Raed M. Shubair, Gerhard Fettweis</p></summary>
<p>

**Abstract:** IEEE 802.11p standard defines wireless technology protocols that enable vehicular transportation and manage traffic efficiency. A major challenge in the development of this technology is ensuring communication reliability in highly dynamic vehicular environments, where the wireless communication channels are doubly selective, thus making channel estimation and tracking a relevant problem to investigate. In this paper, a novel deep learning (DL)-based weighted interpolation estimator is proposed to accurately estimate vehicular channels especially in high mobility scenarios. The proposed estimator is based on modifying the pilot allocation of the IEEE 802.11p standard so that more transmission data rates are achieved. Extensive numerical experiments demonstrate that the developed estimator significantly outperforms the recently proposed DL-based frame-by-frame estimators in different vehicular scenarios, while substantially reducing the overall computational complexity.

</p>
</details>

<details><summary><b>An Uncertainty-aware Hierarchical Probabilistic Network for Early Prediction, Quantification and Segmentation of Pulmonary Tumour Growth</b>
<a href="https://arxiv.org/abs/2104.08789">arxiv:2104.08789</a>
&#x1F4C8; 1 <br>
<p>Xavier Rafael-Palou, Anton Aubanell, Mario Ceresa, Vicent Ribas, Gemma Piella, Miguel A. González Ballester</p></summary>
<p>

**Abstract:** Early detection and quantification of tumour growth would help clinicians to prescribe more accurate treatments and provide better surgical planning. However, the multifactorial and heterogeneous nature of lung tumour progression hampers identification of growth patterns. In this study, we present a novel method based on a deep hierarchical generative and probabilistic framework that, according to radiological guidelines, predicts tumour growth, quantifies its size and provides a semantic appearance of the future nodule. Unlike previous deterministic solutions, the generative characteristic of our approach also allows us to estimate the uncertainty in the predictions, especially important for complex and doubtful cases. Results of evaluating this method on an independent test set reported a tumour growth balanced accuracy of 74%, a tumour growth size MAE of 1.77 mm and a tumour segmentation Dice score of 78%. These surpassed the performances of equivalent deterministic and alternative generative solutions (i.e. probabilistic U-Net, Bayesian test dropout and Pix2Pix GAN) confirming the suitability of our approach.

</p>
</details>

<details><summary><b>TSGN: Transaction Subgraph Networks for Identifying Ethereum Phishing Accounts</b>
<a href="https://arxiv.org/abs/2104.08767">arxiv:2104.08767</a>
&#x1F4C8; 1 <br>
<p>Jinhuan Wang, Pengtao Chen, Shanqing Yu, Qi Xuan</p></summary>
<p>

**Abstract:** Blockchain technology and, in particular, blockchain-based transaction offers us information that has never been seen before in the financial world. In contrast to fiat currencies, transactions through virtual currencies like Bitcoin are completely public. And these transactions of cryptocurrencies are permanently recorded on Blockchain and are available at any time. Therefore, this allows us to build transaction networks (TN) to analyze illegal phenomenons such as phishing scams in blockchain from a network perspective. In this paper, we propose a Transaction SubGraph Network (TSGN) based classification model to identify phishing accounts in Ethereum. Firstly we extract transaction subgraphs for each address and then expand these subgraphs into corresponding TSGNs based on the different mapping mechanisms. We find that TSGNs can provide more potential information to benefit the identification of phishing accounts. Moreover, Directed-TSGNs, by introducing direction attributes, can retain the transaction flow information that captures the significant topological pattern of phishing scams. By comparing with the TSGN, Directed-TSGN indeed has much lower time complexity, benefiting the graph representation learning. Experimental results demonstrate that, combined with network representation algorithms, the TSGN model can capture more features to enhance the classification algorithm and improve phishing nodes' identification accuracy in the Ethereum networks.

</p>
</details>

<details><summary><b>Benchmarking the Benchmark -- Analysis of Synthetic NIDS Datasets</b>
<a href="https://arxiv.org/abs/2104.09029">arxiv:2104.09029</a>
&#x1F4C8; 0 <br>
<p>Siamak Layeghy, Marcus Gallagher, Marius Portmann</p></summary>
<p>

**Abstract:** Network Intrusion Detection Systems (NIDSs) are an increasingly important tool for the prevention and mitigation of cyber attacks. A number of labelled synthetic datasets generated have been generated and made publicly available by researchers, and they have become the benchmarks via which new ML-based NIDS classifiers are being evaluated. Recently published results show excellent classification performance with these datasets, increasingly approaching 100 percent performance across key evaluation metrics such as accuracy, F1 score, etc. Unfortunately, we have not yet seen these excellent academic research results translated into practical NIDS systems with such near-perfect performance. This motivated our research presented in this paper, where we analyse the statistical properties of the benign traffic in three of the more recent and relevant NIDS datasets, (CIC, UNSW, ...). As a comparison, we consider two datasets obtained from real-world production networks, one from a university network and one from a medium size Internet Service Provider (ISP). Our results show that the two real-world datasets are quite similar among themselves in regards to most of the considered statistical features. Equally, the three synthetic datasets are also relatively similar within their group. However, and most importantly, our results show a distinct difference of most of the considered statistical features between the three synthetic datasets and the two real-world datasets. Since ML relies on the basic assumption of training and test datasets being sampled from the same distribution, this raises the question of how well the performance results of ML-classifiers trained on the considered synthetic datasets can translate and generalise to real-world networks. We believe this is an interesting and relevant question which provides motivation for further research in this space.

</p>
</details>

<details><summary><b>FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks</b>
<a href="https://arxiv.org/abs/2104.08815">arxiv:2104.08815</a>
&#x1F4C8; 0 <br>
<p>Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr</p></summary>
<p>

**Abstract:** Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature. Herein, we present the FedNLP, a benchmarking framework for evaluating federated learning methods on four different task formulations: text classification, sequence tagging, question answering, and seq2seq. We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under various non-IID partitioning strategies. Our extensive experiments with FedNLP provide empirical comparisons between FL methods and helps us better understand the inherent challenges of this direction. The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.

</p>
</details>

<details><summary><b>Stochastic Optimization of Areas UnderPrecision-Recall Curves with Provable Convergence</b>
<a href="https://arxiv.org/abs/2104.08736">arxiv:2104.08736</a>
&#x1F4C8; 0 <br>
<p>Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang</p></summary>
<p>

**Abstract:** Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets demonstrate that our proposed method outperforms prior methods on imbalanced problems in terms of AUPRC. To the best of our knowledge, our work represents the first attempt to optimize AUPRC with provable convergence. The SOAP has been implemented in the libAUC library at~\url{https://libauc.org/}.

</p>
</details>


[Next Page]({{ '/2021/04/17/2021.04.17.html' | relative_url }})
