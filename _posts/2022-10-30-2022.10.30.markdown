Prev: [2022.10.29]({{ '/2022/10/29/2022.10.29.html' | relative_url }})  Next: [2022.10.31]({{ '/2022/10/31/2022.10.31.html' | relative_url }})
{% raw %}
## Summary for 2022-10-30, created on 2022-11-09


<details><summary><b>DiffusER: Discrete Diffusion via Edit-based Reconstruction</b>
<a href="https://arxiv.org/abs/2210.16886">arxiv:2210.16886</a>
&#x1F4C8; 298 <br>
<p>Machel Reid, Vincent J. Hellendoorn, Graham Neubig</p></summary>
<p>

**Abstract:** In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.

</p>
</details>

<details><summary><b>Dataset Distillation via Factorization</b>
<a href="https://arxiv.org/abs/2210.16774">arxiv:2210.16774</a>
&#x1F4C8; 54 <br>
<p>Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, Xinchao Wang</p></summary>
<p>

**Abstract:** In this paper, we study \xw{dataset distillation (DD)}, from a novel perspective and introduce a \emph{dataset factorization} approach, termed \emph{HaBa}, which is a plug-and-play strategy portable to any existing DD baseline. Unlike conventional DD approaches that aim to produce distilled and representative samples, \emph{HaBa} explores decomposing a dataset into two components: data \emph{Ha}llucination networks and \emph{Ba}ses, where the latter is fed into the former to reconstruct image samples. The flexible combinations between bases and hallucination networks, therefore, equip the distilled data with exponential informativeness gain, which largely increase the representation capability of distilled datasets. To furthermore increase the data efficiency of compression results, we further introduce a pair of adversarial contrastive constraints on the resultant hallucination networks and bases, which increase the diversity of generated images and inject more discriminant information into the factorization. Extensive comparisons and experiments demonstrate that our method can yield significant improvement on downstream classification tasks compared with previous state of the arts, while reducing the total number of compressed parameters by up to 65\%. Moreover, distilled datasets by our approach also achieve \textasciitilde10\% higher accuracy than baseline methods in cross-architecture generalization. Our code is available \href{https://github.com/Huage001/DatasetFactorization}{here}.

</p>
</details>

<details><summary><b>A Law of Data Separation in Deep Learning</b>
<a href="https://arxiv.org/abs/2210.17020">arxiv:2210.17020</a>
&#x1F4C8; 21 <br>
<p>Hangfeng He, Weijie J. Su</p></summary>
<p>

**Abstract:** Multilayer neural networks have achieved superhuman performance in many artificial intelligence applications. However, their black-box nature obscures the underlying mechanism for transforming input data into labels throughout all layers, thus hindering architecture design for new tasks and interpretation for high-stakes decision makings. We addressed this problem by introducing a precise law that governs how real-world deep neural networks separate data according to their class membership from the bottom layers to the top layers in classification problems. This law shows that each layer roughly improves a certain measure of data separation by an \textit{equal} multiplicative factor. This law manifests in modern architectures such as AlexNet, VGGNet, and ResNet in the late phase of training. This law together with the perspective of data separation offers practical guidelines for designing network architectures, improving model robustness and out-of-sample performance during training, as well as interpreting deep learning predictions.

</p>
</details>

<details><summary><b>DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision</b>
<a href="https://arxiv.org/abs/2210.16906">arxiv:2210.16906</a>
&#x1F4C8; 20 <br>
<p>Mohammad Ali Alomrani, Mahdi Biparva, Yingxue Zhang, Mark Coates</p></summary>
<p>

**Abstract:** The challenge in learning from dynamic graphs for predictive tasks lies in extracting fine-grained temporal motifs from an ever-evolving graph. Moreover, task labels are often scarce, costly to obtain, and highly imbalanced for large dynamic graphs. Recent advances in self-supervised learning on graphs demonstrate great potential, but focus on static graphs. State-of-the-art (SoTA) models for dynamic graphs are not only incompatible with the self-supervised learning (SSL) paradigm but also fail to forecast interactions beyond the very near future. To address these limitations, we present DyG2Vec, an SSL-compatible, efficient model for representation learning on dynamic graphs. DyG2Vec uses a window-based mechanism to generate task-agnostic node embeddings that can be used to forecast future interactions. DyG2Vec significantly outperforms SoTA baselines on benchmark datasets for downstream tasks while only requiring a fraction of the training/inference time. We adapt two SSL evaluation mechanisms to make them applicable to dynamic graphs and thus show that SSL pre-training helps learn more robust temporal node representations, especially for scenarios with few labels.

</p>
</details>

<details><summary><b>Classical ensemble of Quantum-classical ML algorithms for Phishing detection in Ethereum transaction networks</b>
<a href="https://arxiv.org/abs/2211.00004">arxiv:2211.00004</a>
&#x1F4C8; 15 <br>
<p>Anupama Ray, Sai Sakunthala Guddanti, Vishnu Ajith, Dhinakaran Vinayagamurthy</p></summary>
<p>

**Abstract:** Ethereum is one of the most valuable blockchain networks in terms of the total monetary value locked in it, and arguably been the most active network where new blockchain innovations in research and applications are demonstrated. But, this also leads to Ethereum network being susceptible to a wide variety of threats and attacks in an attempt to gain unreasonable advantage or to undermine the value of the users. Even with the state-of-art classical ML algorithms, detecting such attacks is still hard. This motivated us to build a hybrid system of quantum-classical algorithms that improves phishing detection in financial transaction networks. This paper presents a classical ensemble pipeline of classical and quantum algorithms and a detailed study benchmarking existing Quantum Machine Learning algorithms such as Quantum Support Vector Machine and Variational Quantum Classifier. With the current generation of quantum hardware available, smaller datasets are more suited to the QML models and most research restricts to hundreds of samples. However, we experimented on different data sizes and report results with a test data of 12K transaction nodes, which is to the best of the authors knowledge the largest QML experiment run so far on any real quantum hardware. The classical ensembles of quantum-classical models improved the macro F-score and phishing F-score. One key observation is QSVM constantly gives lower false positives, thereby higher precision compared with any other classical or quantum network, which is always preferred for any anomaly detection problem. This is true for QSVMs when used individually or via bagging of same models or in combination with other classical/quantum models making it the most advantageous quantum algorithm so far. The proposed ensemble framework is generic and can be applied for any classification task

</p>
</details>

<details><summary><b>A Solvable Model of Neural Scaling Laws</b>
<a href="https://arxiv.org/abs/2210.16859">arxiv:2210.16859</a>
&#x1F4C8; 15 <br>
<p>Alexander Maloney, Daniel A. Roberts, James Sully</p></summary>
<p>

**Abstract:** Large language models with a huge number of parameters, when trained on near internet-sized number of tokens, have been empirically shown to obey neural scaling laws: specifically, their performance behaves predictably as a power law in either parameters or dataset size until bottlenecked by the other resource. To understand this better, we first identify the necessary properties allowing such scaling laws to arise and then propose a statistical model -- a joint generative data model and random feature model -- that captures this neural scaling phenomenology. By solving this model in the dual limit of large training set size and large number of parameters, we gain insight into (i) the statistical structure of datasets and tasks that lead to scaling laws, (ii) the way nonlinear feature maps, such as those provided by neural networks, enable scaling laws when trained on these datasets, (iii) the optimality of the equiparameterization scaling of training sets and parameters, and (iv) whether such scaling laws can break down and how they behave when they do. Key findings are the manner in which the power laws that occur in the statistics of natural datasets are extended by nonlinear random feature maps and then translated into power-law scalings of the test loss and how the finite extent of the data's spectral power law causes the model's performance to plateau.

</p>
</details>

<details><summary><b>A simple, efficient and scalable contrastive masked autoencoder for learning visual representations</b>
<a href="https://arxiv.org/abs/2210.16870">arxiv:2210.16870</a>
&#x1F4C8; 10 <br>
<p>Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan</p></summary>
<p>

**Abstract:** We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are complementary to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies demonstrate that CAN achieves strong downstream performance under both linear and finetuning evaluations on transfer learning and robustness tasks. CAN outperforms MAE and SimCLR when pre-training on ImageNet, but is especially useful for pre-training on larger uncurated datasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on ImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L models.

</p>
</details>

<details><summary><b>An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks</b>
<a href="https://arxiv.org/abs/2210.16773">arxiv:2210.16773</a>
&#x1F4C8; 10 <br>
<p>Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel</p></summary>
<p>

**Abstract:** Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) -- it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5. Our code and datasets are available at https://github. com/uclnlp/EMAT.

</p>
</details>

<details><summary><b>Evaluation of large-scale synthetic data for Grammar Error Correction</b>
<a href="https://arxiv.org/abs/2210.17035">arxiv:2210.17035</a>
&#x1F4C8; 8 <br>
<p>Vanya Bannihatti Kumar</p></summary>
<p>

**Abstract:** Grammar Error Correction(GEC) mainly relies on the availability of high quality of large amount of synthetic parallel data of grammatically correct and erroneous sentence pairs. The quality of the synthetic data is evaluated on how well the GEC system performs when pre-trained using it. But this does not provide much insight into what are the necessary factors which define the quality of these data. So this work aims to introduce 3 metrics - reliability, diversity and distribution match to provide more insight into the quality of large-scale synthetic data generated for the GEC task, as well as automatically evaluate them. Evaluating these three metrics automatically can also help in providing feedback to the data generation systems and thereby improve the quality of the synthetic data generated dynamically

</p>
</details>

<details><summary><b>A view on model misspecification in uncertainty quantification</b>
<a href="https://arxiv.org/abs/2210.16938">arxiv:2210.16938</a>
&#x1F4C8; 8 <br>
<p>Yuko Kato, David M. J. Tax, Marco Loog</p></summary>
<p>

**Abstract:** Estimating uncertainty of machine learning models is essential to assess the quality of the predictions that these models provide. However, there are several factors that influence the quality of uncertainty estimates, one of which is the amount of model misspecification. Model misspecification always exists as models are mere simplifications or approximations to reality. The question arises whether the estimated uncertainty under model misspecification is reliable or not. In this paper, we argue that model misspecification should receive more attention, by providing thought experiments and contextualizing these with relevant literature.

</p>
</details>

<details><summary><b>Uncertainty-DTW for Time Series and Sequences</b>
<a href="https://arxiv.org/abs/2211.00005">arxiv:2211.00005</a>
&#x1F4C8; 5 <br>
<p>Lei Wang, Piotr Koniusz</p></summary>
<p>

**Abstract:** Dynamic Time Warping (DTW) is used for matching pairs of sequences and celebrated in applications such as forecasting the evolution of time series, clustering time series or even matching sequence pairs in few-shot action recognition. The transportation plan of DTW contains a set of paths; each path matches frames between two sequences under a varying degree of time warping, to account for varying temporal intra-class dynamics of actions. However, as DTW is the smallest distance among all paths, it may be affected by the feature uncertainty which varies across time steps/frames. Thus, in this paper, we propose to model the so-called aleatoric uncertainty of a differentiable (soft) version of DTW. To this end, we model the heteroscedastic aleatoric uncertainty of each path by the product of likelihoods from Normal distributions, each capturing variance of pair of frames. (The path distance is the sum of base distances between features of pairs of frames of the path.) The Maximum Likelihood Estimation (MLE) applied to a path yields two terms: (i) a sum of Euclidean distances weighted by the variance inverse, and (ii) a sum of log-variance regularization terms. Thus, our uncertainty-DTW is the smallest weighted path distance among all paths, and the regularization term (penalty for the high uncertainty) is the aggregate of log-variances along the path. The distance and the regularization term can be used in various objectives. We showcase forecasting the evolution of time series, estimating the Fréchet mean of time series, and supervised/unsupervised few-shot action recognition of the articulated human 3D body joints.

</p>
</details>

<details><summary><b>Decentralized Channel Management in WLANs with Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2210.16949">arxiv:2210.16949</a>
&#x1F4C8; 5 <br>
<p>Zhan Gao, Yulin Shao, Deniz Gunduz, Amanda Prorok</p></summary>
<p>

**Abstract:** Wireless local area networks (WLANs) manage multiple access points (APs) and assign scarce radio frequency resources to APs for satisfying traffic demands of associated user devices. This paper considers the channel allocation problem in WLANs that minimizes the mutual interference among APs, and puts forth a learning-based solution that can be implemented in a decentralized manner. We formulate the channel allocation problem as an unsupervised learning problem, parameterize the control policy of radio channels with graph neural networks (GNNs), and train GNNs with the policy gradient method in a model-free manner. The proposed approach allows for a decentralized implementation due to the distributed nature of GNNs and is equivariant to network permutations. The former provides an efficient and scalable solution for large network scenarios, and the latter renders our algorithm independent of the AP reordering. Empirical results are presented to evaluate the proposed approach and corroborate theoretical findings.

</p>
</details>

<details><summary><b>DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2210.17052">arxiv:2210.17052</a>
&#x1F4C8; 4 <br>
<p>Won-Seok Choi, Dong-Sig Han, Hyundo Lee, Junseok Park, Byoung-Tak Zhang</p></summary>
<p>

**Abstract:** In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.

</p>
</details>

<details><summary><b>Few-Shot Classification of Skin Lesions from Dermoscopic Images by Meta-Learning Representative Embeddings</b>
<a href="https://arxiv.org/abs/2210.16954">arxiv:2210.16954</a>
&#x1F4C8; 4 <br>
<p>Karthik Desingu, Mirunalini P., Aravindan Chandrabose</p></summary>
<p>

**Abstract:** Annotated images and ground truth for the diagnosis of rare and novel diseases are scarce. This is expected to prevail, considering the small number of affected patient population and limited clinical expertise to annotate images. Further, the frequently occurring long-tailed class distributions in skin lesion and other disease classification datasets cause conventional training approaches to lead to poor generalization due to biased class priors. Few-shot learning, and meta-learning in general, aim to overcome these issues by aiming to perform well in low data regimes. This paper focuses on improving meta-learning for the classification of dermoscopic images. Specifically, we propose a baseline supervised method on the meta-training set that allows a network to learn highly representative and generalizable feature embeddings for images, that are readily transferable to new few-shot learning tasks. We follow some of the previous work in literature that posit that a representative feature embedding can be more effective than complex meta-learning algorithms. We empirically prove the efficacy of the proposed meta-training method on dermoscopic images for learning embeddings, and show that even simple linear classifiers trained atop these representations suffice to outperform some of the usual meta-learning methods.

</p>
</details>

<details><summary><b>See as a Bee: UV Sensor for Aerial Strawberry Crop Monitoring</b>
<a href="https://arxiv.org/abs/2210.16923">arxiv:2210.16923</a>
&#x1F4C8; 4 <br>
<p>Megan Heath, Ali Imran, David St-Onge</p></summary>
<p>

**Abstract:** Precision agriculture aims to use technological tools for the agro-food sector to increase productivity, cut labor costs, and reduce the use of resources. This work takes inspiration from bees vision to design a remote sensing system tailored to incorporate UV-reflectance into a flower detector. We demonstrate how this approach can provide feature-rich images for deep learning strawberry flower detection and we apply it to a scalable, yet cost effective aerial monitoring robotic system in the field. We also compare the performance of our UV-G-B image detector with a similar work that utilizes RGB images.

</p>
</details>

<details><summary><b>Modular Hybrid Autoregressive Transducer</b>
<a href="https://arxiv.org/abs/2210.17049">arxiv:2210.17049</a>
&#x1F4C8; 3 <br>
<p>Zhong Meng, Tongzhou Chen, Rohit Prabhavalkar, Yu Zhang, Gary Wang, Kartik Audhkhasi, Jesse Emond, Trevor Strohman, Bhuvana Ramabhadran, W. Ronny Huang, Ehsan Variani, Yinghui Huang, Pedro J. Moreno</p></summary>
<p>

**Abstract:** Text-only adaptation of a transducer model remains challenging for end-to-end speech recognition since the transducer has no clearly separated acoustic model (AM), language model (LM) or blank model. In this work, we propose a modular hybrid autoregressive transducer (MHAT) that has structurally separated label and blank decoders to predict label and blank distributions, respectively, along with a shared acoustic encoder. The encoder and label decoder outputs are directly projected to AM and internal LM scores and then added to compute label posteriors. We train MHAT with an internal LM loss and a HAT loss to ensure that its internal LM becomes a standalone neural LM that can be effectively adapted to text. Moreover, text adaptation of MHAT fosters a much better LM fusion than internal LM subtraction-based methods. On Google's large-scale production data, a multi-domain MHAT adapted with 100B sentences achieves relative WER reductions of up to 12.4% without LM fusion and 21.5% with LM fusion from 400K-hour trained HAT.

</p>
</details>

<details><summary><b>Semantic-Native Communication: A Simplicial Complex Perspective</b>
<a href="https://arxiv.org/abs/2210.16970">arxiv:2210.16970</a>
&#x1F4C8; 3 <br>
<p>Qiyang Zhao, Mehdi Bennis, Merouane Debbah, Daniel Benevides da Costa</p></summary>
<p>

**Abstract:** Semantic communication enables intelligent agents to extract meaning (or semantics) of information via interaction, to carry out collaborative tasks. In this paper, we study semantic communication from a topological space perspective, in which higher-order data semantics live in a simplicial complex. Specifically, a transmitter first maps its data into a $k$-order simplicial complex and then learns its high-order correlations. The simplicial structure and corresponding features are encoded into semantic embeddings in latent space for transmission. Subsequently, the receiver decodes the structure and infers the missing or distorted data. The transmitter and receiver collaboratively train a simplicial convolutional autoencoder to accomplish the semantic communication task. Experiments are carried out on a real dataset of Semantic Scholar Open Research Corpus, where one part of the semantic embedding is missing or distorted during communication. Numerical results show that the simplicial convolutional autoencoder enabled semantic communication effectively rebuilds the simplicial features and infer the missing data with $95\%$ accuracy, while achieving stable performance under channel noise. In contrast, the conventional autoencoder enabled communication fails to infer any missing data. Moreover, our approach is shown to effectively infer the distorted data without prior simplicial structure knowledge at the receiver, by learning extracted semantic information during communications. Leveraging the topological nature of information, the proposed method is also shown to be more reliable and efficient compared to several baselines, notably at low signal-to-noise (SNR) levels.

</p>
</details>

<details><summary><b>ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis</b>
<a href="https://arxiv.org/abs/2210.16943">arxiv:2210.16943</a>
&#x1F4C8; 3 <br>
<p>Xu Cao, Wenqian Ye, Elena Sizikova, Xue Bai, Megan Coffee, Hongwu Zeng, Jianguo Cao</p></summary>
<p>

**Abstract:** Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with very high prevalence around the world. Research progress in the field of ASD facial analysis in pediatric patients has been hindered due to a lack of well-established baselines. In this paper, we propose the use of the Vision Transformer (ViT) for the computational analysis of pediatric ASD. The presented model, known as ViTASD, distills knowledge from large facial expression datasets and offers model structure transferability. Specifically, ViTASD employs a vanilla ViT to extract features from patients' face images and adopts a lightweight decoder with a Gaussian Process layer to enhance the robustness for ASD analysis. Extensive experiments conducted on standard ASD facial analysis benchmarks show that our method outperforms all of the representative approaches in ASD facial analysis, while the ViTASD-L achieves a new state-of-the-art. Our code and pretrained models are available at https://github.com/IrohXu/ViTASD.

</p>
</details>

<details><summary><b>Learning to Compare Nodes in Branch and Bound with Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2210.16934">arxiv:2210.16934</a>
&#x1F4C8; 3 <br>
<p>Abdel Ghani Labassi, Didier Chételat, Andrea Lodi</p></summary>
<p>

**Abstract:** Branch-and-bound approaches in integer programming require ordering portions of the space to explore next, a problem known as node comparison. We propose a new siamese graph neural network model to tackle this problem, where the nodes are represented as bipartite graphs with attributes. Similar to prior work, we train our model to imitate a diving oracle that plunges towards the optimal solution. We evaluate our method by solving the instances in a plain framework where the nodes are explored according to their rank. On three NP-hard benchmarks chosen to be particularly primal-difficult, our approach leads to faster solving and smaller branch- and-bound trees than the default ranking function of the open-source solver SCIP, as well as competing machine learning methods. Moreover, these results generalize to instances larger than used for training. Code for reproducing the experiments can be found at https://github.com/ds4dm/learn2comparenodes.

</p>
</details>

<details><summary><b>On Rate-Distortion Theory in Capacity-Limited Cognition & Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2210.16877">arxiv:2210.16877</a>
&#x1F4C8; 3 <br>
<p>Dilip Arumugam, Mark K. Ho, Noah D. Goodman, Benjamin Van Roy</p></summary>
<p>

**Abstract:** Throughout the cognitive-science literature, there is widespread agreement that decision-making agents operating in the real world do so under limited information-processing capabilities and without access to unbounded cognitive or computational resources. Prior work has drawn inspiration from this fact and leveraged an information-theoretic model of such behaviors or policies as communication channels operating under a bounded rate constraint. Meanwhile, a parallel line of work also capitalizes on the same principles from rate-distortion theory to formalize capacity-limited decision making through the notion of a learning target, which facilitates Bayesian regret bounds for provably-efficient learning algorithms. In this paper, we aim to elucidate this latter perspective by presenting a brief survey of these information-theoretic models of capacity-limited decision making in biological and artificial agents.

</p>
</details>

<details><summary><b>Nonlinear Causal Discovery via Kernel Anchor Regression</b>
<a href="https://arxiv.org/abs/2210.16775">arxiv:2210.16775</a>
&#x1F4C8; 3 <br>
<p>Wenqi Shi, Wenkai Xu</p></summary>
<p>

**Abstract:** Learning causal relationships is a fundamental problem in science. Anchor regression has been developed to address this problem for a large class of causal graphical models, though the relationships between the variables are assumed to be linear. In this work, we tackle the nonlinear setting by proposing kernel anchor regression (KAR). Beyond the natural formulation using a classic two-stage least square estimator, we also study an improved variant that involves nonparametric regression in three separate stages. We provide convergence results for the proposed KAR estimators and the identifiability conditions for KAR to learn the nonlinear structural equation models (SEM). Experimental results demonstrate the superior performances of the proposed KAR estimators over existing baselines.

</p>
</details>

<details><summary><b>Changes from Classical Statistics to Modern Statistics and Data Science</b>
<a href="https://arxiv.org/abs/2211.03756">arxiv:2211.03756</a>
&#x1F4C8; 2 <br>
<p>Kai Zhang, Shan Liu, Momiao Xiong</p></summary>
<p>

**Abstract:** A coordinate system is a foundation for every quantitative science, engineering, and medicine. Classical physics and statistics are based on the Cartesian coordinate system. The classical probability and hypothesis testing theory can only be applied to Euclidean data. However, modern data in the real world are from natural language processing, mathematical formulas, social networks, transportation and sensor networks, computer visions, automations, and biomedical measurements. The Euclidean assumption is not appropriate for non Euclidean data. This perspective addresses the urgent need to overcome those fundamental limitations and encourages extensions of classical probability theory and hypothesis testing , diffusion models and stochastic differential equations from Euclidean space to non Euclidean space. Artificial intelligence such as natural language processing, computer vision, graphical neural networks, manifold regression and inference theory, manifold learning, graph neural networks, compositional diffusion models for automatically compositional generations of concepts and demystifying machine learning systems, has been rapidly developed. Differential manifold theory is the mathematic foundations of deep learning and data science as well. We urgently need to shift the paradigm for data analysis from the classical Euclidean data analysis to both Euclidean and non Euclidean data analysis and develop more and more innovative methods for describing, estimating and inferring non Euclidean geometries of modern real datasets. A general framework for integrated analysis of both Euclidean and non Euclidean data, composite AI, decision intelligence and edge AI provide powerful innovative ideas and strategies for fundamentally advancing AI. We are expected to marry statistics with AI, develop a unified theory of modern statistics and drive next generation of AI and data science.

</p>
</details>

<details><summary><b>Gravitational Dimensionality Reduction Using Newtonian Gravity and Einstein's General Relativity</b>
<a href="https://arxiv.org/abs/2211.01369">arxiv:2211.01369</a>
&#x1F4C8; 2 <br>
<p>Benyamin Ghojogh, Smriti Sharma</p></summary>
<p>

**Abstract:** Due to the effectiveness of using machine learning in physics, it has been widely received increased attention in the literature. However, the notion of applying physics in machine learning has not been given much awareness to. This work is a hybrid of physics and machine learning where concepts of physics are used in machine learning. We propose the supervised Gravitational Dimensionality Reduction (GDR) algorithm where the data points of every class are moved to each other for reduction of intra-class variances and better separation of classes. For every data point, the other points are considered to be gravitational particles, such as stars, where the point is attracted to the points of its class by gravity. The data points are first projected onto a spacetime manifold using principal component analysis. We propose two variants of GDR -- one with the Newtonian gravity and one with the Einstein's general relativity. The former uses Newtonian gravity in a straight line between points but the latter moves data points along the geodesics of spacetime manifold. For GDR with relativity gravitation, we use both Schwarzschild and Minkowski metric tensors to cover both general relativity and special relativity. Our simulations show the effectiveness of GDR in discrimination of classes.

</p>
</details>

<details><summary><b>Transposed Variational Auto-encoder with Intrinsic Feature Learning for Traffic Forecasting</b>
<a href="https://arxiv.org/abs/2211.00641">arxiv:2211.00641</a>
&#x1F4C8; 2 <br>
<p>Leyan Deng, Chenwang Wu, Defu Lian, Min Zhou</p></summary>
<p>

**Abstract:** In this technical report, we present our solutions to the Traffic4cast 2022 core challenge and extended challenge. In this competition, the participants are required to predict the traffic states for the future 15-minute based on the vehicle counter data in the previous hour. Compared to other competitions in the same series, this year focuses on the prediction of different data sources and sparse vertex-to-edge generalization. To address these issues, we introduce the Transposed Variational Auto-encoder (TVAE) model to reconstruct the missing data and Graph Attention Networks (GAT) to strengthen the correlations between learned representations. We further apply feature selection to learn traffic patterns from diverse but easily available data. Our solutions have ranked first in both challenges on the final leaderboard. The source code is available at \url{https://github.com/Daftstone/Traffic4cast}

</p>
</details>

<details><summary><b>MEDS-Net: Self-Distilled Multi-Encoders Network with Bi-Direction Maximum Intensity projections for Lung Nodule Detection</b>
<a href="https://arxiv.org/abs/2211.00003">arxiv:2211.00003</a>
&#x1F4C8; 2 <br>
<p>Muhammad Usman, Azka Rehman, Abdullah Shahid, Siddique Latif, Shi Sub Byon, Byoung Dai Lee, Sung Hyun Kim, Byung il Lee, Yeong Gil Shin</p></summary>
<p>

**Abstract:** In this study, we propose a lung nodule detection scheme which fully incorporates the clinic workflow of radiologists. Particularly, we exploit Bi-Directional Maximum intensity projection (MIP) images of various thicknesses (i.e., 3, 5 and 10mm) along with a 3D patch of CT scan, consisting of 10 adjacent slices to feed into self-distillation-based Multi-Encoders Network (MEDS-Net). The proposed architecture first condenses 3D patch input to three channels by using a dense block which consists of dense units which effectively examine the nodule presence from 2D axial slices. This condensed information, along with the forward and backward MIP images, is fed to three different encoders to learn the most meaningful representation, which is forwarded into the decoded block at various levels. At the decoder block, we employ a self-distillation mechanism by connecting the distillation block, which contains five lung nodule detectors. It helps to expedite the convergence and improves the learning ability of the proposed architecture. Finally, the proposed scheme reduces the false positives by complementing the main detector with auxiliary detectors. The proposed scheme has been rigorously evaluated on 888 scans of LUNA16 dataset and obtained a CPM score of 93.6\%. The results demonstrate that incorporating of bi-direction MIP images enables MEDS-Net to effectively distinguish nodules from surroundings which help to achieve the sensitivity of 91.5% and 92.8% with false positives rate of 0.25 and 0.5 per scan, respectively.

</p>
</details>

<details><summary><b>GotFlow3D: Recurrent Graph Optimal Transport for Learning 3D Flow Motion in Particle Tracking</b>
<a href="https://arxiv.org/abs/2210.17012">arxiv:2210.17012</a>
&#x1F4C8; 2 <br>
<p>Jiaming Liang, Chao Xu, Shengze Cai</p></summary>
<p>

**Abstract:** Flow visualization technologies such as particle tracking velocimetry (PTV) are broadly used in understanding the all-pervasiveness three-dimensional (3D) turbulent flow from nature and industrial processes. Despite the advances in 3D acquisition techniques, the developed motion estimation algorithms in particle tracking remain great challenges of large particle displacements, dense particle distributions and high computational cost. By introducing a novel deep neural network based on recurrent Graph Optimal Transport, called GotFlow3D, we present an end-to-end solution to learn the 3D fluid flow motion from double-frame particle sets. The proposed network constructs two graphs in the geometric and feature space and further enriches the original particle representations with the fused intrinsic and extrinsic features learnt from a graph neural network. The extracted deep features are subsequently utilized to make optimal transport plans indicating the correspondences of particle pairs, which are then iteratively and adaptively retrieved to guide the recurrent flow learning. Experimental evaluations, including assessments on numerical experiments and validations on real-world experiments, demonstrate that the proposed GotFlow3D achieves state-of-the-art performance against both recently-developed scene flow learners and particle tracking algorithms, with impressive accuracy, robustness and generalization ability, which can provide deeper insight into the complex dynamics of broad physical and biological systems.

</p>
</details>

<details><summary><b>STN: a new tensor network method to identify stimulus category from brain activity pattern</b>
<a href="https://arxiv.org/abs/2210.16993">arxiv:2210.16993</a>
&#x1F4C8; 2 <br>
<p>Chunyu Liu, Jiacai Zhang</p></summary>
<p>

**Abstract:** Neural decoding is still a challenge and hot topic in neurocomputing science. Recently, many studies have shown that brain network pattern containing rich spatial and temporal structure information, which represented the activation information of brain under external stimuli. The traditional method is to extract brain network features directly from the common machine learning method, then put these features into the classifier, and realize to decode external stimuli. However, this method cannot effectively extract the multi-dimensional structural information, which is hidden in the brain network. The tensor researchers show that the tensor decomposition model can fully mine unique spatio-temporal structure characteristics in multi-dimensional structure data. This research proposed a stimulus constrain tensor brain model(STN), which involved the tensor decomposition idea and stimulus category constraint information. The model was verified on the real neuroimaging data sets (MEG and fMRI). The experimental results show that the STN model achieved more 11.06% and 18.46% compared with others methods on two modal data sets. These results imply the superiority of extracting discriminative characteristics about STN model, especially for decoding object stimuli with semantic information.

</p>
</details>

<details><summary><b>Foreign Object Debris Detection for Airport Pavement Images based on Self-supervised Localization and Vision Transformer</b>
<a href="https://arxiv.org/abs/2210.16901">arxiv:2210.16901</a>
&#x1F4C8; 2 <br>
<p>Travis Munyer, Daniel Brinkman, Xin Zhong, Chenyu Huang, Iason Konstantzos</p></summary>
<p>

**Abstract:** Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.

</p>
</details>

<details><summary><b>Time-rEversed diffusioN tEnsor Transformer: A new TENET of Few-Shot Object Detection</b>
<a href="https://arxiv.org/abs/2210.16897">arxiv:2210.16897</a>
&#x1F4C8; 2 <br>
<p>Shan Zhang, Naila Murray, Lei Wang, Piotr Koniusz</p></summary>
<p>

**Abstract:** In this paper, we tackle the challenging problem of Few-shot Object Detection. Existing FSOD pipelines (i) use average-pooled representations that result in information loss; and/or (ii) discard position information that can help detect object instances. Consequently, such pipelines are sensitive to large intra-class appearance and geometric variations between support and query images. To address these drawbacks, we propose a Time-rEversed diffusioN tEnsor Transformer (TENET), which i) forms high-order tensor representations that capture multi-way feature occurrences that are highly discriminative, and ii) uses a transformer that dynamically extracts correlations between the query image and the entire support set, instead of a single average-pooled support embedding. We also propose a Transformer Relation Head (TRH), equipped with higher-order representations, which encodes correlations between query regions and the entire support set, while being sensitive to the positional variability of object instances. Our model achieves state-of-the-art results on PASCAL VOC, FSOD, and COCO.

</p>
</details>

<details><summary><b>Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues</b>
<a href="https://arxiv.org/abs/2210.16838">arxiv:2210.16838</a>
&#x1F4C8; 2 <br>
<p>Jiao Ou, Jinchao Zhang, Yang Feng, Jie Zhou</p></summary>
<p>

**Abstract:** The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is labor-intensive and time-consuming. In this paper, we propose a data augmentation method to automatically augment high-quality responses with different semantics by counterfactual inference. Specifically, given an observed dialogue, our counterfactual generation model first infers semantically different responses by replacing the observed reply perspective with substituted ones. Furthermore, our data selection method filters out detrimental augmented responses. Experimental results show that our data augmentation method can augment high-quality responses with different semantics for a given dialogue history, and can outperform competitive baselines on multiple downstream tasks.

</p>
</details>

<details><summary><b>Robust Data Valuation via Variance Reduced Data Shapley</b>
<a href="https://arxiv.org/abs/2210.16835">arxiv:2210.16835</a>
&#x1F4C8; 2 <br>
<p>Mengmeng Wu, Ruoxi Jia, Changle Lin, Wei Huang, Xiangyu Chang</p></summary>
<p>

**Abstract:** Data valuation, especially quantifying data value in algorithmic prediction and decision-making, is a fundamental problem in data trading scenarios. The most widely used method is to define the data Shapley and approximate it by means of the permutation sampling algorithm. To make up for the large estimation variance of the permutation sampling that hinders the development of the data marketplace, we propose a more robust data valuation method using stratified sampling, named variance reduced data Shapley (VRDS for short). We theoretically show how to stratify, how many samples are taken at each stratum, and the sample complexity analysis of VRDS. Finally, the effectiveness of VRDS is illustrated in different types of datasets and data removal applications.

</p>
</details>

<details><summary><b>Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition</b>
<a href="https://arxiv.org/abs/2210.16820">arxiv:2210.16820</a>
&#x1F4C8; 2 <br>
<p>Lei Wang, Piotr Koniusz</p></summary>
<p>

**Abstract:** We propose a Few-shot Learning pipeline for 3D skeleton-based action recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). To factor out misalignment between query and support sequences of 3D body joints, we propose an advanced variant of Dynamic Time Warping which jointly models each smooth path between the query and support frames to achieve simultaneously the best alignment in the temporal and simulated camera viewpoint spaces for end-to-end learning under the limited few-shot training data. Sequences are encoded with a temporal block encoder based on Simple Spectral Graph Convolution, a lightweight linear Graph Neural Network backbone. We also include a setting with a transformer. Finally, we propose a similarity-based loss which encourages the alignment of sequences of the same class while preventing the alignment of unrelated sequences. We show state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II.

</p>
</details>

<details><summary><b>Symmetric Saliency-based Adversarial Attack To Speaker Identification</b>
<a href="https://arxiv.org/abs/2210.16777">arxiv:2210.16777</a>
&#x1F4C8; 2 <br>
<p>Jiadi Yao, Xing Chen, Xiao-Lei Zhang, Wei-Qiang Zhang, Kunde Yang</p></summary>
<p>

**Abstract:** Adversarial attack approaches to speaker identification either need high computational cost or are not very effective, to our knowledge. To address this issue, in this paper, we propose a novel generation-network-based approach, called symmetric saliency-based encoder-decoder (SSED), to generate adversarial voice examples to speaker identification. It contains two novel components. First, it uses a novel saliency map decoder to learn the importance of speech samples to the decision of a targeted speaker identification system, so as to make the attacker focus on generating artificial noise to the important samples. It also proposes an angular loss function to push the speaker embedding far away from the source speaker. Our experimental results demonstrate that the proposed SSED yields the state-of-the-art performance, i.e. over 97% targeted attack success rate and a signal-to-noise level of over 39 dB on both the open-set and close-set speaker identification tasks, with a low computational cost.

</p>
</details>

<details><summary><b>Advertising strategy for profit-maximization: a novel practice on Tmall's online ads manager platforms</b>
<a href="https://arxiv.org/abs/2211.01160">arxiv:2211.01160</a>
&#x1F4C8; 1 <br>
<p>Lianghai Xiao, Yixing Zhao, Jiwei Chen</p></summary>
<p>

**Abstract:** Ads manager platform gains popularity among numerous e-commercial vendors/advertisers. It helps advertisers to facilitate the process of displaying their ads to target customers. One of the main challenges faced by advertisers, especially small and medium-sized enterprises, is to configure their advertising strategy properly. An ineffective advertising strategy will bring too many ``just looking'' clicks and, eventually, generate high advertising expenditure unproportionally to the growth of sales. In this paper, we present a novel profit-maximization model for online advertising optimization. The optimization problem is constructed to find optimal set of features to maximize the probability that target customers buy advertising products. We further reformulate the optimization problem to a knapsack problem with changeable parameters, and introduce a self-adjusted algorithm for finding the solution to the problem. Numerical experiment based on statistical data from Tmall show that our proposed method can optimize the advertising strategy given expenditure budget effectively.

</p>
</details>

<details><summary><b>FrozenQubits: Boosting Fidelity of QAOA by Skipping Hotspot Nodes</b>
<a href="https://arxiv.org/abs/2210.17037">arxiv:2210.17037</a>
&#x1F4C8; 1 <br>
<p>Ramin Ayanzadeh, Narges Alavisamani, Poulami Das, Moinuddin Qureshi</p></summary>
<p>

**Abstract:** Quantum Approximate Optimization Algorithm (QAOA) is one of the leading candidates for demonstrating the quantum advantage using near-term quantum computers. Unfortunately, high device error rates limit us from reliably running QAOA circuits for problems with more than a few qubits. In QAOA, the problem graph is translated into a quantum circuit such that every edge corresponds to two 2-qubit CNOT operations in each layer of the circuit. As CNOTs are extremely error-prone, the fidelity of QAOA circuits is dictated by the number of edges in the problem graph.
  We observe that majority of graphs corresponding to real-world applications follow the ``power-law`` distribution, where some hotspot nodes have significantly higher number of connections. We leverage this insight and propose ``FrozenQubits`` that freezes the hotspot nodes or qubits and intelligently partitions the state-space of the given problem into several smaller sub-spaces which are then solved independently. The corresponding QAOA sub-circuits are significantly less vulnerable to gate and decoherence errors due to the reduced number of CNOT operations in each sub-circuit. Unlike prior circuit-cutting approaches, FrozenQubits does not require any exponentially complex post-processing step. Our evaluations with 5,300 QAOA circuits on eight different quantum computers from IBM shows that FrozenQubits can improve the quality of solutions by 8.73x on average (and by up to 57x), albeit utilizing 2x more quantum resources.

</p>
</details>

<details><summary><b>Uncertainty Aware Trader-Company Method: Interpretable Stock Price Prediction Capturing Uncertainty</b>
<a href="https://arxiv.org/abs/2210.17030">arxiv:2210.17030</a>
&#x1F4C8; 1 <br>
<p>Yugo Fujimoto, Kei Nakagawa, Kentaro Imajo, Kentaro Minami</p></summary>
<p>

**Abstract:** Machine learning is an increasingly popular tool with some success in predicting stock prices. One promising method is the Trader-Company~(TC) method, which takes into account the dynamism of the stock market and has both high predictive power and interpretability. Machine learning-based stock prediction methods including the TC method have been concentrating on point prediction. However, point prediction in the absence of uncertainty estimates lacks credibility quantification and raises concerns about safety. The challenge in this paper is to make an investment strategy that combines high predictive power and the ability to quantify uncertainty. We propose a novel approach called Uncertainty Aware Trader-Company Method~(UTC) method. The core idea of this approach is to combine the strengths of both frameworks by merging the TC method with the probabilistic modeling, which provides probabilistic predictions and uncertainty estimations. We expect this to retain the predictive power and interpretability of the TC method while capturing the uncertainty. We theoretically prove that the proposed method estimates the posterior variance and does not introduce additional biases from the original TC method. We conduct a comprehensive evaluation of our approach based on the synthetic and real market datasets. We confirm with synthetic data that the UTC method can detect situations where the uncertainty increases and the prediction is difficult. We also confirmed that the UTC method can detect abrupt changes in data generating distributions. We demonstrate with real market data that the UTC method can achieve higher returns and lower risks than baselines.

</p>
</details>

<details><summary><b>Emotional Brain State Classification on fMRI Data Using Deep Residual and Convolutional Networks</b>
<a href="https://arxiv.org/abs/2210.17015">arxiv:2210.17015</a>
&#x1F4C8; 1 <br>
<p>Maxime Tchibozo, Donggeun Kim, Zijing Wang, Xiaofu He</p></summary>
<p>

**Abstract:** The goal of emotional brain state classification on functional MRI (fMRI) data is to recognize brain activity patterns related to specific emotion tasks performed by subjects during an experiment. Distinguishing emotional brain states from other brain states using fMRI data has proven to be challenging due to two factors: a difficulty to generate fast yet accurate predictions in short time frames, and a difficulty to extract emotion features which generalize to unseen subjects. To address these challenges, we conducted an experiment in which 22 subjects viewed pictures designed to stimulate either negative, neutral or rest emotional responses while their brain activity was measured using fMRI. We then developed two distinct Convolution-based approaches to decode emotional brain states using only spatial information from single, minimally pre-processed (slice timing and realignment) fMRI volumes. In our first approach, we trained a 1D Convolutional Network (84.9% accuracy; chance level 33%) to classify 3 emotion conditions using One-way Analysis of Variance (ANOVA) voxel selection combined with hyperalignment. In our second approach, we trained a 3D ResNet-50 model (78.0% accuracy; chance level 50%) to classify 2 emotion conditions from single 3D fMRI volumes directly. Our Convolutional and Residual classifiers successfully learned group-level emotion features and could decode emotion conditions from fMRI volumes in milliseconds. These approaches could potentially be used in brain computer interfaces and real-time fMRI neurofeedback research.

</p>
</details>

<details><summary><b>Embedding Space Augmentation for Weakly Supervised Learning in Whole-Slide Images</b>
<a href="https://arxiv.org/abs/2210.17013">arxiv:2210.17013</a>
&#x1F4C8; 1 <br>
<p>Imaad Zaffar, Guillaume Jaume, Nasir Rajpoot, Faisal Mahmood</p></summary>
<p>

**Abstract:** Multiple Instance Learning (MIL) is a widely employed framework for learning on gigapixel whole-slide images (WSIs) from WSI-level annotations. In most MIL based analytical pipelines for WSI-level analysis, the WSIs are often divided into patches and deep features for patches (i.e., patch embeddings) are extracted prior to training to reduce the overall computational cost and cope with the GPUs' limited RAM. To overcome this limitation, we present EmbAugmenter, a data augmentation generative adversarial network (DA-GAN) that can synthesize data augmentations in the embedding space rather than in the pixel space, thereby significantly reducing the computational requirements. Experiments on the SICAPv2 dataset show that our approach outperforms MIL without augmentation and is on par with traditional patch-level augmentation for MIL training while being substantially faster.

</p>
</details>

<details><summary><b>Representation Learning for General-sum Low-rank Markov Games</b>
<a href="https://arxiv.org/abs/2210.16976">arxiv:2210.16976</a>
&#x1F4C8; 1 <br>
<p>Chengzhuo Ni, Yuda Song, Xuezhou Zhang, Chi Jin, Mengdi Wang</p></summary>
<p>

**Abstract:** We study multi-agent general-sum Markov games with nonlinear function approximation. We focus on low-rank Markov games whose transition matrix admits a hidden low-rank structure on top of an unknown non-linear representation. The goal is to design an algorithm that (1) finds an $\varepsilon$-equilibrium policy sample efficiently without prior knowledge of the environment or the representation, and (2) permits a deep-learning friendly implementation. We leverage representation learning and present a model-based and a model-free approach to construct an effective representation from the collected data. For both approaches, the algorithm achieves a sample complexity of poly$(H,d,A,1/\varepsilon)$, where $H$ is the game horizon, $d$ is the dimension of the feature vector, $A$ is the size of the joint action space and $\varepsilon$ is the optimality gap. When the number of players is large, the above sample complexity can scale exponentially with the number of players in the worst case. To address this challenge, we consider Markov games with a factorized transition structure and present an algorithm that escapes such exponential scaling. To our best knowledge, this is the first sample-efficient algorithm for multi-agent general-sum Markov games that incorporates (non-linear) function approximation. We accompany our theoretical result with a neural network-based implementation of our algorithm and evaluate it against the widely used deep RL baseline, DQN with fictitious play.

</p>
</details>

<details><summary><b>Reward Shaping Using Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2210.16956">arxiv:2210.16956</a>
&#x1F4C8; 1 <br>
<p>Hani Sami, Hadi Otrok, Jamal Bentahar, Azzam Mourad, Ernesto Damiani</p></summary>
<p>

**Abstract:** In this paper, we propose Value Iteration Network for Reward Shaping (VIN-RS), a potential-based reward shaping mechanism using Convolutional Neural Network (CNN). The proposed VIN-RS embeds a CNN trained on computed labels using the message passing mechanism of the Hidden Markov Model. The CNN processes images or graphs of the environment to predict the shaping values. Recent work on reward shaping still has limitations towards training on a representation of the Markov Decision Process (MDP) and building an estimate of the transition matrix. The advantage of VIN-RS is to construct an effective potential function from an estimated MDP while automatically inferring the environment transition matrix. The proposed VIN-RS estimates the transition matrix through a self-learned convolution filter while extracting environment details from the input frames or sampled graphs. Due to (1) the previous success of using message passing for reward shaping; and (2) the CNN planning behavior, we use these messages to train the CNN of VIN-RS. Experiments are performed on tabular games, Atari 2600 and MuJoCo, for discrete and continuous action space. Our results illustrate promising improvements in the learning speed and maximum cumulative reward compared to the state-of-the-art.

</p>
</details>

<details><summary><b>Improving Bilingual Lexicon Induction with Cross-Encoder Reranking</b>
<a href="https://arxiv.org/abs/2210.16953">arxiv:2210.16953</a>
&#x1F4C8; 1 <br>
<p>Yaoyiran Li, Fangyu Liu, Ivan Vulić, Anna Korhonen</p></summary>
<p>

**Abstract:** Bilingual lexicon induction (BLI) with limited bilingual supervision is a crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI methods rely on the induction of cross-lingual word embeddings (CLWEs) to capture cross-lingual word similarities; such CLWEs are obtained 1) via traditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs from multilingual pretrained language models (mPLMs), or 3) through combining the former two options. In this work, we propose a novel semi-supervised post-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking), applicable to any precalculated CLWE space, which improves their BLI capability. The key idea is to 'extract' cross-lingual lexical knowledge from mPLMs, and then combine it with the original CLWEs. This crucial step is done via 1) creating a word similarity dataset, comprising positive word pairs (i.e., true translations) and hard negative pairs induced from the original CLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a cross-encoder manner to predict the similarity scores. At inference, we 3) combine the similarity score from the original CLWE space with the score from the BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on two standard BLI benchmarks spanning a wide spectrum of diverse languages: it substantially outperforms a series of strong baselines across the board. We also validate the robustness of BLICEr with different CLWEs.

</p>
</details>

<details><summary><b>Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation</b>
<a href="https://arxiv.org/abs/2210.16898">arxiv:2210.16898</a>
&#x1F4C8; 1 <br>
<p>Ehsan Khodapanah Aghdam, Reza Azad, Maral Zarvani, Dorit Merhof</p></summary>
<p>

**Abstract:** Melanoma is caused by the abnormal growth of melanocytes in human skin. Like other cancers, this life-threatening skin cancer can be treated with early diagnosis. To support a diagnosis by automatic skin lesion segmentation, several Fully Convolutional Network (FCN) approaches, specifically the U-Net architecture, have been proposed. The U-Net model with a symmetrical architecture has exhibited superior performance in the segmentation task. However, the locality restriction of the convolutional operation incorporated in the U-Net architecture limits its performance in capturing long-range dependency, which is crucial for the segmentation task in medical images. To address this limitation, recently a Transformer based U-Net architecture that replaces the CNN blocks with the Swin Transformer module has been proposed to capture both local and global representation. In this paper, we propose Att-SwinU-Net, an attention-based Swin U-Net extension, for medical image segmentation. In our design, we seek to enhance the feature re-usability of the network by carefully designing the skip connection path. We argue that the classical concatenation operation utilized in the skip connection path can be further improved by incorporating an attention mechanism. By performing a comprehensive ablation study on several skin lesion segmentation datasets, we demonstrate the effectiveness of our proposed attention mechanism.

</p>
</details>

<details><summary><b>Planning to the Information Horizon of BAMDPs via Epistemic State Abstraction</b>
<a href="https://arxiv.org/abs/2210.16872">arxiv:2210.16872</a>
&#x1F4C8; 1 <br>
<p>Dilip Arumugam, Satinder Singh</p></summary>
<p>

**Abstract:** The Bayes-Adaptive Markov Decision Process (BAMDP) formalism pursues the Bayes-optimal solution to the exploration-exploitation trade-off in reinforcement learning. As the computation of exact solutions to Bayesian reinforcement-learning problems is intractable, much of the literature has focused on developing suitable approximation algorithms. In this work, before diving into algorithm design, we first define, under mild structural assumptions, a complexity measure for BAMDP planning. As efficient exploration in BAMDPs hinges upon the judicious acquisition of information, our complexity measure highlights the worst-case difficulty of gathering information and exhausting epistemic uncertainty. To illustrate its significance, we establish a computationally-intractable, exact planning algorithm that takes advantage of this measure to show more efficient planning. We then conclude by introducing a specific form of state abstraction with the potential to reduce BAMDP complexity and gives rise to a computationally-tractable, approximate planning algorithm.

</p>
</details>

<details><summary><b>Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings</b>
<a href="https://arxiv.org/abs/2210.16848">arxiv:2210.16848</a>
&#x1F4C8; 1 <br>
<p>Jiangbin Zheng, Yile Wang, Ge Wang, Jun Xia, Yufei Huang, Guojiang Zhao, Yue Zhang, Stan Z. Li</p></summary>
<p>

**Abstract:** Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.

</p>
</details>

<details><summary><b>Adaptive Speech Quality Aware Complex Neural Network for Acoustic Echo Cancellation with Supervised Contrastive Learning</b>
<a href="https://arxiv.org/abs/2210.16791">arxiv:2210.16791</a>
&#x1F4C8; 1 <br>
<p>Bozhong Liu, Xiaoxi Yu, Hantao Huang</p></summary>
<p>

**Abstract:** Acoustic echo cancellation (AEC) is designed to remove echoes, reverberation, and unwanted added sounds from the microphone signal while maintaining the quality of the near-end speaker's speech. This paper proposes adaptive speech quality complex neural networks to focus on specific tasks for real-time acoustic echo cancellation. In specific, we propose a complex modularize neural network with different stages to focus on feature extraction, acoustic separation, and mask optimization receptively. Furthermore, we adopt the contrastive learning framework and novel speech quality aware loss functions to further improve the performance. The model is trained with 72 hours for pre-training and then 72 hours for fine-tuning. The proposed model outperforms the state-of-the-art performance.

</p>
</details>

<details><summary><b>Parameter-Efficient Tuning Makes a Good Classification Head</b>
<a href="https://arxiv.org/abs/2210.16771">arxiv:2210.16771</a>
&#x1F4C8; 1 <br>
<p>Zhuoyi Yang, Ming Ding, Yanhui Guo, Qingsong Lv, Jie Tang</p></summary>
<p>

**Abstract:** In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.

</p>
</details>

<details><summary><b>Block-Wise Dynamic-Precision Neural Network Training Acceleration via Online Quantization Sensitivity Analytics</b>
<a href="https://arxiv.org/abs/2210.17047">arxiv:2210.17047</a>
&#x1F4C8; 0 <br>
<p>Ruoyang Liu, Chenhan Wei, Yixiong Yang, Wenxun Wang, Huazhong Yang, Yongpan Liu</p></summary>
<p>

**Abstract:** Data quantization is an effective method to accelerate neural network training and reduce power consumption. However, it is challenging to perform low-bit quantized training: the conventional equal-precision quantization will lead to either high accuracy loss or limited bit-width reduction, while existing mixed-precision methods offer high compression potential but failed to perform accurate and efficient bit-width assignment. In this work, we propose DYNASTY, a block-wise dynamic-precision neural network training framework. DYNASTY provides accurate data sensitivity information through fast online analytics, and maintains stable training convergence with an adaptive bit-width map generator. Network training experiments on CIFAR-100 and ImageNet dataset are carried out, and compared to 8-bit quantization baseline, DYNASTY brings up to $5.1\times$ speedup and $4.7\times$ energy consumption reduction with no accuracy drop and negligible hardware overhead.

</p>
</details>

<details><summary><b>CodeEditor: Learning to Edit Source Code with Pre-trained Models</b>
<a href="https://arxiv.org/abs/2210.17040">arxiv:2210.17040</a>
&#x1F4C8; 0 <br>
<p>Jia Li, Ge Li, Zhuo Li, Zhi Jin, Xing Hu, Kechi Zhang, Zhiyi Fu</p></summary>
<p>

**Abstract:** Developers often perform repetitive code editing activities for various reasons (e.g., code refactor) during software development. Many deep learning models are applied to automate code editing by learning from the code editing history. Recently, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for code editing.
  In this paper, we propose a pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect real-world code snippets as the ground truth and use a generator to rewrite them into natural but inferior versions. Then, we pre-train our CodeEditor to edit inferior versions into the ground truth, to learn edit patterns. We conduct experiments on four datasets and evaluate models in three settings. (1) In the fine-tuning setting, we fine-tune the pre-trained CodeEditor with four datasets. CodeEditor outperforms SOTA baselines by 15%, 25.5%, and 9.4% and 26.6% on four datasets. (2) In the few-shot setting, we fine-tune the pre-trained CodeEditor with limited data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor without fine-tuning. CodeEditor correctly edits 1,113 programs while SOTA baselines can not work. The results prove that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing.

</p>
</details>

<details><summary><b>Poison Attack and Defense on Deep Source Code Processing Models</b>
<a href="https://arxiv.org/abs/2210.17029">arxiv:2210.17029</a>
&#x1F4C8; 0 <br>
<p>Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia</p></summary>
<p>

**Abstract:** In the software engineering community, deep learning (DL) has recently been applied to many source code processing tasks. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat, namely poison attack. The attackers aim to inject insidious backdoors into models by poisoning the training data with poison samples. Poisoned models work normally with clean inputs but produce targeted erroneous results with poisoned inputs embedded with triggers. By activating backdoors, attackers can manipulate the poisoned models in security-related scenarios.
  To verify the vulnerability of existing deep source code processing models to the poison attack, we present a poison attack framework for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable even human-imperceptible poison samples and attack models by poisoning the training data with poison samples. To defend against the poison attack, we further propose an effective defense approach named CodeDetector to detect poison samples in the training data. CodeDetector can be applied to many model architectures and effectively defend against multiple poison attack approaches. We apply our CodePoisoner and CodeDetector to three tasks, including defect detection, clone detection, and code repair. The results show that (1) CodePoisoner achieves a high attack success rate (max: 100%) in misleading models to targeted erroneous behaviors. It validates that existing deep source code processing models have a strong vulnerability to the poison attack. (2) CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help practitioners notice the poison attack and inspire the design of more advanced defense techniques.

</p>
</details>

<details><summary><b>Almost Sure Convergence Rates of Stochastic Zeroth-order Gradient Descent for Łojasiewicz Functions</b>
<a href="https://arxiv.org/abs/2210.16997">arxiv:2210.16997</a>
&#x1F4C8; 0 <br>
<p>Tianyu Wang</p></summary>
<p>

**Abstract:** We prove \emph{almost sure convergence rates} of Zeroth-order Gradient Descent (SZGD) algorithms for Łojasiewicz functions. The SZGD algorithm iterates as \begin{align*}
  x_{t+1} = x_t - η_t \widehat{\nabla} f (x_t), \qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function that satisfies the Łojasiewicz inequality with Łojasiewicz exponent $θ$, $η_t$ is the step size (learning rate), and $ \widehat{\nabla} f (x_t) $ is the approximate gradient estimated using zeroth-order information. We show that, for {smooth} Łojasiewicz functions, the sequence $\{ x_t \}_{t\in\mathbb{N}}$ governed by SZGD converges to a bounded point $x_\infty$ almost surely, and $x_\infty$ is a critical point of $f$. If $θ\in (0,\frac{1}{2}]$, $ f (x_t) - f (x_\infty) $, $ \sum_{s=t}^\infty \| x_s - x_\infty \|^2$ and $ \| x_t - x_\infty \| $ ($\| \cdot \|$ is the Euclidean norm) converge to zero \emph{linearly almost surely}. If $θ\in (\frac{1}{2}, 1)$, then $ f (x_t) - f (x_\infty) $ (and $ \sum_{s=t}^\infty \| x_{s+1} - x_s \|^2 $) converges to zero at rate $o \left( t^{\frac{1}{1 - 2θ}} \log t \right) $ almost surely; $ \| x_{t} - x_\infty \| $ converges to zero at rate $o \left( t^{\frac{1-θ}{1-2θ}} \log t \right) $ almost surely. To the best of our knowledge, this paper provides the first \emph{almost sure convergence rate} guarantee for stochastic zeroth order algorithms for Łojasiewicz functions.

</p>
</details>

<details><summary><b>Comparison of two artificial neural networks trained for the surrogate modeling of stress in materially heterogeneous elastoplastic solids</b>
<a href="https://arxiv.org/abs/2210.16994">arxiv:2210.16994</a>
&#x1F4C8; 0 <br>
<p>Sarthak Kapoor, Jaber Rezaei Mianroodi, Mohammad Khorrami, Nima S. Siboni, Bob Svendsen</p></summary>
<p>

**Abstract:** The purpose of this work is the systematic comparison of the application of two artificial neural networks (ANNs) to the surrogate modeling of the stress field in materially heterogeneous periodic polycrystalline microstructures. The first ANN is a UNet-based convolutional neural network (CNN) for periodic data, and the second is based on Fourier neural operators (FNO). Both of these were trained, validated, and tested with results from the numerical solution of the boundary-value problem (BVP) for quasi-static mechanical equilibrium in periodic grain microstructures with square domains. More specifically, these ANNs were trained to correlate the spatial distribution of material properties with the equilibrium stress field under uniaxial tensile loading. The resulting trained ANNs (tANNs) calculate the stress field for a given microstructure on the order of 1000 (UNet) to 2500 (FNO) times faster than the numerical solution of the corresponding BVP.
  For microstructures in the test dataset, the FNO-based tANN, or simply FNO, is more accurate than its UNet-based counterpart; the normalized mean absolute error of different stress components for the former is 0.25-0.40% as compared to 1.41-2.15% for the latter. Errors in FNO are restricted to grain boundary regions, whereas the error in U-Net also comes from within the grain. In comparison to U-Net, errors in FNO are more robust to large variations in spatial resolution as well as small variations in grain density. On other hand, errors in U-Net are robust to variations in boundary box aspect ratio, whereas errors in FNO increase as the domain becomes rectangular. Both tANNs are however unable to reproduce strong stress gradients, especially around regions of stress concentration.

</p>
</details>

<details><summary><b>Learning Heuristics for the Maximum Clique Enumeration Problem Using Low Dimensional Representations</b>
<a href="https://arxiv.org/abs/2210.16963">arxiv:2210.16963</a>
&#x1F4C8; 0 <br>
<p>Ali Baran Taşdemir, Tuna Karacan, Emir Kaan Kırmacı, Lale Özkahya</p></summary>
<p>

**Abstract:** Approximate solutions to various NP-hard combinatorial optimization problems have been found by learned heuristics using complex learning models. In particular, vertex (node) classification in graphs has been a helpful method towards finding the decision boundary to distinguish vertices in an optimal set from the rest. By following this approach, we use a learning framework for a pruning process of the input graph towards reducing the runtime of the maximum clique enumeration problem. We extensively study the role of using different vertex representations on the performance of this heuristic method, using graph embedding algorithms, such as Node2vec and DeepWalk, and representations using higher-order graph features comprising local subgraph counts. Our results show that Node2Vec and DeepWalk are promising embedding methods in representing nodes towards classification purposes. We observe that using local graph features in the classification process produce more accurate results when combined with a feature elimination process. Finally, we provide tests on random graphs to show the robustness and scalability of our method.

</p>
</details>

<details><summary><b>Learning to Defer to Multiple Experts: Consistent Surrogate Losses, Confidence Calibration, and Conformal Ensembles</b>
<a href="https://arxiv.org/abs/2210.16955">arxiv:2210.16955</a>
&#x1F4C8; 0 <br>
<p>Rajeev Verma, Daniel Barrejón, Eric Nalisnick</p></summary>
<p>

**Abstract:** We study the statistical properties of learning to defer (L2D) to multiple experts. In particular, we address the open problems of deriving a consistent surrogate loss, confidence calibration, and principled ensembling of experts. Firstly, we derive two consistent surrogates -- one based on a softmax parameterization, the other on a one-vs-all (OvA) parameterization -- that are analogous to the single expert losses proposed by Mozannar and Sontag (2020) and Verma and Nalisnick (2022), respectively. We then study the frameworks' ability to estimate P( m_j = y | x ), the probability that the jth expert will correctly predict the label for x. Theory shows the softmax-based loss causes mis-calibration to propagate between the estimates while the OvA-based loss does not (though in practice, we find there are trade offs). Lastly, we propose a conformal inference technique that chooses a subset of experts to query when the system defers. We perform empirical validation on tasks for galaxy, skin lesion, and hate speech classification.

</p>
</details>

<details><summary><b>Forget Embedding Layers: Representation Learning for Cold-start in Recommender Systems</b>
<a href="https://arxiv.org/abs/2210.16928">arxiv:2210.16928</a>
&#x1F4C8; 0 <br>
<p>Kuba Weimann, Tim O. F. Conrad</p></summary>
<p>

**Abstract:** Recommender systems suffer from the cold-start problem whenever a new user joins the platform or a new item is added to the catalog. To address item cold-start, we propose to replace the embedding layer in sequential recommenders with a dynamic storage that has no learnable weights and can keep an arbitrary number of representations. In this paper, we present FELRec, a large embedding network that refines the existing representations of users and items in a recursive manner, as new information becomes available. In contrast to similar approaches, our model represents new users and items without side information or time-consuming fine-tuning. During item cold-start, our method outperforms similar method by 29.50%-47.45%. Further, our proposed model generalizes well to previously unseen datasets. The source code is publicly available at github.com/kweimann/FELRec.

</p>
</details>

<details><summary><b>Distributionally Robust Domain Adaptation</b>
<a href="https://arxiv.org/abs/2210.16894">arxiv:2210.16894</a>
&#x1F4C8; 0 <br>
<p>Akram S. Awad, George K. Atia</p></summary>
<p>

**Abstract:** Domain Adaptation (DA) has recently received significant attention due to its potential to adapt a learning model across source and target domains with mismatched distributions. Since DA methods rely exclusively on the given source and target domain samples, they generally yield models that are vulnerable to noise and unable to adapt to unseen samples from the target domain, which calls for DA methods that guarantee the robustness and generalization of the learned models. In this paper, we propose DRDA, a distributionally robust domain adaptation method. DRDA leverages a distributionally robust optimization (DRO) framework to learn a robust decision function that minimizes the worst-case target domain risk and generalizes to any sample from the target domain by transferring knowledge from a given labeled source domain sample. We utilize the Maximum Mean Discrepancy (MMD) metric to construct an ambiguity set of distributions that provably contains the source and target domain distributions with high probability. Hence, the risk is shown to upper bound the out-of-sample target domain loss. Our experimental results demonstrate that our formulation outperforms existing robust learning approaches.

</p>
</details>

<details><summary><b>STGC-GNNs: A GNN-based traffic prediction framework with a spatial-temporal Granger causality graph</b>
<a href="https://arxiv.org/abs/2210.16789">arxiv:2210.16789</a>
&#x1F4C8; 0 <br>
<p>Silu He, Qinyao Luo, Ronghua Du, Ling Zhao, Haifeng Li</p></summary>
<p>

**Abstract:** The key to traffic prediction is to accurately depict the temporal dynamics of traffic flow traveling in a road network, so it is important to model the spatial dependence of the road network. The essence of spatial dependence is to accurately describe how traffic information transmission is affected by other nodes in the road network, and the GNN-based traffic prediction model, as a benchmark for traffic prediction, has become the most common method for the ability to model spatial dependence by transmitting traffic information with the message passing mechanism. However, existing methods model a local and static spatial dependence, which cannot transmit the global-dynamic traffic information (GDTi) required for long-term prediction. The challenge is the difficulty of detecting the precise transmission of GDTi due to the uncertainty of individual transport, especially for long-term transmission. In this paper, we propose a new hypothesis\: GDTi behaves macroscopically as a transmitting causal relationship (TCR) underlying traffic flow, which remains stable under dynamic changing traffic flow. We further propose spatial-temporal Granger causality (STGC) to express TCR, which models global and dynamic spatial dependence. To model global transmission, we model the causal order and causal lag of TCRs global transmission by a spatial-temporal alignment algorithm. To capture dynamic spatial dependence, we approximate the stable TCR underlying dynamic traffic flow by a Granger causality test. The experimental results on three backbone models show that using STGC to model the spatial dependence has better results than the original model for 45 min and 1 h long-term prediction.

</p>
</details>

<details><summary><b>Explainable Predictive Decision Mining for Operational Support</b>
<a href="https://arxiv.org/abs/2210.16786">arxiv:2210.16786</a>
&#x1F4C8; 0 <br>
<p>Gyunam Park, Aaron Küsters, Mara Tews, Cameron Pitsch, Jonathan Schneider, Wil M. P. van der Aalst</p></summary>
<p>

**Abstract:** Several decision points exist in business processes (e.g., whether a purchase order needs a manager's approval or not), and different decisions are made for different process instances based on their characteristics (e.g., a purchase order higher than $500 needs a manager approval). Decision mining in process mining aims to describe/predict the routing of a process instance at a decision point of the process. By predicting the decision, one can take proactive actions to improve the process. For instance, when a bottleneck is developing in one of the possible decisions, one can predict the decision and bypass the bottleneck. However, despite its huge potential for such operational support, existing techniques for decision mining have focused largely on describing decisions but not on predicting them, deploying decision trees to produce logical expressions to explain the decision. In this work, we aim to enhance the predictive capability of decision mining to enable proactive operational support by deploying more advanced machine learning algorithms. Our proposed approach provides explanations of the predicted decisions using SHAP values to support the elicitation of proactive actions. We have implemented a Web application to support the proposed approach and evaluated the approach using the implementation.

</p>
</details>

<details><summary><b>Mitigating Unfairness via Evolutionary Multi-objective Ensemble Learning</b>
<a href="https://arxiv.org/abs/2210.16754">arxiv:2210.16754</a>
&#x1F4C8; 0 <br>
<p>Zhang Qingquan, Liu Jialin, Zhang Zeqi, Wen Junyi, Mao Bifei, Yao Xin</p></summary>
<p>

**Abstract:** In the literature of mitigating unfairness in machine learning, many fairness measures are designed to evaluate predictions of learning models and also utilised to guide the training of fair models. It has been theoretically and empirically shown that there exist conflicts and inconsistencies among accuracy and multiple fairness measures. Optimising one or several fairness measures may sacrifice or deteriorate other measures. Two key questions should be considered, how to simultaneously optimise accuracy and multiple fairness measures, and how to optimise all the considered fairness measures more effectively. In this paper, we view the mitigating unfairness problem as a multi-objective learning problem considering the conflicts among fairness measures. A multi-objective evolutionary learning framework is used to simultaneously optimise several metrics (including accuracy and multiple fairness measures) of machine learning models. Then, ensembles are constructed based on the learning models in order to automatically balance different metrics. Empirical results on eight well-known datasets demonstrate that compared with the state-of-the-art approaches for mitigating unfairness, our proposed algorithm can provide decision-makers with better tradeoffs among accuracy and multiple fairness metrics. Furthermore, the high-quality models generated by the framework can be used to construct an ensemble to automatically achieve a better tradeoff among all the considered fairness metrics than other ensemble methods. Our code is publicly available at https://github.com/qingquan63/FairEMOL

</p>
</details>


{% endraw %}
Prev: [2022.10.29]({{ '/2022/10/29/2022.10.29.html' | relative_url }})  Next: [2022.10.31]({{ '/2022/10/31/2022.10.31.html' | relative_url }})