## Summary for 2021-07-08, created on 2021-12-19


<details><summary><b>Demystifying the Draft EU Artificial Intelligence Act</b>
<a href="https://arxiv.org/abs/2107.03721">arxiv:2107.03721</a>
&#x1F4C8; 99 <br>
<p>Michael Veale, Frederik Zuiderveen Borgesius</p></summary>
<p>

**Abstract:** In April 2021, the European Commission proposed a Regulation on Artificial Intelligence, known as the AI Act. We present an overview of the Act and analyse its implications, drawing on scholarship ranging from the study of contemporary AI practices to the structure of EU product safety regimes over the last four decades. Aspects of the AI Act, such as different rules for different risk-levels of AI, make sense. But we also find that some provisions of the Draft AI Act have surprising legal implications, whilst others may be largely ineffective at achieving their stated goals. Several overarching aspects, including the enforcement regime and the risks of maximum harmonisation pre-empting legitimate national AI policy, engender significant concern. These issues should be addressed as a priority in the legislative process.

</p>
</details>

<details><summary><b>RMA: Rapid Motor Adaptation for Legged Robots</b>
<a href="https://arxiv.org/abs/2107.04034">arxiv:2107.04034</a>
&#x1F4C8; 90 <br>
<p>Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik</p></summary>
<p>

**Abstract:** Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments. Video results at https://ashish-kmr.github.io/rma-legged-robots/

</p>
</details>

<details><summary><b>3D Neural Scene Representations for Visuomotor Control</b>
<a href="https://arxiv.org/abs/2107.04004">arxiv:2107.04004</a>
&#x1F4C8; 46 <br>
<p>Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba</p></summary>
<p>

**Abstract:** Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.

</p>
</details>

<details><summary><b>Bag of Tricks for Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2107.03719">arxiv:2107.03719</a>
&#x1F4C8; 26 <br>
<p>Thomas Elsken, Benedikt Staffler, Arber Zela, Jan Hendrik Metzen, Frank Hutter</p></summary>
<p>

**Abstract:** While neural architecture search methods have been successful in previous years and led to new state-of-the-art performance on various problems, they have also been criticized for being unstable, being highly sensitive with respect to their hyperparameters, and often not performing better than random search. To shed some light on this issue, we discuss some practical considerations that help improve the stability, efficiency and overall performance.

</p>
</details>

<details><summary><b>Quantum belief function</b>
<a href="https://arxiv.org/abs/2107.03930">arxiv:2107.03930</a>
&#x1F4C8; 24 <br>
<p>Qianli Zhou, Guojing Tian, Yong Deng</p></summary>
<p>

**Abstract:** The belief function in Dempster Shafer evidence theory can express more information than the traditional Bayesian distribution. It is widely used in approximate reasoning, decision-making and information fusion. However, its power exponential explosion characteristics leads to the extremely high computational complexity when handling large amounts of elements in classic computers. In order to solve the problem, we encode the basic belief assignment (BBA) into quantum states, which makes each qubit correspond to control an element. Besides the high efficiency, this quantum expression is very conducive to measure the similarity between two BBAs, and the measuring quantum algorithm we come up with has exponential acceleration theoretically compared to the corresponding classical algorithm. In addition, we simulate our quantum version of BBA on Qiskit platform, which ensures the rationality of our algorithm experimentally. We believe our results will shed some light on utilizing the characteristic of quantum computation to handle belief function more conveniently.

</p>
</details>

<details><summary><b>Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers</b>
<a href="https://arxiv.org/abs/2107.03996">arxiv:2107.03996</a>
&#x1F4C8; 21 <br>
<p>Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, Xiaolong Wang</p></summary>
<p>

**Abstract:** We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/ .

</p>
</details>

<details><summary><b>Active Safety Envelopes using Light Curtains with Probabilistic Guarantees</b>
<a href="https://arxiv.org/abs/2107.04000">arxiv:2107.04000</a>
&#x1F4C8; 10 <br>
<p>Siddharth Ancha, Gaurav Pathak, Srinivasa G. Narasimhan, David Held</p></summary>
<p>

**Abstract:** To safely navigate unknown environments, robots must accurately perceive dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR sensor, we explore the use of a much cheaper and higher resolution sensor: programmable light curtains. Light curtains are controllable depth sensors that sense only along a surface that a user selects. We use light curtains to estimate the safety envelope of a scene: a hypothetical surface that separates the robot from all obstacles. We show that generating light curtains that sense random locations (from a particular distribution) can quickly discover the safety envelope for scenes with unknown objects. Importantly, we produce theoretical safety guarantees on the probability of detecting an obstacle using random curtains. We combine random curtains with a machine learning based model that forecasts and tracks the motion of the safety envelope efficiently. Our method accurately estimates safety envelopes while providing probabilistic safety guarantees that can be used to certify the efficacy of a robot perception system to detect and avoid dynamic obstacles. We evaluate our approach in a simulated urban driving environment and a real-world environment with moving pedestrians using a light curtain device and show that we can estimate safety envelopes efficiently and effectively. Project website: https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees

</p>
</details>

<details><summary><b>Imitation by Predicting Observations</b>
<a href="https://arxiv.org/abs/2107.03851">arxiv:2107.03851</a>
&#x1F4C8; 10 <br>
<p>Andrew Jaegle, Yury Sulsky, Arun Ahuja, Jake Bruce, Rob Fergus, Greg Wayne</p></summary>
<p>

**Abstract:** Imitation learning enables agents to reuse and adapt the hard-won expertise of others, offering a solution to several key challenges in learning behavior. Although it is easy to observe behavior in the real-world, the underlying actions may not be accessible. We present a new method for imitation solely from observations that achieves comparable performance to experts on challenging continuous control tasks while also exhibiting robustness in the presence of observations unrelated to the task. Our method, which we call FORM (for "Future Observation Reward Model") is derived from an inverse RL objective and imitates using a model of expert behavior learned by generative modelling of the expert's observations, without needing ground truth actions. We show that FORM performs comparably to a strong baseline IRL method (GAIL) on the DeepMind Control Suite benchmark, while outperforming GAIL in the presence of task-irrelevant features.

</p>
</details>

<details><summary><b>A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models</b>
<a href="https://arxiv.org/abs/2107.03844">arxiv:2107.03844</a>
&#x1F4C8; 9 <br>
<p>Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury</p></summary>
<p>

**Abstract:** Bangla -- ranked as the 6th most widely spoken language across the world (https://www.ethnologue.com/guides/ethnologue200), with 230 million native speakers -- is still considered as a low-resource language in the natural language processing (NLP) community. With three decades of research, Bangla NLP (BNLP) is still lagging behind mainly due to the scarcity of resources and the challenges that come with it. There is sparse work in different areas of BNLP; however, a thorough survey reporting previous work and recent advances is yet to be done. In this study, we first provide a review of Bangla NLP tasks, resources, and tools available to the research community; we benchmark datasets collected from various platforms for nine NLP tasks using current state-of-the-art algorithms (i.e., transformer-based models). We provide comparative results for the studied NLP tasks by comparing monolingual vs. multilingual models of varying sizes. We report our results using both individual and consolidated datasets and provide data splits for future research. We reviewed a total of 108 papers and conducted 175 sets of experiments. Our results show promising performance using transformer-based models while highlighting the trade-off with computational costs. We hope that such a comprehensive survey will motivate the community to build on and further advance the research on Bangla NLP.

</p>
</details>

<details><summary><b>EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments</b>
<a href="https://arxiv.org/abs/2107.04174">arxiv:2107.04174</a>
&#x1F4C8; 8 <br>
<p>Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark Broyles, Hao Jiang, Jie Shen, Maja Pantic, Vamsi Krishna Ithapu, Ravish Mehra</p></summary>
<p>

**Abstract:** Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the cocktail party effect. Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities. Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data. To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment. In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer. We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics. The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.

</p>
</details>

<details><summary><b>Offline Meta-Reinforcement Learning with Online Self-Supervision</b>
<a href="https://arxiv.org/abs/2107.03974">arxiv:2107.03974</a>
&#x1F4C8; 8 <br>
<p>Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, Sergey Levine</p></summary>
<p>

**Abstract:** Meta-reinforcement learning (RL) can meta-train policies that adapt to new tasks with orders of magnitude less data than standard RL, but meta-training itself is costly and time-consuming. If we can meta-train on offline data, then we can reuse the same static dataset, labeled once with rewards for different tasks, to meta-train policies that adapt to a variety of new tasks at meta-test time. Although this capability would make meta-RL a practical tool for real-world use, offline meta-RL presents additional challenges beyond online meta-RL or standard offline RL settings. Meta-RL learns an exploration strategy that collects data for adapting, and also meta-trains a policy that quickly adapts to data from a new task. Since this policy was meta-trained on a fixed, offline dataset, it might behave unpredictably when adapting to data collected by the learned exploration strategy, which differs systematically from the offline data and thus induces distributional shift. We do not want to remove this distributional shift by simply adopting a conservative exploration strategy, because learning an exploration strategy enables an agent to collect better data for faster adaptation. Instead, we propose a hybrid offline meta-RL algorithm, which uses offline data with rewards to meta-train an adaptive policy, and then collects additional unsupervised online data, without any reward labels to bridge this distribution shift. By not requiring reward labels for online collection, this data can be much cheaper to collect. We compare our method to prior work on offline meta-RL on simulated robot locomotion and manipulation tasks and find that using additional unsupervised online data collection leads to a dramatic improvement in the adaptive capabilities of the meta-trained policies, matching the performance of fully online meta-RL on a range of challenging domains that require generalization to new tasks.

</p>
</details>

<details><summary><b>Optimizing Data Processing in Space for Object Detection in Satellite Imagery</b>
<a href="https://arxiv.org/abs/2107.03774">arxiv:2107.03774</a>
&#x1F4C8; 8 <br>
<p>Martina Lofqvist, José Cano</p></summary>
<p>

**Abstract:** There is a proliferation in the number of satellites launched each year, resulting in downlinking of terabytes of data each day. The data received by ground stations is often unprocessed, making this an expensive process considering the large data sizes and that not all of the data is useful. This, coupled with the increasing demand for real-time data processing, has led to a growing need for on-orbit processing solutions. In this work, we investigate the performance of CNN-based object detectors on constrained devices by applying different image compression techniques to satellite data. We examine the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier; low-power, high-performance computers, with integrated GPUs, small enough to fit on-board a nanosatellite. We take a closer look at object detection networks, including the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of execution time, memory consumption, and accuracy, and are compared against a baseline containing a server with two powerful GPUs. The results show that by applying image compression techniques, we are able to improve the execution time and memory consumption, achieving a fully runnable dataset. A lossless compression technique achieves roughly a 10% reduction in execution time and about a 3% reduction in memory consumption, with no impact on the accuracy. While a lossy compression technique improves the execution time by up to 144% and the memory consumption is reduced by as much as 97%. However, it has a significant impact on accuracy, varying depending on the compression ratio. Thus the application and ratio of these compression techniques may differ depending on the required level of accuracy for a particular task.

</p>
</details>

<details><summary><b>MCMC Variational Inference via Uncorrected Hamiltonian Annealing</b>
<a href="https://arxiv.org/abs/2107.04150">arxiv:2107.04150</a>
&#x1F4C8; 7 <br>
<p>Tomas Geffner, Justin Domke</p></summary>
<p>

**Abstract:** Given an unnormalized target distribution we want to obtain approximate samples from it and a tight lower bound on its (log) normalization constant log Z. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful method that can be used to do this. Its main drawback is that it uses non-differentiable transition kernels, which makes tuning its many parameters hard. We propose a framework to use an AIS-like procedure with Uncorrected Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to tight and differentiable lower bounds on log Z. We show empirically that our method yields better performances than other competing approaches, and that the ability to tune its parameters using reparameterization gradients may lead to large performance improvements.

</p>
</details>

<details><summary><b>Multi-Modality Task Cascade for 3D Object Detection</b>
<a href="https://arxiv.org/abs/2107.04013">arxiv:2107.04013</a>
&#x1F4C8; 7 <br>
<p>Jinhyung Park, Xinshuo Weng, Yunze Man, Kris Kitani</p></summary>
<p>

**Abstract:** Point clouds and RGB images are naturally complementary modalities for 3D visual understanding - the former provides sparse but accurate locations of points on objects, while the latter contains dense color and texture information. Despite this potential for close sensor fusion, many methods train two models in isolation and use simple feature concatenation to represent 3D sensor data. This separated training scheme results in potentially sub-optimal performance and prevents 3D tasks from being used to benefit 2D tasks that are often useful on their own. To provide a more integrated approach, we propose a novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box proposals to improve 2D segmentation predictions, which are then used to further refine the 3D boxes. We show that including a 2D network between two stages of 3D modules significantly improves both 2D and 3D task performance. Moreover, to prevent the 3D module from over-relying on the overfitted 2D predictions, we propose a dual-head 2D segmentation training and inference scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D segmentation predictions. Evaluating our model on the challenging SUN RGB-D dataset, we improve upon state-of-the-art results of both single modality and fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be released $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$

</p>
</details>

<details><summary><b>Malware Classification Using Deep Boosted Learning</b>
<a href="https://arxiv.org/abs/2107.04008">arxiv:2107.04008</a>
&#x1F4C8; 7 <br>
<p>Muhammad Asam, Saddam Hussain Khan, Tauseef Jamal, Umme Zahoora, Asifullah Khan</p></summary>
<p>

**Abstract:** Malicious activities in cyberspace have gone further than simply hacking machines and spreading viruses. It has become a challenge for a nations survival and hence has evolved to cyber warfare. Malware is a key component of cyber-crime, and its analysis is the first line of defence against attack. This work proposes a novel deep boosted hybrid learning-based malware classification framework and named as Deep boosted Feature Space-based Malware classification (DFS-MC). In the proposed framework, the discrimination power is enhanced by fusing the feature spaces of the best performing customized CNN architectures models and its discrimination by an SVM for classification. The discrimination capacity of the proposed classification framework is assessed by comparing it against the standard customized CNNs. The customized CNN models are implemented in two ways: softmax classifier and deep hybrid learning-based malware classification. In the hybrid learning, Deep features are extracted from customized CNN architectures and fed into the conventional machine learning classifier to improve the classification performance. We also introduced the concept of transfer learning in a customized CNN architecture based malware classification framework through fine-tuning. The performance of the proposed malware classification approaches are validated on the MalImg malware dataset using the hold-out cross-validation technique. Experimental comparisons were conducted by employing innovative, customized CNN, trained from scratch and fine-tuning the customized CNN using transfer learning. The proposed classification framework DFS-MC showed improved results, Accuracy: 98.61%, F-score: 0.96, Precision: 0.96, and Recall: 0.96.

</p>
</details>

<details><summary><b>Nearest neighbour approaches for Emotion Detection in Tweets</b>
<a href="https://arxiv.org/abs/2107.05394">arxiv:2107.05394</a>
&#x1F4C8; 6 <br>
<p>Olha Kaminska, Chris Cornelis, Veronique Hoste</p></summary>
<p>

**Abstract:** Emotion detection is an important task that can be applied to social media data to discover new knowledge. While the use of deep learning methods for this task has been prevalent, they are black-box models, making their decisions hard to interpret for a human operator. Therefore, in this paper, we propose an approach using weighted $k$ Nearest Neighbours (kNN), a simple, easy to implement, and explainable machine learning model. These qualities can help to enhance results' reliability and guide error analysis. In particular, we apply the weighted kNN model to the shared emotion detection task in tweets from SemEval-2018. Tweets are represented using different text embedding methods and emotion lexicon vocabulary scores, and classification is done by an ensemble of weighted kNN models. Our best approaches obtain results competitive with state-of-the-art solutions and open up a promising alternative path to neural network methods.

</p>
</details>

<details><summary><b>CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics</b>
<a href="https://arxiv.org/abs/2107.03964">arxiv:2107.03964</a>
&#x1F4C8; 6 <br>
<p>Sibendu Paul, Kunal Rao, Giuseppe Coviello, Murugan Sankaradas, Oliver Po, Y. Charlie Hu, Srimat T. Chakradhar</p></summary>
<p>

**Abstract:** Video analytics systems critically rely on video cameras, which capture high-quality video frames, to achieve high analytics accuracy. Although modern video cameras often expose tens of configurable parameter settings that can be set by end-users, deployment of surveillance cameras today often uses a fixed set of parameter settings because the end-users lack the skill or understanding to reconfigure these parameters.
  In this paper, we first show that in a typical surveillance camera deployment, environmental condition changes can significantly affect the accuracy of analytics units such as person detection, face detection and face recognition, and how such adverse impact can be mitigated by dynamically adjusting camera settings. We then propose CAMTUNER, a framework that can be easily applied to an existing video analytics pipeline (VAP) to enable automatic and dynamic adaptation of complex camera settings to changing environmental conditions, and autonomously optimize the accuracy of analytics units (AUs) in the VAP. CAMTUNER is based on SARSA reinforcement learning (RL) and it incorporates two novel components: a light-weight analytics quality estimator and a virtual camera. CAMTUNER is implemented in a system with AXIS surveillance cameras and several VAPs (with various AUs) that processed day-long customer videos captured at airport entrances. Our evaluations show that CAMTUNER can adapt quickly to changing environments. We compared CAMTUNER with two alternative approaches where either static camera settings were used, or a strawman approach where camera settings were manually changed every hour (based on human perception of quality). We observed that for the face detection and person detection AUs, CAMTUNER is able to achieve up to 13.8% and 9.2% higher accuracy, respectively, compared to the best of the two approaches (average improvement of 8% for both AUs).

</p>
</details>

<details><summary><b>MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications</b>
<a href="https://arxiv.org/abs/2107.03653">arxiv:2107.03653</a>
&#x1F4C8; 6 <br>
<p>Nikhil Pratap Ghanathe, Vivek Seshadri, Rahul Sharma, Steve Wilton, Aayan Kumar</p></summary>
<p>

**Abstract:** Recent breakthroughs in ML have produced new classes of models that allow ML inference to run directly on milliwatt-powered IoT devices. On one hand, existing ML-to-FPGA compilers are designed for deep neural-networks on large FPGAs. On the other hand, general-purpose HLS tools fail to exploit properties specific to ML inference, thereby resulting in suboptimal performance. We propose MAFIA, a tool to compile ML inference on small form-factor FPGAs for IoT applications. MAFIA provides native support for linear algebra operations and can express a variety of ML algorithms, including state-of-the-art models. We show that MAFIA-generated programs outperform best-performing variant of a commercial HLS compiler by 2.5x on average.

</p>
</details>

<details><summary><b>Fuzzy-Rough Nearest Neighbour Approaches for Emotion Detection in Tweets</b>
<a href="https://arxiv.org/abs/2107.05392">arxiv:2107.05392</a>
&#x1F4C8; 5 <br>
<p>Olha Kaminska, Chris Cornelis, Veronique Hoste</p></summary>
<p>

**Abstract:** Social media are an essential source of meaningful data that can be used in different tasks such as sentiment analysis and emotion recognition. Mostly, these tasks are solved with deep learning methods. Due to the fuzzy nature of textual data, we consider using classification methods based on fuzzy rough sets. Specifically, we develop an approach for the SemEval-2018 emotion detection task, based on the fuzzy rough nearest neighbour (FRNN) classifier enhanced with ordered weighted average (OWA) operators. We use tuned ensembles of FRNN--OWA models based on different text embedding methods. Our results are competitive with the best SemEval solutions based on more complicated deep learning methods.

</p>
</details>

<details><summary><b>On the Variance of the Fisher Information for Deep Learning</b>
<a href="https://arxiv.org/abs/2107.04205">arxiv:2107.04205</a>
&#x1F4C8; 5 <br>
<p>Alexander Soen, Ke Sun</p></summary>
<p>

**Abstract:** In the realm of deep learning, the Fisher information matrix (FIM) gives novel insights and useful tools to characterize the loss landscape, perform second-order optimization, and build geometric learning theories. The exact FIM is either unavailable in closed form or too expensive to compute. In practice, it is almost always estimated based on empirical samples. We investigate two such estimators based on two equivalent representations of the FIM -- both unbiased and consistent. Their estimation quality is naturally gauged by their variance given in closed form. We analyze how the parametric structure of a deep neural network can affect the variance. The meaning of this variance measure and its upper bounds are then discussed in the context of deep learning.

</p>
</details>

<details><summary><b>A Systematic Survey of Text Worlds as Embodied Natural Language Environments</b>
<a href="https://arxiv.org/abs/2107.04132">arxiv:2107.04132</a>
&#x1F4C8; 5 <br>
<p>Peter A Jansen</p></summary>
<p>

**Abstract:** Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich high-level action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.

</p>
</details>

<details><summary><b>Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.04050">arxiv:2107.04050</a>
&#x1F4C8; 5 <br>
<p>Barna Pasztor, Ilija Bogunovic, Andreas Krause</p></summary>
<p>

**Abstract:** Learning in multi-agent systems is highly challenging due to the inherent complexity introduced by agents' interactions. We tackle systems with a huge population of interacting agents (e.g., swarms) via Mean-Field Control (MFC). MFC considers an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. Specifically, we consider the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm $\text{M}^3\text{-UCRL}$ that runs in episodes and provably solves this problem. $\text{M}^3\text{-UCRL}$ uses upper-confidence bounds to balance exploration and exploitation during policy learning. Our main theoretical contributions are the first general regret bounds for model-based RL for MFC, obtained via a novel mean-field type analysis. $\text{M}^3\text{-UCRL}$ can be instantiated with different models such as neural networks or Gaussian Processes, and effectively combined with neural network policy learning. We empirically demonstrate the convergence of $\text{M}^3\text{-UCRL}$ on the swarm motion problem of controlling an infinite population of agents seeking to maximize location-dependent reward and avoid congested areas.

</p>
</details>

<details><summary><b>CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems</b>
<a href="https://arxiv.org/abs/2107.03884">arxiv:2107.03884</a>
&#x1F4C8; 5 <br>
<p>Aadesh Gupta, Kaustubh D. Dhole, Rahul Tarway, Swetha Prabhakar, Ashish Shrivastava</p></summary>
<p>

**Abstract:** Domain-specific dialogue systems generally determine user intents by relying on sentence-level classifiers which mainly focus on single action sentences. Such classifiers are not designed to effectively handle complex queries composed of conditional and sequential clauses that represent multiple actions. We attempt to decompose such queries into smaller single-action sub-queries that are reasonable for intent classifiers to understand in a dialogue pipeline. We release CANDLE (Conditional & AND type Expressions), a dataset consisting of 3124 utterances manually tagged with conditional and sequential labels and demonstrates this decomposition by training two baseline taggers.

</p>
</details>

<details><summary><b>Heterogeneous Global Graph Neural Networks for Personalized Session-based Recommendation</b>
<a href="https://arxiv.org/abs/2107.03813">arxiv:2107.03813</a>
&#x1F4C8; 5 <br>
<p>Yitong Pang, Lingfei Wu, Qi Shen, Yiming Zhang, Zhihua Wei, Fangli Xu, Ethan Chang, Bo Long, Jian Pei</p></summary>
<p>

**Abstract:** Predicting the next interaction of a short-term interaction session is a challenging task in session-based recommendation. Almost all existing works rely on item transition patterns, and neglect the impact of user historical sessions while modeling user preference, which often leads to non-personalized recommendation. Additionally, existing personalized session-based recommenders capture user preference only based on the sessions of the current user, but ignore the useful item-transition patterns from other user's historical sessions. To address these issues, we propose a novel Heterogeneous Global Graph Neural Networks (HG-GNN) to exploit the item transitions over all sessions in a subtle manner for better inferring user preference from the current and historical sessions. To effectively exploit the item transitions over all sessions from users, we propose a novel heterogeneous global graph that contains item transitions of sessions, user-item interactions and global co-occurrence items. Moreover, to capture user preference from sessions comprehensively, we propose to learn two levels of user representations from the global graph via two graph augmented preference encoders. Specifically, we design a novel heterogeneous graph neural network (HGNN) on the heterogeneous global graph to learn the long-term user preference and item representations with rich semantics. Based on the HGNN, we propose the Current Preference Encoder and the Historical Preference Encoder to capture the different levels of user preference from the current and historical sessions, respectively. To achieve personalized recommendation, we integrate the representations of the user current preference and historical interests to generate the final user preference representation. Extensive experimental results on three real-world datasets show that our model outperforms other state-of-the-art methods.

</p>
</details>

<details><summary><b>Selective Focusing Learning in Conditional GANs</b>
<a href="https://arxiv.org/abs/2107.08792">arxiv:2107.08792</a>
&#x1F4C8; 4 <br>
<p>Kyeongbo Kong, Kyunghun Kim, Woo-Jin Song, Suk-Ju Kang</p></summary>
<p>

**Abstract:** Conditional generative adversarial networks (cGANs) have demonstrated remarkable success due to their class-wise controllability and superior quality for complex generation tasks. Typical cGANs solve the joint distribution matching problem by decomposing two easier sub-problems: marginal matching and conditional matching. From our toy experiments, we found that it is the best to apply only conditional matching to certain samples due to the content-aware optimization of the discriminator. This paper proposes a simple (a few lines of code) but effective training methodology, selective focusing learning, which enforces the discriminator and generator to learn easy samples of each class rapidly while maintaining diversity. Our key idea is to selectively apply conditional and joint matching for the data in each mini-batch. We conducted experiments on recent cGAN variants in ImageNet (64x64 and 128x128), CIFAR-10, and CIFAR-100 datasets, and improved the performance significantly (up to 35.18% in terms of FID) without sacrificing diversity.

</p>
</details>

<details><summary><b>Levi Graph AMR Parser using Heterogeneous Attention</b>
<a href="https://arxiv.org/abs/2107.04152">arxiv:2107.04152</a>
&#x1F4C8; 4 <br>
<p>Han He, Jinho D. Choi</p></summary>
<p>

**Abstract:** Coupled with biaffine decoders, transformers have been effectively adapted to text-to-graph transduction and achieved state-of-the-art performance on AMR parsing. Many prior works, however, rely on the biaffine decoder for either or both arc and label predictions although most features used by the decoder may be learned by the transformer already. This paper presents a novel approach to AMR parsing by combining heterogeneous data (tokens, concepts, labels) as one input to a transformer to learn attention, and use only attention matrices from the transformer to predict all elements in AMR graphs (concepts, arcs, labels). Although our models use significantly fewer parameters than the previous state-of-the-art graph parser, they show similar or better accuracy on AMR 2.0 and 3.0.

</p>
</details>

<details><summary><b>Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration</b>
<a href="https://arxiv.org/abs/2107.04144">arxiv:2107.04144</a>
&#x1F4C8; 4 <br>
<p>Saad Abbasi, Mohammad Javad Shafiee, Ellick Chan, Alexander Wong</p></summary>
<p>

**Abstract:** The fine-grained relationship between form and function with respect to deep neural network architecture design and hardware-specific acceleration is one area that is not well studied in the research literature, with form often dictated by accuracy as opposed to hardware function. In this study, a comprehensive empirical exploration is conducted to investigate the impact of deep neural network architecture design on the degree of inference speedup that can be achieved via hardware-specific acceleration. More specifically, we empirically study the impact of a variety of commonly used macro-architecture design patterns across different architectural depths through the lens of OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental results showed that while leveraging hardware-specific acceleration achieved an average inference speed-up of 380%, the degree of inference speed-up varied drastically depending on the macro-architecture design pattern, with the greatest speedup achieved on the depthwise bottleneck convolution design pattern at 550%. Furthermore, we conduct an in-depth exploration of the correlation between FLOPs requirement, level 3 cache efficacy, and network latency with increasing architectural depth and width. Finally, we analyze the inference time reductions using hardware-specific acceleration when compared to native deep learning frameworks across a wide variety of hand-crafted deep convolutional neural network architecture designs as well as ones found via neural architecture search strategies. We found that the DARTS-derived architecture to benefit from the greatest improvement from hardware-specific software acceleration (1200%) while the depthwise bottleneck convolution-based MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.

</p>
</details>

<details><summary><b>Learning to Delegate for Large-scale Vehicle Routing</b>
<a href="https://arxiv.org/abs/2107.04139">arxiv:2107.04139</a>
&#x1F4C8; 4 <br>
<p>Sirui Li, Zhongxia Yan, Cathy Wu</p></summary>
<p>

**Abstract:** Vehicle routing problems (VRPs) form a class of combinatorial problems with wide practical applications. While previous heuristic or learning-based works achieve decent solutions on small problem instances of up to 100 cities, their performance deteriorates in large problems. This article presents a novel learning-augmented local search framework to solve large-scale VRP. The method iteratively improves the solution by identifying appropriate subproblems and $\textit{delegating}$ their improvement to a black box subsolver. At each step, we leverage spatial locality to consider only a linear number of subproblems, rather than exponential. We frame subproblem selection as regression and train a Transformer on a generated training set of problem instances. Our method accelerates state-of-the-art VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000. Learned subproblem selection offers a 1.5x to 2x speedup over heuristic or random selection. Our results generalize to a variety of VRP distributions, variants, and solvers.

</p>
</details>

<details><summary><b>CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation</b>
<a href="https://arxiv.org/abs/2107.04099">arxiv:2107.04099</a>
&#x1F4C8; 4 <br>
<p>Andrea Liew, Chun Cheng Lee, Boon Leong Lan, Maxine Tan</p></summary>
<p>

**Abstract:** Convolutional neural networks (CNNs) have been used quite successfully for semantic segmentation of brain tumors. However, current CNNs and attention mechanisms are stochastic in nature and neglect the morphological indicators used by radiologists to manually annotate regions of interest. In this paper, we introduce a channel and spatial wise asymmetric attention (CASPIAN) by leveraging the inherent structure of tumors to detect regions of saliency. To demonstrate the efficacy of our proposed layer, we integrate this into a well-established convolutional neural network (CNN) architecture to achieve higher Dice scores, with less GPU resources. Also, we investigate the inclusion of auxiliary multiscale and multiplanar attention branches to increase the spatial context crucial in semantic segmentation tasks. The resulting architecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole tumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven by the scarcity of brain tumor data, we investigate the Noisy Student method for segmentation tasks. Our new Noisy Student Curriculum Learning paradigm, which infuses noise incrementally to increase the complexity of the training images exposed to the network, further boosts the enhancing tumor region to 81.53%. Additional validation performed on the BraTS2020 data shows that the Noisy Student Curriculum Learning method works well without any additional training or finetuning.

</p>
</details>

<details><summary><b>Robust Counterfactual Explanations on Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2107.04086">arxiv:2107.04086</a>
&#x1F4C8; 4 <br>
<p>Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, Yong Zhang</p></summary>
<p>

**Abstract:** Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.

</p>
</details>

<details><summary><b>Scaling Gaussian Processes with Derivative Information Using Variational Inference</b>
<a href="https://arxiv.org/abs/2107.04061">arxiv:2107.04061</a>
&#x1F4C8; 4 <br>
<p>Misha Padidar, Xinran Zhu, Leo Huang, Jacob R. Gardner, David Bindel</p></summary>
<p>

**Abstract:** Gaussian processes with derivative information are useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Incorporating derivative observations, however, comes with a dominating $O(N^3D^3)$ computational cost when training on $N$ points in $D$ input dimensions. This is intractable for even moderately sized problems. While recent work has addressed this intractability in the low-$D$ setting, the high-$N$, high-$D$ setting is still unexplored and of great value, particularly as machine learning problems increasingly become high dimensional. In this paper, we introduce methods to achieve fully scalable Gaussian process regression with derivatives using variational inference. Analogous to the use of inducing values to sparsify the labels of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information of a training set. This enables us to construct a variational posterior that incorporates derivative information but whose size depends neither on the full dataset size $N$ nor the full dimensionality $D$. We demonstrate the full scalability of our approach on a variety of tasks, ranging from a high dimensional stellarator fusion regression task to training graph convolutional neural networks on Pubmed using Bayesian optimization. Surprisingly, we find that our approach can improve regression performance even in settings where only label data is available.

</p>
</details>

<details><summary><b>Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases</b>
<a href="https://arxiv.org/abs/2107.03985">arxiv:2107.03985</a>
&#x1F4C8; 4 <br>
<p>Subhashini Venugopalan, Joel Shor, Manoj Plakal, Jimmy Tobin, Katrin Tomanek, Jordan R. Green, Michael P. Brenner</p></summary>
<p>

**Abstract:** Automatic classification of disordered speech can provide an objective tool for identifying the presence and severity of speech impairment. Classification approaches can also help identify hard-to-recognize speech samples to teach ASR systems about the variable manifestations of impaired speech. Here, we develop and compare different deep learning techniques to classify the intelligibility of disordered speech on selected phrases. We collected samples from a diverse set of 661 speakers with a variety of self-reported disorders speaking 29 words or phrases, which were rated by speech-language pathologists for their overall intelligibility using a five-point Likert scale. We then evaluated classifiers developed using 3 approaches: (1) a convolutional neural network (CNN) trained for the task, (2) classifiers trained on non-semantic speech representations from CNNs that used an unsupervised objective [1], and (3) classifiers trained on the acoustic (encoder) embeddings from an ASR system trained on typical speech [2]. We found that the ASR encoder's embeddings considerably outperform the other two on detecting and classifying disordered speech. Further analysis shows that the ASR embeddings cluster speech by the spoken phrase, while the non-semantic embeddings cluster speech by speaker. Also, longer phrases are more indicative of intelligibility deficits than single words.

</p>
</details>

<details><summary><b>Locally differentially private estimation of nonlinear functionals of discrete distributions</b>
<a href="https://arxiv.org/abs/2107.03940">arxiv:2107.03940</a>
&#x1F4C8; 4 <br>
<p>Cristina Butucea, Yann Issartel</p></summary>
<p>

**Abstract:** We study the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $α$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_γ = \sum_{k=1}^K p_k^γ$, $γ>0$ as a function of $K, \, n$ and $α$. In the non-interactive case, we study two plug-in type estimators of $F_γ$, for all $γ>0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to the privacy constraint the rates we attain are slower and similar to those obtained in the Gaussian model by Collier et al. (2020). In the interactive case, we introduce for all $γ>1$ a two-step procedure which attains the faster parametric rate $(n α^2)^{-1/2}$ when $γ\geq 2$. We give lower bounds results over all $α$-LDP mechanisms and all estimators using the private samples.

</p>
</details>

<details><summary><b>Degrees of riskiness, falsifiability, and truthlikeness. A neo-Popperian account applicable to probabilistic theories</b>
<a href="https://arxiv.org/abs/2107.03772">arxiv:2107.03772</a>
&#x1F4C8; 4 <br>
<p>Leander Vignero, Sylvia Wenmackers</p></summary>
<p>

**Abstract:** In this paper, we take a fresh look at three Popperian concepts: riskiness, falsifiability, and truthlikeness (or verisimilitude) of scientific hypotheses or theories. First, we make explicit the dimensions that underlie the notion of riskiness. Secondly, we examine if and how degrees of falsifiability can be defined, and how they are related to various dimensions of the concept of riskiness as well as the experimental context. Thirdly, we consider the relation of riskiness to (expected degrees of) truthlikeness. Throughout, we pay special attention to probabilistic theories and we offer a tentative, quantitative account of verisimilitude for probabilistic theories.

</p>
</details>

<details><summary><b>Discrete-time Temporal Network Embedding via Implicit Hierarchical Learning in Hyperbolic Space</b>
<a href="https://arxiv.org/abs/2107.03767">arxiv:2107.03767</a>
&#x1F4C8; 4 <br>
<p>Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, Irwin King</p></summary>
<p>

**Abstract:** Representation learning over temporal networks has drawn considerable attention in recent years. Efforts are mainly focused on modeling structural dependencies and temporal evolving regularities in Euclidean space which, however, underestimates the inherent complex and hierarchical properties in many real-world temporal networks, leading to sub-optimal embeddings. To explore these properties of a complex temporal network, we propose a hyperbolic temporal graph network (HTGN) that fully takes advantage of the exponential capacity and hierarchical awareness of hyperbolic geometry. More specially, HTGN maps the temporal graph into hyperbolic space, and incorporates hyperbolic graph neural network and hyperbolic gated recurrent neural network, to capture the evolving behaviors and implicitly preserve hierarchical information simultaneously. Furthermore, in the hyperbolic space, we propose two important modules that enable HTGN to successfully model temporal networks: (1) hyperbolic temporal contextual self-attention (HTA) module to attend to historical states and (2) hyperbolic temporal consistency (HTC) module to ensure stability and generalization. Experimental results on multiple real-world datasets demonstrate the superiority of HTGN for temporal graph embedding, as it consistently outperforms competing methods by significant margins in various temporal link prediction tasks. Specifically, HTGN achieves AUC improvement up to 9.98% for link prediction and 11.4% for new link prediction. Moreover, the ablation study further validates the representational ability of hyperbolic geometry and the effectiveness of the proposed HTA and HTC modules.

</p>
</details>

<details><summary><b>Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation</b>
<a href="https://arxiv.org/abs/2107.03742">arxiv:2107.03742</a>
&#x1F4C8; 4 <br>
<p>Nikolay Jetchev, Gökhan Yildirim, Christian Bracher, Roland Vollgraf</p></summary>
<p>

**Abstract:** Attention is a general reasoning mechanism than can flexibly deal with image information, but its memory requirements had made it so far impractical for high resolution image generation. We present Grid Partitioned Attention (GPA), a new approximate attention algorithm that leverages a sparse inductive bias for higher computational and memory efficiency in image domains: queries attend only to few keys, spatially close queries attend to close keys due to correlations. Our paper introduces the new attention layer, analyzes its complexity and how the trade-off between memory usage and model power can be tuned by the hyper-parameters.We will show how such attention enables novel deep learning architectures with copying modules that are especially useful for conditional image generation tasks like pose morphing. Our contributions are (i) algorithm and code1of the novel GPA layer, (ii) a novel deep attention-copying architecture, and (iii) new state-of-the art experimental results in human pose morphing generation benchmarks.

</p>
</details>

<details><summary><b>Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.03685">arxiv:2107.03685</a>
&#x1F4C8; 4 <br>
<p>Nicolò Botteghi, Luuk Grefte, Mannes Poel, Beril Sirmacek, Christoph Brune, Edwin Dertien, Stefano Stramigioli</p></summary>
<p>

**Abstract:** Inspection and maintenance are two crucial aspects of industrial pipeline plants. While robotics has made tremendous progress in the mechanic design of in-pipe inspection robots, the autonomous control of such robots is still a big open challenge due to the high number of actuators and the complex manoeuvres required. To address this problem, we investigate the usage of Deep Reinforcement Learning for achieving autonomous navigation of in-pipe robots in pipeline networks with complex topologies. Moreover, we introduce a hierarchical policy decomposition based on Hierarchical Reinforcement Learning to learn robust high-level navigation skills. We show that the hierarchical structure introduced in the policy is fundamental for solving the navigation task through pipes and necessary for achieving navigation performances superior to human-level control.

</p>
</details>

<details><summary><b>Generalization Error of GAN from the Discriminator's Perspective</b>
<a href="https://arxiv.org/abs/2107.03633">arxiv:2107.03633</a>
&#x1F4C8; 4 <br>
<p>Hongkang Yang, Weinan E</p></summary>
<p>

**Abstract:** The generative adversarial network (GAN) is a well-known model for learning high-dimensional distributions, but the mechanism for its generalization ability is not understood. In particular, GAN is vulnerable to the memorization phenomenon, the eventual convergence to the empirical distribution. We consider a simplified GAN model with the generator replaced by a density, and analyze how the discriminator contributes to generalization. We show that with early stopping, the generalization error measured by Wasserstein metric escapes from the curse of dimensionality, despite that in the long term, memorization is inevitable. In addition, we present a hardness of learning result for WGAN.

</p>
</details>

<details><summary><b>End-to-end Malaria Diagnosis and 3D Cell Rendering with Deep Learning</b>
<a href="https://arxiv.org/abs/2108.04220">arxiv:2108.04220</a>
&#x1F4C8; 3 <br>
<p>Vignav Ramesh</p></summary>
<p>

**Abstract:** Malaria is a parasitic infection that poses a significant burden on global health. It kills one child every 30 seconds and over one million people annually. If diagnosed in a timely manner, however, most people can be effectively treated with antimalarial therapy. Several deaths due to malaria are byproducts of disparities in the social determinants of health; the current gold standard for diagnosing malaria requires microscopes, reagents, and other equipment that most patients of low socioeconomic brackets do not have access to. In this paper, we propose a convolutional neural network (CNN) architecture that allows for rapid automated diagnosis of malaria (achieving a high classification accuracy of 98%), as well as a deep neural network (DNN) based three-dimensional (3D) modeling algorithm that renders 3D models of parasitic cells in augmented reality (AR). This creates an opportunity to optimize the current workflow for malaria diagnosis and demonstrates potential for deep learning models to improve telemedicine practices and patient health literacy on a global scale.

</p>
</details>

<details><summary><b>Classifying Component Function in Product Assemblies with Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2107.07042">arxiv:2107.07042</a>
&#x1F4C8; 3 <br>
<p>Vincenzo Ferrero, Kaveh Hassani, Daniele Grandi, Bryony DuPont</p></summary>
<p>

**Abstract:** Function is defined as the ensemble of tasks that enable the product to complete the designed purpose. Functional tools, such as functional modeling, offer decision guidance in the early phase of product design, where explicit design decisions are yet to be made. Function-based design data is often sparse and grounded in individual interpretation. As such, function-based design tools can benefit from automatic function classification to increase data fidelity and provide function representation models that enable function-based intelligent design agents. Function-based design data is commonly stored in manually generated design repositories. These design repositories are a collection of expert knowledge and interpretations of function in product design bounded by function-flow and component taxonomies. In this work, we represent a structured taxonomy-based design repository as assembly-flow graphs, then leverage a graph neural network (GNN) model to perform automatic function classification. We support automated function classification by learning from repository data to establish the ground truth of component function assignment. Experimental results show that our GNN model achieves a micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and 0.783 for tier 3 (specific) functions. Given the imbalance of data features, the results are encouraging. Our efforts in this paper can be a starting point for more sophisticated applications in knowledge-based CAD systems and Design-for-X consideration in function-based design.

</p>
</details>

<details><summary><b>Computer-Aided Diagnosis of Low Grade Endometrial Stromal Sarcoma (LGESS)</b>
<a href="https://arxiv.org/abs/2107.05426">arxiv:2107.05426</a>
&#x1F4C8; 3 <br>
<p>Xinxin Yang, Mark Stamp</p></summary>
<p>

**Abstract:** Low grade endometrial stromal sarcoma (LGESS) is rare form of cancer, accounting for about 0.2% of all uterine cancer cases. Approximately 75% of LGESS patients are initially misdiagnosed with leiomyoma, which is a type of benign tumor, also known as fibroids. In this research, uterine tissue biopsy images of potential LGESS patients are preprocessed using segmentation and staining normalization algorithms. A variety of classic machine learning and leading deep learning models are then applied to classify tissue images as either benign or cancerous. For the classic techniques considered, the highest classification accuracy we attain is about 0.85, while our best deep learning model achieves an accuracy of approximately 0.87. These results indicate that properly trained learning algorithms can play a useful role in the diagnosis of LGESS.

</p>
</details>

<details><summary><b>On lattice-free boosted MMI training of HMM and CTC-based full-context ASR models</b>
<a href="https://arxiv.org/abs/2107.04154">arxiv:2107.04154</a>
&#x1F4C8; 3 <br>
<p>Xiaohui Zhang, Vimal Manohar, David Zhang, Frank Zhang, Yangyang Shi, Nayan Singhal, Julian Chan, Fuchun Peng, Yatharth Saraf, Mike Seltzer</p></summary>
<p>

**Abstract:** Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.

</p>
</details>

<details><summary><b>Multitask Multi-database Emotion Recognition</b>
<a href="https://arxiv.org/abs/2107.04127">arxiv:2107.04127</a>
&#x1F4C8; 3 <br>
<p>Manh Tu Vu, Marie Beurton-Aimar</p></summary>
<p>

**Abstract:** In this work, we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning model on multi-databases to perform two tasks: seven basic facial expressions prediction and valence-arousal estimation. Since these databases do not contains labels for all the two tasks, we have applied the distillation knowledge technique to train two networks: one teacher and one student model. The student model will be trained using both ground truth labels and soft labels derived from the pretrained teacher model. During the training, we add one more task, which is the combination of the two mentioned tasks, for better exploiting inter-task correlations. We also exploit the sharing videos between the two tasks of the AffWild2 database that is used in the competition, to further improve the performance of the network. Experiment results shows that the network have achieved promising results on the validation set of the AffWild2 database. Code and pretrained model are publicly available at https://github.com/glmanhtu/multitask-abaw-2021

</p>
</details>

<details><summary><b>Many Objective Bayesian Optimization</b>
<a href="https://arxiv.org/abs/2107.04126">arxiv:2107.04126</a>
&#x1F4C8; 3 <br>
<p>Lucia Asencio Martín, Eduardo C. Garrido-Merchán</p></summary>
<p>

**Abstract:** Some real problems require the evaluation of expensive and noisy objective functions. Moreover, the analytical expression of these objective functions may be unknown. These functions are known as black-boxes, for example, estimating the generalization error of a machine learning algorithm and computing its prediction time in terms of its hyper-parameters. Multi-objective Bayesian optimization (MOBO) is a set of methods that has been successfully applied for the simultaneous optimization of black-boxes. Concretely, BO methods rely on a probabilistic model of the objective functions, typically a Gaussian process. This model generates a predictive distribution of the objectives. However, MOBO methods have problems when the number of objectives in a multi-objective optimization problem are 3 or more, which is the many objective setting. In particular, the BO process is more costly as more objectives are considered, computing the quality of the solution via the hyper-volume is also more costly and, most importantly, we have to evaluate every objective function, wasting expensive computational, economic or other resources. However, as more objectives are involved in the optimization problem, it is highly probable that some of them are redundant and not add information about the problem solution. A measure that represents how similar are GP predictive distributions is proposed. We also propose a many objective Bayesian optimization algorithm that uses this metric to determine whether two objectives are redundant. The algorithm stops evaluating one of them if the similarity is found, saving resources and not hurting the performance of the multi-objective BO algorithm. We show empirical evidence in a set of toy, synthetic, benchmark and real experiments that GPs predictive distributions of the effectiveness of the metric and the algorithm.

</p>
</details>

<details><summary><b>Ensembles of Randomized NNs for Pattern-based Time Series Forecasting</b>
<a href="https://arxiv.org/abs/2107.04091">arxiv:2107.04091</a>
&#x1F4C8; 3 <br>
<p>Grzegorz Dudek, Paweł Pełka</p></summary>
<p>

**Abstract:** In this work, we propose an ensemble forecasting approach based on randomized neural networks. Improved randomized learning streamlines the fitting abilities of individual learners by generating network parameters in accordance with the data and target function features. A pattern-based representation of time series makes the proposed approach suitable for forecasting time series with multiple seasonality. We propose six strategies for controlling the diversity of ensemble members. Case studies conducted on four real-world forecasting problems verified the effectiveness and superior performance of the proposed ensemble forecasting approach. It outperformed statistical models as well as state-of-the-art machine learning models in terms of forecasting accuracy. The proposed approach has several advantages: fast and easy training, simple architecture, ease of implementation, high accuracy and the ability to deal with nonstationarity and multiple seasonality in time series.

</p>
</details>

<details><summary><b>Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images</b>
<a href="https://arxiv.org/abs/2107.04062">arxiv:2107.04062</a>
&#x1F4C8; 3 <br>
<p>Nico Zettler, Andre Mastmeyer</p></summary>
<p>

**Abstract:** A two-step concept for 3D segmentation on 5 abdominal organs inside volumetric CT images is presented. First each relevant organ's volume of interest is extracted as bounding box. The extracted volume acts as input for a second stage, wherein two compared U-Nets with different architectural dimensions re-construct an organ segmentation as label mask. In this work, we focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results indicate Dice improvements of about 6\% at maximum. In this study to our surprise, liver and kidneys for instance were tackled significantly better using the faster and GPU-memory saving 2D U-Nets. For other abdominal key organs, there were no significant differences, but we observe highly significant advantages for the 2D U-Net in terms of GPU computational efforts for all organs under study.

</p>
</details>

<details><summary><b>3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image</b>
<a href="https://arxiv.org/abs/2107.04055">arxiv:2107.04055</a>
&#x1F4C8; 3 <br>
<p>Haibo Qi, Yuhan Wang, Xinyu Liu</p></summary>
<p>

**Abstract:** In this paper, a 3D-RegNet-based neural network is proposed for diagnosing the physical condition of patients with coronavirus (Covid-19) infection. In the application of clinical medicine, lung CT images are utilized by practitioners to determine whether a patient is infected with coronavirus. However, there are some laybacks can be considered regarding to this diagnostic method, such as time consuming and low accuracy. As a relatively large organ of human body, important spatial features would be lost if the lungs were diagnosed utilizing two dimensional slice image. Therefore, in this paper, a deep learning model with 3D image was designed. The 3D image as input data was comprised of two-dimensional pulmonary image sequence and from which relevant coronavirus infection 3D features were extracted and classified. The results show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC value of 0.8807 have been achieved.

</p>
</details>

<details><summary><b>Inspiration through Observation: Demonstrating the Influence of Automatically Generated Text on Creative Writing</b>
<a href="https://arxiv.org/abs/2107.04007">arxiv:2107.04007</a>
&#x1F4C8; 3 <br>
<p>Melissa Roemmele</p></summary>
<p>

**Abstract:** Getting machines to generate text perceived as creative is a long-pursued goal. A growing body of research directs this goal towards augmenting the creative writing abilities of human authors. In this paper, we pursue this objective by analyzing how observing examples of automatically generated text influences writing. In particular, we examine a task referred to as sentence infilling, which involves transforming a list of words into a complete sentence. We emphasize "storiability" as a desirable feature of the resulting sentences, where "storiable" sentences are those that suggest a story a reader would be curious to hear about. Both humans and an automated system (based on a neural language model) performed this sentence infilling task. In one setting, people wrote sentences on their own; in a different setting, people observed the sentences produced by the model while writing their own sentences. Readers then assigned storiability preferences to the resulting sentences in a subsequent evaluation. We find that human-authored sentences were judged as more storiable when authors observed the generated examples, and that storiability increased as authors derived more semantic content from the examples. This result gives evidence of an "inspiration through observation" paradigm for human-computer collaborative writing, through which human writing can be enhanced by text generation models without directly copying their output.

</p>
</details>

<details><summary><b>Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification</b>
<a href="https://arxiv.org/abs/2107.03920">arxiv:2107.03920</a>
&#x1F4C8; 3 <br>
<p>Niccolò Dalmasso, David Zhao, Rafael Izbicki, Ann B. Lee</p></summary>
<p>

**Abstract:** Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, outside the asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce reliable measures of uncertainty. This paper presents a statistical framework for LFI that unifies classical statistics with modern machine learning to: (1) efficiently construct frequentist confidence sets and hypothesis tests with finite-sample guarantees of nominal coverage (type I error control) and power; (2) provide practical diagnostics for assessing empirical coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that estimates a test statistic, like the likelihood ratio, can be plugged into our framework to create valid confidence sets and compute diagnostics, without costly Monte Carlo samples at fixed parameter settings. In this work, we specifically study the power of two test statistics (ACORE and BFF), which, respectively, maximize versus integrate an odds function over the parameter space. Our study offers multifaceted perspectives on the challenges in LF2I.

</p>
</details>

<details><summary><b>Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation</b>
<a href="https://arxiv.org/abs/2107.03846">arxiv:2107.03846</a>
&#x1F4C8; 3 <br>
<p>Lucas Fidon, Michael Aertsen, Doaa Emam, Nada Mufti, Frédéric Guffens, Thomas Deprest, Philippe Demaerel, Anna L. David, Andrew Melbourne, Sébastien Ourselin, Jan Deprest, Tom Vercauteren</p></summary>
<p>

**Abstract:** Deep neural networks have increased the accuracy of automatic segmentation, however, their accuracy depends on the availability of a large number of fully segmented images. Methods to train deep neural networks using images for which some, but not all, regions of interest are segmented are necessary to make better use of partially annotated datasets. In this paper, we propose the first axiomatic definition of label-set loss functions that are the loss functions that can handle partially segmented images. We prove that there is one and only one method to convert a classical loss function for fully segmented images into a proper label-set loss function. Our theory also allows us to define the leaf-Dice loss, a label-set generalization of the Dice loss particularly suited for partial supervision with only missing labels. Using the leaf-Dice loss, we set a new state of the art in partially supervised learning for fetal brain 3D MRI segmentation. We achieve a deep neural network able to segment white matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter, deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of anatomically normal fetuses or with open spina bifida. Our implementation of the proposed label-set loss functions is available at https://github.com/LucasFidon/label-set-loss-functions

</p>
</details>

<details><summary><b>Analytically Tractable Hidden-States Inference in Bayesian Neural Networks</b>
<a href="https://arxiv.org/abs/2107.03759">arxiv:2107.03759</a>
&#x1F4C8; 3 <br>
<p>Luong-Ha Nguyen, James-A. Goulet</p></summary>
<p>

**Abstract:** With few exceptions, neural networks have been relying on backpropagation and gradient descent as the inference engine in order to learn the model parameters, because the closed-form Bayesian inference for neural networks has been considered to be intractable. In this paper, we show how we can leverage the tractable approximate Gaussian inference's (TAGI) capabilities to infer hidden states, rather than only using it for inferring the network's parameters. One novel aspect it allows is to infer hidden states through the imposition of constraints designed to achieve specific objectives, as illustrated through three examples: (1) the generation of adversarial-attack examples, (2) the usage of a neural network as a black-box optimization method, and (3) the application of inference on continuous-action reinforcement learning. These applications showcase how tasks that were previously reserved to gradient-based optimization approaches can now be approached with analytically tractable inference

</p>
</details>

<details><summary><b>Probabilistic Time Series Forecasting with Implicit Quantile Networks</b>
<a href="https://arxiv.org/abs/2107.03743">arxiv:2107.03743</a>
&#x1F4C8; 3 <br>
<p>Adèle Gouttes, Kashif Rasul, Mateusz Koren, Johannes Stephan, Tofigh Naghibi</p></summary>
<p>

**Abstract:** Here, we propose a general method for probabilistic time series forecasting. We combine an autoregressive recurrent neural network to model temporal dynamics with Implicit Quantile Networks to learn a large class of distributions over a time-series target. When compared to other probabilistic neural forecasting models on real- and simulated data, our approach is favorable in terms of point-wise prediction accuracy as well as on estimating the underlying temporal distribution.

</p>
</details>

<details><summary><b>Adaptation of Quadruped Robot Locomotion with Meta-Learning</b>
<a href="https://arxiv.org/abs/2107.03741">arxiv:2107.03741</a>
&#x1F4C8; 3 <br>
<p>Arsen Kuzhamuratov, Dmitry Sorokin, Alexander Ulanov, A. I. Lvovsky</p></summary>
<p>

**Abstract:** Animals have remarkable abilities to adapt locomotion to different terrains and tasks. However, robots trained by means of reinforcement learning are typically able to solve only a single task and a transferred policy is usually inferior to that trained from scratch. In this work, we demonstrate that meta-reinforcement learning can be used to successfully train a robot capable to solve a wide range of locomotion tasks. The performance of the meta-trained robot is similar to that of a robot that is trained on a single task.

</p>
</details>

<details><summary><b>Parameterization of Forced Isotropic Turbulent Flow using Autoencoders and Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2107.06264">arxiv:2107.06264</a>
&#x1F4C8; 2 <br>
<p> Kanishk, Tanishk Nandal, Prince Tyagi, Raj Kumar Singh</p></summary>
<p>

**Abstract:** Autoencoders and generative neural network models have recently gained popularity in fluid mechanics due to their spontaneity and low processing time instead of high fidelity CFD simulations. Auto encoders are used as model order reduction tools in applications of fluid mechanics by compressing input high-dimensional data using an encoder to map the input space into a lower-dimensional latent space. Whereas, generative models such as Variational Auto-encoders (VAEs) and Generative Adversarial Networks (GANs) are proving to be effective in generating solutions to chaotic models with high 'randomness' such as turbulent flows. In this study, forced isotropic turbulence flow is generated by parameterizing into some basic statistical characteristics. The models trained on pre-simulated data from dependencies on these characteristics and the flow generation is then affected by varying these parameters. The latent vectors pushed along the generator models like the decoders and generators contain independent entries which can be used to create different outputs with similar properties. The use of neural network-based architecture removes the need for dependency on the classical mesh-based Navier-Stoke equation estimation which is prominent in many CFD softwares.

</p>
</details>

<details><summary><b>Probabilistic Trajectory Prediction with Structural Constraints</b>
<a href="https://arxiv.org/abs/2107.04193">arxiv:2107.04193</a>
&#x1F4C8; 2 <br>
<p>Weiming Zhi, Lionel Ott, Fabio Ramos</p></summary>
<p>

**Abstract:** This work addresses the problem of predicting the motion trajectories of dynamic objects in the environment. Recent advances in predicting motion patterns often rely on machine learning techniques to extrapolate motion patterns from observed trajectories, with no mechanism to directly incorporate known rules. We propose a novel framework, which combines probabilistic learning and constrained trajectory optimisation. The learning component of our framework provides a distribution over future motion trajectories conditioned on observed past coordinates. This distribution is then used as a prior to a constrained optimisation problem which enforces chance constraints on the trajectory distribution. This results in constraint-compliant trajectory distributions which closely resemble the prior. In particular, we focus our investigation on collision constraints, such that extrapolated future trajectory distributions conform to the environment structure. We empirically demonstrate on real-world and simulated datasets the ability of our framework to learn complex probabilistic motion trajectories for motion data, while directly enforcing constraints to improve generalisability, producing more robust and higher quality trajectory distributions.

</p>
</details>

<details><summary><b>Machine Learning for Stuttering Identification: Review, Challenges & Future Directions</b>
<a href="https://arxiv.org/abs/2107.04057">arxiv:2107.04057</a>
&#x1F4C8; 2 <br>
<p>Shakeel Ahmad Sheikh, Md Sahidullah, Fabrice Hirsch, Slim Ouni</p></summary>
<p>

**Abstract:** Stuttering is a speech disorder during which the flow of speech is interrupted by involuntary pauses and repetition of sounds. Stuttering identification is an interesting interdisciplinary domain research problem which involves pathology, psychology, acoustics, and signal processing that makes it hard and complicated to detect. Recent developments in machine and deep learning have dramatically revolutionized speech domain, however minimal attention has been given to stuttering identification. This work fills the gap by trying to bring researchers together from interdisciplinary fields. In this paper, we review comprehensively acoustic features, statistical and deep learning based stuttering/disfluency classification methods. We also present several challenges and possible future directions.

</p>
</details>

<details><summary><b>A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware</b>
<a href="https://arxiv.org/abs/2107.03992">arxiv:2107.03992</a>
&#x1F4C8; 2 <br>
<p>Philipp Plank, Arjun Rao, Andreas Wild, Wolfgang Maass</p></summary>
<p>

**Abstract:** Spike-based neuromorphic hardware holds the promise to provide more energy efficient implementations of Deep Neural Networks (DNNs) than standard hardware such as GPUs. But this requires to understand how DNNs can be emulated in an event-based sparse firing regime, since otherwise the energy-advantage gets lost. In particular, DNNs that solve sequence processing tasks typically employ Long Short-Term Memory (LSTM) units that are hard to emulate with few spikes. We show that a facet of many biological neurons, slow after-hyperpolarizing (AHP) currents after each spike, provides an efficient solution. AHP-currents can easily be implemented in neuromorphic hardware that supports multi-compartment neuron models, such as Intel's Loihi chip. Filter approximation theory explains why AHP-neurons can emulate the function of LSTM units. This yields a highly energy-efficient approach to time series classification. Furthermore it provides the basis for implementing with very sparse firing an important class of large DNNs that extract relations between words and sentences in a text in order to answer questions about the text.

</p>
</details>

<details><summary><b>Privacy Concerns in Chatbot Interactions: When to Trust and When to Worry</b>
<a href="https://arxiv.org/abs/2107.03959">arxiv:2107.03959</a>
&#x1F4C8; 2 <br>
<p>Rahime Belen Saglam, Jason R. C. Nurse, Duncan Hodges</p></summary>
<p>

**Abstract:** Through advances in their conversational abilities, chatbots have started to request and process an increasing variety of sensitive personal information. The accurate disclosure of sensitive information is essential where it is used to provide advice and support to users in the healthcare and finance sectors. In this study, we explore users' concerns regarding factors associated with the use of sensitive data by chatbot providers. We surveyed a representative sample of 491 British citizens. Our results show that the user concerns focus on deleting personal information and concerns about their data's inappropriate use. We also identified that individuals were concerned about losing control over their data after a conversation with conversational agents. We found no effect from a user's gender or education but did find an effect from the user's age, with those over 45 being more concerned than those under 45. We also considered the factors that engender trust in a chatbot. Our respondents' primary focus was on the chatbot's technical elements, with factors such as the response quality being identified as the most critical factor. We again found no effect from the user's gender or education level; however, when we considered some social factors (e.g. avatars or perceived 'friendliness'), we found those under 45 years old rated these as more important than those over 45. The paper concludes with a discussion of these results within the context of designing inclusive, digital systems that support a wide range of users.

</p>
</details>

<details><summary><b>Manifold Hypothesis in Data Analysis: Double Geometrically-Probabilistic Approach to Manifold Dimension Estimation</b>
<a href="https://arxiv.org/abs/2107.03903">arxiv:2107.03903</a>
&#x1F4C8; 2 <br>
<p>Alexander Ivanov, Gleb Nosovskiy, Alexey Chekunov, Denis Fedoseev, Vladislav Kibkalo, Mikhail Nikulin, Fedor Popelenskiy, Stepan Komkov, Ivan Mazurenko, Aleksandr Petiushko</p></summary>
<p>

**Abstract:** Manifold hypothesis states that data points in high-dimensional space actually lie in close vicinity of a manifold of much lower dimension. In many cases this hypothesis was empirically verified and used to enhance unsupervised and semi-supervised learning. Here we present new approach to manifold hypothesis checking and underlying manifold dimension estimation. In order to do it we use two very different methods simultaneously - one geometric, another probabilistic - and check whether they give the same result. Our geometrical method is a modification for sparse data of a well-known box-counting algorithm for Minkowski dimension calculation. The probabilistic method is new. Although it exploits standard nearest neighborhood distance, it is different from methods which were previously used in such situations. This method is robust, fast and includes special preliminary data transformation. Experiments on real datasets show that the suggested approach based on two methods combination is powerful and effective.

</p>
</details>

<details><summary><b>Bootstrapping Generalization of Process Models Discovered From Event Data</b>
<a href="https://arxiv.org/abs/2107.03876">arxiv:2107.03876</a>
&#x1F4C8; 2 <br>
<p>Artem Polyvyanyy, Alistair Moffat, Luciano García-Bañuelos</p></summary>
<p>

**Abstract:** Process mining studies ways to derive value from process executions recorded in event logs of IT-systems, with process discovery the task of inferring a process model for an event log emitted by some unknown system. One quality criterion for discovered process models is generalization. Generalization seeks to quantify how well the discovered model describes future executions of the system, and is perhaps the least understood quality criterion in process mining. The lack of understanding is primarily a consequence of generalization seeking to measure properties over the entire future behavior of the system, when the only available sample of behavior is that provided by the event log itself. In this paper, we draw inspiration from computational statistics, and employ a bootstrap approach to estimate properties of a population based on a sample. Specifically, we define an estimator of the model's generalization based on the event log it was discovered from, and then use bootstrapping to measure the generalization of the model with respect to the system, and its statistical significance. Experiments demonstrate the feasibility of the approach in industrial settings.

</p>
</details>

<details><summary><b>Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review</b>
<a href="https://arxiv.org/abs/2107.03869">arxiv:2107.03869</a>
&#x1F4C8; 2 <br>
<p>Ahmad Kamal Bin Mohd Nor, Srinivasa Rao Pedapait, Masdi Muhammad</p></summary>
<p>

**Abstract:** A state-of-the-art systematic review on XAI applied to Prognostic and Health Management (PHM) of industrial asset is presented. This work provides an overview of the general trend of XAI in PHM, answers the question of accuracy versus explainability, the extent of human involvement, the explanation assessment and uncertainty quantification in PHM-XAI domain. Research articles associated with the subject, from 2015 to 2021 were selected from five known databases following PRISMA guidelines. Data was then extracted from the selected articles and examined. Several findings were synthesized. Firstly, while the discipline is still young, the analysis indicated the growing acceptance of XAI in PHM domain. Secondly, XAI functions as a double edge sword, where it is assimilated as a tool to execute PHM tasks as well as a mean of explanation, particularly in diagnostic and anomaly detection activities, implying a real need for XAI in PHM. Thirdly, the review showed that PHM-XAI papers produce either good or excellent result in general, suggesting that PHM performance is unaffected by XAI. Fourthly, human role, evaluation metrics and uncertainty management are areas requiring further attention by the PHM community. Adequate assessment metrics to cater for PHM need are urgently needed.Finally, most case study featured on the accepted articles are based on real, industrial data, indicating that the available PHM-XAI blends are fit to solve complex,real-world challenges, increasing the confidence in AI adoption in the industry.

</p>
</details>

<details><summary><b>Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models</b>
<a href="https://arxiv.org/abs/2107.03863">arxiv:2107.03863</a>
&#x1F4C8; 2 <br>
<p>Felix L. Rios, Giusi Moffa, Jack Kuipers</p></summary>
<p>

**Abstract:** Describing the relationship between the variables in a study domain and modelling the data generating mechanism is a fundamental problem in many empirical sciences. Probabilistic graphical models are one common approach to tackle the problem. Learning the graphical structure is computationally challenging and a fervent area of current research with a plethora of algorithms being developed. To facilitate the benchmarking of different methods, we present a novel automated workflow, called benchpress for producing scalable, reproducible, and platform-independent benchmarks of structure learning algorithms for probabilistic graphical models. Benchpress is interfaced via a simple JSON-file, which makes it accessible for all users, while the code is designed in a fully modular fashion to enable researchers to contribute additional methodologies. Benchpress currently provides an interface to a large number of state-of-the-art algorithms from libraries such as BDgraph, BiDAG, bnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as a variety of methods for data generating models and performance evaluation. Alongside user-defined models and randomly generated datasets, the software tool also includes a number of standard datasets and graphical models from the literature, which may be included in a benchmarking workflow. We demonstrate the applicability of this workflow for learning Bayesian networks in four typical data scenarios. The source code and documentation is publicly available from http://github.com/felixleopoldo/benchpress.

</p>
</details>

<details><summary><b>SSSE: Efficiently Erasing Samples from Trained Machine Learning Models</b>
<a href="https://arxiv.org/abs/2107.03860">arxiv:2107.03860</a>
&#x1F4C8; 2 <br>
<p>Alexandra Peste, Dan Alistarh, Christoph H. Lampert</p></summary>
<p>

**Abstract:** The availability of large amounts of user-provided data has been key to the success of machine learning for many real-world tasks. Recently, an increasing awareness has emerged that users should be given more control about how their data is used. In particular, users should have the right to prohibit the use of their data for training machine learning systems, and to have it erased from already trained systems. While several sample erasure methods have been proposed, all of them have drawbacks which have prevented them from gaining widespread adoption. Most methods are either only applicable to very specific families of models, sacrifice too much of the original model's accuracy, or they have prohibitive memory or computational requirements. In this paper, we propose an efficient and effective algorithm, SSSE, for samples erasure, that is applicable to a wide class of machine learning models. From a second-order analysis of the model's loss landscape we derive a closed-form update step of the model parameters that only requires access to the data to be erased, not to the original training set. Experiments on three datasets, CelebFaces attributes (CelebA), Animals with Attributes 2 (AwA2) and CIFAR10, show that in certain cases SSSE can erase samples almost as well as the optimal, yet impractical, gold standard of training a new model from scratch with only the permitted data.

</p>
</details>

<details><summary><b>Consistency of the Maximal Information Coefficient Estimator</b>
<a href="https://arxiv.org/abs/2107.03836">arxiv:2107.03836</a>
&#x1F4C8; 2 <br>
<p>John Lazarsfeld, Aaron Johnson</p></summary>
<p>

**Abstract:** The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is a statistic for measuring dependence between variable pairs in large datasets. In this note, we prove that MIC is a consistent estimator of the corresponding population statistic MIC$_*$. This corrects an error in an argument of Reshef et al. (JMLR, 2016), which we describe.

</p>
</details>

<details><summary><b>Federated Learning as a Mean-Field Game</b>
<a href="https://arxiv.org/abs/2107.03770">arxiv:2107.03770</a>
&#x1F4C8; 2 <br>
<p>Arash Mehrjou</p></summary>
<p>

**Abstract:** We establish a connection between federated learning, a concept from machine learning, and mean-field games, a concept from game theory and control theory. In this analogy, the local federated learners are considered as the players and the aggregation of the gradients in a central server is the mean-field effect. We present federated learning as a differential game and discuss the properties of the equilibrium of this game. We hope this novel view to federated learning brings together researchers from these two distinct areas to work on fundamental problems of large-scale distributed and privacy-preserving learning algorithms.

</p>
</details>

<details><summary><b>Image Resolution Susceptibility of Face Recognition Models</b>
<a href="https://arxiv.org/abs/2107.03769">arxiv:2107.03769</a>
&#x1F4C8; 2 <br>
<p>Martin Knoche, Stefan Hörmann, Gerhard Rigoll</p></summary>
<p>

**Abstract:** Face recognition approaches often rely on equal image resolution for verification faces on two images. However, in practical applications, those image resolutions are usually not in the same range due to different image capture mechanisms or sources. In this work, we first analyze the impact of image resolutions on the face verification performance with a state-of-the-art face recognition model. For images, synthetically reduced to $5\, \times 5\, \mathrm{px}$ resolution, the verification performance drops from $99.23\%$ increasingly down to almost $55\%$. Especially, for cross-resolution image pairs (one high- and one low-resolution image), the verification accuracy decreases even further. We investigate this behavior more in-depth by looking at the feature distances for every 2-image test pair. To tackle this problem, we propose the following two methods: 1) Train a state-of-the-art face-recognition model straightforward with $50\%$ low-resolution images directly within each batch. \\ 2) Train a siamese-network structure and adding a cosine distance feature loss between high- and low-resolution features. Both methods show an improvement for cross-resolution scenarios and can increase the accuracy at very low resolution to approximately $70\%$. However, a disadvantage is that a specific model needs to be trained for every resolution-pair ...

</p>
</details>

<details><summary><b>Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning</b>
<a href="https://arxiv.org/abs/2107.03751">arxiv:2107.03751</a>
&#x1F4C8; 2 <br>
<p>Luis Lucas, David Tomas, Jose Garcia-Rodriguez</p></summary>
<p>

**Abstract:** One of the main issues related to unsupervised machine learning is the cost of processing and extracting useful information from large datasets. In this work, we propose a classifier ensemble based on the transferable learning capabilities of the CLIP neural network architecture in multimodal environments (image and text) from social media. For this purpose, we used the InstaNY100K dataset and proposed a validation approach based on sampling techniques. Our experiments, based on image classification tasks according to the labels of the Places dataset, are performed by first considering only the visual part, and then adding the associated texts as support. The results obtained demonstrated that trained neural networks such as CLIP can be successfully applied to image classification with little fine-tuning, and considering the associated texts to the images can help to improve the accuracy depending on the goal. The results demonstrated what seems to be a promising research direction.

</p>
</details>

<details><summary><b>Encoding Domain Information with Sparse Priors for Inferring Explainable Latent Variables</b>
<a href="https://arxiv.org/abs/2107.03730">arxiv:2107.03730</a>
&#x1F4C8; 2 <br>
<p>Arber Qoku, Florian Buettner</p></summary>
<p>

**Abstract:** Latent variable models are powerful statistical tools that can uncover relevant variation between patients or cells, by inferring unobserved hidden states from observable high-dimensional data. A major shortcoming of current methods, however, is their inability to learn sparse and interpretable hidden states. Additionally, in settings where partial knowledge on the latent structure of the data is readily available, a statistically sound integration of prior information into current methods is challenging. To address these issues, we propose spex-LVM, a factorial latent variable model with sparse priors to encourage the inference of explainable factors driven by domain-relevant information. spex-LVM utilizes existing knowledge of curated biomedical pathways to automatically assign annotated attributes to latent factors, yielding interpretable results tailored to the corresponding domain of interest. Evaluations on simulated and real single-cell RNA-seq datasets demonstrate that our model robustly identifies relevant structure in an inherently explainable manner, distinguishes technical noise from sources of biomedical variation, and provides dataset-specific adaptations of existing pathway annotations. Implementation is available at https://github.com/MLO-lab/spexlvm.

</p>
</details>

<details><summary><b>Complete Scanning Application Using OpenCv</b>
<a href="https://arxiv.org/abs/2107.03700">arxiv:2107.03700</a>
&#x1F4C8; 2 <br>
<p>Ayushe Gangal, Peeyush Kumar, Sunita Kumari</p></summary>
<p>

**Abstract:** In the following paper, we have combined the various basic functionalities provided by the NumPy library and OpenCv library, which is an open source for Computer Vision applications, like conversion of colored images to grayscale, calculating threshold, finding contours and using those contour points to take perspective transform of the image inputted by the user, using Python version 3.7. Additional features include cropping, rotating and saving as well. All these functions and features, when implemented step by step, results in a complete scanning application. The applied procedure involves the following steps: Finding contours, applying Perspective transform and brightening the image, Adaptive Thresholding and applying filters for noise cancellation, and Rotation features and perspective transform for a special cropping algorithm. The described technique is implemented on various samples.

</p>
</details>

<details><summary><b>MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs</b>
<a href="https://arxiv.org/abs/2107.03673">arxiv:2107.03673</a>
&#x1F4C8; 2 <br>
<p>Lulu Zhang, Tao Luo, Yaoyu Zhang, Zhi-Qin John Xu, Zheng Ma</p></summary>
<p>

**Abstract:** In this paper, we propose a model-operator-data network (MOD-Net) for solving PDEs. A MOD-Net is driven by a model to solve PDEs based on operator representation with regularization from data. In this work, we use a deep neural network to parameterize the Green's function. The empirical risk consists of the mean square of the governing equation, boundary conditions, and a few labels, which are numerically computed by traditional schemes on coarse grid points with cheap computation cost. With only the labeled dataset or only the model constraints, it is insufficient to accurately train a MOD-Net for complicate problems. Intuitively, the labeled dataset works as a regularization in addition to the model constraints. The MOD-Net is much efficient than original neural operator because the MOD-Net also uses the information of governing equation and the boundary conditions of the PDE rather than purely the expensive labels. Since the MOD-Net learns the Green's function of a PDE, it solves a type of PDEs but not a specific case. We numerically show MOD-Net is very efficient in solving Poisson equation and one-dimensional Boltzmann equation. For non-linear PDEs, where the concept of the Green's function does not apply, the non-linear MOD-Net can be similarly used as an ansatz for solving non-linear PDEs.

</p>
</details>

<details><summary><b>Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?</b>
<a href="https://arxiv.org/abs/2107.03651">arxiv:2107.03651</a>
&#x1F4C8; 2 <br>
<p>Daniel Bar-David, Laura Bar-David, Yinon Shapira, Rina Leibu, Dalia Dori, Ronit Schneor, Anath Fischer, Shiri Soudry</p></summary>
<p>

**Abstract:** To explore the clinical validity of elastic deformation of optical coherence tomography (OCT) images for data augmentation in the development of deep-learning model for detection of diabetic macular edema (DME).

</p>
</details>

<details><summary><b>A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing</b>
<a href="https://arxiv.org/abs/2107.03640">arxiv:2107.03640</a>
&#x1F4C8; 2 <br>
<p>Ningyuan Xu, Jiayan Zhuang, Yaojun Wu, Jiangjian Xiao</p></summary>
<p>

**Abstract:** Angular measurements is essential to make a resonable treatment for Hallux valgus (HV), a common forefoot deformity. However, it still depends on manual labeling and measurement, which is time-consuming and sometimes unreliable. Automating this process is a thing of concern. However, it lack of dataset and the keypoints based method which made a great success in pose estimation is not suitable for this field.To solve the problems, we made a dataset and developed an algorithm based on deep learning and linear regression. It shows great fitting ability to the ground truth.

</p>
</details>

<details><summary><b>SpecGrav -- Detection of Gravitational Waves using Deep Learning</b>
<a href="https://arxiv.org/abs/2107.03607">arxiv:2107.03607</a>
&#x1F4C8; 2 <br>
<p>Hrithika Dodia, Himanshu Tandel, Lynette D'Mello</p></summary>
<p>

**Abstract:** Gravitational waves are ripples in the fabric of space-time that travel at the speed of light. The detection of gravitational waves by LIGO is a major breakthrough in the field of astronomy. Deep Learning has revolutionized many industries including health care, finance and education. Deep Learning techniques have also been explored for detection of gravitational waves to overcome the drawbacks of traditional matched filtering method. However, in several researches, the training phase of neural network is very time consuming and hardware devices with large memory are required for the task. In order to reduce the extensive amount of hardware resources and time required in training a neural network for detecting gravitational waves, we made SpecGrav. We use 2D Convolutional Neural Network and spectrograms of gravitational waves embedded in noise to detect gravitational waves from binary black hole merger and binary neutron star merger. The training phase of our neural network was of about just 19 minutes on a 2GB GPU.

</p>
</details>

<details><summary><b>Core-set Sampling for Efficient Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2107.06869">arxiv:2107.06869</a>
&#x1F4C8; 1 <br>
<p>Jae-hun Shim, Kyeongbo Kong, Suk-Ju Kang</p></summary>
<p>

**Abstract:** Neural architecture search (NAS), an important branch of automatic machine learning, has become an effective approach to automate the design of deep learning models. However, the major issue in NAS is how to reduce the large search time imposed by the heavy computational burden. While most recent approaches focus on pruning redundant sets or developing new search methodologies, this paper attempts to formulate the problem based on the data curation manner. Our key strategy is to search the architecture using summarized data distribution, i.e., core-set. Typically, many NAS algorithms separate searching and training stages, and the proposed core-set methodology is only used in search stage, thus their performance degradation can be minimized. In our experiments, we were able to save overall computational time from 30.8 hours to 3.5 hours, 8.8x reduction, on a single RTX 3090 GPU without sacrificing accuracy.

</p>
</details>

<details><summary><b>Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2107.03987">arxiv:2107.03987</a>
&#x1F4C8; 1 <br>
<p>Jianing Wang, Dingjie Su, Yubo Fan, Srijata Chakravorti, Jack H. Noble, Benoit M. Dawant</p></summary>
<p>

**Abstract:** We propose an atlas-based method to segment the intracochlear anatomy (ICA) in the post-implantation CT (Post-CT) images of cochlear implant (CI) recipients that preserves the point-to-point correspondence between the meshes in the atlas and the segmented volumes. To solve this problem, which is challenging because of the strong artifacts produced by the implant, we use a pair of co-trained deep networks that generate dense deformation fields (DDFs) in opposite directions. One network is tasked with registering an atlas image to the Post-CT images and the other network is tasked with registering the Post-CT images to the atlas image. The networks are trained using loss functions based on voxel-wise labels, image content, fiducial registration error, and cycle-consistency constraint. The segmentation of the ICA in the Post-CT images is subsequently obtained by transferring the predefined segmentation meshes of the ICA in the atlas image to the Post-CT images using the corresponding DDFs generated by the trained registration networks. Our model can learn the underlying geometric features of the ICA even though they are obscured by the metal artifacts. We show that our end-to-end network produces results that are comparable to the current state of the art (SOTA) that relies on a two-steps approach that first uses conditional generative adversarial networks to synthesize artifact-free images from the Post-CT images and then uses an active shape model-based method to segment the ICA in the synthetic images. Our method requires a fraction of the time needed by the SOTA, which is important for end-user acceptance.

</p>
</details>

<details><summary><b>A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images</b>
<a href="https://arxiv.org/abs/2107.03904">arxiv:2107.03904</a>
&#x1F4C8; 1 <br>
<p>Shuang Liang</p></summary>
<p>

**Abstract:** In this paper, we present a hybrid deep learning framework named CTNet which combines convolutional neural network and transformer together for the detection of COVID-19 via 3D chest CT images. It consists of a CNN feature extractor module with SE attention to extract sufficient features from CT scans, together with a transformer model to model the discriminative features of the 3D CT scans. Compared to previous works, CTNet provides an effective and efficient method to perform COVID-19 diagnosis via 3D CT scans with data resampling strategy. Advanced results on a large and public benchmarks, COV19-CT-DB database was achieved by the proposed CTNet, over the state-of-the-art baseline approachproposed together with the dataset.

</p>
</details>

<details><summary><b>Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation</b>
<a href="https://arxiv.org/abs/2107.03887">arxiv:2107.03887</a>
&#x1F4C8; 1 <br>
<p>Shuo Wang, Chen Qin, Nicolo Savioli, Chen Chen, Declan O'Regan, Stuart Cook, Yike Guo, Daniel Rueckert, Wenjia Bai</p></summary>
<p>

**Abstract:** In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution segmentation of the heart is essential for detailed description of its anatomical structures. However, due to the limit of acquisition duration and respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in clinical routine. The segmentation of these images provides a low-resolution representation of cardiac anatomy, which may contain artefacts caused by motion. Here we propose a novel latent optimisation framework that jointly performs motion correction and super resolution for cardiac image segmentations. Given a low-resolution segmentation as input, the framework accounts for inter-slice motion in cardiac MR imaging and super-resolves the input into a high-resolution segmentation consistent with input. A multi-view loss is incorporated to leverage information from both short-axis view and long-axis view of cardiac imaging. To solve the inverse problem, iterative optimisation is performed in a latent space, which ensures the anatomical plausibility. This alleviates the need of paired low-resolution and high-resolution images for supervised learning. Experiments on two cardiac MR datasets show that the proposed framework achieves high performance, comparable to state-of-the-art super-resolution approaches and with better cross-domain generalisability and anatomical plausibility.

</p>
</details>

<details><summary><b>Direct detection of plasticity onset through total-strain profile evolution</b>
<a href="https://arxiv.org/abs/2107.03738">arxiv:2107.03738</a>
&#x1F4C8; 1 <br>
<p>Stefanos Papanikolaou, Mikko J. Alava</p></summary>
<p>

**Abstract:** Plastic yielding in solids strongly depends on various conditions, such as temperature and loading rate and indeed, sample-dependent knowledge of yield points in structural materials promotes reliability in mechanical behavior. Commonly, yielding is measured through controlled mechanical testing at small or large scales, in ways that either distinguish elastic (stress) from total deformation measurements, or by identifying plastic slip contributions. In this paper we argue that instead of separate elastic/plastic measurements, yielding can be unraveled through statistical analysis of total strain fluctuations during the evolution sequence of profiles measured in-situ, through digital image correlation. We demonstrate two distinct ways of precisely quantifying yield locations in widely applicable crystal plasticity models, that apply in polycrystalline solids, either by using principal component analysis or discrete wavelet transforms. We test and compare these approaches in synthetic data of polycrystal simulations and a variety of yielding responses, through changes of the applied loading rates and the strain-rate sensitivity exponents.

</p>
</details>

<details><summary><b>Deep Learning Based Image Retrieval in the JPEG Compressed Domain</b>
<a href="https://arxiv.org/abs/2107.03648">arxiv:2107.03648</a>
&#x1F4C8; 1 <br>
<p>Shrikant Temburwar, Bulla Rajesh, Mohammed Javed</p></summary>
<p>

**Abstract:** Content-based image retrieval (CBIR) systems on pixel domain use low-level features, such as colour, texture and shape, to retrieve images. In this context, two types of image representations i.e. local and global image features have been studied in the literature. Extracting these features from pixel images and comparing them with images from the database is very time-consuming. Therefore, in recent years, there has been some effort to accomplish image analysis directly in the compressed domain with lesser computations. Furthermore, most of the images in our daily transactions are stored in the JPEG compressed format. Therefore, it would be ideal if we could retrieve features directly from the partially decoded or compressed data and use them for retrieval. Here, we propose a unified model for image retrieval which takes DCT coefficients as input and efficiently extracts global and local features directly in the JPEG compressed domain for accurate image retrieval. The experimental findings indicate that our proposed model performed similarly to the current DELG model which takes RGB features as an input with reference to mean average precision while having a faster training and retrieval speed.

</p>
</details>

<details><summary><b>A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks</b>
<a href="https://arxiv.org/abs/2107.03645">arxiv:2107.03645</a>
&#x1F4C8; 1 <br>
<p>Leonhard Heindel, Peter Hantschke, Markus Kästner</p></summary>
<p>

**Abstract:** Modern Internet of Things solutions are used in a variety of different areas, ranging from connected vehicles and healthcare to industrial applications. They rely on a large amount of interconnected sensors, which can lead to both technical and economical challenges. Virtual sensing techniques aim to reduce the number of physical sensors in a system by using data from available measurements to estimate additional unknown quantities of interest. Successful model-based solutions include Kalman filters or the combination of finite element models and modal analysis, while many data-driven methods rely on machine learning algorithms. The presented hybrid virtual sensing approach combines Long Short-Term Memory networks with frequency response function models in order to estimate the behavior of non-linear dynamic systems with multiple input and output channels. Network training and prediction make use of short signal subsequences, which are later recombined by applying a windowing technique. The frequency response function model acts as a baseline estimate which perfectly captures linear dynamic systems and is augmented by the non-linear Long Short-Term Memory network following two different hybrid modeling strategies. The approach is tested using a non-linear experimental dataset, which results from measurements of a three-component servo-hydraulic fatigue test bench. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes are used to evaluate the approximation quality of the proposed method. In addition to virtual sensing, the algorithm is also applied to a forward prediction task. Synthetic data are used in a separate study to estimate the prediction quality on datasets of different size.

</p>
</details>

<details><summary><b>Regional Differential Information Entropy for Super-Resolution Image Quality Assessment</b>
<a href="https://arxiv.org/abs/2107.03642">arxiv:2107.03642</a>
&#x1F4C8; 1 <br>
<p>Ningyuan Xu, Jiayan Zhuang, Jiangjian Xiao, Chengbin Peng</p></summary>
<p>

**Abstract:** PSNR and SSIM are the most widely used metrics in super-resolution problems, because they are easy to use and can evaluate the similarities between generated images and reference images. However, single image super-resolution is an ill-posed problem, there are multiple corresponding high-resolution images for the same low-resolution image. The similarities can't totally reflect the restoration effect. The perceptual quality of generated images is also important, but PSNR and SSIM do not reflect perceptual quality well. To solve the problem, we proposed a method called regional differential information entropy to measure both of the similarities and perceptual quality. To overcome the problem that traditional image information entropy can't reflect the structure information, we proposed to measure every region's information entropy with sliding window. Considering that the human visual system is more sensitive to the brightness difference at low brightness, we take $γ$ quantization rather than linear quantization. To accelerate the method, we reorganized the calculation procedure of information entropy with a neural network. Through experiments on our IQA dataset and PIPAL, this paper proves that RDIE can better quantify perceptual quality of images especially GAN-based images.

</p>
</details>

<details><summary><b>Calliope -- A Polyphonic Music Transformer</b>
<a href="https://arxiv.org/abs/2107.05546">arxiv:2107.05546</a>
&#x1F4C8; 0 <br>
<p>Andrea Valenti, Stefano Berti, Davide Bacciu</p></summary>
<p>

**Abstract:** The polyphonic nature of music makes the application of deep learning to music modelling a challenging task. On the other hand, the Transformer architecture seems to be a good fit for this kind of data. In this work, we present Calliope, a novel autoencoder model based on Transformers for the efficient modelling of multi-track sequences of polyphonic music. The experiments show that our model is able to improve the state of the art on musical sequence reconstruction and generation, with remarkably good results especially on long sequences.

</p>
</details>

<details><summary><b>Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics</b>
<a href="https://arxiv.org/abs/2107.04092">arxiv:2107.04092</a>
&#x1F4C8; 0 <br>
<p>Dennis Bautembach, Iason Oikonomidis, Antonis Argyros</p></summary>
<p>

**Abstract:** We present two novel optimizations that accelerate clock-based spiking neural network (SNN) simulators. The first one targets spike timing dependent plasticity (STDP). It combines lazy- with event-driven plasticity and efficiently facilitates the computation of pre- and post-synaptic spikes using bitfields and integer intrinsics. It offers higher bandwidth than event-driven plasticity alone and achieves a 1.5x-2x speedup over our closest competitor. The second optimization targets spike delivery. We partition our graph representation in a way that bounds the number of neurons that need be updated at any given time which allows us to perform said update in shared memory instead of global memory. This is 2x-2.5x faster than our closest competitor. Both optimizations represent the final evolutionary stages of years of iteration on STDP and spike delivery inside "Spice" (/spaIk/), our state of the art SNN simulator. The proposed optimizations are not exclusive to our graph representation or pipeline but are applicable to a multitude of simulator designs. We evaluate our performance on three well-established models and compare ourselves against three other state of the art simulators.

</p>
</details>

<details><summary><b>Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs</b>
<a href="https://arxiv.org/abs/2107.03815">arxiv:2107.03815</a>
&#x1F4C8; 0 <br>
<p>Yikang Zhang, Zhuo Chen, Zhao Zhong</p></summary>
<p>

**Abstract:** In this paper, we propose a Collaboration of Experts (CoE) framework to pool together the expertise of multiple networks towards a common aim. Each expert is an individual network with expertise on a unique portion of the dataset, which enhances the collective capacity. Given a sample, an expert is selected by the delegator, which simultaneously outputs a rough prediction to support early termination. To fulfill this framework, we propose three modules to impel each model to play its role, namely weight generation module (WGM), label generation module (LGM) and variance calculation module (VCM). Our method achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE further achieves the accuracy of 80.0% with only 100M FLOPs for the first time. More importantly, our method is hardware friendly and achieves a 3-6x speedup compared with some existing conditional computation approaches.

</p>
</details>

<details><summary><b>Deep Metric Learning Model for Imbalanced Fault Diagnosis</b>
<a href="https://arxiv.org/abs/2107.03786">arxiv:2107.03786</a>
&#x1F4C8; 0 <br>
<p>Xingtai Gui, Jiyang Zhang</p></summary>
<p>

**Abstract:** Intelligent diagnosis method based on data-driven and deep learning is an attractive and meaningful field in recent years. However, in practical application scenarios, the imbalance of time-series fault is an urgent problem to be solved. This paper proposes a novel deep metric learning model, where imbalanced fault data and a quadruplet data pair design manner are considered. Based on such data pair, a quadruplet loss function which takes into account the inter-class distance and the intra-class data distribution are proposed. This quadruplet loss pays special attention to imbalanced sample pair. The reasonable combination of quadruplet loss and softmax loss function can reduce the impact of imbalance. Experiment results on two open-source datasets show that the proposed method can effectively and robustly improve the performance of imbalanced fault diagnosis.

</p>
</details>


[Next Page](2021/2021-07/2021-07-07.md)
