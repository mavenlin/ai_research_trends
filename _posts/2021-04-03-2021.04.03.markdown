## Summary for 2021-04-03, created on 2021-12-22


<details><summary><b>Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</b>
<a href="https://arxiv.org/abs/2104.01409">arxiv:2104.01409</a>
&#x1F4C8; 10 <br>
<p>Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, Nam Soo Kim</p></summary>
<p>

**Abstract:** Although neural text-to-speech (TTS) models have attracted a lot of attention and succeeded in generating human-like speech, there is still room for improvements to its naturalness and architectural efficiency. In this work, we propose a novel non-autoregressive TTS model, namely Diff-TTS, which achieves highly natural and efficient speech synthesis. Given the text, Diff-TTS exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via diffusion time steps. In order to learn the mel-spectrogram distribution conditioned on the text, we present a likelihood-based optimization method for TTS. Furthermore, to boost up the inference speed, we leverage the accelerated sampling method that allows Diff-TTS to generate raw waveforms much faster without significantly degrading perceptual quality. Through experiments, we verified that Diff-TTS generates 28 times faster than the real-time with a single NVIDIA 2080Ti GPU.

</p>
</details>

<details><summary><b>MMBERT: Multimodal BERT Pretraining for Improved Medical VQA</b>
<a href="https://arxiv.org/abs/2104.01394">arxiv:2104.01394</a>
&#x1F4C8; 9 <br>
<p>Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, U Deva Priyakumar, CV Jawahar</p></summary>
<p>

**Abstract:** Images in the medical domain are fundamentally different from the general domain images. Consequently, it is infeasible to directly employ general domain Visual Question Answering (VQA) models for the medical domain. Additionally, medical images annotation is a costly and time-consuming process. To overcome these limitations, we propose a solution inspired by self-supervised pretraining of Transformer-style architectures for NLP, Vision and Language tasks. Our method involves learning richer medical image and text semantic representations using Masked Language Modeling (MLM) with image features as the pretext task on a large medical image+caption dataset. The proposed solution achieves new state-of-the-art performance on two VQA datasets for radiology images -- VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of previous best solutions. Moreover, our solution provides attention maps which help in model interpretability. The code is available at https://github.com/VirajBagal/MMBERT

</p>
</details>

<details><summary><b>Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing</b>
<a href="https://arxiv.org/abs/2104.01375">arxiv:2104.01375</a>
&#x1F4C8; 8 <br>
<p>Ioannis Kakogeorgiou, Konstantinos Karantzalos</p></summary>
<p>

**Abstract:** Although deep neural networks hold the state-of-the-art in several remote sensing tasks, their black-box operation hinders the understanding of their decisions, concealing any bias and other shortcomings in datasets and model performance. To this end, we have applied explainable artificial intelligence (XAI) methods in remote sensing multi-label classification tasks towards producing human-interpretable explanations and improve transparency. In particular, we utilized and trained deep learning models with state-of-the-art performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were employed towards understanding and interpreting models' predictions, along with quantitative metrics to assess and compare their performance. Numerous experiments were performed to assess the overall performance of XAI methods for straightforward prediction cases, competing multiple labels, as well as misclassification cases. According to our findings, Occlusion, Grad-CAM and Lime were the most interpretable and reliable XAI methods. However, none delivers high-resolution outputs, while apart from Grad-CAM, both Lime and Occlusion are computationally expensive. We also highlight different aspects of XAI performance and elaborate with insights on black-box decisions in order to improve transparency, understand their behavior and reveal, as well, datasets' particularities.

</p>
</details>

<details><summary><b>Detection of COVID-19 Disease using Deep Neural Networks with Ultrasound Imaging</b>
<a href="https://arxiv.org/abs/2104.01509">arxiv:2104.01509</a>
&#x1F4C8; 7 <br>
<p>Carlos Rojas-Azabache, Karen Vilca-Janampa, Renzo Guerrero-Huayta, Dennis Núñez-Fernández</p></summary>
<p>

**Abstract:** The new coronavirus 2019 (COVID-2019) has rapidly become a pandemic and has had a devastating effect on both everyday life, public health and the global economy. It is critical to detect positive cases as early as possible to prevent the further spread of this epidemic and to treat affected patients quickly. The need for auxiliary diagnostic tools has increased as accurate automated tool kits are not available. This paper presents a work in progress that proposes the analysis of images of lung ultrasound scans using a convolutional neural network. The trained model will be used on a Raspberry Pi to predict on new images.

</p>
</details>

<details><summary><b>Removing Pixel Noises and Spatial Artifacts with Generative Diversity Denoising Methods</b>
<a href="https://arxiv.org/abs/2104.01374">arxiv:2104.01374</a>
&#x1F4C8; 7 <br>
<p>Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, Florian Jug</p></summary>
<p>

**Abstract:** Image denoising and artefact removal are complex inverse problems admitting many potential solutions. Variational Autoencoders (VAEs) can be used to learn a whole distribution of sensible solutions, from which one can sample efficiently. However, such a generative approach to image restoration is only studied in the context of pixel-wise noise removal (e.g. Poisson or Gaussian noise). While important, a plethora of application domains suffer from imaging artefacts (structured noises) that alter groups of pixels in correlated ways. In this work we show, for the first time, that generative diversity denoising (GDD) approaches can learn to remove structured noises without supervision. To this end, we investigate two existing GDD architectures, introduce a new one based on hierarchical VAEs, and compare their performances against a total of seven state-of-the-art baseline methods on five sources of structured noise (including tomography reconstruction artefacts and microscopy artefacts). We find that GDD methods outperform all unsupervised baselines and in many cases not lagging far behind supervised results (in some occasions even superseding them). In addition to structured noise removal, we also show that our new GDD method produces new state-of-the-art (SOTA) results on seven out of eight benchmark datasets for pixel-noise removal. Finally, we offer insights into the daunting question of how GDD methods distinguish structured noise, which we like to see removed, from image signals, which we want to see retained.

</p>
</details>

<details><summary><b>Cross-Modal learning for Audio-Visual Video Parsing</b>
<a href="https://arxiv.org/abs/2104.04598">arxiv:2104.04598</a>
&#x1F4C8; 6 <br>
<p>Jatin Lamba,  Abhishek, Jayaprakash Akula, Rishabh Dabral, Preethi Jyothi, Ganesh Ramakrishnan</p></summary>
<p>

**Abstract:** In this paper, we present a novel approach to the audio-visual video parsing (AVVP) task that demarcates events from a video separately for audio and visual modalities. The proposed parsing approach simultaneously detects the temporal boundaries in terms of start and end times of such events. We show how AVVP can benefit from the following techniques geared towards effective cross-modal learning: (i) adversarial training and skip connections (ii) global context aware attention and, (iii) self-supervised pretraining using an audio-video grounding objective to obtain cross-modal audio-video representations. We present extensive experimental evaluations on the Look, Listen, and Parse (LLP) dataset and show that we outperform the state-of-the-art Hybrid Attention Network (HAN) on all five metrics proposed for AVVP. We also present several ablations to validate the effect of pretraining, global attention and adversarial training.

</p>
</details>

<details><summary><b>STL Robustness Risk over Discrete-Time Stochastic Processes</b>
<a href="https://arxiv.org/abs/2104.01503">arxiv:2104.01503</a>
&#x1F4C8; 5 <br>
<p>Lars Lindemann, Nikolai Matni, George J. Pappas</p></summary>
<p>

**Abstract:** We present a framework to interpret signal temporal logic (STL) formulas over discrete-time stochastic processes in terms of the induced risk. Each realization of a stochastic process either satisfies or violates an STL formula. In fact, we can assign a robustness value to each realization that indicates how robustly this realization satisfies an STL formula. We then define the risk of a stochastic process not satisfying an STL formula robustly, referred to as the STL robustness risk. In our definition, we permit general classes of risk measures such as, but not limited to, the conditional value-at-risk. While in general hard to compute, we propose an approximation of the STL robustness risk. This approximation has the desirable property of being an upper bound of the STL robustness risk when the chosen risk measure is monotone, a property satisfied by most risk measures. Motivated by the interest in data-driven approaches, we present a sampling-based method for estimating the approximate STL robustness risk from data for the value-at-risk. While we consider the value-at-risk, we highlight that such sampling-based methods are viable for other risk measures.

</p>
</details>

<details><summary><b>Generative Locally Linear Embedding</b>
<a href="https://arxiv.org/abs/2104.01525">arxiv:2104.01525</a>
&#x1F4C8; 4 <br>
<p>Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley</p></summary>
<p>

**Abstract:** Locally Linear Embedding (LLE) is a nonlinear spectral dimensionality reduction and manifold learning method. It has two main steps which are linear reconstruction and linear embedding of points in the input space and embedding space, respectively. In this work, we propose two novel generative versions of LLE, named Generative LLE (GLLE), whose linear reconstruction steps are stochastic rather than deterministic. GLLE assumes that every data point is caused by its linear reconstruction weights as latent factors. The proposed GLLE algorithms can generate various LLE embeddings stochastically while all the generated embeddings relate to the original LLE embedding. We propose two versions for stochastic linear reconstruction, one using expectation maximization and another with direct sampling from a derived distribution by optimization. The proposed GLLE methods are closely related to and inspired by variational inference, factor analysis, and probabilistic principal component analysis. Our simulations show that the proposed GLLE methods work effectively in unfolding and generating submanifolds of data.

</p>
</details>

<details><summary><b>Towards Self-Adaptive Metric Learning On the Fly</b>
<a href="https://arxiv.org/abs/2104.01495">arxiv:2104.01495</a>
&#x1F4C8; 4 <br>
<p>Yang Gao, Yi-Fan Li, Swarup Chandra, Latifur Khan, Bhavani Thuraisingham</p></summary>
<p>

**Abstract:** Good quality similarity metrics can significantly facilitate the performance of many large-scale, real-world applications. Existing studies have proposed various solutions to learn a Mahalanobis or bilinear metric in an online fashion by either restricting distances between similar (dissimilar) pairs to be smaller (larger) than a given lower (upper) bound or requiring similar instances to be separated from dissimilar instances with a given margin. However, these linear metrics learned by leveraging fixed bounds or margins may not perform well in real-world applications, especially when data distributions are complex. We aim to address the open challenge of "Online Adaptive Metric Learning" (OAML) for learning adaptive metric functions on the fly. Unlike traditional online metric learning methods, OAML is significantly more challenging since the learned metric could be non-linear and the model has to be self-adaptive as more instances are observed. In this paper, we present a new online metric learning framework that attempts to tackle the challenge by learning an ANN-based metric with adaptive model complexity from a stream of constraints. In particular, we propose a novel Adaptive-Bound Triplet Loss (ABTL) to effectively utilize the input constraints and present a novel Adaptive Hedge Update (AHU) method for online updating the model parameters. We empirically validate the effectiveness and efficacy of our framework on various applications such as real-world image classification, facial verification, and image retrieval.

</p>
</details>

<details><summary><b>Training Deep Normalizing Flow Models in Highly Incomplete Data Scenarios with Prior Regularization</b>
<a href="https://arxiv.org/abs/2104.01482">arxiv:2104.01482</a>
&#x1F4C8; 4 <br>
<p>Edgar A. Bernal</p></summary>
<p>

**Abstract:** Deep generative frameworks including GANs and normalizing flow models have proven successful at filling in missing values in partially observed data samples by effectively learning -- either explicitly or implicitly -- complex, high-dimensional statistical distributions. In tasks where the data available for learning is only partially observed, however, their performance decays monotonically as a function of the data missingness rate. In high missing data rate regimes (e.g., 60% and above), it has been observed that state-of-the-art models tend to break down and produce unrealistic and/or semantically inaccurate data. We propose a novel framework to facilitate the learning of data distributions in high paucity scenarios that is inspired by traditional formulations of solutions to ill-posed problems. The proposed framework naturally stems from posing the process of learning from incomplete data as a joint optimization task of the parameters of the model being learned and the missing data values. The method involves enforcing a prior regularization term that seamlessly integrates with objectives used to train explicit and tractable deep generative frameworks such as deep normalizing flow models. We demonstrate via extensive experimental validation that the proposed framework outperforms competing techniques, particularly as the rate of data paucity approaches unity.

</p>
</details>

<details><summary><b>No Need for Interactions: Robust Model-Based Imitation Learning using Neural ODE</b>
<a href="https://arxiv.org/abs/2104.01390">arxiv:2104.01390</a>
&#x1F4C8; 4 <br>
<p>HaoChih Lin, Baopu Li, Xin Zhou, Jiankun Wang, Max Q. -H. Meng</p></summary>
<p>

**Abstract:** Interactions with either environments or expert policies during training are needed for most of the current imitation learning (IL) algorithms. For IL problems with no interactions, a typical approach is Behavior Cloning (BC). However, BC-like methods tend to be affected by distribution shift. To mitigate this problem, we come up with a Robust Model-Based Imitation Learning (RMBIL) framework that casts imitation learning as an end-to-end differentiable nonlinear closed-loop tracking problem. RMBIL applies Neural ODE to learn a precise multi-step dynamics and a robust tracking controller via Nonlinear Dynamics Inversion (NDI) algorithm. Then, the learned NDI controller will be combined with a trajectory generator, a conditional VAE, to imitate an expert's behavior. Theoretical derivation shows that the controller network can approximate an NDI when minimizing the training loss of Neural ODE. Experiments on Mujoco tasks also demonstrate that RMBIL is competitive to the state-of-the-art generative adversarial method (GAIL) and achieves at least 30% performance gain over BC in uneven surfaces.

</p>
</details>

<details><summary><b>Deepfake Detection Scheme Based on Vision Transformer and Distillation</b>
<a href="https://arxiv.org/abs/2104.01353">arxiv:2104.01353</a>
&#x1F4C8; 4 <br>
<p>Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim</p></summary>
<p>

**Abstract:** Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition.

</p>
</details>

<details><summary><b>Random Features for the Neural Tangent Kernel</b>
<a href="https://arxiv.org/abs/2104.01351">arxiv:2104.01351</a>
&#x1F4C8; 4 <br>
<p>Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, Jinwoo Shin</p></summary>
<p>

**Abstract:** The Neural Tangent Kernel (NTK) has discovered connections between deep neural networks and kernel methods with insights of optimization and generalization. Motivated by this, recent works report that NTK can achieve better performances compared to training neural networks on small-scale datasets. However, results under large-scale settings are hardly studied due to the computational limitation of kernel methods. In this work, we propose an efficient feature map construction of the NTK of fully-connected ReLU network which enables us to apply it to large-scale datasets. We combine random features of the arc-cosine kernels with a sketching-based algorithm which can run in linear with respect to both the number of data points and input dimension. We show that dimension of the resulting features is much smaller than other baseline feature map constructions to achieve comparable error bounds both in theory and practice. We additionally utilize the leverage score based sampling for improved bounds of arc-cosine random features and prove a spectral approximation guarantee of the proposed feature map to the NTK matrix of two-layer neural network. We benchmark a variety of machine learning tasks to demonstrate the superiority of the proposed scheme. In particular, our algorithm can run tens of magnitude faster than the exact kernel methods for large-scale settings without performance loss.

</p>
</details>

<details><summary><b>Uncertainty for Identifying Open-Set Errors in Visual Object Detection</b>
<a href="https://arxiv.org/abs/2104.01328">arxiv:2104.01328</a>
&#x1F4C8; 4 <br>
<p>Dimity Miller, Niko Sünderhauf, Michael Milford, Feras Dayoub</p></summary>
<p>

**Abstract:** Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset. We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific open-set datasets to evaluate open-set performance in object detection.

</p>
</details>

<details><summary><b>Knowledge Distillation For Wireless Edge Learning</b>
<a href="https://arxiv.org/abs/2104.06374">arxiv:2104.06374</a>
&#x1F4C8; 3 <br>
<p>Ahmed P. Mohamed, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal</p></summary>
<p>

**Abstract:** In this paper, we propose a framework for predicting frame errors in the collaborative spectrally congested wireless environments of the DARPA Spectrum Collaboration Challenge (SC2) via a recently collected dataset. We employ distributed deep edge learning that is shared among edge nodes and a central cloud. Using this close-to-practice dataset, we find that widely used federated learning approaches, specially those that are privacy preserving, are worse than local training for a wide range of settings. We hence utilize the synthetic minority oversampling technique to maintain privacy via avoiding the transfer of local data to the cloud, and utilize knowledge distillation with an aim to benefit from high cloud computing and storage capabilities. The proposed framework achieves overall better performance than both local and federated training approaches, while being robust against catastrophic failures as well as challenging channel conditions that result in high frame error rates.

</p>
</details>

<details><summary><b>Influencing Reinforcement Learning through Natural Language Guidance</b>
<a href="https://arxiv.org/abs/2104.01506">arxiv:2104.01506</a>
&#x1F4C8; 3 <br>
<p>Tasmia Tasrin, Md Sultan Al Nahian, Habarakadage Perera, Brent Harrison</p></summary>
<p>

**Abstract:** Interactive reinforcement learning agents use human feedback or instruction to help them learn in complex environments. Often, this feedback comes in the form of a discrete signal that is either positive or negative. While informative, this information can be difficult to generalize on its own. In this work, we explore how natural language advice can be used to provide a richer feedback signal to a reinforcement learning agent by extending policy shaping, a well-known Interactive reinforcement learning technique. Usually policy shaping employs a human feedback policy to help an agent to learn more about how to achieve its goal. In our case, we replace this human feedback policy with policy generated based on natural language advice. We aim to inspect if the generated natural language reasoning provides support to a deep reinforcement learning agent to decide its actions successfully in any given environment. So, we design our model with three networks: first one is the experience driven, next is the advice generator and third one is the advice driven. While the experience driven reinforcement learning agent chooses its actions being influenced by the environmental reward, the advice driven neural network with generated feedback by the advice generator for any new state selects its actions to assist the reinforcement learning agent to better policy shaping.

</p>
</details>

<details><summary><b>COHORTNEY: Non-Parametric Clustering of Event Sequences</b>
<a href="https://arxiv.org/abs/2104.01440">arxiv:2104.01440</a>
&#x1F4C8; 3 <br>
<p>Vladislav Zhuzhel, Rodrigo Rivera-Castro, Nina Kaploukhaya, Liliya Mironova, Alexey Zaytsev, Evgeny Burnaev</p></summary>
<p>

**Abstract:** Cohort analysis is a pervasive activity in web analytics. One divides users into groups according to specific criteria and tracks their behavior over time. Despite its extensive use, academic circles do not discuss cohort analysis to evaluate user behavior online. This work introduces an unsupervised non-parametric approach to group Internet users based on their activities. In comparison, canonical methods in marketing and engineering-based techniques underperform. COHORTNEY is the first machine learning-based cohort analysis algorithm with a robust theoretical explanation.

</p>
</details>

<details><summary><b>Gradient-based Adversarial Deep Modulation Classification with Data-driven Subsampling</b>
<a href="https://arxiv.org/abs/2104.06375">arxiv:2104.06375</a>
&#x1F4C8; 2 <br>
<p>Jinho Yi, Aly El Gamal</p></summary>
<p>

**Abstract:** Automatic modulation classification can be a core component for intelligent spectrally efficient wireless communication networks, and deep learning techniques have recently been shown to deliver superior performance to conventional model-based strategies, particularly when distinguishing between a large number of modulation types. However, such deep learning techniques have also been recently shown to be vulnerable to gradient-based adversarial attacks that rely on subtle input perturbations, which would be particularly feasible in a wireless setting via jamming. One such potent attack is the one known as the Carlini-Wagner attack, which we consider in this work. We further consider a data-driven subsampling setting, where several recently introduced deep-learning-based algorithms are employed to select a subset of samples that lead to reducing the final classifier's training time with minimal loss in accuracy. In this setting, the attacker has to make an assumption about the employed subsampling strategy, in order to calculate the loss gradient. Based on state of the art techniques available to both the attacker and defender, we evaluate best strategies under various assumptions on the knowledge of the other party's strategy. Interestingly, in presence of knowledgeable attackers, we identify computational cost reduction opportunities for the defender with no or minimal loss in performance.

</p>
</details>

<details><summary><b>SetConv: A New Approach for Learning from Imbalanced Data</b>
<a href="https://arxiv.org/abs/2104.06313">arxiv:2104.06313</a>
&#x1F4C8; 2 <br>
<p>Yang Gao, Yi-Fan Li, Yu Lin, Charu Aggarwal, Latifur Khan</p></summary>
<p>

**Abstract:** For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.

</p>
</details>

<details><summary><b>Learning-Based UAV Trajectory Optimization with Collision Avoidance and Connectivity Constraints</b>
<a href="https://arxiv.org/abs/2104.06256">arxiv:2104.06256</a>
&#x1F4C8; 2 <br>
<p>Xueyuan Wang, M. Cenk Gursoy</p></summary>
<p>

**Abstract:** Unmanned aerial vehicles (UAVs) are expected to be an integral part of wireless networks, and determining collision-free trajectories for multiple UAVs while satisfying requirements of connectivity with ground base stations (GBSs) is a challenging task. In this paper, we first reformulate the multi-UAV trajectory optimization problem with collision avoidance and wireless connectivity constraints as a sequential decision making problem in the discrete time domain. We, then, propose a decentralized deep reinforcement learning approach to solve the problem. More specifically, a value network is developed to encode the expected time to destination given the agent's joint state (including the agent's information, the nearby agents' observable information, and the locations of the nearby GBSs). A signal-to-interference-plus-noise ratio (SINR)-prediction neural network is also designed, using accumulated SINR measurements obtained when interacting with the cellular network, to map the GBSs' locations into the SINR levels in order to predict the UAV's SINR. Numerical results show that with the value network and SINR-prediction network, real-time navigation for multi-UAVs can be efficiently performed in various environments with high success rate.

</p>
</details>

<details><summary><b>A Modified Convolutional Network for Auto-encoding based on Pattern Theory Growth Function</b>
<a href="https://arxiv.org/abs/2104.02651">arxiv:2104.02651</a>
&#x1F4C8; 2 <br>
<p>Erico Tjoa</p></summary>
<p>

**Abstract:** This brief paper reports the shortcoming of a variant of convolutional neural network whose components are developed based on the pattern theory framework.

</p>
</details>

<details><summary><b>Golden Tortoise Beetle Optimizer: A Novel Nature-Inspired Meta-heuristic Algorithm for Engineering Problems</b>
<a href="https://arxiv.org/abs/2104.01521">arxiv:2104.01521</a>
&#x1F4C8; 2 <br>
<p>Omid Tarkhaneh, Neda Alipour, Amirahmad Chapnevis, Haifeng Shen</p></summary>
<p>

**Abstract:** This paper proposes a novel nature-inspired meta-heuristic algorithm called the Golden Tortoise Beetle Optimizer (GTBO) to solve optimization problems. It mimics golden tortoise beetle's behavior of changing colors to attract opposite sex for mating and its protective strategy that uses a kind of anal fork to deter predators. The algorithm is modeled based on the beetle's dual attractiveness and survival strategy to generate new solutions for optimization problems. To measure its performance, the proposed GTBO is compared with five other nature-inspired evolutionary algorithms on 24 well-known benchmark functions investigating the trade-off between exploration and exploitation, local optima avoidance, and convergence towards the global optima is statistically significant. We particularly applied GTBO to two well-known engineering problems including the welded beam design problem and the gear train design problem. The results demonstrate that the new algorithm is more efficient than the five baseline algorithms for both problems. A sensitivity analysis is also performed to reveal different impacts of the algorithm's key control parameters and operators on GTBO's performance.

</p>
</details>

<details><summary><b>Mitigating Gradient-based Adversarial Attacks via Denoising and Compression</b>
<a href="https://arxiv.org/abs/2104.01494">arxiv:2104.01494</a>
&#x1F4C8; 2 <br>
<p>Rehana Mahfuz, Rajeev Sahay, Aly El Gamal</p></summary>
<p>

**Abstract:** Gradient-based adversarial attacks on deep neural networks pose a serious threat, since they can be deployed by adding imperceptible perturbations to the test data of any network, and the risk they introduce cannot be assessed through the network's original training performance. Denoising and dimensionality reduction are two distinct methods that have been independently investigated to combat such attacks. While denoising offers the ability to tailor the defense to the specific nature of the attack, dimensionality reduction offers the advantage of potentially removing previously unseen perturbations, along with reducing the training time of the network being defended. We propose strategies to combine the advantages of these two defense mechanisms. First, we propose the cascaded defense, which involves denoising followed by dimensionality reduction. To reduce the training time of the defense for a small trade-off in performance, we propose the hidden layer defense, which involves feeding the output of the encoder of a denoising autoencoder into the network. Further, we discuss how adaptive attacks against these defenses could become significantly weak when an alternative defense is used, or when no defense is used. In this light, we propose a new metric to evaluate a defense which measures the sensitivity of the adaptive attack to modifications in the defense. Finally, we present a guideline for building an ordered repertoire of defenses, a.k.a. a defense infrastructure, that adjusts to limited computational resources in presence of uncertainty about the attack strategy.

</p>
</details>

<details><summary><b>Exponentiated Gradient Reweighting for Robust Training Under Label Noise and Beyond</b>
<a href="https://arxiv.org/abs/2104.01493">arxiv:2104.01493</a>
&#x1F4C8; 2 <br>
<p>Negin Majidi, Ehsan Amid, Hossein Talebi, Manfred K. Warmuth</p></summary>
<p>

**Abstract:** Many learning tasks in machine learning can be viewed as taking a gradient step towards minimizing the average loss of a batch of examples in each training iteration. When noise is prevalent in the data, this uniform treatment of examples can lead to overfitting to noisy examples with larger loss values and result in poor generalization. Inspired by the expert setting in on-line learning, we present a flexible approach to learning from noisy examples. Specifically, we treat each training example as an expert and maintain a distribution over all examples. We alternate between updating the parameters of the model using gradient descent and updating the example weights using the exponentiated gradient update. Unlike other related methods, our approach handles a general class of loss functions and can be applied to a wide range of noise types and applications. We show the efficacy of our approach for multiple learning settings, namely noisy principal component analysis and a variety of noisy classification problems.

</p>
</details>

<details><summary><b>A surrogate loss function for optimization of $F_β$ score in binary classification with imbalanced data</b>
<a href="https://arxiv.org/abs/2104.01459">arxiv:2104.01459</a>
&#x1F4C8; 2 <br>
<p>Namgil Lee, Heejung Yang, Hojin Yoo</p></summary>
<p>

**Abstract:** The $F_β$ score is a commonly used measure of classification performance, which plays crucial roles in classification tasks with imbalanced data sets. However, the $F_β$ score cannot be used as a loss function by gradient-based learning algorithms for optimizing neural network parameters due to its non-differentiability. On the other hand, commonly used loss functions such as the binary cross-entropy (BCE) loss are not directly related to performance measures such as the $F_β$ score, so that neural networks optimized by using the loss functions may not yield optimal performance measures. In this study, we investigate a relationship between classification performance measures and loss functions in terms of the gradients with respect to the model parameters. Then, we propose a differentiable surrogate loss function for the optimization of the $F_β$ score. We show that the gradient paths of the proposed surrogate $F_β$ loss function approximate the gradient paths of the large sample limit of the $F_β$ score. Through numerical experiments using ResNets and benchmark image data sets, it is demonstrated that the proposed surrogate $F_β$ loss function is effective for optimizing $F_β$ scores under class imbalances in binary classification tasks compared with other loss functions.

</p>
</details>

<details><summary><b>Unsupervised Domain Adaptation with Global and Local Graph Neural Networks in Limited Labeled Data Scenario: Application to Disaster Management</b>
<a href="https://arxiv.org/abs/2104.01436">arxiv:2104.01436</a>
&#x1F4C8; 2 <br>
<p>Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar</p></summary>
<p>

**Abstract:** Identification and categorization of social media posts generated during disasters are crucial to reduce the sufferings of the affected people. However, lack of labeled data is a significant bottleneck in learning an effective categorization system for a disaster. This motivates us to study the problem as unsupervised domain adaptation (UDA) between a previous disaster with labeled data (source) and a current disaster (target). However, if the amount of labeled data available is limited, it restricts the learning capabilities of the model. To handle this challenge, we utilize limited labeled data along with abundantly available unlabeled data, generated during a source disaster to propose a novel two-part graph neural network. The first-part extracts domain-agnostic global information by constructing a token level graph across domains and the second-part preserves local instance-level semantics. In our experiments, we show that the proposed method outperforms state-of-the-art techniques by $2.74\%$ weighted F$_1$ score on average on two standard public dataset in the area of disaster management. We also report experimental results for granular actionable multi-label classification datasets in disaster domain for the first time, on which we outperform BERT by $3.00\%$ on average w.r.t weighted F$_1$. Additionally, we show that our approach can retain performance when very limited labeled data is available.

</p>
</details>

<details><summary><b>IDOL-Net: An Interactive Dual-Domain Parallel Network for CT Metal Artifact Reduction</b>
<a href="https://arxiv.org/abs/2104.01405">arxiv:2104.01405</a>
&#x1F4C8; 2 <br>
<p>Tao Wang, Wenjun Xia, Zexin Lu, Huaiqiang Sun, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang</p></summary>
<p>

**Abstract:** Due to the presence of metallic implants, the imaging quality of computed tomography (CT) would be heavily degraded. With the rapid development of deep learning, several network models have been proposed for metal artifact reduction (MAR). Since the dual-domain MAR methods can leverage the hybrid information from both sinogram and image domains, they have significantly improved the performance compared to single-domain methods. However,current dual-domain methods usually operate on both domains in a specific order, which implicitly imposes a certain priority prior into MAR and may ignore the latent information interaction between both domains. To address this problem, in this paper, we propose a novel interactive dualdomain parallel network for CT MAR, dubbed as IDOLNet. Different from existing dual-domain methods, the proposed IDOL-Net is composed of two modules. The disentanglement module is utilized to generate high-quality prior sinogram and image as the complementary inputs. The follow-up refinement module consists of two parallel and interactive branches that simultaneously operate on image and sinogram domain, fully exploiting the latent information interaction between both domains. The simulated and clinical results demonstrate that the proposed IDOL-Net outperforms several state-of-the-art models in both qualitative and quantitative aspects.

</p>
</details>

<details><summary><b>Property-driven Training: All You (N)Ever Wanted to Know About</b>
<a href="https://arxiv.org/abs/2104.01396">arxiv:2104.01396</a>
&#x1F4C8; 2 <br>
<p>Marco Casadio, Matthew Daggitt, Ekaterina Komendantskaya, Wen Kokke, Daniel Kienitz, Rob Stewart</p></summary>
<p>

**Abstract:** Neural networks are known for their ability to detect general patterns in noisy data. This makes them a popular tool for perception components in complex AI systems. Paradoxically, they are also known for being vulnerable to adversarial attacks. In response, various methods such as adversarial training, data-augmentation and Lipschitz robustness training have been proposed as means of improving their robustness. However, as this paper explores, these training methods each optimise for a different definition of robustness. We perform an in-depth comparison of these different definitions, including their relationship, assumptions, interpretability and verifiability after training. We also look at constraint-driven training, a general approach designed to encode arbitrary constraints, and show that not all of these definitions are directly encodable. Finally we perform experiments to compare the applicability and efficacy of the training methods at ensuring the network obeys these different definitions. These results highlight that even the encoding of such a simple piece of knowledge such as robustness in neural network training is fraught with difficult choices and pitfalls.

</p>
</details>

<details><summary><b>Joint Geometric and Topological Analysis of Hierarchical Datasets</b>
<a href="https://arxiv.org/abs/2104.01395">arxiv:2104.01395</a>
&#x1F4C8; 2 <br>
<p>Lior Aloni, Omer Bobrowski, Ronen Talmon</p></summary>
<p>

**Abstract:** In a world abundant with diverse data arising from complex acquisition techniques, there is a growing need for new data analysis methods. In this paper we focus on high-dimensional data that are organized into several hierarchical datasets. We assume that each dataset consists of complex samples, and every sample has a distinct irregular structure modeled by a graph. The main novelty in this work lies in the combination of two complementing powerful data-analytic approaches: topological data analysis (TDA) and geometric manifold learning. Geometry primarily contains local information, while topology inherently provides global descriptors. Based on this combination, we present a method for building an informative representation of hierarchical datasets. At the finer (sample) level, we devise a new metric between samples based on manifold learning that facilitates quantitative structural analysis. At the coarser (dataset) level, we employ TDA to extract qualitative structural information from the datasets. We showcase the applicability and advantages of our method on simulated data and on a corpus of hyper-spectral images. We show that an ensemble of hyper-spectral images exhibits a hierarchical structure that fits well the considered setting. In addition, we show that our new method gives rise to superior classification results compared to state-of-the-art methods.

</p>
</details>

<details><summary><b>Learning Mobile CNN Feature Extraction Toward Fast Computation of Visual Object Tracking</b>
<a href="https://arxiv.org/abs/2104.01381">arxiv:2104.01381</a>
&#x1F4C8; 2 <br>
<p>Tsubasa Murate, Takashi Watanabe, Masaki Yamada</p></summary>
<p>

**Abstract:** In this paper, we construct a lightweight, high-precision and high-speed object tracking using a trained CNN. Conventional methods with trained CNNs use VGG16 network which requires powerful computational resources. Therefore, there is a problem that it is difficult to apply in low computation resources environments. To solve this problem, we use MobileNetV3, which is a CNN for mobile terminals.Based on Feature Map Selection Tracking, we propose a new architecture that extracts effective features of MobileNet for object tracking. The architecture requires no online learning but only offline learning. In addition, by using features of objects other than tracking target, the features of tracking target are extracted more efficiently. We measure the tracking accuracy with Visual Tracker Benchmark and confirm that the proposed method can perform high-precision and high-speed calculation even in low computation resource environments.

</p>
</details>

<details><summary><b>Simple Uncoupled No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium</b>
<a href="https://arxiv.org/abs/2104.01520">arxiv:2104.01520</a>
&#x1F4C8; 1 <br>
<p>Gabriele Farina, Andrea Celli, Alberto Marchesi, Nicola Gatti</p></summary>
<p>

**Abstract:** The existence of simple uncoupled no-regret learning dynamics that converge to correlated equilibria in normal-form games is a celebrated result in the theory of multi-agent systems. Specifically, it has been known for more than 20 years that when all players seek to minimize their internal regret in a repeated normal-form game, the empirical frequency of play converges to a normal-form correlated equilibrium. Extensive-form games generalize normal-form games by modeling both sequential and simultaneous moves, as well as imperfect information. Because of the sequential nature and presence of private information in the game, correlation in extensive-form games possesses significantly different properties than its counterpart in normal-form games, many of which are still open research directions. Extensive-form correlated equilibrium (EFCE) has been proposed as the natural extensive-form counterpart to the classical notion of correlated equilibrium in normal-form games. Compared to the latter, the constraints that define the set of EFCEs are significantly more complex, as the correlation device must keep into account the evolution of beliefs of each player as they make observations throughout the game. Due to that significant added complexity, the existence of uncoupled learning dynamics leading to an EFCE has remained a challenging open research question for a long time. In this article, we settle that question by giving the first uncoupled no-regret dynamics that converge to the set of EFCEs in n-player general-sum extensive-form games with perfect recall. We show that each iterate can be computed in time polynomial in the size of the game tree, and that, when all players play repeatedly according to our learning dynamics, the empirical frequency of play is proven to be a O(T^-0.5)-approximate EFCE with high probability after T game repetitions, and an EFCE almost surely in the limit.

</p>
</details>

<details><summary><b>Late fusion of machine learning models using passively captured interpersonal social interactions and motion from smartphones predicts decompensation in heart failure</b>
<a href="https://arxiv.org/abs/2104.01511">arxiv:2104.01511</a>
&#x1F4C8; 1 <br>
<p>Ayse S. Cakmak, Samuel Densen, Gabriel Najarro, Pratik Rout, Christopher J. Rozell, Omer T. Inan, Amit J. Shah, Gari D. Clifford</p></summary>
<p>

**Abstract:** Objective: Worldwide, heart failure (HF) is a major cause of morbidity and mortality and one of the leading causes of hospitalization. Early detection of HF symptoms and pro-active management may reduce adverse events. Approach: Twenty-eight participants were monitored using a smartphone app after discharge from hospitals, and each clinical event during the enrollment (N=110 clinical events) was recorded. Motion, social, location, and clinical survey data collected via the smartphone-based monitoring system were used to develop and validate an algorithm for predicting or classifying HF decompensation events (hospitalizations or clinic visit) versus clinic monitoring visits in which they were determined to be compensated or stable. Models based on single modality as well as early and late fusion approaches combining patient-reported outcomes and passive smartphone data were evaluated. Results: The highest AUCPr for classifying decompensation with a late fusion approach was 0.80 using leave one subject out cross-validation. Significance: Passively collected data from smartphones, especially when combined with weekly patient-reported outcomes, may reflect behavioral and physiological changes due to HF and thus could enable prediction of HF decompensation.

</p>
</details>

<details><summary><b>MR-Contrast-Aware Image-to-Image Translations with Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2104.01449">arxiv:2104.01449</a>
&#x1F4C8; 1 <br>
<p>Jonas Denck, Jens Guehring, Andreas Maier, Eva Rothgang</p></summary>
<p>

**Abstract:** Purpose
  A Magnetic Resonance Imaging (MRI) exam typically consists of several sequences that yield different image contrasts. Each sequence is parameterized through multiple acquisition parameters that influence image contrast, signal-to-noise ratio, acquisition time, and/or resolution. Depending on the clinical indication, different contrasts are required by the radiologist to make a diagnosis. As MR sequence acquisition is time consuming and acquired images may be corrupted due to motion, a method to synthesize MR images with adjustable contrast properties is required.
  Methods
  Therefore, we trained an image-to-image generative adversarial network conditioned on the MR acquisition parameters repetition time and echo time. Our approach is motivated by style transfer networks, whereas the "style" for an image is explicitly given in our case, as it is determined by the MR acquisition parameters our network is conditioned on.
  Results
  This enables us to synthesize MR images with adjustable image contrast. We evaluated our approach on the fastMRI dataset, a large set of publicly available MR knee images, and show that our method outperforms a benchmark pix2pix approach in the translation of non-fat-saturated MR images to fat-saturated images. Our approach yields a peak signal-to-noise ratio and structural similarity of 24.48 and 0.66, surpassing the pix2pix benchmark model significantly.
  Conclusion
  Our model is the first that enables fine-tuned contrast synthesis, which can be used to synthesize missing MR contrasts or as a data augmentation technique for AI training in MRI.

</p>
</details>

<details><summary><b>Deep Reinforcement Learning Powered IRS-Assisted Downlink NOMA</b>
<a href="https://arxiv.org/abs/2104.01414">arxiv:2104.01414</a>
&#x1F4C8; 1 <br>
<p>Muhammad Shehab, Bekir S. Ciftler, Tamer Khattab, Mohamed Abdallah, Daniele Trinchero</p></summary>
<p>

**Abstract:** In this work, we examine an intelligent reflecting surface (IRS) assisted downlink non-orthogonal multiple access (NOMA) scenario with the aim of maximizing the sum rate of users. The optimization problem at the IRS is quite complicated, and non-convex, since it requires the tuning of the phase shift reflection matrix. Driven by the rising deployment of deep reinforcement learning (DRL) techniques that are capable of coping with solving non-convex optimization problems, we employ DRL to predict and optimally tune the IRS phase shift matrices. Simulation results reveal that IRS assisted NOMA based on our utilized DRL scheme achieves high sum rate compared to OMA based one, and as the transmit power increases, the capability of serving more users increases. Furthermore, results show that imperfect successive interference cancellation (SIC) has a deleterious impact on the data rate of users performing SIC. As the imperfection increases by ten times, the rate decreases by more than 10%.

</p>
</details>

<details><summary><b>Distributed Reinforcement Learning for Age of Information Minimization in Real-Time IoT Systems</b>
<a href="https://arxiv.org/abs/2104.01527">arxiv:2104.01527</a>
&#x1F4C8; 0 <br>
<p>Sihua Wang, Mingzhe Chen, Zhaohui Yang, Changchuan Yin, Walid Saad, Shuguang Cui, H. Vincent Poor</p></summary>
<p>

**Abstract:** In this paper, the problem of minimizing the weighted sum of age of information (AoI) and total energy consumption of Internet of Things (IoT) devices is studied. In the considered model, each IoT device monitors a physical process that follows nonlinear dynamics. As the dynamics of the physical process vary over time, each device must find an optimal sampling frequency to sample the real-time dynamics of the physical system and send sampled information to a base station (BS). Due to limited wireless resources, the BS can only select a subset of devices to transmit their sampled information. Thus, edge devices must cooperatively sample their monitored dynamics based on the local observations and the BS must collect the sampled information from the devices immediately, hence avoiding the additional time and energy used for sampling and information transmission. To this end, it is necessary to jointly optimize the sampling policy of each device and the device selection scheme of the BS so as to accurately monitor the dynamics of the physical process using minimum energy. This problem is formulated as an optimization problem whose goal is to minimize the weighted sum of AoI cost and energy consumption. To solve this problem, we propose a novel distributed reinforcement learning (RL) approach for the sampling policy optimization. The proposed algorithm enables edge devices to cooperatively find the global optimal sampling policy using their own local observations. Given the sampling policy, the device selection scheme can be optimized thus minimizing the weighted sum of AoI and energy consumption of all devices. Simulations with real data of PM 2.5 pollution show that the proposed algorithm can reduce the sum of AoI by up to 17.8% and 33.9% and the total energy consumption by up to 13.2% and 35.1%, compared to a conventional deep Q network method and a uniform sampling policy.

</p>
</details>

<details><summary><b>Explanatory models in neuroscience: Part 1 -- taking mechanistic abstraction seriously</b>
<a href="https://arxiv.org/abs/2104.01490">arxiv:2104.01490</a>
&#x1F4C8; 0 <br>
<p>Rosa Cao, Daniel Yamins</p></summary>
<p>

**Abstract:** Despite the recent success of neural network models in mimicking animal performance on visual perceptual tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system is taken to require fleshing out the parts, organization, and activities of a system, and how those give rise to behaviors of interest. However, it remains somewhat controversial what it means for a model to describe a mechanism, and whether neural network models qualify as explanatory.
  We argue that certain kinds of neural network models are actually good examples of mechanistic models, when the right notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify a level of description that is both abstract but detailed enough to be "runnable", and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are those already in use in experimental neuroscience, and are of the kind deployed in the construction of more familiar computational models, just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals.
  In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.

</p>
</details>

<details><summary><b>Explanatory models in neuroscience: Part 2 -- constraint-based intelligibility</b>
<a href="https://arxiv.org/abs/2104.01489">arxiv:2104.01489</a>
&#x1F4C8; 0 <br>
<p>Rosa Cao, Daniel Yamins</p></summary>
<p>

**Abstract:** Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the context of neural network models for neuroscience, concerns have been raised about model intelligibility, and how they relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are causally responsible for that behavior. In biological systems, many of these dependencies are naturally "top-down": ethological imperatives interact with evolutionary and developmental constraints under natural selection. We describe how the optimization techniques used to construct NN models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are -- because when a challenging ecologically-relevant goal is shared by a NN and the brain, it places tight constraints on the possible mechanisms exhibited in both kinds of systems. By combining two familiar modes of explanation -- one based on bottom-up mechanism (whose relation to neural network models we address in a companion paper) and the other on top-down constraints, these models illuminate brain function.

</p>
</details>


[Next Page]({{ '/2021/04/02/2021.04.02.html' | relative_url }})
