## Summary for 2021-07-11, created on 2021-12-19


<details><summary><b>Neural Waveshaping Synthesis</b>
<a href="https://arxiv.org/abs/2107.05050">arxiv:2107.05050</a>
&#x1F4C8; 39 <br>
<p>Ben Hayes, Charalampos Saitis, György Fazekas</p></summary>
<p>

**Abstract:** We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fréchet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.

</p>
</details>

<details><summary><b>One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images</b>
<a href="https://arxiv.org/abs/2107.05047">arxiv:2107.05047</a>
&#x1F4C8; 20 <br>
<p>Weina Jin, Xiaoxiao Li, Ghassan Hamarneh</p></summary>
<p>

**Abstract:** Being able to explain the prediction to clinical end-users is a necessity to leverage the power of AI models for clinical decision support. For medical images, saliency maps are the most common form of explanation. The maps highlight important features for AI model's prediction. Although many saliency map methods have been proposed, it is unknown how well they perform on explaining decisions on multi-modal medical images, where each modality/channel carries distinct clinical meanings of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the MSFI (Modality-Specific Feature Importance) metric to examine whether saliency maps can highlight modality-specific important features. MSFI encodes the clinical requirements on modality prioritization and modality-specific feature localization. Our evaluations on 16 commonly used saliency map methods, including a clinician user study, show that although most saliency map methods captured modality importance information in general, most of them failed to highlight modality-specific important features consistently and precisely. The evaluation results guide the choices of saliency map methods and provide insights to propose new ones targeting clinical applications.

</p>
</details>

<details><summary><b>BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis</b>
<a href="https://arxiv.org/abs/2107.05097">arxiv:2107.05097</a>
&#x1F4C8; 7 <br>
<p>Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, Carl Yang</p></summary>
<p>

**Abstract:** Interpretable brain network models for disease prediction are of great value for the advancement of neuroscience. GNNs are promising to model complicated network data, but they are prone to overfitting and suffer from poor interpretability, which prevents their usage in decision-critical scenarios like healthcare. To bridge this gap, we propose BrainNNExplainer, an interpretable GNN framework for brain network analysis. It is mainly composed of two jointly learned modules: a backbone prediction model that is specifically designed for brain networks and an explanation generator that highlights disease-specific prominent brain network connections. Extensive experimental results with visualizations on two challenging disease prediction datasets demonstrate the unique interpretability and outstanding performance of BrainNNExplainer.

</p>
</details>

<details><summary><b>SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network</b>
<a href="https://arxiv.org/abs/2107.05093">arxiv:2107.05093</a>
&#x1F4C8; 7 <br>
<p>Shuo-En Chang, Yi-Cheng Yang, En-Ting Lin, Pei-Yung Hsiao, Li-Chen Fu</p></summary>
<p>

**Abstract:** Recently, there has been a panoptic segmentation task combining semantic and instance segmentation, in which the goal is to classify each pixel with the corresponding instance ID. In this work, we propose a solution to tackle the panoptic segmentation task. The overall structure combines the bottom-up method and the top-down method. Therefore, not only can there be better performance, but also the execution speed can be maintained. The network mainly pays attention to the quality of the mask. In the previous work, we can see that the uneven contour of the object is more likely to appear, resulting in low-quality prediction. Accordingly, we propose enhancement features and corresponding loss functions for the silhouette of objects and backgrounds to improve the mask. Meanwhile, we use the new proposed confidence score to solve the occlusion problem and make the network tend to use higher quality masks as prediction results. To verify our research, we used the COCO dataset and CityScapes dataset to do experiments and obtained competitive results with fast inference time.

</p>
</details>

<details><summary><b>Repo2Vec: A Comprehensive Embedding Approach for Determining Repository Similarity</b>
<a href="https://arxiv.org/abs/2107.05112">arxiv:2107.05112</a>
&#x1F4C8; 6 <br>
<p>Md Omar Faruk Rokon, Pei Yan, Risul Islam, Michalis Faloutsos</p></summary>
<p>

**Abstract:** How can we identify similar repositories and clusters among a large online archive, such as GitHub? Determiningrepository similarity is an essential building block in studying the dynamics and the evolution of such software ecosystems. The key challenge is to determine the right representation for the diverse repository features in a way that: (a) it captures all aspects of the available information, and (b) it is readily usable by MLalgorithms. We propose Repo2Vec, a comprehensive embedding approach to represent a repository as a distributed vector by combining features from three types of information sources. As our key novelty, we consider three types of information: (a)metadata, (b) the structure of the repository, and (c) the source code. We also introduce a series of embedding approaches to represent and combine these information types into a single embedding. We evaluate our method with two real datasets from GitHub for a combined 1013 repositories. First, we show that our method outperforms previous methods in terms of precision (93%vs 78%), with nearly twice as many Strongly Similar repositories and 30% fewer False Positives. Second, we show how Repo2Vecprovides a solid basis for: (a) distinguishing between malware and benign repositories, and (b) identifying a meaningful hierarchical clustering. For example, we achieve 98% precision and 96%recall in distinguishing malware and benign repositories. Overall, our work is a fundamental building block for enabling many repository analysis functions such as repository categorization by target platform or intention, detecting code-reuse and clones, and identifying lineage and evolution.

</p>
</details>

<details><summary><b>Generating stable molecules using imitation and reinforcement learning</b>
<a href="https://arxiv.org/abs/2107.05007">arxiv:2107.05007</a>
&#x1F4C8; 6 <br>
<p>Søren Ager Meldgaard, Jonas Köhler, Henrik Lund Mortensen, Mads-Peter V. Christiansen, Frank Noé, Bjørk Hammer</p></summary>
<p>

**Abstract:** Chemical space is routinely explored by machine learning methods to discover interesting molecules, before time-consuming experimental synthesizing is attempted. However, these methods often rely on a graph representation, ignoring 3D information necessary for determining the stability of the molecules. We propose a reinforcement learning approach for generating molecules in cartesian coordinates allowing for quantum chemical prediction of the stability. To improve sample-efficiency we learn basic chemical rules from imitation learning on the GDB-11 database to create an initial model applicable for all stoichiometries. We then deploy multiple copies of the model conditioned on a specific stoichiometry in a reinforcement learning setting. The models correctly identify low energy molecules in the database and produce novel isomers not found in the training set. Finally, we apply the model to larger molecules to show how reinforcement learning further refines the imitation learning model in domains far from the training data.

</p>
</details>

<details><summary><b>Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving</b>
<a href="https://arxiv.org/abs/2107.04991">arxiv:2107.04991</a>
&#x1F4C8; 6 <br>
<p>Ferhat Ozgur Catak, Tao Yue, Shaukat Ali</p></summary>
<p>

**Abstract:** Object detection in autonomous cars is commonly based on camera images and Lidar inputs, which are often used to train prediction models such as deep artificial neural networks for decision making for object recognition, adjusting speed, etc. A mistake in such decision making can be damaging; thus, it is vital to measure the reliability of decisions made by such prediction models via uncertainty measurement. Uncertainty, in deep learning models, is often measured for classification problems. However, deep learning models in autonomous driving are often multi-output regression models. Hence, we propose a novel method called PURE (Prediction sURface uncErtainty) for measuring prediction uncertainty of such regression models. We formulate the object recognition problem as a regression model with more than one outputs for finding object locations in a 2-dimensional camera view. For evaluation, we modified three widely-applied object recognition models (i.e., YoLo, SSD300 and SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET datasets. Results showed the statistically significant negative correlation between prediction surface uncertainty and prediction accuracy suggesting that uncertainty significantly impacts the decisions made by autonomous driving.

</p>
</details>

<details><summary><b>Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder</b>
<a href="https://arxiv.org/abs/2107.06642">arxiv:2107.06642</a>
&#x1F4C8; 5 <br>
<p>Manh Luong, Viet Anh Tran</p></summary>
<p>

**Abstract:** Voice conversion is a challenging task which transforms the voice characteristics of a source speaker to a target speaker without changing linguistic content. Recently, there have been many works on many-to-many Voice Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results, however, these methods lack the ability to disentangle speaker identity and linguistic content to achieve good performance on unseen speaker scenarios. In this paper, we propose a new method based on feature disentanglement to tackle many to many voice conversion. The method has the capability to disentangle speaker identity and linguistic content from utterances, it can convert from many source speakers to many target speakers with a single autoencoder network. Moreover, it naturally deals with the unseen target speaker scenarios. We perform both objective and subjective evaluations to show the competitive performance of our proposed method compared with other state-of-the-art models in terms of naturalness and target speaker similarity.

</p>
</details>

<details><summary><b>TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2107.05188">arxiv:2107.05188</a>
&#x1F4C8; 5 <br>
<p>Yao Chang, Hu Menghan, Zhai Guangtao, Zhang Xiao-Ping</p></summary>
<p>

**Abstract:** In recent years, computer-aided diagnosis has become an increasingly popular topic. Methods based on convolutional neural networks have achieved good performance in medical image segmentation and classification. Due to the limitations of the convolution operation, the long-term spatial features are often not accurately obtained. Hence, we propose a TransClaw U-Net network structure, which combines the convolution operation with the transformer operation in the encoding part. The convolution part is applied for extracting the shallow spatial features to facilitate the recovery of the image resolution after upsampling. The transformer part is used to encode the patches, and the self-attention mechanism is used to obtain global information between sequences. The decoding part retains the bottom upsampling structure for better detail segmentation performance. The experimental results on Synapse Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net is better than other network structures. The ablation experiments also prove the generalization performance of TransClaw U-Net.

</p>
</details>

<details><summary><b>Fairer Software Made Easier (using "Keys")</b>
<a href="https://arxiv.org/abs/2107.05088">arxiv:2107.05088</a>
&#x1F4C8; 4 <br>
<p>Tim Menzies, Kewen Peng, Andre Lustosa</p></summary>
<p>

**Abstract:** Can we simplify explanations for software analytics? Maybe. Recent results show that systems often exhibit a "keys effect"; i.e. a few key features control the rest. Just to say the obvious, for systems controlled by a few keys, explanation and control is just a matter of running a handful of "what-if" queries across the keys. By exploiting the keys effect, it should be possible to dramatically simplify even complex explanations, such as those required for ethical AI systems.

</p>
</details>

<details><summary><b>Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration</b>
<a href="https://arxiv.org/abs/2107.05080">arxiv:2107.05080</a>
&#x1F4C8; 4 <br>
<p>Xuan Kan, Hejie Cui, Carl Yang</p></summary>
<p>

**Abstract:** Relation prediction among entities in images is an important step in scene graph generation (SGG), which further impacts various visual understanding and reasoning tasks. Existing SGG frameworks, however, require heavy training yet are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we stress that such incapability is due to the lack of commonsense reasoning,i.e., the ability to associate similar entities and infer similar relations based on general understanding of the world. To fill this gap, we propose CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to integrate commonsense knowledge for SGG, especially for zero-shot relation prediction. Specifically, we develop novel graph mining pipelines to model the neighborhoods and paths around entities in an external commonsense knowledge graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive quantitative evaluations and qualitative case studies on both original and manipulated datasets from Visual Genome demonstrate the effectiveness of our proposed approach.

</p>
</details>

<details><summary><b>SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs</b>
<a href="https://arxiv.org/abs/2107.05074">arxiv:2107.05074</a>
&#x1F4C8; 4 <br>
<p>Satyen Kale, Ayush Sekhari, Karthik Sridharan</p></summary>
<p>

**Abstract:** Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the method of choice for learning with large over-parameterized models. A popular theory for explaining why SGD works well in practice is that the algorithm has an implicit regularization that biases its output towards a good solution. Perhaps the theoretically most well understood learning setting for SGD is that of Stochastic Convex Optimization (SCO), where it is well known that SGD learns at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this paper, we consider the problem of SCO and explore the role of implicit regularization, batch size and multiple epochs for SGD. Our main contributions are threefold:
  (a) We show that for any regularizer, there is an SCO problem for which Regularized Empirical Risk Minimzation fails to learn. This automatically rules out any implicit regularization based explanation for the success of SGD.
  (b) We provide a separation between SGD and learning via Gradient Descent on empirical loss (GD) in terms of sample complexity. We show that there is an SCO problem such that GD with any step size and number of iterations can only learn at a suboptimal rate: at least $\widetildeΩ(1/n^{5/12})$.
  (c) We present a multi-epoch variant of SGD commonly used in practice. We prove that this algorithm is at least as good as single pass SGD in the worst case. However, for certain SCO problems, taking multiple passes over the dataset can significantly outperform single pass SGD.
  We extend our results to the general learning setting by showing a problem which is learnable for any data distribution, and for this problem, SGD is strictly better than RERM for any regularization function. We conclude by discussing the implications of our results for deep learning, and show a separation between SGD and ERM for two layer diagonal neural networks.

</p>
</details>

<details><summary><b>Blending Pruning Criteria for Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2107.05033">arxiv:2107.05033</a>
&#x1F4C8; 4 <br>
<p>Wei He, Zhongzhan Huang, Mingfu Liang, Senwei Liang, Haizhao Yang</p></summary>
<p>

**Abstract:** The advancement of convolutional neural networks (CNNs) on various vision applications has attracted lots of attention. Yet the majority of CNNs are unable to satisfy the strict requirement for real-world deployment. To overcome this, the recent popular network pruning is an effective method to reduce the redundancy of the models. However, the ranking of filters according to their "importance" on different pruning criteria may be inconsistent. One filter could be important according to a certain criterion, while it is unnecessary according to another one, which indicates that each criterion is only a partial view of the comprehensive "importance". From this motivation, we propose a novel framework to integrate the existing filter pruning criteria by exploring the criteria diversity. The proposed framework contains two stages: Criteria Clustering and Filters Importance Calibration. First, we condense the pruning criteria via layerwise clustering based on the rank of "importance" score. Second, within each cluster, we propose a calibration factor to adjust their significance for each selected blending candidates and search for the optimal blending criterion via Evolutionary Algorithm. Quantitative results on the CIFAR-100 and ImageNet benchmarks show that our framework outperforms the state-of-the-art baselines, regrading to the compact model performance after pruning.

</p>
</details>

<details><summary><b>Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking</b>
<a href="https://arxiv.org/abs/2107.05002">arxiv:2107.05002</a>
&#x1F4C8; 4 <br>
<p>Gaochen Wu, Bin Xu, Yuxin Qin, Fei Kong, Bangchang Liu, Hongwen Zhao, Dejie Chang</p></summary>
<p>

**Abstract:** Extractive Reading Comprehension (ERC) has made tremendous advances enabled by the availability of large-scale high-quality ERC training data. Despite of such rapid progress and widespread application, the datasets in languages other than high-resource languages such as English remain scarce. To address this issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by modelling existing high-quality extractive reading comprehension datasets in a multilingual environment. To be specific, we present multilingual adaptive attention (MAA) to combine intra-attention and inter-attention to learn more general generalizable semantic and lexical knowledge from each pair of language families. Furthermore, to make full use of existing datasets, we adopt a new training framework to train our model by calculating task-level similarities between each existing dataset and target dataset. The experimental results show that our XLTT model surpasses six baselines on two multilingual ERC benchmarks, especially more effective for low-resource languages with 3.9 and 4.1 average improvement in F1 and EM, respectively.

</p>
</details>

<details><summary><b>A Deep-Bayesian Framework for Adaptive Speech Duration Modification</b>
<a href="https://arxiv.org/abs/2107.04973">arxiv:2107.04973</a>
&#x1F4C8; 4 <br>
<p>Ravi Shankar, Archana Venkataraman</p></summary>
<p>

**Abstract:** We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances. We train a masked convolutional encoder-decoder network to produce this attention map via a stochastic version of the mean absolute error loss function; our model also predicts the length of the target speech signal using the encoder embeddings. The predicted length determines the number of steps for the decoder operation. During inference, we generate the attention map as a proxy for the similarity matrix between the given input speech and an unknown target speech signal. Using this similarity matrix, we compute a warping path of alignment between the two signals. Our experiments demonstrate that this adaptive framework produces similar results to dynamic time warping, which relies on a known target signal, on both voice conversion and emotion conversion tasks. We also show that our technique results in a high quality of generated speech that is on par with state-of-the-art vocoders.

</p>
</details>

<details><summary><b>The Role of Social Movements, Coalitions, and Workers in Resisting Harmful Artificial Intelligence and Contributing to the Development of Responsible AI</b>
<a href="https://arxiv.org/abs/2107.14052">arxiv:2107.14052</a>
&#x1F4C8; 3 <br>
<p>Susan von Struensee</p></summary>
<p>

**Abstract:** There is mounting public concern over the influence that AI based systems has in our society. Coalitions in all sectors are acting worldwide to resist hamful applications of AI. From indigenous people addressing the lack of reliable data, to smart city stakeholders, to students protesting the academic relationships with sex trafficker and MIT donor Jeffery Epstein, the questionable ethics and values of those heavily investing in and profiting from AI are under global scrutiny. There are biased, wrongful, and disturbing assumptions embedded in AI algorithms that could get locked in without intervention. Our best human judgment is needed to contain AI's harmful impact. Perhaps one of the greatest contributions of AI will be to make us ultimately understand how important human wisdom truly is in life on earth.

</p>
</details>

<details><summary><b>Deep Transfer Learning Based Intrusion Detection System for Electric Vehicular Networks</b>
<a href="https://arxiv.org/abs/2107.05172">arxiv:2107.05172</a>
&#x1F4C8; 3 <br>
<p>Sk. Tanzir Mehedi, Adnan Anwar, Ziaur Rahman, Kawsar Ahmed</p></summary>
<p>

**Abstract:** The Controller Area Network (CAN) bus works as an important protocol in the real-time In-Vehicle Network (IVN) systems for its simple, suitable, and robust architecture. The risk of IVN devices has still been insecure and vulnerable due to the complex data-intensive architectures which greatly increase the accessibility to unauthorized networks and the possibility of various types of cyberattacks. Therefore, the detection of cyberattacks in IVN devices has become a growing interest. With the rapid development of IVNs and evolving threat types, the traditional machine learning-based IDS has to update to cope with the security requirements of the current environment. Nowadays, the progression of deep learning, deep transfer learning, and its impactful outcome in several areas has guided as an effective solution for network intrusion detection. This manuscript proposes a deep transfer learning-based IDS model for IVN along with improved performance in comparison to several other existing models. The unique contributions include effective attribute selection which is best suited to identify malicious CAN messages and accurately detect the normal and abnormal activities, designing a deep transfer learning-based LeNet model, and evaluating considering real-world data. To this end, an extensive experimental performance evaluation has been conducted. The architecture along with empirical analyses shows that the proposed IDS greatly improves the detection accuracy over the mainstream machine learning, deep learning, and benchmark deep transfer learning models and has demonstrated better performance for real-time IVN security.

</p>
</details>

<details><summary><b>Stateful Detection of Model Extraction Attacks</b>
<a href="https://arxiv.org/abs/2107.05166">arxiv:2107.05166</a>
&#x1F4C8; 3 <br>
<p>Soham Pal, Yash Gupta, Aditya Kanade, Shirish Shevade</p></summary>
<p>

**Abstract:** Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.

</p>
</details>

<details><summary><b>Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning</b>
<a href="https://arxiv.org/abs/2107.05127">arxiv:2107.05127</a>
&#x1F4C8; 3 <br>
<p>Muhammad Azmi Umer, Chuadhry Mujeeb Ahmed, Muhammad Taha Jilani, Aditya P. Mathur</p></summary>
<p>

**Abstract:** Adversarial learning is used to test the robustness of machine learning algorithms under attack and create attacks that deceive the anomaly detection methods in Industrial Control System (ICS). Given that security assessment of an ICS demands that an exhaustive set of possible attack patterns is studied, in this work, we propose an association rule mining-based attack generation technique. The technique has been implemented using data from a secure Water Treatment plant. The proposed technique was able to generate more than 300,000 attack patterns constituting a vast majority of new attack vectors which were not seen before. Automatically generated attacks improve our understanding of the potential attacks and enable the design of robust attack detection techniques.

</p>
</details>

<details><summary><b>Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising</b>
<a href="https://arxiv.org/abs/2107.05115">arxiv:2107.05115</a>
&#x1F4C8; 3 <br>
<p>Basit O. Alawode, Mudassir Masood, Tarig Ballal, Tareq Al-Naffouri</p></summary>
<p>

**Abstract:** In spite of the improvements achieved by the several denoising algorithms over the years, many of them still fail at preserving the fine details of the image after denoising. This is as a result of the smooth-out effect they have on the images. Most neural network-based algorithms have achieved better quantitative performance than the classical denoising algorithms. However, they also suffer from qualitative (visual) performance as a result of the smooth-out effect. In this paper, we propose an algorithm to address this shortcoming. We propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image denoising. This algorithm performs collaborative denoising of image patches in the sparse domain using a set of optimized neural network models. This results in a fast algorithm that is able to excellently obtain a trade-off between noise removal and details preservation. Extensive experiments show that the DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and qualitatively (visually) better than many of the state-of-the-art denoising algorithms.

</p>
</details>

<details><summary><b>Remote Blood Oxygen Estimation From Videos Using Neural Networks</b>
<a href="https://arxiv.org/abs/2107.05087">arxiv:2107.05087</a>
&#x1F4C8; 3 <br>
<p>Joshua Mathew, Xin Tian, Min Wu, Chau-Wai Wong</p></summary>
<p>

**Abstract:** Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance.

</p>
</details>

<details><summary><b>Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2107.05085">arxiv:2107.05085</a>
&#x1F4C8; 3 <br>
<p>Gorkem Polat, Yesim Dogrusoz Serinagaoglu, Ugur Halici</p></summary>
<p>

**Abstract:** Recent studies have shown that lung cancer screening using annual low-dose computed tomography (CT) reduces lung cancer mortality by 20% compared to traditional chest radiography. Therefore, CT lung screening has started to be used widely all across the world. However, analyzing these images is a serious burden for radiologists. The number of slices in a CT scan can be up to 600. Therefore, computer-aided-detection (CAD) systems are very important for faster and more accurate assessment of the data. In this study, we proposed a framework that analyzes CT lung screenings using convolutional neural networks (CNNs) to reduce false positives. We trained our model with different volume sizes and showed that volume size plays a critical role in the performance of the system. We also used different fusions in order to show their power and effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D convolutional operations applied to 3D data could result in information loss. The proposed framework has been tested on the dataset provided by the LUNA16 Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.

</p>
</details>

<details><summary><b>Locality Relationship Constrained Multi-view Clustering Framework</b>
<a href="https://arxiv.org/abs/2107.05073">arxiv:2107.05073</a>
&#x1F4C8; 3 <br>
<p>Xiangzhu Meng, Wei Wei, Wenzhe Liu</p></summary>
<p>

**Abstract:** In most practical applications, it's common to utilize multiple features from different views to represent one object. Among these works, multi-view subspace-based clustering has gained extensive attention from many researchers, which aims to provide clustering solutions to multi-view data. However, most existing methods fail to take full use of the locality geometric structure and similarity relationship among samples under the multi-view scenario. To solve these issues, we propose a novel multi-view learning method with locality relationship constraint to explore the problem of multi-view clustering, called Locality Relationship Constrained Multi-view Clustering Framework (LRC-MCF). LRC-MCF aims to explore the diversity, geometric, consensus and complementary information among different views, by capturing the locality relationship information and the common similarity relationships among multiple views. Moreover, LRC-MCF takes sufficient consideration to weights of different views in finding the common-view locality structure and straightforwardly produce the final clusters. To effectually reduce the redundancy of the learned representations, the low-rank constraint on the common similarity matrix is considered additionally. To solve the minimization problem of LRC-MCF, an Alternating Direction Minimization (ADM) method is provided to iteratively calculate all variables LRC-MCF. Extensive experimental results on seven benchmark multi-view datasets validate the effectiveness of the LRC-MCF method.

</p>
</details>

<details><summary><b>BCNet: A Deep Convolutional Neural Network for Breast Cancer Grading</b>
<a href="https://arxiv.org/abs/2107.05037">arxiv:2107.05037</a>
&#x1F4C8; 3 <br>
<p>Pouya Hallaj Zavareh, Atefeh Safayari, Hamidreza Bolhasani</p></summary>
<p>

**Abstract:** Breast cancer has become one of the most prevalent cancers by which people all over the world are affected and is posed serious threats to human beings, in a particular woman. In order to provide effective treatment or prevention of this cancer, disease diagnosis in the early stages would be of high importance. There have been various methods to detect this disorder in which using images have to play a dominant role. Deep learning has been recently adopted widely in different areas of science, especially medicine. In breast cancer detection problems, some diverse deep learning techniques have been developed on different datasets and resulted in good accuracy. In this article, we aimed to present a deep neural network model to classify histopathological images from the Databiox image dataset as the first application on this image database. Our proposed model named BCNet has taken advantage of the transfer learning approach in which VGG16 is selected from available pertained models as a feature extractor. Furthermore, to address the problem of insufficient data, we employed the data augmentation technique to expand the input dataset. All implementations in this research, ranging from pre-processing actions to depicting the diagram of the model architecture, have been carried out using tf.keras API. As a consequence of the proposed model execution, the significant validation accuracy of 88% and evaluation accuracy of 72% obtained.

</p>
</details>

<details><summary><b>Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training</b>
<a href="https://arxiv.org/abs/2107.05031">arxiv:2107.05031</a>
&#x1F4C8; 3 <br>
<p>Fangyuan Zhang, Tianxiang Pan, Bin Wang</p></summary>
<p>

**Abstract:** This study delves into semi-supervised object detection (SSOD) to improve detector performance with additional unlabeled data. State-of-the-art SSOD performance has been achieved recently by self-training, in which training supervision consists of ground truths and pseudo-labels. In current studies, we observe that class imbalance in SSOD severely impedes the effectiveness of self-training. To address the class imbalance, we propose adaptive class-rebalancing self-training (ACRST) with a novel memory module called CropBank. ACRST adaptively rebalances the training data with foreground instances extracted from the CropBank, thereby alleviating the class imbalance. Owing to the high complexity of detection tasks, we observe that both self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD. Therefore, we propose a novel two-stage filtering algorithm to generate accurate pseudo-labels. Our method achieves satisfactory improvements on MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP improvement compared with state-of-the-art methods.

</p>
</details>

<details><summary><b>Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper</b>
<a href="https://arxiv.org/abs/2107.05001">arxiv:2107.05001</a>
&#x1F4C8; 3 <br>
<p>Shami Nisimov, Yaniv Gurwicz, Raanan Y. Rohekar, Gal Novik</p></summary>
<p>

**Abstract:** Causal discovery from observational data is an important tool in many branches of science. Under certain assumptions it allows scientists to explain phenomena, predict, and make decisions. In the large sample limit, sound and complete causal discovery algorithms have been previously introduced, where a directed acyclic graph (DAG), or its equivalence class, representing causal relations is searched. However, in real-world cases, only finite training data is available, which limits the power of statistical tests used by these algorithms, leading to errors in the inferred causal model. This is commonly addressed by devising a strategy for using as few as possible statistical tests. In this paper, we introduce such a strategy in the form of a recursive wrapper for existing constraint-based causal discovery algorithms, which preserves soundness and completeness. It recursively clusters the observed variables using the normalized min-cut criterion from the outset, and uses a baseline causal discovery algorithm during backtracking for learning local sub-graphs. It then combines them and ensures completeness. By an ablation study, using synthetic data, and by common real-world benchmarks, we demonstrate that our approach requires significantly fewer statistical tests, learns more accurate graphs, and requires shorter run-times than the baseline algorithm.

</p>
</details>

<details><summary><b>Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates</b>
<a href="https://arxiv.org/abs/2107.04974">arxiv:2107.04974</a>
&#x1F4C8; 3 <br>
<p>Rose McDonald, Boris Kovalerchuk</p></summary>
<p>

**Abstract:** It is challenging for humans to enable visual knowledge discovery in data with more than 2-3 dimensions with a naked eye. This chapter explores the efficiency of discovering predictive machine learning models interactively using new Elliptic Paired coordinates (EPC) visualizations. It is shown that EPC are capable to visualize multidimensional data and support visual machine learning with preservation of multidimensional information in 2-D. Relative to parallel and radial coordinates, EPC visualization requires only a half of the visual elements for each n-D point. An interactive software system EllipseVis, which is developed in this work, processes high-dimensional datasets, creates EPC visualizations, and produces predictive classification models by discovering dominance rules in EPC. By using interactive and automatic processes it discovers zones in EPC with a high dominance of a single class. The EPC methodology has been successful in discovering non-linear predictive models with high coverage and precision in the computational experiments. This can benefit multiple domains by producing visually appealing dominance rules. This chapter presents results of successful testing the EPC non-linear methodology in experiments using real and simulated data, EPC generalized to the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of coordinates to optimize the visual discovery, introduction of an alternative EPC design and introduction of the concept of incompact machine learning methodology based on EPC/DEPC.

</p>
</details>

<details><summary><b>Internet-of-Things Devices and Assistive Technologies for Healthcare: Applications, Challenges, and Opportunities</b>
<a href="https://arxiv.org/abs/2107.14112">arxiv:2107.14112</a>
&#x1F4C8; 2 <br>
<p>Marc Jayson Baucas, Petros Spachos, Stefano Gregori</p></summary>
<p>

**Abstract:** Medical conditions and cases are growing at a rapid pace, where physical space is starting to be constrained. Hospitals and clinics no longer have the ability to accommodate large numbers of incoming patients. It is clear that the current state of the health industry needs to improve its valuable and limited resources. The evolution of the Internet of Things (IoT) devices along with assistive technologies can alleviate the problem in healthcare, by being a convenient and easy means of accessing healthcare services wirelessly. There is a plethora of IoT devices and potential applications that can take advantage of the unique characteristics that these technologies can offer. However, at the same time, these services pose novel challenges that need to be properly addressed. In this article, we review some popular categories of IoT-based applications for healthcare along with their devices. Then, we describe the challenges and discuss how research can properly address the open issues and improve the already existing implementations in healthcare. Further possible solutions are also discussed to show their potential in being viable solutions for future healthcare applications

</p>
</details>

<details><summary><b>A Systematic Literature Review of Automated ICD Coding and Classification Systems using Discharge Summaries</b>
<a href="https://arxiv.org/abs/2107.10652">arxiv:2107.10652</a>
&#x1F4C8; 2 <br>
<p>Rajvir Kaur, Jeewani Anupama Ginige, Oliver Obst</p></summary>
<p>

**Abstract:** Codification of free-text clinical narratives have long been recognised to be beneficial for secondary uses such as funding, insurance claim processing and research. The current scenario of assigning codes is a manual process which is very expensive, time-consuming and error prone. In recent years, many researchers have studied the use of Natural Language Processing (NLP), related Machine Learning (ML) and Deep Learning (DL) methods and techniques to resolve the problem of manual coding of clinical narratives and to assist human coders to assign clinical codes more accurately and efficiently. This systematic literature review provides a comprehensive overview of automated clinical coding systems that utilises appropriate NLP, ML and DL methods and techniques to assign ICD codes to discharge summaries. We have followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and conducted a comprehensive search of publications from January, 2010 to December 2020 in four academic databases- PubMed, ScienceDirect, Association for Computing Machinery(ACM) Digital Library, and the Association for Computational Linguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the inclusion criteria. This review identified: datasets having discharge summaries; NLP techniques along with some other data extraction processes, different feature extraction and embedding techniques. To measure the performance of classification methods, different evaluation metrics are used. Lastly, future research directions are provided to scholars who are interested in automated ICD code assignment. Efforts are still required to improve ICD code prediction accuracy, availability of large-scale de-identified clinical corpora with the latest version of the classification system. This can be a platform to guide and share knowledge with the less experienced coders and researchers.

</p>
</details>

<details><summary><b>DiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual Network for the DiCOVA Challenge 2021</b>
<a href="https://arxiv.org/abs/2107.06126">arxiv:2107.06126</a>
&#x1F4C8; 2 <br>
<p>Jiangeng Chang, Shaoze Cui, Mengling Feng</p></summary>
<p>

**Abstract:** In this paper, we propose a deep residual network-based method, namely the DiCOVA-Net, to identify COVID-19 infected patients based on the acoustic recording of their coughs. Since there are far more healthy people than infected patients, this classification problem faces the challenge of imbalanced data. To improve the model's ability to recognize minority class (the infected patients), we introduce data augmentation and cost-sensitive methods into our model. Besides, considering the particularity of this task, we deploy some fine-tuning techniques to adjust the pre-training ResNet50. Furthermore, to improve the model's generalizability, we use ensemble learning to integrate prediction results from multiple base classifiers generated using different random seeds. To evaluate the proposed DiCOVA-Net's performance, we conducted experiments with the DiCOVA challenge dataset. The results show that our method has achieved 85.43\% in AUC, among the top of all competing teams.

</p>
</details>

<details><summary><b>Functional Magnetic Resonance Imaging data augmentation through conditional ICA</b>
<a href="https://arxiv.org/abs/2107.06104">arxiv:2107.06104</a>
&#x1F4C8; 2 <br>
<p>Badr Tajini, Hugo Richard, Bertrand Thirion</p></summary>
<p>

**Abstract:** Advances in computational cognitive neuroimaging research are related to the availability of large amounts of labeled brain imaging data, but such data are scarce and expensive to generate. While powerful data generation mechanisms, such as Generative Adversarial Networks (GANs), have been designed in the last decade for computer vision, such improvements have not yet carried over to brain imaging. A likely reason is that GANs training is ill-suited to the noisy, high-dimensional and small-sample data available in functional neuroimaging. In this paper, we introduce Conditional Independent Components Analysis (Conditional ICA): a fast functional Magnetic Resonance Imaging (fMRI) data augmentation technique, that leverages abundant resting-state data to create images by sampling from an ICA decomposition. We then propose a mechanism to condition the generator on classes observed with few samples. We first show that the generative mechanism is successful at synthesizing data indistinguishable from observations, and that it yields gains in classification accuracy in brain decoding problems. In particular it outperforms GANs while being much easier to optimize and interpret. Lastly, Conditional ICA enhances classification accuracy in eight datasets without further parameters tuning.

</p>
</details>

<details><summary><b>Pattern Discovery and Validation Using Scientific Research Methods</b>
<a href="https://arxiv.org/abs/2107.06065">arxiv:2107.06065</a>
&#x1F4C8; 2 <br>
<p>Dirk Riehle, Nikolay Harutyunyan, Ann Barcomb</p></summary>
<p>

**Abstract:** Pattern discovery, the process of discovering previously unrecognized patterns, is often performed as an ad-hoc process with little resulting certainty in the quality of the proposed patterns. Pattern validation, the process of validating the accuracy of proposed patterns, remains dominated by the simple heuristic of "the rule of three". This article shows how to use established scientific research methods for the purpose of pattern discovery and validation. We present a specific approach, called the handbook method, that uses the qualitative survey, action research, and case study research for pattern discovery and evaluation, and we discuss the underlying principle of using scientific methods in general. We evaluate the handbook method using three exploratory studies and demonstrate its usefulness.

</p>
</details>

<details><summary><b>Deep-learning-based Hyperspectral imaging through a RGB camera</b>
<a href="https://arxiv.org/abs/2107.05190">arxiv:2107.05190</a>
&#x1F4C8; 2 <br>
<p>Xinyu Gao, Tianlang Wang, Jing Yang, Jinchao Tao, Yanqing Qiu, Yanlong Meng, Banging Mao, Pengwei Zhou, Yi Li</p></summary>
<p>

**Abstract:** Hyperspectral image (HSI) contains both spatial pattern and spectral information which has been widely used in food safety, remote sensing, and medical detection. However, the acquisition of hyperspectral images is usually costly due to the complicated apparatus for the acquisition of optical spectrum. Recently, it has been reported that HSI can be reconstructed from single RGB image using convolution neural network (CNN) algorithms. Compared with the traditional hyperspectral cameras, the method based on CNN algorithms is simple, portable and low cost. In this study, we focused on the influence of the RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated with a monochromator were used as the standard light source to calibrate the CSS. And the experimental results show that the CSS plays a significant role in the reconstruction accuracy of an HSI. In addition, we proposed a new HSI reconstruction network where the dimensional structure of the original hyperspectral datacube was modified by 3D matrix transpose to improve the reconstruction accuracy.

</p>
</details>

<details><summary><b>MOOCRep: A Unified Pre-trained Embedding of MOOC Entities</b>
<a href="https://arxiv.org/abs/2107.05154">arxiv:2107.05154</a>
&#x1F4C8; 2 <br>
<p>Shalini Pandey, Jaideep Srivastava</p></summary>
<p>

**Abstract:** Many machine learning models have been built to tackle information overload issues on Massive Open Online Courses (MOOC) platforms. These models rely on learning powerful representations of MOOC entities. However, they suffer from the problem of scarce expert label data. To overcome this problem, we propose to learn pre-trained representations of MOOC entities using abundant unlabeled data from the structure of MOOCs which can directly be applied to the downstream tasks. While existing pre-training methods have been successful in NLP areas as they learn powerful textual representation, their models do not leverage the richer information about MOOC entities. This richer information includes the graph relationship between the lectures, concepts, and courses along with the domain knowledge about the complexity of a concept. We develop MOOCRep, a novel method based on Transformer language model trained with two pre-training objectives : 1) graph-based objective to capture the powerful signal of entities and relations that exist in the graph, and 2) domain-oriented objective to effectively incorporate the complexity level of concepts. Our experiments reveal that MOOCRep's embeddings outperform state-of-the-art representation learning methods on two tasks important for education community, concept pre-requisite prediction and lecture recommendation.

</p>
</details>

<details><summary><b>Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks</b>
<a href="https://arxiv.org/abs/2107.05134">arxiv:2107.05134</a>
&#x1F4C8; 2 <br>
<p>Carles Domingo-Enrich, Alberto Bietti, Marylou Gabrié, Joan Bruna, Eric Vanden-Eijnden</p></summary>
<p>

**Abstract:** Energy-based models (EBMs) are generative models that are usually trained via maximum likelihood estimation. This approach becomes challenging in generic situations where the trained energy is nonconvex, due to the need to sample the Gibbs distribution associated with this energy. Using general Fenchel duality results, we derive variational principles dual to maximum likelihood EBMs with shallow overparametrized neural network energies, both in the active (aka feature-learning) and lazy regimes. In the active regime, this dual formulation leads to a training algorithm in which one updates concurrently the particles in the sample space and the neurons in the parameter space of the energy. We also consider a variant of this algorithm in which the particles are sometimes restarted at random samples drawn from the data set, and show that performing these restarts at every iteration step corresponds to score matching training. Using intermediate parameter setups in our dual algorithm thereby gives a way to interpolate between maximum likelihood and score matching training. These results are illustrated in simple numerical experiments.

</p>
</details>

<details><summary><b>Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation</b>
<a href="https://arxiv.org/abs/2107.05124">arxiv:2107.05124</a>
&#x1F4C8; 2 <br>
<p>Gabriel de Souza P. Moreira, Sara Rabhi, Ronay Ak, Md Yasin Kabir, Even Oldridge</p></summary>
<p>

**Abstract:** Session-based recommendation is an important task for e-commerce services, where a large number of users browse anonymously or may have very distinct interests for different sessions. In this paper we present one of the winning solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce Data Challenge. Our solution was inspired by NLP techniques and consists of an ensemble of two Transformer architectures - Transformer-XL and XLNet - trained with autoregressive and autoencoding approaches. To leverage most of the rich dataset made available for the competition, we describe how we prepared multi-model features by combining tabular events with textual and image vectors. We also present a model prediction analysis to better understand the effectiveness of our architectures for the session-based recommendation.

</p>
</details>

<details><summary><b>eGHWT: The extended Generalized Haar-Walsh Transform</b>
<a href="https://arxiv.org/abs/2107.05121">arxiv:2107.05121</a>
&#x1F4C8; 2 <br>
<p>Naoki Saito, Yiqun Shao</p></summary>
<p>

**Abstract:** Extending computational harmonic analysis tools from the classical setting of regular lattices to the more general setting of graphs and networks is very important and much research has been done recently. The Generalized Haar-Walsh Transform (GHWT) developed by Irion and Saito (2014) is a multiscale transform for signals on graphs, which is a generalization of the classical Haar and Walsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh Transform (eGHWT), which is a generalization of the adapted time-frequency tilings of Thiele and Villemoes (1996). The eGHWT examines not only the efficiency of graph-domain partitions but also that of "sequency-domain" partitions simultaneously. Consequently, the eGHWT and its associated best-basis selection algorithm for graph signals significantly improve the performance of the previous GHWT with the similar computational cost, $O(N \log N)$, where $N$ is the number of nodes of an input graph. While the GHWT best-basis algorithm seeks the most suitable orthonormal basis for a given task among more than $(1.5)^N$ possible orthonormal bases in $\mathbb{R}^N$, the eGHWT best-basis algorithm can find a better one by searching through more than $0.618\cdot(1.84)^N$ possible orthonormal bases in $\mathbb{R}^N$. This article describes the details of the eGHWT best-basis algorithm and demonstrates its superiority using several examples including genuine graph signals as well as conventional digital images viewed as graph signals. Furthermore, we also show how the eGHWT can be extended to 2D signals and matrix-form data by viewing them as a tensor product of graphs generated from their columns and rows and demonstrate its effectiveness on applications such as image approximation.

</p>
</details>

<details><summary><b>Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective</b>
<a href="https://arxiv.org/abs/2107.05101">arxiv:2107.05101</a>
&#x1F4C8; 2 <br>
<p>Racine Ly</p></summary>
<p>

**Abstract:** The improvement of computers' capacities, advancements in algorithmic techniques, and the significant increase of available data have enabled the recent developments of Artificial Intelligence (AI) technology. One of its branches, called Machine Learning (ML), has shown strong capacities in mimicking characteristics attributed to human intelligence, such as vision, speech, and problem-solving. However, as previous technological revolutions suggest, their most significant impacts could be mostly expected on other sectors that were not traditional users of that technology. The agricultural sector is vital for African economies; improving yields, mitigating losses, and effective management of natural resources are crucial in a climate change era. Machine Learning is a technology with an added value in making predictions, hence the potential to reduce uncertainties and risk across sectors, in this case, the agricultural sector. The purpose of this paper is to contextualize and discuss barriers to ML-based solutions for African agriculture. In the second section, we provided an overview of ML technology from a historical and technical perspective and its main driving force. In the third section, we provided a brief review of the current use of ML in agriculture. Finally, in section 4, we discuss ML growing interest in Africa and the potential barriers to creating and using ML-based solutions in the agricultural sector.

</p>
</details>

<details><summary><b>Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process</b>
<a href="https://arxiv.org/abs/2107.05071">arxiv:2107.05071</a>
&#x1F4C8; 2 <br>
<p>Yunsong Xie, Ryan Stearrett</p></summary>
<p>

**Abstract:** A cross-benchmark has been done on three critical aspects, data imputing, feature selection and regression algorithms, for machine learning based chemical vapor deposition (CVD) virtual metrology (VM). The result reveals that linear feature selection regression algorithm would extensively under-fit the VM data. Data imputing is also necessary to achieve a higher prediction accuracy as the data availability is only ~70% when optimal accuracy is obtained. This work suggests a nonlinear feature selection and regression algorithm combined with nearest data imputing algorithm would provide a prediction accuracy as high as 0.7. This would lead to 70% reduced CVD processing variation, which is believed to will lead to reduced frequency of physical metrology as well as more reliable mass-produced wafer with improved quality.

</p>
</details>

<details><summary><b>NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection</b>
<a href="https://arxiv.org/abs/2107.05023">arxiv:2107.05023</a>
&#x1F4C8; 2 <br>
<p>Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao Van Long, Tran Quang Trung, Nguyen Thi Thuy, Dinh Viet Sang</p></summary>
<p>

**Abstract:** Automatic polyp segmentation has proven to be immensely helpful for endoscopy procedures, reducing the missing rate of adenoma detection for endoscopists while increasing efficiency. However, classifying a polyp as being neoplasm or not and segmenting it at the pixel level is still a challenging task for doctors to perform in a limited time. In this work, we propose a fine-grained formulation for the polyp segmentation problem. Our formulation aims to not only segment polyp regions, but also identify those at high risk of malignancy with high accuracy. In addition, we present a UNet-based neural network architecture called NeoUNet, along with a hybrid loss function to solve this problem. Experiments show highly competitive results for NeoUNet on our benchmark dataset compared to existing polyp segmentation models.

</p>
</details>

<details><summary><b>Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results</b>
<a href="https://arxiv.org/abs/2107.04982">arxiv:2107.04982</a>
&#x1F4C8; 2 <br>
<p>Mohamad H Danesh, Alan Fern</p></summary>
<p>

**Abstract:** We study the problem of out-of-distribution dynamics (OODD) detection, which involves detecting when the dynamics of a temporal process change compared to the training-distribution dynamics. This is relevant to applications in control, reinforcement learning (RL), and multi-variate time-series, where changes to test time dynamics can impact the performance of learning controllers/predictors in unknown ways. This problem is particularly important in the context of deep RL, where learned controllers often overfit to the training environment. Currently, however, there is a lack of established OODD benchmarks for the types of environments commonly used in RL research. Our first contribution is to design a set of OODD benchmarks derived from common RL environments with varying types and intensities of OODD. Our second contribution is to design a strong OODD baseline approach based on recurrent implicit quantile networks (RIQNs), which monitors autoregressive prediction errors for OODD detection. Our final contribution is to evaluate the RIQN approach on the benchmarks to provide baseline results for future comparison.

</p>
</details>

<details><summary><b>Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning</b>
<a href="https://arxiv.org/abs/2107.04971">arxiv:2107.04971</a>
&#x1F4C8; 2 <br>
<p>Sridevi Narayana Wagle, Boris Kovalerchuk</p></summary>
<p>

**Abstract:** Machine learning algorithms often produce models considered as complex black-box models by both end users and developers. They fail to explain the model in terms of the domain they are designed for. The proposed Iterative Visual Logical Classifier (IVLC) is an interpretable machine learning algorithm that allows end users to design a model and classify data with more confidence and without having to compromise on the accuracy. Such technique is especially helpful when dealing with sensitive and crucial data like cancer data in the medical domain with high cost of errors. With the help of the proposed interactive and lossless multidimensional visualization, end users can identify the pattern in the data based on which they can make explainable decisions. Such options would not be possible in black box machine learning methodologies. The interpretable IVLC algorithm is supported by the Interactive Shifted Paired Coordinates Software System (SPCVis). It is a lossless multidimensional data visualization system with user interactive features. The interactive approach provides flexibility to the end user to perform data classification as self-service without having to rely on a machine learning expert. Interactive pattern discovery becomes challenging while dealing with large data sets with hundreds of dimensions/features. To overcome this problem, this chapter proposes an automated classification approach combined with new Coordinate Order Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm automatically generates the coordinate pair sequences that best represent the data separation and the genetic algorithm helps optimizing the proposed IVLC algorithm by automatically generating the areas for data classification. The feasibility of the approach is shown by experiments on benchmark datasets covering both interactive and automated processes used for data classification.

</p>
</details>

<details><summary><b>Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks</b>
<a href="https://arxiv.org/abs/2107.10302">arxiv:2107.10302</a>
&#x1F4C8; 1 <br>
<p>Kendra Albert, Maggie Delano, Bogdan Kulynych, Ram Shankar Siva Kumar</p></summary>
<p>

**Abstract:** Attacks from adversarial machine learning (ML) have the potential to be used "for good": they can be used to run counter to the existing power structures within ML, creating breathing space for those who would otherwise be the targets of surveillance and control. But most research on adversarial ML has not engaged in developing tools for resistance against ML systems. Why? In this paper, we review the broader impact statements that adversarial ML researchers wrote as part of their NeurIPS 2020 papers and assess the assumptions that authors have about the goals of their work. We also collect information about how authors view their work's impact more generally. We find that most adversarial ML researchers at NeurIPS hold two fundamental assumptions that will make it difficult for them to consider socially beneficial uses of attacks: (1) it is desirable to make systems robust, independent of context, and (2) attackers of systems are normatively bad and defenders of systems are normatively good. That is, despite their expressed and supposed neutrality, most adversarial ML researchers believe that the goal of their work is to secure systems, making it difficult to conceptualize and build tools for disrupting the status quo.

</p>
</details>

<details><summary><b>Hybrid Ant Swarm-Based Data Clustering</b>
<a href="https://arxiv.org/abs/2107.07382">arxiv:2107.07382</a>
&#x1F4C8; 1 <br>
<p>Md Ali Azam, Abir Hossen, Md Hafizur Rahman</p></summary>
<p>

**Abstract:** Biologically inspired computing techniques are very effective and useful in many areas of research including data clustering. Ant clustering algorithm is a nature-inspired clustering technique which is extensively studied for over two decades. In this study, we extend the ant clustering algorithm (ACA) to a hybrid ant clustering algorithm (hACA). Specifically, we include a genetic algorithm in standard ACA to extend the hybrid algorithm for better performance. We also introduced novel pick up and drop off rules to speed up the clustering performance. We study the performance of the hACA algorithm and compare with standard ACA as a benchmark.

</p>
</details>

<details><summary><b>Spectro-Temporal RF Identification using Deep Learning</b>
<a href="https://arxiv.org/abs/2107.05114">arxiv:2107.05114</a>
&#x1F4C8; 1 <br>
<p>Hai N. Nguyen, Marinos Vomvas, Triet Vo-Huu, Guevara Noubir</p></summary>
<p>

**Abstract:** RF emissions detection, classification, and spectro-temporal localization are crucial not only for tasks relating to understanding, managing, and protecting the RF spectrum, but also for safety and security applications such as detecting intruding drones or jammers. Achieving this goal for wideband spectrum and in real-time performance is a challenging problem. We present WRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal detection, framework and system. Our resulting deep learning model is capable to detect, classify, and precisely locate RF emissions in time and frequency using RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q streams). Such capabilities are made feasible by leveraging a deep-learning based one-stage object detection framework, and transfer learning to a multi-channel image-based RF signals representation. We also introduce an iterative training approach which leverages synthesized and augmented RF data to efficiently build large labelled datasets of RF emissions (SPREAD). WRIST detector achieves 90 mean Average Precision even in extremely congested environment in the wild. WRIST model classifies five technologies (Bluetooth, Lightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are making our curated and annotated dataset available to the whole community. It consists of nearly 1 million fully labelled RF emissions collected from various off-the-shelf wireless radios in a range of environments and spanning the five classes of emissions.

</p>
</details>

<details><summary><b>QoS Prediction for 5G Connected and Automated Driving</b>
<a href="https://arxiv.org/abs/2107.05000">arxiv:2107.05000</a>
&#x1F4C8; 1 <br>
<p>Apostolos Kousaridas, Ramya Panthangi Manjunath, Jose Mauricio Perdomo, Chan Zhou, Ernst Zielinski, Steffen Schmitz, Andreas Pfadler</p></summary>
<p>

**Abstract:** 5G communication system can support the demanding quality-of-service (QoS) requirements of many advanced vehicle-to-everything (V2X) use cases. However, the safe and efficient driving, especially of automated vehicles, may be affected by sudden changes of the provided QoS. For that reason, the prediction of the QoS changes and the early notification of these predicted changes to the vehicles have been recently enabled by 5G communication systems. This solution enables the vehicles to avoid or mitigate the effect of sudden QoS changes at the application level. This article describes how QoS prediction could be generated by a 5G communication system and delivered to a V2X application. The tele-operated driving use case is used as an example to analyze the feasibility of a QoS prediction scheme. Useful recommendations for the development of a QoS prediction solution are provided, while open research topics are identified.

</p>
</details>

<details><summary><b>Leveraging Domain Adaptation for Low-Resource Geospatial Machine Learning</b>
<a href="https://arxiv.org/abs/2107.04983">arxiv:2107.04983</a>
&#x1F4C8; 1 <br>
<p>Jack Lynch, Sam Wookey</p></summary>
<p>

**Abstract:** Machine learning in remote sensing has matured alongside a proliferation in availability and resolution of geospatial imagery, but its utility is bottlenecked by the need for labeled data. What's more, many labeled geospatial datasets are specific to certain regions, instruments, or extreme weather events. We investigate the application of modern domain-adaptation to multiple proposed geospatial benchmarks, uncovering unique challenges and proposing solutions to them.

</p>
</details>

<details><summary><b>AoI-minimizing Scheduling in UAV-relayed IoT Networks</b>
<a href="https://arxiv.org/abs/2107.05181">arxiv:2107.05181</a>
&#x1F4C8; 0 <br>
<p>Biplav Choudhury, Vijay K. Shah, Aidin Ferdowsi, Jeffrey H. Reed, Y. Thomas Hou</p></summary>
<p>

**Abstract:** Due to flexibility, autonomy and low operational cost, unmanned aerial vehicles (UAVs), as fixed aerial base stations, are increasingly being used as \textit{relays} to collect time-sensitive information (i.e., status updates) from IoT devices and deliver it to the nearby terrestrial base station (TBS), where the information gets processed. In order to ensure timely delivery of information to the TBS (from all IoT devices), optimal scheduling of time-sensitive information over two hop UAV-relayed IoT networks (i.e., IoT device to the UAV [hop 1], and UAV to the TBS [hop 2]) becomes a critical challenge. To address this, we propose scheduling policies for Age of Information (AoI) minimization in such two-hop UAV-relayed IoT networks. To this end, we present a low-complexity MAF-MAD scheduler, that employs Maximum AoI First (MAF) policy for sampling of IoT devices at UAV (hop 1) and Maximum AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS (hop 2). We show that MAF-MAD is the optimal scheduler under ideal conditions, i.e., error-free channels and generate-at-will traffic generation at IoT devices. On the contrary, for realistic conditions, we propose a Deep-Q-Networks (DQN) based scheduler. Our simulation results show that DQN-based scheduler outperforms MAF-MAD scheduler and three other baseline schedulers, i.e., Maximal AoI First (MAF), Round Robin (RR) and Random, employed at both hops under general conditions when the network is small (with 10's of IoT devices). However, it does not scale well with network size whereas MAF-MAD outperforms all other schedulers under all considered scenarios for larger networks.

</p>
</details>


[Next Page]({{ '/2021/07/10/2021.07.10.html' | relative_url }})
