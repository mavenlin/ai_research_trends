Prev: [2021.12.25]({{ '/2021/12/25/2021.12.25.html' | relative_url }})  Next: [2021.12.27]({{ '/2021/12/27/2021.12.27.html' | relative_url }})
{% raw %}
## Summary for 2021-12-26, created on 2022-01-05


<details><summary><b>The Statistical Complexity of Interactive Decision Making</b>
<a href="https://arxiv.org/abs/2112.13487">arxiv:2112.13487</a>
&#x1F4C8; 97 <br>
<p>Dylan J. Foster, Sham M. Kakade, Jian Qian, Alexander Rakhlin</p></summary>
<p>

**Abstract:** A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide:
  1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit.
  2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which transforms any algorithm for supervised estimation into an online algorithm for decision making. E2D attains a regret bound matching our lower bound, thereby achieving optimal sample-efficient learning as characterized by the Decision-Estimation Coefficient.
  Taken together, these results constitute a theory of learnability for interactive decision making. When applied to reinforcement learning settings, the Decision-Estimation Coefficient recovers essentially all existing hardness results and lower bounds. More broadly, the approach can be viewed as a decision-theoretic analogue of the classical Le Cam theory of statistical estimation; it also unifies a number of existing approaches -- both Bayesian and frequentist.

</p>
</details>

<details><summary><b>Itô-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives</b>
<a href="https://arxiv.org/abs/2112.13339">arxiv:2112.13339</a>
&#x1F4C8; 49 <br>
<p>Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, Yotaro Watanabe</p></summary>
<p>

**Abstract:** Denoising Diffusion Probabilistic Models (DDPMs) have been attracting attention recently as a new challenger to popular deep neural generative models including GAN, VAE, etc. However, DDPMs have a disadvantage that they often require a huge number of refinement steps during the synthesis. To address this problem, this paper proposes a new DDPM sampler based on a second-order numerical scheme for stochastic differential equations (SDEs), while the conventional sampler is based on a first-order numerical scheme. In general, it is not easy to compute the derivatives that are required in higher-order numerical schemes. However, in the case of DDPM, this difficulty is alleviated by the trick which the authors call "ideal derivative substitution". The newly derived higher-order sampler was applied to both image and speech generation tasks, and it is experimentally observed that the proposed sampler could synthesize plausible images and audio signals in relatively smaller number of refinement steps.

</p>
</details>

<details><summary><b>Acoustic scene classification using auditory datasets</b>
<a href="https://arxiv.org/abs/2112.13450">arxiv:2112.13450</a>
&#x1F4C8; 9 <br>
<p>Jayesh Kumpawat, Shubhajit Dey</p></summary>
<p>

**Abstract:** The approach used not only challenges some of the fundamental mathematical techniques used so far in early experiments of the same trend but also introduces new scopes and new horizons for interesting results. The physics governing spectrograms have been optimized in the project along with exploring how it handles the intense requirements of the problem at hand. Major contributions and developments brought under the light, through this project involve using better mathematical techniques and problem-specific machine learning methods. Improvised data analysis and data augmentation for audio datasets like frequency masking and random frequency-time stretching are used in the project and hence are explained in this paper. In the used methodology, the audio transforms principle were also tried and explored, and indeed the insights gained were used constructively in the later stages of the project. Using a deep learning principle is surely one of them. Also, in this paper, the potential scopes and upcoming research openings in both short and long term tunnel of time has been presented. Although much of the results gained are domain-specific as of now, they are surely potent enough to produce novel solutions in various different domains of diverse backgrounds.

</p>
</details>

<details><summary><b>Novel Dual-Channel Long Short-Term Memory Compressed Capsule Networks for Emotion Recognition</b>
<a href="https://arxiv.org/abs/2112.13350">arxiv:2112.13350</a>
&#x1F4C8; 6 <br>
<p>Ismail Shahin, Noor Hindawi, Ali Bou Nassif, Adi Alhudhaif, Kemal Polat</p></summary>
<p>

**Abstract:** Recent analysis on speech emotion recognition has made considerable advances with the use of MFCCs spectrogram features and the implementation of neural network approaches such as convolutional neural networks (CNNs). Capsule networks (CapsNet) have gained gratitude as alternatives to CNNs with their larger capacities for hierarchical representation. To address these issues, this research introduces a text-independent and speaker-independent SER novel architecture, where a dual-channel long short-term memory compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features of CapsNet. Our proposed novel classifier can ensure the energy efficiency of the model and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet. Moreover, the grid search approach is used to attain optimal solutions. Results witnessed an improved performance and reduction in the training and testing running time. The speech datasets used to evaluate our algorithm are: Arabic Emirati-accented corpus, English speech under simulated and actual stress corpus, English Ryerson audio-visual database of emotional speech and song corpus, and crowd-sourced emotional multimodal actors dataset. This work reveals that the optimum feature extraction method compared to other known methods is MFCCs delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers, CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our results demonstrate that the proposed work yields average emotion recognition accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and 31.9% based on CapsNet, CNN, support vector machine, multi-layer perceptron, k-nearest neighbor, radial basis function, and naive Bayes, respectively.

</p>
</details>

<details><summary><b>Delivery Issues Identification from Customer Feedback Data</b>
<a href="https://arxiv.org/abs/2112.13372">arxiv:2112.13372</a>
&#x1F4C8; 5 <br>
<p>Ankush Chopra, Mahima Arora, Shubham Pandey</p></summary>
<p>

**Abstract:** Millions of packages are delivered successfully by online and local retail stores across the world every day. The proper delivery of packages is needed to ensure high customer satisfaction and repeat purchases. These deliveries suffer various problems despite the best efforts from the stores. These issues happen not only due to the large volume and high demand for low turnaround time but also due to mechanical operations and natural factors. These issues range from receiving wrong items in the package to delayed shipment to damaged packages because of mishandling during transportation. Finding solutions to various delivery issues faced by both sending and receiving parties plays a vital role in increasing the efficiency of the entire process. This paper shows how to find these issues using customer feedback from the text comments and uploaded images. We used transfer learning for both Text and Image models to minimize the demand for thousands of labeled examples. The results show that the model can find different issues. Furthermore, it can also be used for tasks like bottleneck identification, process improvement, automating refunds, etc. Compared with the existing process, the ensemble of text and image models proposed in this paper ensures the identification of several types of delivery issues, which is more suitable for the real-life scenarios of delivery of items in retail businesses. This method can supply a new idea of issue detection for the delivery of packages in similar industries.

</p>
</details>

<details><summary><b>AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms</b>
<a href="https://arxiv.org/abs/2112.13366">arxiv:2112.13366</a>
&#x1F4C8; 5 <br>
<p>Albert Podusenko, Bart van Erp, Magnus Koudahl, Bert de Vries</p></summary>
<p>

**Abstract:** In this paper we present AIDA, which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of AIDA is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with their HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that AIDA comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our GitHub repository.

</p>
</details>

<details><summary><b>Budget Sensitive Reannotation of Noisy Relation Classification Data Using Label Hierarchy</b>
<a href="https://arxiv.org/abs/2112.13320">arxiv:2112.13320</a>
&#x1F4C8; 5 <br>
<p>Akshay Parekh, Ashish Anand, Amit Awekar</p></summary>
<p>

**Abstract:** Large crowd-sourced datasets are often noisy and relation classification (RC) datasets are no exception. Reannotating the entire dataset is one probable solution however it is not always viable due to time and budget constraints. This paper addresses the problem of efficient reannotation of a large noisy dataset for the RC. Our goal is to catch more annotation errors in the dataset while reannotating fewer instances. Existing work on RC dataset reannotation lacks the flexibility about how much data to reannotate. We introduce the concept of a reannotation budget to overcome this limitation. The immediate follow-up problem is: Given a specific reannotation budget, which subset of the data should we reannotate? To address this problem, we present two strategies to selectively reannotate RC datasets. Our strategies utilize the taxonomic hierarchy of relation labels. The intuition of our work is to rely on the graph distance between actual and predicted relation labels in the label hierarchy graph. We evaluate our reannotation strategies on the well-known TACRED dataset. We design our experiments to answer three specific research questions. First, does our strategy select novel candidates for reannotation? Second, for a given reannotation budget is our reannotation strategy more efficient at catching annotation errors? Third, what is the impact of data reannotation on RC model performance measurement? Experimental results show that our both reannotation strategies are novel and efficient. Our analysis indicates that the current reported performance of RC models on noisy TACRED data is inflated.

</p>
</details>

<details><summary><b>Automatic Configuration for Optimal Communication Scheduling in DNN Training</b>
<a href="https://arxiv.org/abs/2112.13509">arxiv:2112.13509</a>
&#x1F4C8; 4 <br>
<p>Yiqing Ma, Hao Wang, Yiming Zhang, Kai Chen</p></summary>
<p>

**Abstract:** ByteScheduler partitions and rearranges tensor transmissions to improve the communication efficiency of distributed Deep Neural Network (DNN) training. The configuration of hyper-parameters (i.e., the partition size and the credit size) is critical to the effectiveness of partitioning and rearrangement. Currently, ByteScheduler adopts Bayesian Optimization (BO) to find the optimal configuration for the hyper-parameters beforehand. In practice, however, various runtime factors (e.g., worker node status and network conditions) change over time, making the statically-determined one-shot configuration result suboptimal for real-world DNN training. To address this problem, we present a real-time configuration method (called AutoByte) that automatically and timely searches the optimal hyper-parameters as the training systems dynamically change. AutoByte extends the ByteScheduler framework with a meta-network, which takes the system's runtime statistics as its input and outputs predictions for speedups under specific configurations. Evaluation results on various DNN models show that AutoByte can dynamically tune the hyper-parameters with low resource usage, and deliver up to 33.2\% higher performance than the best static configuration in ByteScheduler.

</p>
</details>

<details><summary><b>PreDisM: Pre-Disaster Modelling With CNN Ensembles for At-Risk Communities</b>
<a href="https://arxiv.org/abs/2112.13465">arxiv:2112.13465</a>
&#x1F4C8; 4 <br>
<p>Vishal Anand, Yuki Miura</p></summary>
<p>

**Abstract:** The machine learning community has recently had increased interest in the climate and disaster damage domain due to a marked increased occurrences of natural hazards (e.g., hurricanes, forest fires, floods, earthquakes). However, not enough attention has been devoted to mitigating probable destruction from impending natural hazards. We explore this crucial space by predicting building-level damages on a before-the-fact basis that would allow state actors and non-governmental organizations to be best equipped with resource distribution to minimize or preempt losses. We introduce PreDisM that employs an ensemble of ResNets and fully connected layers over decision trees to capture image-level and meta-level information to accurately estimate weakness of man-made structures to disaster-occurrences. Our model performs well and is responsive to tuning across types of disasters and highlights the space of preemptive hazard damage modelling.

</p>
</details>

<details><summary><b>Sinogram upsampling using Primal-Dual UNet for undersampled CT and radial MRI reconstruction</b>
<a href="https://arxiv.org/abs/2112.13443">arxiv:2112.13443</a>
&#x1F4C8; 4 <br>
<p>Philipp Ernst, Soumick Chatterjee, Georg Rose, Oliver Speck, Andreas Nürnberger</p></summary>
<p>

**Abstract:** CT and MRI are two widely used clinical imaging modalities for non-invasive diagnosis. However, both of these modalities come with certain problems. CT uses harmful ionising radiation, and MRI suffers from slow acquisition speed. Both problems can be tackled by undersampling, such as sparse sampling. However, such undersampled data leads to lower resolution and introduces artefacts. Several techniques, including deep learning based methods, have been proposed to reconstruct such data. However, the undersampled reconstruction problem for these two modalities was always considered as two different problems and tackled separately by different research works. This paper proposes a unified solution for both sparse CT and undersampled radial MRI reconstruction, achieved by applying Fourier transform-based pre-processing on the radial MRI and then reconstructing both modalities using sinogram upsampling combined with filtered back-projection. The Primal-Dual network is a deep learning based method for reconstructing sparsely-sampled CT data. This paper introduces Primal-Dual UNet, which improves the Primal-Dual network in terms of accuracy and reconstruction speed. The proposed method resulted in an average SSIM of 0.932 while performing sparse CT reconstruction for fan-beam geometry with a sparsity level of 16, achieving a statistically significant improvement over the previous model, which resulted in 0.919. Furthermore, the proposed model resulted in 0.903 and 0.957 average SSIM while reconstructing undersampled brain and abdominal MRI data with an acceleration factor of 16 - statistically significant improvements over the original model, which resulted in 0.867 and 0.949. Finally, this paper shows that the proposed network not only improves the overall image quality, but also improves the image quality for the regions-of-interest; as well as generalises better in presence of a needle.

</p>
</details>

<details><summary><b>ArT: All-round Thinker for Unsupervised Commonsense Question-Answering</b>
<a href="https://arxiv.org/abs/2112.13428">arxiv:2112.13428</a>
&#x1F4C8; 4 <br>
<p>Jiawei Wang, Hai Zhao</p></summary>
<p>

**Abstract:** Without labeled question-answer pairs for necessary training, unsupervised commonsense question-answering (QA) appears to be extremely challenging due to its indispensable unique prerequisite on commonsense source like knowledge bases (KBs), which are usually highly resource consuming in construction. Recently pre-trained language models (PrLMs) show effectiveness as an alternative for commonsense clues when they play a role of knowledge generator. However, existing work simply generates hundreds of pseudo-answers, or roughly performs knowledge generation according to templates once for all, which may result in much noise and thus hinders the quality of generated knowledge. Motivated by human thinking experience, we propose an approach of All-round Thinker (ArT) by fully taking association during knowledge generating. In detail, our model first focuses on key parts in the given context, and then generates highly related knowledge on such a basis in an association way like human thinking. Besides, for casual reasoning, a reverse thinking mechanism is proposed to conduct bidirectional inferring between cause and effect. ArT is totally unsupervised and KBs-free. We evaluate it on three commonsense QA benchmarks: COPA, SocialIQA and SCT. On all scales of PrLM backbones, ArT shows its brilliant performance and outperforms previous advanced unsupervised models.

</p>
</details>

<details><summary><b>Omitted Variable Bias in Machine Learned Causal Models</b>
<a href="https://arxiv.org/abs/2112.13398">arxiv:2112.13398</a>
&#x1F4C8; 4 <br>
<p>Victor Chernozhukov, Carlos Cinelli, Whitney Newey, Amit Sharma, Vasilis Syrgkanis</p></summary>
<p>

**Abstract:** We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects in partially linear models, or in nonseparable models with a binary treatment) the bound is shown to depend on two easily interpretable quantities: the nonparametric partial $R^2$ (Pearson's "correlation ratio") of the unobserved variables with the treatment and with the outcome. Therefore, simple plausibility judgments on the maximum explanatory power of omitted variables (in explaining treatment and outcome variation) are sufficient to place overall bounds on the size of the bias. Finally, leveraging debiased machine learning, we provide flexible and efficient statistical inference methods to estimate the components of the bounds that are identifiable from the observed distribution.

</p>
</details>

<details><summary><b>Novel Hybrid DNN Approaches for Speaker Verification in Emotional and Stressful Talking Environments</b>
<a href="https://arxiv.org/abs/2112.13353">arxiv:2112.13353</a>
&#x1F4C8; 4 <br>
<p>Ismail Shahin, Ali Bou Nassif, Nawel Nemmour, Ashraf Elnagar, Adi Alhudhaif, Kemal Polat</p></summary>
<p>

**Abstract:** In this work, we conducted an empirical comparative study of the performance of text-independent speaker verification in emotional and stressful environments. This work combined deep models with shallow architecture, which resulted in novel hybrid classifiers. Four distinct hybrid models were utilized: deep neural network-hidden Markov model (DNN-HMM), deep neural network-Gaussian mixture model (DNN-GMM), Gaussian mixture model-deep neural network (GMM-DNN), and hidden Markov model-deep neural network (HMM-DNN). All models were based on novel implemented architecture. The comparative study used three distinct speech datasets: a private Arabic dataset and two public English databases, namely, Speech Under Simulated and Actual Stress (SUSAS) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). The test results of the aforementioned hybrid models demonstrated that the proposed HMM-DNN leveraged the verification performance in emotional and stressful environments. Results also showed that HMM-DNN outperformed all other hybrid models in terms of equal error rate (EER) and area under the curve (AUC) evaluation metrics. The average resulting verification system based on the three datasets yielded EERs of 7.19%, 16.85%, 11.51%, and 11.90% based on HMM-DNN, DNN-HMM, DNN-GMM, and GMM-DNN, respectively. Furthermore, we found that the DNN-GMM model demonstrated the least computational complexity compared to all other hybrid models in both talking environments. Conversely, the HMM-DNN model required the greatest amount of training time. Findings also demonstrated that EER and AUC values depended on the database when comparing average emotional and stressful performances.

</p>
</details>

<details><summary><b>Recent Trends in Artificial Intelligence-inspired Electronic Thermal Management</b>
<a href="https://arxiv.org/abs/2112.14837">arxiv:2112.14837</a>
&#x1F4C8; 3 <br>
<p>Aviral Chharia, Nishi Mehta, Shivam Gupta, Shivam Prajapati</p></summary>
<p>

**Abstract:** The rise of computation-based methods in thermal management has gained immense attention in recent years due to the ability of deep learning to solve complex 'physics' problems, which are otherwise difficult to be approached using conventional techniques. Thermal management is required in electronic systems to keep them from overheating and burning, enhancing their efficiency and lifespan. For a long time, numerical techniques have been employed to aid in the thermal management of electronics. However, they come with some limitations. To increase the effectiveness of traditional numerical approaches and address the drawbacks faced in conventional approaches, researchers have looked at using artificial intelligence at various stages of the thermal management process. The present study discusses in detail, the current uses of deep learning in the domain of 'electronic' thermal management.

</p>
</details>

<details><summary><b>Parallelized and Randomized Adversarial Imitation Learning for Safety-Critical Self-Driving Vehicles</b>
<a href="https://arxiv.org/abs/2112.14710">arxiv:2112.14710</a>
&#x1F4C8; 3 <br>
<p>Won Joon Yun, MyungJae Shin, Soyi Jung, Sean Kwon, Joongheon Kim</p></summary>
<p>

**Abstract:** Self-driving cars and autonomous driving research has been receiving considerable attention as major promising prospects in modern artificial intelligence applications. According to the evolution of advanced driver assistance system (ADAS), the design of self-driving vehicle and autonomous driving systems becomes complicated and safety-critical. In general, the intelligent system simultaneously and efficiently activates ADAS functions. Therefore, it is essential to consider reliable ADAS function coordination to control the driving system, safely. In order to deal with this issue, this paper proposes a randomized adversarial imitation learning (RAIL) algorithm. The RAIL is a novel derivative-free imitation learning method for autonomous driving with various ADAS functions coordination; and thus it imitates the operation of decision maker that controls autonomous driving with various ADAS functions. The proposed method is able to train the decision maker that deals with the LIDAR data and controls the autonomous driving in multi-lane complex highway environments. The simulation-based evaluation verifies that the proposed method achieves desired performance.

</p>
</details>

<details><summary><b>Attribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings</b>
<a href="https://arxiv.org/abs/2112.13416">arxiv:2112.13416</a>
&#x1F4C8; 3 <br>
<p>Tiantian Feng, Hanieh Hashemi, Rajat Hebbar, Murali Annavaram, Shrikanth S. Narayanan</p></summary>
<p>

**Abstract:** Speech emotion recognition (SER) processes speech signals to detect and characterize expressed perceived emotions. Many SER application systems often acquire and transmit speech data collected at the client-side to remote cloud platforms for inference and decision making. However, speech data carry rich information not only about emotions conveyed in vocal expressions, but also other sensitive demographic traits such as gender, age and language background. Consequently, it is desirable for SER systems to have the ability to classify emotion constructs while preventing unintended/improper inferences of sensitive and demographic information. Federated learning (FL) is a distributed machine learning paradigm that coordinates clients to train a model collaboratively without sharing their local data. This training approach appears secure and can improve privacy for SER. However, recent works have demonstrated that FL approaches are still vulnerable to various privacy attacks like reconstruction attacks and membership inference attacks. Although most of these have focused on computer vision applications, such information leakages exist in the SER systems trained using the FL technique. To assess the information leakage of SER systems trained using FL, we propose an attribute inference attack framework that infers sensitive attribute information of the clients from shared gradients or model parameters, corresponding to the FedSGD and the FedAvg training algorithms, respectively. As a use case, we empirically evaluate our approach for predicting the client's gender information using three SER benchmark datasets: IEMOCAP, CREMA-D, and MSP-Improv. We show that the attribute inference attack is achievable for SER systems trained using FL. We further identify that most information leakage possibly comes from the first layer in the SER model.

</p>
</details>

<details><summary><b>Continuous Offline Handwriting Recognition using Deep Learning Models</b>
<a href="https://arxiv.org/abs/2112.13328">arxiv:2112.13328</a>
&#x1F4C8; 3 <br>
<p>Jorge Sueiras</p></summary>
<p>

**Abstract:** Handwritten text recognition is an open problem of great interest in the area of automatic document image analysis. The transcription of handwritten content present in digitized documents is significant in analyzing historical archives or digitizing information from handwritten documents, forms, and communications. In the last years, great advances have been made in this area due to applying deep learning techniques to its resolution. This Thesis addresses the offline continuous handwritten text recognition (HTR) problem, consisting of developing algorithms and models capable of transcribing the text present in an image without the need for the text to be segmented into characters. For this purpose, we have proposed a new recognition model based on integrating two types of deep learning architectures: convolutional neural networks (CNN) and sequence-to-sequence (seq2seq) models, respectively. The convolutional component of the model is oriented to identify relevant features present in characters, and the seq2seq component builds the transcription of the text by modeling the sequential nature of the text. For the design of this new model, an extensive analysis of the capabilities of different convolutional architectures in the simplified problem of isolated character recognition has been carried out in order to identify the most suitable ones to be integrated into the continuous model. Additionally, extensive experimentation of the proposed model for the continuous problem has been carried out to determine its robustness to changes in parameterization. The generalization capacity of the model has also been validated by evaluating it on three handwritten text databases using different languages: IAM in English, RIMES in French, and Osborne in Spanish, respectively. The new proposed model provides competitive results with those obtained with other well-established methodologies.

</p>
</details>

<details><summary><b>Time Series Data Mining Algorithms Towards Scalable and Real-Time Behavior Monitoring</b>
<a href="https://arxiv.org/abs/2112.14630">arxiv:2112.14630</a>
&#x1F4C8; 2 <br>
<p>Alireza Abdoli</p></summary>
<p>

**Abstract:** In recent years, there have been unprecedented technological advances in sensor technology, and sensors have become more affordable than ever. Thus, sensor-driven data collection is increasingly becoming an attractive and practical option for researchers around the globe. Such data is typically extracted in the form of time series data, which can be investigated with data mining techniques to summarize behaviors of a range of subjects including humans and animals. While enabling cheap and mass collection of data, continuous sensor data recording results in datasets which are big in size and volume, which are challenging to process and analyze with traditional techniques in a timely manner. Such collected sensor data is typically extracted in the form of time series data. There are two main approaches in the literature, namely, shape-based classification and feature-based classification. Shape-based classification determines the best class according to a distance measure. Feature-based classification, on the other hand, measures properties of the time series and finds the best class according to the set of features defined for the time series. In this dissertation, we demonstrate that neither of the two techniques will dominate for some problems, but that some combination of both might be the best. In other words, on a single problem, it might be possible that one of the techniques is better for one subset of the behaviors, and the other technique is better for another subset of behaviors. We introduce a hybrid algorithm to classify behaviors, using both shape and feature measures, in weakly labeled time series data collected from sensors to quantify specific behaviors performed by the subject. We demonstrate that our algorithm can robustly classify real, noisy, and complex datasets, based on a combination of shape and features, and tested our proposed algorithm on real-world datasets.

</p>
</details>

<details><summary><b>Duck swarm algorithm: a novel swarm intelligence algorithm</b>
<a href="https://arxiv.org/abs/2112.13508">arxiv:2112.13508</a>
&#x1F4C8; 2 <br>
<p>Mengjian Zhang, Guihua Wen, Jing Yang</p></summary>
<p>

**Abstract:** A swarm intelligence-based optimization algorithm, named Duck Swarm Algorithm (DSA), is proposed in this paper. This algorithm is inspired by the searching for food sources and foraging behaviors of the duck swarm. The performance of DSA is verified by using eighteen benchmark functions, where it is statistical (best, mean, standard deviation, and average running time) results are compared with seven well-known algorithms like Particle swarm optimization (PSO), Firefly algorithm (FA), Chicken swarm optimization (CSO), Grey wolf optimizer (GWO), Sine cosine algorithm (SCA), and Marine-predators algorithm (MPA), and Archimedes optimization algorithm (AOA). Moreover, the Wilcoxon rank-sum test, Friedman test, and convergence curves of the comparison results are used to prove the superiority of the DSA against other algorithms. The results demonstrate that DSA is a high-performance optimization method in terms of convergence speed and exploration-exploitation balance for solving high-dimension optimization functions. Also, DSA is applied for the optimal design of two constrained engineering problems (the Three-bar truss problem, and the Sawmill operation problem). Additionally, four engineering constraint problems have also been used to analyze the performance of the proposed DSA. Overall, the comparison results revealed that the DSA is a promising and very competitive algorithm for solving different optimization problems.

</p>
</details>

<details><summary><b>New Methods & Metrics for LFQA tasks</b>
<a href="https://arxiv.org/abs/2112.13432">arxiv:2112.13432</a>
&#x1F4C8; 2 <br>
<p>Suchismit Mahapatra, Vladimir Blagojevic, Pablo Bertorello, Prasanna Kumar</p></summary>
<p>

**Abstract:** Long-form question answering (LFQA) tasks require retrieving the documents pertinent to a query, using them to form a paragraph-length answer. Despite considerable progress in LFQA modeling, fundamental issues impede its progress: i) train/validation/test dataset overlap, ii) absence of automatic metrics and iii) generated answers not being "grounded" in retrieved documents. This work addresses every one these critical bottlenecks, contributing natural language inference/generation (NLI/NLG) methods and metrics that make significant strides to their alleviation.

</p>
</details>

<details><summary><b>Generative Kernel Continual learning</b>
<a href="https://arxiv.org/abs/2112.13410">arxiv:2112.13410</a>
&#x1F4C8; 2 <br>
<p>Mohammad Mahdi Derakhshani, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</p></summary>
<p>

**Abstract:** Kernel continual learning by \citet{derakhshani2021kernel} has recently emerged as a strong continual learner due to its non-parametric ability to tackle task interference and catastrophic forgetting. Unfortunately its success comes at the expense of an explicit memory to store samples from past tasks, which hampers scalability to continual learning settings with a large number of tasks. In this paper, we introduce generative kernel continual learning, which explores and exploits the synergies between generative models and kernels for continual learning. The generative model is able to produce representative samples for kernel learning, which removes the dependence on memory in kernel continual learning. Moreover, as we replay only on the generative model, we avoid task interference while being computationally more efficient compared to previous methods that need replay on the entire model. We further introduce a supervised contrastive regularization, which enables our model to generate even more discriminative samples for better kernel-based classification performance. We conduct extensive experiments on three widely-used continual learning benchmarks that demonstrate the abilities and benefits of our contributions. Most notably, on the challenging SplitCIFAR100 benchmark, with just a simple linear kernel we obtain the same accuracy as kernel continual learning with variational random features for one tenth of the memory, or a 10.1\% accuracy gain for the same memory budget.

</p>
</details>

<details><summary><b>MPCLeague: Robust MPC Platform for Privacy-Preserving Machine Learning</b>
<a href="https://arxiv.org/abs/2112.13338">arxiv:2112.13338</a>
&#x1F4C8; 2 <br>
<p>Ajith Suresh</p></summary>
<p>

**Abstract:** In the modern era of computing, machine learning tools have demonstrated their potential in vital sectors, such as healthcare and finance, to derive proper inferences. The sensitive and confidential nature of the data in such sectors raises genuine concerns for data privacy. This motivated the area of Privacy-preserving Machine Learning (PPML), where privacy of data is guaranteed. In this thesis, we design an efficient platform, MPCLeague, for PPML in the Secure Outsourced Computation (SOC) setting using Secure Multi-party Computation (MPC) techniques.
  MPC, the holy-grail problem of secure distributed computing, enables a set of n mutually distrusting parties to perform joint computation on their private inputs in a way that no coalition of t parties can learn more information than the output (privacy) or affect the true output of the computation (correctness). While MPC, in general, has been a subject of extensive research, the area of MPC with a small number of parties has drawn popularity of late mainly due to its application to real-time scenarios, efficiency and simplicity. This thesis focuses on designing efficient MPC frameworks for 2, 3 and 4 parties, with at most one corruption and supports ring structures.
  At the heart of this thesis are four frameworks - ASTRA, SWIFT, Tetrad, ABY2.0 - catered to different settings. The practicality of our framework is argued through improvements in the benchmarking of widely used ML algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Support Vector Machines. We propose two variants for each of our frameworks, with one variant aiming to minimise the execution time while the other focuses on the monetary cost. The concrete efficiency gains of our frameworks coupled with the stronger security guarantee of robustness make our platform an ideal choice for a real-time deployment of PPML techniques.

</p>
</details>

<details><summary><b>Wireless-Enabled Asynchronous Federated Fourier Neural Network for Turbulence Prediction in Urban Air Mobility (UAM)</b>
<a href="https://arxiv.org/abs/2201.00626">arxiv:2201.00626</a>
&#x1F4C8; 1 <br>
<p>Tengchan Zeng, Omid Semiari, Walid Saad, Mehdi Bennis</p></summary>
<p>

**Abstract:** To meet the growing mobility needs in intra-city transportation, the concept of urban air mobility (UAM) has been proposed in which vertical takeoff and landing (VTOL) aircraft are used to provide a ride-hailing service. In UAM, aircraft can operate in designated air spaces known as corridors, that link the aerodromes. A reliable communication network between GBSs and aircraft enables UAM to adequately utilize the airspace and create a fast, efficient, and safe transportation system. In this paper, to characterize the wireless connectivity performance for UAM, a spatial model is proposed. For this setup, the distribution of the distance between an arbitrarily selected GBS and its associated aircraft and the Laplace transform of the interference experienced by the GBS are derived. Using these results, the signal-to-interference ratio (SIR)-based connectivity probability is determined to capture the connectivity performance of the UAM aircraft-to-ground communication network. Then, leveraging these connectivity results, a wireless-enabled asynchronous federated learning (AFL) framework that uses a Fourier neural network is proposed to tackle the challenging problem of turbulence prediction during UAM operations. For this AFL scheme, a staleness-aware global aggregation scheme is introduced to expedite the convergence to the optimal turbulence prediction model used by UAM aircraft. Simulation results validate the theoretical derivations for the UAM wireless connectivity. The results also demonstrate that the proposed AFL framework converges to the optimal turbulence prediction model faster than the synchronous federated learning baselines and a staleness-free AFL approach. Furthermore, the results characterize the performance of wireless connectivity and convergence of the aircraft's turbulence model under different parameter settings, offering useful UAM design guidelines.

</p>
</details>

<details><summary><b>Neuro-Symbolic Hierarchical Rule Induction</b>
<a href="https://arxiv.org/abs/2112.13418">arxiv:2112.13418</a>
&#x1F4C8; 1 <br>
<p>Claire Glanois, Xuening Feng, Zhaohui Jiang, Paul Weng, Matthieu Zimmer, Dong Li, Wulong Liu</p></summary>
<p>

**Abstract:** We propose an efficient interpretable neuro-symbolic model to solve Inductive Logic Programming (ILP) problems. In this model, which is built from a set of meta-rules organised in a hierarchical structure, first-order rules are invented by learning embeddings to match facts and body predicates of a meta-rule. To instantiate it, we specifically design an expressive set of generic meta-rules, and demonstrate they generate a consequent fragment of Horn clauses. During training, we inject a controlled \pw{Gumbel} noise to avoid local optima and employ interpretability-regularization term to further guide the convergence to interpretable rules. We empirically validate our model on various tasks (ILP, visual genome, reinforcement learning) against several state-of-the-art methods.

</p>
</details>

<details><summary><b>Perlin Noise Improve Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2112.13408">arxiv:2112.13408</a>
&#x1F4C8; 1 <br>
<p>Chengjun Tang, Kun Zhang, Chunfang Xing, Yong Ding, Zengmin Xu</p></summary>
<p>

**Abstract:** Adversarial examples are some special input that can perturb the output of a deep neural network, in order to make produce intentional errors in the learning algorithms in the production environment. Most of the present methods for generating adversarial examples require gradient information. Even universal perturbations that are not relevant to the generative model rely to some extent on gradient information. Procedural noise adversarial examples is a new way of adversarial example generation, which uses computer graphics noise to generate universal adversarial perturbations quickly while not relying on gradient information. Combined with the defensive idea of adversarial training, we use Perlin noise to train the neural network to obtain a model that can defend against procedural noise adversarial examples. In combination with the use of model fine-tuning methods based on pre-trained models, we obtain faster training as well as higher accuracy. Our study shows that procedural noise adversarial examples are defensible, but why procedural noise can generate adversarial examples and how to defend against other kinds of procedural noise adversarial examples that may emerge in the future remain to be investigated.

</p>
</details>

<details><summary><b>Abstractions of General Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.13404">arxiv:2112.13404</a>
&#x1F4C8; 1 <br>
<p>Sultan J. Majeed</p></summary>
<p>

**Abstract:** The field of artificial intelligence (AI) is devoted to the creation of artificial decision-makers that can perform (at least) on par with the human counterparts on a domain of interest. Unlike the agents in traditional AI, the agents in artificial general intelligence (AGI) are required to replicate human intelligence in almost every domain of interest. Moreover, an AGI agent should be able to achieve this without (virtually any) further changes, retraining, or fine-tuning of the parameters. 
The real world is non-stationary, non-ergodic, and non-Markovian: we, humans, can neither revisit our past nor are the most recent observations sufficient statistics. Yet, we excel at a variety of complex tasks. Many of these tasks require longterm planning. We can associate this success to our natural faculty to abstract away task-irrelevant information from our overwhelming sensory experience. We make task-specific mental models of the world without much effort. Due to this ability to abstract, we can plan on a significantly compact representation of a task without much loss of performance. 
Not only this, we also abstract our actions to produce high-level plans: the level of action-abstraction can be anywhere between small muscle movements to a mental notion of "doing an action". It is natural to assume that any AGI agent competing with humans (at every plausible domain) should also have these abilities to abstract its experiences and actions. 
This thesis is an inquiry into the existence of such abstractions which aid efficient planing for a wide range of domains, and most importantly, these abstractions come with some optimality guarantees.

</p>
</details>

<details><summary><b>Reducing Planning Complexity of General Reinforcement Learning with Non-Markovian Abstractions</b>
<a href="https://arxiv.org/abs/2112.13386">arxiv:2112.13386</a>
&#x1F4C8; 1 <br>
<p>Sultan J. Majeed, Marcus Hutter</p></summary>
<p>

**Abstract:** The field of General Reinforcement Learning (GRL) formulates the problem of sequential decision-making from ground up. The history of interaction constitutes a "ground" state of the system, which never repeats. On the one hand, this generality allows GRL to model almost every domain possible, e.g.\ Bandits, MDPs, POMDPs, PSRs, and history-based environments. On the other hand, in general, the near-optimal policies in GRL are functions of complete history, which hinders not only learning but also planning in GRL. The usual way around for the planning part is that the agent is given a Markovian abstraction of the underlying process. So, it can use any MDP planning algorithm to find a near-optimal policy. The Extreme State Aggregation (ESA) framework has extended this idea to non-Markovian abstractions without compromising on the possibility of planning through a (surrogate) MDP. A distinguishing feature of ESA is that it proves an upper bound of $O\left(\varepsilon^{-A} \cdot (1-γ)^{-2A}\right)$ on the number of states required for the surrogate MDP (where $A$ is the number of actions, $γ$ is the discount-factor, and $\varepsilon$ is the optimality-gap) which holds \emph{uniformly} for \emph{all} domains. While the possibility of a universal bound is quite remarkable, we show that this bound is very loose. We propose a novel non-MDP abstraction which allows for a much better upper bound of $O\left(\varepsilon^{-1} \cdot (1-γ)^{-2} \cdot A \cdot 2^{A}\right)$. Furthermore, we show that this bound can be improved further to $O\left(\varepsilon^{-1} \cdot (1-γ)^{-2} \cdot \log^3 A \right)$ by using an action-sequentialization method.

</p>
</details>

<details><summary><b>A Trained Regularization Approach Based on Born Iterative Method for Electromagnetic Imaging</b>
<a href="https://arxiv.org/abs/2112.13367">arxiv:2112.13367</a>
&#x1F4C8; 0 <br>
<p>Abdulla Desmal</p></summary>
<p>

**Abstract:** A trained-based Born iterative method (TBIM) is developed for electromagnetic imaging (EMI) applications. The proposed TBIM consists of a nested loop; the outer loop executes TBIM iteration steps, while the inner loop executes a trained iterative shrinkage thresholding algorithm (TISTA). The applied TISTA runs linear Landweber iterations implemented with a trained regularization network designed based on U-net architecture. A normalization process was imposed in TISTA that made TISTA training applicable within the proposed TBIM. The iterative utilization of the regularization network in TISTA is a bottleneck that demands high memory allocation through the training process. Therefore TISTA within each TBIM step was trained separately. The TISTA regularization network in each TBIM step was initialized using the weights from the previous TBIM step. The above approach achieved high-quality image restoration after running few TBIM steps while maintained low memory allocation through the training process. The proposed framework can be extended to Newton or quasi-Newton schemes, where within each Newton iteration, a linear ill-posed problem is optimized that differs from one example to another. The numerical results illustrated in this work show the superiority of the proposed TBIM compared to the conventional sparse-based Born iterative method (SBIM).

</p>
</details>

<details><summary><b>The Quantum Version of Prediction for Binary Classification Problem by Ensemble Methods</b>
<a href="https://arxiv.org/abs/2112.13346">arxiv:2112.13346</a>
&#x1F4C8; 0 <br>
<p>Kamil Khadiev, Liliia Safina</p></summary>
<p>

**Abstract:** In this work, we consider the performance of using a quantum algorithm to predict a result for a binary classification problem if a machine learning model is an ensemble from any simple classifiers. Such an approach is faster than classical prediction and uses quantum and classical computing, but it is based on a probabilistic algorithm. Let $N$ be a number of classifiers from an ensemble model and $O(T)$ be the running time of prediction on one classifier. In classical case, an ensemble model gets answers from each classifier and "averages" the result. The running time in classical case is $O\left( N \cdot T \right)$. We propose an algorithm which works in $O\left(\sqrt{N} \cdot T\right)$.

</p>
</details>


{% endraw %}
Prev: [2021.12.25]({{ '/2021/12/25/2021.12.25.html' | relative_url }})  Next: [2021.12.27]({{ '/2021/12/27/2021.12.27.html' | relative_url }})