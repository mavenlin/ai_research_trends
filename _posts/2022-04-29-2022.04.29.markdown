Prev: [2022.04.28]({{ '/2022/04/28/2022.04.28.html' | relative_url }})  Next: [2022.04.30]({{ '/2022/04/30/2022.04.30.html' | relative_url }})
{% raw %}
## Summary for 2022-04-29, created on 2022-05-06


<details><summary><b>Flamingo: a Visual Language Model for Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2204.14198">arxiv:2204.14198</a>
&#x1F4C8; 14800 <br>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals</p></summary>
<p>

**Abstract:** Building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. Flamingo models include key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of the proposed Flamingo models, exploring and measuring their ability to rapidly adapt to a variety of image and video understanding benchmarks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple choice visual question-answering. For tasks lying anywhere on this spectrum, we demonstrate that a single Flamingo model can achieve a new state of the art for few-shot learning, simply by prompting the model with task-specific examples. On many of these benchmarks, Flamingo actually surpasses the performance of models that are fine-tuned on thousands of times more task-specific data.

</p>
</details>

<details><summary><b>Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery</b>
<a href="https://arxiv.org/abs/2204.13906">arxiv:2204.13906</a>
&#x1F4C8; 28 <br>
<p>Daesol Cho, Jigang Kim, H. Jin Kim</p></summary>
<p>

**Abstract:** Current reinforcement learning (RL) in robotics often experiences difficulty in generalizing to new downstream tasks due to the innate task-specific training paradigm. To alleviate it, unsupervised RL, a framework that pre-trains the agent in a task-agnostic manner without access to the task-specific reward, leverages active exploration for distilling diverse experience into essential skills or reusable knowledge. For exploiting such benefits also in robotic manipulation, we propose an unsupervised method for transferable manipulation skill discovery that ties structured exploration toward interacting behavior and transferable skill learning. It not only enables the agent to learn interaction behavior, the key aspect of the robotic manipulation learning, without access to the environment reward, but also to generalize to arbitrary downstream manipulation tasks with the learned task-agnostic skills. Through comparative experiments, we show that our approach achieves the most diverse interacting behavior and significantly improves sample efficiency in downstream tasks including the extension to multi-object, multitask problems.

</p>
</details>

<details><summary><b>PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining</b>
<a href="https://arxiv.org/abs/2204.14095">arxiv:2204.14095</a>
&#x1F4C8; 22 <br>
<p>Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Chunhua Shen</p></summary>
<p>

**Abstract:** Large-scale vision-language pre-training has achieved promising results on downstream tasks. Existing methods highly rely on the assumption that the image-text pairs crawled from the Internet are in perfect one-to-one correspondence. However, in real scenarios, this assumption can be difficult to hold: the text description, obtained by crawling the affiliated metadata of the image, often suffer from semantic mismatch and mutual compatibility. To address these issues, here we introduce PyramidCLIP, which constructs an input pyramid with different semantic levels, and aligns visual elements and linguistic elements in the form of hierarchy via intra-level semantics alignment and cross-level relation alignment. Furthermore, we adjust the objective function by softening the loss of negative samples (unpaired samples) so as to weaken the strict constraint during the pre-training stage, thus mitigating the risk of the model being over-confident. Experiments on three downstream tasks, including zero-shot image classification, zero-shot image-text retrieval and image object detection, verify the effectiveness of the proposed PyramidCLIP. In particular, with the same amount of pre-training data of 15 millions image-text pairs, PyramidCLIP exceeds CLIP by 19.2%/18.5%/19.6% respectively, with the image encoder being ResNet-50/ViT-B32/ViT-B16 on ImageNet zero-shot classification top-1 accuracy. When scaling to larger datasets, the results of PyramidCLIP only trained for 8 epochs using 128M image-text pairs are very close to that of CLIP trained for 32 epochs using 400M training data.

</p>
</details>

<details><summary><b>Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN</b>
<a href="https://arxiv.org/abs/2204.14079">arxiv:2204.14079</a>
&#x1F4C8; 22 <br>
<p>Dongyeun Lee, Jae Young Lee, Doyeon Kim, Jaehyun Choi, Junmo Kim</p></summary>
<p>

**Abstract:** Transfer learning of StyleGAN has recently shown great potential to solve diverse tasks, especially in domain translation. Previous methods utilized a source model by swapping or freezing weights during transfer learning, however, they have limitations on visual quality and controlling source features. In other words, they require additional models that are computationally demanding and have restricted control steps that prevent a smooth transition. In this paper, we propose a new approach to overcome these limitations. Instead of swapping or freezing, we introduce a simple feature matching loss to improve generation quality. In addition, to control the degree of source features, we train a target model with the proposed strategy, FixNoise, to preserve the source features only in a disentangled subspace of a target feature space. Owing to the disentangled feature space, our method can smoothly control the degree of the source features in a single model. Extensive experiments demonstrate that the proposed method can generate more consistent and realistic images than previous works.

</p>
</details>

<details><summary><b>How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?</b>
<a href="https://arxiv.org/abs/2204.14268">arxiv:2204.14268</a>
&#x1F4C8; 21 <br>
<p>Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek, Mohit Bansal, Francisco Guzman</p></summary>
<p>

**Abstract:** A multilingual tokenizer is a fundamental component of multilingual neural machine translation. It is trained from a multilingual corpus. Since a skewed data distribution is considered to be harmful, a sampling strategy is usually used to balance languages in the corpus. However, few works have systematically answered how language imbalance in tokenizer training affects downstream performance. In this work, we analyze how translation performance changes as the data ratios among languages vary in the tokenizer training corpus. We find that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than we usually expected. Two features, UNK rate and closeness to the character level, can warn of poor downstream performance before performing the task. We also distinguish language sampling for tokenizer training from sampling for model training and show that the model is more sensitive to the latter.

</p>
</details>

<details><summary><b>KERMIT -- A Transformer-Based Approach for Knowledge Graph Matching</b>
<a href="https://arxiv.org/abs/2204.13931">arxiv:2204.13931</a>
&#x1F4C8; 16 <br>
<p>Sven Hertling, Jan Portisch, Heiko Paulheim</p></summary>
<p>

**Abstract:** One of the strongest signals for automated matching of knowledge graphs and ontologies are textual concept descriptions. With the rise of transformer-based language models, text comparison based on meaning (rather than lexical features) is available to researchers. However, performing pairwise comparisons of all textual descriptions of concepts in two knowledge graphs is expensive and scales quadratically (or even worse if concepts have more than one description). To overcome this problem, we follow a two-step approach: we first generate matching candidates using a pre-trained sentence transformer (so called bi-encoder). In a second step, we use fine-tuned transformer cross-encoders to generate the best candidates. We evaluate our approach on multiple datasets and show that it is feasible and produces competitive results.

</p>
</details>

<details><summary><b>SciEv: Finding Scientific Evidence Papers for Scientific News</b>
<a href="https://arxiv.org/abs/2205.00126">arxiv:2205.00126</a>
&#x1F4C8; 10 <br>
<p>Md Reshad Ul Hoque, Jiang Li, Jian Wu</p></summary>
<p>

**Abstract:** In the past decade, many scientific news media that report scientific breakthroughs and discoveries emerged, bringing science and technology closer to the general public. However, not all scientific news article cites proper sources, such as original scientific papers. A portion of scientific news articles contain misinterpreted, exaggerated, or distorted information that deviates from facts asserted in the original papers. Manually identifying proper citations is laborious and costly. Therefore, it is necessary to automatically search for pertinent scientific papers that could be used as evidence for a given piece of scientific news. We propose a system called SciEv that searches for scientific evidence papers given a scientific news article. The system employs a 2-stage query paradigm with the first stage retrieving candidate papers and the second stage reranking them. The key feature of SciEv is it uses domain knowledge entities (DKEs) to find candidates in the first stage, which proved to be more effective than regular keyphrases. In the reranking stage, we explore different document representations for news articles and candidate papers. To evaluate our system, we compiled a pilot dataset consisting of 100 manually curated (news,paper) pairs from ScienceAlert and similar websites. To our best knowledge, this is the first dataset of this kind. Our experiments indicate that the transformer model performs the best for DKE extraction. The system achieves a P@1=50%, P@5=71%, and P@10=74% when it uses a TFIDF-based text representation. The transformer-based re-ranker achieves a comparable performance but costs twice as much time. We will collect more data and test the system for user experience.

</p>
</details>

<details><summary><b>Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI</b>
<a href="https://arxiv.org/abs/2205.00029">arxiv:2205.00029</a>
&#x1F4C8; 10 <br>
<p>Pragaash Ponnusamy, Clint Solomon Mathialagan, Gustavo Aguilar, Chengyuan Ma, Chenlei Guo</p></summary>
<p>

**Abstract:** Self-learning paradigms in large-scale conversational AI agents tend to leverage user feedback in bridging between what they say and what they mean. However, such learning, particularly in Markov-based query rewriting systems have far from addressed the impact of these models on future training where successive feedback is inevitably contingent on the rewrite itself, especially in a continually updating environment. In this paper, we explore the consequences of this inherent lack of self-awareness towards impairing the model performance, ultimately resulting in both Type I and II errors over time. To that end, we propose augmenting the Markov Graph construction with a superposition-based adjacency matrix. Here, our method leverages an induced stochasticity to reactively learn a locally-adaptive decision boundary based on the performance of the individual rewrites in a bi-variate beta setting. We also surface a data augmentation strategy that leverages template-based generation in abridging complex conversation hierarchies of dialogs so as to simplify the learning process. All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.

</p>
</details>

<details><summary><b>Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows</b>
<a href="https://arxiv.org/abs/2204.13939">arxiv:2204.13939</a>
&#x1F4C8; 9 <br>
<p>Marcel Arpogaus, Marcus Voss, Beate Sick, Mark Nigge-Uricher, Oliver Dürr</p></summary>
<p>

**Abstract:** The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.

</p>
</details>

<details><summary><b>End-to-end Spoken Conversational Question Answering: Task, Dataset and Model</b>
<a href="https://arxiv.org/abs/2204.14272">arxiv:2204.14272</a>
&#x1F4C8; 8 <br>
<p>Chenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, Yuexian Zou</p></summary>
<p>

**Abstract:** In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogue flows given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. The performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of cross-modal information integration. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering tasks.

</p>
</details>

<details><summary><b>Evolutionary Approach to Security Games with Signaling</b>
<a href="https://arxiv.org/abs/2204.14173">arxiv:2204.14173</a>
&#x1F4C8; 7 <br>
<p>Adam Żychowski, Jacek Mańdziuk, Elizabeth Bondi, Aravind Venugopal, Milind Tambe, Balaraman Ravindran</p></summary>
<p>

**Abstract:** Green Security Games have become a popular way to model scenarios involving the protection of natural resources, such as wildlife. Sensors (e.g. drones equipped with cameras) have also begun to play a role in these scenarios by providing real-time information. Incorporating both human and sensor defender resources strategically is the subject of recent work on Security Games with Signaling (SGS). However, current methods to solve SGS do not scale well in terms of time or memory. We therefore propose a novel approach to SGS, which, for the first time in this domain, employs an Evolutionary Computation paradigm: EASGS. EASGS effectively searches the huge SGS solution space via suitable solution encoding in a chromosome and a specially-designed set of operators. The operators include three types of mutations, each focusing on a particular aspect of the SGS solution, optimized crossover and a local coverage improvement scheme (a memetic aspect of EASGS). We also introduce a new set of benchmark games, based on dense or locally-dense graphs that reflect real-world SGS settings. In the majority of 342 test game instances, EASGS outperforms state-of-the-art methods, including a reinforcement learning method, in terms of time scalability, nearly constant memory utilization, and quality of the returned defender's strategies (expected payoffs).

</p>
</details>

<details><summary><b>Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain</b>
<a href="https://arxiv.org/abs/2204.13883">arxiv:2204.13883</a>
&#x1F4C8; 7 <br>
<p>Karn N. Watcharasupat, Kenneth Ooi, Bhan Lam, Trevor Wong, Zhen-Ting Ong, Woon-Seng Gan</p></summary>
<p>

**Abstract:** The selection of maskers and playback gain levels in a soundscape augmentation system is crucial to its effectiveness in improving the overall acoustic comfort of a given environment. Traditionally, the selection of appropriate maskers and gain levels has been informed by expert opinion, which may not representative of the target population, or by listening tests, which can be time-consuming and labour-intensive. Furthermore, the resulting static choices of masker and gain are often inflexible to the dynamic nature of real-world soundscapes. In this work, we utilized a deep learning model to perform joint selection of the optimal masker and its gain level for a given soundscape. The proposed model was designed with highly modular building blocks, allowing for an optimized inference process that can quickly search through a large number of masker and gain combinations. In addition, we introduced the use of feature-domain soundscape augmentation conditioned on the digital gain level, eliminating the computationally expensive waveform-domain mixing process during inference time, as well as the tedious pre-calibration process required for new maskers. The proposed system was validated on a large-scale dataset of subjective responses to augmented soundscapes with more than 440 participants, ensuring the ability of the model to predict combined effect of the masker and its gain level on the perceptual pleasantness level.

</p>
</details>

<details><summary><b>The Directional Bias Helps Stochastic Gradient Descent to Generalize in Kernel Regression Models</b>
<a href="https://arxiv.org/abs/2205.00061">arxiv:2205.00061</a>
&#x1F4C8; 6 <br>
<p>Yiling Luo, Xiaoming Huo, Yajun Mei</p></summary>
<p>

**Abstract:** We study the Stochastic Gradient Descent (SGD) algorithm in nonparametric statistics: kernel regression in particular. The directional bias property of SGD, which is known in the linear regression setting, is generalized to the kernel regression. More specifically, we prove that SGD with moderate and annealing step-size converges along the direction of the eigenvector that corresponds to the largest eigenvalue of the Gram matrix. In addition, the Gradient Descent (GD) with a moderate or small step-size converges along the direction that corresponds to the smallest eigenvalue. These facts are referred to as the directional bias properties; they may interpret how an SGD-computed estimator has a potentially smaller generalization error than a GD-computed estimator. The application of our theory is demonstrated by simulation studies and a case study that is based on the FashionMNIST dataset.

</p>
</details>

<details><summary><b>Joint Multisided Exposure Fairness for Recommendation</b>
<a href="https://arxiv.org/abs/2205.00048">arxiv:2205.00048</a>
&#x1F4C8; 6 <br>
<p>Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, Xue Liu</p></summary>
<p>

**Abstract:** Prior research on exposure fairness in the context of recommender systems has focused mostly on disparities in the exposure of individual or groups of items to individual users of the system. The problem of how individual or groups of items may be systemically under or over exposed to groups of users, or even all users, has received relatively less attention. However, such systemic disparities in information exposure can result in observable social harms, such as withholding economic opportunities from historically marginalized groups (allocative harm) or amplifying gendered and racialized stereotypes (representational harm). Previously, Diaz et al. developed the expected exposure metric -- that incorporates existing user browsing models that have previously been developed for information retrieval -- to study fairness of content exposure to individual users. We extend their proposed framework to formalize a family of exposure fairness metrics that model the problem jointly from the perspective of both the consumers and producers. Specifically, we consider group attributes for both types of stakeholders to identify and mitigate fairness concerns that go beyond individual users and items towards more systemic biases in recommendation. Furthermore, we study and discuss the relationships between the different exposure fairness dimensions proposed in this paper, as well as demonstrate how stochastic ranking policies can be optimized towards said fairness goals.

</p>
</details>

<details><summary><b>Adversarial attacks on an optical neural network</b>
<a href="https://arxiv.org/abs/2205.01226">arxiv:2205.01226</a>
&#x1F4C8; 5 <br>
<p>Shuming Jiao, Ziwei Song, Shuiying Xiang</p></summary>
<p>

**Abstract:** Adversarial attacks have been extensively investigated for machine learning systems including deep learning in the digital domain. However, the adversarial attacks on optical neural networks (ONN) have been seldom considered previously. In this work, we first construct an accurate image classifier with an ONN using a mesh of interconnected Mach-Zehnder interferometers (MZI). Then a corresponding adversarial attack scheme is proposed for the first time. The attacked images are visually very similar to the original ones but the ONN system becomes malfunctioned and generates wrong classification results in most time. The results indicate that adversarial attack is also a significant issue for optical machine learning systems.

</p>
</details>

<details><summary><b>Multimodal Representation Learning With Text and Images</b>
<a href="https://arxiv.org/abs/2205.00142">arxiv:2205.00142</a>
&#x1F4C8; 5 <br>
<p>Aishwarya Jayagopal, Ankireddy Monica Aiswarya, Ankita Garg, Srinivasan Kolumam Nandakumar</p></summary>
<p>

**Abstract:** In recent years, multimodal AI has seen an upward trend as researchers are integrating data of different types such as text, images, speech into modelling to get the best results. This project leverages multimodal AI and matrix factorization techniques for representation learning, on text and image data simultaneously, thereby employing the widely used techniques of Natural Language Processing (NLP) and Computer Vision. The learnt representations are evaluated using downstream classification and regression tasks. The methodology adopted can be extended beyond the scope of this project as it uses Auto-Encoders for unsupervised representation learning.

</p>
</details>

<details><summary><b>ExSum: From Local Explanations to Model Understanding</b>
<a href="https://arxiv.org/abs/2205.00130">arxiv:2205.00130</a>
&#x1F4C8; 5 <br>
<p>Yilun Zhou, Marco Tulio Ribeiro, Julie Shah</p></summary>
<p>

**Abstract:** Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce explanation summary (ExSum), a mathematical framework for quantifying model understanding, and propose metrics for its quality assessment. On two domains, ExSum highlights various limitations in the current practice, helps develop accurate model understanding, and reveals easily overlooked properties of the model. We also connect understandability to other properties of explanations such as human alignment, robustness, and counterfactual minimality and plausibility.

</p>
</details>

<details><summary><b>A Scalable 5,6-Qubit Grover's Quantum Search Algorithm</b>
<a href="https://arxiv.org/abs/2205.00117">arxiv:2205.00117</a>
&#x1F4C8; 5 <br>
<p>Dinesh Reddy Vemula, Debanjan Konar, Sudeep Satheesan, Sri Mounica Kalidasu, Attila Cangi</p></summary>
<p>

**Abstract:** Recent studies have been spurred on by the promise of advanced quantum computing technology, which has led to the development of quantum computer simulations on classical hardware. Grover's quantum search algorithm is one of the well-known applications of quantum computing, enabling quantum computers to perform a database search (unsorted array) and quadratically outperform their classical counterparts in terms of time. Given the restricted access to database search for an oracle model (black-box), researchers have demonstrated various implementations of Grover's circuit for two to four qubits on various platforms. However, larger search spaces have not yet been explored. In this paper, a scalable Quantum Grover Search algorithm is introduced and implemented using 5-qubit and 6-qubit quantum circuits, along with a design pattern for ease of building an Oracle for a higher order of qubits. For our implementation, the probability of finding the correct entity is in the high nineties. The accuracy of the proposed 5-qubit and 6-qubit circuits is benchmarked against the state-of-the-art implementations for 3-qubit and 4-qubit. Furthermore, the reusability of the proposed quantum circuits using subroutines is also illustrated by the opportunity for large-scale implementation of quantum algorithms in the future.

</p>
</details>

<details><summary><b>Finite Entailment of UCRPQs over ALC Ontologies</b>
<a href="https://arxiv.org/abs/2204.14261">arxiv:2204.14261</a>
&#x1F4C8; 5 <br>
<p>Vıctor Gutiérrez-Basulto, Albert Gutowski, Yazmın Ibáñez-Garcıa, Filip Murlak</p></summary>
<p>

**Abstract:** We investigate the problem of finite entailment of ontology-mediated queries. We consider the expressive query language, unions of conjunctive regular path queries (UCRPQs), extending the well-known class of union of conjunctive queries, with regular expressions over roles. We look at ontologies formulated using the description logic ALC, and show a tight 2EXPTIME upper bound for entailment of UCRPQs. At the core of our decision procedure, there is a novel automata-based technique introducing a stratification of interpretations induced by the deterministic finite automaton underlying the input UCRPQ

</p>
</details>

<details><summary><b>Industry-academia research collaboration and knowledge co-creation: Patterns and anti-patterns</b>
<a href="https://arxiv.org/abs/2204.14180">arxiv:2204.14180</a>
&#x1F4C8; 5 <br>
<p>Dusica Marijan, Sagar Sen</p></summary>
<p>

**Abstract:** Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry-academia collaborations. This paper reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research-practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry-academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.

</p>
</details>

<details><summary><b>Adversarial Distortion Learning for Medical Image Denoising</b>
<a href="https://arxiv.org/abs/2204.14100">arxiv:2204.14100</a>
&#x1F4C8; 5 <br>
<p>Morteza Ghahremani, Mohammad Khateri, Alejandra Sierra, Jussi Tohka</p></summary>
<p>

**Abstract:** We present a novel adversarial distortion learning (ADL) for denoising two- and three-dimensional (2D/3D) biomedical image data. The proposed ADL consists of two auto-encoders: a denoiser and a discriminator. The denoiser removes noise from input data and the discriminator compares the denoised result to its noise-free counterpart. This process is repeated until the discriminator cannot differentiate the denoised data from the reference. Both the denoiser and the discriminator are built upon a proposed auto-encoder called Efficient-Unet. Efficient-Unet has a light architecture that uses the residual blocks and a novel pyramidal approach in the backbone to efficiently extract and re-use feature maps. During training, the textural information and contrast are controlled by two novel loss functions. The architecture of Efficient-Unet allows generalizing the proposed method to any sort of biomedical data. The 2D version of our network was trained on ImageNet and tested on biomedical datasets whose distribution is completely different from ImageNet; so, there is no need for re-training. Experimental results carried out on magnetic resonance imaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that the proposed method achieved the best on each benchmark. Our implementation and pre-trained models are available at https://github.com/mogvision/ADL.

</p>
</details>

<details><summary><b>Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling</b>
<a href="https://arxiv.org/abs/2204.14017">arxiv:2204.14017</a>
&#x1F4C8; 5 <br>
<p>KiYoon Yoo, Nojun Kwak</p></summary>
<p>

**Abstract:** Recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. However, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial purpose. This paper investigates the feasibility of model poisoning for backdoor attacks through \textit{rare word embeddings of NLP models} in text classification and sequence-to-sequence tasks. In text classification, less than 1\% of adversary clients suffices to manipulate the model output without any drop in the performance of clean sentences. For a less complex dataset, a mere 0.1\% of adversary clients is enough to poison the global model effectively. We also propose a technique specialized in the federated learning scheme called gradient ensemble, which enhances the backdoor performance in all experimental settings.

</p>
</details>

<details><summary><b>Local Explanation of Dimensionality Reduction</b>
<a href="https://arxiv.org/abs/2204.14012">arxiv:2204.14012</a>
&#x1F4C8; 5 <br>
<p>Avraam Bardos, Ioannis Mollas, Nick Bassiliades, Grigorios Tsoumakas</p></summary>
<p>

**Abstract:** Dimensionality reduction (DR) is a popular method for preparing and analyzing high-dimensional data. Reduced data representations are less computationally intensive and easier to manage and visualize, while retaining a significant percentage of their original information. Aside from these advantages, these reduced representations can be difficult or impossible to interpret in most circumstances, especially when the DR approach does not provide further information about which features of the original space led to their construction. This problem is addressed by Interpretable Machine Learning, a subfield of Explainable Artificial Intelligence that addresses the opacity of machine learning models. However, current research on Interpretable Machine Learning has been focused on supervised tasks, leaving unsupervised tasks like Dimensionality Reduction unexplored. In this paper, we introduce LXDR, a technique capable of providing local interpretations of the output of DR techniques. Experiment results and two LXDR use case examples are presented to evaluate its usefulness.

</p>
</details>

<details><summary><b>User Experience Design for Automatic Credibility Assessment of News Content About COVID-19</b>
<a href="https://arxiv.org/abs/2204.13943">arxiv:2204.13943</a>
&#x1F4C8; 5 <br>
<p>Konstantin Schulz, Jens Rauenbusch, Jan Fillies, Lisa Rutenburg, Dimitrios Karvelas, Georg Rehm</p></summary>
<p>

**Abstract:** The increasingly rapid spread of information about COVID-19 on the web calls for automatic measures of quality assurance. In that context, we check the credibility of news content using selected linguistic features. We present two empirical studies to evaluate the usability of graphical interfaces that offer such credibility assessment. In a moderated qualitative interview with six participants, we identify rating scale, sub-criteria and algorithm authorship as important predictors of the usability. A subsequent quantitative online survey with 50 participants reveals a conflict between transparency and conciseness in the interface design, as well as a perceived hierarchy of metadata: the authorship of a news text is more important than the authorship of the credibility algorithm used to assess the content quality. Finally, we make suggestions for future research, such as proactively documenting credibility-related metadata for Natural Language Processing and Language Technology services and establishing an explicit hierarchical taxonomy of usability predictors for automatic credibility assessment.

</p>
</details>

<details><summary><b>Maxmin Participatory Budgeting</b>
<a href="https://arxiv.org/abs/2204.13923">arxiv:2204.13923</a>
&#x1F4C8; 5 <br>
<p>Gogulapati Sreedurga, Mayank Ratan Bhardwaj, Y. Narahari</p></summary>
<p>

**Abstract:** Participatory Budgeting (PB) is a popular voting method by which a limited budget is divided among a set of projects, based on the preferences of voters over the projects. PB is broadly categorised as divisible PB (if the projects are fractionally implementable) and indivisible PB (if the projects are atomic). Egalitarianism, an important objective in PB, has not received much attention in the context of indivisible PB. This paper addresses this gap through a detailed study of a natural egalitarian rule, Maxmin Participatory Budgeting (MPB), in the context of indivisible PB. Our study is in two parts: (1) computational (2) axiomatic. In the first part, we prove that MPB is computationally hard and give pseudo-polynomial time and polynomial-time algorithms when parameterized by certain well-motivated parameters. We propose an algorithm that achieves for MPB, additive approximation guarantees for restricted spaces of instances and empirically show that our algorithm in fact gives exact optimal solutions on real-world PB datasets. We also establish an upper bound on the approximation ratio achievable for MPB by the family of exhaustive strategy-proof PB algorithms. In the second part, we undertake an axiomatic study of the MPB rule by generalizing known axioms in the literature. Our study leads to the proposal of a new axiom, maximal coverage, which captures fairness aspects. We prove that MPB satisfies maximal coverage.

</p>
</details>

<details><summary><b>To Know by the Company Words Keep and What Else Lies in the Vicinity</b>
<a href="https://arxiv.org/abs/2205.00148">arxiv:2205.00148</a>
&#x1F4C8; 4 <br>
<p>Jake Ryland Williams, Hunter Scott Heidenreich</p></summary>
<p>

**Abstract:** The development of state-of-the-art (SOTA) Natural Language Processing (NLP) systems has steadily been establishing new techniques to absorb the statistics of linguistic data. These techniques often trace well-known constructs from traditional theories, and we study these connections to close gaps around key NLP methods as a means to orient future work. For this, we introduce an analytic model of the statistics learned by seminal algorithms (including GloVe and Word2Vec), and derive insights for systems that use these algorithms and the statistics of co-occurrence, in general. In this work, we derive -- to the best of our knowledge -- the first known solution to Word2Vec's softmax-optimized, skip-gram algorithm. This result presents exciting potential for future development as a direct solution to a deep learning (DL) language model's (LM's) matrix factorization. However, we use the solution to demonstrate a seemingly-universal existence of a property that word vectors exhibit and which allows for the prophylactic discernment of biases in data -- prior to their absorption by DL models. To qualify our work, we conduct an analysis of independence, i.e., on the density of statistical dependencies in co-occurrence models, which in turn renders insights on the distributional hypothesis' partial fulfillment by co-occurrence statistics.

</p>
</details>

<details><summary><b>Identification of Physical Processes and Unknown Parameters of 3D Groundwater Contaminant Problems via Theory-guided U-net</b>
<a href="https://arxiv.org/abs/2205.00134">arxiv:2205.00134</a>
&#x1F4C8; 4 <br>
<p>Tianhao He, Haibin Chang, Dongxiao Zhang</p></summary>
<p>

**Abstract:** Identification of unknown physical processes and parameters of groundwater contaminant sources is a challenging task due to their ill-posed and non-unique nature. Numerous works have focused on determining nonlinear physical processes through model selection methods. However, identifying corresponding nonlinear systems for different physical phenomena using numerical methods can be computationally prohibitive. With the advent of machine learning (ML) algorithms, more efficient surrogate models based on neural networks (NNs) have been developed in various disciplines. In this work, a theory-guided U-net (TgU-net) framework is proposed for surrogate modeling of three-dimensional (3D) groundwater contaminant problems in order to efficiently elucidate their involved processes and unknown parameters. In TgU-net, the underlying governing equations are embedded into the loss function of U-net as soft constraints. For the considered groundwater contaminant problem, sorption is considered to be a potential process of an uncertain type, and three equilibrium sorption isotherm types (i.e., linear, Freundlich, and Langmuir) are considered. Different from traditional approaches in which one model corresponds to one equation, these three sorption types are modeled through only one TgU-net surrogate. The three mentioned sorption terms are integrated into one equation by assigning indicators. Accurate predictions illustrate the satisfactory generalizability and extrapolability of the constructed TgU-net. Furthermore, based on the constructed TgU-net surrogate, a data assimilation method is employed to identify the physical process and parameters simultaneously. This work shows the possibility of governing equation discovery of physical problems that contain multiple and even uncertain processes by using deep learning and data assimilation methods.

</p>
</details>

<details><summary><b>Training Language Models with Natural Language Feedback</b>
<a href="https://arxiv.org/abs/2204.14146">arxiv:2204.14146</a>
&#x1F4C8; 4 <br>
<p>Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez</p></summary>
<p>

**Abstract:** Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization.

</p>
</details>

<details><summary><b>The scope for AI-augmented interpretation of building blueprints in commercial and industrial property insurance</b>
<a href="https://arxiv.org/abs/2205.01671">arxiv:2205.01671</a>
&#x1F4C8; 3 <br>
<p>Long Chen, Mao Ye, Alistair Milne, John Hillier, Frances Oglesby</p></summary>
<p>

**Abstract:** This report, commissioned by the WTW research network, investigates the use of AI in property risk assessment. It (i) reviews existing work on risk assessment in commercial and industrial properties and automated information extraction from building blueprints; and (ii) presents an exploratory 'proof-of concept-solution' exploring the feasibility of using machine learning for the automated extraction of information from building blueprints to support insurance risk assessment.

</p>
</details>

<details><summary><b>Preoperative brain tumor imaging: models and software for segmentation and standardized reporting</b>
<a href="https://arxiv.org/abs/2204.14199">arxiv:2204.14199</a>
&#x1F4C8; 3 <br>
<p>D. Bouget, A. Pedersen, A. S. Jakola, V. Kavouridis, K. E. Emblem, R. S. Eijgelaar, I. Kommers, H. Ardon, F. Barkhof, L. Bello, M. S. Berger, M. C. Nibali, J. Furtner, S. Hervey-Jumper, A. J. S. Idema, B. Kiesel, A. Kloet, E. Mandonnet, D. M. J. Müller, P. A. Robe, M. Rossi, T. Sciortino, W. Van den Brink, M. Wagemakers, G. Widhalm</p></summary>
<p>

**Abstract:** For patients suffering from brain tumor, prognosis estimation and treatment decisions are made by a multidisciplinary team based on a set of preoperative MR scans. Currently, the lack of standardized and automatic methods for tumor detection and generation of clinical reports represents a major hurdle. In this study, we investigate glioblastomas, lower grade gliomas, meningiomas, and metastases, through four cohorts of up to 4000 patients. Tumor segmentation models were trained using the AGU-Net architecture with different preprocessing steps and protocols. Segmentation performances were assessed in-depth using a wide-range of voxel and patient-wise metrics covering volume, distance, and probabilistic aspects. Finally, two software solutions have been developed, enabling an easy use of the trained models and standardized generation of clinical reports: Raidionics and Raidionics-Slicer. Segmentation performances were quite homogeneous across the four different brain tumor types, with an average true positive Dice ranging between 80% and 90%, patient-wise recall between 88% and 98%, and patient-wise precision around 95%. With our Raidionics software, running on a desktop computer with CPU support, tumor segmentation can be performed in 16 to 54 seconds depending on the dimensions of the MRI volume. For the generation of a standardized clinical report, including the tumor segmentation and features computation, 5 to 15 minutes are necessary. All trained models have been made open-access together with the source code for both software solutions and validation metrics computation. In the future, an automatic classification of the brain tumor type would be necessary to replace manual user input. Finally, the inclusion of post-operative segmentation in both software solutions will be key for generating complete post-operative standardized clinical reports.

</p>
</details>

<details><summary><b>Explainable AI via Learning to Optimize</b>
<a href="https://arxiv.org/abs/2204.14174">arxiv:2204.14174</a>
&#x1F4C8; 3 <br>
<p>Howard Heaton, Samy Wu Fung</p></summary>
<p>

**Abstract:** Indecipherable black boxes are common in machine learning (ML), but applications increasingly require explainable artificial intelligence (XAI). The core of XAI is to establish transparent and interpretable data-driven algorithms. This work provides concrete tools for XAI in situations where prior knowledge must be encoded and untrustworthy inferences flagged. We use the "learn to optimize" (L2O) methodology wherein each inference solves a data-driven optimization problem. Our L2O models are straightforward to implement, directly encode prior knowledge, and yield theoretical guarantees (e.g. satisfaction of constraints). We also propose use of interpretable certificates to verify whether model inferences are trustworthy. Numerical examples are provided in the applications of dictionary-based signal recovery, CT imaging, and arbitrage trading of cryptoassets.

</p>
</details>

<details><summary><b>Tractable Uncertainty for Structure Learning</b>
<a href="https://arxiv.org/abs/2204.14170">arxiv:2204.14170</a>
&#x1F4C8; 3 <br>
<p>Benjie Wang, Matthew Wicker, Marta Kwiatkowska</p></summary>
<p>

**Abstract:** Bayesian structure learning allows one to capture uncertainty over the causal directed acyclic graph (DAG) responsible for generating given data. In this work, we present Tractable Uncertainty for STructure learning (TRUST), a framework for approximate posterior inference that relies on probabilistic circuits as the representation of our posterior belief. In contrast to sample-based posterior approximations, our representation can capture a much richer space of DAGs, while being able to tractably answer a range of useful inference queries. We empirically show how probabilistic circuits can be used as an augmented representation for structure learning methods, leading to improvement in both the quality of inferred structures and posterior uncertainty. Experimental results also demonstrate the improved representational capacity of TRUST, outperforming competing methods on conditional query answering.

</p>
</details>

<details><summary><b>Learning Anisotropic Interaction Rules from Individual Trajectories in a Heterogeneous Cellular Population</b>
<a href="https://arxiv.org/abs/2204.14141">arxiv:2204.14141</a>
&#x1F4C8; 3 <br>
<p>Daniel A. Messenger, Graycen E. Wheeler, Xuedong Liu, David M. Bortz</p></summary>
<p>

**Abstract:** Interacting particle system (IPS) models have proven to be highly successful for describing the spatial movement of organisms. However, it has proven challenging to infer the interaction rules directly from data. In the field of equation discovery, the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) methodology has been shown to be very computationally efficient for identifying the governing equations of complex systems, even in the presence of substantial noise. Motivated by the success of IPS models to describe the spatial movement of organisms, we develop WSINDy for second order IPSs to model the movement of communities of cells. Specifically, our approach learns the directional interaction rules that govern the dynamics of a heterogeneous population of migrating cells. Rather than aggregating cellular trajectory data into a single best-fit model, we learn the models for each individual cell. These models can then be efficiently classified according to the active classes of interactions present in the model. From these classifications, aggregated models are constructed hierarchically to simultaneously identify different species of cells present in the population and determine best-fit models for each species. We demonstrate the efficiency and proficiency of the method on several test scenarios, motivated by common cell migration experiments.

</p>
</details>

<details><summary><b>Bayesian Information Criterion for Event-based Multi-trial Ensemble data</b>
<a href="https://arxiv.org/abs/2204.14096">arxiv:2204.14096</a>
&#x1F4C8; 3 <br>
<p>Kaidi Shao, Nikos K. Logothetis, Michel Besserve</p></summary>
<p>

**Abstract:** Transient recurring phenomena are ubiquitous in many scientific fields like neuroscience and meteorology. Time inhomogenous Vector Autoregressive Models (VAR) may be used to characterize peri-event system dynamics associated with such phenomena, and can be learned by exploiting multi-dimensional data gathering samples of the evolution of the system in multiple time windows comprising, each associated with one occurrence of the transient phenomenon, that we will call "trial". However, optimal VAR model order selection methods, commonly relying on the Akaike or Bayesian Information Criteria (AIC/BIC), are typically not designed for multi-trial data. Here we derive the BIC methods for multi-trial ensemble data which are gathered after the detection of the events. We show using simulated bivariate AR models that the multi-trial BIC is able to recover the real model order. We also demonstrate with simulated transient events and real data that the multi-trial BIC is able to estimate a sufficiently small model order for dynamic system modeling.

</p>
</details>

<details><summary><b>Exploration and Exploitation in Federated Learning to Exclude Clients with Poisoned Data</b>
<a href="https://arxiv.org/abs/2204.14020">arxiv:2204.14020</a>
&#x1F4C8; 3 <br>
<p>Shadha Tabatabai, Ihab Mohammed, Basheer Qolomany, Abdullatif Albasser, Kashif Ahmad, Mohamed Abdallah, Ala Al-Fuqaha</p></summary>
<p>

**Abstract:** Federated Learning (FL) is one of the hot research topics, and it utilizes Machine Learning (ML) in a distributed manner without directly accessing private data on clients. However, FL faces many challenges, including the difficulty to obtain high accuracy, high communication cost between clients and the server, and security attacks related to adversarial ML. To tackle these three challenges, we propose an FL algorithm inspired by evolutionary techniques. The proposed algorithm groups clients randomly in many clusters, each with a model selected randomly to explore the performance of different models. The clusters are then trained in a repetitive process where the worst performing cluster is removed in each iteration until one cluster remains. In each iteration, some clients are expelled from clusters either due to using poisoned data or low performance. The surviving clients are exploited in the next iteration. The remaining cluster with surviving clients is then used for training the best FL model (i.e., remaining FL model). Communication cost is reduced since fewer clients are used in the final training of the FL model. To evaluate the performance of the proposed algorithm, we conduct a number of experiments using FEMNIST dataset and compare the result against the random FL algorithm. The experimental results show that the proposed algorithm outperforms the baseline algorithm in terms of accuracy, communication cost, and security.

</p>
</details>

<details><summary><b>Robust Solutions for Multi-Defender Stackelberg Security Games</b>
<a href="https://arxiv.org/abs/2204.14000">arxiv:2204.14000</a>
&#x1F4C8; 3 <br>
<p>Dolev Mutzari, Yonatan Aumann, Sarit Kraus</p></summary>
<p>

**Abstract:** Multi-defender Stackelberg Security Games (MSSG) have recently gained increasing attention in the literature. However, the solutions offered to date are highly sensitive, wherein even small perturbations in the attacker's utility or slight uncertainties thereof can dramatically change the defenders' resulting payoffs and alter the equilibrium. In this paper, we introduce a robust model for MSSGs, which admits solutions that are resistant to small perturbations or uncertainties in the game's parameters. First, we formally define the notion of robustness, as well as the robust MSSG model. Then, for the non-cooperative setting, we prove the existence of a robust approximate equilibrium in any such game, and provide an efficient construction thereof. For the cooperative setting, we show that any such game admits a robust approximate alpha-core, provide an efficient construction thereof, and prove that stronger types of the core may be empty. Interestingly, the robust solutions can substantially increase the defenders' utilities over those of the non-robust ones.

</p>
</details>

<details><summary><b>Statistical applications of contrastive learning</b>
<a href="https://arxiv.org/abs/2204.13999">arxiv:2204.13999</a>
&#x1F4C8; 3 <br>
<p>Michael U. Gutmann, Steven Kleinegesse, Benjamin Rhodes</p></summary>
<p>

**Abstract:** The likelihood function plays a crucial role in statistical inference and experimental design. However, it is computationally intractable for several important classes of statistical models, including energy-based models and simulator-based models. Contrastive learning is an intuitive and computationally feasible alternative to likelihood-based learning. We here first provide an introduction to contrastive learning and then show how we can use it to derive methods for diverse statistical problems, namely parameter estimation for energy-based models, Bayesian inference for simulator-based models, as well as experimental design.

</p>
</details>

<details><summary><b>PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning</b>
<a href="https://arxiv.org/abs/2204.13957">arxiv:2204.13957</a>
&#x1F4C8; 3 <br>
<p>Linlin Chao, Taifeng Wang, Wei Chu</p></summary>
<p>

**Abstract:** Knowledge graph (KG) embedding methods which map entities and relations to unique embeddings in the KG have shown promising results on many reasoning tasks. However, the same embedding dimension for both dense entities and sparse entities will cause either over parameterization (sparse entities) or under fitting (dense entities). Normally, a large dimension is set to get better performance. Meanwhile, the inference time grows log-linearly with the number of entities for all entities are traversed and compared. Both the parameter and inference become challenges when working with huge amounts of entities. Thus, we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient solution. Inspired from tensor decomposition methods, we find that decompose entity embedding matrix into low rank matrices can reduce more than half of the parameters while maintaining comparable performance. To accelerate model inference, we propose a self-supervised auxiliary task, which can be seen as fine-grained entity typing. By randomly masking and recovering entities' connected relations, the task learns the co-occurrence of entity and relations. Utilizing the fine grained typing, we can filter unrelated entities during inference and get targets with possibly sub-linear time requirement. Experiments on link prediction benchmarks demonstrate the proposed key capabilities. Moreover, we prove effectiveness of the proposed solution on the Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the state of the art performance.

</p>
</details>

<details><summary><b>Learned Gradient of a Regularizer for Plug-and-Play Gradient Descent</b>
<a href="https://arxiv.org/abs/2204.13940">arxiv:2204.13940</a>
&#x1F4C8; 3 <br>
<p>Rita Fermanian, Mikael Le Pendu, Christine Guillemot</p></summary>
<p>

**Abstract:** The Plug-and-Play (PnP) framework allows integrating advanced image denoising priors into optimization algorithms, to efficiently solve a variety of image restoration tasks. The Plug-and-Play alternating direction method of multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two examples of such methods that made a breakthrough in image restoration. However, while the former method only applies to proximal algorithms, it has recently been shown that there exists no regularization that explains the RED algorithm when the denoisers lack Jacobian symmetry, which happen to be the case of most practical denoisers. To the best of our knowledge, there exists no method for training a network that directly represents the gradient of a regularizer, which can be directly used in Plug-and-Play gradient-based algorithms. We show that it is possible to train a denoiser along with a network that corresponds to the gradient of its regularizer. We use this gradient of the regularizer in gradient-based optimization methods and obtain better results comparing to other generic Plug-and-Play approaches. We also show that the regularizer can be used as a pre-trained network for unrolled gradient descent. Lastly, we show that the resulting denoiser allows for a quick convergence of the Plug-and-Play ADMM.

</p>
</details>

<details><summary><b>A study of tree-based methods and their combination</b>
<a href="https://arxiv.org/abs/2204.13916">arxiv:2204.13916</a>
&#x1F4C8; 3 <br>
<p>Yinuo Zeng</p></summary>
<p>

**Abstract:** Tree-based methods are popular machine learning techniques used in various fields. In this work, we review their foundations and a general framework the importance sampled learning ensemble (ISLE) that accelerates their fitting process. Furthermore, we describe a model combination strategy called the adaptive regression by mixing (ARM), which is feasible for tree-based methods via ISLE. Moreover, three modified ISLEs are proposed, and their performance are evaluated on the real data sets.

</p>
</details>

<details><summary><b>Concept Activation Vectors for Generating User-Defined 3D Shapes</b>
<a href="https://arxiv.org/abs/2205.02102">arxiv:2205.02102</a>
&#x1F4C8; 2 <br>
<p>Stefan Druc, Aditya Balu, Peter Wooldridge, Adarsh Krishnamurthy, Soumik Sarkar</p></summary>
<p>

**Abstract:** We explore the interpretability of 3D geometric deep learning models in the context of Computer-Aided Design (CAD). The field of parametric CAD can be limited by the difficulty of expressing high-level design concepts in terms of a few numeric parameters. In this paper, we use a deep learning architectures to encode high dimensional 3D shapes into a vectorized latent representation that can be used to describe arbitrary concepts. Specifically, we train a simple auto-encoder to parameterize a dataset of complex shapes. To understand the latent encoded space, we use the idea of Concept Activation Vectors (CAV) to reinterpret the latent space in terms of user-defined concepts. This allows modification of a reference design to exhibit more or fewer characteristics of a chosen concept or group of concepts. We also test the statistical significance of the identified concepts and determine the sensitivity of a physical quantity of interest across the dataset.

</p>
</details>

<details><summary><b>Learn to Understand Negation in Video Retrieval</b>
<a href="https://arxiv.org/abs/2205.00132">arxiv:2205.00132</a>
&#x1F4C8; 2 <br>
<p>Ziyue Wang, Aozhu Chen, Fan Hu, Xirong Li</p></summary>
<p>

**Abstract:** Negation is a common linguistic skill that allows human to express what we do NOT want. Naturally, one might expect video retrieval to support natural-language queries with negation, e.g., finding shots of kids sitting on the floor and not playing with the dog. However, the state-of-the-art deep learning based video retrieval models lack such ability, as they are typically trained on video description datasets such as MSR-VTT and VATEX that lack negated descriptions. Their retrieved results basically ignore the negator in the sample query, incorrectly returning videos showing kids playing with the dog. In this paper, we present the first study on learning to understand negation in video retrieval and make contributions as follows. First, by re-purposing two existing datasets, i.e. MSR-VTT and VATEX, we propose a new evaluation protocol for testing video retrieval with negation. Second, we propose a learning based method for training a negation-aware video retrieval model. The key idea is to first construct a soft negative caption for a specific training video by partially negating its original caption, and then compute a bidirectionally constrained loss on the triplet. This auxiliary loss is then weightedly added to a standard retrieval loss. Experiments on the re-purposed benchmarks show that re-training the CLIP (Contrastive Language-Image Pre-Training) model by the proposed method clearly improves its ability to handle queries with negation. In addition, its performance on the original benchmarks is also improved. Data and source code will be released.

</p>
</details>

<details><summary><b>Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection</b>
<a href="https://arxiv.org/abs/2205.00122">arxiv:2205.00122</a>
&#x1F4C8; 2 <br>
<p>Chuang Niu, Ge Wang</p></summary>
<p>

**Abstract:** Early detection of lung nodules with computed tomography (CT) is critical for the longer survival of lung cancer patients and better quality of life. Computer-aided detection/diagnosis (CAD) is proven valuable as a second or concurrent reader in this context. However, accurate detection of lung nodules remains a challenge for such CAD systems and even radiologists due to not only the variability in size, location, and appearance of lung nodules but also the complexity of lung structures. This leads to a high false-positive rate with CAD, compromising its clinical efficacy. Motivated by recent computer vision techniques, here we present a self-supervised region-based 3D transformer model to identify lung nodules among a set of candidate regions. Specifically, a 3D vision transformer (ViT) is developed that divides a CT image volume into a sequence of non-overlap cubes, extracts embedding features from each cube with an embedding layer, and analyzes all embedding features with a self-attention mechanism for the prediction. To effectively train the transformer model on a relatively small dataset, the region-based contrastive learning method is used to boost the performance by pre-training the 3D transformer with public CT images. Our experiments show that the proposed method can significantly improve the performance of lung nodule screening in comparison with the commonly used 3D convolutional neural networks.

</p>
</details>

<details><summary><b>Prompt Consistency for Zero-Shot Task Generalization</b>
<a href="https://arxiv.org/abs/2205.00049">arxiv:2205.00049</a>
&#x1F4C8; 2 <br>
<p>Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig</p></summary>
<p>

**Abstract:** One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.

</p>
</details>

<details><summary><b>What do we Really Know about State of the Art NER?</b>
<a href="https://arxiv.org/abs/2205.00034">arxiv:2205.00034</a>
&#x1F4C8; 2 <br>
<p>Sowmya Vajjala, Ramya Balasubramaniam</p></summary>
<p>

**Abstract:** Named Entity Recognition (NER) is a well researched NLP task and is widely used in real world NLP scenarios. NER research typically focuses on the creation of new ways of training NER, with relatively less emphasis on resources and evaluation. Further, state of the art (SOTA) NER models, trained on standard datasets, typically report only a single performance measure (F-score) and we don't really know how well they do for different entity types and genres of text, or how robust are they to new, unseen entities. In this paper, we perform a broad evaluation of NER using a popular dataset, that takes into consideration various text genres and sources constituting the dataset at hand. Additionally, we generate six new adversarial test sets through small perturbations in the original test set, replacing select entities while retaining the context. We also train and test our models on randomly generated train/dev/test splits followed by an experiment where the models are trained on a select set of genres but tested genres not seen in training. These comprehensive evaluation strategies were performed using three SOTA NER models. Based on our results, we recommend some useful reporting practices for NER researchers, that could help in providing a better understanding of a SOTA model's performance in future.

</p>
</details>

<details><summary><b>Framework for Behavioral Disorder Detection Using Machine Learning and Application of Virtual Cognitive Behavioral Therapy in COVID-19 Pandemic</b>
<a href="https://arxiv.org/abs/2204.13900">arxiv:2204.13900</a>
&#x1F4C8; 2 <br>
<p>Tasnim Niger, Hasanur Rayhan, Rashidul Islam, Kazi Asif Abdullah Noor, Kamrul Hasan</p></summary>
<p>

**Abstract:** In this modern world, people are becoming more self-centered and unsocial. On the other hand, people are stressed, becoming more anxious during COVID-19 pandemic situation and exhibits symptoms of behavioral disorder. To measure the symptoms of behavioral disorder, usually psychiatrist use long hour sessions and inputs from specific questionnaire. This process is time consuming and sometime is ineffective to detect the right behavioral disorder. Also, reserved people sometime hesitate to follow this process. We have created a digital framework which can detect behavioral disorder and prescribe virtual Cognitive Behavioral Therapy (vCBT) for recovery. By using this framework people can input required data that are highly responsible for the three behavioral disorders namely depression, anxiety and internet addiction. We have applied machine learning technique to detect specific behavioral disorder from samples. This system guides the user with basic understanding and treatment through vCBT from anywhere any time which would potentially be the steppingstone for the user to be conscious and pursue right treatment.

</p>
</details>

<details><summary><b>AL-PINNs: Augmented Lagrangian relaxation method for Physics-Informed Neural Networks</b>
<a href="https://arxiv.org/abs/2205.01059">arxiv:2205.01059</a>
&#x1F4C8; 1 <br>
<p>Hwijae Son, Sung Woong Cho, Hyung Ju Hwang</p></summary>
<p>

**Abstract:** Physics-Informed Neural Networks (PINNs) has become a prominent application of deep learning in scientific computation, as it is a powerful approximator of solutions to nonlinear partial differential equations (PDEs). There have been numerous attempts to facilitate the training process of PINNs by adjusting the weight of each component of the loss function, called adaptive loss balancing algorithms. In this paper, we propose an Augmented Lagrangian relaxation method for PINNs (AL-PINNs). We treat the initial and boundary conditions as constraints for the optimization problem of the PDE residual. By employing Augmented Lagrangian relaxation, the constrained optimization problem becomes a sequential max-min problem so that the learnable parameters $λ$'s adaptively balance each loss component. Our theoretical analysis reveals that the sequence of minimizers of the proposed loss functions converges to an actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon equations. We demonstrate through various numerical experiments that AL-PINNs yields a much smaller relative error compared with that of state-of-the-art adaptive loss balancing algorithms.

</p>
</details>

<details><summary><b>Markov Abstractions for PAC Reinforcement Learning in Non-Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2205.01053">arxiv:2205.01053</a>
&#x1F4C8; 1 <br>
<p>Alessandro Ronca, Gabriel Paludo Licks, Giuseppe De Giacomo</p></summary>
<p>

**Abstract:** Our work aims at developing reinforcement learning algorithms that do not rely on the Markov assumption. We consider the class of Non-Markov Decision Processes where histories can be abstracted into a finite set of states while preserving the dynamics. We call it a Markov abstraction since it induces a Markov Decision Process over a set of states that encode the non-Markov dynamics. This phenomenon underlies the recently introduced Regular Decision Processes (as well as POMDPs where only a finite number of belief states is reachable). In all such kinds of decision process, an agent that uses a Markov abstraction can rely on the Markov property to achieve optimal behaviour. We show that Markov abstractions can be learned during reinforcement learning. For these two tasks, any algorithms satisfying some basic requirements can be employed. We show that our approach has PAC guarantees when the employed algorithms have PAC guarantees, and we also provide an experimental evaluation.

</p>
</details>

<details><summary><b>Gaze-enhanced Crossmodal Embeddings for Emotion Recognition</b>
<a href="https://arxiv.org/abs/2205.00129">arxiv:2205.00129</a>
&#x1F4C8; 1 <br>
<p>Ahmed Abdou, Ekta Sood, Philipp Müller, Andreas Bulling</p></summary>
<p>

**Abstract:** Emotional expressions are inherently multimodal -- integrating facial behavior, speech, and gaze -- but their automatic recognition is often limited to a single modality, e.g. speech during a phone call. While previous work proposed crossmodal emotion embeddings to improve monomodal recognition performance, despite its importance, an explicit representation of gaze was not included. We propose a new approach to emotion recognition that incorporates an explicit representation of gaze in a crossmodal emotion embedding framework. We show that our method outperforms the previous state of the art for both audio-only and video-only emotion classification on the popular One-Minute Gradual Emotion Recognition dataset. Furthermore, we report extensive ablation experiments and provide detailed insights into the performance of different state-of-the-art gaze representations and integration strategies. Our results not only underline the importance of gaze for emotion recognition but also demonstrate a practical and highly effective approach to leveraging gaze information for this task.

</p>
</details>

<details><summary><b>Implicit Regularization Properties of Variance Reduced Stochastic Mirror Descent</b>
<a href="https://arxiv.org/abs/2205.00058">arxiv:2205.00058</a>
&#x1F4C8; 1 <br>
<p>Yiling Luo, Xiaoming Huo, Yajun Mei</p></summary>
<p>

**Abstract:** In machine learning and statistical data analysis, we often run into objective function that is a summation: the number of terms in the summation possibly is equal to the sample size, which can be enormous. In such a setting, the stochastic mirror descent (SMD) algorithm is a numerically efficient method -- each iteration involving a very small subset of the data. The variance reduction version of SMD (VRSMD) can further improve SMD by inducing faster convergence. On the other hand, algorithms such as gradient descent and stochastic gradient descent have the implicit regularization property that leads to better performance in terms of the generalization errors. Little is known on whether such a property holds for VRSMD. We prove here that the discrete VRSMD estimator sequence converges to the minimum mirror interpolant in the linear regression. This establishes the implicit regularization property for VRSMD. As an application of the above result, we derive a model estimation accuracy result in the setting when the true model is sparse. We use numerical examples to illustrate the empirical power of VRSMD.

</p>
</details>

<details><summary><b>A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making</b>
<a href="https://arxiv.org/abs/2205.00033">arxiv:2205.00033</a>
&#x1F4C8; 1 <br>
<p>Jakob Schoeffer</p></summary>
<p>

**Abstract:** Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. This is not only problematic from a legal perspective, but non-transparent systems are also prone to yield unfair outcomes because their sanity is challenging to assess and calibrate in the first place -- which is particularly worrisome for human decision-subjects. Based on this observation and building upon existing work, I aim to make the following three main contributions through my doctoral thesis: (a) understand how (potential) decision-subjects perceive algorithmic decisions (with varying degrees of transparency of the underlying ADS), as compared to similar decisions made by humans; (b) evaluate different tools for transparent decision-making with respect to their effectiveness in enabling people to appropriately assess the quality and fairness of ADS; and (c) develop human-understandable technical artifacts for fair automated decision-making. Over the course of the first half of my PhD program, I have already addressed substantial pieces of (a) and (c), whereas (b) will be the major focus of the second half.

</p>
</details>

<details><summary><b>Lipschitz-based Surrogate Model for High-dimensional Computationally Expensive Problems</b>
<a href="https://arxiv.org/abs/2204.14236">arxiv:2204.14236</a>
&#x1F4C8; 1 <br>
<p>Jakub Kudela, Radomil Matousek</p></summary>
<p>

**Abstract:** Standard evolutionary optimization algorithms assume that the evaluation of the objective and constraint functions is straightforward and computationally cheap. However, in many real-world optimization problems, the computations of the objective function or constraints involve computationally expensive numerical simulations or physical experiments. Surrogate-assisted evolutionary algorithms (SAEAs) have recently gained increased attention because of their search capabilities for solving these computationally expensive optimization problems. The main idea of SAEAs is the integration of an evolutionary algorithm with a selected surrogate model. In this paper, we propose a novel surrogate model based on a Lipschitz underestimation of the expensive-to-compute objective function. We also develop a differential evolution-based algorithm, that utilizes the Lipschitz-based surrogate model, along with a standard radial basis function surrogate model and a local search procedure. This algorithm, called Lipschitz Surrogate-assisted Differential Evolution (LSADE), is designed for high-dimensional computationally expensive problems. The experimental results on seven benchmark functions of dimensions 30, 50, 100, and 200 show that the proposed method utilizing the Lipschitz-based surrogate model is competitive compared with the state-of-the-art algorithms under a limited computational budget, being especially effective for the very complicated benchmark functions in high dimensions.

</p>
</details>

<details><summary><b>Segmentation of kidney stones in endoscopic video feeds</b>
<a href="https://arxiv.org/abs/2204.14175">arxiv:2204.14175</a>
&#x1F4C8; 1 <br>
<p>Zachary A Stoebner, Daiwei Lu, Seok Hee Hong, Nicholas L Kavoussi, Ipek Oguz</p></summary>
<p>

**Abstract:** Image segmentation has been increasingly applied in medical settings as recent developments have skyrocketed the potential applications of deep learning. Urology, specifically, is one field of medicine that is primed for the adoption of a real-time image segmentation system with the long-term aim of automating endoscopic stone treatment. In this project, we explored supervised deep learning models to annotate kidney stones in surgical endoscopic video feeds. In this paper, we describe how we built a dataset from the raw videos and how we developed a pipeline to automate as much of the process as possible. For the segmentation task, we adapted and analyzed three baseline deep learning models -- U-Net, U-Net++, and DenseNet -- to predict annotations on the frames of the endoscopic videos with the highest accuracy above 90\%. To show clinical potential for real-time use, we also confirmed that our best trained model can accurately annotate new videos at 30 frames per second. Our results demonstrate that the proposed method justifies continued development and study of image segmentation to annotate ureteroscopic video feeds.

</p>
</details>

<details><summary><b>Family of Two Dimensional Transition Metal Dichlorides Fundamental Properties, Structural Defects, and Environmental Stability</b>
<a href="https://arxiv.org/abs/2205.00874">arxiv:2205.00874</a>
&#x1F4C8; 0 <br>
<p>Andrey A. Kistanov, Stepan A. Shcherbinin, Romain Botella, Artur Davletshin, Wei Cao</p></summary>
<p>

**Abstract:** A large number of novel two-dimensional (2D) materials are constantly discovered and deposed into the databases. Consolidate implementation of machine learning algorithms and density functional theory (DFT) based predictions have allowed creating several databases containing an unimaginable amount of 2D samples. The next step in this chain, the investigation leads to a comprehensive study of the functionality of the invented materials. In this work, a family of transition metal dichlorides has been screened out for systematical investigation of their structural stability, fundamental properties, structural defects, and environmental stability via DFT based calculations. The work highlights the importance of using the potential of the invented materials and proposes a comprehensive characterization of a new family of 2D materials.

</p>
</details>


{% endraw %}
Prev: [2022.04.28]({{ '/2022/04/28/2022.04.28.html' | relative_url }})  Next: [2022.04.30]({{ '/2022/04/30/2022.04.30.html' | relative_url }})