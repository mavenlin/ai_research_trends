Prev: [2020.12.31]({{ '/2020/12/31/2020.12.31.html' | relative_url }})  Next: [2021.01.02]({{ '/2021/01/02/2021.01.02.html' | relative_url }})
{% raw %}
## Summary for 2021-01-01, created on 2021-12-24


<details><summary><b>Learning the Predictability of the Future</b>
<a href="https://arxiv.org/abs/2101.01600">arxiv:2101.01600</a>
&#x1F4C8; 70 <br>
<p>Dídac Surís, Ruoshi Liu, Carl Vondrick</p></summary>
<p>

**Abstract:** We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation.

</p>
</details>

<details><summary><b>UnitedQA: A Hybrid Approach for Open Domain Question Answering</b>
<a href="https://arxiv.org/abs/2101.00178">arxiv:2101.00178</a>
&#x1F4C8; 47 <br>
<p>Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao</p></summary>
<p>

**Abstract:** To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models. We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models. We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.

</p>
</details>

<details><summary><b>Key Phrase Extraction & Applause Prediction</b>
<a href="https://arxiv.org/abs/2101.03235">arxiv:2101.03235</a>
&#x1F4C8; 46 <br>
<p>Krishna Yadav, Lakshya Choudhary</p></summary>
<p>

**Abstract:** With the increase in content availability over the internet it is very difficult to get noticed. It has become an upmost the priority of the blog writers to get some feedback over their creations to be confident about the impact of their article. We are training a machine learning model to learn popular article styles, in the form of vector space representations using various word embeddings, and their popularity based on claps and tags.

</p>
</details>

<details><summary><b>Modeling Fine-Grained Entity Types with Box Embeddings</b>
<a href="https://arxiv.org/abs/2101.00345">arxiv:2101.00345</a>
&#x1F4C8; 27 <br>
<p>Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett</p></summary>
<p>

**Abstract:** Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types' complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.

</p>
</details>

<details><summary><b>Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers</b>
<a href="https://arxiv.org/abs/2101.00234">arxiv:2101.00234</a>
&#x1F4C8; 23 <br>
<p>Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo</p></summary>
<p>

**Abstract:** Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.

</p>
</details>

<details><summary><b>CIZSL++: Creativity Inspired Generative Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2101.00173">arxiv:2101.00173</a>
&#x1F4C8; 15 <br>
<p>Mohamed Elhoseiny, Kai Yi, Mohamed Elfeki</p></summary>
<p>

**Abstract:** Zero-shot learning (ZSL) aims at understanding unseen categories with no training examples from class-level descriptions. To improve the discriminative power of ZSL, we model the visual learning process of unseen categories with inspiration from the psychology of human creativity for producing novel art. First, we propose CIZSL-v1 as a creativity inspired model for generative ZSL. We relate ZSL to human creativity by observing that ZSL is about recognizing the unseen, and creativity is about creating a likable unseen. We introduce a learning signal inspired by creativity literature that explores the unseen space with hallucinated class-descriptions and encourages careful deviation of their visual feature generations from seen classes while allowing knowledge transfer from seen to unseen classes. Second, CIZSL-v2 is proposed as an improved version of CIZSL-v1 for generative zero-shot learning. CIZSL-v2 consists of an investigation of additional inductive losses for unseen classes along with a semantic guided discriminator. Empirically, we show consistently that CIZSL losses can improve generative ZSL models on the challenging task of generalized ZSL from a noisy text on CUB and NABirds datasets. We also show the advantage of our approach to Attribute-based ZSL on AwA2, aPY, and SUN datasets. We also show that CIZSL-v2 has improved performance compared to CIZSL-v1.

</p>
</details>

<details><summary><b>Code Generation from Natural Language with Less Prior and More Monolingual Data</b>
<a href="https://arxiv.org/abs/2101.00259">arxiv:2101.00259</a>
&#x1F4C8; 10 <br>
<p>Sajad Norouzi, Keyi Tang, Yanshuai Cao</p></summary>
<p>

**Abstract:** Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice.

</p>
</details>

<details><summary><b>Neural Architecture Search via Combinatorial Multi-Armed Bandit</b>
<a href="https://arxiv.org/abs/2101.00336">arxiv:2101.00336</a>
&#x1F4C8; 9 <br>
<p>Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey</p></summary>
<p>

**Abstract:** Neural Architecture Search (NAS) has gained significant popularity as an effective tool for designing high performance deep neural networks (DNNs). NAS can be performed via policy gradient, evolutionary algorithms, differentiable architecture search or tree-search methods. While significant progress has been made for both policy gradient and differentiable architecture search, tree-search methods have so far failed to achieve comparable accuracy or search efficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed Bandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large search space into smaller blocks where tree-search methods can be applied more effectively and efficiently. We further leverage a tree-based method called Nested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our approach discovers a cell structure that achieves a low error rate that is comparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times faster than current tree-search methods. Moreover, the discovered structure transfers well to large-scale datasets such as ImageNet.

</p>
</details>

<details><summary><b>Iranis: A Large-scale Dataset of Farsi License Plate Characters</b>
<a href="https://arxiv.org/abs/2101.00295">arxiv:2101.00295</a>
&#x1F4C8; 9 <br>
<p>Ali Tourani, Sajjad Soroori, Asadollah Shahbahrami, Alireza Akoushideh</p></summary>
<p>

**Abstract:** Providing huge amounts of data is a fundamental demand when dealing with Deep Neural Networks (DNNs). Employing these algorithms to solve computer vision problems resulted in the advent of various image datasets to feed the most common visual imagery deep structures, known as Convolutional Neural Networks (CNNs). In this regard, some datasets can be found that contain hundreds or even thousands of images for license plate detection and optical character recognition purposes. However, no publicly available image dataset provides such data for the recognition of Farsi characters used in car license plates. The gap has to be filled due to the numerous advantages of developing accurate deep learning-based systems for law enforcement and surveillance purposes. This paper introduces a large-scale dataset that includes images of numbers and characters used in Iranian car license plates. The dataset, named Iranis, contains more than 83,000 images of Farsi numbers and letters collected from real-world license plate images captured by various cameras. The variety of instances in terms of camera shooting angle, illumination, resolution, and contrast make the dataset a proper choice for training DNNs. Dataset images are manually annotated for object detection and image classification. Finally, and to build a baseline for Farsi character recognition, the paper provides a performance analysis using a YOLO v.3 object detector.

</p>
</details>

<details><summary><b>Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis</b>
<a href="https://arxiv.org/abs/2101.00318">arxiv:2101.00318</a>
&#x1F4C8; 8 <br>
<p>Xiaofeng Liu, Xiongchang Liu, Bo Hu, Wenxuan Ji, Fangxu Xing, Jun Lu, Jane You, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo</p></summary>
<p>

**Abstract:** Recent advances in unsupervised domain adaptation (UDA) show that transferable prototypical learning presents a powerful means for class conditional alignment, which encourages the closeness of cross-domain class centroids. However, the cross-domain inner-class compactness and the underlying fine-grained subtype structure remained largely underexplored. In this work, we propose to adaptively carry out the fine-grained subtype-aware alignment by explicitly enforcing the class-wise separation and subtype-wise compactness with intermediate pseudo labels. Our key insight is that the unlabeled subtypes of a class can be divergent to one another with different conditional and label shifts, while inheriting the local proximity within a subtype. The cases of with or without the prior information on subtype numbers are investigated to discover the underlying subtype structure in an online fashion. The proposed subtype-aware dynamic UDA achieves promising results on medical diagnosis tasks.

</p>
</details>

<details><summary><b>Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers</b>
<a href="https://arxiv.org/abs/2101.00169">arxiv:2101.00169</a>
&#x1F4C8; 8 <br>
<p>Daniel Szelogowski</p></summary>
<p>

**Abstract:** Current AI-generated music lacks fundamental principles of good compositional techniques. By narrowing down implementation issues both programmatically and musically, we can create a better understanding of what parameters are necessary for a generated composition nearly indistinguishable from that of a master composer.

</p>
</details>

<details><summary><b>Socially Responsible AI Algorithms: Issues, Purposes, and Challenges</b>
<a href="https://arxiv.org/abs/2101.02032">arxiv:2101.02032</a>
&#x1F4C8; 7 <br>
<p>Lu Cheng, Kush R. Varshney, Huan Liu</p></summary>
<p>

**Abstract:** In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI's indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.

</p>
</details>

<details><summary><b>B-SMALL: A Bayesian Neural Network approach to Sparse Model-Agnostic Meta-Learning</b>
<a href="https://arxiv.org/abs/2101.00203">arxiv:2101.00203</a>
&#x1F4C8; 7 <br>
<p>Anish Madan, Ranjitha Prasad</p></summary>
<p>

**Abstract:** There is a growing interest in the learning-to-learn paradigm, also known as meta-learning, where models infer on new tasks using a few training examples. Recently, meta-learning based methods have been widely used in few-shot classification, regression, reinforcement learning, and domain adaptation. The model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that obtains model parameter initialization at meta-training phase. In the meta-test phase, this initialization is rapidly adapted to new tasks by using gradient descent. However, meta-learning models are prone to overfitting since there are insufficient training tasks resulting in over-parameterized models with poor generalization performance for unseen tasks. In this paper, we propose a Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL algorithm. The proposed framework incorporates a sparse variational loss term alongside the loss function of MAML, which uses a sparsifying approximated KL divergence as a regularizer. We demonstrate the performance of B-MAML using classification and regression tasks, and highlight that training a sparsifying BNN using MAML indeed improves the parameter footprint of the model while performing at par or even outperforming the MAML approach. We also illustrate applicability of our approach in distributed sensor networks, where sparsity and meta-learning can be beneficial.

</p>
</details>

<details><summary><b>New-Type Hoeffding's Inequalities and Application in Tail Bounds</b>
<a href="https://arxiv.org/abs/2101.00360">arxiv:2101.00360</a>
&#x1F4C8; 6 <br>
<p>Pingyi Fan</p></summary>
<p>

**Abstract:** It is well known that Hoeffding's inequality has a lot of applications in the signal and information processing fields. How to improve Hoeffding's inequality and find the refinements of its applications have always attracted much attentions. An improvement of Hoeffding inequality was recently given by Hertz \cite{r1}. Eventhough such an improvement is not so big, it still can be used to update many known results with original Hoeffding's inequality, especially for Hoeffding-Azuma inequality for martingales. However, the results in original Hoeffding's inequality and its refinement one by Hertz only considered the first order moment of random variables. In this paper, we present a new type of Hoeffding's inequalities, where the high order moments of random variables are taken into account. It can get some considerable improvements in the tail bounds evaluation compared with the known results. It is expected that the developed new type Hoeffding's inequalities could get more interesting applications in some related fields that use Hoeffding's results.

</p>
</details>

<details><summary><b>Identity-aware Facial Expression Recognition in Compressed Video</b>
<a href="https://arxiv.org/abs/2101.00317">arxiv:2101.00317</a>
&#x1F4C8; 6 <br>
<p>Xiaofeng Liu, Linghao Jin, Xu Han, Jun Lu, Jane You, Lingsheng Kong</p></summary>
<p>

**Abstract:** This paper targets to explore the inter-subject variations eliminated facial expression representation in the compressed video domain. Most of the previous methods process the RGB images of a sequence, while the off-the-shelf and valuable expression-related muscle movement already embedded in the compression format. In the up to two orders of magnitude compressed domain, we can explicitly infer the expression from the residual frames and possible to extract identity factors from the I frame with a pre-trained face recognition network. By enforcing the marginal independent of them, the expression feature is expected to be purer for the expression and be robust to identity shifts. We do not need the identity label or multiple expression samples from the same person for identity elimination. Moreover, when the apex frame is annotated in the dataset, the complementary constraint can be further added to regularize the feature-level game. In testing, only the compressed residual frames are required to achieve expression prediction. Our solution can achieve comparable or better performance than the recent decoded image based methods on the typical FER benchmarks with about 3$\times$ faster inference with compressed data.

</p>
</details>

<details><summary><b>A Survey on Deep Reinforcement Learning for Audio-Based Applications</b>
<a href="https://arxiv.org/abs/2101.00240">arxiv:2101.00240</a>
&#x1F4C8; 6 <br>
<p>Siddique Latif, Heriberto Cuayáhuitl, Farrukh Pervez, Fahad Shamshad, Hafiz Shehbaz Ali, Erik Cambria</p></summary>
<p>

**Abstract:** Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising application in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together the research studies across different speech and music-related areas. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting challenges faced by audio-based DRL agents and highlighting open areas for future research and investigation.

</p>
</details>

<details><summary><b>An Artificial Intelligence System for Combined Fruit Detection and Georeferencing, Using RTK-Based Perspective Projection in Drone Imagery</b>
<a href="https://arxiv.org/abs/2101.00339">arxiv:2101.00339</a>
&#x1F4C8; 5 <br>
<p>Angus Baird, Stefano Giani</p></summary>
<p>

**Abstract:** This work presents an Artificial Intelligence (AI) system, based on the Faster Region-Based Convolution Neural Network (Faster R-CNN) framework, which detects and counts apples from oblique, aerial drone imagery of giant commercial orchards. To reduce computational cost, a novel precursory stage to the network is designed to preprocess raw imagery into cropped images of individual trees. Unique geospatial identifiers are allocated to these using the perspective projection model. This employs Real-Time Kinematic (RTK) data, Digital Terrain and Surface Models (DTM and DSM), as well as internal and external camera parameters. The bulk of experiments however focus on tuning hyperparameters in the detection network itself. Apples which are on trees and apples which are on the ground are treated as separate classes. A mean Average Precision (mAP) metric, calibrated by the size of the two classes, is devised to mitigate spurious results. Anchor box design is of key interest due to the scale of the apples. As such, a k-means clustering approach, never before seen in literature for Faster R-CNN, resulted in the most significant improvements to calibrated mAP. Other experiments showed that the maximum number of box proposals should be 225; the initial learning rate of 0.001 is best applied to the adaptive RMS Prop optimiser; and ResNet 101 is the ideal base feature extractor when considering mAP and, to a lesser extent, inference time. The amalgamation of the optimal hyperparameters leads to a model with a calibrated mAP of 0.7627.

</p>
</details>

<details><summary><b>Energy-constrained Self-training for Unsupervised Domain Adaptation</b>
<a href="https://arxiv.org/abs/2101.00316">arxiv:2101.00316</a>
&#x1F4C8; 5 <br>
<p>Xiaofeng Liu, Bo Hu, Xiongchang Liu, Jun Lu, Jane You, Lingsheng Kong</p></summary>
<p>

**Abstract:** Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a labeled source domain distribution to perform well on an unlabeled target domain. Recently, the deep self-training involves an iterative process of predicting on the target domain and then taking the confident predictions as hard pseudo-labels for retraining. However, the pseudo-labels are usually unreliable, and easily leading to deviated solutions with propagated errors. In this paper, we resort to the energy-based model and constrain the training of the unlabeled target sample with the energy function minimization objective. It can be applied as a simple additional regularization. In this framework, it is possible to gain the benefits of the energy-based model, while retaining strong discriminative performance following a plug-and-play fashion. We deliver extensive experiments on the most popular and large scale UDA benchmarks of image classification as well as semantic segmentation to demonstrate its generality and effectiveness.

</p>
</details>

<details><summary><b>VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words</b>
<a href="https://arxiv.org/abs/2101.00265">arxiv:2101.00265</a>
&#x1F4C8; 5 <br>
<p>Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee</p></summary>
<p>

**Abstract:** Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.

</p>
</details>

<details><summary><b>Cutting-edge 3D Medical Image Segmentation Methods in 2020: Are Happy Families All Alike?</b>
<a href="https://arxiv.org/abs/2101.00232">arxiv:2101.00232</a>
&#x1F4C8; 5 <br>
<p>Jun Ma</p></summary>
<p>

**Abstract:** Segmentation is one of the most important and popular tasks in medical image analysis, which plays a critical role in disease diagnosis, surgical planning, and prognosis evaluation. During the past five years, on the one hand, thousands of medical image segmentation methods have been proposed for various organs and lesions in different medical images, which become more and more challenging to fairly compare different methods. On the other hand, international segmentation challenges can provide a transparent platform to fairly evaluate and compare different methods. In this paper, we present a comprehensive review of the top methods in ten 3D medical image segmentation challenges during 2020, covering a variety of tasks and datasets. We also identify the "happy-families" practices in the cutting-edge segmentation methods, which are useful for developing powerful segmentation approaches. Finally, we discuss open research problems that should be addressed in the future. We also maintain a list of cutting-edge segmentation methods at \url{https://github.com/JunMa11/SOTA-MedSeg}.

</p>
</details>

<details><summary><b>Biologically Inspired Hexagonal Deep Learning for Hexagonal Image Generation</b>
<a href="https://arxiv.org/abs/2101.00337">arxiv:2101.00337</a>
&#x1F4C8; 4 <br>
<p>Tobias Schlosser, Frederik Beuth, Danny Kowerko</p></summary>
<p>

**Abstract:** Whereas conventional state-of-the-art image processing systems of recording and output devices almost exclusively utilize square arranged methods, biological models, however, suggest an alternative, evolutionarily-based structure. Inspired by the human visual perception system, hexagonal image processing in the context of machine learning offers a number of key advantages that can benefit both researchers and users alike. The hexagonal deep learning framework Hexnet leveraged in this contribution serves therefore the generation of hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the results of our created test environment show, the proposed models can surpass current approaches of conventional image generation. While resulting in a reduction of the models' complexity in the form of trainable parameters, they furthermore allow an increase of test rates in comparison to their square counterparts.

</p>
</details>

<details><summary><b>When Is Generalizable Reinforcement Learning Tractable?</b>
<a href="https://arxiv.org/abs/2101.00300">arxiv:2101.00300</a>
&#x1F4C8; 4 <br>
<p>Dhruv Malik, Yuanzhi Li, Pradeep Ravikumar</p></summary>
<p>

**Abstract:** Agents trained by reinforcement learning (RL) often fail to generalize beyond the environment they were trained in, even when presented with new scenarios that seem similar to the training environment. We study the query complexity required to train RL agents that generalize to multiple environments. Intuitively, tractable generalization is only possible when the environments are similar or close in some sense. To capture this, we introduce Weak Proximity, a natural structural condition that requires the environments to have highly similar transition and reward functions and share a policy providing optimal value. Despite such shared structure, we prove that tractable generalization is impossible in the worst case. This holds even when each individual environment can be efficiently solved to obtain an optimal linear policy, and when the agent possesses a generative model. Our lower bound applies to the more complex task of representation learning for the purpose of efficient generalization to multiple environments. On the positive side, we introduce Strong Proximity, a strengthened condition which we prove is sufficient for efficient generalization.

</p>
</details>

<details><summary><b>Characterizing Fairness Over the Set of Good Models Under Selective Labels</b>
<a href="https://arxiv.org/abs/2101.00352">arxiv:2101.00352</a>
&#x1F4C8; 3 <br>
<p>Amanda Coston, Ashesh Rambachan, Alexandra Chouldechova</p></summary>
<p>

**Abstract:** Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the "Rashomon Effect." These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or "the set of good models." Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) replace an existing model with one that has better fairness properties; or 2) audit for predictive bias. We illustrate these uses cases on a real-world credit-scoring task and a recidivism prediction task.

</p>
</details>

<details><summary><b>The Bayesian Method of Tensor Networks</b>
<a href="https://arxiv.org/abs/2101.00245">arxiv:2101.00245</a>
&#x1F4C8; 3 <br>
<p>Erdong Guo, David Draper</p></summary>
<p>

**Abstract:** Bayesian learning is a powerful learning framework which combines the external information of the data (background information) with the internal information (training data) in a logically consistent way in inference and prediction. By Bayes rule, the external information (prior distribution) and the internal information (training data likelihood) are combined coherently, and the posterior distribution and the posterior predictive (marginal) distribution obtained by Bayes rule summarize the total information needed in the inference and prediction, respectively. In this paper, we study the Bayesian framework of the Tensor Network from two perspective. First, we introduce the prior distribution to the weights in the Tensor Network and predict the labels of the new observations by the posterior predictive (marginal) distribution. Since the intractability of the parameter integral in the normalization constant computation, we approximate the posterior predictive distribution by Laplace approximation and obtain the out-product approximation of the hessian matrix of the posterior distribution of the Tensor Network model. Second, to estimate the parameters of the stationary mode, we propose a stable initialization trick to accelerate the inference process by which the Tensor Network can converge to the stationary path more efficiently and stably with gradient descent method. We verify our work on the MNIST, Phishing Website and Breast Cancer data set. We study the Bayesian properties of the Bayesian Tensor Network by visualizing the parameters of the model and the decision boundaries in the two dimensional synthetic data set. For a application purpose, our work can reduce the overfitting and improve the performance of normal Tensor Network model.

</p>
</details>

<details><summary><b>More than just an auxiliary loss: Anti-spoofing Backbone Training via Adversarial Pseudo-depth Generation</b>
<a href="https://arxiv.org/abs/2101.00200">arxiv:2101.00200</a>
&#x1F4C8; 3 <br>
<p>Chang Keun Paik, Naeun Ko, Youngjoon Yoo</p></summary>
<p>

**Abstract:** In this paper, a new method of training pipeline is discussed to achieve significant performance on the task of anti-spoofing with RGB image. We explore and highlight the impact of using pseudo-depth to pre-train a network that will be used as the backbone to the final classifier. While the usage of pseudo-depth for anti-spoofing task is not a new idea on its own, previous endeavours utilize pseudo-depth simply as another medium to extract features for performing prediction, or as part of many auxiliary losses in aiding the training of the main classifier, normalizing the importance of pseudo-depth as just another semantic information. Through this work, we argue that there exists a significant advantage in training the final classifier can be gained by the pre-trained generator learning to predict the corresponding pseudo-depth of a given facial image, from a Generative Adversarial Network framework. Our experimental results indicate that our method results in a much more adaptable system that can generalize beyond intra-dataset samples, but to inter-dataset samples, which it has never seen before during training. Quantitatively, our method approaches the baseline performance of the current state of the art anti-spoofing models with 15.8x less parameters used. Moreover, experiments showed that the introduced methodology performs well only using basic binary label without additional semantic information which indicates potential benefits of this work in industrial and application based environment where trade-off between additional labelling and resources are considered.

</p>
</details>

<details><summary><b>Detecting residues of cosmic events using residual neural network</b>
<a href="https://arxiv.org/abs/2101.00195">arxiv:2101.00195</a>
&#x1F4C8; 3 <br>
<p>Hrithika Dodia</p></summary>
<p>

**Abstract:** The detection of gravitational waves is considered to be one of the most magnificent discoveries of the century. Due to the high computational cost of matched filtering pipeline, there is a hunt for an alternative powerful system. I present, for the first time, the use of 1D residual neural network for detection of gravitational waves. Residual networks have transformed many fields like image classification, face recognition and object detection with their robust structure. With increase in sensitivity of LIGO detectors we expect many more sources of gravitational waves in the universe to be detected. However, deep learning networks are trained only once. When used for classification task, deep neural networks are trained to predict only a fixed number of classes. Therefore, when a new type of gravitational wave is to be detected, this turns out to be a drawback of deep learning. Shallow neural networks can be used to learn data with simple patterns but fail to give good results with increase in complexity of data. Remodelling the neural network with detection of each new type of GW is highly infeasible. In this letter, I also discuss ways to reduce the time required to adapt to such changes in detection of gravitational waves for deep learning methods. Primarily, I aim to create a custom residual neural network for 1-dimensional time series inputs, which can learn a ton of features from dataset without giving up on increasing the number of classes or increasing the complexity of data. I use the two class of binary coalescence signals (Binary Black Hole Merger and Binary Neutron Star Merger signals) detected by LIGO to check the performance of residual structure on gravitational waves detection.

</p>
</details>

<details><summary><b>SDA: Improving Text Generation with Self Data Augmentation</b>
<a href="https://arxiv.org/abs/2101.03236">arxiv:2101.03236</a>
&#x1F4C8; 2 <br>
<p>Ping Yu, Ruiyi Zhang, Yang Zhao, Yizhe Zhang, Chunyuan Li, Changyou Chen</p></summary>
<p>

**Abstract:** Data augmentation has been widely used to improve deep neural networks in many research fields, such as computer vision. However, less work has been done in the context of text, partially due to its discrete nature and the complexity of natural languages. In this paper, we propose to improve the standard maximum likelihood estimation (MLE) paradigm by incorporating a self-imitation-learning phase for automatic data augmentation. Unlike most existing sentence-level augmentation strategies, which are only applied to specific models, our method is more general and could be easily adapted to any MLE-based training procedure. In addition, our framework allows task-specific evaluation metrics to be designed to flexibly control the generated sentences, for example, in terms of controlling vocabulary usage and avoiding nontrivial repetitions. Extensive experimental results demonstrate the superiority of our method on two synthetic and several standard real datasets, significantly improving related baselines.

</p>
</details>

<details><summary><b>Quaternion higher-order singular value decomposition and its applications in color image processing</b>
<a href="https://arxiv.org/abs/2101.00364">arxiv:2101.00364</a>
&#x1F4C8; 2 <br>
<p>Jifei Miao, Kit Ian Kou</p></summary>
<p>

**Abstract:** Higher-order singular value decomposition (HOSVD) is one of the most efficient tensor decomposition techniques. It has the salient ability to represent high_dimensional data and extract features. In more recent years, the quaternion has proven to be a very suitable tool for color pixel representation as it can well preserve cross-channel correlation of color channels. Motivated by the advantages of the HOSVD and the quaternion tool, in this paper, we generalize the HOSVD to the quaternion domain and define quaternion-based HOSVD (QHOSVD). Due to the non-commutability of quaternion multiplication, QHOSVD is not a trivial extension of the HOSVD. They have similar but different calculation procedures. The defined QHOSVD can be widely used in various visual data processing with color pixels. In this paper, we present two applications of the defined QHOSVD in color image processing: multi_focus color image fusion and color image denoising. The experimental results on the two applications respectively demonstrate the competitive performance of the proposed methods over some existing ones.

</p>
</details>

<details><summary><b>Reinforcement Learning for Flexibility Design Problems</b>
<a href="https://arxiv.org/abs/2101.00355">arxiv:2101.00355</a>
&#x1F4C8; 2 <br>
<p>Yehua Wei, Lei Zhang, Ruiyi Zhang, Shijing Si, Hao Zhang, Lawrence Carin</p></summary>
<p>

**Abstract:** Flexibility design problems are a class of problems that appear in strategic decision-making across industries, where the objective is to design a ($e.g.$, manufacturing) network that affords flexibility and adaptivity. The underlying combinatorial nature and stochastic objectives make flexibility design problems challenging for standard optimization methods. In this paper, we develop a reinforcement learning (RL) framework for flexibility design problems. Specifically, we carefully design mechanisms with noisy exploration and variance reduction to ensure empirical success and show the unique advantage of RL in terms of fast-adaptation. Empirical results show that the RL-based method consistently finds better solutions compared to classical heuristics.

</p>
</details>

<details><summary><b>Integrated Optimization of Predictive and Prescriptive Tasks</b>
<a href="https://arxiv.org/abs/2101.00354">arxiv:2101.00354</a>
&#x1F4C8; 2 <br>
<p>Mehmet Kolcu, Alper E. Murat</p></summary>
<p>

**Abstract:** In traditional machine learning techniques, the degree of closeness between true and predicted values generally measures the quality of predictions. However, these learning algorithms do not consider prescription problems where the predicted values will be used as input to decision problems. In this paper, we efficiently leverage feature variables, and we propose a new framework directly integrating predictive tasks under prescriptive tasks in order to prescribe consistent decisions. We train the parameters of predictive algorithm within a prescription problem via bilevel optimization techniques. We present the structure of our method and demonstrate its performance using synthetic data compared to classical methods like point-estimate-based, stochastic optimization and recently developed machine learning based optimization methods. In addition, we control generalization error using different penalty approaches and optimize the integration over validation data set.

</p>
</details>

<details><summary><b>TenIPS: Inverse Propensity Sampling for Tensor Completion</b>
<a href="https://arxiv.org/abs/2101.00323">arxiv:2101.00323</a>
&#x1F4C8; 2 <br>
<p>Chengrun Yang, Lijun Ding, Ziyang Wu, Madeleine Udell</p></summary>
<p>

**Abstract:** Tensors are widely used to represent multiway arrays of data. The recovery of missing entries in a tensor has been extensively studied, generally under the assumption that entries are missing completely at random (MCAR). However, in most practical settings, observations are missing not at random (MNAR): the probability that a given entry is observed (also called the propensity) may depend on other entries in the tensor or even on the value of the missing entry. In this paper, we study the problem of completing a partially observed tensor with MNAR observations, without prior information about the propensities. To complete the tensor, we assume that both the original tensor and the tensor of propensities have low multilinear rank. The algorithm first estimates the propensities using a convex relaxation and then predicts missing values using a higher-order SVD approach, reweighting the observed tensor by the inverse propensities. We provide finite-sample error bounds on the resulting complete tensor. Numerical experiments demonstrate the effectiveness of our approach.

</p>
</details>

<details><summary><b>Interval Type-2 Enhanced Possibilistic Fuzzy C-Means Clustering for Gene Expression Data Analysis</b>
<a href="https://arxiv.org/abs/2101.00304">arxiv:2101.00304</a>
&#x1F4C8; 2 <br>
<p>Shahabeddin Sotudian, Mohammad Hossein Fazel Zarandi</p></summary>
<p>

**Abstract:** Both FCM and PCM clustering methods have been widely applied to pattern recognition and data clustering. Nevertheless, FCM is sensitive to noise and PCM occasionally generates coincident clusters. PFCM is an extension of the PCM model by combining FCM and PCM, but this method still suffers from the weaknesses of PCM and FCM. In the current paper, the weaknesses of the PFCM algorithm are corrected and the enhanced possibilistic fuzzy c-means (EPFCM) clustering algorithm is presented. EPFCM can still be sensitive to noise. Therefore, we propose an interval type-2 enhanced possibilistic fuzzy c-means (IT2EPFCM) clustering method by utilizing two fuzzifiers $(m_1, m_2)$ for fuzzy memberships and two fuzzifiers $(θ_1, θ_2)$ for possibilistic typicalities. Our computational results show the superiority of the proposed approaches compared with several state-of-the-art techniques in the literature. Finally, the proposed methods are implemented for analyzing microarray gene expression data.

</p>
</details>

<details><summary><b>An iterative K-FAC algorithm for Deep Learning</b>
<a href="https://arxiv.org/abs/2101.00218">arxiv:2101.00218</a>
&#x1F4C8; 2 <br>
<p>Yingshi Chen</p></summary>
<p>

**Abstract:** Kronecker-factored Approximate Curvature (K-FAC) method is a high efficiency second order optimizer for the deep learning. Its training time is less than SGD(or other first-order method) with same accuracy in many large-scale problems. The key of K-FAC is to approximates Fisher information matrix (FIM) as a block-diagonal matrix where each block is an inverse of tiny Kronecker factors. In this short note, we present CG-FAC -- an new iterative K-FAC algorithm. It uses conjugate gradient method to approximate the nature gradient. This CG-FAC method is matrix-free, that is, no need to generate the FIM matrix, also no need to generate the Kronecker factors A and G. We prove that the time and memory complexity of iterative CG-FAC is much less than that of standard K-FAC algorithm.

</p>
</details>

<details><summary><b>A Hybrid MLP-SVM Model for Classification using Spatial-Spectral Features on Hyper-Spectral Images</b>
<a href="https://arxiv.org/abs/2101.00214">arxiv:2101.00214</a>
&#x1F4C8; 2 <br>
<p>Ginni Garg, Dheeraj Kumar,  ArvinderPal, Yash Sonker, Ritu Garg</p></summary>
<p>

**Abstract:** There are many challenges in the classification of hyper spectral images such as large dimensionality, scarcity of labeled data and spatial variability of spectral signatures. In this proposed method, we make a hybrid classifier (MLP-SVM) using multilayer perceptron (MLP) and support vector machine (SVM) which aimed to improve the various classification parameters such as accuracy, precision, recall, f-score and to predict the region without ground truth. In proposed method, outputs from the last hidden layer of the neural net-ork become the input to the SVM, which finally classifies into various desired classes. In the present study, we worked on Indian Pines, U. Pavia and Salinas dataset with 16, 9, 16 classes and 200, 103 and 204 reflectance bands respectively, which is provided by AVIRIS and ROSIS sensor of NASA Jet propulsion laboratory. The proposed method significantly increases the accuracy on testing dataset to 93.22%, 96.87%, 93.81% as compare to 86.97%, 88.58%, 88.85% and 91.61%, 96.20%, 90.68% based on individual classifiers SVM and MLP on Indian Pines, U. Pavia and Salinas datasets respectively.

</p>
</details>

<details><summary><b>A Multi-disciplinary Ensemble Algorithm for Clustering Heterogeneous Datasets</b>
<a href="https://arxiv.org/abs/2102.08361">arxiv:2102.08361</a>
&#x1F4C8; 1 <br>
<p>Bryar A. Hassan, Tarik A. Rashid</p></summary>
<p>

**Abstract:** Clustering is a commonly used method for exploring and analysing data where the primary objective is to categorise observations into similar clusters. In recent decades, several algorithms and methods have been developed for analysing clustered data. We notice that most of these techniques deterministically define a cluster based on the value of the attributes, distance, and density of homogenous and single-featured datasets. However, these definitions are not successful in adding clear semantic meaning to the clusters produced. Evolutionary operators and statistical and multi-disciplinary techniques may help in generating meaningful clusters. Based on this premise, we propose a new evolutionary clustering algorithm (ECAStar) based on social class ranking and meta-heuristic algorithms for stochastically analysing heterogeneous and multiple-featured datasets. The ECAStar is integrated with recombinational evolutionary operators, Levy flight optimisation, and some statistical techniques, such as quartiles and percentiles, as well as the Euclidean distance of the K-means algorithm. Experiments are conducted to evaluate the ECAStar against five conventional approaches: K-means (KM), K-meansPlusPlus (KMPlusPlus), expectation maximisation (EM), learning vector quantisation (LVQ), and the genetic algorithm for clusteringPlusPlus (GENCLUSTPlusPlus).

</p>
</details>

<details><summary><b>Quantifying Spatial Homogeneity of Urban Road Networks via Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2101.00307">arxiv:2101.00307</a>
&#x1F4C8; 1 <br>
<p>Jiawei Xue, Nan Jiang, Senwei Liang, Qiyuan Pang, Takahiro Yabe, Satish V. Ukkusuri, Jianzhu Ma</p></summary>
<p>

**Abstract:** Quantifying the topological similarities of different parts of urban road networks (URNs) enables us to understand the urban growth patterns. While conventional statistics provide useful information about characteristics of either a single node's direct neighbors or the entire network, such metrics fail to measure the similarities of subnetworks considering local indirect neighborhood relationships. In this study, we propose a graph-based machine-learning method to quantify the spatial homogeneity of subnetworks. We apply the method to 11,790 urban road networks across 30 cities worldwide to measure the spatial homogeneity of road networks within each city and across different cities. We find that intra-city spatial homogeneity is highly associated with socioeconomic statuses such as GDP and population growth. Moreover, inter-city spatial homogeneity obtained by transferring the model across different cities, reveals the inter-city similarity of urban network structures originating in Europe, passed on to cities in the US and Asia. Socioeconomic development and inter-city similarity revealed using our method can be leveraged to understand and transfer insights across cities. It also enables us to address urban policy challenges including network planning in rapidly urbanizing areas and combating regional inequality.

</p>
</details>

<details><summary><b>Efficient Learning-based Scheduling for Information Freshness in Wireless Networks</b>
<a href="https://arxiv.org/abs/2101.00257">arxiv:2101.00257</a>
&#x1F4C8; 1 <br>
<p>Bin Li</p></summary>
<p>

**Abstract:** Motivated by the recent trend of integrating artificial intelligence into the Internet-of-Things (IoT), we consider the problem of scheduling packets from multiple sensing sources to a central controller over a wireless network. Here, packets from different sensing sources have different values or degrees of importance to the central controller for intelligent decision making. In such a setup, it is critical to provide timely and valuable information for the central controller. In this paper, we develop a parameterized maximum-weight type scheduling policy that combines both the AoI metrics and Upper Confidence Bound (UCB) estimates in its weight measure with parameter $η$. Here, UCB estimates balance the tradeoff between exploration and exploitation in learning and are critical for yielding a small cumulative regret. We show that our proposed algorithm yields the running average total age at most by $O(N^2η)$. We also prove that our proposed algorithm achieves the cumulative regret over time horizon $T$ at most by $O(NT/η+\sqrt{NT\log T})$. This reveals a tradeoff between the cumulative regret and the running average total age: when increasing $η$, the cumulative regret becomes smaller, but is at the cost of increasing running average total age. Simulation results are provided to evaluate the efficiency of our proposed algorithm.

</p>
</details>

<details><summary><b>On Stochastic Variance Reduced Gradient Method for Semidefinite Optimization</b>
<a href="https://arxiv.org/abs/2101.00236">arxiv:2101.00236</a>
&#x1F4C8; 1 <br>
<p>Jinshan Zeng, Yixuan Zha, Ke Ma, Yuan Yao</p></summary>
<p>

**Abstract:** The low-rank stochastic semidefinite optimization has attracted rising attention due to its wide range of applications. The nonconvex reformulation based on the low-rank factorization, significantly improves the computational efficiency but brings some new challenge to the analysis. The stochastic variance reduced gradient (SVRG) method has been regarded as one of the most effective methods. SVRG in general consists of two loops, where a reference full gradient is first evaluated in the outer loop and then used to yield a variance reduced estimate of the current gradient in the inner loop. Two options have been suggested to yield the output of the inner loop, where Option I sets the output as its last iterate, and Option II yields the output via random sampling from all the iterates in the inner loop. However, there is a significant gap between the theory and practice of SVRG when adapted to the stochastic semidefinite programming (SDP). SVRG practically works better with Option I, while most of existing theoretical results focus on Option II. In this paper, we fill this gap via exploiting a new semi-stochastic variant of the original SVRG with Option I adapted to the semidefinite optimization. Equipped with this, we establish the global linear submanifold convergence (i.e., converging exponentially fast to a submanifold of a global minimum under the orthogonal group action) of the proposed SVRG method, given a provable initialization scheme and under certain smoothness and restricted strongly convex assumptions. Our analysis includes the effects of the mini-batch size and update frequency in the inner loop as well as two practical step size strategies, the fixed and stabilized Barzilai-Borwein step sizes. Some numerical results in matrix sensing demonstrate the efficiency of proposed SVRG method outperforming Option II counterpart as well as others.

</p>
</details>

<details><summary><b>Dynamic Federated Learning-Based Economic Framework for Internet-of-Vehicles</b>
<a href="https://arxiv.org/abs/2101.00191">arxiv:2101.00191</a>
&#x1F4C8; 1 <br>
<p>Yuris Mulya Saputra, Dinh Thai Hoang, Diep N. Nguyen, Le-Nam Tran, Shimin Gong, Eryk Dutkiewicz</p></summary>
<p>

**Abstract:** Federated learning (FL) can empower Internet-of-Vehicles (IoV) networks by leveraging smart vehicles (SVs) to participate in the learning process with minimum data exchanges and privacy disclosure. The collected data and learned knowledge can help the vehicular service provider (VSP) improve the global model accuracy, e.g., for road safety as well as better profits for both VSP and participating SVs. Nonetheless, there exist major challenges when implementing the FL in IoV networks, such as dynamic activities and diverse quality-of-information (QoI) from a large number of SVs, VSP's limited payment budget, and profit competition among SVs. In this paper, we propose a novel dynamic FL-based economic framework for an IoV network to address these challenges. Specifically, the VSP first implements an SV selection method to determine a set of the best SVs for the FL process according to the significance of their current locations and information history at each learning round. Then, each selected SV can collect on-road information and offer a payment contract to the VSP based on its collected QoI. For that, we develop a multi-principal one-agent contract-based policy to maximize the profits of the VSP and learning SVs under the VSP's limited payment budget and asymmetric information between the VSP and SVs. Through experimental results using real-world on-road datasets, we show that our framework can converge 57% faster (even with only 10% of active SVs in the network) and obtain much higher social welfare of the network (up to 27.2 times) compared with those of other baseline FL methods.

</p>
</details>

<details><summary><b>ECG-Based Driver Stress Levels Detection System Using Hyperparameter Optimization</b>
<a href="https://arxiv.org/abs/2101.00165">arxiv:2101.00165</a>
&#x1F4C8; 1 <br>
<p>Mohammad Naim Rastgoo, Bahareh Nakisa, Andry Rakotonirainy, Frederic Maire, Vinod Chandran</p></summary>
<p>

**Abstract:** Stress and driving are a dangerous combination which can lead to crashes, as evidenced by the large number of road traffic crashes that involve stress. Motivated by the need to address the significant costs of driver stress, it is essential to build a practical system that can classify driver stress level with high accuracy. However, the performance of an accurate driving stress levels classification system depends on hyperparameter optimization choices such as data segmentation (windowing hyperparameters). The configuration setting of hyperparameters, which has an enormous impact on the system performance, are typically hand-tuned while evaluating the algorithm. This tuning process is time consuming and often depends on personal experience. There are also no generic optimal values for hyperparameters values. In this work, we propose a meta-heuristic approach to support automated hyperparameter optimization and provide a real-time driver stress detection system. This is the first systematic study of optimizing windowing hyperparameters based on Electrocardiogram (ECG) signal in the domain of driving safety. Our approach is to propose a framework based on Particle Swarm Optimization algorithm (PSO) to select an optimal/near optimal windowing hyperparameters values. The performance of the proposed framework is evaluated on two datasets: a public dataset (DRIVEDB dataset) and our collected dataset using an advanced simulator. DRIVEDB dataset was collected in a real time driving scenario, and our dataset was collected using an advanced driving simulator in the control environment. We demonstrate that optimising the windowing hyperparameters yields significant improvement in terms of accuracy. The most accurate built model applied to the public dataset and our dataset, based on the selected windowing hyperparameters, achieved 92.12% and 77.78% accuracy, respectively.

</p>
</details>

<details><summary><b>Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering</b>
<a href="https://arxiv.org/abs/2101.00294">arxiv:2101.00294</a>
&#x1F4C8; 0 <br>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen</p></summary>
<p>

**Abstract:** Current open-domain question answering systems often follow a Retriever-Reader architecture, where the retriever first retrieves relevant passages and the reader then reads the retrieved passages to form an answer. In this paper, we propose a simple and effective passage reranking method, named Reader-guIDEd Reranker (RIDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking. We show that RIDER, despite its simplicity, achieves 10 to 20 absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains without refining the retriever or reader. In addition, RIDER, without any training, outperforms state-of-the-art transformer-based supervised rerankers. Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input after passage reranking.

</p>
</details>

<details><summary><b>On a Faster $R$-Linear Convergence Rate of the Barzilai-Borwein Method</b>
<a href="https://arxiv.org/abs/2101.00205">arxiv:2101.00205</a>
&#x1F4C8; 0 <br>
<p>Dawei Li, Ruoyu Sun</p></summary>
<p>

**Abstract:** The Barzilai-Borwein (BB) method has demonstrated great empirical success in nonlinear optimization. However, the convergence speed of BB method is not well understood, as the known convergence rate of BB method for quadratic problems is much worse than the steepest descent (SD) method. Therefore, there is a large discrepancy between theory and practice. To shrink this gap, we prove that the BB method converges $R$-linearly at a rate of $1-1/κ$, where $κ$ is the condition number, for strongly convex quadratic problems. In addition, an example with the theoretical rate of convergence is constructed, indicating the tightness of our bound.

</p>
</details>


{% endraw %}
Prev: [2020.12.31]({{ '/2020/12/31/2020.12.31.html' | relative_url }})  Next: [2021.01.02]({{ '/2021/01/02/2021.01.02.html' | relative_url }})