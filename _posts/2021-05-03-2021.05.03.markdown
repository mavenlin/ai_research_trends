## Summary for 2021-05-03, created on 2021-12-21


<details><summary><b>Graph Learning: A Survey</b>
<a href="https://arxiv.org/abs/2105.00696">arxiv:2105.00696</a>
&#x1F4C8; 119 <br>
<p>Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, Huan Liu</p></summary>
<p>

**Abstract:** Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.

</p>
</details>

<details><summary><b>Learning to drive from a world on rails</b>
<a href="https://arxiv.org/abs/2105.00636">arxiv:2105.00636</a>
&#x1F4C8; 23 <br>
<p>Dian Chen, Vladlen Koltun, Philipp Krähenbühl</p></summary>
<p>

**Abstract:** We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a nonreactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. At the time of writing, our method ranks first on the CARLA leaderboard, attaining a 25% higher driving score while using 40 times less data. Our method is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.

</p>
</details>

<details><summary><b>Initialization and Regularization of Factorized Neural Layers</b>
<a href="https://arxiv.org/abs/2105.01029">arxiv:2105.01029</a>
&#x1F4C8; 16 <br>
<p>Mikhail Khodak, Neil Tenenholtz, Lester Mackey, Nicolò Fusi</p></summary>
<p>

**Abstract:** Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.

</p>
</details>

<details><summary><b>Curious Representation Learning for Embodied Intelligence</b>
<a href="https://arxiv.org/abs/2105.01060">arxiv:2105.01060</a>
&#x1F4C8; 15 <br>
<p>Yilun Du, Chuang Gan, Phillip Isola</p></summary>
<p>

**Abstract:** Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.

</p>
</details>

<details><summary><b>Switching Contexts: Transportability Measures for NLP</b>
<a href="https://arxiv.org/abs/2105.00823">arxiv:2105.00823</a>
&#x1F4C8; 9 <br>
<p>Guy Marshall, Mokanarangan Thayaparan, Philip Osborne, Andre Freitas</p></summary>
<p>

**Abstract:** This paper explores the topic of transportability, as a sub-area of generalisability. By proposing the utilisation of metrics based on well-established statistics, we are able to estimate the change in performance of NLP models in new contexts. Defining a new measure for transportability may allow for better estimation of NLP system performance in new domains, and is crucial when assessing the performance of NLP systems in new tasks and domains. Through several instances of increasing complexity, we demonstrate how lightweight domain similarity measures can be used as estimators for the transportability in NLP applications. The proposed transportability measures are evaluated in the context of Named Entity Recognition and Natural Language Inference tasks.

</p>
</details>

<details><summary><b>Bird-Area Water-Bodies Dataset (BAWD) and Predictive AI Model for Avian Botulism Outbreak (AVI-BoT)</b>
<a href="https://arxiv.org/abs/2105.00924">arxiv:2105.00924</a>
&#x1F4C8; 8 <br>
<p>Narayani Bhatia, Devang Mahesh, Jashandeep Singh, Manan Suri</p></summary>
<p>

**Abstract:** Avian botulism caused by a bacterium, Clostridium botulinum, causes a paralytic disease in birds often leading to high fatality, and is usually diagnosed using molecular techniques. Diagnostic techniques for Avian botulism include: Mouse Bioassay, ELISA, PCR, all of which are time-consuming, laborious and require invasive sample collection from affected sites. In this study, we build a first-ever multi-spectral, remote-sensing imagery based global Bird-Area Water-bodies Dataset (BAWD) (i.e. fused satellite images of water-body sites important for avian fauna) backed by on-ground reporting evidence of outbreaks. In the current version, BAWD covers a total ground area of 904 sq.km from two open source satellite projects (Sentinel and Landsat). BAWD consists of 17 topographically diverse global sites spanning across 4 continents, with locations monitored over a time-span of 3 years (2016-2020). Using BAWD and state-of-the-art deep-learning techniques we propose a first-ever Artificial Intelligence based (AI) model to predict potential outbreak of Avian botulism called AVI-BoT (Aerosol, Visible, Infra-red (NIR/SWIR) and Bands of Thermal). AVI-BoT uses fused multi-spectral satellite images of water-bodies (10-bands) as input to generate a spatial prediction map depicting probability of potential Avian botulism outbreaks. We also train and investigate a simpler (5-band) Causative-Factor model (based on prominent physiological factors reported in literature as conducive for outbreak) to predict Avian botulism. Using AVI-BoT, we achieve a training accuracy of 0.94 and validation accuracy of 0.96 on BAWD, far superior in comparison to our Causative factors model. The proposed technique presents a scale-able, low-cost, non-invasive methodology for continuous monitoring of bird-habitats against botulism outbreaks with the potential of saving valuable fauna lives.

</p>
</details>

<details><summary><b>How Bayesian Should Bayesian Optimisation Be?</b>
<a href="https://arxiv.org/abs/2105.00894">arxiv:2105.00894</a>
&#x1F4C8; 7 <br>
<p>George De Ath, Richard Everson, Jonathan Fieldsend</p></summary>
<p>

**Abstract:** Bayesian optimisation (BO) uses probabilistic surrogate models - usually Gaussian processes (GPs) - for the optimisation of expensive black-box functions. At each BO iteration, the GP hyperparameters are fit to previously-evaluated data by maximising the marginal likelihood. However, this fails to account for uncertainty in the hyperparameters themselves, leading to overconfident model predictions. This uncertainty can be accounted for by taking the Bayesian approach of marginalising out the model hyperparameters.
  We investigate whether a fully-Bayesian treatment of the Gaussian process hyperparameters in BO (FBBO) leads to improved optimisation performance. Since an analytic approach is intractable, we compare FBBO using three approximate inference schemes to the maximum likelihood approach, using the Expected Improvement (EI) and Upper Confidence Bound (UCB) acquisition functions paired with ARD and isotropic Matern kernels, across 15 well-known benchmark problems for 4 observational noise settings. FBBO using EI with an ARD kernel leads to the best performance in the noise-free setting, with much less difference between combinations of BO components when the noise is increased. FBBO leads to over-exploration with UCB, but is not detrimental with EI. Therefore, we recommend that FBBO using EI with an ARD kernel as the default choice for BO.

</p>
</details>

<details><summary><b>Fast Multi-Step Critiquing for VAE-based Recommender Systems</b>
<a href="https://arxiv.org/abs/2105.00774">arxiv:2105.00774</a>
&#x1F4C8; 7 <br>
<p>Diego Antognini, Boi Faltings</p></summary>
<p>

**Abstract:** Recent studies have shown that providing personalized explanations alongside recommendations increases trust and perceived quality. Furthermore, it gives users an opportunity to refine the recommendations by critiquing parts of the explanations. On one hand, current recommender systems model the recommendation, explanation, and critiquing objectives jointly, but this creates an inherent trade-off between their respective performance. On the other hand, although recent latent linear critiquing approaches are built upon an existing recommender system, they suffer from computational inefficiency at inference due to the objective optimized at each conversation's turn. We address these deficiencies with M&Ms-VAE, a novel variational autoencoder for recommendation and explanation that is based on multimodal modeling assumptions. We train the model under a weak supervision scheme to simulate both fully and partially observed variables. Then, we leverage the generalization ability of a trained M&Ms-VAE model to embed the user preference and the critique separately. Our work's most important innovation is our critiquing module, which is built upon and trained in a self-supervised manner with a simple ranking objective. Experiments on four real-world datasets demonstrate that among state-of-the-art models, our system is the first to dominate or match the performance in terms of recommendation, explanation, and multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x faster than the best baselines. Finally, we show that our model infers coherent joint and cross generation, even under weak supervision, thanks to our multimodal-based modeling and training scheme.

</p>
</details>

<details><summary><b>Bias in Knowledge Graphs -- an Empirical Study with Movie Recommendation and Different Language Editions of DBpedia</b>
<a href="https://arxiv.org/abs/2105.00674">arxiv:2105.00674</a>
&#x1F4C8; 7 <br>
<p>Michael Matthias Voit, Heiko Paulheim</p></summary>
<p>

**Abstract:** Public knowledge graphs such as DBpedia and Wikidata have been recognized as interesting sources of background knowledge to build content-based recommender systems. They can be used to add information about the items to be recommended and links between those. While quite a few approaches for exploiting knowledge graphs have been proposed, most of them aim at optimizing the recommendation strategy while using a fixed knowledge graph. In this paper, we take a different approach, i.e., we fix the recommendation strategy and observe changes when using different underlying knowledge graphs. Particularly, we use different language editions of DBpedia. We show that the usage of different knowledge graphs does not only lead to differently biased recommender systems, but also to recommender systems that differ in performance for particular fields of recommendations.

</p>
</details>

<details><summary><b>Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization</b>
<a href="https://arxiv.org/abs/2105.01015">arxiv:2105.01015</a>
&#x1F4C8; 6 <br>
<p>Julia Guerrero-Viu, Sven Hauns, Sergio Izquierdo, Guilherme Miotto, Simon Schrodi, Andre Biedenkapp, Thomas Elsken, Difan Deng, Marius Lindauer, Frank Hutter</p></summary>
<p>

**Abstract:** Neural architecture search (NAS) and hyperparameter optimization (HPO) make deep learning accessible to non-experts by automatically finding the architecture of the deep neural network to use and tuning the hyperparameters of the used training pipeline. While both NAS and HPO have been studied extensively in recent years, NAS methods typically assume fixed hyperparameters and vice versa - there exists little work on joint NAS + HPO. Furthermore, NAS has recently often been framed as a multi-objective optimization problem, in order to take, e.g., resource requirements into account. In this paper, we propose a set of methods that extend current approaches to jointly optimize neural architectures and hyperparameters with respect to multiple objectives. We hope that these methods will serve as simple baselines for future research on multi-objective joint NAS + HPO. To facilitate this, all our code is available at https://github.com/automl/multi-obj-baselines.

</p>
</details>

<details><summary><b>Automatic Collection Creation and Recommendation</b>
<a href="https://arxiv.org/abs/2105.01004">arxiv:2105.01004</a>
&#x1F4C8; 6 <br>
<p>Sanidhya Singal, Piyush Singh, Manjeet Dahiya</p></summary>
<p>

**Abstract:** We present a collection recommender system that can automatically create and recommend collections of items at a user level. Unlike regular recommender systems, which output top-N relevant items, a collection recommender system outputs collections of items such that the items in the collections are relevant to a user, and the items within a collection follow a specific theme. Our system builds on top of the user-item representations learnt by item recommender systems. We employ dimensionality reduction and clustering techniques along with intuitive heuristics to create collections with their ratings and titles.
  We test these ideas in a real-world setting of music recommendation, within a popular music streaming service. We find that there is a 2.3x increase in recommendation-driven consumption when recommending collections over items. Further, it results in effective utilization of real estate and leads to recommending a more and diverse set of items. To our knowledge, these are first of its kind experiments at such a large scale.

</p>
</details>

<details><summary><b>Robust 3D Cell Segmentation: Extending the View of Cellpose</b>
<a href="https://arxiv.org/abs/2105.00794">arxiv:2105.00794</a>
&#x1F4C8; 6 <br>
<p>Dennis Eschweiler, Richard S. Smith, Johannes Stegmaier</p></summary>
<p>

**Abstract:** Increasing data set sizes of 3D microscopy imaging experiments demand for an automation of segmentation processes to be able to extract meaningful biomedical information. Due to the shortage of annotated 3D image data that can be used for machine learning-based approaches, 3D segmentation approaches are required to be robust and to generalize well to unseen data. The Cellpose approach proposed by Stringer \textit{et al.} \cite{stringer2020} proved to be such a generalist approach for cell instance segmentation tasks. In this paper, we extend the Cellpose approach to improve segmentation accuracy on 3D image data and we further show how the formulation of the gradient maps can be simplified while still being robust and reaching similar segmentation accuracy. The code is publicly available and was integrated into two established open-source applications that allow using the 3D extension of Cellpose without any programming knowledge.

</p>
</details>

<details><summary><b>Streaming end-to-end speech recognition with jointly trained neural feature enhancement</b>
<a href="https://arxiv.org/abs/2105.01254">arxiv:2105.01254</a>
&#x1F4C8; 5 <br>
<p>Chanwoo Kim, Abhinav Garg, Dhananjaya Gowda, Seongkyu Mun, Changwoo Han</p></summary>
<p>

**Abstract:** In this paper, we present a streaming end-to-end speech recognition model based on Monotonic Chunkwise Attention (MoCha) jointly trained with enhancement layers. Even though the MoCha attention enables streaming speech recognition with recognition accuracy comparable to a full attention-based approach, training this model is sensitive to various factors such as the difficulty of training examples, hyper-parameters, and so on. Because of these issues, speech recognition accuracy of a MoCha-based model for clean speech drops significantly when a multi-style training approach is applied. Inspired by Curriculum Learning [1], we introduce two training strategies: Gradual Application of Enhanced Features (GAEF) and Gradual Reduction of Enhanced Loss (GREL). With GAEF, the model is initially trained using clean features. Subsequently, the portion of outputs from the enhancement layers gradually increases. With GREL, the portion of the Mean Squared Error (MSE) loss for the enhanced output gradually reduces as training proceeds. In experimental results on the LibriSpeech corpus and noisy far-field test sets, the proposed model with GAEF-GREL training strategies shows significantly better results than the conventional multi-style training approach.

</p>
</details>

<details><summary><b>Quality Assurance Challenges for Machine Learning Software Applications During Software Development Life Cycle Phases</b>
<a href="https://arxiv.org/abs/2105.01195">arxiv:2105.01195</a>
&#x1F4C8; 5 <br>
<p>Md Abdullah Al Alamin, Gias Uddin</p></summary>
<p>

**Abstract:** In the past decades, the revolutionary advances of Machine Learning (ML) have shown a rapid adoption of ML models into software systems of diverse types. Such Machine Learning Software Applications (MLSAs) are gaining importance in our daily lives. As such, the Quality Assurance (QA) of MLSAs is of paramount importance. Several research efforts are dedicated to determining the specific challenges we can face while adopting ML models into software systems. However, we are aware of no research that offered a holistic view of the distribution of those ML quality assurance challenges across the various phases of software development life cycles (SDLC). This paper conducts an in-depth literature review of a large volume of research papers that focused on the quality assurance of ML models. We developed a taxonomy of MLSA quality assurance issues by mapping the various ML adoption challenges across different phases of SDLC. We provide recommendations and research opportunities to improve SDLC practices based on the taxonomy. This mapping can help prioritize quality assurance efforts of MLSAs where the adoption of ML models can be considered crucial.

</p>
</details>

<details><summary><b>Systematic Assessment of Hyperdimensional Computing for Epileptic Seizure Detection</b>
<a href="https://arxiv.org/abs/2105.00934">arxiv:2105.00934</a>
&#x1F4C8; 5 <br>
<p>Una Pale, Tomas Teijeiro, David Atienza</p></summary>
<p>

**Abstract:** Hyperdimensional computing is a promising novel paradigm for low-power embedded machine learning. It has been applied on different biomedical applications, and particularly on epileptic seizure detection. Unfortunately, due to differences in data preparation, segmentation, encoding strategies, and performance metrics, results are hard to compare, which makes building upon that knowledge difficult. Thus, the main goal of this work is to perform a systematic assessment of the HD computing framework for the detection of epileptic seizures, comparing different feature approaches mapped to HD vectors. More precisely, we test two previously implemented features as well as several novel approaches with HD computing on epileptic seizure detection. We evaluate them in a comparable way, i.e., with the same preprocessing setup, and with the identical performance measures. We use two different datasets in order to assess the generalizability of our conclusions. The systematic assessment involved three primary aspects relevant for potential wearable implementations: 1) detection performance, 2) memory requirements, and 3) computational complexity. Our analysis shows a significant difference in detection performance between approaches, but also that the ones with the highest performance might not be ideal for wearable applications due to their high memory or computational requirements. Furthermore, we evaluate a post-processing strategy to adjust the predictions to the dynamics of epileptic seizures, showing that performance is significantly improved in all the approaches and also that after post-processing, differences in performance are much smaller between approaches.

</p>
</details>

<details><summary><b>Generative Adversarial Reward Learning for Generalized Behavior Tendency Inference</b>
<a href="https://arxiv.org/abs/2105.00822">arxiv:2105.00822</a>
&#x1F4C8; 5 <br>
<p>Xiaocong Chen, Lina Yao, Xianzhi Wang, Aixin Sun, Wenjie Zhang, Quan Z. Sheng</p></summary>
<p>

**Abstract:** Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through dynamic interactions, e.g., in reinforcement learning based recommender systems. Reward function is crucial for most of reinforcement learning applications as it can provide the guideline about the optimization. However, current reinforcement-learning-based methods rely on manually-defined reward functions, which cannot adapt to dynamic and noisy environments. Besides, they generally use task-specific reward functions that sacrifice generalization ability. We propose a generative inverse reinforcement learning for user behavioral preference modelling, to address the above issues. Instead of using predefined reward functions, our model can automatically learn the rewards from user's actions based on discriminative actor-critic network and Wasserstein GAN. Our model provides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments show our method outperforms state-of-the-art methods in a variety of scenarios, namely traffic signal control, online recommender systems, and scanpath prediction.

</p>
</details>

<details><summary><b>Improving Landslide Detection on SAR Data through Deep Learning</b>
<a href="https://arxiv.org/abs/2105.00782">arxiv:2105.00782</a>
&#x1F4C8; 5 <br>
<p>Lorenzo Nava, Oriol Monserrat, Filippo Catani</p></summary>
<p>

**Abstract:** In this letter, we use deep-learning convolution neural networks (CNNs) to assess the landslide mapping and classification performances on optical images (from Sentinel-2) and SAR images (from Sentinel-1). The training and test zones used to independently evaluate the performance of the CNNs on different datasets are located in the eastern Iburi subprefecture in Hokkaido, where, at 03.08 local time (JST) on September 6, 2018, an Mw 6.6 earthquake triggered about 8000 coseismic landslides. We analyzed the conditions before and after the earthquake exploiting multi-polarization SAR as well as optical data by means of a CNN implemented in TensorFlow that points out the locations where the Landslide class is predicted as more likely. As expected, the CNN run on optical images proved itself excellent for the landslide detection task, achieving an overall accuracy of 99.20% while CNNs based on the combination of ground range detected (GRD) SAR data reached overall accuracies beyond 94%. Our findings show that the integrated use of SAR data may also allow for rapid mapping even during storms and under dense cloud cover and seems to provide comparable accuracy to classical optical change detection in landslide recognition and mapping.

</p>
</details>

<details><summary><b>Spectral Machine Learning for Pancreatic Mass Imaging Classification</b>
<a href="https://arxiv.org/abs/2105.00728">arxiv:2105.00728</a>
&#x1F4C8; 5 <br>
<p>Yiming Liu, Ying Chen, Guangming Pan, Weichung Wang, Wei-Chih Liao, Yee Liang Thian, Cheng E. Chee, Constantinos P. Anastassiades</p></summary>
<p>

**Abstract:** We present a novel spectral machine learning (SML) method in screening for pancreatic mass using CT imaging. Our algorithm is trained with approximately 30,000 images from 250 patients (50 patients with normal pancreas and 200 patients with abnormal pancreas findings) based on public data sources. A test accuracy of 94.6 percents was achieved in the out-of-sample diagnosis classification based on a total of approximately 15,000 images from 113 patients, whereby 26 out of 32 patients with normal pancreas and all 81 patients with abnormal pancreas findings were correctly diagnosed. SML is able to automatically choose fundamental images (on average 5 or 9 images for each patient) in the diagnosis classification and achieve the above mentioned accuracy. The computational time is 75 seconds for diagnosing 113 patients in a laptop with standard CPU running environment. Factors that influenced high performance of a well-designed integration of spectral learning and machine learning included: 1) use of eigenvectors corresponding to several of the largest eigenvalues of sample covariance matrix (spike eigenvectors) to choose input attributes in classification training, taking into account only the fundamental information of the raw images with less noise; 2) removal of irrelevant pixels based on mean-level spectral test to lower the challenges of memory capacity and enhance computational efficiency while maintaining superior classification accuracy; 3) adoption of state-of-the-art machine learning classification, gradient boosting and random forest. Our methodology showcases practical utility and improved accuracy of image diagnosis in pancreatic mass screening in the era of AI.

</p>
</details>

<details><summary><b>One Model to Rule them All: Towards Zero-Shot Learning for Databases</b>
<a href="https://arxiv.org/abs/2105.00642">arxiv:2105.00642</a>
&#x1F4C8; 5 <br>
<p>Benjamin Hilprecht, Carsten Binnig</p></summary>
<p>

**Abstract:** In this paper, we present our vision of so called zero-shot learning for databases which is a new learning approach for database components. Zero-shot learning for databases is inspired by recent advances in transfer learning of models such as GPT-3 and can support a new database out-of-the box without the need to train a new model. Furthermore, it can easily be extended to few-shot learning by further retraining the model on the unseen database. As a first concrete contribution in this paper, we show the feasibility of zero-shot learning for the task of physical cost estimation and present very promising initial results. Moreover, as a second contribution we discuss the core challenges related to zero-shot learning for databases and present a roadmap to extend zero-shot learning towards many other tasks beyond cost estimation or even beyond classical database systems and workloads.

</p>
</details>

<details><summary><b>VersaGNN: a Versatile accelerator for Graph neural networks</b>
<a href="https://arxiv.org/abs/2105.01280">arxiv:2105.01280</a>
&#x1F4C8; 4 <br>
<p>Feng Shi, Ahren Yiqiao Jin, Song-Chun Zhu</p></summary>
<p>

**Abstract:** \textit{Graph Neural Network} (GNN) is a promising approach for analyzing graph-structured data that tactfully captures their dependency information via node-level message passing. It has achieved state-of-the-art performances in many tasks, such as node classification, graph matching, clustering, and graph generation. As GNNs operate on non-Euclidean data, their irregular data access patterns cause considerable computational costs and overhead on conventional architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase performs vector additions where vectors are fetched with irregular strides. The \textit{Transformation} (or \textit{Node Embedding}) phase can be either dense or sparse-dense matrix multiplication. In this work, We propose \textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware accelerator that unifies dense and sparse matrix multiplication. By applying this single optimized systolic array to both aggregation and transformation phases, we have significantly reduced chip sizes and energy consumption. We then divide the computing engine into blocked systolic arrays to support the \textit{Strassen}'s algorithm for dense matrix multiplication, dramatically scaling down the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we also introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones to reduce computational cycles. Compared with current state-of-the-art GNN software frameworks, \textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$ energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy reduction on GPU.

</p>
</details>

<details><summary><b>Synthesizing time-series wound prognosis factors from electronic medical records using generative adversarial networks</b>
<a href="https://arxiv.org/abs/2105.01159">arxiv:2105.01159</a>
&#x1F4C8; 4 <br>
<p>Farnaz H. Foomani, D. M. Anisuzzaman, Jeffrey Niezgoda, Jonathan Niezgoda, William Guns, Sandeep Gopalakrishnan, Zeyun Yu</p></summary>
<p>

**Abstract:** Wound prognostic models not only provide an estimate of wound healing time to motivate patients to follow up their treatments but also can help clinicians to decide whether to use a standard care or adjuvant therapies and to assist them with designing clinical trials. However, collecting prognosis factors from Electronic Medical Records (EMR) of patients is challenging due to privacy, sensitivity, and confidentiality. In this study, we developed time series medical generative adversarial networks (GANs) to generate synthetic wound prognosis factors using very limited information collected during routine care in a specialized wound care facility. The generated prognosis variables are used in developing a predictive model for chronic wound healing trajectory. Our novel medical GAN can produce both continuous and categorical features from EMR. Moreover, we applied temporal information to our model by considering data collected from the weekly follow-ups of patients. Conditional training strategies were utilized to enhance training and generate classified data in terms of healing or non-healing. The ability of the proposed model to generate realistic EMR data was evaluated by TSTR (test on the synthetic, train on the real), discriminative accuracy, and visualization. We utilized samples generated by our proposed GAN in training a prognosis model to demonstrate its real-life application. Using the generated samples in training predictive models improved the classification accuracy by 6.66-10.01% compared to the previous EMR-GAN. Additionally, the suggested prognosis classifier has achieved the area under the curve (AUC) of 0.975, 0.968, and 0.849 when training the network using data from the first three visits, first two visits, and first visit, respectively. These results indicate a significant improvement in wound healing prediction compared to the previous prognosis models.

</p>
</details>

<details><summary><b>What can the millions of random treatments in nonexperimental data reveal about causes?</b>
<a href="https://arxiv.org/abs/2105.01152">arxiv:2105.01152</a>
&#x1F4C8; 4 <br>
<p>Andre F. Ribeiro, Frank Neffke, Ricardo Hausmann</p></summary>
<p>

**Abstract:** We propose a new method to estimate causal effects from nonexperimental data. Each pair of sample units is first associated with a stochastic 'treatment' - differences in factors between units - and an effect - a resultant outcome difference. It is then proposed that all such pairs can be combined to provide more accurate estimates of causal effects in observational data, provided a statistical model connecting combinatorial properties of treatments to the accuracy and unbiasedness of their effects. The article introduces one such model and a Bayesian approach to combine the $O(n^2)$ pairwise observations typically available in nonexperimnetal data. This also leads to an interpretation of nonexperimental datasets as incomplete, or noisy, versions of ideal factorial experimental designs.
  This approach to causal effect estimation has several advantages: (1) it expands the number of observations, converting thousands of individuals into millions of observational treatments; (2) starting with treatments closest to the experimental ideal, it identifies noncausal variables that can be ignored in the future, making estimation easier in each subsequent iteration while departing minimally from experiment-like conditions; (3) it recovers individual causal effects in heterogeneous populations. We evaluate the method in simulations and the National Supported Work (NSW) program, an intensively studied program whose effects are known from randomized field experiments. We demonstrate that the proposed approach recovers causal effects in common NSW samples, as well as in arbitrary subpopulations and an order-of-magnitude larger supersample with the entire national program data, outperforming Statistical, Econometrics and Machine Learning estimators in all cases...

</p>
</details>

<details><summary><b>Reinforcement Learning for Ridesharing: A Survey</b>
<a href="https://arxiv.org/abs/2105.01099">arxiv:2105.01099</a>
&#x1F4C8; 4 <br>
<p>Zhiwei Qin, Hongtu Zhu, Jieping Ye</p></summary>
<p>

**Abstract:** In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. Papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. Popular data sets and open simulation environments are also introduced. Subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.

</p>
</details>

<details><summary><b>AI-assisted super-resolution cosmological simulations II: Halo substructures, velocities and higher order statistics</b>
<a href="https://arxiv.org/abs/2105.01016">arxiv:2105.01016</a>
&#x1F4C8; 4 <br>
<p>Yueying Ni, Yin Li, Patrick Lachance, Rupert A. C. Croft, Tiziana Di Matteo, Simeon Bird, Yu Feng</p></summary>
<p>

**Abstract:** In this work, we expand and test the capabilities of our recently developed super-resolution (SR) model to generate high-resolution (HR) realizations of the full phase-space matter distribution, including both displacement and velocity, from computationally cheap low-resolution (LR) cosmological N-body simulations. The SR model enhances the simulation resolution by generating 512 times more tracer particles, extending into the deeply non-linear regime where complex structure formation processes take place. We validate the SR model by deploying the model in 10 test simulations of box size 100 Mpc/h, and examine the matter power spectra, bispectra and 2D power spectra in redshift space. We find the generated SR field matches the true HR result at percent level down to scales of k ~ 10 h/Mpc. We also identify and inspect dark matter halos and their substructures. Our SR model generate visually authentic small-scale structures, that cannot be resolved by the LR input, and are in good statistical agreement with the real HR results. The SR model performs satisfactorily on the halo occupation distribution, halo correlations in both real and redshift space, and the pairwise velocity distribution, matching the HR results with comparable scatter, thus demonstrating its potential in making mock halo catalogs. The SR technique can be a powerful and promising tool for modelling small-scale galaxy formation physics in large cosmological volumes.

</p>
</details>

<details><summary><b>Robotic Surgery With Lean Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.01006">arxiv:2105.01006</a>
&#x1F4C8; 4 <br>
<p>Yotam Barnoy, Molly O'Brien, Will Wang, Gregory Hager</p></summary>
<p>

**Abstract:** As surgical robots become more common, automating away some of the burden of complex direct human operation becomes ever more feasible. Model-free reinforcement learning (RL) is a promising direction toward generalizable automated surgical performance, but progress has been slowed by the lack of efficient and realistic learning environments. In this paper, we describe adding reinforcement learning support to the da Vinci Skill Simulator, a training simulation used around the world to allow surgeons to learn and rehearse technical skills. We successfully teach an RL-based agent to perform sub-tasks in the simulator environment, using either image or state data. As far as we know, this is the first time an RL-based agent is taught from visual data in a surgical robotics environment. Additionally, we tackle the sample inefficiency of RL using a simple-to-implement system which we term hybrid-batch learning (HBL), effectively adding a second, long-term replay buffer to the Q-learning process. Additionally, this allows us to bootstrap learning from images from the data collected using the easier task of learning from state. We show that HBL decreases our learning times significantly.

</p>
</details>

<details><summary><b>Enhanced U-Net: A Feature Enhancement Network for Polyp Segmentation</b>
<a href="https://arxiv.org/abs/2105.00999">arxiv:2105.00999</a>
&#x1F4C8; 4 <br>
<p>Krushi Patel, Andres M. Bur, Guanghui Wang</p></summary>
<p>

**Abstract:** Colonoscopy is a procedure to detect colorectal polyps which are the primary cause for developing colorectal cancer. However, polyp segmentation is a challenging task due to the diverse shape, size, color, and texture of polyps, shuttle difference between polyp and its background, as well as low contrast of the colonoscopic images. To address these challenges, we propose a feature enhancement network for accurate polyp segmentation in colonoscopy images. Specifically, the proposed network enhances the semantic information using the novel Semantic Feature Enhance Module (SFEM). Furthermore, instead of directly adding encoder features to the respective decoder layer, we introduce an Adaptive Global Context Module (AGCM), which focuses only on the encoder's significant and hard fine-grained features. The integration of these two modules improves the quality of features layer by layer, which in turn enhances the final feature representation. The proposed approach is evaluated on five colonoscopy datasets and demonstrates superior performance compared to other state-of-the-art models.

</p>
</details>

<details><summary><b>Explaining Outcomes of Multi-Party Dialogues using Causal Learning</b>
<a href="https://arxiv.org/abs/2105.00944">arxiv:2105.00944</a>
&#x1F4C8; 4 <br>
<p>Priyanka Sinha, Pabitra Mitra, Antonio Anastasio Bruto da Costa, Nikolaos Kekatos</p></summary>
<p>

**Abstract:** Multi-party dialogues are common in enterprise social media on technical as well as non-technical topics. The outcome of a conversation may be positive or negative. It is important to analyze why a dialogue ends with a particular sentiment from the point of view of conflict analysis as well as future collaboration design. We propose an explainable time series mining algorithm for such analysis. A dialogue is represented as an attributed time series of occurrences of keywords, EMPATH categories, and inferred sentiments at various points in its progress. A special decision tree, with decision metrics that take into account temporal relationships between dialogue events, is used for predicting the cause of the outcome sentiment. Interpretable rules mined from the classifier are used to explain the prediction. Experimental results are presented for the enterprise social media posts in a large company.

</p>
</details>

<details><summary><b>Deep Neural Network for Musical Instrument Recognition using MFCCs</b>
<a href="https://arxiv.org/abs/2105.00933">arxiv:2105.00933</a>
&#x1F4C8; 4 <br>
<p>Saranga Kingkor Mahanta, Abdullah Faiz Ur Rahman Khilji, Partha Pakray</p></summary>
<p>

**Abstract:** The task of efficient automatic music classification is of vital importance and forms the basis for various advanced applications of AI in the musical domain. Musical instrument recognition is the task of instrument identification by virtue of its audio. This audio, also termed as the sound vibrations are leveraged by the model to match with the instrument classes. In this paper, we use an artificial neural network (ANN) model that was trained to perform classification on twenty different classes of musical instruments. Here we use use only the mel-frequency cepstral coefficients (MFCCs) of the audio data. Our proposed model trains on the full London philharmonic orchestra dataset which contains twenty classes of instruments belonging to the four families viz. woodwinds, brass, percussion, and strings. Based on experimental results our model achieves state-of-the-art accuracy on the same.

</p>
</details>

<details><summary><b>Exploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation</b>
<a href="https://arxiv.org/abs/2105.00708">arxiv:2105.00708</a>
&#x1F4C8; 4 <br>
<p>Yan-Bo Lin, Yu-Chiang Frank Wang</p></summary>
<p>

**Abstract:** Human perceives rich auditory experience with distinct sound heard by ears. Videos recorded with binaural audio particular simulate how human receives ambient sound. However, a large number of videos are with monaural audio only, which would degrade the user experience due to the lack of ambient information. To address this issue, we propose an audio spatialization framework to convert a monaural video into a binaural one exploiting the relationship across audio and visual components. By preserving the left-right consistency in both audio and visual modalities, our learning strategy can be viewed as a self-supervised learning technique, and alleviates the dependency on a large amount of video data with ground truth binaural audio data during training. Experiments on benchmark datasets confirm the effectiveness of our proposed framework in both semi-supervised and fully supervised scenarios, with ablation studies and visualization further support the use of our model for audio spatialization.

</p>
</details>

<details><summary><b>Non-I.I.D. Multi-Instance Learning for Predicting Instance and Bag Labels using Variational Auto-Encoder</b>
<a href="https://arxiv.org/abs/2105.01276">arxiv:2105.01276</a>
&#x1F4C8; 3 <br>
<p>Weijia Zhang</p></summary>
<p>

**Abstract:** Multi-instance learning is a type of weakly supervised learning. It deals with tasks where the data is a set of bags and each bag is a set of instances. Only the bag labels are observed whereas the labels for the instances are unknown. An important advantage of multi-instance learning is that by representing objects as a bag of instances, it is able to preserve the inherent dependencies among parts of the objects. Unfortunately, most existing algorithms assume all instances to be \textit{identically and independently distributed}, which violates real-world scenarios since the instances within a bag are rarely independent. In this work, we propose the Multi-Instance Variational Auto-Encoder (MIVAE) algorithm which explicitly models the dependencies among the instances for predicting both bag labels and instance labels. Experimental results on several multi-instance benchmarks and end-to-end medical imaging datasets demonstrate that MIVAE performs better than state-of-the-art algorithms for both instance label and bag label prediction tasks.

</p>
</details>

<details><summary><b>Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding</b>
<a href="https://arxiv.org/abs/2105.01187">arxiv:2105.01187</a>
&#x1F4C8; 3 <br>
<p>Zhengling Qi, Rui Miao, Xiaoke Zhang</p></summary>
<p>

**Abstract:** Data-driven individualized decision making has recently received increasing research interests. Most existing methods rely on the assumption of no unmeasured confounding, which unfortunately cannot be ensured in practice especially in observational studies. Motivated by the recent proposed proximal causal inference, we develop several proximal learning approaches to estimating optimal individualized treatment regimes (ITRs) in the presence of unmeasured confounding. In particular, we establish several identification results for different classes of ITRs, exhibiting the trade-off between the risk of making untestable assumptions and the value function improvement in decision making. Based on these results, we propose several classification-based approaches to finding a variety of restricted in-class optimal ITRs and develop their theoretical properties. The appealing numerical performance of our proposed methods is demonstrated via an extensive simulation study and one real data application.

</p>
</details>

<details><summary><b>Towards A Multi-agent System for Online Hate Speech Detection</b>
<a href="https://arxiv.org/abs/2105.01129">arxiv:2105.01129</a>
&#x1F4C8; 3 <br>
<p>Gaurav Sahu, Robin Cohen, Olga Vechtomova</p></summary>
<p>

**Abstract:** This paper envisions a multi-agent system for detecting the presence of hate speech in online social media platforms such as Twitter and Facebook. We introduce a novel framework employing deep learning techniques to coordinate the channels of textual and im-age processing. Our experimental results aim to demonstrate the effectiveness of our methods for classifying online content, training the proposed neural network model to effectively detect hateful instances in the input. We conclude with a discussion of how our system may be of use to provide recommendations to users who are managing online social networks, showcasing the immense potential of intelligent multi-agent systems towards delivering social good.

</p>
</details>

<details><summary><b>Algorithms are not neutral: Bias in collaborative filtering</b>
<a href="https://arxiv.org/abs/2105.01031">arxiv:2105.01031</a>
&#x1F4C8; 3 <br>
<p>Catherine Stinson</p></summary>
<p>

**Abstract:** Discussions of algorithmic bias tend to focus on examples where either the data or the people building the algorithms are biased. This gives the impression that clean data and good intentions could eliminate bias. The neutrality of the algorithms themselves is defended by prominent Artificial Intelligence researchers. However, algorithms are not neutral. In addition to biased data and biased algorithm makers, AI algorithms themselves can be biased. This is illustrated with the example of collaborative filtering, which is known to suffer from popularity, and homogenizing biases. Iterative information filtering algorithms in general create a selection bias in the course of learning from user responses to documents that the algorithm recommended. These are not merely biases in the statistical sense; these statistical biases can cause discriminatory outcomes. Data points on the margins of distributions of human data tend to correspond to marginalized people. Popularity and homogenizing biases have the effect of further marginalizing the already marginal. This source of bias warrants serious attention given the ubiquity of algorithmic decision-making.

</p>
</details>

<details><summary><b>Embedded training of neural-network sub-grid-scale turbulence models</b>
<a href="https://arxiv.org/abs/2105.01030">arxiv:2105.01030</a>
&#x1F4C8; 3 <br>
<p>Jonathan F. MacArt, Justin Sirignano, Jonathan B. Freund</p></summary>
<p>

**Abstract:** The weights of a deep neural network model are optimized in conjunction with the governing flow equations to provide a model for sub-grid-scale stresses in a temporally developing plane turbulent jet at Reynolds number $Re_0=6\,000$. The objective function for training is first based on the instantaneous filtered velocity fields from a corresponding direct numerical simulation, and the training is by a stochastic gradient descent method, which uses the adjoint Navier--Stokes equations to provide the end-to-end sensitivities of the model weights to the velocity fields. In-sample and out-of-sample testing on multiple dual-jet configurations show that its required mesh density in each coordinate direction for prediction of mean flow, Reynolds stresses, and spectra is half that needed by the dynamic Smagorinsky model for comparable accuracy. The same neural-network model trained directly to match filtered sub-grid-scale stresses -- without the constraint of being embedded within the flow equations during the training -- fails to provide a qualitatively correct prediction. The coupled formulation is generalized to train based only on mean-flow and Reynolds stresses, which are more readily available in experiments. The mean-flow training provides a robust model, which is important, though a somewhat less accurate prediction for the same coarse meshes, as might be anticipated due to the reduced information available for training in this case. The anticipated advantage of the formulation is that the inclusion of resolved physics in the training increases its capacity to extrapolate. This is assessed for the case of passive scalar transport, for which it outperforms established models due to improved mixing predictions.

</p>
</details>

<details><summary><b>Full-Reference Speech Quality Estimation with Attentional Siamese Neural Networks</b>
<a href="https://arxiv.org/abs/2105.00783">arxiv:2105.00783</a>
&#x1F4C8; 3 <br>
<p>Gabriel Mittags, Sebastian Möller</p></summary>
<p>

**Abstract:** In this paper, we present a full-reference speech quality prediction model with a deep learning approach. The model determines a feature representation of the reference and the degraded signal through a siamese recurrent convolutional network that shares the weights for both signals as input. The resulting features are then used to align the signals with an attention mechanism and are finally combined to estimate the overall speech quality. The proposed network architecture represents a simple solution for the time-alignment problem that occurs for speech signals transmitted through Voice-Over-IP networks and shows how the clean reference signal can be incorporated into speech quality models that are based on end-to-end trained neural networks.

</p>
</details>

<details><summary><b>Recognition of Oracle Bone Inscriptions by using Two Deep Learning Models</b>
<a href="https://arxiv.org/abs/2105.00777">arxiv:2105.00777</a>
&#x1F4C8; 3 <br>
<p>Yoshiyuki Fujikawa, Hengyi Li, Xuebin Yue, Aravinda C V, Amar Prabhu G, Lin Meng</p></summary>
<p>

**Abstract:** Oracle bone inscriptions (OBIs) contain some of the oldest characters in the world and were used in China about 3000 years ago. As an ancient form of literature, OBIs store a lot of information that can help us understand the world history, character evaluations, and more. However, as OBIs were found only discovered about 120 years ago, few studies have described them, and the aging process has made the inscriptions less legible. Hence, automatic character detection and recognition has become an important issue. This paper aims to design a online OBI recognition system for helping preservation and organization the cultural heritage. We evaluated two deep learning models for OBI recognition, and have designed an API that can be accessed online for OBI recognition. In the first stage, you only look once (YOLO) is applied for detecting and recognizing OBIs. However, not all of the OBIs can be detected correctly by YOLO, so we next utilize MobileNet to recognize the undetected OBIs by manually cropping the undetected OBI in the image. MobileNet is used for this second stage of recognition as our evaluation of ten state-of-the-art models showed that it is the best network for OBI recognition due to its superior performance in terms of accuracy, loss and time consumption. We installed our system on an application programming interface (API) and opened it for OBI detection and recognition.

</p>
</details>

<details><summary><b>ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders</b>
<a href="https://arxiv.org/abs/2105.01279">arxiv:2105.01279</a>
&#x1F4C8; 2 <br>
<p>Yan Song, Tong Zhang, Yonggang Wang, Kai-Fu Lee</p></summary>
<p>

**Abstract:** Pre-trained text encoders have drawn sustaining attention in natural language processing (NLP) and shown their capability in obtaining promising results in different tasks. Recent studies illustrated that external self-supervised signals (or knowledge extracted by unsupervised learning, such as n-grams) are beneficial to provide useful semantic evidence for understanding languages such as Chinese, so as to improve the performance on various downstream tasks accordingly. To further enhance the encoders, in this paper, we propose to pre-train n-gram-enhanced encoders with a large volume of data and advanced techniques for training. Moreover, we try to extend the encoder to different languages as well as different domains, where it is confirmed that the same architecture is applicable to these varying circumstances and new state-of-the-art performance is observed from a long list of NLP tasks across languages and domains.

</p>
</details>

<details><summary><b>Graph Pooling via Coarsened Graph Infomax</b>
<a href="https://arxiv.org/abs/2105.01275">arxiv:2105.01275</a>
&#x1F4C8; 2 <br>
<p>Yunsheng Pang, Yunxiang Zhao, Dongsheng Li</p></summary>
<p>

**Abstract:** Graph pooling that summaries the information in a large graph into a compact form is essential in hierarchical graph representation learning. Existing graph pooling methods either suffer from high computational complexity or cannot capture the global dependencies between graphs before and after pooling. To address the problems of existing graph pooling methods, we propose Coarsened Graph Infomax Pooling (CGIPool) that maximizes the mutual information between the input and the coarsened graph of each pooling layer to preserve graph-level dependencies. To achieve mutual information neural maximization, we apply contrastive learning and propose a self-attention-based algorithm for learning positive and negative samples. Extensive experimental results on seven datasets illustrate the superiority of CGIPool comparing to the state-of-the-art methods.

</p>
</details>

<details><summary><b>Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification</b>
<a href="https://arxiv.org/abs/2105.01198">arxiv:2105.01198</a>
&#x1F4C8; 2 <br>
<p>Maysam Behmanesh, Peyman Adibi, Hossein Karshenas</p></summary>
<p>

**Abstract:** Support vector machines (SVMs) are powerful supervised learning tools developed to solve classification problems. However, SVMs are likely to perform poorly in the classification of imbalanced data. The rough set theory presents a mathematical tool for inference in nondeterministic cases that provides methods for removing irrelevant information from data. In this work, we propose an approach that efficiently used fuzzy rough set theory in weighted least squares twin support vector machine called FRLSTSVM for classification of imbalanced data. The first innovation is introducing a new fuzzy rough set-based under-sampling strategy to make the classifier robust in terms of the imbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM, data points from the minority class remain unchanged while a subset of data points in the majority class are selected using a new method. In this model, we embed the weight biases in the LSTSVM formulations to overcome the bias phenomenon in the original twin SVM for the classification of imbalanced data. In order to determine these weights in this formulation, we introduce a new strategy that uses fuzzy rough set theory as the second innovation. Experimental results on the famous imbalanced datasets, compared to the related traditional SVM-based methods, demonstrate the superiority of the proposed FRLSTSVM model in the imbalanced data classification.

</p>
</details>

<details><summary><b>Automated Estimation of Total Lung Volume using Chest Radiographs and Deep Learning</b>
<a href="https://arxiv.org/abs/2105.01181">arxiv:2105.01181</a>
&#x1F4C8; 2 <br>
<p>Ecem Sogancioglu, Keelin Murphy, Ernst Th. Scholten, Luuk H. Boulogne, Mathias Prokop, Bram van Ginneken</p></summary>
<p>

**Abstract:** Total lung volume is an important quantitative biomarker and is used for the assessment of restrictive lung diseases. In this study, we investigate the performance of several deep-learning approaches for automated measurement of total lung volume from chest radiographs. 7621 posteroanterior and lateral view chest radiographs (CXR) were collected from patients with chest CT available. Similarly, 928 CXR studies were chosen from patients with pulmonary function test (PFT) results. The reference total lung volume was calculated from lung segmentation on CT or PFT data, respectively. This dataset was used to train deep-learning architectures to predict total lung volume from chest radiographs. The experiments were constructed in a step-wise fashion with increasing complexity to demonstrate the effect of training with CT-derived labels only and the sources of error. The optimal models were tested on 291 CXR studies with reference lung volume obtained from PFT. The optimal deep-learning regression model showed an MAE of 408 ml and a MAPE of 8.1\% and Pearson's r = 0.92 using both frontal and lateral chest radiographs as input. CT-derived labels were useful for pre-training but the optimal performance was obtained by fine-tuning the network with PFT-derived labels. We demonstrate, for the first time, that state-of-the-art deep learning solutions can accurately measure total lung volume from plain chest radiographs. The proposed model can be used to obtain total lung volume from routinely acquired chest radiographs at no additional cost and could be a useful tool to identify trends over time in patients referred regularly for chest x-rays.

</p>
</details>

<details><summary><b>Learning Good State and Action Representations via Tensor Decomposition</b>
<a href="https://arxiv.org/abs/2105.01136">arxiv:2105.01136</a>
&#x1F4C8; 2 <br>
<p>Chengzhuo Ni, Anru Zhang, Yaqi Duan, Mengdi Wang</p></summary>
<p>

**Abstract:** The transition kernel of a continuous-state-action Markov decision process (MDP) admits a natural tensor structure. This paper proposes a tensor-inspired unsupervised learning method to identify meaningful low-dimensional state and action representations from empirical trajectories. The method exploits the MDP's tensor structure by kernelization, importance sampling and low-Tucker-rank approximation. This method can be further used to cluster states and actions respectively and find the best discrete MDP abstraction. We provide sharp statistical error bounds for tensor concentration and the preservation of diffusion distance after embedding.

</p>
</details>

<details><summary><b>MRC-LSTM: A Hybrid Approach of Multi-scale Residual CNN and LSTM to Predict Bitcoin Price</b>
<a href="https://arxiv.org/abs/2105.00707">arxiv:2105.00707</a>
&#x1F4C8; 2 <br>
<p>Qiutong Guo, Shun Lei, Qing Ye, Zhiyang Fang</p></summary>
<p>

**Abstract:** Bitcoin, one of the major cryptocurrencies, presents great opportunities and challenges with its tremendous potential returns accompanying high risks. The high volatility of Bitcoin and the complex factors affecting them make the study of effective price forecasting methods of great practical importance to financial investors and researchers worldwide. In this paper, we propose a novel approach called MRC-LSTM, which combines a Multi-scale Residual Convolutional neural network (MRC) and a Long Short-Term Memory (LSTM) to implement Bitcoin closing price prediction. Specifically, the Multi-scale residual module is based on one-dimensional convolution, which is not only capable of adaptive detecting features of different time scales in multivariate time series, but also enables the fusion of these features. LSTM has the ability to learn long-term dependencies in series, which is widely used in financial time series forecasting. By mixing these two methods, the model is able to obtain highly expressive features and efficiently learn trends and interactions of multivariate time series. In the study, the impact of external factors such as macroeconomic variables and investor attention on the Bitcoin price is considered in addition to the trading information of the Bitcoin market. We performed experiments to predict the daily closing price of Bitcoin (USD), and the experimental results show that MRC-LSTM significantly outperforms a variety of other network structures. Furthermore, we conduct additional experiments on two other cryptocurrencies, Ethereum and Litecoin, to further confirm the effectiveness of the MRC-LSTM in short-term forecasting for multivariate time series of cryptocurrencies.

</p>
</details>

<details><summary><b>Wearable and Continuous Prediction of Passage of Time Perception for Monitoring Mental Health</b>
<a href="https://arxiv.org/abs/2105.02808">arxiv:2105.02808</a>
&#x1F4C8; 1 <br>
<p>Lara Orlandic, Adriana Arza Valdes, David Atienza</p></summary>
<p>

**Abstract:** A person's passage of time perception (POTP) is strongly linked to their mental state and stress response, and can therefore provide an easily quantifiable means of continuous mental health monitoring. In this work, we develop a custom experiment and Machine Learning (ML) models for predicting POTP from biomarkers acquired from wearable biosensors. We first confirm that individuals experience time passing slower than usual during fear or sadness (p = 0.046) and faster than usual during cognitive tasks (p = 2 x 10^-5). Then, we group together the experimental segments associated with fast, slow, and normal POTP, and train a ML model to classify between these states based on a person's biomarkers. The classifier had a weighted average F-1 score of 79%, with the fast-passing time class having the highest F-1 score of 93%. Next, we classify each individual's POTP regardless of the task at hand, achieving an F-1 score of 77.1% when distinguishing time passing faster rather than slower than usual. In the two classifiers, biomarkers derived from the respiration, electrocardiogram, skin conductance, and skin temperature signals contributed most to the classifier output, thus enabling real-time POTP monitoring using noninvasive, wearable biosensors.

</p>
</details>

<details><summary><b>Learning to Continuously Optimize Wireless Resource in a Dynamic Environment: A Bilevel Optimization Perspective</b>
<a href="https://arxiv.org/abs/2105.01696">arxiv:2105.01696</a>
&#x1F4C8; 1 <br>
<p>Haoran Sun, Wenqiang Pu, Xiao Fu, Tsung-Hui Chang, Mingyi Hong</p></summary>
<p>

**Abstract:** There has been a growing interest in developing data-driven, and in particular deep neural network (DNN) based methods for modern communication tasks. For a few popular tasks such as power control, beamforming, and MIMO detection, these methods achieve state-of-the-art performance while requiring less computational efforts, less resources for acquiring channel state information (CSI), etc. However, it is often challenging for these approaches to learn in a dynamic environment.
  This work develops a new approach that enables data-driven methods to continuously learn and optimize resource allocation strategies in a dynamic environment. Specifically, we consider an ``episodically dynamic" setting where the environment statistics change in ``episodes", and in each episode the environment is stationary. We propose to build the notion of continual learning (CL) into wireless system design, so that the learning model can incrementally adapt to the new episodes, {\it without forgetting} knowledge learned from the previous episodes. Our design is based on a novel bilevel optimization formulation which ensures certain ``fairness" across different data samples. We demonstrate the effectiveness of the CL approach by integrating it with two popular DNN based models for power control and beamforming, respectively, and testing using both synthetic and ray-tracing based data sets. These numerical results show that the proposed CL approach is not only able to adapt to the new scenarios quickly and seamlessly, but importantly, it also maintains high performance over the previously encountered scenarios as well.

</p>
</details>

<details><summary><b>COVID-Net CT-S: 3D Convolutional Neural Network Architectures for COVID-19 Severity Assessment using Chest CT Images</b>
<a href="https://arxiv.org/abs/2105.01284">arxiv:2105.01284</a>
&#x1F4C8; 1 <br>
<p>Hossein Aboutalebi, Saad Abbasi, Mohammad Javad Shafiee, Alexander Wong</p></summary>
<p>

**Abstract:** The health and socioeconomic difficulties caused by the COVID-19 pandemic continues to cause enormous tensions around the world. In particular, this extraordinary surge in the number of cases has put considerable strain on health care systems around the world. A critical step in the treatment and management of COVID-19 positive patients is severity assessment, which is challenging even for expert radiologists given the subtleties at different stages of lung disease severity. Motivated by this challenge, we introduce COVID-Net CT-S, a suite of deep convolutional neural networks for predicting lung disease severity due to COVID-19 infection. More specifically, a 3D residual architecture design is leveraged to learn volumetric visual indicators characterizing the degree of COVID-19 lung disease severity. Experimental results using the patient cohort collected by the China National Center for Bioinformation (CNCB) showed that the proposed COVID-Net CT-S networks, by leveraging volumetric features, can achieve significantly improved severity assessment performance when compared to traditional severity assessment networks that learn and leverage 2D visual features to characterize COVID-19 severity.

</p>
</details>

<details><summary><b>Citadel: Protecting Data Privacy and Model Confidentiality for Collaborative Learning with SGX</b>
<a href="https://arxiv.org/abs/2105.01281">arxiv:2105.01281</a>
&#x1F4C8; 1 <br>
<p>Chengliang Zhang, Junzhe Xia, Baichen Yang, Huancheng Puyang, Wei Wang, Ruichuan Chen, Istemi Ekin Akkus, Paarijaat Aditya, Feng Yan</p></summary>
<p>

**Abstract:** With the advancement of machine learning (ML) and its growing awareness, many organizations who own data but not ML expertise (data owner) would like to pool their data and collaborate with those who have expertise but need data from diverse sources to train truly generalizable models (model owner). In such collaborative ML, the data owner wants to protect the privacy of its training data, while the model owner desires the confidentiality of the model and the training method which may contain intellectual properties. However, existing private ML solutions, such as federated learning and split learning, cannot meet the privacy requirements of both data and model owners at the same time.
  This paper presents Citadel, a scalable collaborative ML system that protects the privacy of both data owner and model owner in untrusted infrastructures with the help of Intel SGX. Citadel performs distributed training across multiple training enclaves running on behalf of data owners and an aggregator enclave on behalf of the model owner. Citadel further establishes a strong information barrier between these enclaves by means of zero-sum masking and hierarchical aggregation to prevent data/model leakage during collaborative training. Compared with the existing SGX-protected training systems, Citadel enables better scalability and stronger privacy guarantees for collaborative ML. Cloud deployment with various ML models shows that Citadel scales to a large number of enclaves with less than 1.73X slowdown caused by SGX.

</p>
</details>

<details><summary><b>End-to-End Learning for Uplink MU-SIMO Joint Transmitter and Non-Coherent Receiver Design in Fading Channels</b>
<a href="https://arxiv.org/abs/2105.01260">arxiv:2105.01260</a>
&#x1F4C8; 1 <br>
<p>Songyan Xue, Yi Ma, Na Yi</p></summary>
<p>

**Abstract:** In this paper, a novel end-to-end learning approach, namely JTRD-Net, is proposed for uplink multiuser single-input multiple-output (MU-SIMO) joint transmitter and non-coherent receiver design (JTRD) in fading channels. The basic idea lies in the use of artificial neural networks (ANNs) to replace traditional communication modules at both transmitter and receiver sides. More specifically, the transmitter side is modeled as a group of parallel linear layers, which are responsible for multiuser waveform design; and the non-coherent receiver is formed by a deep feed-forward neural network (DFNN) so as to provide multiuser detection (MUD) capabilities. The entire JTRD-Net can be trained from end to end to adapt to channel statistics through deep learning. After training, JTRD-Net can work efficiently in a non-coherent manner without requiring any levels of channel state information (CSI). In addition to the network architecture, a novel weight-initialization method, namely symmetrical-interval initialization, is proposed for JTRD-Net. It is shown that the symmetrical-interval initialization outperforms the conventional method (e.g. Xavier initialization) in terms of well-balanced convergence-rate among users. Simulation results show that the proposed JTRD-Net approach takes significant advantages in terms of reliability and scalability over baseline schemes on both i.i.d. complex Gaussian channels and spatially-correlated channels.

</p>
</details>

<details><summary><b>Self-Supervised Approach for Facial Movement Based Optical Flow</b>
<a href="https://arxiv.org/abs/2105.01256">arxiv:2105.01256</a>
&#x1F4C8; 1 <br>
<p>Muhannad Alkaddour, Usman Tariq, Abhinav Dhall</p></summary>
<p>

**Abstract:** Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.

</p>
</details>

<details><summary><b>Regret-Optimal Full-Information Control</b>
<a href="https://arxiv.org/abs/2105.01244">arxiv:2105.01244</a>
&#x1F4C8; 1 <br>
<p>Oron Sabag, Gautam Goel, Sahin Lale, Babak Hassibi</p></summary>
<p>

**Abstract:** We consider the infinite-horizon, discrete-time full-information control problem. Motivated by learning theory, as a criterion for controller design we focus on regret, defined as the difference between the LQR cost of a causal controller (that has only access to past and current disturbances) and the LQR cost of a clairvoyant one (that has also access to future disturbances). In the full-information setting, there is a unique optimal non-causal controller that in terms of LQR cost dominates all other controllers. Since the regret itself is a function of the disturbances, we consider the worst-case regret over all possible bounded energy disturbances, and propose to find a causal controller that minimizes this worst-case regret. The resulting controller has the interpretation of guaranteeing the smallest possible regret compared to the best non-causal controller, no matter what the future disturbances are. We show that the regret-optimal control problem can be reduced to a Nehari problem, i.e., to approximate an anticausal operator with a causal one in the operator norm. In the state-space setting, explicit formulas for the optimal regret and for the regret-optimal controller (in both the causal and the strictly causal settings) are derived. The regret-optimal controller is the sum of the classical $H_2$ state-feedback law and a finite-dimensional controller obtained from the Nehari problem. The controller construction simply requires the solution to the standard LQR Riccati equation, in addition to two Lyapunov equations. Simulations over a range of plants demonstrates that the regret-optimal controller interpolates nicely between the $H_2$ and the $H_\infty$ optimal controllers, and generally has $H_2$ and $H_\infty$ costs that are simultaneously close to their optimal values. The regret-optimal controller thus presents itself as a viable option for control system design.

</p>
</details>

<details><summary><b>Polynomial-Time Algorithms for Multi-Agent Minimal-Capacity Planning</b>
<a href="https://arxiv.org/abs/2105.01225">arxiv:2105.01225</a>
&#x1F4C8; 1 <br>
<p>Murat Cubuktepe, František Blahoudek, Ufuk Topcu</p></summary>
<p>

**Abstract:** We study the problem of minimizing the resource capacity of autonomous agents cooperating to achieve a shared task. More specifically, we consider high-level planning for a team of homogeneous agents that operate under resource constraints in stochastic environments and share a common goal: given a set of target locations, ensure that each location will be visited infinitely often by some agent almost surely. We formalize the dynamics of agents by consumption Markov decision processes. In a consumption Markov decision process, the agent has a resource of limited capacity. Each action of the agent may consume some amount of the resource. To avoid exhaustion, the agent can replenish its resource to full capacity in designated reload states. The resource capacity restricts the capabilities of the agent. The objective is to assign target locations to agents, and each agent is only responsible for visiting the assigned subset of target locations repeatedly. Moreover, the assignment must ensure that the agents can carry out their tasks with minimal resource capacity. We reduce the problem of finding target assignments for a team of agents with the lowest possible capacity to an equivalent graph-theoretical problem. We develop an algorithm that solves this graph problem in time that is \emph{polynomial} in the number of agents, target locations, and size of the consumption Markov decision process. We demonstrate the applicability and scalability of the algorithm in a scenario where hundreds of unmanned underwater vehicles monitor hundreds of locations in environments with stochastic ocean currents.

</p>
</details>

<details><summary><b>Weakly-Supervised Universal Lesion Segmentation with Regional Level Set Loss</b>
<a href="https://arxiv.org/abs/2105.01218">arxiv:2105.01218</a>
&#x1F4C8; 1 <br>
<p>Youbao Tang, Jinzheng Cai, Ke Yan, Lingyun Huang, Guotong Xie, Jing Xiao, Jingjing Lu, Gigin Lin, Le Lu</p></summary>
<p>

**Abstract:** Accurately segmenting a variety of clinically significant lesions from whole body computed tomography (CT) scans is a critical task on precision oncology imaging, denoted as universal lesion segmentation (ULS). Manual annotation is the current clinical practice, being highly time-consuming and inconsistent on tumor's longitudinal assessment. Effectively training an automatic segmentation model is desirable but relies heavily on a large number of pixel-wise labelled data. Existing weakly-supervised segmentation approaches often struggle with regions nearby the lesion boundaries. In this paper, we present a novel weakly-supervised universal lesion segmentation method by building an attention enhanced model based on the High-Resolution Network (HRNet), named AHRNet, and propose a regional level set (RLS) loss for optimizing lesion boundary delineation. AHRNet provides advanced high-resolution deep image features by involving a decoder, dual-attention and scale attention mechanisms, which are crucial to performing accurate lesion segmentation. RLS can optimize the model reliably and effectively in a weakly-supervised fashion, forcing the segmentation close to lesion boundary. Extensive experimental results demonstrate that our method achieves the best performance on the publicly large-scale DeepLesion dataset and a hold-out test set.

</p>
</details>

<details><summary><b>Event Camera Simulator Design for Modeling Attention-based Inference Architectures</b>
<a href="https://arxiv.org/abs/2105.01203">arxiv:2105.01203</a>
&#x1F4C8; 1 <br>
<p>Md Jubaer Hossain Pantho, Joel Mandebi Mbongue, Pankaj Bhowmik, Christophe Bobda</p></summary>
<p>

**Abstract:** In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras are not generally available in the market, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator's relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads.

</p>
</details>

<details><summary><b>Consistent Density Estimation Under Discrete Mixture Models</b>
<a href="https://arxiv.org/abs/2105.01108">arxiv:2105.01108</a>
&#x1F4C8; 1 <br>
<p>Luc Devroye, Alex Dytso</p></summary>
<p>

**Abstract:** This work considers a problem of estimating a mixing probability density $f$ in the setting of discrete mixture models. The paper consists of three parts.
  The first part focuses on the construction of an $L_1$ consistent estimator of $f$. In particular, under the assumptions that the probability measure $μ$ of the observation is atomic, and the map from $f$ to $μ$ is bijective, it is shown that there exists an estimator $f_n$ such that for every density $f$ $\lim_{n\to \infty} \mathbb{E} \left[ \int |f_n -f | \right]=0$.
  The second part discusses the implementation details. Specifically, it is shown that the consistency for every $f$ can be attained with a computationally feasible estimator.
  The third part, as a study case, considers a Poisson mixture model. In particular, it is shown that in the Poisson noise setting, the bijection condition holds and, hence, estimation can be performed consistently for every $f$.

</p>
</details>

<details><summary><b>Alternate Model Growth and Pruning for Efficient Training of Recommendation Systems</b>
<a href="https://arxiv.org/abs/2105.01064">arxiv:2105.01064</a>
&#x1F4C8; 1 <br>
<p>Xiaocong Du, Bhargav Bhushanam, Jiecao Yu, Dhruv Choudhary, Tianxiang Gao, Sherman Wong, Louis Feng, Jongsoo Park, Yu Cao, Arun Kejariwal</p></summary>
<p>

**Abstract:** Deep learning recommendation systems at scale have provided remarkable gains through increasing model capacity (i.e. wider and deeper neural networks), but it comes at significant training cost and infrastructure cost. Model pruning is an effective technique to reduce computation overhead for deep neural networks by removing redundant parameters. However, modern recommendation systems are still thirsty for model capacity due to the demand for handling big data. Thus, pruning a recommendation model at scale results in a smaller model capacity and consequently lower accuracy. To reduce computation cost without sacrificing model capacity, we propose a dynamic training scheme, namely alternate model growth and pruning, to alternatively construct and prune weights in the course of training. Our method leverages structured sparsification to reduce computational cost without hurting the model capacity at the end of offline training so that a full-size model is available in the recurring training stage to learn new data in real-time. To the best of our knowledge, this is the first work to provide in-depth experiments and discussion of applying structural dynamics to recommendation systems at scale to reduce training cost. The proposed method is validated with an open-source deep-learning recommendation model (DLRM) and state-of-the-art industrial-scale production models.

</p>
</details>

<details><summary><b>Recovering Barabási-Albert Parameters of Graphs through Disentanglement</b>
<a href="https://arxiv.org/abs/2105.00997">arxiv:2105.00997</a>
&#x1F4C8; 1 <br>
<p>Cristina Guzman, Daphna Keidar, Tristan Meynier, Andreas Opedal, Niklas Stoehr</p></summary>
<p>

**Abstract:** Classical graph modeling approaches such as Erdős Rényi (ER) random graphs or Barabási-Albert (BA) graphs, here referred to as stylized models, aim to reproduce properties of real-world graphs in an interpretable way. While useful, graph generation with stylized models requires domain knowledge and iterative trial and error simulation. Previous work by Stoehr et al. (2019) addresses these issues by learning the generation process from graph data, using a disentanglement-focused deep autoencoding framework, more specifically, a $β$-Variational Autoencoder ($β$-VAE). While they successfully recover the generative parameters of ER graphs through the model's latent variables, their model performs badly on sequentially generated graphs such as BA graphs, due to their oversimplified decoder. We focus on recovering the generative parameters of BA graphs by replacing their $β$-VAE decoder with a sequential one. We first learn the generative BA parameters in a supervised fashion using a Graph Neural Network (GNN) and a Random Forest Regressor, by minimizing the squared loss between the true generative parameters and the latent variables. Next, we train a $β$-VAE model, combining the GNN encoder from the first stage with an LSTM-based decoder with a customized loss.

</p>
</details>

<details><summary><b>Mean Field Equilibrium in Multi-Armed Bandit Game with Continuous Reward</b>
<a href="https://arxiv.org/abs/2105.00767">arxiv:2105.00767</a>
&#x1F4C8; 1 <br>
<p>Xiong Wang, Riheng Jia</p></summary>
<p>

**Abstract:** Mean field game facilitates analyzing multi-armed bandit (MAB) for a large number of agents by approximating their interactions with an average effect. Existing mean field models for multi-agent MAB mostly assume a binary reward function, which leads to tractable analysis but is usually not applicable in practical scenarios. In this paper, we study the mean field bandit game with a continuous reward function. Specifically, we focus on deriving the existence and uniqueness of mean field equilibrium (MFE), thereby guaranteeing the asymptotic stability of the multi-agent system. To accommodate the continuous reward function, we encode the learned reward into an agent state, which is in turn mapped to its stochastic arm playing policy and updated using realized observations. We show that the state evolution is upper semi-continuous, based on which the existence of MFE is obtained. As the Markov analysis is mainly for the case of discrete state, we transform the stochastic continuous state evolution into a deterministic ordinary differential equation (ODE). On this basis, we can characterize a contraction mapping for the ODE to ensure a unique MFE for the bandit game. Extensive evaluations validate our MFE characterization, and exhibit tight empirical regret of the MAB problem.

</p>
</details>

<details><summary><b>Comparison Analysis of Facebook's Prophet, Amazon's DeepAR+ and CNN-QR Algorithms for Successful Real-World Sales Forecasting</b>
<a href="https://arxiv.org/abs/2105.00694">arxiv:2105.00694</a>
&#x1F4C8; 1 <br>
<p>Emir Zunic, Kemal Korjenic, Sead Delalic, Zlatko Subara</p></summary>
<p>

**Abstract:** By successfully solving the problem of forecasting, the processes in the work of various companies are optimized and savings are achieved. In this process, the analysis of time series data is of particular importance. Since the creation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting models, algorithms have attracted a great deal of attention. The paper presents the application and comparison of the above algorithms for sales forecasting in distribution companies. A detailed comparison of the performance of algorithms over real data with different lengths of sales history was made. The results show that Prophet gives better results for items with a longer history and frequent sales, while Amazon's algorithms show superiority for items without a long history and items that are rarely sold.

</p>
</details>

<details><summary><b>Heart-Darts: Classification of Heartbeats Using Differentiable Architecture Search</b>
<a href="https://arxiv.org/abs/2105.00693">arxiv:2105.00693</a>
&#x1F4C8; 1 <br>
<p>Jindi Lv, Qing Ye, Yanan Sun, Juan Zhao, Jiancheng Lv</p></summary>
<p>

**Abstract:** Arrhythmia is a cardiovascular disease that manifests irregular heartbeats. In arrhythmia detection, the electrocardiogram (ECG) signal is an important diagnostic technique. However, manually evaluating ECG signals is a complicated and time-consuming task. With the application of convolutional neural networks (CNNs), the evaluation process has been accelerated and the performance is improved. It is noteworthy that the performance of CNNs heavily depends on their architecture design, which is a complex process grounded on expert experience and trial-and-error. In this paper, we propose a novel approach, Heart-Darts, to efficiently classify the ECG signals by automatically designing the CNN model with the differentiable architecture search (i.e., Darts, a cell-based neural architecture search method). Specifically, we initially search a cell architecture by Darts and then customize a novel CNN model for ECG classification based on the obtained cells. To investigate the efficiency of the proposed method, we evaluate the constructed model on the MIT-BIH arrhythmia database. Additionally, the extensibility of the proposed CNN model is validated on two other new databases. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art CNN models in ECG classification in terms of both performance and generalization capability.

</p>
</details>

<details><summary><b>Explaining how your AI system is fair</b>
<a href="https://arxiv.org/abs/2105.00667">arxiv:2105.00667</a>
&#x1F4C8; 1 <br>
<p>Boris Ruf, Marcin Detyniecki</p></summary>
<p>

**Abstract:** To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying "fairness" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.

</p>
</details>

<details><summary><b>An Algorithm for Recommending Groceries Based on an Item Ranking Method</b>
<a href="https://arxiv.org/abs/2105.00650">arxiv:2105.00650</a>
&#x1F4C8; 1 <br>
<p>Gourab Nath, Jaydip Sen</p></summary>
<p>

**Abstract:** This research proposes a new recommender system algorithm for online grocery shopping. The algorithm is based on the perspective that, since the grocery items are usually bought in bulk, a grocery recommender system should be capable of recommending the items in bulk. The algorithm figures out the possible dishes a user may cook based on the items added to the basket and recommends the ingredients accordingly. Our algorithm does not depend on the user ratings. Customers usually do not have the patience to rate the groceries they purchase. Therefore, algorithms that are not dependent on user ratings need to be designed. Instead of using a brute force search, this algorithm limits the search space to a set of only a few probably food categories. Each food category consists of several food subcategories. For example, "fried rice" and "biryani" are food subcategories that belong to the food category "rice". For each food category, items are ranked according to how well they can differentiate a food subcategory. To each food subcategory in the activated search space, this algorithm attaches a score. The score is calculated based on the rank of the items added to the basket. Once the score exceeds a threshold value, its corresponding subcategory gets activated. The algorithm then uses a basket-to-recipe similarity measure to identify the best recipe matches within the activated subcategories only. This reduces the search space to a great extent. We may argue that this algorithm is similar to the content-based recommender system in some sense, but it does not suffer from the limitations like limited content, over-specialization, or the new user problem.

</p>
</details>

<details><summary><b>Schema-Aware Deep Graph Convolutional Networks for Heterogeneous Graphs</b>
<a href="https://arxiv.org/abs/2105.00644">arxiv:2105.00644</a>
&#x1F4C8; 1 <br>
<p>Saurav Manchanda, Da Zheng, George Karypis</p></summary>
<p>

**Abstract:** Graph convolutional network (GCN) based approaches have achieved significant progress for solving complex, graph-structured problems. GCNs incorporate the graph structure information and the node (or edge) features through message passing and computes 'deep' node representations. Despite significant progress in the field, designing GCN architectures for heterogeneous graphs still remains an open challenge. Due to the schema of a heterogeneous graph, useful information may reside multiple hops away. A key question is how to perform message passing to incorporate information of neighbors multiple hops away while avoiding the well-known over-smoothing problem in GCNs. To address this question, we propose our GCN framework 'Deep Heterogeneous Graph Convolutional Network (DHGCN)', which takes advantage of the schema of a heterogeneous graph and uses a hierarchical approach to effectively utilize information many hops away. It first computes representations of the target nodes based on their 'schema-derived ego-network' (SEN). It then links the nodes of the same type with various pre-defined metapaths and performs message passing along these links to compute final node representations. Our design choices naturally capture the way a heterogeneous graph is generated from the schema. The experimental results on real and synthetic datasets corroborate the design choice and illustrate the performance gains relative to competing alternatives.

</p>
</details>

<details><summary><b>EBIC.JL -- an Efficient Implementation of Evolutionary Biclustering Algorithm in Julia</b>
<a href="https://arxiv.org/abs/2105.01196">arxiv:2105.01196</a>
&#x1F4C8; 0 <br>
<p>Paweł Renc, Patryk Orzechowski, Aleksander Byrski, Jarosław Wąs, Jason H. Moore</p></summary>
<p>

**Abstract:** Biclustering is a data mining technique which searches for local patterns in numeric tabular data with main application in bioinformatics. This technique has shown promise in multiple areas, including development of biomarkers for cancer, disease subtype identification, or gene-drug interactions among others. In this paper we introduce EBIC.JL - an implementation of one of the most accurate biclustering algorithms in Julia, a modern highly parallelizable programming language for data science. We show that the new version maintains comparable accuracy to its predecessor EBIC while converging faster for the majority of the problems. We hope that this open source software in a high-level programming language will foster research in this promising field of bioinformatics and expedite development of new biclustering methods for big data.

</p>
</details>

<details><summary><b>RL-IoT: Reinforcement Learning to Interact with IoT Devices</b>
<a href="https://arxiv.org/abs/2105.00884">arxiv:2105.00884</a>
&#x1F4C8; 0 <br>
<p>Giulia Milan, Luca Vassio, Idilio Drago, Marco Mellia</p></summary>
<p>

**Abstract:** Our life is getting filled by Internet of Things (IoT) devices. These devices often rely on closed or poorly documented protocols, with unknown formats and semantics. Learning how to interact with such devices in an autonomous manner is the key for interoperability and automatic verification of their capabilities. In this paper, we propose RL-IoT, a system that explores how to automatically interact with possibly unknown IoT devices. We leverage reinforcement learning (RL) to recover the semantics of protocol messages and to take control of the device to reach a given goal, while minimizing the number of interactions. We assume to know only a database of possible IoT protocol messages, whose semantics are however unknown. RL-IoT exchanges messages with the target IoT device, learning those commands that are useful to reach the given goal. Our results show that RL-IoT is able to solve both simple and complex tasks. With properly tuned parameters, RL-IoT learns how to perform actions with the target device, a Yeelight smart bulb in our case study, completing non-trivial patterns with as few as 400 interactions. RL-IoT paves the road for automatic interactions with poorly documented IoT protocols, thus enabling interoperable systems.

</p>
</details>

<details><summary><b>Learning swimming escape patterns for larval fish under energy constraints</b>
<a href="https://arxiv.org/abs/2105.00771">arxiv:2105.00771</a>
&#x1F4C8; 0 <br>
<p>Ioannis Mandralis, Pascal Weber, Guido Novati, Petros Koumoutsakos</p></summary>
<p>

**Abstract:** Swimming organisms can escape their predators by creating and harnessing unsteady flow fields through their body motions. Stochastic optimization and flow simulations have identified escape patterns that are consistent with those observed in natural larval swimmers. However, these patterns have been limited by the specification of a particular cost function and depend on a prescribed functional form of the body motion. Here, we deploy reinforcement learning to discover swimmer escape patterns for larval fish under energy constraints. The identified patterns include the C-start mechanism, in addition to more energetically efficient escapes. We find that maximizing distance with limited energy requires swimming via short bursts of accelerating motion interlinked with phases of gliding. The present, data efficient, reinforcement learning algorithm results in an array of patterns that reveal practical flow optimization principles for efficient swimming and the methodology can be transferred to the control of aquatic robotic devices operating under energy constraints.

</p>
</details>


[Next Page]({{ '/2021/05/02/2021.05.02.html' | relative_url }})
