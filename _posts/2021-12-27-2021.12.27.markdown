Prev: [2021.12.26]({{ '/2021/12/26/2021.12.26.html' | relative_url }})  Next: [2021.12.28]({{ '/2021/12/28/2021.12.28.html' | relative_url }})
{% raw %}
## Summary for 2021-12-27, created on 2022-01-06


<details><summary><b>Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies</b>
<a href="https://arxiv.org/abs/2112.13835">arxiv:2112.13835</a>
&#x1F4C8; 105 <br>
<p>Paul Vicol, Luke Metz, Jascha Sohl-Dickstein</p></summary>
<p>

**Abstract:** Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.

</p>
</details>

<details><summary><b>Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks</b>
<a href="https://arxiv.org/abs/2112.13896">arxiv:2112.13896</a>
&#x1F4C8; 99 <br>
<p>Kevin Lee Hunter, Lawrence Spracklen, Subutai Ahmad</p></summary>
<p>

**Abstract:** In principle, sparse neural networks should be significantly more efficient than traditional dense networks. Neurons in the brain exhibit two types of sparsity; they are sparsely interconnected and sparsely active. These two types of sparsity, called weight sparsity and activation sparsity, when combined, offer the potential to reduce the computational cost of neural networks by two orders of magnitude. Despite this potential, today's neural networks deliver only modest performance benefits using just weight sparsity, because traditional computing hardware cannot efficiently process sparse networks. In this article we introduce Complementary Sparsity, a novel technique that significantly improves the performance of dual sparse networks on existing hardware. We demonstrate that we can achieve high performance running weight-sparse networks, and we can multiply those speedups by incorporating activation sparsity. Using Complementary Sparsity, we show up to 100X improvement in throughput and energy efficiency performing inference on FPGAs. We analyze scalability and resource tradeoffs for a variety of kernels typical of commercial convolutional networks such as ResNet-50 and MobileNetV2. Our results with Complementary Sparsity suggest that weight plus activation sparsity can be a potent combination for efficiently scaling future AI models.

</p>
</details>

<details><summary><b>Transformer Uncertainty Estimation with Hierarchical Stochastic Attention</b>
<a href="https://arxiv.org/abs/2112.13776">arxiv:2112.13776</a>
&#x1F4C8; 69 <br>
<p>Jiahuan Pei, Cheng Wang, György Szarvas</p></summary>
<p>

**Abstract:** Transformers are state-of-the-art in a wide range of NLP tasks and have also been applied to many real-world products. Understanding the reliability and certainty of transformer model predictions is crucial for building trustable machine learning applications, e.g., medical diagnosis. Although many recent transformer extensions have been proposed, the study of the uncertainty estimation of transformer models is under-explored. In this work, we propose a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance. This is achieved by learning a hierarchical stochastic self-attention that attends to values and a set of learnable centroids, respectively. Then new attention heads are formed with a mixture of sampled centroids using the Gumbel-Softmax trick. We theoretically show that the self-attention approximation by sampling from a Gumbel distribution is upper bounded. We empirically evaluate our model on two text classification tasks with both in-domain (ID) and out-of-domain (OOD) datasets. The experimental results demonstrate that our approach: (1) achieves the best predictive performance and uncertainty trade-off among compared methods; (2) exhibits very competitive (in most cases, improved) predictive performance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble methods in uncertainty estimation on OOD datasets.

</p>
</details>

<details><summary><b>Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement</b>
<a href="https://arxiv.org/abs/2112.13510">arxiv:2112.13510</a>
&#x1F4C8; 65 <br>
<p>Fuwei Zhang, Zhao Zhang, Xiang Ao, Dehong Gao, Fuzhen Zhuang, Yi Wei, Qing He</p></summary>
<p>

**Abstract:** Cross-Lingual Information Retrieval (CLIR) aims to rank the documents written in a language different from the user's query. The intrinsic gap between different languages is an essential challenge for CLIR. In this paper, we introduce the multilingual knowledge graph (KG) to the CLIR task due to the sufficient information of entities in multiple languages. It is regarded as a "silver bullet" to simultaneously perform explicit alignment between queries and documents and also broaden the representations of queries. And we propose a model named CLIR with hierarchical knowledge enhancement (HIKE) for our task. The proposed model encodes the textual information in queries, documents and the KG with multilingual BERT, and incorporates the KG information in the query-document matching process with a hierarchical information fusion mechanism. Particularly, HIKE first integrates the entities and their neighborhood in KG into query representations with a knowledge-level fusion, then combines the knowledge from both source and target languages to further mitigate the linguistic gap with a language-level fusion. Finally, experimental results demonstrate that HIKE achieves substantial improvements over state-of-the-art competitors.

</p>
</details>

<details><summary><b>LINDA: Unsupervised Learning to Interpolate in Natural Language Processing</b>
<a href="https://arxiv.org/abs/2112.13969">arxiv:2112.13969</a>
&#x1F4C8; 48 <br>
<p>Yekyung Kim, Seohyeong Jeong, Kyunghyun Cho</p></summary>
<p>

**Abstract:** Despite the success of mixup in data augmentation, its applicability to natural language processing (NLP) tasks has been limited due to the discrete and variable-length nature of natural languages. Recent studies have thus relied on domain-specific heuristics and manually crafted resources, such as dictionaries, in order to apply mixup in NLP. In this paper, we instead propose an unsupervised learning approach to text interpolation for the purpose of data augmentation, to which we refer as "Learning to INterpolate for Data Augmentation" (LINDA), that does not require any heuristics nor manually crafted resources but learns to interpolate between any pair of natural language sentences over a natural language manifold. After empirically demonstrating the LINDA's interpolation capability, we show that LINDA indeed allows us to seamlessly apply mixup in NLP and leads to better generalization in text classification both in-domain and out-of-domain.

</p>
</details>

<details><summary><b>SPViT: Enabling Faster Vision Transformers via Soft Token Pruning</b>
<a href="https://arxiv.org/abs/2112.13890">arxiv:2112.13890</a>
&#x1F4C8; 10 <br>
<p>Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, Yanzhi Wang</p></summary>
<p>

**Abstract:** Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViTs, and the related edge device deployment, we leverage the input token sparsity and propose a computation-aware soft pruning framework, which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens generated by the selector module into a package token that will participate in subsequent calculations rather than being completely discarded. Our framework is bound to the trade-off between accuracy and computation constraints of specific edge devices through our proposed computation-aware training strategy. Experimental results show that our framework significantly reduces the computation cost of ViTs while maintaining comparable performance on image classification. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. For example, our method reduces the latency of DeiT-T to 26 ms (26%$\sim $41% superior to existing works) on the mobile device with 0.25%$\sim $4% higher top-1 accuracy on ImageNet. Our code will be released soon.

</p>
</details>

<details><summary><b>Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?</b>
<a href="https://arxiv.org/abs/2112.13906">arxiv:2112.13906</a>
&#x1F4C8; 9 <br>
<p>Sedigheh Eslami, Gerard de Melo, Christoph Meinel</p></summary>
<p>

**Abstract:** Contrastive Language--Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image--text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. This work evaluates the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments are conducted on two MedVQA benchmark datasets and investigate two MedVQA methods, MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via Conditional Reasoning). For each of these, we assess the merits of visual representation learning using PubMedCLIP, the original CLIP, and state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only on visual data. We open source the code for our MedVQA pipeline and pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison to MAML's visual encoder. PubMedCLIP achieves the best results with gains in the overall accuracy of up to 3%. Individual examples illustrate the strengths of PubMedCLIP in comparison to the previously widely used MAML networks. Visual representation learning with language supervision in PubMedCLIP leads to noticeable improvements for MedVQA. Our experiments reveal distributional differences in the two MedVQA benchmark datasets that have not been imparted in previous work and cause different back-end visual encoders in PubMedCLIP to exhibit different behavior on these datasets. Moreover, we witness fundamental performance differences of VQA in general versus medical domains.

</p>
</details>

<details><summary><b>ViR:the Vision Reservoir</b>
<a href="https://arxiv.org/abs/2112.13545">arxiv:2112.13545</a>
&#x1F4C8; 8 <br>
<p>Xian Wei, Bin Wang, Mingsong Chen, Ji Yuan, Hai Lan, Jiehuang Shi, Xuan Tang, Bo Jin, Guozhang Chen, Dongping Yang</p></summary>
<p>

**Abstract:** The most recent year has witnessed the success of applying the Vision Transformer (ViT) for image classification. However, there are still evidences indicating that ViT often suffers following two aspects, i) the high computation and the memory burden from applying the multiple Transformer layers for pre-training on a large-scale dataset, ii) the over-fitting when training on small datasets from scratch. To address these problems, a novel method, namely, Vision Reservoir computing (ViR), is proposed here for image classification, as a parallel to ViT. By splitting each image into a sequence of tokens with fixed length, the ViR constructs a pure reservoir with a nearly fully connected topology to replace the Transformer module in ViT. Two kinds of deep ViR models are subsequently proposed to enhance the network performance. Comparative experiments between the ViR and the ViT are carried out on several image classification benchmarks. Without any pre-training process, the ViR outperforms the ViT in terms of both model and computational complexity. Specifically, the number of parameters of the ViR is about 15% even 5% of the ViT, and the memory footprint is about 20% to 40% of the ViT. The superiority of the ViR performance is explained by Small-World characteristics, Lyapunov exponents, and memory capacity.

</p>
</details>

<details><summary><b>Improving Nonparametric Classification via Local Radial Regression with an Application to Stock Prediction</b>
<a href="https://arxiv.org/abs/2112.13951">arxiv:2112.13951</a>
&#x1F4C8; 7 <br>
<p>Ruixing Cao, Akifumi Okuno, Kei Nakagawa, Hidetoshi Shimodaira</p></summary>
<p>

**Abstract:** For supervised classification problems, this paper considers estimating the query's label probability through local regression using observed covariates. Well-known nonparametric kernel smoother and $k$-nearest neighbor ($k$-NN) estimator, which take label average over a ball around the query, are consistent but asymptotically biased particularly for a large radius of the ball. To eradicate such bias, local polynomial regression (LPoR) and multiscale $k$-NN (MS-$k$-NN) learn the bias term by local regression around the query and extrapolate it to the query itself. However, their theoretical optimality has been shown for the limit of the infinite number of training samples. For correcting the asymptotic bias with fewer observations, this paper proposes a local radial regression (LRR) and its logistic regression variant called local radial logistic regression (LRLR), by combining the advantages of LPoR and MS-$k$-NN. The idea is simple: we fit the local regression to observed labels by taking the radial distance as the explanatory variable and then extrapolate the estimated label probability to zero distance. Our numerical experiments, including real-world datasets of daily stock indices, demonstrate that LRLR outperforms LPoR and MS-$k$-NN.

</p>
</details>

<details><summary><b>Astronomical Image Colorization and upscaling with Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2112.13865">arxiv:2112.13865</a>
&#x1F4C8; 7 <br>
<p>Shreyas Kalvankar, Hrushikesh Pandit, Pranav Parwate, Atharva Patil, Snehal Kamalapur</p></summary>
<p>

**Abstract:** Automatic colorization of images without human intervention has been a subject of interest in the machine learning community for a brief period of time. Assigning color to an image is a highly ill-posed problem because of its innate nature of possessing very high degrees of freedom; given an image, there is often no single color-combination that is correct. Besides colorization, another problem in reconstruction of images is Single Image Super Resolution, which aims at transforming low resolution images to a higher resolution. This research aims to provide an automated approach for the problem by focusing on a very specific domain of images, namely astronomical images, and process them using Generative Adversarial Networks (GANs). We explore the usage of various models in two different color spaces, RGB and L*a*b. We use transferred learning owing to a small data set, using pre-trained ResNet-18 as a backbone, i.e. encoder for the U-net and fine-tune it further. The model produces visually appealing images which hallucinate high resolution, colorized data in these results which does not exist in the original image. We present our results by evaluating the GANs quantitatively using distance metrics such as L1 distance and L2 distance in each of the color spaces across all channels to provide a comparative analysis. We use Frechet inception distance (FID) to compare the distribution of the generated images with the distribution of the real image to assess the model's performance.

</p>
</details>

<details><summary><b>Using maps to predict economic activity</b>
<a href="https://arxiv.org/abs/2112.13850">arxiv:2112.13850</a>
&#x1F4C8; 7 <br>
<p>Imryoung Jeong, Hyunjoo Yang</p></summary>
<p>

**Abstract:** We introduce a novel machine learning approach to leverage historical and contemporary maps to systematically predict economic statistics. Remote sensing data have been used as reliable proxies for local economic activity. However, they have only become available in recent years, thus limiting their applicability for long-term analysis. Historical maps, on the other hand, date back several decades. Our simple algorithm extracts meaningful features from the maps based on their color compositions. The grid-level population predictions by our approach outperform the conventional CNN-based predictions using raw map images. It also predicts population better than other approaches using night light satellite images or land cover classifications as the input for predictions.

</p>
</details>

<details><summary><b>MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer</b>
<a href="https://arxiv.org/abs/2112.13513">arxiv:2112.13513</a>
&#x1F4C8; 7 <br>
<p>Tianyi Zhang, Yunlu Feng, Yu Zhao, Guangda Fan, Aiming Yang, Shangqin Lyu, Peng Zhang, Fan Song, Chenbin Ma, Yangyang Sun, Youdan Feng, Guanglei Zhang</p></summary>
<p>

**Abstract:** Pancreatic cancer is one of the most malignant cancers in the world, which deteriorates rapidly with very high mortality. The rapid on-site evaluation (ROSE) technique innovates the workflow by immediately analyzing the fast stained cytopathological images with on-site pathologists, which enables faster diagnosis in this time-pressured process. However, the wider expansion of ROSE diagnosis has been hindered by the lack of experienced pathologists. To overcome this problem, we propose a hybrid high-performance deep learning model to enable the automated workflow, thus freeing the occupation of the valuable time of pathologists. By firstly introducing the Transformer block into this field with our particular multi-stage hybrid design, the spatial features generated by the convolutional neural network (CNN) significantly enhance the Transformer global modeling. Turning multi-stage spatial features as global attention guidance, this design combines the robustness from the inductive bias of CNN with the sophisticated global modeling power of Transformer. A dataset of 4240 ROSE images is collected to evaluate the method in this unexplored field. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in classification accuracy, which is distinctively higher than the state-of-the-art models. Facing the need for interpretability, MSHT outperforms its counterparts with more accurate attention regions. The results demonstrate that the MSHT can distinguish cancer samples accurately at an unprecedented image scale, laying the foundation for deploying automatic decision systems and enabling the expansion of ROSE in clinical practice. The code and records are available at: https://github.com/sagizty/Multi-Stage-Hybrid-Transformer.

</p>
</details>

<details><summary><b>MedShift: identifying shift data for medical dataset curation</b>
<a href="https://arxiv.org/abs/2112.13885">arxiv:2112.13885</a>
&#x1F4C8; 6 <br>
<p>Xiaoyuan Guo, Judy Wawira Gichoya, Hari Trivedi, Saptarshi Purkayastha, Imon Banerjee</p></summary>
<p>

**Abstract:** To curate a high-quality dataset, identifying data variance between the internal and external sources is a fundamental and crucial step. However, methods to detect shift or variance in data have not been significantly researched. Challenges to this are the lack of effective approaches to learn dense representation of a dataset and difficulties of sharing private data across medical institutions. To overcome the problems, we propose a unified pipeline called MedShift to detect the top-level shift samples and thus facilitate the medical curation. Given an internal dataset A as the base source, we first train anomaly detectors for each class of dataset A to learn internal distributions in an unsupervised way. Second, without exchanging data across sources, we run the trained anomaly detectors on an external dataset B for each class. The data samples with high anomaly scores are identified as shift data. To quantify the shiftness of the external dataset, we cluster B's data into groups class-wise based on the obtained scores. We then train a multi-class classifier on A and measure the shiftness with the classifier's performance variance on B by gradually dropping the group with the largest anomaly score for each class. Additionally, we adapt a dataset quality metric to help inspect the distribution differences for multiple medical sources. We verify the efficacy of MedShift with musculoskeletal radiographs (MURA) and chest X-rays datasets from more than one external source. Experiments show our proposed shift data detection pipeline can be beneficial for medical centers to curate high-quality datasets more efficiently. An interface introduction video to visualize our results is available at https://youtu.be/V3BF0P1sxQE.

</p>
</details>

<details><summary><b>AU Dataset for Visuo-Haptic Object Recognition for Robots</b>
<a href="https://arxiv.org/abs/2112.13761">arxiv:2112.13761</a>
&#x1F4C8; 6 <br>
<p>Lasse Emil R. Bonner, Daniel Daugaard Buhl, Kristian Kristensen, Nicolás Navarro-Guerrero</p></summary>
<p>

**Abstract:** Multimodal object recognition is still an emerging field. Thus, publicly available datasets are still rare and of small size. This dataset was developed to help fill this void and presents multimodal data for 63 objects with some visual and haptic ambiguity. The dataset contains visual, kinesthetic and tactile (audio/vibrations) data. To completely solve sensory ambiguity, sensory integration/fusion would be required. This report describes the creation and structure of the dataset. The first section explains the underlying approach used to capture the visual and haptic properties of the objects. The second section describes the technical aspects (experimental setup) needed for the collection of the data. The third section introduces the objects, while the final section describes the structure and content of the dataset.

</p>
</details>

<details><summary><b>BALanCe: Deep Bayesian Active Learning via Equivalence Class Annealing</b>
<a href="https://arxiv.org/abs/2112.13737">arxiv:2112.13737</a>
&#x1F4C8; 6 <br>
<p>Renyu Zhang, Aly A. Khan, Robert L. Grossman, Yuxin Chen</p></summary>
<p>

**Abstract:** Active learning has demonstrated data efficiency in many fields. Existing active learning algorithms, especially in the context of deep Bayesian active models, rely heavily on the quality of uncertainty estimations of the model. However, such uncertainty estimates could be heavily biased, especially with limited and imbalanced training data. In this paper, we propose BALanCe, a Bayesian deep active learning framework that mitigates the effect of such biases. Concretely, BALanCe employs a novel acquisition function which leverages the structure captured by equivalence hypothesis classes and facilitates differentiation among different equivalence classes. Intuitively, each equivalence class consists of instantiations of deep models with similar predictions, and BALanCe adaptively adjusts the size of the equivalence classes as learning progresses. Besides the fully sequential setting, we further propose Batch-BALanCe -- a generalization of the sequential algorithm to the batched setting -- to efficiently select batches of training examples that are jointly effective for model improvement. We show that Batch-BALanCe achieves state-of-the-art performance on several benchmark datasets for active learning, and that both algorithms can effectively handle realistic challenges that often involve multi-class and imbalanced data.

</p>
</details>

<details><summary><b>DAM-AL: Dilated Attention Mechanism with Attention Loss for 3D Infant Brain Image Segmentation</b>
<a href="https://arxiv.org/abs/2112.13559">arxiv:2112.13559</a>
&#x1F4C8; 6 <br>
<p>Dinh-Hieu Hoang, Gia-Han Diep, Minh-Triet Tran, Ngan T. H Le</p></summary>
<p>

**Abstract:** While Magnetic Resonance Imaging (MRI) has played an essential role in infant brain analysis, segmenting MRI into a number of tissues such as gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) is crucial and complex due to the extremely low intensity contrast between tissues at around 6-9 months of age as well as amplified noise, myelination, and incomplete volume. In this paper, we tackle those limitations by developing a new deep learning model, named DAM-AL, which contains two main contributions, i.e., dilated attention mechanism and hard-case attention loss. Our DAM-AL network is designed with skip block layers and atrous block convolution. It contains both channel-wise attention at high-level context features and spatial attention at low-level spatial structural features. Our attention loss consists of two terms corresponding to region information and hard samples attention. Our proposed DAM-AL has been evaluated on the infant brain iSeg 2017 dataset and the experiments have been conducted on both validation and testing sets. We have benchmarked DAM-AL on Dice coefficient and ASD metrics and compared it with state-of-the-art methods.

</p>
</details>

<details><summary><b>A Moment in the Sun: Solar Nowcasting from Multispectral Satellite Data using Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2112.13974">arxiv:2112.13974</a>
&#x1F4C8; 5 <br>
<p>Akansha Singh Bansal, Trapit Bansal, David Irwin</p></summary>
<p>

**Abstract:** Solar energy is now the cheapest form of electricity in history. Unfortunately, significantly increasing the grid's fraction of solar energy remains challenging due to its variability, which makes balancing electricity's supply and demand more difficult. While thermal generators' ramp rate -- the maximum rate that they can change their output -- is finite, solar's ramp rate is essentially infinite. Thus, accurate near-term solar forecasting, or nowcasting, is important to provide advance warning to adjust thermal generator output in response to solar variations to ensure a balanced supply and demand. To address the problem, this paper develops a general model for solar nowcasting from abundant and readily available multispectral satellite data using self-supervised learning. Specifically, we develop deep auto-regressive models using convolutional neural networks (CNN) and long short-term memory networks (LSTM) that are globally trained across multiple locations to predict raw future observations of the spatio-temporal data collected by the recently launched GOES-R series of satellites. Our model estimates a location's future solar irradiance based on satellite observations, which we feed to a regression model trained on smaller site-specific solar data to provide near-term solar photovoltaic (PV) forecasts that account for site-specific characteristics. We evaluate our approach for different coverage areas and forecast horizons across 25 solar sites and show that our approach yields errors close to that of a model using ground-truth observations.

</p>
</details>

<details><summary><b>Online Adversarial Distillation for Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2112.13966">arxiv:2112.13966</a>
&#x1F4C8; 5 <br>
<p>Can Wang, Zhe Wang, Defang Chen, Sheng Zhou, Yan Feng, Chun Chen</p></summary>
<p>

**Abstract:** Knowledge distillation has recently become a popular technique to improve the model generalization ability on convolutional neural networks. However, its effect on graph neural networks is less than satisfactory since the graph topology and node attributes are likely to change in a dynamic way and in this case a static teacher model is insufficient in guiding student training. In this paper, we tackle this challenge by simultaneously training a group of graph neural networks in an online distillation fashion, where the group knowledge plays a role as a dynamic virtual teacher and the structure changes in graph neural networks are effectively captured. To improve the distillation performance, two types of knowledge are transferred among the students to enhance each other: local knowledge reflecting information in the graph topology and node attributes, and global knowledge reflecting the prediction over classes. We transfer the global knowledge with KL-divergence as the vanilla knowledge distillation does, while exploiting the complicated structure of the local knowledge with an efficient adversarial cyclic learning framework. Extensive experiments verified the effectiveness of our proposed online adversarial distillation approach.

</p>
</details>

<details><summary><b>SPIDER: Searching Personalized Neural Architecture for Federated Learning</b>
<a href="https://arxiv.org/abs/2112.13939">arxiv:2112.13939</a>
&#x1F4C8; 5 <br>
<p>Erum Mushtaq, Chaoyang He, Jie Ding, Salman Avestimehr</p></summary>
<p>

**Abstract:** Federated learning (FL) is an efficient learning framework that assists distributed machine learning when data cannot be shared with a centralized server due to privacy and regulatory restrictions. Recent advancements in FL use predefined architecture-based learning for all the clients. However, given that clients' data are invisible to the server and data distributions are non-identical across clients, a predefined architecture discovered in a centralized setting may not be an optimal solution for all the clients in FL. Motivated by this challenge, in this work, we introduce SPIDER, an algorithmic framework that aims to Search Personalized neural architecture for federated learning. SPIDER is designed based on two unique features: (1) alternately optimizing one architecture-homogeneous global model (Supernet) in a generic FL manner and one architecture-heterogeneous local model that is connected to the global model by weight sharing-based regularization (2) achieving architecture-heterogeneous local model by a novel neural architecture search (NAS) method that can select optimal subnet progressively using operation-level perturbation on the accuracy value as the criterion. Experimental results demonstrate that SPIDER outperforms other state-of-the-art personalization methods, and the searched personalized architectures are more inference efficient.

</p>
</details>

<details><summary><b>GPU-accelerated Faster Mean Shift with euclidean distance metrics</b>
<a href="https://arxiv.org/abs/2112.13891">arxiv:2112.13891</a>
&#x1F4C8; 5 <br>
<p>Le You, Han Jiang, Jinyong Hu, Chorng Chang, Lingxi Chen, Xintong Cui, Mengyang Zhao</p></summary>
<p>

**Abstract:** Handling clustering problems are important in data statistics, pattern recognition and image processing. The mean-shift algorithm, a common unsupervised algorithms, is widely used to solve clustering problems. However, the mean-shift algorithm is restricted by its huge computational resource cost. In previous research[10], we proposed a novel GPU-accelerated Faster Mean-shift algorithm, which greatly speed up the cosine-embedding clustering problem. In this study, we extend and improve the previous algorithm to handle Euclidean distance metrics. Different from conventional GPU-based mean-shift algorithms, our algorithm adopts novel Seed Selection & Early Stopping approaches, which greatly increase computing speed and reduce GPU memory consumption. In the simulation testing, when processing a 200K points clustering problem, our algorithm achieved around 3 times speedup compared to the state-of-the-art GPU-based mean-shift algorithms with optimized GPU memory consumption. Moreover, in this study, we implemented a plug-and-play model for faster mean-shift algorithm, which can be easily deployed. (Plug-and-play model is available: https://github.com/masqm/Faster-Mean-Shift-Euc)

</p>
</details>

<details><summary><b>Adversarial Attack for Asynchronous Event-based Data</b>
<a href="https://arxiv.org/abs/2112.13534">arxiv:2112.13534</a>
&#x1F4C8; 5 <br>
<p>Wooju Lee, Hyun Myung</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95\% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.

</p>
</details>

<details><summary><b>Relative velocity-based reward functions for crowd navigation of robots</b>
<a href="https://arxiv.org/abs/2112.13984">arxiv:2112.13984</a>
&#x1F4C8; 4 <br>
<p>Xiaoqing Yang, Fei Li</p></summary>
<p>

**Abstract:** How to navigate effectively in crowd environments with socially acceptable standards remains the key problem to be solved for the development of mobile robots. Recent work has shown the effectiveness of deep reinforcement learning in addressing crowd navigation, but the learning becomes progressively less effective as the speed of pedestrians increases. To improve the effectiveness of deep reinforcement learning, we redesigned the reward function by introducing the penalty term of relative speed in the reward function. The newly designed reward function is tested on three mainstream deep reinforcement learning algorithms: deep reinforcement learning collision avoidance (CADRL), deep learning based long and short-term memory (LSTM RL), and reinforcement learning based on socialist riselection (SARL). The results of the experiments show that our model navigates in a safer way, outperforming the current model in key metrics such as success rate, collision rate, and hazard frequency.

</p>
</details>

<details><summary><b>Improving Depth Estimation using Location Information</b>
<a href="https://arxiv.org/abs/2112.13925">arxiv:2112.13925</a>
&#x1F4C8; 4 <br>
<p>Ahmed Zaitoon, Hossam El Din Abd El Munim, Hazem Abbas</p></summary>
<p>

**Abstract:** The ability to accurately estimate depth information is crucial for many autonomous applications to recognize the surrounded environment and predict the depth of important objects. One of the most recently used techniques is monocular depth estimation where the depth map is inferred from a single image. This paper improves the self-supervised deep learning techniques to perform accurate generalized monocular depth estimation. The main idea is to train the deep model to take into account a sequence of the different frames, each frame is geotagged with its location information. This makes the model able to enhance depth estimation given area semantics. We demonstrate the effectiveness of our model to improve depth estimation results. The model is trained in a realistic environment and the results show improvements in the depth map after adding the location data to the model training phase.

</p>
</details>

<details><summary><b>Non-Reference Quality Monitoring of Digital Images using Gradient Statistics and Feedforward Neural Networks</b>
<a href="https://arxiv.org/abs/2112.13893">arxiv:2112.13893</a>
&#x1F4C8; 4 <br>
<p>Nisar Ahmed, Hafiz Muhammad Shahzad Asif, Hassan Khalid</p></summary>
<p>

**Abstract:** Digital images contain a lot of redundancies, therefore, compressions are applied to reduce the image size without the loss of reasonable image quality. The same become more prominent in the case of videos that contains image sequences and higher compression ratios are achieved in low throughput networks. Assessment of the quality of images in such scenarios becomes of particular interest. Subjective evaluation in most of the scenarios becomes infeasible so objective evaluation is preferred. Among the three objective quality measures, full-reference and reduced-reference methods require an original image in some form to calculate the quality score which is not feasible in scenarios such as broadcasting or IP video. Therefore, a non-reference quality metric is proposed to assess the quality of digital images which calculates luminance and multiscale gradient statistics along with mean subtracted contrast normalized products as features to train a Feedforward Neural Network with Scaled Conjugate Gradient. The trained network has provided good regression and R2 measures and further testing on LIVE Image Quality Assessment database release-2 has shown promising results. Pearson, Kendall, and Spearman's correlation are calculated between predicted and actual quality scores and their results are comparable to the state-of-the-art systems. Moreover, the proposed metric is computationally faster than its counterparts and can be used for the quality assessment of image sequences.

</p>
</details>

<details><summary><b>HeteroQA: Learning towards Question-and-Answering through Multiple Information Sources via Heterogeneous Graph Modeling</b>
<a href="https://arxiv.org/abs/2112.13597">arxiv:2112.13597</a>
&#x1F4C8; 4 <br>
<p>Shen Gao, Yuchi Zhang, Yongliang Wang, Yang Dong, Xiuying Chen, Dongyan Zhao, Rui Yan</p></summary>
<p>

**Abstract:** Community Question Answering (CQA) is a well-defined task that can be used in many scenarios, such as E-Commerce and online user community for special interests.
  In these communities, users can post articles, give comment, raise a question and answer it.
  These data form the heterogeneous information sources where each information source have their own special structure and context (comments attached to an article or related question with answers).
  Most of the CQA methods only incorporate articles or Wikipedia to extract knowledge and answer the user's question.
  However, various types of information sources in the community are not fully explored by these CQA methods and these multiple information sources (MIS) can provide more related knowledge to user's questions.
  Thus, we propose a question-aware heterogeneous graph transformer to incorporate the MIS in the user community to automatically generate the answer.
  To evaluate our proposed method, we conduct the experiments on two datasets: $\text{MSM}^{\text{plus}}$ the modified version of benchmark dataset MS-MARCO and the AntQA dataset which is the first large-scale CQA dataset with four types of MIS.
  Extensive experiments on two datasets show that our model outperforms all the baselines in terms of all the metrics.

</p>
</details>

<details><summary><b>Depth estimation of endoscopy using sim-to-real transfer</b>
<a href="https://arxiv.org/abs/2112.13595">arxiv:2112.13595</a>
&#x1F4C8; 4 <br>
<p>Bong Hyuk Jeong, Hang Keun Kim, Young Don Son</p></summary>
<p>

**Abstract:** In order to use the navigation system effectively, distance information sensors such as depth sensors are essential. Since depth sensors are difficult to use in endoscopy, many groups propose a method using convolutional neural networks. In this paper, the ground truth of the depth image and the endoscopy image is generated through endoscopy simulation using the colon model segmented by CT colonography. Photo-realistic simulation images can be created using a sim-to-real approach using cycleGAN for endoscopy images. By training the generated dataset, we propose a quantitative endoscopy depth estimation network. The proposed method represents a better-evaluated score than the existing unsupervised training-based results.

</p>
</details>

<details><summary><b>Classification of Histopathology Images of Lung Cancer Using Convolutional Neural Network (CNN)</b>
<a href="https://arxiv.org/abs/2112.13553">arxiv:2112.13553</a>
&#x1F4C8; 4 <br>
<p>Neha Baranwal, Preethi Doravari, Renu Kachhoria</p></summary>
<p>

**Abstract:** Cancer is the uncontrollable cell division of abnormal cells inside the human body, which can spread to other body organs. It is one of the non-communicable diseases (NCDs) and NCDs accounts for 71% of total deaths worldwide whereas lung cancer is the second most diagnosed cancer after female breast cancer. Cancer survival rate of lung cancer is only 19%. There are various methods for the diagnosis of lung cancer, such as X-ray, CT scan, PET-CT scan, bronchoscopy and biopsy. However, to know the subtype of lung cancer based on the tissue type H and E staining is widely used, where the staining is done on the tissue aspirated from a biopsy. Studies have reported that the type of histology is associated with prognosis and treatment in lung cancer. Therefore, early and accurate detection of lung cancer histology is an urgent need and as its treatment is dependent on the type of histology, molecular profile and stage of the disease, it is most essential to analyse the histopathology images of lung cancer. Hence, to speed up the vital process of diagnosis of lung cancer and reduce the burden on pathologists, Deep learning techniques are used. These techniques have shown improved efficacy in the analysis of histopathology slides of cancer. Several studies reported the importance of convolution neural networks (CNN) in the classification of histopathological pictures of various cancer types such as brain, skin, breast, lung, colorectal cancer. In this study tri-category classification of lung cancer images (normal, adenocarcinoma and squamous cell carcinoma) are carried out by using ResNet 50, VGG-19, Inception_ResNet_V2 and DenseNet for the feature extraction and triplet loss to guide the CNN such that it increases inter-cluster distance and reduces intra-cluster distance.

</p>
</details>

<details><summary><b>MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules</b>
<a href="https://arxiv.org/abs/2201.00404">arxiv:2201.00404</a>
&#x1F4C8; 3 <br>
<p>Ranjeet Ranjan Jha, Abhishek Bhardwaj, Devin Garg, Arnav Bhavsar, Aditya Nigam</p></summary>
<p>

**Abstract:** Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder (ASD) by using network-based functional connectivity. It has been shown that ASD is associated with brain regions and their inter-connections. However, discriminating based on connectivity patterns among imaging data of the control population and that of ASD patients' brains is a non-trivial task. In order to tackle said classification task, we propose a novel deep learning architecture (MHATC) consisting of multi-head attention and temporal consolidation modules for classifying an individual as a patient of ASD. The devised architecture results from an in-depth analysis of the limitations of current deep neural network solutions for similar applications. Our approach is not only robust but computationally efficient, which can allow its adoption in a variety of other research and clinical settings.

</p>
</details>

<details><summary><b>Last-Iterate Convergence of Saddle Point Optimizers via High-Resolution Differential Equations</b>
<a href="https://arxiv.org/abs/2112.13826">arxiv:2112.13826</a>
&#x1F4C8; 3 <br>
<p>Tatjana Chavdarova, Michael I. Jordan, Manolis Zampetakis</p></summary>
<p>

**Abstract:** Several widely-used first-order saddle point optimization methods yield an identical continuous-time ordinary differential equation (ODE) to that of the Gradient Descent Ascent (GDA) method when derived naively. However, their convergence properties are very different even on simple bilinear games.
  We use a technique from fluid dynamics called High-Resolution Differential Equations (HRDEs) to design ODEs of several saddle point optimization methods. On bilinear games, the convergence properties of the derived HRDEs correspond to that of the starting discrete methods. Using these techniques, we show that the HRDE of Optimistic Gradient Descent Ascent (OGDA) has last-iterate convergence for general monotone variational inequalities. To our knowledge, this is the first continuous-time dynamics shown to converge for such a general setting. Moreover, we provide the rates for the best-iterate convergence of the OGDA method, relying solely on the first-order smoothness of the monotone operator.

</p>
</details>

<details><summary><b>Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech</b>
<a href="https://arxiv.org/abs/2112.13758">arxiv:2112.13758</a>
&#x1F4C8; 3 <br>
<p>Gaoussou Youssouf Kebe, Luke E. Richards, Edward Raff, Francis Ferraro, Cynthia Matuszek</p></summary>
<p>

**Abstract:** Learning to understand grounded language, which connects natural language to percepts, is a critical research area. Prior work in grounded language acquisition has focused primarily on textual inputs. In this work we demonstrate the feasibility of performing grounded language acquisition on paired visual percepts and raw speech inputs. This will allow interactions in which language about novel tasks and environments is learned from end users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems. We leverage recent work in self-supervised speech representation models and show that learned representations of speech can make language grounding systems more inclusive towards specific groups while maintaining or even increasing general performance.

</p>
</details>

<details><summary><b>Computationally Efficient Approximations for Matrix-based Renyi's Entropy</b>
<a href="https://arxiv.org/abs/2112.13720">arxiv:2112.13720</a>
&#x1F4C8; 3 <br>
<p>Tieliang Gong, Yuxin Dong, Shujian Yu, Hong Chen, Bo Dong, Chen Li, Qinghua Zheng</p></summary>
<p>

**Abstract:** The recently developed matrix based Renyi's entropy enables measurement of information in data simply using the eigenspectrum of symmetric positive semi definite (PSD) matrices in reproducing kernel Hilbert space, without estimation of the underlying data distribution. This intriguing property makes the new information measurement widely adopted in multiple statistical inference and learning tasks. However, the computation of such quantity involves the trace operator on a PSD matrix $G$ to power $α$(i.e., $tr(G^α)$), with a normal complexity of nearly $O(n^3)$, which severely hampers its practical usage when the number of samples (i.e., $n$) is large. In this work, we present computationally efficient approximations to this new entropy functional that can reduce its complexity to even significantly less than $O(n^2)$. To this end, we first develop randomized approximations to $\tr(\G^α)$ that transform the trace estimation into matrix-vector multiplications problem. We extend such strategy for arbitrary values of $α$ (integer or non-integer). We then establish the connection between the matrix-based Renyi's entropy and PSD matrix approximation, which enables us to exploit both clustering and block low-rank structure of $\G$ to further reduce the computational cost. We theoretically provide approximation accuracy guarantees and illustrate the properties of different approximations. Large-scale experimental evaluations on both synthetic and real-world data corroborate our theoretical findings, showing promising speedup with negligible loss in accuracy.

</p>
</details>

<details><summary><b>Multi-Image Visual Question Answering</b>
<a href="https://arxiv.org/abs/2112.13706">arxiv:2112.13706</a>
&#x1F4C8; 3 <br>
<p>Harsh Raj, Janhavi Dadhania, Akhilesh Bhardwaj</p></summary>
<p>

**Abstract:** While a lot of work has been done on developing models to tackle the problem of Visual Question Answering, the ability of these models to relate the question to the image features still remain less explored. We present an empirical study of different feature extraction methods with different loss functions. We propose New dataset for the task of Visual Question Answering with multiple image inputs having only one ground truth, and benchmark our results on them. Our final model utilising Resnet + RCNN image features and Bert embeddings, inspired from stacked attention network gives 39% word accuracy and 99% image accuracy on CLEVER+TinyImagenet dataset.

</p>
</details>

<details><summary><b>Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN</b>
<a href="https://arxiv.org/abs/2112.13626">arxiv:2112.13626</a>
&#x1F4C8; 3 <br>
<p>André Ferreira, Ricardo Magalhães, Sébastien Mériaux, Victor Alves</p></summary>
<p>

**Abstract:** Translational brain research using Magnetic Resonance Imaging (MRI) is becoming increasingly popular as animal models are an essential part of scientific studies and more ultra-high-field scanners are becoming available. Some disadvantages of MRI are the availability of MRI scanners and the time required for a full scanning session (it usually takes over 30 minutes). Privacy laws and the 3Rs ethics rule also make it difficult to create large datasets for training deep learning models. Generative Adversarial Networks (GANs) can perform data augmentation with higher quality than other techniques. In this work, the alpha-GAN architecture is used to test its ability to produce realistic 3D MRI scans of the rat brain. As far as the authors are aware, this is the first time that a GAN-based approach has been used for data augmentation in preclinical data. The generated scans are evaluated using various qualitative and quantitative metrics. A Turing test conducted by 4 experts has shown that the generated scans can trick almost any expert. The generated scans were also used to evaluate their impact on the performance of an existing deep learning model developed for segmenting the rat brain into white matter, grey matter and cerebrospinal fluid. The models were compared using the Dice score. The best results for whole brain and white matter segmentation were obtained when 174 real scans and 348 synthetic scans were used, with improvements of 0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans resulted in improvements of 0.0038 and 0.0764 for grey matter and CSF segmentation, respectively. Thus, by using the proposed new normalisation layer and loss functions, it was possible to improve the realism of the generated rat MRI scans and it was shown that using the generated data improved the segmentation model more than using the conventional data augmentation.

</p>
</details>

<details><summary><b>PRIME: A Few Primitives Can Boost Robustness to Common Corruptions</b>
<a href="https://arxiv.org/abs/2112.13547">arxiv:2112.13547</a>
&#x1F4C8; 3 <br>
<p>Apostolos Modas, Rahul Rade, Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard</p></summary>
<p>

**Abstract:** Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to many common corruptions of their data. To fix this vulnerability, prior works have mostly focused on increasing the complexity of their training pipelines, combining multiple methods, in the name of diversity. However, in this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that consists of simple families of max-entropy image transformations. We show that PRIME outperforms the prior art for corruption robustness, while its simplicity and plug-and-play nature enables it to be combined with other methods to further boost their robustness. Furthermore, we analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes.

</p>
</details>

<details><summary><b>A Graph Attention Learning Approach to Antenna Tilt Optimization</b>
<a href="https://arxiv.org/abs/2112.14843">arxiv:2112.14843</a>
&#x1F4C8; 2 <br>
<p>Yifei Jin, Filippo Vannella, Maxime Bouton, Jaeseong Jeong, Ezeddin Al Hakim</p></summary>
<p>

**Abstract:** 6G will move mobile networks towards increasing levels of complexity. To deal with this complexity, optimization of network parameters is key to ensure high performance and timely adaptivity to dynamic network environments. The optimization of the antenna tilt provides a practical and cost-efficient method to improve coverage and capacity in the network. Previous methods based on Reinforcement Learning (RL) have shown great promise for tilt optimization by learning adaptive policies outperforming traditional tilt optimization methods. However, most existing RL methods are based on single-cell features representation, which fails to fully characterize the agent state, resulting in suboptimal performance. Also, most of such methods lack scalability, due to state-action explosion, and generalization ability. In this paper, we propose a Graph Attention Q-learning (GAQ) algorithm for tilt optimization. GAQ relies on a graph attention mechanism to select relevant neighbors information, improve the agent state representation, and update the tilt control policy based on a history of observations using a Deep Q-Network (DQN). We show that GAQ efficiently captures important network information and outperforms standard DQN with local information by a large margin. In addition, we demonstrate its ability to generalize to network deployments of different sizes and densities.

</p>
</details>

<details><summary><b>Video Reconstruction from a Single Motion Blurred Image using Learned Dynamic Phase Coding</b>
<a href="https://arxiv.org/abs/2112.14768">arxiv:2112.14768</a>
&#x1F4C8; 2 <br>
<p>Erez Yosef, Shay Elmalem, Raja Giryes</p></summary>
<p>

**Abstract:** Video reconstruction from a single motion-blurred image is a challenging problem, which can enhance existing cameras' capabilities. Recently, several works addressed this task using conventional imaging and deep learning. Yet, such purely-digital methods are inherently limited, due to direction ambiguity and noise sensitivity. Some works proposed to address these limitations using non-conventional image sensors, however, such sensors are extremely rare and expensive. To circumvent these limitations with simpler means, we propose a hybrid optical-digital method for video reconstruction that requires only simple modifications to existing optical systems. We use a learned dynamic phase-coding in the lens aperture during the image acquisition to encode the motion trajectories, which serve as prior information for the video reconstruction process. The proposed computational camera generates a sharp frame burst of the scene at various frame rates from a single coded motion-blurred image, using an image-to-video convolutional neural network. We present advantages and improved performance compared to existing methods, using both simulations and a real-world camera prototype.

</p>
</details>

<details><summary><b>HiKonv: High Throughput Quantized Convolution With Novel Bit-wise Management and Computation</b>
<a href="https://arxiv.org/abs/2112.13972">arxiv:2112.13972</a>
&#x1F4C8; 2 <br>
<p>Xinheng Liu, Yao Chen, Prakhar Ganesh, Junhao Pan, Jinjun Xiong, Deming Chen</p></summary>
<p>

**Abstract:** Quantization for Convolutional Neural Network (CNN) has shown significant progress with the intention of reducing the cost of computation and storage with low-bitwidth data inputs. There are, however, no systematic studies on how an existing full-bitwidth processing unit, such as CPUs and DSPs, can be better utilized to carry out significantly higher computation throughput for convolution under various quantized bitwidths. In this study, we propose HiKonv, a unified solution that maximizes the compute throughput of a given underlying processing unit to process low-bitwidth quantized data inputs through novel bit-wise parallel computation. We establish theoretical performance bounds using a full-bitwidth multiplier for highly parallelized low-bitwidth convolution, and demonstrate new breakthroughs for high-performance computing in this critical domain. For example, a single 32-bit processing unit can deliver 128 binarized convolution operations (multiplications and additions) under one CPU instruction, and a single 27x18 DSP core can deliver eight convolution operations with 4-bit inputs in one cycle. We demonstrate the effectiveness of HiKonv on CPU and FPGA for both convolutional layers or a complete DNN model. For a convolutional layer quantized to 4-bit, HiKonv achieves a 3.17x latency improvement over the baseline implementation using C++ on CPU. Compared to the DAC-SDC 2020 champion model for FPGA, HiKonv achieves a 2.37x throughput improvement and 2.61x DSP efficiency improvement, respectively.

</p>
</details>

<details><summary><b>RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes</b>
<a href="https://arxiv.org/abs/2112.13934">arxiv:2112.13934</a>
&#x1F4C8; 2 <br>
<p>Salman Habib, Allison Beemer, Joerg Kliewer</p></summary>
<p>

**Abstract:** In this work we propose RELDEC, a novel approach for sequential decoding of moderate length low-density parity-check (LDPC) codes. The main idea behind RELDEC is that an optimized decoding policy is subsequently obtained via reinforcement learning based on a Markov decision process (MDP). In contrast to our previous work, where an agent learns to schedule only a single check node (CN) within a group (cluster) of CNs per iteration, in this work we train the agent to schedule all CNs in a cluster, and all clusters in every iteration. That is, in each learning step of RELDEC an agent learns to schedule CN clusters sequentially depending on a reward associated with the outcome of scheduling a particular cluster. We also modify the state space representation of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes than those studied in our previous work. Furthermore, to address decoding under varying channel conditions, we propose two related schemes, namely, agile meta-RELDEC (AM-RELDEC) and meta-RELDEC (M-RELDEC), both of which employ meta-reinforcement learning. The proposed RELDEC scheme significantly outperforms standard flooding and random sequential decoding for a variety of LDPC codes, including codes designed for 5G new radio.

</p>
</details>

<details><summary><b>Resource-Efficient and Delay-Aware Federated Learning Design under Edge Heterogeneity</b>
<a href="https://arxiv.org/abs/2112.13926">arxiv:2112.13926</a>
&#x1F4C8; 2 <br>
<p>David Nickel, Frank Po-Chen Lin, Seyyedali Hosseinalipour, Nicolo Michelusi, Christopher G. Brinton</p></summary>
<p>

**Abstract:** Federated learning (FL) has emerged as a popular methodology for distributing machine learning across wireless edge devices. In this work, we consider optimizing the tradeoff between model performance and resource utilization in FL, under device-server communication delays and device computation heterogeneity. Our proposed StoFedDelAv algorithm incorporates a local-global model combiner into the FL synchronization step. We theoretically characterize the convergence behavior of StoFedDelAv and obtain the optimal combiner weights, which consider the global model delay and expected local gradient error at each device. We then formulate a network-aware optimization problem which tunes the minibatch sizes of the devices to jointly minimize energy consumption and machine learning training loss, and solve the non-convex problem through a series of convex approximations. Our simulations reveal that StoFedDelAv outperforms the current art in FL in terms of model convergence speed and network resource utilization when the minibatch size and the combiner weights are adjusted. Additionally, our method can reduce the number of uplink communication rounds required during the model training period to reach the same accuracy.

</p>
</details>

<details><summary><b>HOPE: A Task-Oriented and Human-Centric Evaluation Framework Using Professional Post-Editing Towards More Effective MT Evaluation</b>
<a href="https://arxiv.org/abs/2112.13833">arxiv:2112.13833</a>
&#x1F4C8; 2 <br>
<p>Serge Gladkoff, Lifeng Han</p></summary>
<p>

**Abstract:** Traditional automatic evaluation metrics for machine translation have been widely criticized by linguists due to their low accuracy, lack of transparency, focus on language mechanics rather than semantics, and low agreement with human quality evaluation. Human evaluations in the form of MQM-like scorecards have always been carried out in real industry setting by both clients and translation service providers (TSPs). However, traditional human translation quality evaluations are costly to perform and go into great linguistic detail, raise issues as to inter-rater reliability (IRR) and are not designed to measure quality of worse than premium quality translations. In this work, we introduce HOPE, a task-oriented and human-centric evaluation framework for machine translation output based on professional post-editing annotations. It contains only a limited number of commonly occurring error types, and use a scoring model with geometric progression of error penalty points (EPPs) reflecting error severity level to each translation unit. The initial experimental work carried out on English-Russian language pair MT outputs on marketing content type of text from highly technical domain reveals that our evaluation framework is quite effective in reflecting the MT output quality regarding both overall system-level performance and segment-level transparency, and it increases the IRR for error type interpretation. The approach has several key advantages, such as ability to measure and compare less than perfect MT output from different systems, ability to indicate human perception of quality, immediate estimation of the labor effort required to bring MT output to premium quality, low-cost and faster application, as well as higher IRR. Our experimental data is available at \url{https://github.com/lHan87/HOPE}.

</p>
</details>

<details><summary><b>Infant Brain Age Classification: 2D CNN Outperforms 3D CNN in Small Dataset</b>
<a href="https://arxiv.org/abs/2112.13811">arxiv:2112.13811</a>
&#x1F4C8; 2 <br>
<p>Mahdieh Shabanian, Markus Wenzel, John P. DeVincenzo</p></summary>
<p>

**Abstract:** Determining if the brain is developing normally is a key component of pediatric neuroradiology and neurology. Brain magnetic resonance imaging (MRI) of infants demonstrates a specific pattern of development beyond simply myelination. While radiologists have used myelination patterns, brain morphology and size characteristics to determine age-adequate brain maturity, this requires years of experience in pediatric neuroradiology. With no standardized criteria, visual estimation of the structural maturity of the brain from MRI before three years of age remains dominated by inter-observer and intra-observer variability. A more objective estimation of brain developmental age could help physicians identify many neurodevelopmental conditions and diseases earlier and more reliably. Such data, however, is naturally hard to obtain, and the observer ground truth not much of a gold standard due to subjectivity of assessment. In this light, we explore the general feasibility to tackle this task, and the utility of different approaches, including two- and three-dimensional convolutional neural networks (CNN) that were trained on a fusion of T1-weighted, T2-weighted, and proton density (PD) weighted sequences from 84 individual subjects divided into four age groups from birth to 3 years of age. In the best performing approach, we achieved an accuracy of 0.90 [95% CI:0.86-0.94] using a 2D CNN on a central axial thick slab. We discuss the comparison to 3D networks and show how the performance compares to the use of only one sequence (T1w). In conclusion, despite the theoretical superiority of 3D CNN approaches, in limited-data situations, such approaches are inferior to simpler architectures. The code can be found in https://github.com/shabanian2018/Age_MRI-Classification

</p>
</details>

<details><summary><b>A Multi-channel Training Method Boost the Performance</b>
<a href="https://arxiv.org/abs/2112.13727">arxiv:2112.13727</a>
&#x1F4C8; 2 <br>
<p>Yingdong Hu</p></summary>
<p>

**Abstract:** Deep convolutional neural network has made huge revolution and shown its superior performance on computer vision tasks such as classification and segmentation. Recent years, researches devote much effort to scaling down size of network while maintaining its ability, to adapt to the limited memory on embedded systems like mobile phone. In this paper, we propose a multi-channel training procedure which can highly facilitate the performance and robust of the target network. The proposed procedure contains two sets of networks and two information pipelines which can work independently hinge on the computation ability of the embedded platform, while in the mean time, the classification accuracy is also admirably enhanced.

</p>
</details>

<details><summary><b>Graph Collaborative Reasoning</b>
<a href="https://arxiv.org/abs/2112.13705">arxiv:2112.13705</a>
&#x1F4C8; 2 <br>
<p>Hanxiong Chen, Yunqi Li, Shaoyun Shi, Shuchang Liu, He Zhu, Yongfeng Zhang</p></summary>
<p>

**Abstract:** Graphs can represent relational information among entities and graph structures are widely used in many intelligent tasks such as search, recommendation, and question answering. However, most of the graph-structured data in practice suffers from incompleteness, and thus link prediction becomes an important research problem. Though many models are proposed for link prediction, the following two problems are still less explored: (1) Most methods model each link independently without making use of the rich information from relevant links, and (2) existing models are mostly designed based on associative learning and do not take reasoning into consideration. With these concerns, in this paper, we propose Graph Collaborative Reasoning (GCR), which can use the neighbor link information for relational reasoning on graphs from logical reasoning perspectives. We provide a simple approach to translate a graph structure into logical expressions, so that the link prediction task can be converted into a neural logic reasoning problem. We apply logical constrained neural modules to build the network architecture according to the logical expression and use back propagation to efficiently learn the model parameters, which bridges differentiable learning and symbolic reasoning in a unified architecture. To show the effectiveness of our work, we conduct experiments on graph-related tasks such as link prediction and recommendation based on commonly used benchmark datasets, and our graph collaborative reasoning approach achieves state-of-the-art performance.

</p>
</details>

<details><summary><b>Self-normalized Classification of Parkinson's Disease DaTscan Images</b>
<a href="https://arxiv.org/abs/2112.13637">arxiv:2112.13637</a>
&#x1F4C8; 2 <br>
<p>Yuan Zhou, Hemant D. Tagare</p></summary>
<p>

**Abstract:** Classifying SPECT images requires a preprocessing step which normalizes the images using a normalization region. The choice of the normalization region is not standard, and using different normalization regions introduces normalization region-dependent variability. This paper mathematically analyzes the effect of the normalization region to show that normalized-classification is exactly equivalent to a subspace separation of the half rays of the images under multiplicative equivalence. Using this geometry, a new self-normalized classification strategy is proposed. This strategy eliminates the normalizing region altogether. The theory is used to classify DaTscan images of 365 Parkinson's disease (PD) subjects and 208 healthy control (HC) subjects from the Parkinson's Progression Marker Initiative (PPMI). The theory is also used to understand PD progression from baseline to year 4.

</p>
</details>

<details><summary><b>Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic</b>
<a href="https://arxiv.org/abs/2112.13530">arxiv:2112.13530</a>
&#x1F4C8; 2 <br>
<p>Yufeng Zhang, Siyu Chen, Zhuoran Yang, Michael I. Jordan, Zhaoran Wang</p></summary>
<p>

**Abstract:** Actor-critic (AC) algorithms, empowered by neural networks, have had significant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed throughout training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy optimization (PPO) with a smaller stepsize. In the continuous-time and infinite-width limiting regime, when the timescales are properly separated, we prove that neural AC finds the globally optimal policy at a sublinear rate. Additionally, we prove that the feature representation induced by the critic network is allowed to evolve within a neighborhood of the initial one.

</p>
</details>

<details><summary><b>Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?</b>
<a href="https://arxiv.org/abs/2112.13521">arxiv:2112.13521</a>
&#x1F4C8; 2 <br>
<p>Han Zhong, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan</p></summary>
<p>

**Abstract:** We study multi-player general-sum Markov games with one of the players designated as the leader and the other players regarded as followers. In particular, we focus on the class of games where the followers are myopic, i.e., they aim to maximize their instantaneous rewards. For such a game, our goal is to find a Stackelberg-Nash equilibrium (SNE), which is a policy pair $(π^*, ν^*)$ such that (i) $π^*$ is the optimal policy for the leader when the followers always play their best response, and (ii) $ν^*$ is the best response policy of the followers, which is a Nash equilibrium of the followers' game induced by $π^*$. We develop sample-efficient reinforcement learning (RL) algorithms for solving for an SNE in both online and offline settings. Our algorithms are optimistic and pessimistic variants of least-squares value iteration, and they are readily able to incorporate function approximation tools in the setting of large state spaces. Furthermore, for the case with linear function approximation, we prove that our algorithms achieve sublinear regret and suboptimality under online and offline setups respectively. To the best of our knowledge, we establish the first provably efficient RL algorithms for solving for SNEs in general-sum Markov games with myopic followers.

</p>
</details>

<details><summary><b>An EEG-based approach for Parkinson's disease diagnosis using Capsule network</b>
<a href="https://arxiv.org/abs/2201.00628">arxiv:2201.00628</a>
&#x1F4C8; 1 <br>
<p>Shujie Wang, Gongshu Wang, Guangying Pei</p></summary>
<p>

**Abstract:** As the second most common neurodegenerative disease, Parkinson's disease has caused serious problems worldwide. However, the cause and mechanism of PD are not clear, and no systematic early diagnosis and treatment of PD have been established. Many patients with PD have not been diagnosed or misdiagnosed. In this paper, we proposed an EEG-based approach to diagnosing Parkinson's disease. It mapped the frequency band energy of electroencephalogram(EEG) signals to 2-dimensional images using the interpolation method and identified classification using capsule network(CapsNet) and achieved 89.34% classification accuracy for short-term EEG sections. A comparison of separate classification accuracy across different EEG bands revealed the highest accuracy in the gamma bands, suggesting that we need to pay more attention to the changes in gamma band changes in the early stages of PD.

</p>
</details>

<details><summary><b>Tracking Most Severe Arm Changes in Bandits</b>
<a href="https://arxiv.org/abs/2112.13838">arxiv:2112.13838</a>
&#x1F4C8; 1 <br>
<p>Joe Suk, Samory Kpotufe</p></summary>
<p>

**Abstract:** In bandits with distribution shifts, one aims to automatically detect an unknown number $L$ of changes in reward distribution, and restart exploration when necessary. While this problem remained open for many years, a recent breakthrough of Auer et al. (2018, 2019) provide the first adaptive procedure to guarantee an optimal (dynamic) regret $\sqrt{LT}$, for $T$ rounds, with no knowledge of $L$. However, not all distributional shifts are equally severe, e.g., suppose no best arm switches occur, then we cannot rule out that a regret $O(\sqrt{T})$ may remain possible; in other words, is it possible to achieve dynamic regret that optimally scales only with an unknown number of severe shifts? This unfortunately has remained elusive, despite various attempts (Auer et al., 2019, Foster et al., 2020).
  We resolve this problem in the case of two-armed bandits: we derive an adaptive procedure that guarantees a dynamic regret of order $\tilde{O}(\sqrt{\tilde{L} T})$, where $\tilde L \ll L$ captures an unknown number of severe best arm changes, i.e., with significant switches in rewards, and which last sufficiently long to actually require a restart. As a consequence, for any number $L$ of distributional shifts outside of these severe shifts, our procedure achieves regret just $\tilde{O}(\sqrt{T})\ll \tilde{O}(\sqrt{LT})$.
  Finally, we note that our notion of severe shift applies in both classical settings of stochastic switching bandits and of adversarial bandits.

</p>
</details>

<details><summary><b>Understanding RoBERTa's Mood: The Role of Contextual-Embeddings as User-Representations for Depression Prediction</b>
<a href="https://arxiv.org/abs/2112.13795">arxiv:2112.13795</a>
&#x1F4C8; 1 <br>
<p>Matthew Matero, Albert Hung, H. Andrew Schwartz</p></summary>
<p>

**Abstract:** Many works in natural language processing have shown connections between a person's personal discourse and their personality, demographics, and mental health states. However, many of the machine learning models that predict such human traits have yet to fully consider the role of pre-trained language models and contextual embeddings. Using a person's degree of depression as a case study, we do an empirical analysis on which off-the-shelf language model, individual layers, and combinations of layers seem most promising when applied to human-level NLP tasks. Notably, despite the standard in past work of suggesting use of either the second-to-last or the last 4 layers, we find layer 19 (sixth-to last) is the most ideal by itself, while when using multiple layers, distributing them across the second half(i.e. Layers 12+) of the 24 layers is best.

</p>
</details>

<details><summary><b>Contextual Sentence Analysis for the Sentiment Prediction on Financial Data</b>
<a href="https://arxiv.org/abs/2112.13790">arxiv:2112.13790</a>
&#x1F4C8; 1 <br>
<p>Elvys Linhares Pontes, Mohamed Benjannet</p></summary>
<p>

**Abstract:** Newsletters and social networks can reflect the opinion about the market and specific stocks from the perspective of analysts and the general public on products and/or services provided by a company. Therefore, sentiment analysis of these texts can provide useful information to help investors trade in the market. In this paper, a hierarchical stack of Transformers model is proposed to identify the sentiment associated with companies and stocks, by predicting a score (of data type real) in a range between -1 and +1. Specifically, we fine-tuned a RoBERTa model to process headlines and microblogs and combined it with additional Transformer layers to process the sentence analysis with sentiment dictionaries to improve the sentiment analysis. We evaluated it on financial data released by SemEval-2017 task 5 and our proposition outperformed the best systems of SemEval-2017 task 5 and strong baselines. Indeed, the combination of contextual sentence analysis with the financial and general sentiment dictionaries provided useful information to our model and allowed it to generate more reliable sentiment scores.

</p>
</details>

<details><summary><b>Hamtajoo: A Persian Plagiarism Checker for Academic Manuscripts</b>
<a href="https://arxiv.org/abs/2112.13742">arxiv:2112.13742</a>
&#x1F4C8; 1 <br>
<p>Vahid Zarrabi, Salar Mohtaj, Habibollah Asghari</p></summary>
<p>

**Abstract:** In recent years, due to the high availability of electronic documents through the Web, the plagiarism has become a serious challenge, especially among scholars. Various plagiarism detection systems have been developed to prevent text re-use and to confront plagiarism. Although it is almost easy to detect duplicate text in academic manuscripts, finding patterns of text re-use that has been semantically changed is of great importance. Another important issue is to deal with less resourced languages, which there are low volume of text for training purposes and also low performance in tools for NLP applications. In this paper, we introduce Hamtajoo, a Persian plagiarism detection system for academic manuscripts. Moreover, we describe the overall structure of the system along with the algorithms used in each stage. In order to evaluate the performance of the proposed system, we used a plagiarism detection corpus comply with the PAN standards.

</p>
</details>

<details><summary><b>Learn Layer-wise Connections in Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2112.13585">arxiv:2112.13585</a>
&#x1F4C8; 1 <br>
<p>Lanning Wei, Huan Zhao, Zhiqiang He</p></summary>
<p>

**Abstract:** In recent years, Graph Neural Networks (GNNs) have shown superior performance on diverse applications on real-world datasets. To improve the model capacity and alleviate the over-smoothing problem, several methods proposed to incorporate the intermediate layers by layer-wise connections. However, due to the highly diverse graph types, the performance of existing methods vary on diverse graphs, leading to a need for data-specific layer-wise connection methods. To address this problem, we propose a novel framework LLC (Learn Layer-wise Connections) based on neural architecture search (NAS) to learn adaptive connections among intermediate layers in GNNs. LLC contains one novel search space which consists of 3 types of blocks and learnable connections, and one differentiable search algorithm to enable the efficient search process. Extensive experiments on five real-world datasets are conducted, and the results show that the searched layer-wise connections can not only improve the performance but also alleviate the over-smoothing problem.

</p>
</details>

<details><summary><b>FitAct: Error Resilient Deep Neural Networks via Fine-Grained Post-Trainable Activation Functions</b>
<a href="https://arxiv.org/abs/2112.13544">arxiv:2112.13544</a>
&#x1F4C8; 1 <br>
<p>Behnam Ghavami, Mani Sadati, Zhenman Fang, Lesley Shannon</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are increasingly being deployed in safety-critical systems such as personal healthcare devices and self-driving cars. In such DNN-based systems, error resilience is a top priority since faults in DNN inference could lead to mispredictions and safety hazards. For latency-critical DNN inference on resource-constrained edge devices, it is nontrivial to apply conventional redundancy-based fault tolerance techniques. In this paper, we propose FitAct, a low-cost approach to enhance the error resilience of DNNs by deploying fine-grained post-trainable activation functions. The main idea is to precisely bound the activation value of each individual neuron via neuron-wise bounded activation functions so that it could prevent fault propagation in the network. To avoid complex DNN model re-training, we propose to decouple the accuracy training and resilience training and develop a lightweight post-training phase to learn these activation functions with precise bound values. Experimental results on widely used DNN models such as AlexNet, VGG16, and ResNet50 demonstrate that FitAct outperforms state-of-the-art studies such as Clip-Act and Ranger in enhancing the DNN error resilience for a wide range of fault rates while adding manageable runtime and memory space overheads.

</p>
</details>

<details><summary><b>Anomaly Detection using Capsule Networks for High-dimensional Datasets</b>
<a href="https://arxiv.org/abs/2112.13514">arxiv:2112.13514</a>
&#x1F4C8; 1 <br>
<p>Inderjeet Singh, Nandyala Hemachandra</p></summary>
<p>

**Abstract:** Anomaly detection is an essential problem in machine learning. Application areas include network security, health care, fraud detection, etc., involving high-dimensional datasets. A typical anomaly detection system always faces the class-imbalance problem in the form of a vast difference in the sample sizes of different classes. They usually have class overlap problems. This study used a capsule network for the anomaly detection task. To the best of our knowledge, this is the first instance where a capsule network is analyzed for the anomaly detection task in a high-dimensional complex data setting. We also handle the related novelty and outlier detection problems. The architecture of the capsule network was suitably modified for a binary classification task. Capsule networks offer a good option for detecting anomalies due to the effect of viewpoint invariance captured in its predictions and viewpoint equivariance captured in internal capsule architecture. We used six-layered under-complete autoencoder architecture with second and third layers containing capsules. The capsules were trained using the dynamic routing algorithm. We created $10$-imbalanced datasets from the original MNIST dataset and compared the performance of the capsule network with $5$ baseline models. Our leading test set measures are F1-score for minority class and area under the ROC curve. We found that the capsule network outperformed every other baseline model on the anomaly detection task by using only ten epochs for training and without using any other data level and algorithm level approach. Thus, we conclude that capsule networks are excellent in modeling complex high-dimensional imbalanced datasets for the anomaly detection task.

</p>
</details>

<details><summary><b>Faster Algorithms and Constant Lower Bounds for the Worst-Case Expected Error</b>
<a href="https://arxiv.org/abs/2112.13832">arxiv:2112.13832</a>
&#x1F4C8; 0 <br>
<p>Jonah Brown-Cohen</p></summary>
<p>

**Abstract:** The study of statistical estimation without distributional assumptions on data values, but with knowledge of data collection methods was recently introduced by Chen, Valiant and Valiant (NeurIPS 2020). In this framework, the goal is to design estimators that minimize the worst-case expected error. Here the expectation is over a known, randomized data collection process from some population, and the data values corresponding to each element of the population are assumed to be worst-case.
  Chen, Valiant and Valiant show that, when data values are $\ell_{\infty}$-normalized, there is a polynomial time algorithm to compute an estimator for the mean with worst-case expected error that is within a factor $\fracπ{2}$ of the optimum within the natural class of semilinear estimators. However, their algorithm is based on optimizing a somewhat complex concave objective function over a constrained set of positive semidefinite matrices, and thus does not come with explicit runtime guarantees beyond being polynomial time in the input.
  In this paper we design provably efficient algorithms for approximating the optimal semilinear estimator based on online convex optimization. In the setting where data values are $\ell_{\infty}$-normalized, our algorithm achieves a $\fracπ{2}$-approximation by iteratively solving a sequence of standard SDPs. When data values are $\ell_2$-normalized, our algorithm iteratively computes the top eigenvector of a sequence of matrices, and does not lose any multiplicative approximation factor. We complement these positive results by stating a simple combinatorial condition which, if satisfied by a data collection process, implies that any (not necessarily semilinear) estimator for the mean has constant worst-case expected error.

</p>
</details>

<details><summary><b>Temporally Constrained Neural Networks (TCNN): A framework for semi-supervised video semantic segmentation</b>
<a href="https://arxiv.org/abs/2112.13815">arxiv:2112.13815</a>
&#x1F4C8; 0 <br>
<p>Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan, Alain Garcia, Nariaki Okamoto, Didier Mutter, Jacques Marescaux, Guido Costamagna, Bernard Dallemagne, Nicolas Padoy</p></summary>
<p>

**Abstract:** A major obstacle to building models for effective semantic segmentation, and particularly video semantic segmentation, is a lack of large and well annotated datasets. This bottleneck is particularly prohibitive in highly specialized and regulated fields such as medicine and surgery, where video semantic segmentation could have important applications but data and expert annotations are scarce. In these settings, temporal clues and anatomical constraints could be leveraged during training to improve performance. Here, we present Temporally Constrained Neural Networks (TCNN), a semi-supervised framework used for video semantic segmentation of surgical videos. In this work, we show that autoencoder networks can be used to efficiently provide both spatial and temporal supervisory signals to train deep learning models. We test our method on a newly introduced video dataset of laparoscopic cholecystectomy procedures, Endoscapes, and an adaptation of a public dataset of cataract surgeries, CaDIS. We demonstrate that lower-dimensional representations of predicted masks can be leveraged to provide a consistent improvement on both sparsely labeled datasets with no additional computational cost at inference time. Further, the TCNN framework is model-agnostic and can be used in conjunction with other model design choices with minimal additional complexity.

</p>
</details>

<details><summary><b>Over-the-Air Multi-Task Federated Learning Over MIMO Interference Channel</b>
<a href="https://arxiv.org/abs/2112.13603">arxiv:2112.13603</a>
&#x1F4C8; 0 <br>
<p>Chenxi Zhong, Huiyuan Yang, Xiaojun Yuan</p></summary>
<p>

**Abstract:** With the explosive growth of data and wireless devices, federated learning (FL) has emerged as a promising technology for large-scale intelligent systems. Utilizing the analog superposition of electromagnetic waves, over-the-air computation is an appealing approach to reduce the burden of communication in the FL model aggregation. However, with the urgent demand for intelligent systems, the training of multiple tasks with over-the-air computation further aggravates the scarcity of communication resources. This issue can be alleviated to some extent by training multiple tasks simultaneously with shared communication resources, but the latter inevitably brings about the problem of inter-task interference. In this paper, we study over-the-air multi-task FL (OA-MTFL) over the multiple-input multiple-output (MIMO) interference channel. We propose a novel model aggregation method for the alignment of local gradients for different devices, which alleviates the straggler problem that exists widely in over-the-air computation due to the channel heterogeneity. We establish a unified communication-computation analysis framework for the proposed OA-MTFL scheme by considering the spatial correlation between devices, and formulate an optimization problem of designing transceiver beamforming and device selection. We develop an algorithm by using alternating optimization (AO) and fractional programming (FP) to solve this problem, which effectively relieves the impact of inter-task interference on the FL learning performance. We show that due to the use of the new model aggregation method, device selection is no longer essential to our scheme, thereby avoiding the heavy computational burden caused by implementing device selection. The numerical results demonstrate the correctness of the analysis and the outstanding performance of the proposed scheme.

</p>
</details>


{% endraw %}
Prev: [2021.12.26]({{ '/2021/12/26/2021.12.26.html' | relative_url }})  Next: [2021.12.28]({{ '/2021/12/28/2021.12.28.html' | relative_url }})