## Summary for 2021-07-17, created on 2021-12-19


<details><summary><b>Visual Representation Learning Does Not Generalize Strongly Within the Same Domain</b>
<a href="https://arxiv.org/abs/2107.08221">arxiv:2107.08221</a>
&#x1F4C8; 14 <br>
<p>Lukas Schott, Julius von Kügelgen, Frederik Träuble, Peter Gehler, Chris Russell, Matthias Bethge, Bernhard Schölkopf, Francesco Locatello, Wieland Brendel</p></summary>
<p>

**Abstract:** An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.

</p>
</details>

<details><summary><b>Learning De-identified Representations of Prosody from Raw Audio</b>
<a href="https://arxiv.org/abs/2107.08248">arxiv:2107.08248</a>
&#x1F4C8; 13 <br>
<p>Jack Weston, Raphael Lenain, Udeepa Meepegama, Emil Fristed</p></summary>
<p>

**Abstract:** We propose a method for learning de-identified prosody representations from raw audio using a contrastive self-supervised signal. Whereas prior work has relied on conditioning models on bottlenecks, we introduce a set of inductive biases that exploit the natural structure of prosody to minimize timbral information and decouple prosody from speaker representations. Despite aggressive downsampling of the input and having no access to linguistic information, our model performs comparably to state-of-the-art speech representations on DAMMP, a new benchmark we introduce for spoken language understanding. We use minimum description length probing to show that our representations have selectively learned the subcomponents of non-timbral prosody, and that the product quantizer naturally disentangles them without using bottlenecks. We derive an information-theoretic definition of speech de-identifiability and use it to demonstrate that our prosody representations are less identifiable than other speech representations.

</p>
</details>

<details><summary><b>Generative Pretraining for Paraphrase Evaluation</b>
<a href="https://arxiv.org/abs/2107.08251">arxiv:2107.08251</a>
&#x1F4C8; 12 <br>
<p>Jack Weston, Raphael Lenain, Udeepa Meepegama, Emil Fristed</p></summary>
<p>

**Abstract:** We introduce ParaBLEU, a paraphrase representation learning model and evaluation metric for text generation. Unlike previous approaches, ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective. ParaBLEU correlates more strongly with human judgements than existing metrics, obtaining new state-of-the-art results on the 2017 WMT Metrics Shared Task. We show that our model is robust to data scarcity, exceeding previous state-of-the-art performance using only $50\%$ of the available training data and surpassing BLEU, ROUGE and METEOR with only $40$ labelled examples. Finally, we demonstrate that ParaBLEU can be used to conditionally generate novel paraphrases from a single demonstration, which we use to confirm our hypothesis that it learns abstract, generalized paraphrase representations.

</p>
</details>

<details><summary><b>On the Copying Behaviors of Pre-Training for Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2107.08212">arxiv:2107.08212</a>
&#x1F4C8; 10 <br>
<p>Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, Zhaopeng Tu</p></summary>
<p>

**Abstract:** Previous studies have shown that initializing neural machine translation (NMT) models with the pre-trained language models (LM) can speed up the model training and boost the model performance. In this work, we identify a critical side-effect of pre-training for NMT, which is due to the discrepancy between the training objectives of LM-based pre-training and NMT. Since the LM objective learns to reconstruct a few source tokens and copy most of them, the pre-training initialization would affect the copying behaviors of NMT models. We provide a quantitative analysis of copying behaviors by introducing a metric called copying ratio, which empirically shows that pre-training based NMT models have a larger copying ratio than the standard one. In response to this problem, we propose a simple and effective method named copying penalty to control the copying behaviors in decoding. Extensive experiments on both in-domain and out-of-domain benchmarks show that the copying penalty method consistently improves translation performance by controlling copying behaviors for pre-training based NMT models. Source code is freely available at https://github.com/SunbowLiu/CopyingPenalty.

</p>
</details>

<details><summary><b>An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation</b>
<a href="https://arxiv.org/abs/2107.08361">arxiv:2107.08361</a>
&#x1F4C8; 9 <br>
<p>Xiangheng He, Junjie Chen, Georgios Rizos, Björn W. Schuller</p></summary>
<p>

**Abstract:** Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation.

</p>
</details>

<details><summary><b>Dynamic Transformer for Efficient Machine Translation on Embedded Devices</b>
<a href="https://arxiv.org/abs/2107.08199">arxiv:2107.08199</a>
&#x1F4C8; 8 <br>
<p>Hishan Parry, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, Geoff V. Merrett</p></summary>
<p>

**Abstract:** The Transformer architecture is widely used for machine translation tasks. However, its resource-intensive nature makes it challenging to implement on constrained embedded devices, particularly where available hardware resources can vary at run-time. We propose a dynamic machine translation model that scales the Transformer architecture based on the available resources at any particular time. The proposed approach, 'Dynamic-HAT', uses a HAT SuperTransformer as the backbone to search for SubTransformers with different accuracy-latency trade-offs at design time. The optimal SubTransformers are sampled from the SuperTransformer at run-time, depending on latency constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses inherited SubTransformers sampled directly from the SuperTransformer with a switching time of <1s. Using inherited SubTransformers results in a BLEU score loss of <1.5% because the SubTransformer configuration is not retrained from scratch after sampling. However, to recover this loss in performance, the dimensions of the design space can be reduced to tailor it to a family of target hardware. The new reduced design space results in a BLEU score increase of approximately 1% for sub-optimal models from the original design space, with a wide range for performance scaling between 0.356s - 1.526s for the GPU and 2.9s - 7.31s for the CPU.

</p>
</details>

<details><summary><b>STRODE: Stochastic Boundary Ordinary Differential Equation</b>
<a href="https://arxiv.org/abs/2107.08273">arxiv:2107.08273</a>
&#x1F4C8; 7 <br>
<p>Hengguan Huang, Hongfu Liu, Hao Wang, Chang Xiao, Ye Wang</p></summary>
<p>

**Abstract:** Perception of time from sequentially acquired sensory inputs is rooted in everyday behaviors of individual organisms. Yet, most algorithms for time-series modeling fail to learn dynamics of random event timings directly from visual or audio inputs, requiring timing annotations during training that are usually unavailable for real-world applications. For instance, neuroscience perspectives on postdiction imply that there exist variable temporal ranges within which the incoming sensory inputs can affect the earlier perception, but such temporal ranges are mostly unannotated for real applications such as automatic speech recognition (ASR). In this paper, we present a probabilistic ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE), that learns both the timings and the dynamics of time series data without requiring any timing annotations during training. STRODE allows the usage of differential equations to sample from the posterior point processes, efficiently and analytically. We further provide theoretical guarantees on the learning of STRODE. Our empirical results show that our approach successfully infers event timings of time series data. Our method achieves competitive or superior performances compared to existing state-of-the-art methods for both synthetic and real-world datasets.

</p>
</details>

<details><summary><b>Subset-of-Data Variational Inference for Deep Gaussian-Processes Regression</b>
<a href="https://arxiv.org/abs/2107.08265">arxiv:2107.08265</a>
&#x1F4C8; 7 <br>
<p>Ayush Jain, P. K. Srijith, Mohammad Emtiyaz Khan</p></summary>
<p>

**Abstract:** Deep Gaussian Processes (DGPs) are multi-layer, flexible extensions of Gaussian processes but their training remains challenging. Sparse approximations simplify the training but often require optimization over a large number of inducing inputs and their locations across layers. In this paper, we simplify the training by setting the locations to a fixed subset of data and sampling the inducing inputs from a variational distribution. This reduces the trainable parameters and computation cost without significant performance degradations, as demonstrated by our empirical results on regression problems. Our modifications simplify and stabilize DGP training while making it amenable to sampling schemes for setting the inducing inputs.

</p>
</details>

<details><summary><b>High-Accuracy Model-Based Reinforcement Learning, a Survey</b>
<a href="https://arxiv.org/abs/2107.08241">arxiv:2107.08241</a>
&#x1F4C8; 7 <br>
<p>Aske Plaat, Walter Kosters, Mike Preuss</p></summary>
<p>

**Abstract:** Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems from game playing and robotics have been solved with deep model-free methods. Unfortunately, the sample complexity of model-free methods is often high. To reduce the number of environment samples, model-based reinforcement learning creates an explicit model of the environment dynamics. Achieving high model accuracy is a challenge in high-dimensional problems. In recent years, a diverse landscape of model-based methods has been introduced to improve model accuracy, using methods such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. Some of these methods succeed in achieving high accuracy at low sample complexity, most do so either in a robotics or in a games context. In this paper, we survey these methods; we explain in detail how they work and what their strengths and weaknesses are. We conclude with a research agenda for future work to make the methods more robust and more widely applicable to other applications.

</p>
</details>

<details><summary><b>Self Training with Ensemble of Teacher Models</b>
<a href="https://arxiv.org/abs/2107.08211">arxiv:2107.08211</a>
&#x1F4C8; 7 <br>
<p>Soumyadeep Ghosh, Sanjay Kumar, Janu Verma, Awanish Kumar</p></summary>
<p>

**Abstract:** In order to train robust deep learning models, large amounts of labelled data is required. However, in the absence of such large repositories of labelled data, unlabeled data can be exploited for the same. Semi-Supervised learning aims to utilize such unlabeled data for training classification models. Recent progress of self-training based approaches have shown promise in this area, which leads to this study where we utilize an ensemble approach for the same. A by-product of any semi-supervised approach may be loss of calibration of the trained model especially in scenarios where unlabeled data may contain out-of-distribution samples, which leads to this investigation on how to adapt to such effects. Our proposed algorithm carefully avoids common pitfalls in utilizing unlabeled data and leads to a more accurate and calibrated supervised model compared to vanilla self-training based student-teacher algorithms. We perform several experiments on the popular STL-10 database followed by an extensive analysis of our approach and study its effects on model accuracy and calibration.

</p>
</details>

<details><summary><b>Minimising quantifier variance under prior probability shift</b>
<a href="https://arxiv.org/abs/2107.08209">arxiv:2107.08209</a>
&#x1F4C8; 7 <br>
<p>Dirk Tasche</p></summary>
<p>

**Abstract:** For the binary prevalence quantification problem under prior probability shift, we determine the asymptotic variance of the maximum likelihood estimator. We find that it is a function of the Brier score for the regression of the class label on the features under the test data set distribution. This observation suggests that optimising the accuracy of a base classifier, as measured by the Brier score, on the training data set helps to reduce the variance of the related quantifier on the test data set. Therefore, we also point out training criteria for the base classifier that imply optimisation of both of the Brier scores on the training and the test data sets.

</p>
</details>

<details><summary><b>Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election</b>
<a href="https://arxiv.org/abs/2107.08319">arxiv:2107.08319</a>
&#x1F4C8; 6 <br>
<p>Karishma Sharma, Emilio Ferrara, Yan Liu</p></summary>
<p>

**Abstract:** Identifying and characterizing disinformation in political discourse on social media is critical to ensure the integrity of elections and democratic processes around the world. Persistent manipulation of social media has resulted in increased concerns regarding the 2020 U.S. Presidential Election, due to its potential to influence individual opinions and social dynamics. In this work, we focus on the identification of distorted facts, in the form of unreliable and conspiratorial narratives in election-related tweets, to characterize discourse manipulation prior to the election. We apply a detection model to separate factual from unreliable (or conspiratorial) claims analyzing a dataset of 242 million election-related tweets. The identified claims are used to investigate targeted topics of disinformation, and conspiracy groups, most notably the far-right QAnon conspiracy group. Further, we characterize account engagements with unreliable and conspiracy tweets, and with the QAnon conspiracy group, by political leaning and tweet types. Finally, using a regression discontinuity design, we investigate whether Twitter's actions to curb QAnon activity on the platform were effective, and how QAnon accounts adapt to Twitter's restrictions.

</p>
</details>

<details><summary><b>Learning Augmented Online Facility Location</b>
<a href="https://arxiv.org/abs/2107.08277">arxiv:2107.08277</a>
&#x1F4C8; 6 <br>
<p>Dimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, Nikolas Patris</p></summary>
<p>

**Abstract:** Following the research agenda initiated by Munoz & Vassilvitskii [1] and Lykouris & Vassilvitskii [2] on learning-augmented online algorithms for classical online optimization problems, in this work, we consider the Online Facility Location problem under this framework. In Online Facility Location (OFL), demands arrive one-by-one in a metric space and must be (irrevocably) assigned to an open facility upon arrival, without any knowledge about future demands.
  We present an online algorithm for OFL that exploits potentially imperfect predictions on the locations of the optimal facilities. We prove that the competitive ratio decreases smoothly from sublogarithmic in the number of demands to constant, as the error, i.e., the total distance of the predicted locations to the optimal facility locations, decreases towards zero. We complement our analysis with a matching lower bound establishing that the dependence of the algorithm's competitive ratio on the error is optimal, up to constant factors. Finally, we evaluate our algorithm on real world data and compare our learning augmented approach with the current best online algorithm for the problem.

</p>
</details>

<details><summary><b>On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems</b>
<a href="https://arxiv.org/abs/2107.08225">arxiv:2107.08225</a>
&#x1F4C8; 6 <br>
<p>Michael Muehlebach, Michael I. Jordan</p></summary>
<p>

**Abstract:** We introduce a class of first-order methods for smooth constrained optimization that are based on an analogy to non-smooth dynamical systems. Two distinctive features of our approach are that (i) projections or optimizations over the entire feasible set are avoided, in stark contrast to projected gradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to become infeasible, which differs from active set or feasible direction methods, where the descent motion stops as soon as a new constraint is encountered. The resulting algorithmic procedure is simple to implement even when constraints are nonlinear, and is suitable for large-scale constrained optimization problems in which the feasible set fails to have a simple structure. The key underlying idea is that constraints are expressed in terms of velocities instead of positions, which has the algorithmic consequence that optimizations over feasible sets at each iteration are replaced with optimizations over local, sparse convex approximations. The result is a simplified suite of algorithms and an expanded range of possible applications in machine learning.

</p>
</details>

<details><summary><b>Sparse Bayesian Learning with Diagonal Quasi-Newton Method For Large Scale Classification</b>
<a href="https://arxiv.org/abs/2107.08195">arxiv:2107.08195</a>
&#x1F4C8; 6 <br>
<p>Jiahua Luo, Chi-Man Vong, Jie Du</p></summary>
<p>

**Abstract:** Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity O(M^3 ) (M: feature size) for updating the regularization priors, making it difficult for practical use. There are three issues in SBL: 1) Inverting the covariance matrix may obtain singular solutions in some cases, which hinders SBL from convergence; 2) Poor scalability to problems with high dimensional feature space or large data size; 3) SBL easily suffers from memory overflow for large-scale data. This paper addresses these issues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly evaluated on non-linear classifiers and linear feature selection using various benchmark datasets of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.

</p>
</details>

<details><summary><b>Towards autonomic orchestration of machine learning pipelines in future networks</b>
<a href="https://arxiv.org/abs/2107.08194">arxiv:2107.08194</a>
&#x1F4C8; 6 <br>
<p>Abhishek Dandekar</p></summary>
<p>

**Abstract:** Machine learning (ML) techniques are being increasingly used in mobile networks for network planning, operation, management, optimisation and much more. These techniques are realised using a set of logical nodes known as ML pipeline. A single network operator might have thousands of such ML pipelines distributed across its network. These pipelines need to be managed and orchestrated across network domains. Thus it is essential to have autonomic multi-domain orchestration of ML pipelines in mobile networks. International Telecommunications Union (ITU) has provided an architectural framework for management and orchestration of ML pipelines in future networks. We extend this framework to enable autonomic orchestration of ML pipelines across multiple network domains. We present our system architecture and describe its application using a smart factory use case. Our work allows autonomic orchestration of multi-domain ML pipelines in a standardised, technology agnostic, privacy preserving fashion.

</p>
</details>

<details><summary><b>DeHumor: Visual Analytics for Decomposing Humor</b>
<a href="https://arxiv.org/abs/2107.08356">arxiv:2107.08356</a>
&#x1F4C8; 3 <br>
<p>Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng Zeng, Yong Wang, Huamin Qu</p></summary>
<p>

**Abstract:** Despite being a critical communication skill, grasping humor is challenging -- a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.

</p>
</details>

<details><summary><b>Gait-learning with morphologically evolving robots generated by L-system</b>
<a href="https://arxiv.org/abs/2107.08249">arxiv:2107.08249</a>
&#x1F4C8; 3 <br>
<p>Jie Luo, Daan Zeeuwe, Agoston E. Eiben</p></summary>
<p>

**Abstract:** When controllers (brains) and morphologies (bodies) of robots simultaneously evolve, this can lead to a problem, namely the brain & body mismatch problem. In this research, we propose a solution of lifetime learning. We set up a system where modular robots can create offspring that inherit the bodies of parents by recombination and mutation. With regards to the brains of the offspring, we use two methods to create them. The first one entails solely evolution which means the brain of a robot child is inherited from its parents. The second approach is evolution plus learning which means the brain of a child is inherited as well, but additionally is developed by a learning algorithm - RevDEknn. We compare these two methods by running experiments in a simulator called Revolve and use efficiency, efficacy, and the morphology intelligence of the robots for the comparison. The experiments show that the evolution plus learning method does not only lead to a higher fitness level, but also to more morphologically evolving robots. This constitutes a quantitative demonstration that changes in the brain can induce changes in the body, leading to the concept of morphological intelligence, which is quantified by the learning delta, meaning the ability of a morphology to facilitate learning.

</p>
</details>

<details><summary><b>FEBR: Expert-Based Recommendation Framework for beneficial and personalized content</b>
<a href="https://arxiv.org/abs/2108.01455">arxiv:2108.01455</a>
&#x1F4C8; 2 <br>
<p>Mohamed Lechiakh, Alexandre Maurer</p></summary>
<p>

**Abstract:** So far, most research on recommender systems focused on maintaining long-term user engagement and satisfaction, by promoting relevant and personalized content. However, it is still very challenging to evaluate the quality and the reliability of this content. In this paper, we propose FEBR (Expert-Based Recommendation Framework), an apprenticeship learning framework to assess the quality of the recommended content on online platforms. The framework exploits the demonstrated trajectories of an expert (assumed to be reliable) in a recommendation evaluation environment, to recover an unknown utility function. This function is used to learn an optimal policy describing the expert's behavior, which is then used in the framework to provide high-quality and personalized recommendations. We evaluate the performance of our solution through a user interest simulation environment (using RecSim). We simulate interactions under the aforementioned expert policy for videos recommendation, and compare its efficiency with standard recommendation methods. The results show that our approach provides a significant gain in terms of content quality, evaluated by experts and watched by users, while maintaining almost the same watch time as the baseline approaches.

</p>
</details>

<details><summary><b>Federated Action Recognition on Heterogeneous Embedded Devices</b>
<a href="https://arxiv.org/abs/2107.12147">arxiv:2107.12147</a>
&#x1F4C8; 2 <br>
<p>Pranjal Jain, Shreyas Goenka, Saurabh Bagchi, Biplab Banerjee, Somali Chaterji</p></summary>
<p>

**Abstract:** Federated learning allows a large number of devices to jointly learn a model without sharing data. In this work, we enable clients with limited computing power to perform action recognition, a computationally heavy task. We first perform model compression at the central server through knowledge distillation on a large dataset. This allows the model to learn complex features and serves as an initialization for model fine-tuning. The fine-tuning is required because the limited data present in smaller datasets is not adequate for action recognition models to learn complex spatio-temporal features. Because the clients present are often heterogeneous in their computing resources, we use an asynchronous federated optimization and we further show a convergence bound. We compare our approach to two baseline approaches: fine-tuning at the central server (no clients) and fine-tuning using (heterogeneous) clients using synchronous federated averaging. We empirically show on a testbed of heterogeneous embedded devices that we can perform action recognition with comparable accuracy to the two baselines above, while our asynchronous learning strategy reduces the training time by 40%, relative to synchronous learning.

</p>
</details>

<details><summary><b>A Survey on Data-driven Software Vulnerability Assessment and Prioritization</b>
<a href="https://arxiv.org/abs/2107.08364">arxiv:2107.08364</a>
&#x1F4C8; 2 <br>
<p>Triet H. M. Le, Huaming Chen, M. Ali Babar</p></summary>
<p>

**Abstract:** Software Vulnerabilities (SVs) are increasing in complexity and scale, posing great security risks to many software systems. Given the limited resources in practice, SV assessment and prioritization help practitioners devise optimal SV mitigation plans based on various SV characteristics. The surge in SV data sources and data-driven techniques such as Machine Learning and Deep Learning have taken SV assessment and prioritization to the next level. Our survey provides a taxonomy of the past research efforts and highlights the best practices for data-driven SV assessment and prioritization. We also discuss the current limitations and propose potential solutions to address such issues.

</p>
</details>

<details><summary><b>Probabilistic Verification of Neural Networks Against Group Fairness</b>
<a href="https://arxiv.org/abs/2107.08362">arxiv:2107.08362</a>
&#x1F4C8; 2 <br>
<p>Bing Sun, Jun Sun, Ting Dai, Lijun Zhang</p></summary>
<p>

**Abstract:** Fairness is crucial for neural networks which are used in applications with important societal implication. Recently, there have been multiple attempts on improving fairness of neural networks, with a focus on fairness testing (e.g., generating individual discriminatory instances) and fairness training (e.g., enhancing fairness through augmented training). In this work, we propose an approach to formally verify neural networks against fairness, with a focus on independence-based fairness such as group fairness. Our method is built upon an approach for learning Markov Chains from a user-provided neural network (i.e., a feed-forward neural network or a recurrent neural network) which is guaranteed to facilitate sound analysis. The learned Markov Chain not only allows us to verify (with Probably Approximate Correctness guarantee) whether the neural network is fair or not, but also facilities sensitivity analysis which helps to understand why fairness is violated. We demonstrate that with our analysis results, the neural weights can be optimized to improve fairness. Our approach has been evaluated with multiple models trained on benchmark datasets and the experiment results show that our approach is effective and efficient.

</p>
</details>

<details><summary><b>Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses</b>
<a href="https://arxiv.org/abs/2107.08346">arxiv:2107.08346</a>
&#x1F4C8; 2 <br>
<p>Haipeng Luo, Chen-Yu Wei, Chung-Wei Lee</p></summary>
<p>

**Abstract:** Policy optimization is a widely-used method in reinforcement learning. Due to its local-search nature, however, theoretical guarantees on global optimality often rely on extra assumptions on the Markov Decision Processes (MDPs) that bypass the challenge of global exploration. To eliminate the need of such assumptions, in this work, we develop a general solution that adds dilated bonuses to the policy update to facilitate global exploration. To showcase the power and generality of this technique, we apply it to several episodic MDP settings with adversarial losses and bandit feedback, improving and generalizing the state-of-the-art. Specifically, in the tabular case, we obtain $\widetilde{\mathcal{O}}(\sqrt{T})$ regret where $T$ is the number of episodes, improving the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al. (2020). When the number of states is infinite, under the assumption that the state-action values are linear in some low-dimensional features, we obtain $\widetilde{\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator, matching the result of Neu and Olkhovskaya (2020) while importantly removing the need of an exploratory policy that their algorithm requires. When a simulator is unavailable, we further consider a linear MDP setting and obtain $\widetilde{\mathcal{O}}({T}^{14/15})$ regret, which is the first result for linear MDPs with adversarial losses and bandit feedback.

</p>
</details>

<details><summary><b>Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.08325">arxiv:2107.08325</a>
&#x1F4C8; 2 <br>
<p>Peide Cai, Hengli Wang, Huaiyang Huang, Yuxuan Liu, Ming Liu</p></summary>
<p>

**Abstract:** Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/

</p>
</details>

<details><summary><b>Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network</b>
<a href="https://arxiv.org/abs/2107.08355">arxiv:2107.08355</a>
&#x1F4C8; 1 <br>
<p>Liupeng Lin, Jie Li, Huanfeng Shen, Lingli Zhao, Qiangqiang Yuan, Xinghua Li</p></summary>
<p>

**Abstract:** The data fusion technology aims to aggregate the characteristics of different data and obtain products with multiple data advantages. To solves the problem of reduced resolution of PolSAR images due to system limitations, we propose a fully polarimetric synthetic aperture radar (PolSAR) images and single-polarization synthetic aperture radar SAR (SinSAR) images fusion network to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the polarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the spatial information of the high-resolution single-polarization SAR (HR-SinSAR) image, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR image and design a cross-attention mechanism to extract features from the joint input data. Besides, based on the physical imaging mechanism, we designed the PolSAR polarimetric loss function for constrained network training. The experimental results confirm the superiority of fusion network over traditional algorithms. The average PSNR is increased by more than 3.6db, and the average MAE is reduced to less than 0.07. Experiments on polarimetric decomposition and polarimetric signature show that it maintains polarimetric information well.

</p>
</details>

<details><summary><b>Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction</b>
<a href="https://arxiv.org/abs/2107.08330">arxiv:2107.08330</a>
&#x1F4C8; 1 <br>
<p>Aishik Konwer, Joseph Bae, Gagandeep Singh, Rishabh Gattu, Syed Ali, Jeremy Green, Tej Phatak, Prateek Prasanna</p></summary>
<p>

**Abstract:** COVID-19 image analysis has mostly focused on diagnostic tasks using single timepoint scans acquired upon disease presentation or admission. We present a deep learning-based approach to predict lung infiltrate progression from serial chest radiographs (CXRs) of COVID-19 patients. Our method first utilizes convolutional neural networks (CNNs) for feature extraction from patches within the concerned lung zone, and also from neighboring and remote boundary regions. The framework further incorporates a multi-scale Gated Recurrent Unit (GRU) with a correlation module for effective predictions. The GRU accepts CNN feature vectors from three different areas as input and generates a fused representation. The correlation module attempts to minimize the correlation loss between hidden representations of concerned and neighboring area feature vectors, while maximizing the loss between the same from concerned and remote regions. Further, we employ an attention module over the output hidden states of each encoder timepoint to generate a context vector. This vector is used as an input to a decoder module to predict patch severity grades at a future timepoint. Finally, we ensemble the patch classification scores to calculate patient-wise grades. Specifically, our framework predicts zone-wise disease severity for a patient on a given day by learning representations from the previous temporal CXRs. Our novel multi-institutional dataset comprises sequential CXR scans from N=93 patients. Our approach outperforms transfer learning and radiomic feature-based baseline approaches on this dataset.

</p>
</details>

<details><summary><b>Otimizacao de Redes Neurais atraves de Algoritmos Geneticos Celulares</b>
<a href="https://arxiv.org/abs/2107.08326">arxiv:2107.08326</a>
&#x1F4C8; 1 <br>
<p>Anderson da Silva, Teresa Ludermir</p></summary>
<p>

**Abstract:** This works proposes a methodology to searching for automatically Artificial Neural Networks (ANN) by using Cellular Genetic Algorithm (CGA). The goal of this methodology is to find compact networks whit good performance for classification problems. The main reason for developing this work is centered at the difficulties of configuring compact ANNs with good performance rating. The use of CGAs aims at seeking the components of the RNA in the same way that a common Genetic Algorithm (GA), but it has the differential of incorporating a Cellular Automaton (CA) to give location for the GA individuals. The location imposed by the CA aims to control the spread of solutions in the populations to maintain the genetic diversity for longer time. This genetic diversity is important for obtain good results with the GAs.

</p>
</details>

<details><summary><b>Tea: Program Repair Using Neural Network Based on Program Information Attention Matrix</b>
<a href="https://arxiv.org/abs/2107.08262">arxiv:2107.08262</a>
&#x1F4C8; 1 <br>
<p>Wenshuo Wang, Chen Wu, Liang Cheng, Yang Zhang</p></summary>
<p>

**Abstract:** The advance in machine learning (ML)-driven natural language process (NLP) points a promising direction for automatic bug fixing for software programs, as fixing a buggy program can be transformed to a translation task. While software programs contain much richer information than one-dimensional natural language documents, pioneering work on using ML-driven NLP techniques for automatic program repair only considered a limited set of such information. We hypothesize that more comprehensive information of software programs, if appropriately utilized, can improve the effectiveness of ML-driven NLP approaches in repairing software programs. As the first step towards proving this hypothesis, we propose a unified representation to capture the syntax, data flow, and control flow aspects of software programs, and devise a method to use such a representation to guide the transformer model from NLP in better understanding and fixing buggy programs. Our preliminary experiment confirms that the more comprehensive information of software programs used, the better ML-driven NLP techniques can perform in fixing bugs in these programs.

</p>
</details>

<details><summary><b>Top-label calibration and multiclass-to-binary reductions</b>
<a href="https://arxiv.org/abs/2107.08353">arxiv:2107.08353</a>
&#x1F4C8; 0 <br>
<p>Chirag Gupta, Aaditya K. Ramdas</p></summary>
<p>

**Abstract:** We investigate the relationship between commonly considered notions of multiclass calibration and the calibration algorithms used to achieve these notions, leading to two broad contributions. First, we propose a new and arguably natural notion of top-label calibration, which requires the reported probability of the most likely label to be calibrated. Along the way, we highlight certain philosophical issues with the closely related and popular notion of confidence calibration. Second, we outline general 'wrapper' multiclass-to-binary (M2B) algorithms that can be used to achieve confidence, top-label, and class-wise calibration, using underlying binary calibration routines. Our wrappers can also be generalized to other notions of calibration, if required for certain practical applications. We instantiate these wrappers with the binary histogram binning (HB) algorithm, and show that the overall procedure has distribution-free calibration guarantees. In an empirical evaluation, we find that with the right M2B wrapper, HB performs significantly better than other calibration approaches. Code for this work has been made publicly available at https://github.com/aigen/df-posthoc-calibration.

</p>
</details>


[Next Page]({{ '/2021/07/16/2021.07.16.html' | relative_url }})
