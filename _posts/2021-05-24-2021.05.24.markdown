## Summary for 2021-05-24, created on 2021-12-21


<details><summary><b>True Few-Shot Learning with Language Models</b>
<a href="https://arxiv.org/abs/2105.11447">arxiv:2105.11447</a>
&#x1F4C8; 40 <br>
<p>Ethan Perez, Douwe Kiela, Kyunghyun Cho</p></summary>
<p>

**Abstract:** Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates ("prompts"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.

</p>
</details>

<details><summary><b>VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator</b>
<a href="https://arxiv.org/abs/2105.11589">arxiv:2105.11589</a>
&#x1F4C8; 26 <br>
<p>Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson Piramuthu, Gokhan Tür, Devi Parikh, Dilek Hakkani-Tür</p></summary>
<p>

**Abstract:** Interactive robots navigating photo-realistic environments face challenges underlying vision-and-language navigation (VLN), but in addition, they need to be trained to handle the dynamic nature of dialogue. However, research in Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts with a guide in natural language in order to reach a goal, treats the dialogue history as a VLN-style static instruction. In this paper, we present VISITRON, a navigator better suited to the interactive regime inherent to CVDN by being trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON is competitive with models on the static CVDN leaderboard. We also propose a generalized interactive regime to fine-tune and evaluate VISITRON and future such models with pre-trained guides for adaptability.

</p>
</details>

<details><summary><b>Self-Attention Networks Can Process Bounded Hierarchical Languages</b>
<a href="https://arxiv.org/abs/2105.11115">arxiv:2105.11115</a>
&#x1F4C8; 23 <br>
<p>Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan</p></summary>
<p>

**Abstract:** Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested parentheses of $k$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process $\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by $D$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with $D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes $\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and $O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show that self-attention networks trained on $\mathsf{Dyck}_{k, D}$ generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.

</p>
</details>

<details><summary><b>Neural Machine Translation with Monolingual Translation Memory</b>
<a href="https://arxiv.org/abs/2105.11269">arxiv:2105.11269</a>
&#x1F4C8; 21 <br>
<p>Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu</p></summary>
<p>

**Abstract:** Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.

</p>
</details>

<details><summary><b>Mapping oil palm density at country scale: An active learning approach</b>
<a href="https://arxiv.org/abs/2105.11207">arxiv:2105.11207</a>
&#x1F4C8; 19 <br>
<p>Andrés C. Rodríguez, Stefano D'Aronco, Konrad Schindler, Jan D. Wegner</p></summary>
<p>

**Abstract:** Accurate mapping of oil palm is important for understanding its past and future impact on the environment. We propose to map and count oil palms by estimating tree densities per pixel for large-scale analysis. This allows for fine-grained analysis, for example regarding different planting patterns. To that end, we propose a new, active deep learning method to estimate oil palm density at large scale from Sentinel-2 satellite images, and apply it to generate complete maps for Malaysia and Indonesia. What makes the regression of oil palm density challenging is the need for representative reference data that covers all relevant geographical conditions across a large territory. Specifically for density estimation, generating reference data involves counting individual trees. To keep the associated labelling effort low we propose an active learning (AL) approach that automatically chooses the most relevant samples to be labelled. Our method relies on estimates of the epistemic model uncertainty and of the diversity among samples, making it possible to retrieve an entire batch of relevant samples in a single iteration. Moreover, our algorithm has linear computational complexity and is easily parallelisable to cover large areas. We use our method to compute the first oil palm density map with $10\,$m Ground Sampling Distance (GSD) , for all of Indonesia and Malaysia and for two different years, 2017 and 2019. The maps have a mean absolute error of $\pm$7.3 trees/$ha$, estimated from an independent validation set. We also analyse density variations between different states within a country and compare them to official estimates. According to our estimates there are, in total, $>1.2$ billion oil palms in Indonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia covering $>6$ million $ha$.

</p>
</details>

<details><summary><b>i-Pulse: A NLP based novel approach for employee engagement in logistics organization</b>
<a href="https://arxiv.org/abs/2106.07341">arxiv:2106.07341</a>
&#x1F4C8; 15 <br>
<p>Rachit Garg, Arvind W Kiwelekar, Laxman D Netak, Akshay Ghodake</p></summary>
<p>

**Abstract:** Although most logistics and freight forwarding organizations, in one way or another, claim to have core values. The engagement of employees is a vast structure that affects almost every part of the company's core environmental values. There is little theoretical knowledge about the relationship between firms and the engagement of employees. Based on research literature, this paper aims to provide a novel approach for insight around employee engagement in a logistics organization by implementing deep natural language processing concepts. The artificial intelligence-enabled solution named Intelligent Pulse (I-Pulse) can evaluate hundreds and thousands of pulse survey comments and provides the actionable insights and gist of employee feedback. I-Pulse allows the stakeholders to think in new ways in their organization, helping them to have a powerful influence on employee engagement, retention, and efficiency. This study is of corresponding interest to researchers and practitioners.

</p>
</details>

<details><summary><b>Learning Security Classifiers with Verified Global Robustness Properties</b>
<a href="https://arxiv.org/abs/2105.11363">arxiv:2105.11363</a>
&#x1F4C8; 12 <br>
<p>Yizheng Chen, Shiqi Wang, Yue Qin, Xiaojing Liao, Suman Jana, David Wagner</p></summary>
<p>

**Abstract:** Many recent works have proposed methods to train classifiers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classifier. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness.
  In this paper, we present a framework and tools for training classifiers that satisfy global robustness properties. We define new notions of global robustness that are more suitable for security classifiers. We design a novel booster-fixer training framework to enforce global robustness properties. We structure our classifier as an ensemble of logic rules and design a new verifier to verify the properties. In our training algorithm, the booster increases the classifier's capacity, and the fixer enforces verified global robustness properties following counterexample guided inductive synthesis.
  We show that we can train classifiers to satisfy different global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classifier's performance. For example, we train a Twitter spam account classifier to satisfy five global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn't satisfy any property.

</p>
</details>

<details><summary><b>Personalized Transformer for Explainable Recommendation</b>
<a href="https://arxiv.org/abs/2105.11601">arxiv:2105.11601</a>
&#x1F4C8; 10 <br>
<p>Lei Li, Yongfeng Zhang, Li Chen</p></summary>
<p>

**Abstract:** Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.

</p>
</details>

<details><summary><b>CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks</b>
<a href="https://arxiv.org/abs/2105.12655">arxiv:2105.12655</a>
&#x1F4C8; 9 <br>
<p>Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, Frederick Reiss</p></summary>
<p>

**Abstract:** Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of AI for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.

</p>
</details>

<details><summary><b>Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2105.11654">arxiv:2105.11654</a>
&#x1F4C8; 9 <br>
<p>Jianhao Ding, Zhaofei Yu, Yonghong Tian, Tiejun Huang</p></summary>
<p>

**Abstract:** Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural networks, have attracted great attentions from researchers and industry. The most efficient way to train deep SNNs is through ANN-SNN conversion. However, the conversion usually suffers from accuracy loss and long inference time, which impede the practical application of SNN. In this paper, we theoretically analyze ANN-SNN conversion and derive sufficient conditions of the optimal conversion. To better correlate ANN-SNN and get greater accuracy, we propose Rate Norm Layer to replace the ReLU activation function in source ANN training, enabling direct conversion from a trained ANN to an SNN. Moreover, we propose an optimal fit curve to quantify the fit between the activation value of source ANN and the actual firing rate of target SNN. We show that the inference time can be reduced by optimizing the upper bound of the fit curve in the revised ANN to achieve fast inference. Our theory can explain the existing work on fast reasoning and get better results. The experimental results show that the proposed method achieves near loss less conversion with VGG-16, PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster reasoning performance under 0.265x energy consumption of the typical method. The code is available at https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.

</p>
</details>

<details><summary><b>Neural Language Models for Nineteenth-Century English</b>
<a href="https://arxiv.org/abs/2105.11321">arxiv:2105.11321</a>
&#x1F4C8; 9 <br>
<p>Kasra Hosseini, Kaspar Beelen, Giovanni Colavizza, Mariona Coll Ardanuy</p></summary>
<p>

**Abstract:** We present four types of neural language models trained on a large historical dataset of books in English, published between 1760-1900 and comprised of ~5.1 billion tokens. The language model architectures include static (word2vec and fastText) and contextualized models (BERT and Flair). For each architecture, we trained a model instance using the whole dataset. Additionally, we trained separate instances on text published before 1850 for the two static models, and four instances considering different time slices for BERT. Our models have already been used in various downstream tasks where they consistently improved performance. In this paper, we describe how the models have been created and outline their reuse potential.

</p>
</details>

<details><summary><b>One2Set: Generating Diverse Keyphrases as a Set</b>
<a href="https://arxiv.org/abs/2105.11134">arxiv:2105.11134</a>
&#x1F4C8; 9 <br>
<p>Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, Qi Zhang</p></summary>
<p>

**Abstract:** Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a $K$-step target assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the duplication ratio of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.

</p>
</details>

<details><summary><b>Few-Shot Upsampling for Protest Size Detection</b>
<a href="https://arxiv.org/abs/2105.11260">arxiv:2105.11260</a>
&#x1F4C8; 8 <br>
<p>Andrew Halterman, Benjamin J. Radford</p></summary>
<p>

**Abstract:** We propose a new task and dataset for a common problem in social science research: "upsampling" coarse document labels to fine-grained labels or spans. We pose the problem in a question answering format, with the answers providing the fine-grained labels. We provide a benchmark dataset and baselines on a socially impactful task: identifying the exact crowd size at protests and demonstrations in the United States given only order-of-magnitude information about protest attendance, a very small sample of fine-grained examples, and English-language news text. We evaluate several baseline models, including zero-shot results from rule-based and question-answering models, few-shot models fine-tuned on a small set of documents, and weakly supervised models using a larger set of coarsely-labeled documents. We find that our rule-based model initially outperforms a zero-shot pre-trained transformer language model but that further fine-tuning on a very small subset of 25 examples substantially improves out-of-sample performance. We also demonstrate a method for fine-tuning the transformer span on only the coarse labels that performs similarly to our rule-based approach. This work will contribute to social scientists' ability to generate data to understand the causes and successes of collective action.

</p>
</details>

<details><summary><b>FILTRA: Rethinking Steerable CNN by Filter Transform</b>
<a href="https://arxiv.org/abs/2105.11636">arxiv:2105.11636</a>
&#x1F4C8; 7 <br>
<p>Bo Li, Qili Wang, Gim Hee Lee</p></summary>
<p>

**Abstract:** Steerable CNN imposes the prior knowledge of transformation invariance or equivariance in the network architecture to enhance the the network robustness on geometry transformation of data and reduce overfitting. It has been an intuitive and widely used technique to construct a steerable filter by augmenting a filter with its transformed copies in the past decades, which is named as filter transform in this paper. Recently, the problem of steerable CNN has been studied from aspect of group representation theory, which reveals the function space structure of a steerable kernel function. However, it is not yet clear on how this theory is related to the filter transform technique. In this paper, we show that kernel constructed by filter transform can also be interpreted in the group representation theory. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable convolution operators. Experiments are executed on multiple datasets to verify the feasibility of the proposed approach.

</p>
</details>

<details><summary><b>AirNet: Neural Network Transmission over the Air</b>
<a href="https://arxiv.org/abs/2105.11166">arxiv:2105.11166</a>
&#x1F4C8; 7 <br>
<p>Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk</p></summary>
<p>

**Abstract:** State-of-the-art performance for many emerging edge applications is achieved by deep neural networks (DNNs). Often, these DNNs are location and time sensitive, and the parameters of a specific DNN must be delivered from an edge server to the edge device rapidly and efficiently to carry out time-sensitive inference tasks. We introduce AirNet, a novel training and analog transmission method that allows efficient wireless delivery of DNNs. We first train the DNN with noise injection to counter the wireless channel noise. We also employ pruning to reduce the channel bandwidth necessary for transmission, and perform knowledge distillation from a larger model to achieve satisfactory performance, despite the channel perturbations. We show that AirNet achieves significantly higher test accuracy compared to digital alternatives under the same bandwidth and power constraints. It also exhibits graceful degradation with channel quality, which reduces the requirement for accurate channel estimation.

</p>
</details>

<details><summary><b>Prevent the Language Model from being Overconfident in Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2105.11098">arxiv:2105.11098</a>
&#x1F4C8; 7 <br>
<p>Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou</p></summary>
<p>

**Abstract:** The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.

</p>
</details>

<details><summary><b>AdaGCN:Adaptive Boosting Algorithm for Graph Convolutional Networks on Imbalanced Node Classification</b>
<a href="https://arxiv.org/abs/2105.11625">arxiv:2105.11625</a>
&#x1F4C8; 6 <br>
<p>S. Shi, Kai Qiao, Shuai Yang, L. Wang, J. Chen, Bin Yan</p></summary>
<p>

**Abstract:** The Graph Neural Network (GNN) has achieved remarkable success in graph data representation. However, the previous work only considered the ideal balanced dataset, and the practical imbalanced dataset was rarely considered, which, on the contrary, is of more significance for the application of GNN. Traditional methods such as resampling, reweighting and synthetic samples that deal with imbalanced datasets are no longer applicable in GNN. Ensemble models can handle imbalanced datasets better compared with single estimator. Besides, ensemble learning can achieve higher estimation accuracy and has better reliability compared with the single estimator. In this paper, we propose an ensemble model called AdaGCN, which uses a Graph Convolutional Network (GCN) as the base estimator during adaptive boosting. In AdaGCN, a higher weight will be set for the training samples that are not properly classified by the previous classifier, and transfer learning is used to reduce computational cost and increase fitting capability. Experiments show that the AdaGCN model we proposed achieves better performance than GCN, GraphSAGE, GAT, N-GCN and the most of advanced reweighting and resampling methods on synthetic imbalanced datasets, with an average improvement of 4.3%. Our model also improves state-of-the-art baselines on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and NELL.

</p>
</details>

<details><summary><b>Can we imitate stock price behavior to reinforcement learn option price?</b>
<a href="https://arxiv.org/abs/2105.11376">arxiv:2105.11376</a>
&#x1F4C8; 6 <br>
<p>Xin Jin</p></summary>
<p>

**Abstract:** This paper presents a framework of imitating the price behavior of the underlying stock for reinforcement learning option price. We use accessible features of the equities pricing data to construct a non-deterministic Markov decision process for modeling stock price behavior driven by principal investor's decision making. However, low signal-to-noise ratio and instability that appear immanent in equity markets pose challenges to determine the state transition (price change) after executing an action (principal investor's decision) as well as decide an action based on current state (spot price). In order to conquer these challenges, we resort to a Bayesian deep neural network for computing the predictive distribution of the state transition led by an action. Additionally, instead of exploring a state-action relationship to formulate a policy, we seek for an episode based visible-hidden state-action relationship to probabilistically imitate principal investor's successive decision making. Our algorithm then maps imitative principal investor's decisions to simulated stock price paths by a Bayesian deep neural network. Eventually the optimal option price is reinforcement learned through maximizing the cumulative risk-adjusted return of a dynamically hedged portfolio over simulated price paths of the underlying.

</p>
</details>

<details><summary><b>PROPm Allocations of Indivisible Goods to Multiple Agents</b>
<a href="https://arxiv.org/abs/2105.11348">arxiv:2105.11348</a>
&#x1F4C8; 6 <br>
<p>Artem Baklanov, Pranav Garimidi, Vasilis Gkatzelis, Daniel Schoepflin</p></summary>
<p>

**Abstract:** We study the classic problem of fairly allocating a set of indivisible goods among a group of agents, and focus on the notion of approximate proportionality known as PROPm. Prior work showed that there exists an allocation that satisfies this notion of fairness for instances involving up to five agents, but fell short of proving that this is true in general. We extend this result to show that a PROPm allocation is guaranteed to exist for all instances, independent of the number of agents or goods. Our proof is constructive, providing an algorithm that computes such an allocation and, unlike prior work, the running time of this algorithm is polynomial in both the number of agents and the number of goods.

</p>
</details>

<details><summary><b>Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes</b>
<a href="https://arxiv.org/abs/2105.11152">arxiv:2105.11152</a>
&#x1F4C8; 6 <br>
<p>Maya Okawa, Tomoharu Iwata, Yusuke Tanaka, Hiroyuki Toda, Takeshi Kurashima, Hisashi Kashima</p></summary>
<p>

**Abstract:** Sequences of events including infectious disease outbreaks, social network activities, and crimes are ubiquitous and the data on such events carry essential information about the underlying diffusion processes between communities (e.g., regions, online user groups). Modeling diffusion processes and predicting future events are crucial in many applications including epidemic control, viral marketing, and predictive policing. Hawkes processes offer a central tool for modeling the diffusion processes, in which the influence from the past events is described by the triggering kernel. However, the triggering kernel parameters, which govern how each community is influenced by the past events, are assumed to be static over time. In the real world, the diffusion processes depend not only on the influences from the past, but also the current (time-evolving) states of the communities, e.g., people's awareness of the disease and people's current interests. In this paper, we propose a novel Hawkes process model that is able to capture the underlying dynamics of community states behind the diffusion processes and predict the occurrences of events based on the dynamics. Specifically, we model the latent dynamic function that encodes these hidden dynamics by a mixture of neural networks. Then we design the triggering kernel using the latent dynamic function and its integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a flexible way to learn complex representations of the time-evolving communities' states, while at the same time it allows to computing the exact likelihood, which makes parameter learning tractable. Extensive experiments on four real-world event datasets show that DHP outperforms five widely adopted methods for event prediction.

</p>
</details>

<details><summary><b>Multi-Level Attentive Convoluntional Neural Network for Crowd Counting</b>
<a href="https://arxiv.org/abs/2105.11422">arxiv:2105.11422</a>
&#x1F4C8; 5 <br>
<p>Mengxiao Tian, Hao Guo, Chengjiang Long</p></summary>
<p>

**Abstract:** Recently the crowd counting has received more and more attention. Especially the technology of high-density environment has become an important research content, and the relevant methods for the existence of extremely dense crowd are not optimal. In this paper, we propose a multi-level attentive Convolutional Neural Network (MLAttnCNN) for crowd counting. We extract high-level contextual information with multiple different scales applied in pooling, and use multi-level attention modules to enrich the characteristics at different layers to achieve more efficient multi-scale feature fusion, which is able to be used to generate a more accurate density map with dilated convolutions and a $1\times 1$ convolution. The extensive experiments on three available public datasets show that our proposed network achieves outperformance to the state-of-the-art approaches.

</p>
</details>

<details><summary><b>View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data</b>
<a href="https://arxiv.org/abs/2105.11354">arxiv:2105.11354</a>
&#x1F4C8; 5 <br>
<p>Payam Karisani, Jinho D. Choi, Li Xiong</p></summary>
<p>

**Abstract:** We present an algorithm based on multi-layer transformers for identifying Adverse Drug Reactions (ADR) in social media data. Our model relies on the properties of the problem and the characteristics of contextual word embeddings to extract two views from documents. Then a classifier is trained on each view to label a set of unlabeled documents to be used as an initializer for a new classifier in the other view. Finally, the initialized classifier in each view is further trained using the initial training examples. We evaluated our model in the largest publicly available ADR dataset. The experiments testify that our model significantly outperforms the transformer-based models pretrained on domain-specific data.

</p>
</details>

<details><summary><b>Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning</b>
<a href="https://arxiv.org/abs/2105.11160">arxiv:2105.11160</a>
&#x1F4C8; 5 <br>
<p>Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush Varshney</p></summary>
<p>

**Abstract:** Recent advances in deep learning have led to breakthroughs in the development of automated skin disease classification. As we observe an increasing interest in these models in the dermatology space, it is crucial to address aspects such as the robustness towards input data distribution shifts. Current skin disease models could make incorrect inferences for test samples from different hardware devices and clinical settings or unknown disease samples, which are out-of-distribution (OOD) from the training samples. To this end, we propose a simple yet effective approach that detect these OOD samples prior to making any decision. The detection is performed via scanning in the latent space representation (e.g., activations of the inner layers of any pre-trained skin disease classifier). The input samples could also perturbed to maximise divergence of OOD samples. We validate our ODD detection approach in two use cases: 1) identify samples collected from different protocols, and 2) detect samples from unknown disease classes. Additionally, we evaluate the performance of the proposed approach and compare it with other state-of-the-art methods. Furthermore, data-driven dermatology applications may deepen the disparity in clinical care across racial and ethnic groups since most datasets are reported to suffer from bias in skin tone distribution. Therefore, we also evaluate the fairness of these OOD detection methods across different skin tones. Our experiments resulted in competitive performance across multiple datasets in detecting OOD samples, which could be used (in the future) to design more effective transfer learning techniques prior to inferring on these samples.

</p>
</details>

<details><summary><b>Continual Learning at the Edge: Real-Time Training on Smartphone Devices</b>
<a href="https://arxiv.org/abs/2105.13127">arxiv:2105.13127</a>
&#x1F4C8; 4 <br>
<p>Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Graffieti, Davide Maltoni</p></summary>
<p>

**Abstract:** On-device training for personalized learning is a challenging research problem. Being able to quickly adapt deep prediction models at the edge is necessary to better suit personal user needs. However, adaptation on the edge poses some questions on both the efficiency and sustainability of the learning process and on the ability to work under shifting data distributions. Indeed, naively fine-tuning a prediction model only on the newly available data results in catastrophic forgetting, a sudden erasure of previously acquired knowledge. In this paper, we detail the implementation and deployment of a hybrid continual learning strategy (AR1*) on a native Android application for real-time on-device personalization without forgetting. Our benchmark, based on an extension of the CORe50 dataset, shows the efficiency and effectiveness of our solution.

</p>
</details>

<details><summary><b>On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud</b>
<a href="https://arxiv.org/abs/2105.11649">arxiv:2105.11649</a>
&#x1F4C8; 4 <br>
<p>Bo Li</p></summary>
<p>

**Abstract:** Ground surface detection in point cloud is widely used as a key module in autonomous driving systems. Different from previous approaches which are mostly developed for lidars with high beam resolution, e.g. Velodyne HDL-64, this paper proposes ground detection techniques applicable to much sparser point cloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The approach is based on the RANSAC scheme of plane fitting. Inlier verification for plane hypotheses is enhanced by exploiting the point-wise tangent, which is a local feature available to compute regardless of the density of lidar beams. Ground surface which is not perfectly planar is fitted by multiple (specifically 4 in our implementation) disjoint plane regions. By assuming these plane regions to be rectanglar and exploiting the integral image technique, our approach approximately finds the optimal region partition and plane hypotheses under the RANSAC scheme with real-time computational complexity.

</p>
</details>

<details><summary><b>A Comparison of Reward Functions in Q-Learning Applied to a Cart Position Problem</b>
<a href="https://arxiv.org/abs/2105.11617">arxiv:2105.11617</a>
&#x1F4C8; 4 <br>
<p>Amartya Mukherjee</p></summary>
<p>

**Abstract:** Growing advancements in reinforcement learning has led to advancements in control theory. Reinforcement learning has effectively solved the inverted pendulum problem and more recently the double inverted pendulum problem. In reinforcement learning, our agents learn by interacting with the control system with the goal of maximizing rewards. In this paper, we explore three such reward functions in the cart position problem. This paper concludes that a discontinuous reward function that gives non-zero rewards to agents only if they are within a given distance from the desired position gives the best results.

</p>
</details>

<details><summary><b>Uncertainty quantification for distributed regression</b>
<a href="https://arxiv.org/abs/2105.11425">arxiv:2105.11425</a>
&#x1F4C8; 4 <br>
<p>Valeriy Avanesov</p></summary>
<p>

**Abstract:** The ever-growing size of the datasets renders well-studied learning techniques, such as Kernel Ridge Regression, inapplicable, posing a serious computational challenge. Divide-and-conquer is a common remedy, suggesting to split the dataset into disjoint partitions, obtain the local estimates and average them, it allows to scale-up an otherwise ineffective base approach. In the current study we suggest a fully data-driven approach to quantify uncertainty of the averaged estimator. Namely, we construct simultaneous element-wise confidence bands for the predictions yielded by the averaged estimator on a given deterministic prediction set. The novel approach features rigorous theoretical guaranties for a wide class of base learners with Kernel Ridge regression being a special case. As a by-product of our analysis we also obtain a sup-norm consistency result for the divide-and-conquer Kernel Ridge Regression. The simulation study supports the theoretical findings.

</p>
</details>

<details><summary><b>Autonomous Kinetic Modeling of Biomass Pyrolysis using Chemical Reaction Neural Networks</b>
<a href="https://arxiv.org/abs/2105.11397">arxiv:2105.11397</a>
&#x1F4C8; 4 <br>
<p>Weiqi Ji, Franz Richter, Michael J. Gollner, Sili Deng</p></summary>
<p>

**Abstract:** Modeling the burning processes of biomass such as wood, grass, and crops is crucial for the modeling and prediction of wildland and urban fire behavior. Despite its importance, the burning of solid fuels remains poorly understood, which can be partly attributed to the unknown chemical kinetics of most solid fuels. Most available kinetic models were built upon expert knowledge, which requires chemical insights and years of experience. This work presents a framework for autonomously discovering biomass pyrolysis kinetic models from thermogravimetric analyzer (TGA) experimental data using the recently developed chemical reaction neural networks (CRNN). The approach incorporated the CRNN model into the framework of neural ordinary differential equations to predict the residual mass in TGA data. In addition to the flexibility of neural-network-based models, the learned CRNN model is fully interpretable, by incorporating the fundamental physics laws, such as the law of mass action and Arrhenius law, into the neural network structure. The learned CRNN model can then be translated into the classical forms of biomass chemical kinetic models, which facilitates the extraction of chemical insights and the integration of the kinetic model into large-scale fire simulations. We demonstrated the effectiveness of the framework in predicting the pyrolysis and oxidation of cellulose. This successful demonstration opens the possibility of rapid and autonomous chemical kinetic modeling of solid fuels, such as wildfire fuels and industrial polymers.

</p>
</details>

<details><summary><b>Room Clearance with Feudal Hierarchical Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.11328">arxiv:2105.11328</a>
&#x1F4C8; 4 <br>
<p>Henry Charlesworth, Adrian Millea, Eddie Pottrill, Rich Riley</p></summary>
<p>

**Abstract:** Reinforcement learning (RL) is a general framework that allows systems to learn autonomously through trial-and-error interaction with their environment. In recent years combining RL with expressive, high-capacity neural network models has led to impressive performance in a diverse range of domains. However, dealing with the large state and action spaces often required for problems in the real world still remains a significant challenge. In this paper we introduce a new simulation environment, "Gambit", designed as a tool to build scenarios that can drive RL research in a direction useful for military analysis. Using this environment we focus on an abstracted and simplified room clearance scenario, where a team of blue agents have to make their way through a building and ensure that all rooms are cleared of (and remain clear) of enemy red agents. We implement a multi-agent version of feudal hierarchical RL that introduces a command hierarchy where a commander at the higher level sends orders to multiple agents at the lower level who simply have to learn to follow these orders. We find that breaking the task down in this way allows us to solve a number of non-trivial floorplans that require the coordination of multiple agents much more efficiently than the standard baseline RL algorithms we compare with. We then go on to explore how qualitatively different behaviour can emerge depending on what we prioritise in the agent's reward function (e.g. clearing the building quickly vs. prioritising rescuing civilians).

</p>
</details>

<details><summary><b>Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces</b>
<a href="https://arxiv.org/abs/2105.11283">arxiv:2105.11283</a>
&#x1F4C8; 4 <br>
<p>Eugene Valassakis, Norman Di Palo, Edward Johns</p></summary>
<p>

**Abstract:** In this paper, we study the problem of zero-shot sim-to-real when the task requires both highly precise control with sub-millimetre error tolerance, and wide task space generalisation. Our framework involves a coarse-to-fine controller, where trajectories begin with classical motion planning using ICP-based pose estimation, and transition to a learned end-to-end controller which maps images to actions and is trained in simulation with domain randomisation. In this way, we achieve precise control whilst also generalising the controller across wide task spaces, and keeping the robustness of vision-based, end-to-end control. Real-world experiments on a range of different tasks show that, by exploiting the best of both worlds, our framework significantly outperforms purely motion planning methods, and purely learning-based methods. Furthermore, we answer a range of questions on best practices for precise sim-to-real transfer, such as how different image sensor modalities and image feature representations perform.

</p>
</details>

<details><summary><b>Retrieval Enhanced Model for Commonsense Generation</b>
<a href="https://arxiv.org/abs/2105.11174">arxiv:2105.11174</a>
&#x1F4C8; 4 <br>
<p>Han Wang, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong, Yichong Xu, Michael Zeng</p></summary>
<p>

**Abstract:** Commonsense generation is a challenging task of generating a plausible sentence describing an everyday scenario using provided concepts. Its requirement of reasoning over commonsense knowledge and compositional generalization ability even puzzles strong pre-trained language generation models. We propose a novel framework using retrieval methods to enhance both the pre-training and fine-tuning for commonsense generation. We retrieve prototype sentence candidates by concept matching and use them as auxiliary input. For fine-tuning, we further boost its performance with a trainable sentence retriever. We demonstrate experimentally on the large-scale CommonGen benchmark that our approach achieves new state-of-the-art results.

</p>
</details>

<details><summary><b>Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads</b>
<a href="https://arxiv.org/abs/2105.11118">arxiv:2105.11118</a>
&#x1F4C8; 4 <br>
<p>John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing Harry Xu</p></summary>
<p>

**Abstract:** A graph neural network (GNN) enables deep learning on structured graph data. There are two major GNN training obstacles: 1) it relies on high-end servers with many GPUs which are expensive to purchase and maintain, and 2) limited memory on GPUs cannot scale to today's billion-edge graphs. This paper presents Dorylus: a distributed system for training GNNs. Uniquely, Dorylus can take advantage of serverless computing to increase scalability at a low cost.
  The key insight guiding our design is computation separation. Computation separation makes it possible to construct a deep, bounded-asynchronous pipeline where graph and tensor parallel tasks can fully overlap, effectively hiding the network latency incurred by Lambdas. With the help of thousands of Lambda threads, Dorylus scales GNN training to billion-edge graphs. Currently, for large graphs, CPU servers offer the best performance-per-dollar over GPU servers. Just using Lambdas on top of CPU servers offers up to 2.75x more performance-per-dollar than training only with CPU servers. Concretely, Dorylus is 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs. Dorylus is up to 3.8x faster and 10.7x cheaper compared to existing sampling-based systems.

</p>
</details>

<details><summary><b>Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps</b>
<a href="https://arxiv.org/abs/2105.11095">arxiv:2105.11095</a>
&#x1F4C8; 4 <br>
<p>Maedeh Jamali, Nader Karim, Pejman Khadivi, Shahram Shirani, Shadrokh Samavi</p></summary>
<p>

**Abstract:** Digital contents have grown dramatically in recent years, leading to increased attention to copyright. Image watermarking has been considered one of the most popular methods for copyright protection. With the recent advancements in applying deep neural networks in image processing, these networks have also been used in image watermarking. Robustness and imperceptibility are two challenging features of watermarking methods that the trade-off between them should be satisfied. In this paper, we propose to use an end-to-end network for watermarking. We use a convolutional neural network (CNN) to control the embedding strength based on the image content. Dynamic embedding helps the network to have the lowest effect on the visual quality of the watermarked image. Different image processing attacks are simulated as a network layer to improve the robustness of the model. Our method is a blind watermarking approach that replicates the watermark string to create a matrix of the same size as the input image. Instead of diffusing the watermark data into the input image, we inject the data into the feature space and force the network to do this in regions that increase the robustness against various attacks. Experimental results show the superiority of the proposed method in terms of imperceptibility and robustness compared to the state-of-the-art algorithms.

</p>
</details>

<details><summary><b>Least-Squares ReLU Neural Network (LSNN) Method For Linear Advection-Reaction Equation</b>
<a href="https://arxiv.org/abs/2105.11632">arxiv:2105.11632</a>
&#x1F4C8; 3 <br>
<p>Zhiqiang Cai, Jingshuang Chen, Min Liu</p></summary>
<p>

**Abstract:** This paper studies least-squares ReLU neural network method for solving the linear advection-reaction problem with discontinuous solution. The method is a discretization of an equivalent least-squares formulation in the set of neural network functions with the ReLU activation function. The method is capable of approximating the discontinuous interface of the underlying problem automatically through the free hyper-planes of the ReLU neural network and, hence, outperforms mesh-based numerical methods in terms of the number of degrees of freedom. Numerical results of some benchmark test problems show that the method can not only approximate the solution with the least number of parameters, but also avoid the common Gibbs phenomena along the discontinuous interface. Moreover, a three-layer ReLU neural network is necessary and sufficient in order to well approximate a discontinuous solution with an interface in $\mathbb{R}^2$ that is not a straight line.

</p>
</details>

<details><summary><b>Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law</b>
<a href="https://arxiv.org/abs/2105.11627">arxiv:2105.11627</a>
&#x1F4C8; 3 <br>
<p>Zhiqiang Cai, Jingshuang Chen, Min Liu</p></summary>
<p>

**Abstract:** We introduced the least-squares ReLU neural network (LSNN) method for solving the linear advection-reaction problem with discontinuous solution and showed that the method outperforms mesh-based numerical methods in terms of the number of degrees of freedom. This paper studies the LSNN method for scalar nonlinear hyperbolic conservation law. The method is a discretization of an equivalent least-squares (LS) formulation in the set of neural network functions with the ReLU activation function. Evaluation of the LS functional is done by using numerical integration and conservative finite volume scheme. Numerical results of some test problems show that the method is capable of approximating the discontinuous interface of the underlying problem automatically through the free breaking lines of the ReLU neural network. Moreover, the method does not exhibit the common Gibbs phenomena along the discontinuous interface.

</p>
</details>

<details><summary><b>KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.11611">arxiv:2105.11611</a>
&#x1F4C8; 3 <br>
<p>Zijian Gao, Kele Xu, Bo Ding, Huaimin Wang, Yiying Li, Hongda Jia</p></summary>
<p>

**Abstract:** Recently, deep reinforcement learning (RL) algorithms have made great progress in multi-agent domain. However, due to characteristics of RL, training for complex tasks would be resource-intensive and time-consuming. To meet this challenge, mutual learning strategy between homogeneous agents is essential, which is under-explored in previous studies, because most existing methods do not consider to use the knowledge of agent models. In this paper, we present an adaptation method of the majority of multi-agent reinforcement learning (MARL) algorithms called KnowSR which takes advantage of the differences in learning between agents. We employ the idea of knowledge distillation (KD) to share knowledge among agents to shorten the training phase. To empirically demonstrate the robustness and effectiveness of KnowSR, we performed extensive experiments on state-of-the-art MARL algorithms in collaborative and competitive scenarios. The results demonstrate that KnowSR outperforms recently reported methodologies, emphasizing the importance of the proposed knowledge sharing for MARL.

</p>
</details>

<details><summary><b>IGO-QNN: Quantum Neural Network Architecture for Inductive Grover Oracularization</b>
<a href="https://arxiv.org/abs/2105.11603">arxiv:2105.11603</a>
&#x1F4C8; 3 <br>
<p>Areeq I. Hasan</p></summary>
<p>

**Abstract:** We propose a novel paradigm of integration of Grover's algorithm in a machine learning framework: the inductive Grover oracular quantum neural network (IGO-QNN). The model defines a variational quantum circuit with hidden layers of parameterized quantum neurons densely connected via entangle synapses to encode a dynamic Grover's search oracle that can be trained from a set of database-hit training examples. This widens the range of problem applications of Grover's unstructured search algorithm to include the vast majority of problems lacking analytic descriptions of solution verifiers, allowing for quadratic speed-up in unstructured search for the set of search problems with relationships between input and output spaces that are tractably underivable deductively. This generalization of Grover's oracularization may prove particularly effective in deep reinforcement learning, computer vision, and, more generally, as a feature vector classifier at the top of an existing model.

</p>
</details>

<details><summary><b>HIN-RNN: A Graph Representation Learning Neural Network for Fraudster Group Detection With No Handcrafted Features</b>
<a href="https://arxiv.org/abs/2105.11602">arxiv:2105.11602</a>
&#x1F4C8; 3 <br>
<p>Saeedreza Shehnepoor, Roberto Togneri, Wei Liu, Mohammed Bennamoun</p></summary>
<p>

**Abstract:** Social reviews are indispensable resources for modern consumers' decision making. For financial gain, companies pay fraudsters preferably in groups to demote or promote products and services since consumers are more likely to be misled by a large number of similar reviews from groups. Recent approaches on fraudster group detection employed handcrafted features of group behaviors without considering the semantic relation between reviews from the reviewers in a group. In this paper, we propose the first neural approach, HIN-RNN, a Heterogeneous Information Network (HIN) Compatible RNN for fraudster group detection that requires no handcrafted features. HIN-RNN provides a unifying architecture for representation learning of each reviewer, with the initial vector as the sum of word embeddings of all review text written by the same reviewer, concatenated by the ratio of negative reviews. Given a co-review network representing reviewers who have reviewed the same items with the same ratings and the reviewers' vector representation, a collaboration matrix is acquired through HIN-RNN training. The proposed approach is confirmed to be effective with marked improvement over state-of-the-art approaches on both the Yelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4% and 2% in terms of recall and F1-value, respectively) datasets.

</p>
</details>

<details><summary><b>Feature Space Exploration For Planning Initial Benthic AUV Surveys</b>
<a href="https://arxiv.org/abs/2105.11598">arxiv:2105.11598</a>
&#x1F4C8; 3 <br>
<p>Jackson Shields, Oscar Pizarro, Stefan B. Williams</p></summary>
<p>

**Abstract:** Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for benthic (seafloor) surveys, where the vehicle collects optical imagery of near the seafloor. Due to the small-sensor footprint of the cameras and the vast areas to be surveyed, these AUVs can not feasibly full coverage of areas larger than a few tens of thousands of square meters. Therefore AUV paths which sample sparsely, yet effectively, the survey areas are necessary. Broad scale acoustic bathymetric data is ready available over large areas, and often is a useful prior of seafloor cover. As such, prior bathymetry can be used to guide AUV data collection. This research proposes methods for planning initial AUV surveys that efficiently explore a feature space representation of the bathymetry, in order to sample from a diverse set of bathymetric terrain. This will enable the AUV to visit areas that likely contain unique habitats and are representative of the entire survey site. The suitability of these methods to plan AUV surveys is evaluated based on the coverage of the feature space and also the ability to visit all classes of benthic habitat on the initial dive. This is a valuable tool for AUV surveys as it increases the utility of initial dives. It also delivers a comprehensive training set to learn a relationship between acoustic bathymetry and visually-derived seafloor classifications.

</p>
</details>

<details><summary><b>A Quantum Hopfield Associative Memory Implemented on an Actual Quantum Processor</b>
<a href="https://arxiv.org/abs/2105.11590">arxiv:2105.11590</a>
&#x1F4C8; 3 <br>
<p>Nathan Eli Miller, Saibal Mukhopadhyay</p></summary>
<p>

**Abstract:** In this work, we present a Quantum Hopfield Associative Memory (QHAM) and demonstrate its capabilities in simulation and hardware using IBM Quantum Experience. The QHAM is based on a quantum neuron design which can be utilized for many different machine learning applications and can be implemented on real quantum hardware without requiring mid-circuit measurement or reset operations. We analyze the accuracy of the neuron and the full QHAM considering hardware errors via simulation with hardware noise models as well as with implementation on the 15-qubit ibmq_16_melbourne device. The quantum neuron and the QHAM are shown to be resilient to noise and require low qubit overhead and gate complexity. We benchmark the QHAM by testing its effective memory capacity and demonstrate its capabilities in the NISQ-era of quantum hardware. This demonstration of the first functional QHAM to be implemented in NISQ-era quantum hardware is a significant step in machine learning at the leading edge of quantum computing.

</p>
</details>

<details><summary><b>TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text</b>
<a href="https://arxiv.org/abs/2105.11559">arxiv:2105.11559</a>
&#x1F4C8; 3 <br>
<p>Taylor Archibald, Mason Poggemann, Aaron Chan, Tony Martinez</p></summary>
<p>

**Abstract:** Stroke order and velocity are helpful features in the fields of signature verification, handwriting recognition, and handwriting synthesis. Recovering these features from offline handwritten text is a challenging and well-studied problem. We propose a new model called TRACE (Trajectory Recovery by an Adaptively-trained Convolutional Encoder). TRACE is a differentiable approach that uses a convolutional recurrent neural network (CRNN) to infer temporal stroke information from long lines of offline handwritten text with many characters and dynamic time warping (DTW) to align predictions and ground truth points. TRACE is perhaps the first system to be trained end-to-end on entire lines of text of arbitrary width and does not require the use of dynamic exemplars. Moreover, the system does not require images to undergo any pre-processing, nor do the predictions require any post-processing. Consequently, the recovered trajectory is differentiable and can be used as a loss function for other tasks, including synthesizing offline handwritten text.
  We demonstrate that temporal stroke information recovered by TRACE from offline data can be used for handwriting synthesis and establish the first benchmarks for a stroke trajectory recovery system trained on the IAM online handwriting dataset.

</p>
</details>

<details><summary><b>Deep Learning-based Damage Mapping with InSAR Coherence Time Series</b>
<a href="https://arxiv.org/abs/2105.11544">arxiv:2105.11544</a>
&#x1F4C8; 3 <br>
<p>Oliver L. Stephenson, Tobias Köhne, Eric Zhan, Brent E. Cahill, Sang-Ho Yun, Zachary E. Ross, Mark Simons</p></summary>
<p>

**Abstract:** Satellite remote sensing is playing an increasing role in the rapid mapping of damage after natural disasters. In particular, synthetic aperture radar (SAR) can image the Earth's surface and map damage in all weather conditions, day and night. However, current SAR damage mapping methods struggle to separate damage from other changes in the Earth's surface. In this study, we propose a novel approach to damage mapping, combining deep learning with the full time history of SAR observations of an impacted region in order to detect anomalous variations in the Earth's surface properties due to a natural disaster. We quantify Earth surface change using time series of Interferometric SAR coherence, then use a recurrent neural network (RNN) as a probabilistic anomaly detector on these coherence time series. The RNN is first trained on pre-event coherence time series, and then forecasts a probability distribution of the coherence between pre- and post-event SAR images. The difference between the forecast and observed co-event coherence provides a measure of the confidence in the identification of damage. The method allows the user to choose a damage detection threshold that is customized for each location, based on the local behavior of coherence through time before the event. We apply this method to calculate estimates of damage for three earthquakes using multi-year time series of Sentinel-1 SAR acquisitions. Our approach shows good agreement with observed damage and quantitative improvement compared to using pre- to co-event coherence loss as a damage proxy.

</p>
</details>

<details><summary><b>Hater-O-Genius Aggression Classification using Capsule Networks</b>
<a href="https://arxiv.org/abs/2105.11219">arxiv:2105.11219</a>
&#x1F4C8; 3 <br>
<p>Parth Patwa, Srinivas PYKL, Amitava Das, Prerana Mukherjee, Viswanath Pulabaigari</p></summary>
<p>

**Abstract:** Contending hate speech in social media is one of the most challenging social problems of our time. There are various types of anti-social behavior in social media. Foremost of them is aggressive behavior, which is causing many social issues such as affecting the social lives and mental health of social media users. In this paper, we propose an end-to-end ensemble-based architecture to automatically identify and classify aggressive tweets. Tweets are classified into three categories - Covertly Aggressive, Overtly Aggressive, and Non-Aggressive. The proposed architecture is an ensemble of smaller subnetworks that are able to characterize the feature embeddings effectively. We demonstrate qualitatively that each of the smaller subnetworks is able to learn unique features. Our best model is an ensemble of Capsule Networks and results in a 65.2% F1 score on the Facebook test set, which results in a performance gain of 0.95% over the TRAC-2018 winners. The code and the model weights are publicly available at https://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.

</p>
</details>

<details><summary><b>Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients</b>
<a href="https://arxiv.org/abs/2105.11187">arxiv:2105.11187</a>
&#x1F4C8; 3 <br>
<p>Chairi Kiourt, Georgios Feretzakis, Konstantinos Dalamarinis, Dimitris Kalles, Georgios Pantos, Ioannis Papadopoulos, Spyros Kouris, George Ioannakis, Evangelos Loupelis, Petros Antonopoulos, Aikaterini Sakagianni</p></summary>
<p>

**Abstract:** The main objective of this work is to utilize state-of-the-art deep learning approaches for the identification of pulmonary embolism in CTPA-Scans for COVID-19 patients, provide an initial assessment of their performance and, ultimately, provide a fast-track prototype solution (system). We adopted and assessed some of the most popular convolutional neural network architectures through transfer learning approaches, to strive to combine good model accuracy with fast training. Additionally, we exploited one of the most popular one-stage object detection models for the localization (through object detection) of the pulmonary embolism regions-of-interests. The models of both approaches are trained on an original CTPA-Scan dataset, where we annotated of 673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary embolism regions-of-interests. We provide a brief assessment of some state-of-the-art image classification models by achieving validation accuracies of 91% in pulmonary embolism classification. Additionally, we achieved a precision of about 68% on average in the object detection model for the pulmonary embolism localization under 50% IoU threshold. For both approaches, we provide the entire training pipelines for future studies (step by step processes through source code). In this study, we present some of the most accurate and fast deep learning models for pulmonary embolism identification in CTPA-Scans images, through classification and localization (object detection) approaches for patients infected by COVID-19. We provide a fast-track solution (system) for the research community of the area, which combines both classification and object detection models for improving the precision of identifying pulmonary embolisms.

</p>
</details>

<details><summary><b>Harmless but Useful: Beyond Separable Equality Constraints in Datalog+/-</b>
<a href="https://arxiv.org/abs/2105.11147">arxiv:2105.11147</a>
&#x1F4C8; 3 <br>
<p>Luigi Bellomarini, Emanuel Sallinger</p></summary>
<p>

**Abstract:** Ontological query answering is the problem of answering queries in the presence of schema constraints representing the domain of interest. Datalog+/- is a common family of languages for schema constraints, including tuple-generating dependencies (TGDs) and equality-generating dependencies (EGDs). The interplay of TGDs and EGDs leads to undecidability or intractability of query answering when adding EGDs to tractable Datalog+/- fragments, like Warded Datalog+/-, for which, in the sole presence of TGDs, query answering is PTIME in data complexity. There have been attempts to limit the interaction of TGDs and EGDs and guarantee tractability, in particular with the introduction of separable EGDs, to make EGDs irrelevant for query answering as long as the set of constraints is satisfied. While being tractable, separable EGDs have limited expressive power.
  We propose a more general class of EGDs, which we call "harmless", that subsume separable EGDs and allow to model a much broader class of problems. Unlike separable EGDs, harmless EGDs, besides enforcing ground equality constraints, specialize the query answer by grounding or renaming the labelled nulls introduced by existential quantification in the TGDs. Harmless EGDs capture the cases when the answer obtained in the presence of EGDs is less general than the one obtained with TGDs only. We conclude that the theoretical problem of deciding whether a set of constraints contains harmless EGDs is undecidable. We contribute a sufficient syntactic condition characterizing harmless EGDs, broad and useful in practice. We focus on Warded Datalog+/- with harmless EGDs and argue that, in such fragment, query answering is decidable and PTIME in data complexity. We study chase-based techniques for query answering in Warded Datalog+/- with harmless EGDs, conducive to an efficient algorithm to be implemented in state-of-the-art reasoners.

</p>
</details>

<details><summary><b>Robust learning with anytime-guaranteed feedback</b>
<a href="https://arxiv.org/abs/2105.11135">arxiv:2105.11135</a>
&#x1F4C8; 3 <br>
<p>Matthew J. Holland</p></summary>
<p>

**Abstract:** Under data distributions which may be heavy-tailed, many stochastic gradient-based learning algorithms are driven by feedback queried at points with almost no performance guarantees on their own. Here we explore a modified "anytime online-to-batch" mechanism which for smooth objectives admits high-probability error bounds while requiring only lower-order moment bounds on the stochastic gradients. Using this conversion, we can derive a wide variety of "anytime robust" procedures, for which the task of performance analysis can be effectively reduced to regret control, meaning that existing regret bounds (for the bounded gradient case) can be robustified and leveraged in a straightforward manner. As a direct takeaway, we obtain an easily implemented stochastic gradient-based algorithm for which all queried points formally enjoy sub-Gaussian error bounds, and in practice show noteworthy gains on real-world data applications.

</p>
</details>

<details><summary><b>Aristotle Said "Happiness is a State of Activity" -- Predicting Mood through Body Sensing with Smartwatches</b>
<a href="https://arxiv.org/abs/2105.15029">arxiv:2105.15029</a>
&#x1F4C8; 2 <br>
<p>P. A. Gloor, A. Fronzetti Colladon, F. Grippa, P. Budner, J. Eirich</p></summary>
<p>

**Abstract:** We measure and predict states of Activation and Happiness using a body sensing application connected to smartwatches. Through the sensors of commercially available smartwatches we collect individual mood states and correlate them with body sensing data such as acceleration, heart rate, light level data, and location, through the GPS sensor built into the smartphone connected to the smartwatch. We polled users on the smartwatch for seven weeks four times per day asking for their mood state. We found that both Happiness and Activation are negatively correlated with heart beats and with the levels of light. People tend to be happier when they are moving more intensely and are feeling less activated during weekends. We also found that people with a lower Conscientiousness and Neuroticism and higher Agreeableness tend to be happy more frequently. In addition, more Activation can be predicted by lower Openness to experience and higher Agreeableness and Conscientiousness. Lastly, we find that tracking people's geographical coordinates might play an important role in predicting Happiness and Activation. The methodology we propose is a first step towards building an automated mood tracking system, to be used for better teamwork and in combination with social network analysis studies.

</p>
</details>

<details><summary><b>An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN Inference on Mobile Devices</b>
<a href="https://arxiv.org/abs/2105.13904">arxiv:2105.13904</a>
&#x1F4C8; 2 <br>
<p>Mohammed Elbtity, Abhishek Singh, Brendan Reidy, Xiaochen Guo, Ramtin Zand</p></summary>
<p>

**Abstract:** In this paper, we develop an in-memory analog computing (IMAC) architecture realizing both synaptic behavior and activation functions within non-volatile memory arrays. Spin-orbit torque magnetoresistive random-access memory (SOT-MRAM) devices are leveraged to realize sigmoidal neurons as well as binarized synapses. First, it is shown the proposed IMAC architecture can be utilized to realize a multilayer perceptron (MLP) classifier achieving orders of magnitude performance improvement compared to previous mixed-signal and digital implementations. Next, a heterogeneous mixed-signal and mixed-precision CPU-IMAC architecture is proposed for convolutional neural networks (CNNs) inference on mobile processors, in which IMAC is designed as a co-processor to realize fully-connected (FC) layers whereas convolution layers are executed in CPU. Architecture-level analytical models are developed to evaluate the performance and energy consumption of the CPU-IMAC architecture. Simulation results exhibit 6.5% and 10% energy savings for CPU-IMAC based realizations of LeNet and VGG CNN models, for MNIST and CIFAR-10 pattern recognition tasks, respectively.

</p>
</details>

<details><summary><b>Exploiting Multi-modal Contextual Sensing for City-bus's Stay Location Characterization: Towards Sub-60 Seconds Accurate Arrival Time Prediction</b>
<a href="https://arxiv.org/abs/2105.13131">arxiv:2105.13131</a>
&#x1F4C8; 2 <br>
<p>Ratna Mandal, Prasenjit Karmakar, Soumyajit Chatterjee, Debaleen Das Spandan, Shouvit Pradhan, Sujoy Saha, Sandip Chakraborty, Subrata Nandi</p></summary>
<p>

**Abstract:** Intelligent city transportation systems are one of the core infrastructures of a smart city. The true ingenuity of such an infrastructure lies in providing the commuters with real-time information about citywide transports like public buses, allowing her to pre-plan the travel. However, providing prior information for transportation systems like public buses in real-time is inherently challenging because of the diverse nature of different stay-locations that a public bus stops. Although straightforward factors stay duration, extracted from unimodal sources like GPS, at these locations look erratic, a thorough analysis of public bus GPS trails for 720km of bus travels at the city of Durgapur, a semi-urban city in India, reveals that several other fine-grained contextual features can characterize these locations accurately. Accordingly, we develop BuStop, a system for extracting and characterizing the stay locations from multi-modal sensing using commuters' smartphones. Using this multi-modal information BuStop extracts a set of granular contextual features that allow the system to differentiate among the different stay-location types. A thorough analysis of BuStop using the collected dataset indicates that the system works with high accuracy in identifying different stay locations like regular bus stops, random ad-hoc stops, stops due to traffic congestion stops at traffic signals, and stops at sharp turns. Additionally, we also develop a proof-of-concept setup on top of BuStop to analyze the potential of the framework in predicting expected arrival time, a critical piece of information required to pre-plan travel, at any given bus stop. Subsequent analysis of the PoC framework, through simulation over the test dataset, shows that characterizing the stay-locations indeed helps make more accurate arrival time predictions with deviations less than 60s from the ground-truth arrival time.

</p>
</details>

<details><summary><b>Pan-sharpening via High-pass Modification Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2105.11576">arxiv:2105.11576</a>
&#x1F4C8; 2 <br>
<p>Jiaming Wang, Zhenfeng Shao, Xiao Huang, Tao Lu, Ruiqian Zhang, Jiayi Ma</p></summary>
<p>

**Abstract:** Most existing deep learning-based pan-sharpening methods have several widely recognized issues, such as spectral distortion and insufficient spatial texture enhancement, we propose a novel pan-sharpening convolutional neural network based on a high-pass modification block. Different from existing methods, the proposed block is designed to learn the high-pass information, leading to enhance spatial information in each band of the multi-spectral-resolution images. To facilitate the generation of visually appealing pan-sharpened images, we propose a perceptual loss function and further optimize the model based on high-level features in the near-infrared space. Experiments demonstrate the superior performance of the proposed method compared to the state-of-the-art pan-sharpening methods, both quantitatively and qualitatively. The proposed model is open-sourced at https://github.com/jiaming-wang/HMB.

</p>
</details>

<details><summary><b>Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems</b>
<a href="https://arxiv.org/abs/2105.11558">arxiv:2105.11558</a>
&#x1F4C8; 2 <br>
<p>Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, Praneeth Netrapalli</p></summary>
<p>

**Abstract:** We consider the setting of vector valued non-linear dynamical systems $X_{t+1} = φ(A^* X_t) + η_t$, where $η_t$ is unbiased noise and $φ: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain {\em expansivity property}. The goal is to learn $A^*$ from a single trajectory $X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is well-studied in the linear case, where $φ$ is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU link function -- a non-expansive but easy to learn link function with i.i.d. samples -- any method would require exponentially many samples (with respect to dimension of $X_t$) from the dynamical system. We validate our results via. simulations and demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized methods designed for the dependency structure in data can significantly outperform standard SGD based methods.

</p>
</details>

<details><summary><b>Deep Descriptive Clustering</b>
<a href="https://arxiv.org/abs/2105.11549">arxiv:2105.11549</a>
&#x1F4C8; 2 <br>
<p>Hongjing Zhang, Ian Davidson</p></summary>
<p>

**Abstract:** Recent work on explainable clustering allows describing clusters when the features are interpretable. However, much modern machine learning focuses on complex data such as images, text, and graphs where deep learning is used but the raw features of data are not interpretable. This paper explores a novel setting for performing clustering on complex data while simultaneously generating explanations using interpretable tags. We propose deep descriptive clustering that performs sub-symbolic representation learning on complex data while generating explanations based on symbolic data. We form good clusters by maximizing the mutual information between empirical distribution on the inputs and the induced clustering labels for clustering objectives. We generate explanations by solving an integer linear programming that generates concise and orthogonal descriptions for each cluster. Finally, we allow the explanation to inform better clustering by proposing a novel pairwise loss with self-generated constraints to maximize the clustering and explanation module's consistency. Experimental results on public data demonstrate that our model outperforms competitive baselines in clustering performance while offering high-quality cluster-level explanations.

</p>
</details>

<details><summary><b>Scalable Cross Validation Losses for Gaussian Process Models</b>
<a href="https://arxiv.org/abs/2105.11535">arxiv:2105.11535</a>
&#x1F4C8; 2 <br>
<p>Martin Jankowiak, Geoff Pleiss</p></summary>
<p>

**Abstract:** We introduce a simple and scalable method for training Gaussian process (GP) models that exploits cross-validation and nearest neighbor truncation. To accommodate binary and multi-class classification we leverage Pòlya-Gamma auxiliary variables and variational inference. In an extensive empirical comparison with a number of alternative methods for scalable GP regression and classification, we find that our method offers fast training and excellent predictive performance. We argue that the good predictive performance can be traced to the non-parametric nature of the resulting predictive distributions as well as to the cross-validation loss, which provides robustness against model mis-specification.

</p>
</details>

<details><summary><b>Informative Bayesian model selection for RR Lyrae star classifiers</b>
<a href="https://arxiv.org/abs/2105.11531">arxiv:2105.11531</a>
&#x1F4C8; 2 <br>
<p>F. Pérez-Galarce, K. Pichara, P. Huijse, M. Catelan, D. Mery</p></summary>
<p>

**Abstract:** Machine learning has achieved an important role in the automatic classification of variable stars, and several classifiers have been proposed over the last decade. These classifiers have achieved impressive performance in several astronomical catalogues. However, some scientific articles have also shown that the training data therein contain multiple sources of bias. Hence, the performance of those classifiers on objects not belonging to the training data is uncertain, potentially resulting in the selection of incorrect models. Besides, it gives rise to the deployment of misleading classifiers. An example of the latter is the creation of open-source labelled catalogues with biased predictions. In this paper, we develop a method based on an informative marginal likelihood to evaluate variable star classifiers. We collect deterministic rules that are based on physical descriptors of RR Lyrae stars, and then, to mitigate the biases, we introduce those rules into the marginal likelihood estimation. We perform experiments with a set of Bayesian Logistic Regressions, which are trained to classify RR Lyraes, and we found that our method outperforms traditional non-informative cross-validation strategies, even when penalized models are assessed. Our methodology provides a more rigorous alternative to assess machine learning models using astronomical knowledge. From this approach, applications to other classes of variable stars and algorithmic improvements can be developed.

</p>
</details>

<details><summary><b>Unbiased Estimation of the Gradient of the Log-Likelihood for a Class of Continuous-Time State-Space Models</b>
<a href="https://arxiv.org/abs/2105.11522">arxiv:2105.11522</a>
&#x1F4C8; 2 <br>
<p>Marco Ballesio, Ajay Jasra</p></summary>
<p>

**Abstract:** In this paper, we consider static parameter estimation for a class of continuous-time state-space models. Our goal is to obtain an unbiased estimate of the gradient of the log-likelihood (score function), which is an estimate that is unbiased even if the stochastic processes involved in the model must be discretized in time. To achieve this goal, we apply a doubly randomized scheme, that involves a novel coupled conditional particle filter (CCPF) on the second level of randomization. Our novel estimate helps facilitate the application of gradient-based estimation algorithms, such as stochastic-gradient Langevin descent. We illustrate our methodology in the context of stochastic gradient descent (SGD) in several numerical examples and compare with the Rhee & Glynn estimator.

</p>
</details>

<details><summary><b>Guided Hyperparameter Tuning Through Visualization and Inference</b>
<a href="https://arxiv.org/abs/2105.11516">arxiv:2105.11516</a>
&#x1F4C8; 2 <br>
<p>Hyekang Joo, Calvin Bao, Ishan Sen, Furong Huang, Leilani Battle</p></summary>
<p>

**Abstract:** For deep learning practitioners, hyperparameter tuning for optimizing model performance can be a computationally expensive task. Though visualization can help practitioners relate hyperparameter settings to overall model performance, significant manual inspection is still required to guide the hyperparameter settings in the next batch of experiments. In response, we present a streamlined visualization system enabling deep learning practitioners to more efficiently explore, tune, and optimize hyperparameters in a batch of experiments. A key idea is to directly suggest more optimal hyperparameter values using a predictive mechanism. We then integrate this mechanism with current visualization practices for deep learning. Moreover, an analysis on the variance in a selected performance metric in the context of the model hyperparameters shows the impact that certain hyperparameters have on the performance metric. We evaluate the tool with a user study on deep learning model builders, finding that our participants have little issue adopting the tool and working with it as part of their workflow.

</p>
</details>

<details><summary><b>Inferring Temporal Logic Properties from Data using Boosted Decision Trees</b>
<a href="https://arxiv.org/abs/2105.11508">arxiv:2105.11508</a>
&#x1F4C8; 2 <br>
<p>Erfan Aasi, Cristian Ioan Vasile, Mahroo Bahreinian, Calin Belta</p></summary>
<p>

**Abstract:** Many autonomous systems, such as robots and self-driving cars, involve real-time decision making in complex environments, and require prediction of future outcomes from limited data. Moreover, their decisions are increasingly required to be interpretable to humans for safe and trustworthy co-existence. This paper is a first step towards interpretable learning-based robot control. We introduce a novel learning problem, called incremental formula and predictor learning, to generate binary classifiers with temporal logic structure from time-series data. The classifiers are represented as pairs of Signal Temporal Logic (STL) formulae and predictors for their satisfaction. The incremental property provides prediction of labels for prefix signals that are revealed over time. We propose a boosted decision-tree algorithm that leverages weak, but computationally inexpensive, learners to increase prediction and runtime performance. The effectiveness and classification accuracy of our algorithms are evaluated on autonomous-driving and naval surveillance case studies.

</p>
</details>

<details><summary><b>Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation</b>
<a href="https://arxiv.org/abs/2105.11486">arxiv:2105.11486</a>
&#x1F4C8; 2 <br>
<p>Ashwin Nalwade, Jackie Kisa</p></summary>
<p>

**Abstract:** Multi-modal magnetic resonance imaging (MRI) is a crucial method for analyzing the human brain. It is usually used for diagnosing diseases and for making valuable decisions regarding the treatments - for instance, checking for gliomas in the human brain. With varying degrees of severity and detection, properly diagnosing gliomas is one of the most daunting and significant analysis tasks in modern-day medicine. Our primary focus is on working with different approaches to perform the segmentation of brain tumors in multimodal MRI scans. Now, the quantity, variability of the data used for training has always been considered to be crucial for developing excellent models. Hence, we also want to experiment with Knowledge Distillation techniques.

</p>
</details>

<details><summary><b>Improving Machine Learning-Based Modeling of Semiconductor Devices by Data Self-Augmentation</b>
<a href="https://arxiv.org/abs/2105.11453">arxiv:2105.11453</a>
&#x1F4C8; 2 <br>
<p>Zeheng Wang, Liang Li, Ross C. C. Leon, Arne Laucht</p></summary>
<p>

**Abstract:** In the electronics industry, introducing Machine Learning (ML)-based techniques can enhance Technology Computer-Aided Design (TCAD) methods. However, the performance of ML models is highly dependent on their training datasets. Particularly in the semiconductor industry, given the fact that the fabrication process of semiconductor devices is complicated and expensive, it is of great difficulty to obtain datasets with sufficient size and good quality. In this paper, we propose a strategy for improving ML-based device modeling by data self-augmentation using variational autoencoder-based techniques, where initially only a few experimental data points are required and TCAD tools are not essential. Taking a deep neural network-based prediction task of the Ohmic resistance value in Gallium Nitride devices as an example, we apply our proposed strategy to augment data points and achieve a reduction in the mean absolute error of predicting the experimental results by up to 70%. The proposed method could be easily modified for different tasks, rendering it of high interest to the semiconductor industry in general.

</p>
</details>

<details><summary><b>Reproducibility Report: Contextualizing Hate Speech Classifiers with Post-hoc Explanation</b>
<a href="https://arxiv.org/abs/2105.11412">arxiv:2105.11412</a>
&#x1F4C8; 2 <br>
<p>Kiran Purohit, Owais Iqbal, Ankan Mullick</p></summary>
<p>

**Abstract:** The presented report evaluates Contextualizing Hate Speech Classifiers with Post-hoc Explanation paper within the scope of ML Reproducibility Challenge 2020. Our work focuses on both aspects constituting the paper: the method itself and the validity of the stated results. In the following sections, we have described the paper, related works, algorithmic frameworks, our experiments and evaluations.

</p>
</details>

<details><summary><b>Classifying Math KCs via Task-Adaptive Pre-Trained BERT</b>
<a href="https://arxiv.org/abs/2105.11343">arxiv:2105.11343</a>
&#x1F4C8; 2 <br>
<p>Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu, Sean McGrew, Dongwon Lee</p></summary>
<p>

**Abstract:** Educational content labeled with proper knowledge components (KCs) are particularly useful to teachers or content organizers. However, manually labeling educational content is labor intensive and error-prone. To address this challenge, prior research proposed machine learning based solutions to auto-label educational content with limited success. In this work, we significantly improve prior research by (1) expanding the input types to include KC descriptions, instructional video titles, and problem descriptions (i.e., three types of prediction task), (2) doubling the granularity of the prediction from 198 to 385 KC labels (i.e., more practical setting but much harder multinomial classification problem), (3) improving the prediction accuracies by 0.5-2.3% using Task-adaptive Pre-trained BERT, outperforming six baselines, and (4) proposing a simple evaluation measure by which we can recover 56-73% of mispredicted KC labels. All codes and data sets in the experiments are available at:https://github.com/tbs17/TAPT-BERT

</p>
</details>

<details><summary><b>Towards Compact CNNs via Collaborative Compression</b>
<a href="https://arxiv.org/abs/2105.11228">arxiv:2105.11228</a>
&#x1F4C8; 2 <br>
<p>Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan Yang, Jincheng Ma, Qi Tian, Rongrong Ji</p></summary>
<p>

**Abstract:** Channel pruning and tensor decomposition have received extensive attention in convolutional neural network compression. However, these two techniques are traditionally deployed in an isolated manner, leading to significant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decomposition to compress CNN models by simultaneously learning the model sparsity and low-rankness. Specifically, we first investigate the compression sensitivity of each layer in the network, and then propose a Global Compression Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove redundant compression units step-by-step, which fully considers the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on various datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012.

</p>
</details>

<details><summary><b>Decentralized, Hybrid MAC Design with Reduced State Information Exchange for Low-Delay IoT Applications</b>
<a href="https://arxiv.org/abs/2105.11213">arxiv:2105.11213</a>
&#x1F4C8; 2 <br>
<p>Avinash Mohan, Arpan Chattopadhyay, Shivam Vinayak Vatsa, Anurag Kumar</p></summary>
<p>

**Abstract:** We consider a system of several collocated nodes sharing a time slotted wireless channel, and seek a MAC that (i) provides low mean delay, (ii) has distributed control (i.e., there is no central scheduler), and (iii) does not require explicit exchange of state information or control signals. The design of such MAC protocols must keep in mind the need for contention access at light traffic, and scheduled access in heavy traffic, leading to the long-standing interest in hybrid, adaptive MACs.
  We first propose EZMAC, a simple extension of an existing decentralized, hybrid MAC called ZMAC. Next, motivated by our results on delay and throughput optimality in partially observed, constrained queuing networks, we develop another decentralized MAC protocol that we term QZMAC. A method to improve the short-term fairness of QZMAC is proposed and analysed, and the resulting modified algorithm is shown to possess better fairness properties than QZMAC. The theory developed to reduce delay is also shown to work %with different traffic types (batch arrivals, for example) and even in the presence of transmission errors and fast fading.
  Extensions to handle time critical traffic (alarms, for example) and hidden nodes are also discussed. Practical implementation issues, such as handling Clear Channel Assessment (CCA) errors, are outlined. We implement and demonstrate the performance of QZMAC on a test bed consisting of CC2420 based Crossbow telosB motes, running the 6TiSCH communication stack on the Contiki operating system over the 2.4GHz ISM band.
  Finally, using simulations, we show that both protocols achieve mean delays much lower than those achieved by ZMAC, and QZMAC provides mean delays very close to the minimum achievable in this setting, i.e., that of the centralized complete knowledge scheduler.

</p>
</details>

<details><summary><b>Fuzzy inference system application for oil-water flow patterns identification</b>
<a href="https://arxiv.org/abs/2105.11181">arxiv:2105.11181</a>
&#x1F4C8; 2 <br>
<p>Yuyan Wu, Haimin Guo, Hongwei Song, Rui Deng</p></summary>
<p>

**Abstract:** With the continuous development of the petroleum industry, long-distance transportation of oil and gas has been the norm. Due to gravity differentiation in horizontal wells and highly deviated wells (non-vertical wells), the water phase at the bottom of the pipeline will cause scaling and corrosion in the pipeline. Scaling and corrosion will make the transportation process difficult, and transportation costs will be considerably increased. Therefore, the study of the oil-water two-phase flow pattern is of great importance to oil production. In this paper, a fuzzy inference system is used to predict the flow pattern of the fluid, get the prediction result, and compares it with the prediction result of the BP neural network. From the comparison of the results, we found that the prediction results of the fuzzy inference system are more accurate and reliable than the prediction results of the BP neural network. At the same time, it can realize real-time monitoring and has less error control. Experimental results demonstrate that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.

</p>
</details>

<details><summary><b>Abusive Language Detection in Heterogeneous Contexts: Dataset Collection and the Role of Supervised Attention</b>
<a href="https://arxiv.org/abs/2105.11119">arxiv:2105.11119</a>
&#x1F4C8; 2 <br>
<p>Hongyu Gong, Alberto Valido, Katherine M. Ingram, Giulia Fanti, Suma Bhat, Dorothy L. Espelage</p></summary>
<p>

**Abstract:** Abusive language is a massive problem in online social platforms. Existing abusive language detection techniques are particularly ill-suited to comments containing heterogeneous abusive language patterns, i.e., both abusive and non-abusive parts. This is due in part to the lack of datasets that explicitly annotate heterogeneity in abusive language. We tackle this challenge by providing an annotated dataset of abusive language in over 11,000 comments from YouTube. We account for heterogeneity in this dataset by separately annotating both the comment as a whole and the individual sentences that comprise each comment. We then propose an algorithm that uses a supervised attention mechanism to detect and categorize abusive content using multi-task learning. We empirically demonstrate the challenges of using traditional techniques on heterogeneous content and the comparative gains in performance of the proposed approach over state-of-the-art methods.

</p>
</details>

<details><summary><b>OFEI: A Semi-black-box Android Adversarial Sample Attack Framework Against DLaaS</b>
<a href="https://arxiv.org/abs/2105.11593">arxiv:2105.11593</a>
&#x1F4C8; 1 <br>
<p>Guangquan Xu, GuoHua Xin, Litao Jiao, Jian Liu, Shaoying Liu, Meiqi Feng, Xi Zheng</p></summary>
<p>

**Abstract:** With the growing popularity of Android devices, Android malware is seriously threatening the safety of users. Although such threats can be detected by deep learning as a service (DLaaS), deep neural networks as the weakest part of DLaaS are often deceived by the adversarial samples elaborated by attackers. In this paper, we propose a new semi-black-box attack framework called one-feature-each-iteration (OFEI) to craft Android adversarial samples. This framework modifies as few features as possible and requires less classifier information to fool the classifier. We conduct a controlled experiment to evaluate our OFEI framework by comparing it with the benchmark methods JSMF, GenAttack and pointwise attack. The experimental results show that our OFEI has a higher misclassification rate of 98.25%. Furthermore, OFEI can extend the traditional white-box attack methods in the image field, such as fast gradient sign method (FGSM) and DeepFool, to craft adversarial samples for Android. Finally, to enhance the security of DLaaS, we use two uncertainties of the Bayesian neural network to construct the combined uncertainty, which is used to detect adversarial samples and achieves a high detection rate of 99.28%.

</p>
</details>

<details><summary><b>Deep neural network enabled corrective source term approach to hybrid analysis and modeling</b>
<a href="https://arxiv.org/abs/2105.11521">arxiv:2105.11521</a>
&#x1F4C8; 1 <br>
<p>Sindre Stenen Blakseth, Adil Rasheed, Trond Kvamsdal, Omer San</p></summary>
<p>

**Abstract:** In this work, we introduce, justify and demonstrate the Corrective Source Term Approach (CoSTA) -- a novel approach to Hybrid Analysis and Modeling (HAM). The objective of HAM is to combine physics-based modeling (PBM) and data-driven modeling (DDM) to create generalizable, trustworthy, accurate, computationally efficient and self-evolving models. CoSTA achieves this objective by augmenting the governing equation of a PBM model with a corrective source term generated using a deep neural network. In a series of numerical experiments on one-dimensional heat diffusion, CoSTA is found to outperform comparable DDM and PBM models in terms of accuracy -- often reducing predictive errors by several orders of magnitude -- while also generalizing better than pure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides a modular framework for leveraging novel developments within both PBM and DDM. Its theoretical foundation also ensures that CoSTA can be used to model any system governed by (deterministic) partial differential equations. Moreover, CoSTA facilitates interpretation of the DNN-generated source term within the context of PBM, which results in improved explainability of the DNN. These factors make CoSTA a potential door-opener for data-driven techniques to enter high-stakes applications previously reserved for pure PBM.

</p>
</details>

<details><summary><b>Design to automate the detection and counting of Tuberculosis(TB) bacilli</b>
<a href="https://arxiv.org/abs/2105.11432">arxiv:2105.11432</a>
&#x1F4C8; 1 <br>
<p>Dinesh Jackson Samuel, Rajesh Kanna Baskaran</p></summary>
<p>

**Abstract:** Tuberculosis is a contagious disease which is one of the leading causes of death, globally. The general diagnosis methods for tuberculosis include microscopic examination, tuberculin skin test, culture method, enzyme linked immunosorbent assay (ELISA) and electronic nose system. World Health Organization (WHO) recommends standard microscopic examination for early diagnosis of tuberculosis. In microscopy, the technician examines field of views (FOVs) in sputum smear for presence of any TB bacilli and counts the number of TB bacilli per FOV to report the level of severity. This process is time consuming with an increased concentration for an experienced staff to examine a single sputum smear. The examination demands for skilled technicians in high-prevalence countries which may lead to overload, fatigue and diminishes the quality of microscopy. Thus, a computer assisted system is proposed and designed for the detection of tuberculosis bacilli to assist pathologists with increased sensitivity and specificity. The manual efforts in detecting and counting the number of TB bacilli is greatly minimized. The system obtains Ziehl-Neelsen stained microscopic images from conventional microscope at 100x magnification and passes the data to the detection system. Initially the segmentation of TB bacilli was done using RGB thresholding and Sauvola's adaptive thresholding algorithm. To eliminate the non-TB bacilli from coarse level segmentation, shape descriptors like area, perimeter, convex hull, major axis length and eccentricity are used to extract only the TB bacilli features. Finally, the TB bacilli are counted using the generated bounding boxes to report the level of severity.

</p>
</details>

<details><summary><b>Augmenting Modelers with Semantic Autocompletion of Processes</b>
<a href="https://arxiv.org/abs/2105.11385">arxiv:2105.11385</a>
&#x1F4C8; 1 <br>
<p>Maayan Goldstein, Cecilia Gonzalez-Alvarez</p></summary>
<p>

**Abstract:** Business process modelers need to have expertise and knowledge of the domain that may not always be available to them. Therefore, they may benefit from tools that mine collections of existing processes and recommend element(s) to be added to a new process that they are constructing. In this paper, we present a method for process autocompletion at design time, that is based on the semantic similarity of sub-processes. By converting sub-processes to textual paragraphs and encoding them as numerical vectors, we can find semantically similar ones, and thereafter recommend the next element. To achieve this, we leverage a state-of-the-art technique for embedding natural language as vectors. We evaluate our approach on open source and proprietary datasets and show that our technique is accurate for processes in various domains.

</p>
</details>

<details><summary><b>DDR-Net: Dividing and Downsampling Mixed Network for Diffeomorphic Image Registration</b>
<a href="https://arxiv.org/abs/2105.11361">arxiv:2105.11361</a>
&#x1F4C8; 1 <br>
<p>Ankita Joshi, Yi Hong</p></summary>
<p>

**Abstract:** Deep diffeomorphic registration faces significant challenges for high-dimensional images, especially in terms of memory limits. Existing approaches either downsample original images, or approximate underlying transformations, or reduce model size. The information loss during the approximation or insufficient model capacity is a hindrance to the registration accuracy for high-dimensional images, e.g., 3D medical volumes. In this paper, we propose a Dividing and Downsampling mixed Registration network (DDR-Net), a general architecture that preserves most of the image information at multiple scales. DDR-Net leverages the global context via downsampling the input and utilizes the local details from divided chunks of the input images. This design reduces the network input size and its memory cost; meanwhile, by fusing global and local information, DDR-Net obtains both coarse-level and fine-level alignments in the final deformation fields. We evaluate DDR-Net on three public datasets, i.e., OASIS, IBSR18, and 3DIRCADB-01, and the experimental results demonstrate our approach outperforms existing approaches.

</p>
</details>

<details><summary><b>Brain tumour segmentation using a triplanar ensemble of U-Nets</b>
<a href="https://arxiv.org/abs/2105.11356">arxiv:2105.11356</a>
&#x1F4C8; 1 <br>
<p>Vaanathi Sundaresan, Ludovica Griffanti, Mark Jenkinson</p></summary>
<p>

**Abstract:** Gliomas appear with wide variation in their characteristics both in terms of their appearance and location on brain MR images, which makes robust tumour segmentation highly challenging, and leads to high inter-rater variability even in manual segmentations. In this work, we propose a triplanar ensemble network, with an independent tumour core prediction module, for accurate segmentation of these tumours and their sub-regions. On evaluating our method on the MICCAI Brain Tumor Segmentation (BraTS) challenge validation dataset, for tumour sub-regions, we achieved a Dice similarity coefficient of 0.77 for both enhancing tumour (ET) and tumour core (TC). In the case of the whole tumour (WT) region, we achieved a Dice value of 0.89, which is on par with the top-ranking methods from BraTS'17-19. Our method achieved an evaluation score that was the equal 5th highest value (with our method ranking in 10th place) in the BraTS'20 challenge, with mean Dice values of 0.81, 0.89 and 0.84 on ET, WT and TC regions respectively on the BraTS'20 unseen test dataset.

</p>
</details>

<details><summary><b>A self-supervised learning strategy for postoperative brain cavity segmentation simulating resections</b>
<a href="https://arxiv.org/abs/2105.11239">arxiv:2105.11239</a>
&#x1F4C8; 1 <br>
<p>Fernando Pérez-García, Reuben Dorent, Michele Rizzi, Francesco Cardinale, Valerio Frazzini, Vincent Navarro, Caroline Essert, Irène Ollivier, Tom Vercauteren, Rachel Sparks, John S. Duncan, Sébastien Ourselin</p></summary>
<p>

**Abstract:** Accurate segmentation of brain resection cavities (RCs) aids in postoperative analysis and determining follow-up treatment. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large annotated datasets for training. Annotation of 3D medical images is time-consuming, requires highly-trained raters, and may suffer from high inter-rater variability. Self-supervised learning strategies can leverage unlabeled data for training.
  We developed an algorithm to simulate resections from preoperative magnetic resonance images (MRIs). We performed self-supervised training of a 3D CNN for RC segmentation using our simulation method. We curated EPISURG, a dataset comprising 430 postoperative and 268 preoperative MRIs from 430 refractory epilepsy patients who underwent resective neurosurgery. We fine-tuned our model on three small annotated datasets from different institutions and on the annotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.
  The model trained on data with simulated resections obtained median (interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4 (36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After fine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8). For comparison, inter-rater agreement between human annotators from our previous study was 84.0 (9.9).
  We present a self-supervised learning strategy for 3D CNNs using simulated RCs to accurately segment real RCs on postoperative MRI. Our method generalizes well to data from different institutions, pathologies and modalities. Source code, segmentation models and the EPISURG dataset are available at https://github.com/fepegar/ressegijcars .

</p>
</details>

<details><summary><b>Smart mobile microscopy: towards fully-automated digitization</b>
<a href="https://arxiv.org/abs/2105.11179">arxiv:2105.11179</a>
&#x1F4C8; 1 <br>
<p>A. Kornilova, I. Kirilenko, D. Iarosh, V. Kutuev, M. Strutovsky</p></summary>
<p>

**Abstract:** Mobile microscopy is a newly formed field that emerged from a combination of optical microscopy capabilities and spread, functionality, and ever-increasing computing resources of mobile devices. Despite the idea of creating a system that would successfully merge a microscope, numerous computer vision methods, and a mobile device is regularly examined, the resulting implementations still require the presence of a qualified operator to control specimen digitization. In this paper, we address the task of surpassing this constraint and present a ``smart'' mobile microscope concept aimed at automatic digitization of the most valuable visual information about the specimen. We perform this through combining automated microscope setup control and classic techniques such as auto-focusing, in-focus filtering, and focus-stacking -- adapted and optimized as parts of a mobile cross-platform library.

</p>
</details>

<details><summary><b>Connect the Dots: In Situ 4D Seismic Monitoring of CO2 Storage with Spatio-temporal CNNs</b>
<a href="https://arxiv.org/abs/2105.11622">arxiv:2105.11622</a>
&#x1F4C8; 0 <br>
<p>Shihang Feng, Xitong Zhang, Brendt Wohlberg, Neill Symons, Youzuo Lin</p></summary>
<p>

**Abstract:** 4D seismic imaging has been widely used in CO$_2$ sequestration projects to monitor the fluid flow in the volumetric subsurface region that is not sampled by wells. Ideally, real-time monitoring and near-future forecasting would provide site operators with great insights to understand the dynamics of the subsurface reservoir and assess any potential risks. However, due to obstacles such as high deployment cost, availability of acquisition equipment, exclusion zones around surface structures, only very sparse seismic imaging data can be obtained during monitoring. That leads to an unavoidable and growing knowledge gap over time. The operator needs to understand the fluid flow throughout the project lifetime and the seismic data are only available at a limited number of times. This is insufficient for understanding the reservoir behavior. To overcome those challenges, we have developed spatio-temporal neural-network-based models that can produce high-fidelity interpolated or extrapolated images effectively and efficiently. Specifically, our models are built on an autoencoder, and incorporate the long short-term memory (LSTM) structure with a new loss function regularized by optical flow. We validate the performance of our models using real 4D post-stack seismic imaging data acquired at the Sleipner CO$_2$ sequestration field. We employ two different strategies in evaluating our models. Numerically, we compare our models with different baseline approaches using classic pixel-based metrics. We also conduct a blind survey and collect a total of 20 responses from domain experts to evaluate the quality of data generated by our models. Via both numerical and expert evaluation, we conclude that our models can produce high-quality 2D/3D seismic imaging data at a reasonable cost, offering the possibility of real-time monitoring or even near-future forecasting of the CO$_2$ storage reservoir.

</p>
</details>

<details><summary><b>Saddle Point Optimization with Approximate Minimization Oracle and its Application to Robust Berthing Control</b>
<a href="https://arxiv.org/abs/2105.11586">arxiv:2105.11586</a>
&#x1F4C8; 0 <br>
<p>Youhei Akimoto, Yoshiki Miyauchi, Atsuo Maki</p></summary>
<p>

**Abstract:** We propose an approach to saddle point optimization relying only on oracles that solve minimization problems approximately. We analyze its convergence property on a strongly convex--concave problem and show its linear convergence toward the global min--max saddle point. Based on the convergence analysis, we develop a heuristic approach to adapt the learning rate. An implementation of the developed approach using the (1+1)-CMA-ES as the minimization oracle, namely Adversarial-CMA-ES, is shown to outperform several existing approaches on test problems. Numerical evaluation confirms the tightness of the theoretical convergence rate bound as well as the efficiency of the learning rate adaptation mechanism. As an example of real-world problems, the suggested optimization method is applied to automatic berthing control problems under model uncertainties, showing its usefulness in obtaining solutions robust to uncertainty.

</p>
</details>

<details><summary><b>FedScale: Benchmarking Model and System Performance of Federated Learning at Scale</b>
<a href="https://arxiv.org/abs/2105.11367">arxiv:2105.11367</a>
&#x1F4C8; 0 <br>
<p>Fan Lai, Yinwei Dai, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury</p></summary>
<p>

**Abstract:** We present FedScale, a diverse set of challenging and realistic benchmark datasets to facilitate scalable, comprehensive, and reproducible federated learning (FL) research. FedScale datasets are large-scale, encompassing a diverse range of important FL tasks, such as image classification, object detection, word prediction, and speech recognition. For each dataset, we provide a unified evaluation protocol using realistic data splits and evaluation metrics. To meet the pressing need for reproducing realistic FL at scale, we have also built an efficient evaluation platform to simplify and standardize the process of FL experimental setup and model evaluation. Our evaluation platform provides flexible APIs to implement new FL algorithms and includes new execution backends with minimal developer efforts. Finally, we perform in-depth benchmark experiments on these datasets. Our experiments suggest fruitful opportunities in heterogeneity-aware co-optimizations of the system and statistical efficiency under realistic FL characteristics. FedScale is open-source with permissive licenses and actively maintained, and we welcome feedback and contributions from the community.

</p>
</details>


[Next Page](2021/2021-05/2021-05-23.md)
