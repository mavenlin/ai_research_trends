## Summary for 2021-02-01, created on 2021-12-23


<details><summary><b>Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms</b>
<a href="https://arxiv.org/abs/2102.00815">arxiv:2102.00815</a>
&#x1F4C8; 31 <br>
<p>Chi Jin, Qinghua Liu, Sobhan Miryoosefi</p></summary>
<p>

**Abstract:** Finding the minimal structural assumptions that empower sample-efficient learning is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by introducing a new complexity measure -- Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not limited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm -- GOLF, and reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang et al., 2017). We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems.

</p>
</details>

<details><summary><b>Spatio-temporal Weather Forecasting and Attention Mechanism on Convolutional LSTMs</b>
<a href="https://arxiv.org/abs/2102.00696">arxiv:2102.00696</a>
&#x1F4C8; 30 <br>
<p>Selim Furkan Tekin, Oguzhan Karaahmetoglu, Fatih Ilhan, Ismail Balaban, Suleyman Serdar Kozat</p></summary>
<p>

**Abstract:** Numerical weather forecasting on high-resolution physical models consume hours of computations on supercomputers. Application of deep learning and machine learning methods in forecasting revealed new solutions in this area. In this paper, we forecast high-resolution numeric weather data using both input weather data and observations by providing a novel deep learning architecture. We formulate the problem as spatio-temporal prediction. Our model is composed of Convolutional Long-short Term Memory, and Convolutional Neural Network units with encoder-decoder structure. We enhance the short-long term performance and interpretability with an attention and a context matcher mechanism. We perform experiments on high-scale, real-life, benchmark numerical weather dataset, ERA5 hourly data on pressure levels, and forecast the temperature. The results show significant improvements in capturing both spatial and temporal correlations with attention matrices focusing on different parts of the input series. Our model obtains the best validation and the best test score among the baseline models, including ConvLSTM forecasting network and U-Net. We provide qualitative and quantitative results and show that our model forecasts 10 time steps with 3 hour frequency with an average of 2 degrees error. Our code and the data are publicly available.

</p>
</details>

<details><summary><b>About Face: A Survey of Facial Recognition Evaluation</b>
<a href="https://arxiv.org/abs/2102.00813">arxiv:2102.00813</a>
&#x1F4C8; 28 <br>
<p>Inioluwa Deborah Raji, Genevieve Fried</p></summary>
<p>

**Abstract:** We survey over 100 face datasets constructed between 1976 to 2019 of 145 million images of over 17 million subjects from a range of sources, demographics and conditions. Our historical survey reveals that these datasets are contextually informed, shaped by changes in political motivations, technological capability and current norms. We discuss how such influences mask specific practices (some of which may actually be harmful or otherwise problematic) and make a case for the explicit communication of such details in order to establish a more grounded understanding of the technology's function in the real world.

</p>
</details>

<details><summary><b>GraphDF: A Discrete Flow Model for Molecular Graph Generation</b>
<a href="https://arxiv.org/abs/2102.01189">arxiv:2102.01189</a>
&#x1F4C8; 22 <br>
<p>Youzhi Luo, Keqiang Yan, Shuiwang Ji</p></summary>
<p>

**Abstract:** We consider the problem of molecular graph generation using deep models. While graphs are discrete, most existing methods use continuous latent variables, resulting in inaccurate modeling of discrete graph structures. In this work, we propose GraphDF, a novel discrete latent variable model for molecular graph generation based on normalizing flow methods. GraphDF uses invertible modulo shift transforms to map discrete latent variables to graph nodes and edges. We show that the use of discrete latent variables reduces computational costs and eliminates the negative effect of dequantization. Comprehensive experimental results show that GraphDF outperforms prior methods on random generation, property optimization, and constrained optimization tasks.

</p>
</details>

<details><summary><b>Information-Theoretic Generalization Bounds for Stochastic Gradient Descent</b>
<a href="https://arxiv.org/abs/2102.00931">arxiv:2102.00931</a>
&#x1F4C8; 21 <br>
<p>Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, Daniel M. Roy</p></summary>
<p>

**Abstract:** We study the generalization properties of the popular stochastic optimization method known as stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.

</p>
</details>

<details><summary><b>Automated Query Reformulation for Efficient Search based on Query Logs From Stack Overflow</b>
<a href="https://arxiv.org/abs/2102.00826">arxiv:2102.00826</a>
&#x1F4C8; 20 <br>
<p>Kaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude, Xiang Chen</p></summary>
<p>

**Abstract:** As a popular Q&A site for programming, Stack Overflow is a treasure for developers. However, the amount of questions and answers on Stack Overflow make it difficult for developers to efficiently locate the information they are looking for. There are two gaps leading to poor search results: the gap between the user's intention and the textual query, and the semantic gap between the query and the post content. Therefore, developers have to constantly reformulate their queries by correcting misspelled words, adding limitations to certain programming languages or platforms, etc. As query reformulation is tedious for developers, especially for novices, we propose an automated software-specific query reformulation approach based on deep learning. With query logs provided by Stack Overflow, we construct a large-scale query reformulation corpus, including the original queries and corresponding reformulated ones. Our approach trains a Transformer model that can automatically generate candidate reformulated queries when given the user's original query. The evaluation results show that our approach outperforms five state-of-the-art baselines, and achieves a 5.6% to 33.5% boost in terms of $\mathit{ExactMatch}$ and a 4.8% to 14.4% boost in terms of $\mathit{GLEU}$.

</p>
</details>

<details><summary><b>Evaluating the Interpretability of Generative Models by Interactive Reconstruction</b>
<a href="https://arxiv.org/abs/2102.01264">arxiv:2102.01264</a>
&#x1F4C8; 10 <br>
<p>Andrew Slavin Ross, Nina Chen, Elisa Zhao Hang, Elena L. Glassman, Finale Doshi-Velez</p></summary>
<p>

**Abstract:** For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on "disentanglement" measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations, where users interactively modify representations to reconstruct target instances. On synthetic datasets, we find performance on this task much more reliably differentiates entangled and disentangled models than baseline approaches. On a real dataset, we find it differentiates between representation learning methods widely believed but never shown to produce more or less interpretable models. In both cases, we ran small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm that our qualitative and quantitative results agreed.

</p>
</details>

<details><summary><b>Multi-modal Ensemble Models for Predicting Video Memorability</b>
<a href="https://arxiv.org/abs/2102.01173">arxiv:2102.01173</a>
&#x1F4C8; 10 <br>
<p>Tony Zhao, Irving Fang, Jeffrey Kim, Gerald Friedland</p></summary>
<p>

**Abstract:** Modeling media memorability has been a consistent challenge in the field of machine learning. The Predicting Media Memorability task in MediaEval2020 is the latest benchmark among similar challenges addressing this topic. Building upon techniques developed in previous iterations of the challenge, we developed ensemble methods with the use of extracted video, image, text, and audio features. Critically, in this work we introduce and demonstrate the efficacy and high generalizability of extracted audio embeddings as a feature for the task of predicting media memorability.

</p>
</details>

<details><summary><b>Diagnosis of Acute Poisoning Using Explainable Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2102.01116">arxiv:2102.01116</a>
&#x1F4C8; 9 <br>
<p>Michael Chary, Ed W Boyer, Michele M Burns</p></summary>
<p>

**Abstract:** Medical toxicology is the clinical specialty that treats the toxic effects of substances, be it an overdose, a medication error, or a scorpion sting. The volume of toxicological knowledge and research has, as with other medical specialties, outstripped the ability of the individual clinician to entirely master and stay current with it. The application of machine learning techniques to medical toxicology is challenging because initial treatment decisions are often based on a few pieces of textual data and rely heavily on prior knowledge. ML techniques often do not represent knowledge in a way that is transparent for the physician, raising barriers to usability. Rule-based systems and decision tree learning are more transparent approaches, but often generalize poorly and require expert curation to implement and maintain. Here, we construct a probabilistic logic network to represent a portion of the knowledge base of a medical toxicologist. Our approach transparently mimics the knowledge representation and clinical decision-making of practicing clinicians. The software, dubbed Tak, performs comparably to humans on straightforward cases and intermediate difficulty cases, but is outperformed by humans on challenging clinical cases. Tak outperforms a decision tree classifier at all levels of difficulty. Probabilistic logic provides one form of explainable artificial intelligence that may be more acceptable for use in healthcare, if it can achieve acceptable levels of performance.

</p>
</details>

<details><summary><b>Understanding Cache Boundness of ML Operators on ARM Processors</b>
<a href="https://arxiv.org/abs/2102.00932">arxiv:2102.00932</a>
&#x1F4C8; 9 <br>
<p>Bernhard Klein, Christoph Gratl, Manfred Mücke, Holger Fröning</p></summary>
<p>

**Abstract:** Machine Learning compilers like TVM allow a fast and flexible deployment on embedded CPUs. This enables the use of non-standard operators, which are common in ML compression techniques. However, it is necessary to understand the limitations of typical compute-intense operators in ML workloads to design a proper solution. This is the first in-detail analysis of dense and convolution operators, generated with TVM, that compares to the fundamental hardware limits of embedded ARM processors. Thereby it explains the gap between computational peak performance, theoretical and measured, and real-world state-of-the-art results, created with TVM and openBLAS. Instead, one can see that single-precision general matrix multiply (GEMM) and convolutions are bound by L1-cache-read bandwidth. Explorations of 8-bit and bit-serial quantized operators show that quantization can be used to achieve relevant speedups compared to cache-bound floating-point operators. However, the performance of quantized operators highly depends on the interaction between data layout and bit packing.

</p>
</details>

<details><summary><b>Automatic Expansion of Domain-Specific Affective Models for Web Intelligence Applications</b>
<a href="https://arxiv.org/abs/2102.00827">arxiv:2102.00827</a>
&#x1F4C8; 9 <br>
<p>Albert Weichselbraun, Jakob Steixner, Adrian M. P. Braşoveanu, Arno Scharl, Max Göbel, Lyndon J. B. Nixon</p></summary>
<p>

**Abstract:** Sentic computing relies on well-defined affective models of different complexity - polarity to distinguish positive and negative sentiment, for example, or more nuanced models to capture expressions of human emotions. When used to measure communication success, even the most granular affective model combined with sophisticated machine learning approaches may not fully capture an organisation's strategic positioning goals. Such goals often deviate from the assumptions of standardised affective models. While certain emotions such as Joy and Trust typically represent desirable brand associations, specific communication goals formulated by marketing professionals often go beyond such standard dimensions. For instance, the brand manager of a television show may consider fear or sadness to be desired emotions for its audience. This article introduces expansion techniques for affective models, combining common and commonsense knowledge available in knowledge graphs with language models and affective reasoning, improving coverage and consistency as well as supporting domain-specific interpretations of emotions. An extensive evaluation compares the performance of different expansion techniques: (i) a quantitative evaluation based on the revisited Hourglass of Emotions model to assess performance on complex models that cover multiple affective categories, using manually compiled gold standard data, and (ii) a qualitative evaluation of a domain-specific affective model for television programme brands. The results of these evaluations demonstrate that the introduced techniques support a variety of embeddings and pre-trained models. The paper concludes with a discussion on applying this approach to other scenarios where affective model resources are scarce.

</p>
</details>

<details><summary><b>VIPPrint: A Large Scale Dataset of Printed and Scanned Images for Synthetic Face Images Detection and Source Linking</b>
<a href="https://arxiv.org/abs/2102.06792">arxiv:2102.06792</a>
&#x1F4C8; 8 <br>
<p>Anselmo Ferreira, Ehsan Nowroozi, Mauro Barni</p></summary>
<p>

**Abstract:** The possibility of carrying out a meaningful forensics analysis on printed and scanned images plays a major role in many applications. First of all, printed documents are often associated with criminal activities, such as terrorist plans, child pornography pictures, and even fake packages. Additionally, printing and scanning can be used to hide the traces of image manipulation or the synthetic nature of images, since the artifacts commonly found in manipulated and synthetic images are gone after the images are printed and scanned. A problem hindering research in this area is the lack of large scale reference datasets to be used for algorithm development and benchmarking. Motivated by this issue, we present a new dataset composed of a large number of synthetic and natural printed face images. To highlight the difficulties associated with the analysis of the images of the dataset, we carried out an extensive set of experiments comparing several printer attribution methods. We also verified that state-of-the-art methods to distinguish natural and synthetic face images fail when applied to print and scanned images. We envision that the availability of the new dataset and the preliminary experiments we carried out will motivate and facilitate further research in this area.

</p>
</details>

<details><summary><b>Civil Rephrases Of Toxic Texts With Self-Supervised Transformers</b>
<a href="https://arxiv.org/abs/2102.05456">arxiv:2102.05456</a>
&#x1F4C8; 8 <br>
<p>Leo Laugier, John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon</p></summary>
<p>

**Abstract:** Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.

</p>
</details>

<details><summary><b>A Stochastic Time Series Model for Predicting Financial Trends using NLP</b>
<a href="https://arxiv.org/abs/2102.01290">arxiv:2102.01290</a>
&#x1F4C8; 8 <br>
<p>Pratyush Muthukumar, Jie Zhong</p></summary>
<p>

**Abstract:** Stock price forecasting is a highly complex and vitally important field of research. Recent advancements in deep neural network technology allow researchers to develop highly accurate models to predict financial trends. We propose a novel deep learning model called ST-GAN, or Stochastic Time-series Generative Adversarial Network, that analyzes both financial news texts and financial numerical data to predict stock trends. We utilize cutting-edge technology like the Generative Adversarial Network (GAN) to learn the correlations among textual and numerical data over time. We develop a new method of training a time-series GAN directly using the learned representations of Naive Bayes' sentiment analysis on financial text data alongside technical indicators from numerical data. Our experimental results show significant improvement over various existing models and prior research on deep neural networks for stock price forecasting.

</p>
</details>

<details><summary><b>Counterfactual Generation with Knockoffs</b>
<a href="https://arxiv.org/abs/2102.00951">arxiv:2102.00951</a>
&#x1F4C8; 8 <br>
<p>Oana-Iuliana Popescu, Maha Shadaydeh, Joachim Denzler</p></summary>
<p>

**Abstract:** Human interpretability of deep neural networks' decisions is crucial, especially in domains where these directly affect human lives. Counterfactual explanations of already trained neural networks can be generated by perturbing input features and attributing importance according to the change in the classifier's outcome after perturbation. Perturbation can be done by replacing features using heuristic or generative in-filling methods. The choice of in-filling function significantly impacts the number of artifacts, i.e., false-positive attributions. Heuristic methods result in false-positive artifacts because the image after the perturbation is far from the original data distribution. Generative in-filling methods reduce artifacts by producing in-filling values that respect the original data distribution. However, current generative in-filling methods may also increase false-negatives due to the high correlation of in-filling values with the original data. In this paper, we propose to alleviate this by generating in-fillings with the statistically-grounded Knockoffs framework, which was developed by Barber and Candès in 2015 as a tool for variable selection with controllable false discovery rate. Knockoffs are statistically null-variables as decorrelated as possible from the original data, which can be swapped with the originals without changing the underlying data distribution. A comparison of different in-filling methods indicates that in-filling with knockoffs can reveal explanations in a more causal sense while still maintaining the compactness of the explanations.

</p>
</details>

<details><summary><b>Neural representation and generation for RNA secondary structures</b>
<a href="https://arxiv.org/abs/2102.00925">arxiv:2102.00925</a>
&#x1F4C8; 8 <br>
<p>Zichao Yan, William L. Hamilton, Mathieu Blanchette</p></summary>
<p>

**Abstract:** Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

</p>
</details>

<details><summary><b>Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition</b>
<a href="https://arxiv.org/abs/2102.01063">arxiv:2102.01063</a>
&#x1F4C8; 7 <br>
<p>Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin</p></summary>
<p>

**Abstract:** Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Our source code and pre-trained models are released on https://github.com/idstcv/ZenNAS.

</p>
</details>

<details><summary><b>Non-uniform Blur Kernel Estimation via Adaptive Basis Decomposition</b>
<a href="https://arxiv.org/abs/2102.01026">arxiv:2102.01026</a>
&#x1F4C8; 7 <br>
<p>Guillermo Carbajal, Patricia Vitoria, Mauricio Delbracio, Pablo Musé, José Lezama</p></summary>
<p>

**Abstract:** Motion blur estimation remains an important task for scene analysis and image restoration. In recent years, the removal of motion blur in photographs has seen impressive progress in the hands of deep learning-based methods, trained to map directly from blurry to sharp images. Characterization of the motion blur, on the other hand, has received less attention, and progress in model-based methods for deblurring lags behind that of data-driven end-to-end approaches. In this work we revisit the problem of characterizing dense, non-uniform motion blur in a single image and propose a general non-parametric model for this task. Given a blurry image, a neural network is trained to estimate a set of image-adaptive basis motion kernels as well as the mixing coefficients at the pixel level, producing a per-pixel motion blur field. We show that our approach overcomes the limitations of existing non-uniform motion blur estimation methods and leads to extremely accurate motion blur kernels. When applied to real motion-blurred images, a variational non-uniform blur removal method fed with the estimated blur kernels produces high-quality restored images. Qualitative and quantitative evaluation shows that these results are competitive or superior to results obtained with existing end-to-end deep learning (DL) based methods, thus bridging the gap between model-based and data-driven approaches.

</p>
</details>

<details><summary><b>CRPS Learning</b>
<a href="https://arxiv.org/abs/2102.00968">arxiv:2102.00968</a>
&#x1F4C8; 7 <br>
<p>Jonathan Berrisch, Florian Ziel</p></summary>
<p>

**Abstract:** Combination and aggregation techniques can significantly improve forecast accuracy. This also holds for probabilistic forecasting methods where predictive distributions are combined. There are several time-varying and adaptive weighting schemes such as Bayesian model averaging (BMA). However, the quality of different forecasts may vary not only over time but also within the distribution. For example, some distribution forecasts may be more accurate in the center of the distributions, while others are better at predicting the tails. Therefore, we introduce a new weighting method that considers the differences in performance over time and within the distribution. We discuss pointwise combination algorithms based on aggregation across quantiles that optimize with respect to the continuous ranked probability score (CRPS). After analyzing the theoretical properties of pointwise CRPS learning, we discuss B- and P-Spline-based estimation techniques for batch and online learning, based on quantile regression and prediction with expert advice. We prove that the proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise CRPS online learning has optimal convergence properties. They are confirmed in simulations and a probabilistic forecasting study for European emission allowance (EUA) prices.

</p>
</details>

<details><summary><b>Text-to-hashtag Generation using Seq2seq Learning</b>
<a href="https://arxiv.org/abs/2102.00904">arxiv:2102.00904</a>
&#x1F4C8; 7 <br>
<p>Augusto Camargo, Wesley Carvalho, Felipe Peressim, Alan Barzilay, Marcelo Finger</p></summary>
<p>

**Abstract:** In this paper, we studied whether models based on BiLSTM and BERT can predict hashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a sizable financial impact on Ecommerce. We processed a corpus of Ecommerce reviews as inputs, and predicted hashtags as outputs. We evaluated the results using four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A word cloud was used as a qualitative metric. While all computer-generated metrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results produced amazing scores. We concluded that the texts predicted by the neural networks are very promising for use as hashtags for products on Ecommerce websites. The code for this work is available at https://github.com/augustocamargo/text-to-hashtag.

</p>
</details>

<details><summary><b>Semantic Grouping Network for Video Captioning</b>
<a href="https://arxiv.org/abs/2102.00831">arxiv:2102.00831</a>
&#x1F4C8; 7 <br>
<p>Hobin Ryu, Sunghun Kang, Haeyong Kang, Chang D. Yoo</p></summary>
<p>

**Abstract:** This paper considers a video caption generating network referred to as Semantic Grouping Network (SGN) that attempts (1) to group video frames with discriminating word phrases of partially decoded caption and then (2) to decode those semantically aligned groups in predicting the next word. As consecutive frames are not likely to provide unique information, prior methods have focused on discarding or merging repetitive information based only on the input video. The SGN learns an algorithm to capture the most discriminating word phrases of the partially decoded caption and a mapping that associates each phrase to the relevant video frames - establishing this mapping allows semantically related frames to be clustered, which reduces redundancy. In contrast to the prior methods, the continuous feedback from decoded words enables the SGN to dynamically update the video representation that adapts to the partially decoded caption. Furthermore, a contrastive attention loss is proposed to facilitate accurate alignment between a word phrase and video frames without manual annotations. The SGN achieves state-of-the-art performances by outperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D score on MSVD and MSR-VTT datasets, respectively. Extensive experiments demonstrate the effectiveness and interpretability of the SGN.

</p>
</details>

<details><summary><b>Learning to Combat Noisy Labels via Classification Margins</b>
<a href="https://arxiv.org/abs/2102.00751">arxiv:2102.00751</a>
&#x1F4C8; 7 <br>
<p>Jason Z. Lin, Jelena Bradic</p></summary>
<p>

**Abstract:** A deep neural network trained on noisy labels is known to quickly lose its power to discriminate clean instances from noisy ones. After the early learning phase has ended, the network memorizes the noisy instances, which leads to a significant degradation in its generalization performance. To resolve this issue, we propose MARVEL (MARgins Via Early Learning), a new robust learning method where the memorization of the noisy instances is curbed. We propose a new test statistic that tracks the goodness of "fit" of every instance based on the epoch-history of its classification margins. If its classification margin is small in a sequence of consecutive learning epochs, that instance is declared noisy and the network abandons learning on it. Consequently, the network first flags a possibly noisy instance, and then waits to see if learning on that instance can be improved and if not, the network learns with confidence that this instance can be safely abandoned. We also propose MARVEL+, where arduous instances can be upweighted, enabling the network to focus and improve its learning on them and consequently its generalization. Experimental results on benchmark datasets with synthetic label noise and real-world datasets show that MARVEL outperforms other baselines consistently across different noise levels, with a significantly larger margin under asymmetric noise.

</p>
</details>

<details><summary><b>"Is depression related to cannabis?": A knowledge-infused model for Entity and Relation Extraction with Limited Supervision</b>
<a href="https://arxiv.org/abs/2102.01222">arxiv:2102.01222</a>
&#x1F4C8; 6 <br>
<p>Kaushik Roy, Usha Lokala, Vedant Khandelwal, Amit Sheth</p></summary>
<p>

**Abstract:** With strong marketing advocacy of the benefits of cannabis use for improved mental health, cannabis legalization is a priority among legislators. However, preliminary scientific research does not conclusively associate cannabis with improved mental health. In this study, we explore the relationship between depression and consumption of cannabis in a targeted social media corpus involving personal use of cannabis with the intent to derive its potential mental health benefit. We use tweets that contain an association among three categories annotated by domain experts - Reason, Effect, and Addiction. The state-of-the-art Natural Langauge Processing techniques fall short in extracting these relationships between cannabis phrases and the depression indicators. We seek to address the limitation by using domain knowledge; specifically, the Drug Abuse Ontology for addiction augmented with Diagnostic and Statistical Manual of Mental Disorders lexicons for mental health. Because of the lack of annotations due to the limited availability of the domain experts' time, we use supervised contrastive learning in conjunction with GPT-3 trained on a vast corpus to achieve improved performance even with limited supervision. Experimental results show that our method can significantly extract cannabis-depression relationships better than the state-of-the-art relation extractor. High-quality annotations can be provided using a nearest neighbor approach using the learned representations that can be used by the scientific community to understand the association between cannabis and depression better.

</p>
</details>

<details><summary><b>GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained Text Style Transfer</b>
<a href="https://arxiv.org/abs/2102.00769">arxiv:2102.00769</a>
&#x1F4C8; 6 <br>
<p>Yukai Shi, Sen Zhang, Chenxing Zhou, Xiaodan Liang, Xiaojun Yang, Liang Lin</p></summary>
<p>

**Abstract:** Non-parallel text style transfer has attracted increasing research interests in recent years. Despite successes in transferring the style based on the encoder-decoder framework, current approaches still lack the ability to preserve the content and even logic of original sentences, mainly due to the large unconstrained model space or too simplified assumptions on latent embedding space. Since language itself is an intelligent product of humans with certain grammars and has a limited rule-based model space by its nature, relieving this problem requires reconciling the model capacity of deep neural networks with the intrinsic model constraints from human linguistic rules. To this end, we propose a method called Graph Transformer based Auto Encoder (GTAE), which models a sentence as a linguistic graph and performs feature extraction and style transfer at the graph level, to maximally retain the content and the linguistic structure of original sentences. Quantitative experiment results on three non-parallel text style transfer tasks show that our model outperforms state-of-the-art methods in content preservation, while achieving comparable performance on transfer accuracy and sentence naturalness.

</p>
</details>

<details><summary><b>Fast rates in structured prediction</b>
<a href="https://arxiv.org/abs/2102.00760">arxiv:2102.00760</a>
&#x1F4C8; 6 <br>
<p>Vivien Cabannes, Alessandro Rudi, Francis Bach</p></summary>
<p>

**Abstract:** Discrete supervised learning problems such as classification are often tackled by introducing a continuous surrogate problem akin to regression. Bounding the original error, between estimate and solution, by the surrogate error endows discrete problems with convergence rates already shown for continuous instances. Yet, current approaches do not leverage the fact that discrete problems are essentially predicting a discrete output when continuous problems are predicting a continuous value. In this paper, we tackle this issue for general structured prediction problems, opening the way to "super fast" rates, that is, convergence rates for the excess risk faster than $n^{-1}$, where $n$ is the number of observations, with even exponential rates with the strongest assumptions. We first illustrate it for predictors based on nearest neighbors, generalizing rates known for binary classification to any discrete problem within the framework of structured prediction. We then consider kernel ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast rates, depending on a parameter characterizing the hardness of the problem, thus allowing, under smoothness assumptions, to bypass the curse of dimensionality.

</p>
</details>

<details><summary><b>Risk Aware and Multi-Objective Decision Making with Distributional Monte Carlo Tree Search</b>
<a href="https://arxiv.org/abs/2102.00966">arxiv:2102.00966</a>
&#x1F4C8; 5 <br>
<p>Conor F. Hayes, Mathieu Reymond, Diederik M. Roijers, Enda Howley, Patrick Mannion</p></summary>
<p>

**Abstract:** In many risk-aware and multi-objective reinforcement learning settings, the utility of the user is derived from the single execution of a policy. In these settings, making decisions based on the average future returns is not suitable. For example, in a medical setting a patient may only have one opportunity to treat their illness. When making a decision, just the expected return -- known in reinforcement learning as the value -- cannot account for the potential range of adverse or positive outcomes a decision may have. Our key insight is that we should use the distribution over expected future returns differently to represent the critical information that the agent requires at decision time. In this paper, we propose Distributional Monte Carlo Tree Search, an algorithm that learns a posterior distribution over the utility of the different possible returns attainable from individual policy executions, resulting in good policies for both risk-aware and multi-objective settings. Moreover, our algorithm outperforms the state-of-the-art in multi-objective reinforcement learning for the expected utility of the returns.

</p>
</details>

<details><summary><b>Riemannian Perspective on Matrix Factorization</b>
<a href="https://arxiv.org/abs/2102.00937">arxiv:2102.00937</a>
&#x1F4C8; 5 <br>
<p>Kwangjun Ahn, Felipe Suarez</p></summary>
<p>

**Abstract:** We study the non-convex matrix factorization approach to matrix completion via Riemannian geometry. Based on an optimization formulation over a Grassmannian manifold, we characterize the landscape based on the notion of principal angles between subspaces. For the fully observed case, our results show that there is a region in which the cost is geodesically convex, and outside of which all critical points are strictly saddle. We empirically study the partially observed case based on our findings.

</p>
</details>

<details><summary><b>Stability and Generalization of the Decentralized Stochastic Gradient Descent</b>
<a href="https://arxiv.org/abs/2102.01302">arxiv:2102.01302</a>
&#x1F4C8; 4 <br>
<p>Tao Sun, Dongsheng Li, Bao Wang</p></summary>
<p>

**Abstract:** The stability and generalization of stochastic gradient-based methods provide valuable insights into understanding the algorithmic performance of machine learning models. As the main workhorse for deep learning, stochastic gradient descent has received a considerable amount of studies. Nevertheless, the community paid little attention to its decentralized variants. In this paper, we provide a novel formulation of the decentralized stochastic gradient descent. Leveraging this formulation together with (non)convex optimization theory, we establish the first stability and generalization guarantees for the decentralized stochastic gradient descent. Our theoretical results are built on top of a few common and mild assumptions and reveal that the decentralization deteriorates the stability of SGD for the first time. We verify our theoretical findings by using a variety of decentralized settings and benchmark machine learning models.

</p>
</details>

<details><summary><b>Causal Inference with the Instrumental Variable Approach and Bayesian Nonparametric Machine Learning</b>
<a href="https://arxiv.org/abs/2102.01199">arxiv:2102.01199</a>
&#x1F4C8; 4 <br>
<p>Robert E. McCulloch, Rodney A. Sparapani, Brent R. Logan, Purushottam W. Laud</p></summary>
<p>

**Abstract:** We provide a new flexible framework for inference with the instrumental variable model. Rather than using linear specifications, functions characterizing the effects of instruments and other explanatory variables are estimated using machine learning via Bayesian Additive Regression Trees (BART). Error terms and their distribution are inferred using Dirichlet Process mixtures. Simulated and real examples show that when the true functions are linear, little is lost. But when nonlinearities are present, dramatic improvements are obtained with virtually no manual tuning.

</p>
</details>

<details><summary><b>Hybrid Information-driven Multi-agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.01004">arxiv:2102.01004</a>
&#x1F4C8; 4 <br>
<p>William A. Dawson, Ruben Glatt, Edward Rusu, Braden C. Soper, Ryan A. Goldhahn</p></summary>
<p>

**Abstract:** Information theoretic sensor management approaches are an ideal solution to state estimation problems when considering the optimal control of multi-agent systems, however they are too computationally intensive for large state spaces, especially when considering the limited computational resources typical of large-scale distributed multi-agent systems. Reinforcement learning (RL) is a promising alternative which can find approximate solutions to distributed optimal control problems that take into account the resource constraints inherent in many systems of distributed agents. However, the RL training can be prohibitively inefficient, especially in low-information environments where agents receive little to no feedback in large portions of the state space. We propose a hybrid information-driven multi-agent reinforcement learning (MARL) approach that utilizes information theoretic models as heuristics to help the agents navigate large sparse state spaces, coupled with information based rewards in an RL framework to learn higher-level policies. This paper presents our ongoing work towards this objective. Our preliminary findings show that such an approach can result in a system of agents that are approximately three orders of magnitude more efficient at exploring a sparse state space than naive baseline metrics. While the work is still in its early stages, it provides a promising direction for future research.

</p>
</details>

<details><summary><b>Semi-Supervised Disentanglement of Class-Related and Class-Independent Factors in VAE</b>
<a href="https://arxiv.org/abs/2102.00892">arxiv:2102.00892</a>
&#x1F4C8; 4 <br>
<p>Sina Hajimiri, Aryo Lotfi, Mahdieh Soleymani Baghshah</p></summary>
<p>

**Abstract:** In recent years, extending variational autoencoder's framework to learn disentangled representations has received much attention. We address this problem by proposing a framework capable of disentangling class-related and class-independent factors of variation in data. Our framework employs an attention mechanism in its latent space in order to improve the process of extracting class-related factors from data. We also deal with the multimodality of data distribution by utilizing mixture models as learnable prior distributions, as well as incorporating the Bhattacharyya coefficient in the objective function to prevent highly overlapping mixtures. Our model's encoder is further trained in a semi-supervised manner, with a small fraction of labeled data, to improve representations' interpretability. Experiments show that our framework disentangles class-related and class-independent factors of variation and learns interpretable features. Moreover, we demonstrate our model's performance with quantitative and qualitative results on various datasets.

</p>
</details>

<details><summary><b>On Scaling Contrastive Representations for Low-Resource Speech Recognition</b>
<a href="https://arxiv.org/abs/2102.00850">arxiv:2102.00850</a>
&#x1F4C8; 4 <br>
<p>Lasse Borgholt, Tycho Max Sylvester Tax, Jakob Drachmann Havtorn, Lars Maaløe, Christian Igel</p></summary>
<p>

**Abstract:** Recent advances in self-supervised learning through contrastive training have shown that it is possible to learn a competitive speech recognition system with as little as 10 minutes of labeled data. However, these systems are computationally expensive since they require pre-training followed by fine-tuning in a large parameter space. We explore the performance of such systems without fine-tuning by training a state-of-the-art speech recognizer on the fixed representations from the computationally demanding wav2vec 2.0 framework. We find performance to decrease without fine-tuning and, in the extreme low-resource setting, wav2vec 2.0 is inferior to its predecessor. In addition, we find that wav2vec 2.0 representations live in a low dimensional subspace and that decorrelating the features of the representations can stabilize training of the automatic speech recognizer. Finally, we propose a bidirectional extension to the original wav2vec framework that consistently improves performance.

</p>
</details>

<details><summary><b>Few-shot Image Classification with Multi-Facet Prototypes</b>
<a href="https://arxiv.org/abs/2102.00801">arxiv:2102.00801</a>
&#x1F4C8; 4 <br>
<p>Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert</p></summary>
<p>

**Abstract:** The aim of few-shot learning (FSL) is to learn how to recognize image categories from a small number of training examples. A central challenge is that the available training examples are normally insufficient to determine which visual features are most characteristic of the considered categories. To address this challenge, we organize these visual features into facets, which intuitively group features of the same kind (e.g. features that are relevant to shape, color, or texture). This is motivated from the assumption that (i) the importance of each facet differs from category to category and (ii) it is possible to predict facet importance from a pre-trained embedding of the category names. In particular, we propose an adaptive similarity measure, relying on predicted facet importance weights for a given set of categories. This measure can be used in combination with a wide array of existing metric-based methods. Experiments on miniImageNet and CUB show that our approach improves the state-of-the-art in metric-based FSL.

</p>
</details>

<details><summary><b>Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays</b>
<a href="https://arxiv.org/abs/2102.00781">arxiv:2102.00781</a>
&#x1F4C8; 4 <br>
<p>Rahul Kumar, Sandeep Mathias, Sriparna Saha, Pushpak Bhattacharyya</p></summary>
<p>

**Abstract:** Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay holistically while there has also been some work done on scoring individual essay traits. In this paper, we describe a way to score essays holistically using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. We also compare our results of the auxiliary task with such tasks done in other AEG systems. To find out which traits work best for different types of essays, we conduct ablation tests for each of the essay traits. We also report the runtime and number of training parameters for each system. We find that MTL-based BiLSTM system gives the best results for scoring the essay holistically, as well as performing well on scoring the essay traits.

</p>
</details>

<details><summary><b>Autonomous Navigation through intersections with Graph ConvolutionalNetworks and Conditional Imitation Learning for Self-driving Cars</b>
<a href="https://arxiv.org/abs/2102.00675">arxiv:2102.00675</a>
&#x1F4C8; 4 <br>
<p>Xiaodong Mei, Yuxiang Sun, Yuying Chen, Congcong Liu, Ming Liu</p></summary>
<p>

**Abstract:** In autonomous driving, navigation through unsignaled intersections with many traffic participants moving around is a challenging task. To provide a solution to this problem, we propose a novel branched network G-CIL for the navigation policy learning. Specifically, we firstly represent such dynamic environments as graph-structured data and propose an effective strategy for edge definition to aggregate surrounding information for the ego-vehicle. Then graph convolutional neural networks are used as the perception module to capture global and geometric features from the environment. To generate safe and efficient navigation policy, we further incorporate it with conditional imitation learning algorithm, to learn driving behaviors directly from expert demonstrations. Our proposed network is capable of handling a varying number of surrounding vehicles and generating optimal control actions (e.g., steering angle and throttle) according to the given high-level commands (e.g., turn left towards the global goal). Evaluations on unsignaled intersections with various traffic densities demonstrate that our end-to-end trainable neural network outperforms the baselines with higher success rate and shorter navigation time.

</p>
</details>

<details><summary><b>Forecasting Action through Contact Representations from First Person Video</b>
<a href="https://arxiv.org/abs/2102.00649">arxiv:2102.00649</a>
&#x1F4C8; 4 <br>
<p>Eadom Dessalene, Chinmaya Devaraj, Michael Maynord, Cornelia Fermuller, Yiannis Aloimonos</p></summary>
<p>

**Abstract:** Human actions involving hand manipulations are structured according to the making and breaking of hand-object contact, and human visual understanding of action is reliant on anticipation of contact as is demonstrated by pioneering work in cognitive science. Taking inspiration from this, we introduce representations and models centered on contact, which we then use in action prediction and anticipation. We annotate a subset of the EPIC Kitchens dataset to include time-to-contact between hands and objects, as well as segmentations of hands and objects. Using these annotations we train the Anticipation Module, a module producing Contact Anticipation Maps and Next Active Object Segmentations - novel low-level representations providing temporal and spatial characteristics of anticipated near future action. On top of the Anticipation Module we apply Egocentric Object Manipulation Graphs (Ego-OMG), a framework for action anticipation and prediction. Ego-OMG models longer term temporal semantic relations through the use of a graph modeling transitions between contact delineated action states. Use of the Anticipation Module within Ego-OMG produces state-of-the-art results, achieving 1st and 2nd place on the unseen and seen test sets, respectively, of the EPIC Kitchens Action Anticipation Challenge, and achieving state-of-the-art results on the tasks of action anticipation and action prediction over EPIC Kitchens. We perform ablation studies over characteristics of the Anticipation Module to evaluate their utility.

</p>
</details>

<details><summary><b>Student sentiment Analysis Using Classification With Feature Extraction Techniques</b>
<a href="https://arxiv.org/abs/2102.05439">arxiv:2102.05439</a>
&#x1F4C8; 3 <br>
<p>Latika Tamrakar, Dr. Padmavati Shrivastava, Dr. S. M. Ghosh</p></summary>
<p>

**Abstract:** Technical growths have empowered, numerous revolutions in the educational system by acquainting with technology into the classroom and by elevating the learning experience. Nowadays Web-based learning is getting much popularity. This paper describes the web-based learning and their effectiveness towards students. One of the prime factors in education or learning system is feedback; it is beneficial to learning if it must be used effectively. In this paper, we worked on how machine learning techniques like Logistic Regression (LR), Support Vector Machine (SVM), Naive Bayes (NB), Decision Tree (DT) can be applied over Web-based learning, emphasis given on sentiment present in the feedback students. We also work on two types of Feature Extraction Technique (FETs) namely Count Vector (CVr) or Bag of Words) (BoW) and Term Frequency and Inverse Document Frequency (TF-IDF) Vector. In the research study, it is our goal for our proposed LR, SVM, NB, and DT models to classify the presence of Student Feedback Dataset (SFB) with improved accuracy with cleaned dataset and feature extraction techniques. The SFB is one of the significant concerns among the student sentimental analysis.

</p>
</details>

<details><summary><b>Human-Machine Collaborative Video Coding Through Cuboidal Partitioning</b>
<a href="https://arxiv.org/abs/2102.01307">arxiv:2102.01307</a>
&#x1F4C8; 3 <br>
<p>Ashek Ahmmed, Manoranjan Paul, Manzur Murshed, David Taubman</p></summary>
<p>

**Abstract:** Video coding algorithms encode and decode an entire video frame while feature coding techniques only preserve and communicate the most critical information needed for a given application. This is because video coding targets human perception, while feature coding aims for machine vision tasks. Recently, attempts are being made to bridge the gap between these two domains. In this work, we propose a video coding framework by leveraging on to the commonality that exists between human vision and machine vision applications using cuboids. This is because cuboids, estimated rectangular regions over a video frame, are computationally efficient, has a compact representation and object centric. Such properties are already shown to add value to traditional video coding systems. Herein cuboidal feature descriptors are extracted from the current frame and then employed for accomplishing a machine vision task in the form of object detection. Experimental results show that a trained classifier yields superior average precision when equipped with cuboidal features oriented representation of the current test frame. Additionally, this representation costs $7\%$ less in bit rate if the captured frames are need be communicated to a receiver.

</p>
</details>

<details><summary><b>Detection of Racial Bias from Physiological Responses</b>
<a href="https://arxiv.org/abs/2102.01287">arxiv:2102.01287</a>
&#x1F4C8; 3 <br>
<p>Fateme Nikseresht, Runze Yan, Rachel Lew, Yingzheng Liu, Rose M. Sebastian, Afsaneh Doryab</p></summary>
<p>

**Abstract:** Despite the evolution of norms and regulations to mitigate the harm from biases, harmful discrimination linked to an individual's unconscious biases persists. Our goal is to better understand and detect the physiological and behavioral indicators of implicit biases. This paper investigates whether we can reliably detect racial bias from physiological responses, including heart rate, conductive skin response, skin temperature, and micro-body movements. We analyzed data from 46 subjects whose physiological data was collected with Empatica E4 wristband while taking an Implicit Association Test (IAT). Our machine learning and statistical analysis show that implicit bias can be predicted from physiological signals with 76.1% accuracy. Our results also show that the EDA signal associated with skin response has the strongest correlation with racial bias and that there are significant differences between the values of EDA features for biased and unbiased participants.

</p>
</details>

<details><summary><b>Time Adaptive Gaussian Model</b>
<a href="https://arxiv.org/abs/2102.01238">arxiv:2102.01238</a>
&#x1F4C8; 3 <br>
<p>Federico Ciech, Veronica Tozzo</p></summary>
<p>

**Abstract:** Multivariate time series analysis is becoming an integral part of data analysis pipelines. Understanding the individual time point connections between covariates as well as how these connections change in time is non-trivial. To this aim, we propose a novel method that leverages on Hidden Markov Models and Gaussian Graphical Models -- Time Adaptive Gaussian Model (TAGM). Our model is a generalization of state-of-the-art methods for the inference of temporal graphical models, its formulation leverages on both aspects of these models providing better results than current methods. In particular,it performs pattern recognition by clustering data points in time; and, it finds probabilistic (and possibly causal) relationships among the observed variables. Compared to current methods for temporal network inference, it reduces the basic assumptions while still showing good inference performances.

</p>
</details>

<details><summary><b>Doubly Robust Thompson Sampling for linear payoffs</b>
<a href="https://arxiv.org/abs/2102.01229">arxiv:2102.01229</a>
&#x1F4C8; 3 <br>
<p>Wonyoung Kim, Gi-soo Kim, Myunghee Cho Paik</p></summary>
<p>

**Abstract:** A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. Since the arm choice depends on the past context and reward pairs, the contexts of chosen arms suffer from correlation and render the analysis difficult. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling (TS) that applies the DR technique used in missing data literature to TS. The proposed algorithm improves the bound of TS by a factor of $\sqrt{d}$, where $d$ is the dimension of the context. A benefit of the proposed method is that it uses all the context data, chosen or not chosen, thus allowing to circumvent the technical definition of unsaturated arms used in theoretical analysis of TS. Empirical studies show the advantage of the proposed algorithm over TS.

</p>
</details>

<details><summary><b>Visual Framing of Science Conspiracy Videos: Integrating Machine Learning with Communication Theories to Study the Use of Color and Brightness</b>
<a href="https://arxiv.org/abs/2102.01163">arxiv:2102.01163</a>
&#x1F4C8; 3 <br>
<p>Kaiping Chen, Sang Jung Kim, Qiantong Gao, Sebastian Raschka</p></summary>
<p>

**Abstract:** Recent years have witnessed an explosion of science conspiracy videos on the Internet, challenging science epistemology and public understanding of science. Scholars have started to examine the persuasion techniques used in conspiracy messages such as uncertainty and fear yet, little is understood about the visual narratives, especially how visual narratives differ in videos that debunk conspiracies versus those that propagate conspiracies. This paper addresses this gap in understanding visual framing in conspiracy videos through analyzing millions of frames from conspiracy and counter-conspiracy YouTube videos using computational methods. We found that conspiracy videos tended to use lower color variance and brightness, especially in thumbnails and earlier parts of the videos. This paper also demonstrates how researchers can integrate textual and visual features in machine learning models to study conspiracies on social media and discusses the implications of computational modeling for scholars interested in studying visual manipulation in the digital era. The analysis of visual and textual features presented in this paper could be useful for future studies focused on designing systems to identify conspiracy content on the Internet.

</p>
</details>

<details><summary><b>Deep Music Information Dynamics</b>
<a href="https://arxiv.org/abs/2102.01133">arxiv:2102.01133</a>
&#x1F4C8; 3 <br>
<p>Shlomo Dubnov</p></summary>
<p>

**Abstract:** Music comprises of a set of complex simultaneous events organized in time. In this paper we introduce a novel framework that we call Deep Musical Information Dynamics, which combines two parallel streams - a low rate latent representation stream that is assumed to capture the dynamics of a thought process contrasted with a higher rate information dynamics derived from the musical data itself. Motivated by rate-distortion theories of human cognition we propose a framework for exploring possible relations between imaginary anticipations existing in the listener's mind and information dynamics of the musical surface itself. This model is demonstrated for the case of symbolic (MIDI) data, as accounting for acoustic surface would require many more layers to capture instrument properties and performance expressive inflections. The mathematical framework is based on variational encoding that first establishes a high rate representation of the musical observations, which is then reduced using a bit-allocation method into a parallel low rate data stream. The combined loss considered here includes both the information rate in terms of time evolution for each stream, and the fidelity of encoding measured in terms of mutual information between the high and low rate representations. In the simulations presented in the paper we are able to juxtapose aspects of latent/imaginary surprisal versus surprisal of the music surface in a manner that is quantifiable and computationally tractable. The set of computational tools is discussed in the paper, suggesting that a trade off between compression and prediction are an important factor in the analysis and design of time-based music generative models.

</p>
</details>

<details><summary><b>Machine-Learned Phase Diagrams of Generalized Kitaev Honeycomb Magnets</b>
<a href="https://arxiv.org/abs/2102.01103">arxiv:2102.01103</a>
&#x1F4C8; 3 <br>
<p>Nihal Rao, Ke Liu, Marc Machaczek, Lode Pollet</p></summary>
<p>

**Abstract:** We use a recently developed interpretable and unsupervised machine-learning method, the tensorial kernel support vector machine (TK-SVM), to investigate the low-temperature classical phase diagram of a generalized Heisenberg-Kitaev-$Γ$ ($J$-$K$-$Γ$) model on a honeycomb lattice. Aside from reproducing phases reported by previous quantum and classical studies, our machine finds a hitherto missed nested zigzag-stripy order and establishes the robustness of a recently identified modulated $S_3 \times Z_3$ phase, which emerges through the competition between the Kitaev and $Γ$ spin liquids, against Heisenberg interactions. The results imply that, in the restricted parameter space spanned by the three primary exchange interactions -- $J$, $K$, and $Γ$, the representative Kitaev material $α$-${\rm RuCl}_3$ lies close to the boundaries of several phases, including a simple ferromagnet, the unconventional $S_3 \times Z_3$ and nested zigzag-stripy magnets. A zigzag order is stabilized by a finite $Γ^{\prime}$ and/or $J_3$ term, whereas the four magnetic orders may compete in particular if $Γ^{\prime}$ is anti-ferromagnetic.

</p>
</details>

<details><summary><b>Automatic Detection of B-lines in Lung Ultrasound Videos From Severe Dengue Patients</b>
<a href="https://arxiv.org/abs/2102.01059">arxiv:2102.01059</a>
&#x1F4C8; 3 <br>
<p>Hamideh Kerdegari, Phung Tran Huy Nhat, Angela McBride, VITAL Consortium, Reza Razavi, Nguyen Van Hao, Louise Thwaites, Sophie Yacoub, Alberto Gomez</p></summary>
<p>

**Abstract:** Lung ultrasound (LUS) imaging is used to assess lung abnormalities, including the presence of B-line artefacts due to fluid leakage into the lungs caused by a variety of diseases. However, manual detection of these artefacts is challenging. In this paper, we propose a novel methodology to automatically detect and localize B-lines in LUS videos using deep neural networks trained with weak labels. To this end, we combine a convolutional neural network (CNN) with a long short-term memory (LSTM) network and a temporal attention mechanism. Four different models are compared using data from 60 patients. Results show that our best model can determine whether one-second clips contain B-lines or not with an F1 score of 0.81, and extracts a representative frame with B-lines with an accuracy of 87.5%.

</p>
</details>

<details><summary><b>Scalable, End-to-End, Deep-Learning-Based Data Reconstruction Chain for Particle Imaging Detectors</b>
<a href="https://arxiv.org/abs/2102.01033">arxiv:2102.01033</a>
&#x1F4C8; 3 <br>
<p>Francois Drielsma, Kazuhiro Terao, Laura Dominé, Dae Heun Koh</p></summary>
<p>

**Abstract:** Recent inroads in Computer Vision (CV) and Machine Learning (ML) have motivated a new approach to the analysis of particle imaging detector data. Unlike previous efforts which tackled isolated CV tasks, this paper introduces an end-to-end, ML-based data reconstruction chain for Liquid Argon Time Projection Chambers (LArTPCs), the state-of-the-art in precision imaging at the intensity frontier of neutrino physics. The chain is a multi-task network cascade which combines voxel-level feature extraction using Sparse Convolutional Neural Networks and particle superstructure formation using Graph Neural Networks. Each algorithm incorporates physics-informed inductive biases, while their collective hierarchy is used to enforce a causal structure. The output is a comprehensive description of an event that may be used for high-level physics inference. The chain is end-to-end optimizable, eliminating the need for time-intensive manual software adjustments. It is also the first implementation to handle the unprecedented pile-up of dozens of high energy neutrino interactions, expected in the 3D-imaging LArTPC of the Deep Underground Neutrino Experiment. The chain is trained as a whole and its performance is assessed at each step using an open simulated data set.

</p>
</details>

<details><summary><b>Quantum Inspired Adaptive Boosting</b>
<a href="https://arxiv.org/abs/2102.00949">arxiv:2102.00949</a>
&#x1F4C8; 3 <br>
<p>Bálint Daróczy, Katalin Friedl, László Kabódi, Attila Pereszlényi, Dániel Szabó</p></summary>
<p>

**Abstract:** Building on the quantum ensemble based classifier algorithm of Schuld and Petruccione [arXiv:1704.02146v1], we devise equivalent classical algorithms which show that this quantum ensemble method does not have advantage over classical algorithms. Essentially, we simplify their algorithm until it is intuitive to come up with an equivalent classical version. One of the classical algorithms is extremely simple and runs in constant time for each input to be classified. We further develop the idea and, as the main contribution of the paper, we propose methods inspired by combining the quantum ensemble method with adaptive boosting. The algorithms were tested and found to be comparable to the AdaBoost algorithm on publicly available data sets.

</p>
</details>

<details><summary><b>DRLDO: A novel DRL based De-ObfuscationSystem for Defense against Metamorphic Malware</b>
<a href="https://arxiv.org/abs/2102.00898">arxiv:2102.00898</a>
&#x1F4C8; 3 <br>
<p>Mohit Sewak, Sanjay K. Sahay, Hemant Rathore</p></summary>
<p>

**Abstract:** In this paper, we propose a novel mechanism to normalize metamorphic and obfuscated malware down at the opcode level and hence create an advanced metamorphic malware de-obfuscation and defense system. We name this system DRLDO, for Deep Reinforcement Learning based De-Obfuscator. With the inclusion of the DRLDO as a sub-component, an existing Intrusion Detection System could be augmented with defensive capabilities against 'zero-day' attacks from obfuscated and metamorphic variants of existing malware. This gains importance, not only because there exists no system to date that uses advanced DRL to intelligently and automatically normalize obfuscation down even to the opcode level, but also because the DRLDO system does not mandate any changes to the existing IDS. The DRLDO system does not even mandate the IDS' classifier to be retrained with any new dataset containing obfuscated samples. Hence DRLDO could be easily retrofitted into any existing IDS deployment. We designed, developed, and conducted experiments on the system to evaluate the same against multiple-simultaneous attacks from obfuscations generated from malware samples from a standardized dataset that contains multiple generations of malware. Experimental results prove that DRLDO was able to successfully make the otherwise un-detectable obfuscated variants of the malware detectable by an existing pre-trained malware classifier. The detection probability was raised well above the cut-off mark to 0.6 for the classifier to detect the obfuscated malware unambiguously. Further, the de-obfuscated variants generated by DRLDO achieved a very high correlation (of 0.99) with the base malware. This observation validates that the DRLDO system is actually learning to de-obfuscate and not exploiting a trivial trick.

</p>
</details>

<details><summary><b>An End-To-End-Trainable Iterative Network Architecture for Accelerated Radial Multi-Coil 2D Cine MR Image Reconstruction</b>
<a href="https://arxiv.org/abs/2102.00783">arxiv:2102.00783</a>
&#x1F4C8; 3 <br>
<p>Andreas Kofler, Markus Haltmeier, Tobias Schaeffter, Christoph Kolbitsch</p></summary>
<p>

**Abstract:** Purpose: Iterative Convolutional Neural Networks (CNNs) which resemble unrolled learned iterative schemes have shown to consistently deliver state-of-the-art results for image reconstruction problems across different imaging modalities. However, because these methodes include the forward model in the architecture, their applicability is often restricted to either relatively small reconstruction problems or to problems with operators which are computationally cheap to compute. As a consequence, they have so far not been applied to dynamic non-Cartesian multi-coil reconstruction problems. Methods: In this work, we propose a CNN-architecture for image reconstruction of accelerated 2D radial cine MRI with multiple receiver coils. The network is based on a computationally light CNN-component and a subsequent conjugate gradient (CG) method which can be jointly trained end-to-end using an efficient training strategy. We investigate the proposed training-strategy and compare our method to other well-known reconstruction techniques with learned and non-learned regularization methods. Results: Our proposed method outperforms all other methods based on non-learned regularization. Further, it performs similar or better than a CNN-based method employing a 3D U-Net and a method using adaptive dictionary learning. In addition, we empirically demonstrate that even by training the network with only iteration, it is possible to increase the length of the network at test time and further improve the results. Conclusions: End-to-end training allows to highly reduce the number of trainable parameters of and stabilize the reconstruction network. Further, because it is possible to change the length of the network at test time, the need to find a compromise between the complexity of the CNN-block and the number of iterations in each CG-block becomes irrelevant.

</p>
</details>

<details><summary><b>Generalized non-stationary bandits</b>
<a href="https://arxiv.org/abs/2102.00725">arxiv:2102.00725</a>
&#x1F4C8; 3 <br>
<p>Anne Gael Manegueu, Alexandra Carpentier, Yi Yu</p></summary>
<p>

**Abstract:** In this paper, we study a non-stationary stochastic bandit problem, which generalizes the switching bandit problem. On top of the switching bandit problem (\textbf{Case a}), we are interested in three concrete examples: (\textbf{b}) the means of the arms are local polynomials, (\textbf{c}) the means of the arms are locally smooth, and (\textbf{d}) the gaps of the arms have a bounded number of inflexion points and where the highest arm mean cannot vary too much in a short range. These three settings are very different, but have in common the following: (i) the number of similarly-sized level sets of the logarithm of the gaps can be controlled, and (ii) the highest mean has a limited number of abrupt changes, and otherwise has limited variations. We propose a single algorithm in this general setting, that in particular solves in an efficient and unified way the four problems (a)-(d) mentioned.

</p>
</details>

<details><summary><b>Binary Classification from Multiple Unlabeled Datasets via Surrogate Set Classification</b>
<a href="https://arxiv.org/abs/2102.00678">arxiv:2102.00678</a>
&#x1F4C8; 3 <br>
<p>Nan Lu, Shida Lei, Gang Niu, Issei Sato, Masashi Sugiyama</p></summary>
<p>

**Abstract:** To cope with high annotation costs, training a classifier only from weakly supervised data has attracted a great deal of attention these days. Among various approaches, strengthening supervision from completely unsupervised classification is a promising direction, which typically employs class priors as the only supervision and trains a binary classifier from unlabeled (U) datasets. While existing risk-consistent methods are theoretically grounded with high flexibility, they can learn only from two U sets. In this paper, we propose a new approach for binary classification from $m$ U-sets for $m\ge2$. Our key idea is to consider an auxiliary classification task called surrogate set classification (SSC), which is aimed at predicting from which U set each observed data is drawn. SSC can be solved by a standard (multi-class) classification method, and we use the SSC solution to obtain the final binary classifier through a certain linear-fractional transformation. We built our method in a flexible and efficient end-to-end deep learning framework and prove it to be classifier-consistent. Through experiments, we demonstrate the superiority of our proposed method over state-of-the-art methods.

</p>
</details>

<details><summary><b>Probabilistic Learning Vector Quantization on Manifold of Symmetric Positive Definite Matrices</b>
<a href="https://arxiv.org/abs/2102.00667">arxiv:2102.00667</a>
&#x1F4C8; 3 <br>
<p>Fengzhen Tang, Haifeng Feng, Peter Tino, Bailu Si, Daxiong Ji</p></summary>
<p>

**Abstract:** In this paper, we develop a new classification method for manifold-valued data in the framework of probabilistic learning vector quantization. In many classification scenarios, the data can be naturally represented by symmetric positive definite matrices, which are inherently points that live on a curved Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, traditional Euclidean machine learning algorithms yield poor results on such data. In this paper, we generalize the probabilistic learning vector quantization algorithm for data points living on the manifold of symmetric positive definite matrices equipped with Riemannian natural metric (affine-invariant metric). By exploiting the induced Riemannian distance, we derive the probabilistic learning Riemannian space quantization algorithm, obtaining the learning rule through Riemannian gradient descent. Empirical investigations on synthetic data, image data , and motor imagery EEG data demonstrate the superior performance of the proposed method.

</p>
</details>

<details><summary><b>Densely Connected Recurrent Residual (Dense R2UNet) Convolutional Neural Network for Segmentation of Lung CT Images</b>
<a href="https://arxiv.org/abs/2102.00663">arxiv:2102.00663</a>
&#x1F4C8; 3 <br>
<p>Kaushik Dutta</p></summary>
<p>

**Abstract:** Deep Learning networks have established themselves as providing state of art performance for semantic segmentation. These techniques are widely applied specifically to medical detection, segmentation and classification. The advent of the U-Net based architecture has become particularly popular for this application. In this paper we present the Dense Recurrent Residual Convolutional Neural Network(Dense R2U CNN) which is a synthesis of Recurrent CNN, Residual Network and Dense Convolutional Network based on the U-Net model architecture. The residual unit helps training deeper network, while the dense recurrent layers enhances feature propagation needed for segmentation. The proposed model tested on the benchmark Lung Lesion dataset showed better performance on segmentation tasks than its equivalent models.

</p>
</details>

<details><summary><b>Towards Speeding up Adversarial Training in Latent Spaces</b>
<a href="https://arxiv.org/abs/2102.00662">arxiv:2102.00662</a>
&#x1F4C8; 3 <br>
<p>Yaguan Qian, Qiqi Shao, Tengteng Yao, Bin Wang, Shouling Ji, Shaoning Zeng, Zhaoquan Gu, Wassim Swaileh</p></summary>
<p>

**Abstract:** Adversarial training is wildly considered as one of the most effective way to defend against adversarial examples. However, existing adversarial training methods consume unbearable time, due to the fact that they need to generate adversarial examples in the large input space. To speed up adversarial training, we propose a novel adversarial training method that does not need to generate real adversarial examples. By adding perturbations to logits to generate Endogenous Adversarial Examples (EAEs) -- the adversarial examples in the latent space, the time consuming gradient calculation can be avoided. Extensive experiments are conducted on CIFAR-10 and ImageNet, and the results show that comparing to state-of-the-art methods, our EAE adversarial training not only shortens the training time, but also enhances the robustness of the model and has less impact on the accuracy of clean examples than the existing methods.

</p>
</details>

<details><summary><b>Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective</b>
<a href="https://arxiv.org/abs/2102.00650">arxiv:2102.00650</a>
&#x1F4C8; 3 <br>
<p>Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, Qian Zhang</p></summary>
<p>

**Abstract:** Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies \citep{muller2019does,yuan2020revisiting} revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available at \url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.

</p>
</details>

<details><summary><b>Layer-based Composite Reputation Bootstrapping</b>
<a href="https://arxiv.org/abs/2102.09951">arxiv:2102.09951</a>
&#x1F4C8; 2 <br>
<p>Sajib Mistry, Athman Bouguettaya, Lie Qu</p></summary>
<p>

**Abstract:** We propose a novel generic reputation bootstrapping framework for composite services. Multiple reputation-related indicators are considered in a layer-based framework to implicitly reflect the reputation of the component services. The importance of an indicator on the future performance of a component service is learned using a modified Random Forest algorithm. We propose a topology-aware Forest Deep Neural Network (fDNN) to find the correlations between the reputation of a composite service and reputation indicators of component services. The trained fDNN model predicts the reputation of a new composite service with the confidence value. Experimental results with real-world dataset prove the efficiency of the proposed approach.

</p>
</details>

<details><summary><b>Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation</b>
<a href="https://arxiv.org/abs/2102.01295">arxiv:2102.01295</a>
&#x1F4C8; 2 <br>
<p>Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi</p></summary>
<p>

**Abstract:** A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulation tasks using a general-purpose robot manipulator and improves computational efficiency.

</p>
</details>

<details><summary><b>Mobile-end Tone Mapping based on Integral Image and Integral Histogram</b>
<a href="https://arxiv.org/abs/2102.01289">arxiv:2102.01289</a>
&#x1F4C8; 2 <br>
<p>Jie Yang, Mengchen Lin, Ziyi Liu, Ulian Shahnovich, Orly Yadid-Pecht</p></summary>
<p>

**Abstract:** Wide dynamic range (WDR) image tone mapping is in high demand in many applications like film production, security monitoring, and photography. It is especially crucial for mobile devices because most of the images taken today are from mobile phones, hence such technology is highly demanded in the consumer market of mobile devices and is essential for a good customer experience. However, high-quality and high-performance WDR image tone mapping implementations are rarely found in the mobile-end. In this paper, we introduce a high performance, mobile-end WDR image tone mapping implementation. It leverages the tone mapping results of multiple receptive fields and calculates a suitable value for each pixel. The utilization of integral image and integral histogram significantly reduce the required computation. Moreover, GPU parallel computation is used to increase the processing speed. The experimental results indicate that our implementation can process a high-resolution WDR image within a second on mobile devices and produce appealing image quality.

</p>
</details>

<details><summary><b>Single Model Deep Learning on Imbalanced Small Datasets for Skin Lesion Classification</b>
<a href="https://arxiv.org/abs/2102.01284">arxiv:2102.01284</a>
&#x1F4C8; 2 <br>
<p>Peng Yao, Shuwei Shen, Mengjuan Xu, Peng Liu, Fan Zhang, Jinyu Xing, Pengfei Shao, Benjamin Kaffenberger, Ronald X. Xu</p></summary>
<p>

**Abstract:** Deep convolutional neural network (DCNN) models have been widely explored for skin disease diagnosis and some of them have achieved the diagnostic outcomes comparable or even superior to those of dermatologists. However, broad implementation of DCNN in skin disease detection is hindered by small size and data imbalance of the publically accessible skin lesion datasets. This paper proposes a novel data augmentation strategy for single model classification of skin lesions based on a small and imbalanced dataset. First, various DCNNs are trained on this dataset to show that the models with moderate complexity outperform the larger models. Second, regularization DropOut and DropBlock are added to reduce overfitting and a Modified RandAugment augmentation strategy is proposed to address the defects of sample underrepresentation in the small dataset. Finally, a novel Multi-Weighted Focal Loss function is introduced to overcome the challenge of uneven sample size and classification difficulty. By combining Modified RandAugment and Multi-weighted Focal Loss in a single DCNN model, we have achieved the classification accuracy comparable to those of multiple ensembling models on the ISIC 2018 challenge test dataset. Our study shows that this method is able to achieve a high classification performance at a low cost of computational resources and inference time, potentially suitable to implement in mobile devices for automated screening of skin lesions and many other malignancies in low resource settings.

</p>
</details>

<details><summary><b>Inducing Meaningful Units from Character Sequences with Slot Attention</b>
<a href="https://arxiv.org/abs/2102.01223">arxiv:2102.01223</a>
&#x1F4C8; 2 <br>
<p>Melika Behjati, James Henderson</p></summary>
<p>

**Abstract:** Characters do not convey meaning, but sequences of characters do. We propose an unsupervised distributional method to learn the abstract meaning-bearing units in a sequence of characters. Rather than segmenting the sequence, this model discovers continuous representations of the "objects" in the sequence, using a recently proposed architecture for object discovery in images called Slot Attention. We train our model on different languages and evaluate the quality of the obtained representations with probing classifiers. Our experiments show promising results in the ability of our units to capture meaning at a higher level of abstraction.

</p>
</details>

<details><summary><b>Fast Training of Provably Robust Neural Networks by SingleProp</b>
<a href="https://arxiv.org/abs/2102.01208">arxiv:2102.01208</a>
&#x1F4C8; 2 <br>
<p>Akhilan Boopathy, Tsui-Wei Weng, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, Luca Daniel</p></summary>
<p>

**Abstract:** Recent works have developed several methods of defending neural networks against adversarial attacks with certified guarantees. However, these techniques can be computationally costly due to the use of certification during training. We develop a new regularizer that is both more efficient than existing certified defenses, requiring only one additional forward propagation through a network, and can be used to train networks with similar certified accuracy. Through experiments on MNIST and CIFAR-10 we demonstrate improvements in training speed and comparable certified accuracy compared to state-of-the-art certified defenses.

</p>
</details>

<details><summary><b>Reconstruction and Segmentation of Parallel MR Data using Image Domain DEEP-SLR</b>
<a href="https://arxiv.org/abs/2102.01172">arxiv:2102.01172</a>
&#x1F4C8; 2 <br>
<p>Aniket Pramanik, Mathews Jacob</p></summary>
<p>

**Abstract:** The main focus of this work is a novel framework for the joint reconstruction and segmentation of parallel MRI (PMRI) brain data. We introduce an image domain deep network for calibrationless recovery of undersampled PMRI data. The proposed approach is the deep-learning (DL) based generalization of local low-rank based approaches for uncalibrated PMRI recovery including CLEAR [6]. Since the image domain approach exploits additional annihilation relations compared to k-space based approaches, we expect it to offer improved performance. To minimize segmentation errors resulting from undersampling artifacts, we combined the proposed scheme with a segmentation network and trained it in an end-to-end fashion. In addition to reducing segmentation errors, this approach also offers improved reconstruction performance by reducing overfitting; the reconstructed images exhibit reduced blurring and sharper edges than independently trained reconstruction network.

</p>
</details>

<details><summary><b>Improving Distantly-Supervised Relation Extraction through BERT-based Label & Instance Embeddings</b>
<a href="https://arxiv.org/abs/2102.01156">arxiv:2102.01156</a>
&#x1F4C8; 2 <br>
<p>Despina Christou, Grigorios Tsoumakas</p></summary>
<p>

**Abstract:** Distantly-supervised relation extraction (RE) is an effective method to scale RE to large corpora but suffers from noisy labels. Existing approaches try to alleviate noise through multi-instance learning and by providing additional information, but manage to recognize mainly the top frequent relations, neglecting those in the long-tail. We propose REDSandT (Relation Extraction with Distant Supervision and Transformers), a novel distantly-supervised transformer-based RE method, that manages to capture a wider set of relations through highly informative instance and label embeddings for RE, by exploiting BERT's pre-trained model, and the relationship between labels and entities, respectively. We guide REDSandT to focus solely on relational tokens by fine-tuning BERT on a structured input, including the sub-tree connecting an entity pair and the entities' types. Using the extracted informative vectors, we shape label embeddings, which we also use as attention mechanism over instances to further reduce noise. Finally, we represent sentences by concatenating relation and instance embeddings. Experiments in the NYT-10 dataset show that REDSandT captures a broader set of relations with higher confidence, achieving state-of-the-art AUC (0.424).

</p>
</details>

<details><summary><b>SGD Generalizes Better Than GD (And Regularization Doesn't Help)</b>
<a href="https://arxiv.org/abs/2102.01117">arxiv:2102.01117</a>
&#x1F4C8; 2 <br>
<p>Idan Amir, Tomer Koren, Roi Livni</p></summary>
<p>

**Abstract:** We give a new separation result between the generalization performance of stochastic gradient descent (SGD) and of full-batch gradient descent (GD) in the fundamental stochastic convex optimization model. While for SGD it is well-known that $O(1/ε^2)$ iterations suffice for obtaining a solution with $ε$ excess expected risk, we show that with the same number of steps GD may overfit and emit a solution with $Ω(1)$ generalization error. Moreover, we show that in fact $Ω(1/ε^4)$ iterations are necessary for GD to match the generalization performance of SGD, which is also tight due to recent work by Bassily et al. (2020). We further discuss how regularizing the empirical risk minimized by GD essentially does not change the above result, and revisit the concepts of stability, implicit bias and the role of the learning algorithm in generalization.

</p>
</details>

<details><summary><b>Revisiting the Prepositional-Phrase Attachment Problem Using Explicit Commonsense Knowledge</b>
<a href="https://arxiv.org/abs/2102.00924">arxiv:2102.00924</a>
&#x1F4C8; 2 <br>
<p>Yida Xin, Henry Lieberman, Peter Chin</p></summary>
<p>

**Abstract:** We revisit the challenging problem of resolving prepositional-phrase (PP) attachment ambiguity. To date, proposed solutions are either rule-based, where explicit grammar rules direct how to resolve ambiguities; or statistical, where the decision is learned from a corpus of labeled examples. We argue that explicit commonsense knowledge bases can provide an essential ingredient for making good attachment decisions. We implemented a module, named Patch-Comm, that can be used by a variety of conventional parsers, to make attachment decisions. Where the commonsense KB does not provide direct answers, we fall back on a more general system that infers "out-of-knowledge-base" assertions in a manner similar to the way some NLP systems handle out-of-vocabulary words. Our results suggest that the commonsense knowledge-based approach can provide the best of both worlds, integrating rule-based and statistical techniques. As the field is increasingly coming to recognize the importance of explainability in AI, a commonsense approach can enable NLP developers to better understand the behavior of systems, and facilitate natural dialogues with end users.

</p>
</details>

<details><summary><b>The Complexity of Learning Linear Temporal Formulas from Examples</b>
<a href="https://arxiv.org/abs/2102.00876">arxiv:2102.00876</a>
&#x1F4C8; 2 <br>
<p>Nathanaël Fijalkow, Guillaume Lagarde</p></summary>
<p>

**Abstract:** In this paper we initiate the study of the computational complexity of learning linear temporal logic (LTL) formulas from examples. We construct approximation algorithms for fragments of LTL and prove hardness results; in particular we obtain tight bounds for approximation of the fragment containing only the next operator and conjunctions, and prove NP-completeness results for many fragments.

</p>
</details>

<details><summary><b>Painless step size adaptation for SGD</b>
<a href="https://arxiv.org/abs/2102.00853">arxiv:2102.00853</a>
&#x1F4C8; 2 <br>
<p>Ilona Kulikovskikh, Tarzan Legović</p></summary>
<p>

**Abstract:** Convergence and generalization are two crucial aspects of performance in neural networks. When analyzed separately, these properties may lead to contradictory results. Optimizing a convergence rate yields fast training, but does not guarantee the best generalization error. To avoid the conflict, recent studies suggest adopting a moderately large step size for optimizers, but the added value on the performance remains unclear. We propose the LIGHT function with the four configurations which regulate explicitly an improvement in convergence and generalization on testing. This contribution allows to: 1) improve both convergence and generalization of neural networks with no need to guarantee their stability; 2) build more reliable and explainable network architectures with no need for overparameterization. We refer to it as "painless" step size adaptation.

</p>
</details>

<details><summary><b>A Novel Approach for Classification and Forecasting of Time Series in Particle Accelerators</b>
<a href="https://arxiv.org/abs/2102.00786">arxiv:2102.00786</a>
&#x1F4C8; 2 <br>
<p>Sichen Li, Mélissa Zacharias, Jochem Snuverink, Jaime Coello de Portugal, Fernando Perez-Cruz, Davide Reggiani, Andreas Adelmann</p></summary>
<p>

**Abstract:** The beam interruptions (interlocks) of particle accelerators, despite being necessary safety measures, lead to abrupt operational changes and a substantial loss of beam time. A novel time series classification approach is applied to decrease beam time loss in the High Intensity Proton Accelerator complex by forecasting interlock events. The forecasting is performed through binary classification of windows of multivariate time series. The time series are transformed into Recurrence Plots which are then classified by a Convolutional Neural Network, which not only captures the inner structure of the time series but also utilizes the advances of image classification techniques. Our best performing interlock-to-stable classifier reaches an Area under the ROC Curve value of $0.71 \pm 0.01$ compared to $0.65 \pm 0.01$ of a Random Forest model, and it can potentially reduce the beam time loss by $0.5 \pm 0.2$ seconds per interlock.

</p>
</details>

<details><summary><b>Towards Explainable Exploratory Landscape Analysis: Extreme Feature Selection for Classifying BBOB Functions</b>
<a href="https://arxiv.org/abs/2102.00736">arxiv:2102.00736</a>
&#x1F4C8; 2 <br>
<p>Quentin Renau, Johann Dreo, Carola Doerr, Benjamin Doerr</p></summary>
<p>

**Abstract:** Facilitated by the recent advances of Machine Learning (ML), the automated design of optimization heuristics is currently shaking up evolutionary computation (EC). Where the design of hand-picked guidelines for choosing a most suitable heuristic has long dominated research activities in the field, automatically trained heuristics are now seen to outperform human-derived choices even for well-researched optimization tasks. ML-based EC is therefore not any more a futuristic vision, but has become an integral part of our community.
  A key criticism that ML-based heuristics are often faced with is their potential lack of explainability, which may hinder future developments. This applies in particular to supervised learning techniques which extrapolate algorithms' performance based on exploratory landscape analysis (ELA). In such applications, it is not uncommon to use dozens of problem features to build the models underlying the specific algorithm selection or configuration task. Our goal in this work is to analyze whether this many features are indeed needed. Using the classification of the BBOB test functions as testbed, we show that a surprisingly small number of features -- often less than four -- can suffice to achieve a 98\% accuracy. Interestingly, the number of features required to meet this threshold is found to decrease with the problem dimension. We show that the classification accuracy transfers to settings in which several instances are involved in training and testing. In the leave-one-instance-out setting, however, classification accuracy drops significantly, and the transformation-invariance of the features becomes a decisive success factor.

</p>
</details>

<details><summary><b>NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.00714">arxiv:2102.00714</a>
&#x1F4C8; 2 <br>
<p>Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang, Yang Yu</p></summary>
<p>

**Abstract:** Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running a highly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be well validated before deployment. In this paper, we present a near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for policy validation. We evaluate existing offline RL algorithms on NeoRL and argue that the performance of a policy should also be compared with the deterministic version of the behavior policy, instead of the dataset reward. The empirical results demonstrate that the tested offline RL algorithms become less competitive to the deterministic policy on many datasets, and the offline policy evaluation hardly helps. The NeoRL suit can be found at http://polixir.ai/research/neorl. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.

</p>
</details>

<details><summary><b>Commonsense Knowledge Mining from Term Definitions</b>
<a href="https://arxiv.org/abs/2102.00651">arxiv:2102.00651</a>
&#x1F4C8; 2 <br>
<p>Zhicheng Liang, Deborah L. McGuinness</p></summary>
<p>

**Abstract:** Commonsense knowledge has proven to be beneficial to a variety of application areas, including question answering and natural language understanding. Previous work explored collecting commonsense knowledge triples automatically from text to increase the coverage of current commonsense knowledge graphs. We investigate a few machine learning approaches to mining commonsense knowledge triples using dictionary term definitions as inputs and provide some initial evaluation of the results. We start from extracting candidate triples using part-of-speech tag patterns from text, and then compare the performance of three existing models for triple scoring. Our experiments show that term definitions contain some valid and novel commonsense knowledge triples for some semantic relations, and also indicate some challenges with using existing triple scoring models.

</p>
</details>

<details><summary><b>End-to-end deep meta modelling to calibrate and optimize energy consumption and comfort</b>
<a href="https://arxiv.org/abs/2105.02814">arxiv:2105.02814</a>
&#x1F4C8; 1 <br>
<p>Max Cohen, Sylvain Le Corff, Maurice Charbit, Marius Preda, Gilles Nozière</p></summary>
<p>

**Abstract:** In this paper, we propose a new end-to-end methodology to optimize the energy performance as well as comfort and air quality in large buildings without any renovation work. We introduce a metamodel based on recurrent neural networks and trained to predict the behavior of a general class of buildings using a database sampled from a simulation program. This metamodel is then deployed in different frameworks and its parameters are calibrated using the specific data of two real buildings. Parameters are estimated by comparing the predictions of the metamodel with real data obtained from sensors using the CMA-ES algorithm, a derivative free optimization procedure. Then, energy consumptions are optimized while maintaining a target thermal comfort and air quality, using the NSGA-II multi-objective optimization procedure. The numerical experiments illustrate how this metamodel ensures a significant gain in energy efficiency, up to almost 10%, while being computationally much more appealing than numerical models and flexible enough to be adapted to several types of buildings.

</p>
</details>

<details><summary><b>A General Framework for the Logical Representation of Combinatorial Exchange Protocols</b>
<a href="https://arxiv.org/abs/2102.02061">arxiv:2102.02061</a>
&#x1F4C8; 1 <br>
<p>Munyque Mittelmann, Sylvain Bouveret, Laurent Perrussel</p></summary>
<p>

**Abstract:** The goal of this paper is to propose a framework for representing and reasoning about the rules governing a combinatorial exchange. Such a framework is at first interest as long as we want to build up digital marketplaces based on auction, a widely used mechanism for automated transactions. Combinatorial exchange is the most general case of auctions, mixing the double and combinatorial variants: agents bid to trade bundles of goods. Hence the framework should fulfill two requirements: (i) it should enable bidders to express their bids on combinations of goods and (ii) it should allow describing the rules governing some market, namely the legal bids, the allocation and payment rules. To do so, we define a logical language in the spirit of the Game Description Language: the Combinatorial Exchange Description Language is the first language for describing combinatorial exchange in a logical framework. The contribution is two-fold: first, we illustrate the general dimension by representing different kinds of protocols, and second, we show how to reason about auction properties in this machine-processable language.

</p>
</details>

<details><summary><b>PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation</b>
<a href="https://arxiv.org/abs/2102.01243">arxiv:2102.01243</a>
&#x1F4C8; 1 <br>
<p>Yuan Gong, Yu-An Chung, James Glass</p></summary>
<p>

**Abstract:** Audio tagging is an active research area and has a wide range of applications. Since the release of AudioSet, great progress has been made in advancing model performance, which mostly comes from the development of novel model architectures and attention modules. However, we find that appropriate training techniques are equally important for building audio tagging models with AudioSet, but have not received the attention they deserve. To fill the gap, in this work, we present PSLA, a collection of training techniques that can noticeably boost the model accuracy including ImageNet pretraining, balanced sampling, data augmentation, label enhancement, model aggregation and their design choices. By training an EfficientNet with these techniques, we obtain a single model (with 13.6M parameters) and an ensemble model that achieve mean average precision (mAP) scores of 0.444 and 0.474 on AudioSet, respectively, outperforming the previous best system of 0.439 with 81M parameters. In addition, our model also achieves a new state-of-the-art mAP of 0.567 on FSD50K.

</p>
</details>

<details><summary><b>The Gene Mover's Distance: Single-cell similarity via Optimal Transport</b>
<a href="https://arxiv.org/abs/2102.01218">arxiv:2102.01218</a>
&#x1F4C8; 1 <br>
<p>Riccardo Bellazzi, Andrea Codegoni, Stefano Gualandi, Giovanna Nicora, Eleonora Vercesi</p></summary>
<p>

**Abstract:** This paper introduces the Gene Mover's Distance, a measure of similarity between a pair of cells based on their gene expression profiles obtained via single-cell RNA sequencing. The underlying idea of the proposed distance is to interpret the gene expression array of a single cell as a discrete probability measure. The distance between two cells is hence computed by solving an Optimal Transport problem between the two corresponding discrete measures. In the Optimal Transport model, we use two types of cost function for measuring the distance between a pair of genes. The first cost function exploits a gene embedding, called gene2vec, which is used to map each gene to a high dimensional vector: the cost of moving a unit of mass of gene expression from a gene to another is set to the Euclidean distance between the corresponding embedded vectors. The second cost function is based on a Pearson distance among pairs of genes. In both cost functions, the more two genes are correlated, the lower is their distance. We exploit the Gene Mover's Distance to solve two classification problems: the classification of cells according to their condition and according to their type. To assess the impact of our new metric, we compare the performances of a $k$-Nearest Neighbor classifier using different distances. The computational results show that the Gene Mover's Distance is competitive with the state-of-the-art distances used in the literature.

</p>
</details>

<details><summary><b>A Tight Bound for Stochastic Submodular Cover</b>
<a href="https://arxiv.org/abs/2102.01149">arxiv:2102.01149</a>
&#x1F4C8; 1 <br>
<p>Lisa Hellerstein, Devorah Kletenik, Srinivasan Parthasarathy</p></summary>
<p>

**Abstract:** We show that the Adaptive Greedy algorithm of Golovin and Krause (2011) achieves an approximation bound of $(\ln (Q/η)+1)$ for Stochastic Submodular Cover: here $Q$ is the "goal value" and $η$ is the smallest non-zero marginal increase in utility deliverable by an item. (For integer-valued utility functions, we show a bound of $H(Q)$, where $H(Q)$ is the $Q^{th}$ Harmonic number.) Although this bound was claimed by Golovin and Krause in the original version of their paper, the proof was later shown to be incorrect by Nan and Saligrama (2017). The subsequent corrected proof of Golovin and Krause (2017) gives a quadratic bound of $(\ln(Q/η) + 1)^2$. Other previous bounds for the problem are $56(\ln(Q/η) + 1)$, implied by work of Im et al. (2016) on a related problem, and $k(\ln (Q/η)+1)$, due to Deshpande et al. (2016) and Hellerstein and Kletenik (2018), where $k$ is the number of states. Our bound generalizes the well-known $(\ln~m + 1)$ approximation bound on the greedy algorithm for the classical Set Cover problem, where $m$ is the size of the ground set.

</p>
</details>

<details><summary><b>Dosimetric impact of physician style variations in contouring CTV for post-operative prostate cancer: A deep learning-based simulation study</b>
<a href="https://arxiv.org/abs/2102.01006">arxiv:2102.01006</a>
&#x1F4C8; 1 <br>
<p>Anjali Balagopal, Dan Nguyen, Maryam Mashayekhi, Howard Morgan, Aurelie Garant, Neil Desai, Raquibul Hannan, Mu-Han Lin, Steve Jiang</p></summary>
<p>

**Abstract:** Inter-observer variation is a significant problem in clinical target volume(CTV) segmentation in postoperative settings, where there is no gross tumor present. In this scenario, the CTV is not an anatomically established structure, but one determined by the physician based on the clinical guideline used, the preferred tradeoff between tumor control and toxicity, their experience and training background, and other factors. This results in high inter-observer variability between physicians. This variability has been considered an issue, but the absence of multiple physician CTV contours for each patient and the significant amount of time required for dose planning have made it impractical to study its dosimetric consequences. In this study, we analyze the impact that variations in physician style have on dose to organs-at-risk(OAR) by simulating the clinical workflow via deep learning. For a given patient previously treated by one physician, we use deep learning-based tools to simulate how other physicians would contour the CTV and how the corresponding dose distributions would look for this patient. To simulate multiple physician styles, we use a previously developed in-house CTV segmentation model that can produce physician style-aware segmentations. The corresponding dose distribution is predicted using another in-house deep learning tool, which, can predict dose within 3% of the prescription dose, on average, on the test data. For every test patient, four different physician style CTVs are considered, and four different dose distributions are analyzed. OAR dose metrics are compared, showing that even though physician style variations result in organs getting different doses, all the important dose metrics except Maximum Dose point are within the clinically acceptable limit.

</p>
</details>

<details><summary><b>Fail-Safe Execution of Deep Learning based Systems through Uncertainty Monitoring</b>
<a href="https://arxiv.org/abs/2102.00902">arxiv:2102.00902</a>
&#x1F4C8; 1 <br>
<p>Michael Weiss, Paolo Tonella</p></summary>
<p>

**Abstract:** Modern software systems rely on Deep Neural Networks (DNN) when processing complex, unstructured inputs, such as images, videos, natural language texts or audio signals. Provided the intractably large size of such input spaces, the intrinsic limitations of learning algorithms, and the ambiguity about the expected predictions for some of the inputs, not only there is no guarantee that DNN's predictions are always correct, but rather developers must safely assume a low, though not negligible, error probability. A fail-safe Deep Learning based System (DLS) is one equipped to handle DNN faults by means of a supervisor, capable of recognizing predictions that should not be trusted and that should activate a healing procedure bringing the DLS to a safe state. In this paper, we propose an approach to use DNN uncertainty estimators to implement such a supervisor. We first discuss the advantages and disadvantages of existing approaches to measure uncertainty for DNNs and propose novel metrics for the empirical assessment of the supervisor that rely on such approaches. We then describe our publicly available tool UNCERTAINTY-WIZARD, which allows transparent estimation of uncertainty for regular tf.keras DNNs. Lastly, we discuss a large-scale study conducted on four different subjects to empirically validate the approach, reporting the lessons-learned as guidance for software engineers who intend to monitor uncertainty for fail-safe execution of DLS.

</p>
</details>

<details><summary><b>Hybrid Beamforming for mmWave MU-MISO Systems Exploiting Multi-agent Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.00735">arxiv:2102.00735</a>
&#x1F4C8; 1 <br>
<p>Qisheng Wang, Xiao Li, Shi Jin, Yijiain Chen</p></summary>
<p>

**Abstract:** In this letter, we investigate the hybrid beamforming based on deep reinforcement learning (DRL) for millimeter Wave (mmWave) multi-user (MU) multiple-input-single-output (MISO) system. A multi-agent DRL method is proposed to solve the exploration efficiency problem in DRL. In the proposed method, prioritized replay buffer and more informative reward are applied to accelerate the convergence. Simulation results show that the proposed architecture achieves higher spectral efficiency and less time consumption than the benchmarks, thus is more suitable for practical applications.

</p>
</details>

<details><summary><b>Despeckling Sentinel-1 GRD images by deep learning and application to narrow river segmentation</b>
<a href="https://arxiv.org/abs/2102.00692">arxiv:2102.00692</a>
&#x1F4C8; 1 <br>
<p>Nicolas Gasnier, Emanuele Dalsasso, Loïc Denis, Florence Tupin</p></summary>
<p>

**Abstract:** This paper presents a despeckling method for Sentinel-1 GRD images based on the recently proposed framework "SAR2SAR": a self-supervised training strategy. Training the deep neural network on collections of Sentinel 1 GRD images leads to a despeckling algorithm that is robust to space-variant spatial correlations of speckle. Despeckled images improve the detection of structures like narrow rivers. We apply a detector based on exogenous information and a linear features detector and show that rivers are better segmented when the processing chain is applied to images pre-processed by our despeckling neural network.

</p>
</details>

<details><summary><b>Exploiting multi-temporal information for improved speckle reduction of Sentinel-1 SAR images by deep learning</b>
<a href="https://arxiv.org/abs/2102.00682">arxiv:2102.00682</a>
&#x1F4C8; 1 <br>
<p>Emanuele Dalsasso, Inès Meraoumia, Loïc Denis, Florence Tupin</p></summary>
<p>

**Abstract:** Deep learning approaches show unprecedented results for speckle reduction in SAR amplitude images. The wide availability of multi-temporal stacks of SAR images can improve even further the quality of denoising. In this paper, we propose a flexible yet efficient way to integrate temporal information into a deep neural network for speckle suppression. Archives provide access to long time-series of SAR images, from which multi-temporal averages can be computed with virtually no remaining speckle fluctuations. The proposed method combines this multi-temporal average and the image at a given date in the form of a ratio image and uses a state-of-the-art neural network to remove the speckle in this ratio image. This simple strategy is shown to offer a noticeable improvement compared to filtering the original image without knowledge of the multi-temporal average.

</p>
</details>

<details><summary><b>A Living Review of Machine Learning for Particle Physics</b>
<a href="https://arxiv.org/abs/2102.02770">arxiv:2102.02770</a>
&#x1F4C8; 0 <br>
<p>Matthew Feickert, Benjamin Nachman</p></summary>
<p>

**Abstract:** Modern machine learning techniques, including deep learning, are rapidly being applied, adapted, and developed for high energy physics. Given the fast pace of this research, we have created a living review with the goal of providing a nearly comprehensive list of citations for those developing and applying these approaches to experimental, phenomenological, or theoretical analyses. As a living document, it will be updated as often as possible to incorporate the latest developments. A list of proper (unchanging) reviews can be found within. Papers are grouped into a small set of topics to be as useful as possible. Suggestions and contributions are most welcome, and we provide instructions for participating.

</p>
</details>

<details><summary><b>Local Differential Privacy Is Equivalent to Contraction of $E_γ$-Divergence</b>
<a href="https://arxiv.org/abs/2102.01258">arxiv:2102.01258</a>
&#x1F4C8; 0 <br>
<p>Shahab Asoodeh, Maryam Aliakbarpour, Flavio P. Calmon</p></summary>
<p>

**Abstract:** We investigate the local differential privacy (LDP) guarantees of a randomized privacy mechanism via its contraction properties. We first show that LDP constraints can be equivalently cast in terms of the contraction coefficient of the $E_γ$-divergence. We then use this equivalent formula to express LDP guarantees of privacy mechanisms in terms of contraction coefficients of arbitrary $f$-divergences. When combined with standard estimation-theoretic tools (such as Le Cam's and Fano's converse methods), this result allows us to study the trade-off between privacy and utility in several testing and minimax and Bayesian estimation problems.

</p>
</details>

<details><summary><b>Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research</b>
<a href="https://arxiv.org/abs/2102.01203">arxiv:2102.01203</a>
&#x1F4C8; 0 <br>
<p>A. Feder Cooper, Ellen Abrams</p></summary>
<p>

**Abstract:** Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.

</p>
</details>


[Next Page](2021/2021-01/2021-01-31.md)
