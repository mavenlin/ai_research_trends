Prev: [2021.12.30]({{ '/2021/12/30/2021.12.30.html' | relative_url }})  Next: [2022.01.01]({{ '/2022/01/01/2022.01.01.html' | relative_url }})
{% raw %}
## Summary for 2021-12-31, created on 2022-01-10


<details><summary><b>A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More</b>
<a href="https://arxiv.org/abs/2112.15594">arxiv:2112.15594</a>
&#x1F4C8; 12900 <br>
<p>Iddo Drori, Sunny Tran, Roman Wang, Newman Cheng, Kevin Liu, Leonard Tang, Elizabeth Ke, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang</p></summary>
<p>

**Abstract:** We demonstrate that a neural network pre-trained on text and fine-tuned on code solves Mathematics problems by program synthesis. We turn questions into programming tasks, automatically generate programs, and then execute them, perfectly solving university-level problems from MIT's large Mathematics courses (Single Variable Calculus 18.01, Multivariable Calculus 18.02, Differential Equations 18.03, Introduction to Probability and Statistics 18.05, Linear Algebra 18.06, and Mathematics for Computer Science 6.042), Columbia University's COMS3251 Computational Linear Algebra course, as well as questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems specifically designed to assess mathematical reasoning. We explore prompt generation methods that enable Transformers to generate question solving programs for these subjects, including solutions with plots. We generate correct answers for a random sample of questions in each topic. We quantify the gap between the original and transformed questions and perform a survey to evaluate the quality and difficulty of generated questions. This is the first work to automatically solve, grade, and generate university-level Mathematics course questions at scale. This represents a milestone for higher education.

</p>
</details>

<details><summary><b>On Distinctive Properties of Universal Perturbations</b>
<a href="https://arxiv.org/abs/2112.15329">arxiv:2112.15329</a>
&#x1F4C8; 44 <br>
<p>Sung Min Park, Kuo-An Wei, Kai Xiao, Jerry Li, Aleksander Madry</p></summary>
<p>

**Abstract:** We identify properties of universal adversarial perturbations (UAPs) that distinguish them from standard adversarial perturbations. Specifically, we show that targeted UAPs generated by projected gradient descent exhibit two human-aligned properties: semantic locality and spatial invariance, which standard targeted adversarial perturbations lack. We also demonstrate that UAPs contain significantly less signal for generalization than standard adversarial perturbations -- that is, UAPs leverage non-robust features to a smaller extent than standard adversarial perturbations.

</p>
</details>

<details><summary><b>Social Neuro AI: Social Interaction as the "dark matter" of AI</b>
<a href="https://arxiv.org/abs/2112.15459">arxiv:2112.15459</a>
&#x1F4C8; 34 <br>
<p>Samuele Bolotta, Guillaume Dumas</p></summary>
<p>

**Abstract:** We are making the case that empirical results from social psychology and social neuroscience along with the framework of dynamics can be of inspiration to the development of more intelligent artificial agents. We specifically argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. In the first section, we aim at demonstrating that social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and investigating the abilities that various animals have at learning from others; we also explore findings from social neuroscience that examine human brains during social interaction and learning. Then, we discuss three proposed lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by the powerful framework offered by dynamics. Third, social embodiment has been demonstrated to provide social interactions between virtual agents and humans with a more sophisticated array of communicative signals. To conclude, we provide a new perspective on the field of multiagent robot systems, exploring how it can advance by following the aforementioned three axes.

</p>
</details>

<details><summary><b>Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments</b>
<a href="https://arxiv.org/abs/2201.00042">arxiv:2201.00042</a>
&#x1F4C8; 21 <br>
<p>Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad</p></summary>
<p>

**Abstract:** A key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.

</p>
</details>

<details><summary><b>Triangular Flows for Generative Modeling: Statistical Consistency, Smoothness Classes, and Fast Rates</b>
<a href="https://arxiv.org/abs/2112.15595">arxiv:2112.15595</a>
&#x1F4C8; 10 <br>
<p>Nicholas J. Irons, Meyer Scetbon, Soumik Pal, Zaid Harchaoui</p></summary>
<p>

**Abstract:** Triangular flows, also known as Knöthe-Rosenblatt measure couplings, comprise an important building block of normalizing flow models for generative modeling and density estimation, including popular autoregressive flow models such as real-valued non-volume preserving transformation models (Real NVP). We present statistical guarantees and sample complexity bounds for triangular flow statistical models. In particular, we establish the statistical consistency and the finite sample convergence rates of the Kullback-Leibler estimator of the Knöthe-Rosenblatt measure coupling using tools from empirical process theory. Our results highlight the anisotropic geometry of function classes at play in triangular flows, shed light on optimal coordinate ordering, and lead to statistical guarantees for Jacobian flows. We conduct numerical experiments on synthetic data to illustrate the practical implications of our theoretical findings.

</p>
</details>

<details><summary><b>Transfer learning for cancer diagnosis in histopathological images</b>
<a href="https://arxiv.org/abs/2112.15523">arxiv:2112.15523</a>
&#x1F4C8; 9 <br>
<p>Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, Abdul Ghani Naim</p></summary>
<p>

**Abstract:** Transfer learning allows us to exploit knowledge gained from one task to assist in solving another but relevant task. In modern computer vision research, the question is which architecture performs better for a given dataset. In this paper, we compare the performance of 14 pre-trained ImageNet models on the histopathologic cancer detection dataset, where each model has been configured as a naive model, feature extractor model, or fine-tuned model. Densenet161 has been shown to have high precision whilst Resnet101 has a high recall. A high precision model is suitable to be used when follow-up examination cost is high, whilst low precision but a high recall/sensitivity model can be used when the cost of follow-up examination is low. Results also show that transfer learning helps to converge a model faster.

</p>
</details>

<details><summary><b>PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability</b>
<a href="https://arxiv.org/abs/2112.15571">arxiv:2112.15571</a>
&#x1F4C8; 8 <br>
<p>Sílvia Casacuberta, Esra Suel, Seth Flaxman</p></summary>
<p>

**Abstract:** In this paper we introduce a new problem within the growing literature of interpretability for convolution neural networks (CNNs). While previous work has focused on the question of how to visually interpret CNNs, we ask what it is that we care to interpret, that is, which layers and neurons are worth our attention? Due to the vast size of modern deep learning network architectures, automated, quantitative methods are needed to rank the relative importance of neurons so as to provide an answer to this question. We present a new statistical method for ranking the hidden neurons in any convolutional layer of a network. We define importance as the maximal correlation between the activation maps and the class score. We provide different ways in which this method can be used for visualization purposes with MNIST and ImageNet, and show a real-world application of our method to air pollution prediction with street-level images.

</p>
</details>

<details><summary><b>InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer</b>
<a href="https://arxiv.org/abs/2112.15320">arxiv:2112.15320</a>
&#x1F4C8; 8 <br>
<p>Chin-Tung Lin, Mu Yang</p></summary>
<p>

**Abstract:** Many social media users prefer consuming content in the form of videos rather than text. However, in order for content creators to produce videos with a high click-through rate, much editing is needed to match the footage to the music. This posts additional challenges for more amateur video makers. Therefore, we propose a novel attention-based model VMT (Video-Music Transformer) that automatically generates piano scores from video frames. Using music generated from models also prevent potential copyright infringements that often come with using existing music. To the best of our knowledge, there is no work besides the proposed VMT that aims to compose music for video. Additionally, there lacks a dataset with aligned video and symbolic music. We release a new dataset composed of over 7 hours of piano scores with fine alignment between pop music videos and MIDI files. We conduct experiments with human evaluation on VMT, SeqSeq model (our baseline), and the original piano version soundtrack. VMT achieves consistent improvements over the baseline on music smoothness and video relevance. In particular, with the relevance scores and our case study, our model has shown the capability of multimodality on frame-level actors' movement for music generation. Our VMT model, along with the new dataset, presents a promising research direction toward composing the matching soundtrack for videos. We have released our code at https://github.com/linchintung/VMT

</p>
</details>

<details><summary><b>Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship</b>
<a href="https://arxiv.org/abs/2112.15402">arxiv:2112.15402</a>
&#x1F4C8; 7 <br>
<p>Quanziang Wang, Yuexiang Li, Dong Wei, Renzhen Wang, Kai Ma, Yefeng Zheng, Deyu Meng</p></summary>
<p>

**Abstract:** Continual learning requires models to learn new tasks while maintaining previously learned knowledge. Various algorithms have been proposed to address this real challenge. Till now, rehearsal-based methods, such as experience replay, have achieved state-of-the-art performance. These approaches save a small part of the data of the past tasks as a memory buffer to prevent models from forgetting previously learned knowledge. However, most of them treat every new task equally, i.e., fixed the hyperparameters of the framework while learning different new tasks. Such a setting lacks the consideration of the relationship/similarity between past and new tasks. For example, the previous knowledge/features learned from dogs are more beneficial for the identification of cats (new task), compared to those learned from buses. In this regard, we propose a meta learning algorithm based on bi-level optimization to adaptively tune the relationship between the knowledge extracted from the past and new tasks. Therefore, the model can find an appropriate direction of gradient during continual learning and avoid the serious overfitting problem on memory buffer. Extensive experiments are conducted on three publicly available datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental results demonstrate that the proposed method can consistently improve the performance of all baselines.

</p>
</details>

<details><summary><b>Deep-learning-based upscaling method for geologic models via theory-guided convolutional neural network</b>
<a href="https://arxiv.org/abs/2201.00698">arxiv:2201.00698</a>
&#x1F4C8; 5 <br>
<p>Nanzhe Wang, Qinzhuo Liao, Haibin Chang, Dongxiao Zhang</p></summary>
<p>

**Abstract:** Large-scale or high-resolution geologic models usually comprise a huge number of grid blocks, which can be computationally demanding and time-consuming to solve with numerical simulators. Therefore, it is advantageous to upscale geologic models (e.g., hydraulic conductivity) from fine-scale (high-resolution grids) to coarse-scale systems. Numerical upscaling methods have been proven to be effective and robust for coarsening geologic models, but their efficiency remains to be improved. In this work, a deep-learning-based method is proposed to upscale the fine-scale geologic models, which can assist to improve upscaling efficiency significantly. In the deep learning method, a deep convolutional neural network (CNN) is trained to approximate the relationship between the coarse grid of hydraulic conductivity fields and the hydraulic heads, which can then be utilized to replace the numerical solvers while solving the flow equations for each coarse block. In addition, physical laws (e.g., governing equations and periodic boundary conditions) can also be incorporated into the training process of the deep CNN model, which is termed the theory-guided convolutional neural network (TgCNN). With the physical information considered, dependence on the data volume of training the deep learning models can be reduced greatly. Several subsurface flow cases are introduced to test the performance of the proposed deep-learning-based upscaling method, including 2D and 3D cases, and isotropic and anisotropic cases. The results show that the deep learning method can provide equivalent upscaling accuracy to the numerical method, and efficiency can be improved significantly compared to numerical upscaling.

</p>
</details>

<details><summary><b>Evaluating Deep Music Generation Methods Using Data Augmentation</b>
<a href="https://arxiv.org/abs/2201.00052">arxiv:2201.00052</a>
&#x1F4C8; 5 <br>
<p>Toby Godwin, Georgios Rizos, Alice Baird, Najla D. Al Futaisi, Vincent Brisse, Bjoern W. Schuller</p></summary>
<p>

**Abstract:** Despite advances in deep algorithmic music generation, evaluation of generated samples often relies on human evaluation, which is subjective and costly. We focus on designing a homogeneous, objective framework for evaluating samples of algorithmically generated music. Any engineered measures to evaluate generated music typically attempt to define the samples' musicality, but do not capture qualities of music such as theme or mood. We do not seek to assess the musical merit of generated music, but instead explore whether generated samples contain meaningful information pertaining to emotion or mood/theme. We achieve this by measuring the change in predictive performance of a music mood/theme classifier after augmenting its training data with generated samples. We analyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP -- and employ a homogeneous framework across all methods to allow for objective comparison. This is the first attempt at augmenting a music genre classification dataset with conditionally generated music. We investigate the classification performance improvement using deep music generation and the ability of the generators to make emotional music by using an additional, emotion annotation of the dataset. Finally, we use a classifier trained on real data to evaluate the label validity of class-conditionally generated samples.

</p>
</details>

<details><summary><b>Improving Baselines in the Wild</b>
<a href="https://arxiv.org/abs/2112.15550">arxiv:2112.15550</a>
&#x1F4C8; 5 <br>
<p>Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber</p></summary>
<p>

**Abstract:** We share our experience with the recently released WILDS benchmark, a collection of ten datasets dedicated to developing models and training strategies which are robust to domain shifts. Several experiments yield a couple of critical observations which we believe are of general interest for any future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW. We show that (1) Conducting separate cross-validation for each evaluation metric is crucial for both datasets, (2) A weak correlation between validation and test performance might make model development difficult for iWildCam, (3) Minor changes in the training of hyper-parameters improve the baseline by a relatively large margin (mainly on FMoW), (4) There is a strong correlation between certain domains and certain target labels (mainly on iWildCam). To the best of our knowledge, no prior work on these datasets has reported these observations despite their obvious importance. Our code is public.

</p>
</details>

<details><summary><b>Machine Learning Trivializing Maps: A First Step Towards Understanding How Flow-Based Samplers Scale Up</b>
<a href="https://arxiv.org/abs/2112.15532">arxiv:2112.15532</a>
&#x1F4C8; 5 <br>
<p>Luigi Del Debbio, Joe Marsh Rossney, Michael Wilson</p></summary>
<p>

**Abstract:** A trivializing map is a field transformation whose Jacobian determinant exactly cancels the interaction terms in the action, providing a representation of the theory in terms of a deterministic transformation of a distribution from which sampling is trivial. Recently, a proof-of-principle study by Albergo, Kanwar and Shanahan [arXiv:1904.12072] demonstrated that approximations of trivializing maps can be `machine-learned' by a class of invertible, differentiable neural models called \textit{normalizing flows}. By ensuring that the Jacobian determinant can be computed efficiently, asymptotically exact sampling from the theory of interest can be performed by drawing samples from a simple distribution and passing them through the network. From a theoretical perspective, this approach has the potential to become more efficient than traditional Markov Chain Monte Carlo sampling techniques, where autocorrelations severely diminish the sampling efficiency as one approaches the continuum limit. A major caveat is that it is not yet understood how the size of models and the cost of training them is expected to scale. As a first step, we have conducted an exploratory scaling study using two-dimensional $φ^4$ with up to $20^2$ lattice sites. Although the scope of our study is limited to a particular model architecture and training algorithm, initial results paint an interesting picture in which training costs grow very quickly indeed. We describe a candidate explanation for the poor scaling, and outline our intentions to clarify the situation in future work.

</p>
</details>

<details><summary><b>Disjoint Contrastive Regression Learning for Multi-Sourced Annotations</b>
<a href="https://arxiv.org/abs/2112.15411">arxiv:2112.15411</a>
&#x1F4C8; 5 <br>
<p>Xiaoqian Ruan, Gaoang Wang</p></summary>
<p>

**Abstract:** Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator, with the assumption that the ranking of samples from the same annotator is unanimous. Secondly, we apply the gradient reversal layer to learn robust representations that are invariant to different annotators. Experiments on the facial expression prediction task, as well as the image quality assessment task, verify the effectiveness of our proposed framework.

</p>
</details>

<details><summary><b>Weakly Supervised Change Detection Using Guided Anisotropic Difusion</b>
<a href="https://arxiv.org/abs/2112.15367">arxiv:2112.15367</a>
&#x1F4C8; 5 <br>
<p>Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau</p></summary>
<p>

**Abstract:** Large scale datasets created from crowdsourced labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose original ideas that help us to leverage such datasets in the context of change detection. First, we propose the guided anisotropic diffusion (GAD) algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering. We then show its potential in two weakly-supervised learning strategies tailored for change detection. The first strategy is an iterative learning method that combines model optimisation and data cleansing using GAD to extract the useful information from a large scale change detection dataset generated from open vector data. The second one incorporates GAD within a novel spatial attention layer that increases the accuracy of weakly supervised networks trained to perform pixel-level predictions from image-level labels. Improvements with respect to state-of-the-art are demonstrated on 4 different public datasets.

</p>
</details>

<details><summary><b>SplitBrain: Hybrid Data and Model Parallel Deep Learning</b>
<a href="https://arxiv.org/abs/2112.15317">arxiv:2112.15317</a>
&#x1F4C8; 5 <br>
<p>Farley Lai, Asim Kadav, Erik Kruus</p></summary>
<p>

**Abstract:** The recent success of deep learning applications has coincided with those widely available powerful computational resources for training sophisticated machine learning models with huge datasets. Nonetheless, training large models such as convolutional neural networks using model parallelism (as opposed to data parallelism) is challenging because the complex nature of communication between model shards makes it difficult to partition the computation efficiently across multiple machines with an acceptable trade-off. This paper presents SplitBrain, a high performance distributed deep learning framework supporting hybrid data and model parallelism. Specifically, SplitBrain provides layer-specific partitioning that co-locates compute intensive convolutional layers while sharding memory demanding layers. A novel scalable group communication is proposed to further improve the training throughput with reduced communication overhead. The results show that SplitBrain can achieve nearly linear speedup while saving up to 67\% of memory consumption for data and model parallel VGG over CIFAR-10.

</p>
</details>

<details><summary><b>DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training</b>
<a href="https://arxiv.org/abs/2201.01155">arxiv:2201.01155</a>
&#x1F4C8; 4 <br>
<p>Xianglin Yang, Yun Lin, Ruofan Liu, Zhenfeng He, Chao Wang, Jin Song Dong, Hong Mei</p></summary>
<p>

**Abstract:** Understanding how the predictions of deep learning models are formed during the training process is crucial to improve model performance and fix model defects, especially when we need to investigate nontrivial training strategies such as active learning, and track the root cause of unexpected training results such as performance degeneration.
  In this work, we propose a time-travelling visual solution DeepVisualInsight (DVI), aiming to manifest the spatio-temporal causality while training a deep learning image classifier. The spatio-temporal causality demonstrates how the gradient-descent algorithm and various training data sampling techniques can influence and reshape the layout of learnt input representation and the classification boundaries in consecutive epochs. Such causality allows us to observe and analyze the whole learning process in the visible low dimensional space. Technically, we propose four spatial and temporal properties and design our visualization solution to satisfy them. These properties preserve the most important information when inverse-)projecting input samples between the visible low-dimensional and the invisible high-dimensional space, for causal analyses. Our extensive experiments show that, comparing to baseline approaches, we achieve the best visualization performance regarding the spatial/temporal properties and visualization efficiency. Moreover, our case study shows that our visual solution can well reflect the characteristics of various training scenarios, showing good potential of DVI as a debugging tool for analyzing deep learning training processes.

</p>
</details>

<details><summary><b>Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems</b>
<a href="https://arxiv.org/abs/2201.00063">arxiv:2201.00063</a>
&#x1F4C8; 4 <br>
<p>Samaa Gazzaz, Vishal Chakraborty, Faisal Nawab</p></summary>
<p>

**Abstract:** Emerging edge applications require both a fast response latency and complex processing. This is infeasible without expensive hardware that can process complex operations -- such as object detection -- within a short time. Many approach this problem by addressing the complexity of the models -- via model compression, pruning and quantization -- or compressing the input. In this paper, we propose a different perspective when addressing the performance challenges. Croesus is a multi-stage approach to edge-cloud systems that provides the ability to find the balance between accuracy and performance. Croesus consists of two stages (that can be generalized to multiple stages): an initial and a final stage. The initial stage performs the computation in real-time using approximate/best-effort computation at the edge. The final stage performs the full computation at the cloud, and uses the results to correct any errors made at the initial stage. In this paper, we demonstrate the implications of such an approach on a video analytics use-case and show how multi-stage processing yields a better balance between accuracy and performance. Moreover, we study the safety of multi-stage transactions via two proposals: multi-stage serializability (MS-SR) and multi-stage invariant confluence with Apologies (MS-IA).

</p>
</details>

<details><summary><b>Optimal Representations for Covariate Shift</b>
<a href="https://arxiv.org/abs/2201.00057">arxiv:2201.00057</a>
&#x1F4C8; 4 <br>
<p>Yangjun Ruan, Yann Dubois, Chris J. Maddison</p></summary>
<p>

**Abstract:** Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised learning methods that only use unlabelled data and augmentations to train robust representations. Our objectives achieve state-of-the-art results on DomainBed, and give insights into the robustness of recent methods, such as CLIP.

</p>
</details>

<details><summary><b>An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training</b>
<a href="https://arxiv.org/abs/2112.15555">arxiv:2112.15555</a>
&#x1F4C8; 4 <br>
<p>Yiju Yang, Tianxiao Zhang, Guanyu Li, Taejoon Kim, Guanghui Wang</p></summary>
<p>

**Abstract:** In this paper, we propose a dual-module network architecture that employs a domain discriminative feature module to encourage the domain invariant feature module to learn more domain invariant features. The proposed architecture can be applied to any model that utilizes domain invariant features for unsupervised domain adaptation to improve its ability to extract domain invariant features. We conduct experiments with the Domain-Adversarial Training of Neural Networks (DANN) model as a representative algorithm. In the training process, we supply the same input to the two modules and then extract their feature distribution and prediction results respectively. We propose a discrepancy loss to find the discrepancy of the prediction results and the feature distribution between the two modules. Through the adversarial training by maximizing the loss of their feature distribution and minimizing the discrepancy of their prediction results, the two modules are encouraged to learn more domain discriminative and domain invariant features respectively. Extensive comparative evaluations are conducted and the proposed approach outperforms the state-of-the-art in most unsupervised domain adaptation tasks.

</p>
</details>

<details><summary><b>on the effectiveness of generative adversarial network on anomaly detection</b>
<a href="https://arxiv.org/abs/2112.15541">arxiv:2112.15541</a>
&#x1F4C8; 4 <br>
<p>Laya Rafiee Sevyeri, Thomas Fevens</p></summary>
<p>

**Abstract:** Identifying anomalies refers to detecting samples that do not resemble the training data distribution. Many generative models have been used to find anomalies, and among them, generative adversarial network (GAN)-based approaches are currently very popular. GANs mainly rely on the rich contextual information of these models to identify the actual training distribution. Following this analogy, we suggested a new unsupervised model based on GANs --a combination of an autoencoder and a GAN. Further, a new scoring function was introduced to target anomalies where a linear combination of the internal representation of the discriminator and the generator's visual representation, plus the encoded representation of the autoencoder, come together to define the proposed anomaly score. The model was further evaluated on benchmark datasets such as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of leukemia images. In all the experiments, our model outperformed its existing counterparts while slightly improving the inference time.

</p>
</details>

<details><summary><b>Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network</b>
<a href="https://arxiv.org/abs/2112.15362">arxiv:2112.15362</a>
&#x1F4C8; 4 <br>
<p>Jiamian Wang, Yulun Zhang, Xin Yuan, Ziyi Meng, Zhiqiang Tao</p></summary>
<p>

**Abstract:** Recently, hyperspectral imaging (HSI) has attracted increasing research attention, especially for the ones based on a coded aperture snapshot spectral imaging (CASSI) system. Existing deep HSI reconstruction models are generally trained on paired data to retrieve original signals upon 2D compressed measurements given by a particular optical hardware mask in CASSI, during which the mask largely impacts the reconstruction performance and could work as a "model hyperparameter" governing on data augmentations. This mask-specific training style will lead to a hardware miscalibration issue, which sets up barriers to deploying deep HSI models among different hardware and noisy environments. To address this challenge, we introduce mask uncertainty for HSI with a complete variational Bayesian learning treatment and explicitly model it through a mask decomposition inspired by real hardware. Specifically, we propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties adapting to varying spatial structures of masks among different hardware. Moreover, we develop a bilevel optimization framework to balance HSI reconstruction and uncertainty estimation, accounting for the hyperparameter property of masks. Extensive experimental results and model discussions validate the effectiveness (over 33/30 dB) of the proposed GST method under two miscalibration scenarios and demonstrate a highly competitive performance compared with the state-of-the-art well-calibrated methods. Our code and pre-trained model are available at https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI

</p>
</details>

<details><summary><b>Sufficient Statistic Memory AMP</b>
<a href="https://arxiv.org/abs/2112.15327">arxiv:2112.15327</a>
&#x1F4C8; 4 <br>
<p>Lei Liu, Shunqi Huang, Brian M. Kurkoski</p></summary>
<p>

**Abstract:** Approximate message passing (AMP) is a promising technique for unknown signal reconstruction of certain high-dimensional linear systems with non-Gaussian signaling. A distinguished feature of the AMP-type algorithms is that their dynamics can be rigorously described by state evolution. However, state evolution does not necessarily guarantee the convergence of iterative algorithms. To solve the convergence problem of AMP-type algorithms in principle, this paper proposes a memory AMP (MAMP) under a sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We show that the covariance matrices of SS-MAMP are L-banded and convergent. Given an arbitrary MAMP, we can construct an SS-MAMP by damping, which not only ensures the convergence of MAMP but also preserves the orthogonality of MAMP, i.e., its dynamics can be rigorously described by state evolution. As a byproduct, we prove that the Bayes-optimal orthogonal/vector AMP (BO-OAMP/VAMP) is an SS-MAMP. As a result, we reveal two interesting properties of BO-OAMP/VAMP for large systems: 1) the covariance matrices are L-banded and are convergent in BO-OAMP/VAMP, and 2) damping and memory are useless (i.e., do not bring performance improvement) in BO-OAMP/VAMP. As an example, we construct a sufficient statistic Bayes-optimal MAMP (BO-MAMP), which is Bayes optimal if its state evolution has a unique fixed point and its MSE is not worse than the original BO-MAMP. Finally, simulations are provided to verify the validity and accuracy of the theoretical results.

</p>
</details>

<details><summary><b>Transformer Embeddings of Irregularly Spaced Events and Their Participants</b>
<a href="https://arxiv.org/abs/2201.00044">arxiv:2201.00044</a>
&#x1F4C8; 3 <br>
<p>Chenghao Yang, Hongyuan Mei, Jason Eisner</p></summary>
<p>

**Abstract:** We propose an approach to modeling irregularly spaced sequences of discrete events. We begin with a continuous-time variant of the Transformer, which was originally formulated (Vaswani et al., 2017) for sequences without timestamps. We embed a possible event (or other boolean fact) at time $t$ by using attention over the events that occurred at times $< t$ (and the facts that were true when they occurred). We control this attention using pattern-matching logic rules that relate events and facts that share participants. These rules determine which previous events will be attended to, as well as how to transform the embeddings of the events and facts into the attentional queries, keys, and values. Other logic rules describe how to change the set of facts in response to events. Our approach closely follows Mei et al. (2020a), and adopts their Datalog Through Time formalism for logic rules. As in that work, a domain expert first writes a set of logic rules that establishes the set of possible events and other facts at each time $t$. Each possible event or other fact is embedded using a neural architecture that is derived from the rules that established it. Our only difference from Mei et al. (2020a) is that we derive a flatter, attention-based neural architecture whereas they used a more serial LSTM architecture. We find that our attention-based approach performs about equally well on the RoboCup dataset, where the logic rules play an important role in improving performance. We also compared these two methods with two previous attention-based methods (Zuo et al., 2020; Zhang et al., 2020a) on simpler synthetic and real domains without logic rules, and found our proposed approach to be at least as good, and sometimes better, than each of the other three methods.

</p>
</details>

<details><summary><b>Importance of Empirical Sample Complexity Analysis for Offline Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.15578">arxiv:2112.15578</a>
&#x1F4C8; 3 <br>
<p>Samin Yeasar Arnob, Riashat Islam, Doina Precup</p></summary>
<p>

**Abstract:** We hypothesize that empirically studying the sample complexity of offline reinforcement learning (RL) is crucial for the practical applications of RL in the real world. Several recent works have demonstrated the ability to learn policies directly from offline data. In this work, we ask the question of the dependency on the number of samples for learning from offline data. Our objective is to emphasize that studying sample complexity for offline RL is important, and is an indicator of the usefulness of existing offline algorithms. We propose an evaluation approach for sample complexity analysis of offline RL.

</p>
</details>

<details><summary><b>Efficient Single Image Super-Resolution Using Dual Path Connections with Multiple Scale Learning</b>
<a href="https://arxiv.org/abs/2112.15386">arxiv:2112.15386</a>
&#x1F4C8; 3 <br>
<p>Bin-Cheng Yang, Gangshan Wu</p></summary>
<p>

**Abstract:** Deep convolutional neural networks have been demonstrated to be effective for SISR in recent years. On the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. However, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. On the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, it either do super-resolution in HR space to have a high computation cost or can not share parameters between models of different scale factors to save parameters and inference time. To tackle these challenges, we propose an efficient single image super-resolution network using dual path connections with multiple scale learning named as EMSRDPN. By introducing dual path connections inspired by Dual Path Networks into EMSRDPN, it uses residual connections and dense connections in an integrated way in most network layers. Dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for SISR. To utilize the feature correlation of multiple scale factors, EMSRDPN shares all network units in LR space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile which can save parameters and support shared inference for multiple scale factors to improve efficiency. Experiments show EMSRDPN achieves better performance and comparable or even better parameter and inference efficiency over SOTA methods.

</p>
</details>

<details><summary><b>Separation of scales and a thermodynamic description of feature learning in some CNNs</b>
<a href="https://arxiv.org/abs/2112.15383">arxiv:2112.15383</a>
&#x1F4C8; 3 <br>
<p>Inbar Seroussi, Zohar Ringel</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are powerful tools for compressing and distilling information. Due to their scale and complexity, often involving billions of inter-dependent internal degrees of freedom, exact analysis approaches often fall short. A common strategy in such cases is to identify slow degrees of freedom that average out the erratic behavior of the underlying fast microscopic variables. Here, we identify such a separation of scales occurring in over-parameterized deep convolutional neural networks (CNNs) at the end of training. It implies that neuron pre-activations fluctuate in a nearly Gaussian manner with a deterministic latent kernel. While for CNNs with infinitely many channels these kernels are inert, for finite CNNs they adapt and learn from data in an analytically tractable manner. The resulting thermodynamic theory of deep learning yields accurate predictions on several deep non-linear CNN toy models. In addition, it provides new ways of analyzing and understanding CNNs.

</p>
</details>

<details><summary><b>Binary Diffing as a Network Alignment Problem via Belief Propagation</b>
<a href="https://arxiv.org/abs/2112.15337">arxiv:2112.15337</a>
&#x1F4C8; 3 <br>
<p>Elie Mengin, Fabrice Rossi</p></summary>
<p>

**Abstract:** In this paper, we address the problem of finding a correspondence, or matching, between the functions of two programs in binary form, which is one of the most common task in binary diffing. We introduce a new formulation of this problem as a particular instance of a graph edit problem over the call graphs of the programs. In this formulation, the quality of a mapping is evaluated simultaneously with respect to both function content and call graph similarities. We show that this formulation is equivalent to a network alignment problem. We propose a solving strategy for this problem based on max-product belief propagation. Finally, we implement a prototype of our method, called QBinDiff, and propose an extensive evaluation which shows that our approach outperforms state of the art diffing tools.

</p>
</details>

<details><summary><b>Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing</b>
<a href="https://arxiv.org/abs/2112.15336">arxiv:2112.15336</a>
&#x1F4C8; 3 <br>
<p>Elie Mengin, Fabrice Rossi</p></summary>
<p>

**Abstract:** In this paper, we present a novel algorithm to address the Network Alignment problem. It is inspired from a previous message passing framework of Bayati et al. [2] and includes several modifications designed to significantly speed up the message updates as well as to enforce their convergence. Experiments show that our proposed model outperforms other state-of-the-art solvers. Finally, we propose an application of our method in order to address the Binary Diffing problem. We show that our solution provides better assignment than the reference differs in almost all submitted instances and outline the importance of leveraging the graphical structure of binary programs.

</p>
</details>

<details><summary><b>Bayesian Optimization of Function Networks</b>
<a href="https://arxiv.org/abs/2112.15311">arxiv:2112.15311</a>
&#x1F4C8; 3 <br>
<p>Raul Astudillo, Peter I. Frazier</p></summary>
<p>

**Abstract:** We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the network takes significant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the standard Bayesian optimization approach observes only the final output, our approach delivers greater query efficiency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efficiently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it finds a globally optimal solution as the number of evaluations grows to infinity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems.

</p>
</details>

<details><summary><b>Distributed Evolution Strategies Using TPUs for Meta-Learning</b>
<a href="https://arxiv.org/abs/2201.00093">arxiv:2201.00093</a>
&#x1F4C8; 2 <br>
<p>Alex Sheng, Derek He</p></summary>
<p>

**Abstract:** Meta-learning traditionally relies on backpropagation through entire tasks to iteratively improve a model's learning dynamics. However, this approach is computationally intractable when scaled to complex tasks. We propose a distributed evolutionary meta-learning strategy using Tensor Processing Units (TPUs) that is highly parallel and scalable to arbitrarily long tasks with no increase in memory cost. Using a Prototypical Network trained with evolution strategies on the Omniglot dataset, we achieved an accuracy of 98.4% on a 5-shot classification problem. Our algorithm used as much as 40 times less memory than automatic differentiation to compute the gradient, with the resulting model achieving accuracy within 1.3% of a backpropagation-trained equivalent (99.6%). We observed better classification accuracy as high as 99.1% with larger population configurations. We further experimentally validate the stability and performance of ES-ProtoNet across a variety of training conditions (varying population size, model size, number of workers, shot, way, ES hyperparameters, etc.). Our contributions are twofold: we provide the first assessment of evolutionary meta-learning in a supervised setting, and create a general framework for distributed evolution strategies on TPUs.

</p>
</details>

<details><summary><b>Dynamic Persistent Homology for Brain Networks via Wasserstein Graph Clustering</b>
<a href="https://arxiv.org/abs/2201.00087">arxiv:2201.00087</a>
&#x1F4C8; 2 <br>
<p>Moo K. Chung, Shih-Gu Huang, Ian C. Carroll, Vince D. Calhoun, H. Hill Goldsmith</p></summary>
<p>

**Abstract:** We present the novel Wasserstein graph clustering for dynamically changing graphs. The Wasserstein clustering penalizes the topological discrepancy between graphs. The Wasserstein clustering is shown to outperform the widely used k-means clustering. The method applied in more accurate determination of the state spaces of dynamically changing functional brain networks.

</p>
</details>

<details><summary><b>Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging</b>
<a href="https://arxiv.org/abs/2201.00084">arxiv:2201.00084</a>
&#x1F4C8; 2 <br>
<p>Taira Watanabe, Kensuke Tanioka, Satoru Hiwa, Tomoyuki Hiroyasu</p></summary>
<p>

**Abstract:** Endoscopic images typically contain several artifacts. The artifacts significantly impact image analysis result in computer-aided diagnosis. Convolutional neural networks (CNNs), a type of deep learning, can removes such artifacts. Various architectures have been proposed for the CNNs, and the accuracy of artifact removal varies depending on the choice of architecture. Therefore, it is necessary to determine the artifact removal accuracy, depending on the selected architecture. In this study, we focus on endoscopic surgical instruments as artifacts, and determine and discuss the artifact removal accuracy using seven different CNN architectures.

</p>
</details>

<details><summary><b>Fast Learning of MNL Model from General Partial Rankings with Application to Network Formation Modeling</b>
<a href="https://arxiv.org/abs/2112.15575">arxiv:2112.15575</a>
&#x1F4C8; 2 <br>
<p>Jiaqi Ma, Xingjian Zhang, Qiaozhu Mei</p></summary>
<p>

**Abstract:** Multinomial Logit (MNL) is one of the most popular discrete choice models and has been widely used to model ranking data. However, there is a long-standing technical challenge of learning MNL from many real-world ranking data: exact calculation of the MNL likelihood of \emph{partial rankings} is generally intractable. In this work, we develop a scalable method for approximating the MNL likelihood of general partial rankings in polynomial time complexity. We also extend the proposed method to learn mixture of MNL. We demonstrate that the proposed methods are particularly helpful for applications to choice-based network formation modeling, where the formation of new edges in a network is viewed as individuals making choices of their friends over a candidate set. The problem of learning mixture of MNL models from partial rankings naturally arises in such applications. And the proposed methods can be used to learn MNL models from network data without the strong assumption that temporal orders of all the edge formation are available. We conduct experiments on both synthetic and real-world network data to demonstrate that the proposed methods achieve more accurate parameter estimation and better fitness of data compared to conventional methods.

</p>
</details>

<details><summary><b>Transfer learning of phase transitions in percolation and directed percolation</b>
<a href="https://arxiv.org/abs/2112.15516">arxiv:2112.15516</a>
&#x1F4C8; 2 <br>
<p>Jianmin Shen, Feiyi Liu, Shiyang Chen, Dian Xu, Xiangna Chen, Shengfeng Deng, Wei Li, Gabor Papp, Chunbin Yang</p></summary>
<p>

**Abstract:** The latest advances of statistical physics have shown remarkable performance of machine learning in identifying phase transitions. In this paper, we apply domain adversarial neural network (DANN) based on transfer learning to studying non-equilibrium and equilibrium phase transition models, which are percolation model and directed percolation (DP) model, respectively. With the DANN, only a small fraction of input configurations (2d images) needs to be labeled, which is automatically chosen, in order to capture the critical point. To learn the DP model, the method is refined by an iterative procedure in determining the critical point, which is a prerequisite for the data collapse in calculating the critical exponent $ν_{\perp}$. We then apply the DANN to a two-dimensional site percolation with configurations filtered to include only the largest cluster which may contain the information related to the order parameter. The DANN learning of both models yields reliable results which are comparable to the ones from Monte Carlo simulations. Our study also shows that the DANN can achieve quite high accuracy at much lower cost, compared to the supervised learning.

</p>
</details>

<details><summary><b>Shift-Equivariant Similarity-Preserving Hypervector Representations of Sequences</b>
<a href="https://arxiv.org/abs/2112.15475">arxiv:2112.15475</a>
&#x1F4C8; 2 <br>
<p>Dmitri A. Rachkovskij</p></summary>
<p>

**Abstract:** Hyperdimensional Computing (HDC), also known as Vector-Symbolic Architectures (VSA), is a promising framework for the development of cognitive architectures and artificial intelligence systems, as well as for technical applications and emerging neuromorphic and nanoscale hardware. HDC/VSA operate with hypervectors, i.e., distributed vector representations of large fixed dimension (usually > 1000). One of the key ingredients of HDC/VSA are the methods for encoding data of various types (from numeric scalars and vectors to graphs) into hypervectors. In this paper, we propose an approach for the formation of hypervectors of sequences that provides both an equivariance with respect to the shift of sequences and preserves the similarity of sequences with identical elements at nearby positions. Our methods represent the sequence elements by compositional hypervectors and exploit permutations of hypervectors for representing the order of sequence elements. We experimentally explored the proposed representations using a diverse set of tasks with data in the form of symbolic strings. Although our approach is feature-free as it forms the hypervector of a sequence from the hypervectors of its symbols at their positions, it demonstrated the performance on a par with the methods that apply various features, such as subsequences. The proposed techniques were designed for the HDC/VSA model known as Sparse Binary Distributed Representations. However, they can be adapted to hypervectors in formats of other HDC/VSA models, as well as for representing sequences of types other than symbolic strings.

</p>
</details>

<details><summary><b>Settling the Bias and Variance of Meta-Gradient Estimation for Meta-Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.15400">arxiv:2112.15400</a>
&#x1F4C8; 2 <br>
<p>Bo Liu, Xidong Feng, Haifeng Zhang, Jun Wang, Yaodong Yang</p></summary>
<p>

**Abstract:** In recent years, gradient based Meta-RL (GMRL) methods have achieved remarkable successes in either discovering effective online hyperparameter for one single task (Xu et al., 2018) or learning good initialisation for multi-task transfer learning (Finn et al., 2017). Despite the empirical successes, it is often neglected that computing meta gradients via vanilla backpropagation is ill-defined. In this paper, we argue that the stochastic meta-gradient estimation adopted by many existing MGRL methods are in fact biased; the bias comes from two sources: 1) the compositional bias that is inborn in the structure of compositional optimisation problems and 2) the bias of multi-step Hessian estimation caused by direct automatic differentiation. To better understand the meta gradient biases, we perform the first of its kind study to quantify the amount for each of them. We start by providing a unifying derivation for existing GMRL algorithms, and then theoretically analyse both the bias and the variance of existing gradient estimation methods. On understanding the underlying principles of bias, we propose two mitigation solutions based on off-policy correction and multi-step Hessian estimation techniques. Comprehensive ablation studies have been conducted and results reveals: (1) The existence of these two biases and how they influence the meta-gradient estimation when combined with different estimator/sample size/step and learning rate. (2) The effectiveness of these mitigation approaches for meta-gradient estimation and thereby the final return on two practical Meta-RL algorithms: LOLA-DiCE and Meta-gradient Reinforcement Learning.

</p>
</details>

<details><summary><b>Processing Images from Multiple IACTs in the TAIGA Experiment with Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2112.15382">arxiv:2112.15382</a>
&#x1F4C8; 2 <br>
<p>Stanislav Polyakov, Andrey Demichev, Alexander Kryukov, Evgeny Postnikov</p></summary>
<p>

**Abstract:** Extensive air showers created by high-energy particles interacting with the Earth atmosphere can be detected using imaging atmospheric Cherenkov telescopes (IACTs). The IACT images can be analyzed to distinguish between the events caused by gamma rays and by hadrons and to infer the parameters of the event such as the energy of the primary particle. We use convolutional neural networks (CNNs) to analyze Monte Carlo-simulated images from the telescopes of the TAIGA experiment. The analysis includes selection of the images corresponding to the showers caused by gamma rays and estimating the energy of the gamma rays. We compare performance of the CNNs using images from a single telescope and the CNNs using images from two telescopes as inputs.

</p>
</details>

<details><summary><b>Robust Entropy-regularized Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2112.15364">arxiv:2112.15364</a>
&#x1F4C8; 2 <br>
<p>Tien Mai, Patrick Jaillet</p></summary>
<p>

**Abstract:** Stochastic and soft optimal policies resulting from entropy-regularized Markov decision processes (ER-MDP) are desirable for exploration and imitation learning applications. Motivated by the fact that such policies are sensitive with respect to the state transition probabilities, and the estimation of these probabilities may be inaccurate, we study a robust version of the ER-MDP model, where the stochastic optimal policies are required to be robust with respect to the ambiguity in the underlying transition probabilities. Our work is at the crossroads of two important schemes in reinforcement learning (RL), namely, robust MDP and entropy regularized MDP. We show that essential properties that hold for the non-robust ER-MDP and robust unregularized MDP models also hold in our settings, making the robust ER-MDP problem tractable. We show how our framework and results can be integrated into different algorithmic schemes including value or (modified) policy iteration, which would lead to new robust RL and inverse RL algorithms to handle uncertainties. Analyses on computational complexity and error propagation under conventional uncertainty settings are also provided.

</p>
</details>

<details><summary><b>Formal Verification of Unknown Dynamical Systems via Gaussian Process Regression</b>
<a href="https://arxiv.org/abs/2201.00655">arxiv:2201.00655</a>
&#x1F4C8; 1 <br>
<p>John Jackson, Luca Laurenti, Eric Frew, Morteza Lahijanian</p></summary>
<p>

**Abstract:** Leveraging autonomous systems in safety-critical scenarios requires verifying their behaviors in the presence of uncertainties and black-box components that influence the system dynamics. In this article, we develop a framework for verifying partially-observable, discrete-time dynamical systems with unmodelled dynamics against temporal logic specifications from a given input-output dataset. The verification framework employs Gaussian process (GP) regression to learn the unknown dynamics from the dataset and abstract the continuous-space system as a finite-state, uncertain Markov decision process (MDP). This abstraction relies on space discretization and transition probability intervals that capture the uncertainty due to the error in GP regression by using reproducible kernel Hilbert space analysis as well as the uncertainty induced by discretization. The framework utilizes existing model checking tools for verification of the uncertain MDP abstraction against a given temporal logic specification. We establish the correctness of extending the verification results on the abstraction to the underlying partially-observable system. We show that the computational complexity of the framework is polynomial in the size of the dataset and discrete abstraction. The complexity analysis illustrates a trade-off between the quality of the verification results and the computational burden to handle larger datasets and finer abstractions. Finally, we demonstrate the efficacy of our learning and verification framework on several case studies with linear, nonlinear, and switched dynamical systems.

</p>
</details>

<details><summary><b>Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images</b>
<a href="https://arxiv.org/abs/2201.00100">arxiv:2201.00100</a>
&#x1F4C8; 1 <br>
<p>Xiaoqiang Wang, Lei Zhu, Siliang Tang, Huazhu Fu, Ping Li, Fei Wu, Yi Yang, Yueting Zhuang</p></summary>
<p>

**Abstract:** Training deep models for RGB-D salient object detection (SOD) often requires a large number of labeled RGB-D images. However, RGB-D data is not easily acquired, which limits the development of RGB-D SOD techniques. To alleviate this issue, we present a Dual-Semi RGB-D Salient Object Detection Network (DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency detection. We first devise a depth decoupling convolutional neural network (DDCNN), which contains a depth estimation branch and a saliency detection branch. The depth estimation branch is trained with RGB-D images and then used to estimate the pseudo depth maps for all unlabeled RGB images to form the paired data. The saliency detection branch is used to fuse the RGB feature and depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned as the backbone in a teacher-student framework for semi-supervised learning. Moreover, we also introduce a consistency loss on the intermediate attention and saliency maps for the unlabeled data, as well as a supervised depth and saliency loss for labeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art methods both quantitatively and qualitatively. We also demonstrate that our semi-supervised DS-Net can further improve the performance, even when using an RGB image with the pseudo depth map.

</p>
</details>

<details><summary><b>Computer Vision Based Parking Optimization System</b>
<a href="https://arxiv.org/abs/2201.00095">arxiv:2201.00095</a>
&#x1F4C8; 1 <br>
<p>Siddharth Chandrasekaran, Jeffrey Matthew Reginald, Wei Wang, Ting Zhu</p></summary>
<p>

**Abstract:** An improvement in technology is linearly related to time and time-relevant problems. It has been seen that as time progresses, the number of problems humans face also increases. However, technology to resolve these problems tends to improve as well. One of the earliest existing problems which started with the invention of vehicles was parking. The ease of resolving this problem using technology has evolved over the years but the problem of parking still remains unsolved. The main reason behind this is that parking does not only involve one problem but it consists of a set of problems within itself. One of these problems is the occupancy detection of the parking slots in a distributed parking ecosystem. In a distributed system, users would find preferable parking spaces as opposed to random parking spaces. In this paper, we propose a web-based application as a solution for parking space detection in different parking spaces. The solution is based on Computer Vision (CV) and is built using the Django framework written in Python 3.0. The solution works to resolve the occupancy detection problem along with providing the user the option to determine the block based on availability and his preference. The evaluation results for our proposed system are promising and efficient. The proposed system can also be integrated with different systems and be used for solving other relevant parking problems.

</p>
</details>

<details><summary><b>How do lexical semantics affect translation? An empirical study</b>
<a href="https://arxiv.org/abs/2201.00075">arxiv:2201.00075</a>
&#x1F4C8; 1 <br>
<p>Vivek Subramanian, Dhanasekar Sundararaman</p></summary>
<p>

**Abstract:** Neural machine translation (NMT) systems aim to map text from one language into another. While there are a wide variety of applications of NMT, one of the most important is translation of natural language. A distinguishing factor of natural language is that words are typically ordered according to the rules of the grammar of a given language. Although many advances have been made in developing NMT systems for translating natural language, little research has been done on understanding how the word ordering of and lexical similarity between the source and target language affect translation performance. Here, we investigate these relationships on a variety of low-resource language pairs from the OpenSubtitles2016 database, where the source language is English, and find that the more similar the target language is to English, the greater the translation performance. In addition, we study the impact of providing NMT models with part of speech of words (POS) in the English sequence and find that, for Transformer-based models, the more dissimilar the target language is from English, the greater the benefit provided by POS.

</p>
</details>

<details><summary><b>Multi-Dimensional Model Compression of Vision Transformer</b>
<a href="https://arxiv.org/abs/2201.00043">arxiv:2201.00043</a>
&#x1F4C8; 1 <br>
<p>Zejiang Hou, Sun-Yuan Kung</p></summary>
<p>

**Abstract:** Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. We firstly propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying deleterious components. Moreover, we cast the multi-dimensional compression as an optimization, learning the optimal pruning policy across the three dimensions that maximizes the compressed model's accuracy under a computational budget. The problem is solved by our adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models, outperforming previous state-of-the-arts.

</p>
</details>

<details><summary><b>TransLog: A Unified Transformer-based Framework for Log Anomaly Detection</b>
<a href="https://arxiv.org/abs/2201.00016">arxiv:2201.00016</a>
&#x1F4C8; 0 <br>
<p>Hongcheng Guo, Xingyu Lin, Jian Yang, Yi Zhuang, Jiaqi Bai, Bo Zhang, Tieqiao Zheng, Zhoujun Li</p></summary>
<p>

**Abstract:** Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios especially for low-resource domains. However, previous deep models merely focused on extracting the semantics of log sequence in the same domain, leading to poor generalization on multi-domain logs. Therefore, we propose a unified Transformer-based framework for log anomaly detection (\ourmethod{}), which is comprised of the pretraining and adapter-based tuning stage. Our model is first pretrained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer the pretrained model to the target domain via the adapter-based tuning. The proposed method is evaluated on three public datasets including one source domain and two target domains. The experimental results demonstrate that our simple yet efficient approach, with fewer trainable parameters and lower training costs in the target domain, achieves state-of-the-art performance on three benchmarks.

</p>
</details>

<details><summary><b>Infinite width (finite depth) neural networks benefit from multi-task learning unlike shallow Gaussian Processes -- an exact quantitative macroscopic characterization</b>
<a href="https://arxiv.org/abs/2112.15577">arxiv:2112.15577</a>
&#x1F4C8; 0 <br>
<p>Jakob Heiss, Josef Teichmann, Hanna Wutte</p></summary>
<p>

**Abstract:** We prove in this paper that optimizing wide ReLU neural networks (NNs) with at least one hidden layer using l2-regularization on the parameters enforces multi-task learning due to representation-learning - also in the limit of width to infinity. This is in contrast to multiple other results in the literature, in which idealized settings are assumed and where wide (ReLU)-NNs loose their ability to benefit from multi-task learning in the infinite width limit. We deduce the ability of multi-task learning from proving an exact quantitative macroscopic characterization of the learned NN in an appropriate function space.

</p>
</details>

<details><summary><b>An overview of the quantitative causality analysis and causal graph reconstruction based on a rigorous formalism of information flow</b>
<a href="https://arxiv.org/abs/2112.14839">arxiv:2112.14839</a>
&#x1F4C8; 0 <br>
<p>X. San Liang</p></summary>
<p>

**Abstract:** Inference of causal relations from data now has become an important field in artificial intelligence. During the past 16 years, causality analysis (in a quantitative sense) has been developed independently in physics from first principles. This short note is a brief summary of this line of work, including part of the theory and several representative applications.

</p>
</details>


{% endraw %}
Prev: [2021.12.30]({{ '/2021/12/30/2021.12.30.html' | relative_url }})  Next: [2022.01.01]({{ '/2022/01/01/2022.01.01.html' | relative_url }})