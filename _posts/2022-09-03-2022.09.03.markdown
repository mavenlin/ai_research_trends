Prev: [2022.09.02]({{ '/2022/09/02/2022.09.02.html' | relative_url }})  Next: [2022.09.04]({{ '/2022/09/04/2022.09.04.html' | relative_url }})
{% raw %}
## Summary for 2022-09-03, created on 2022-09-10


<details><summary><b>Neural Networks for Chess</b>
<a href="https://arxiv.org/abs/2209.01506">arxiv:2209.01506</a>
&#x1F4C8; 136 <br>
<p>Dominik Klein</p></summary>
<p>

**Abstract:** AlphaZero, Leela Chess Zero and Stockfish NNUE revolutionized Computer Chess. This book gives a complete introduction into the technical inner workings of such engines. The book is split into four main chapters -- excluding chapter 1 (introduction) and chapter 6 (conclusion): Chapter 2 introduces neural networks and covers all the basic building blocks that are used to build deep networks such as those used by AlphaZero. Contents include the perceptron, back-propagation and gradient descent, classification, regression, multilayer perceptron, vectorization techniques, convolutional networks, squeeze and excitation networks, fully connected networks, batch normalization and rectified linear units, residual layers, overfitting and underfitting. Chapter 3 introduces classical search techniques used for chess engines as well as those used by AlphaZero. Contents include minimax, alpha-beta search, and Monte Carlo tree search. Chapter 4 shows how modern chess engines are designed. Aside from the ground-breaking AlphaGo, AlphaGo Zero and AlphaZero we cover Leela Chess Zero, Fat Fritz, Fat Fritz 2 and Efficiently Updatable Neural Networks (NNUE) as well as Maia. Chapter 5 is about implementing a miniaturized AlphaZero. Hexapawn, a minimalistic version of chess, is used as an example for that. Hexapawn is solved by minimax search and training positions for supervised learning are generated. Then as a comparison, an AlphaZero-like training loop is implemented where training is done via self-play combined with reinforcement learning. Finally, AlphaZero-like training and supervised training are compared.

</p>
</details>

<details><summary><b>Identify The Beehive Sound Using Deep Learning</b>
<a href="https://arxiv.org/abs/2209.01374">arxiv:2209.01374</a>
&#x1F4C8; 21 <br>
<p>Shah Jafor Sadeek Quaderi, Sadia Afrin Labonno, Sadia Mostafa, Shamim Akhter</p></summary>
<p>

**Abstract:** Flowers play an essential role in removing the duller from the environment. The life cycle of the flowering plants involves pollination, fertilization, flowering, seed-formation, dispersion, and germination. Honeybees pollinate approximately 75% of all flowering plants. Environmental pollution, climate change, natural landscape demolition, and so on, threaten the natural habitats, thus continuously reducing the number of honeybees. As a result, several researchers are attempting to resolve this issue. Applying acoustic classification to recordings of beehive sounds may be a way of detecting changes within them. In this research, we use deep learning techniques, namely Sequential Neural Network, Convolutional Neural Network, and Recurrent Neural Network, on the recorded sounds to classify bee sounds from the nonbeehive noises. In addition, we perform a comparative study among some popular non-deep learning techniques, namely Support Vector Machine, Decision Tree, Random Forest, and Naïve Bayes, with the deep learning techniques. The techniques are also verified on the combined recorded sounds (25-75% noises).

</p>
</details>

<details><summary><b>Equivariant Self-Supervision for Musical Tempo Estimation</b>
<a href="https://arxiv.org/abs/2209.01478">arxiv:2209.01478</a>
&#x1F4C8; 19 <br>
<p>Elio Quinton</p></summary>
<p>

**Abstract:** Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. Contrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. In this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling. Our experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. As an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.

</p>
</details>

<details><summary><b>Generative Modeling via Tree Tensor Network States</b>
<a href="https://arxiv.org/abs/2209.01341">arxiv:2209.01341</a>
&#x1F4C8; 8 <br>
<p>Xun Tang, Yoonhaeng Hur, Yuehaw Khoo, Lexing Ying</p></summary>
<p>

**Abstract:** In this paper, we present a density estimation framework based on tree tensor-network states. The proposed method consists of determining the tree topology with Chow-Liu algorithm, and obtaining a linear system of equations that defines the tensor-network components via sketching techniques. Novel choices of sketch functions are developed in order to consider graphical models that contain loops. Sample complexity guarantees are provided and further corroborated by numerical experiments.

</p>
</details>

<details><summary><b>HammingMesh: A Network Topology for Large-Scale Deep Learning</b>
<a href="https://arxiv.org/abs/2209.01346">arxiv:2209.01346</a>
&#x1F4C8; 7 <br>
<p>Torsten Hoefler, Tommaso Bonato, Daniele De Sensi, Salvatore Di Girolamo, Shigang Li, Marco Heddes, Jon Belk, Deepak Goel, Miguel Castro, Steve Scott</p></summary>
<p>

**Abstract:** Numerous microarchitectural optimizations unlocked tremendous processing power for deep neural networks that in turn fueled the AI revolution. With the exhaustion of such optimizations, the growth of modern AI is now gated by the performance of training systems, especially their data movement. Instead of focusing on single accelerators, we investigate data-movement characteristics of large-scale training at full system scale. Based on our workload analysis, we design HammingMesh, a novel network topology that provides high bandwidth at low cost with high job scheduling flexibility. Specifically, HammingMesh can support full bandwidth and isolation to deep learning training jobs with two dimensions of parallelism. Furthermore, it also supports high global bandwidth for generic traffic. Thus, HammingMesh will power future large-scale deep learning systems with extreme bandwidth requirements.

</p>
</details>

<details><summary><b>Meta-Learning with Less Forgetting on Large-Scale Non-Stationary Task Distributions</b>
<a href="https://arxiv.org/abs/2209.01501">arxiv:2209.01501</a>
&#x1F4C8; 6 <br>
<p>Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Donglin Zhan, Tiehang Duan, Mingchen Gao</p></summary>
<p>

**Abstract:** The paradigm of machine intelligence moves from purely supervised learning to a more practical scenario when many loosely related unlabeled data are available and labeled data is scarce. Most existing algorithms assume that the underlying task distribution is stationary. Here we consider a more realistic and challenging setting in that task distributions evolve over time. We name this problem as Semi-supervised meta-learning with Evolving Task diStributions, abbreviated as SETS. Two key challenges arise in this more realistic setting: (i) how to use unlabeled data in the presence of a large amount of unlabeled out-of-distribution (OOD) data; and (ii) how to prevent catastrophic forgetting on previously learned task distributions due to the task distribution shift. We propose an OOD Robust and knowleDge presErved semi-supeRvised meta-learning approach (ORDER), to tackle these two major challenges. Specifically, our ORDER introduces a novel mutual information regularization to robustify the model with unlabeled OOD data and adopts an optimal transport regularization to remember previously learned knowledge in feature space. In addition, we test our method on a very challenging dataset: SETS on large-scale non-stationary semi-supervised task distributions consisting of (at least) 72K tasks. With extensive experiments, we demonstrate the proposed ORDER alleviates forgetting on evolving task distributions and is more robust to OOD data than related strong baselines.

</p>
</details>

<details><summary><b>From Monte Carlo to neural networks approximations of boundary value problems</b>
<a href="https://arxiv.org/abs/2209.01432">arxiv:2209.01432</a>
&#x1F4C8; 6 <br>
<p>Lucian Beznea, Iulian Cimpean, Oana Lupascu-Stamate, Ionel Popescu, Arghir Zarnescu</p></summary>
<p>

**Abstract:** In this paper we study probabilistic and neural network approximations for solutions to Poisson equation subject to H\" older or $C^2$ data in general bounded domains of $\mathbb{R}^d$. We aim at two fundamental goals.
  The first, and the most important, we show that the solution to Poisson equation can be numerically approximated in the sup-norm by Monte Carlo methods based on a slight change of the walk on spheres algorithm. This provides estimates which are efficient with respect to the prescribed approximation error and without the curse of dimensionality. In addition, the overall number of samples does not not depend on the point at which the approximation is performed.
  As a second goal, we show that the obtained Monte Carlo solver renders ReLU deep neural network (DNN) solutions to Poisson problem, whose sizes depend at most polynomially in the dimension $d$ and in the desired error. In fact we show that the random DNN provides with high probability a small approximation error and low polynomial complexity in the dimension.

</p>
</details>

<details><summary><b>Training Strategies for Improved Lip-reading</b>
<a href="https://arxiv.org/abs/2209.01383">arxiv:2209.01383</a>
&#x1F4C8; 5 <br>
<p>Pingchuan Ma, Yujiang Wang, Stavros Petridis, Jie Shen, Maja Pantic</p></summary>
<p>

**Abstract:** Several training strategies and temporal models have been recently proposed for isolated word lip-reading in a series of independent works. However, the potential of combining the best strategies and investigating the impact of each of them has not been explored. In this paper, we systematically investigate the performance of state-of-the-art data augmentation approaches, temporal models and other training strategies, like self-distillation and using word boundary indicators. Our results show that Time Masking (TM) is the most important augmentation followed by mixup and Densely-Connected Temporal Convolutional Networks (DC-TCN) are the best temporal model for lip-reading of isolated words. Using self-distillation and word boundary indicators is also beneficial but to a lesser extent. A combination of all the above methods results in a classification accuracy of 93.4%, which is an absolute improvement of 4.6% over the current state-of-the-art performance on the LRW dataset. The performance can be further improved to 94.1% by pre-training on additional datasets. An error analysis of the various training strategies reveals that the performance improves by increasing the classification accuracy of hard-to-recognise words.

</p>
</details>

<details><summary><b>DualCam: A Novel Benchmark Dataset for Fine-grained Real-time Traffic Light Detection</b>
<a href="https://arxiv.org/abs/2209.01357">arxiv:2209.01357</a>
&#x1F4C8; 5 <br>
<p>Harindu Jayarathne, Tharindu Samarakoon, Hasara Koralege, Asitha Divisekara, Ranga Rodrigo, Peshala Jayasekara</p></summary>
<p>

**Abstract:** Traffic light detection is essential for self-driving cars to navigate safely in urban areas. Publicly available traffic light datasets are inadequate for the development of algorithms for detecting distant traffic lights that provide important navigation information. We introduce a novel benchmark traffic light dataset captured using a synchronized pair of narrow-angle and wide-angle cameras covering urban and semi-urban roads. We provide 1032 images for training and 813 synchronized image pairs for testing. Additionally, we provide synchronized video pairs for qualitative analysis. The dataset includes images of resolution 1920$\times$1080 covering 10 different classes. Furthermore, we propose a post-processing algorithm for combining outputs from the two cameras. Results show that our technique can strike a balance between speed and accuracy, compared to the conventional approach of using a single camera frame.

</p>
</details>

<details><summary><b>Do Large Language Models know what humans know?</b>
<a href="https://arxiv.org/abs/2209.01515">arxiv:2209.01515</a>
&#x1F4C8; 4 <br>
<p>Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin Bergen</p></summary>
<p>

**Abstract:** Humans can attribute mental states to others, a capacity known as Theory of Mind. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language develop evidence of Theory of Mind. In a pre-registered analysis, we present a linguistic version of the False Belief Task, widely used to assess Theory of Mind, to both human participants and a state-of-the-art Large Language Model, GPT-3. Both are sensitive to others' beliefs, but the language model does not perform as well as the humans, nor does it explain the full extent of their behavior, despite being exposed to more language than a human would in a lifetime. This suggests that while language exposure may in part explain how humans develop Theory of Mind, other mechanisms are also responsible.

</p>
</details>

<details><summary><b>Quantitative Stopword Generation for Sentiment Analysis via Recursive and Iterative Deletion</b>
<a href="https://arxiv.org/abs/2209.01519">arxiv:2209.01519</a>
&#x1F4C8; 3 <br>
<p>Daniel M. DiPietro</p></summary>
<p>

**Abstract:** Stopwords carry little semantic information and are often removed from text data to reduce dataset size and improve machine learning model performance. Consequently, researchers have sought to develop techniques for generating effective stopword sets. Previous approaches have ranged from qualitative techniques relying upon linguistic experts, to statistical approaches that extract word importance using correlations or frequency-dependent metrics computed on a corpus. We present a novel quantitative approach that employs iterative and recursive feature deletion algorithms to see which words can be deleted from a pre-trained transformer's vocabulary with the least degradation to its performance, specifically for the task of sentiment analysis. Empirically, stopword lists generated via this approach drastically reduce dataset size while negligibly impacting model performance, in one such example shrinking the corpus by 28.4% while improving the accuracy of a trained logistic regression model by 0.25%. In another instance, the corpus was shrunk by 63.7% with a 2.8% decrease in accuracy. These promising results indicate that our approach can generate highly effective stopword sets for specific NLP tasks.

</p>
</details>

<details><summary><b>A Novel Nearest Neighbors Algorithm Based on Power Muirhead Mean</b>
<a href="https://arxiv.org/abs/2209.01514">arxiv:2209.01514</a>
&#x1F4C8; 3 <br>
<p>Kourosh Shahnazari, Seyed Moein Ayyoubzadeh</p></summary>
<p>

**Abstract:** This study aimed to propose a novel classifier based on K-Nearest Neighbors which calculates the local means of every class using the Power Muirhead Mean operator. We have called our new method Power Muirhead Mean K-Nearest Neighbors (PMM-KNN) classifier. The PMM-KNN classifier has several parameters which can be determined and fine-tuned for each problem that is countered as an advantage compared to other Nearest Neighbors methods. We used five well-known datasets to assess PMM-KNN performance. The research results demonstrate that the PMM-KNN has outperformed some of the other classification methods.

</p>
</details>

<details><summary><b>A Novel Knowledge-Based Genetic Algorithm for Robot Path Planning in Complex Environments</b>
<a href="https://arxiv.org/abs/2209.01482">arxiv:2209.01482</a>
&#x1F4C8; 3 <br>
<p>Yanrong Hu, Simon X. Yang</p></summary>
<p>

**Abstract:** In this paper, a novel knowledge-based genetic algorithm for path planning of a mobile robot in unstructured complex environments is proposed, where five problem-specific operators are developed for efficient robot path planning. The proposed genetic algorithm incorporates the domain knowledge of robot path planning into its specialized operators, some of which also combine a local search technique. A unique and simple representation of the robot path is proposed and a simple but effective path evaluation method is developed, where the collisions can be accurately detected and the quality of a robot path is well reflected. The proposed algorithm is capable of finding a near-optimal robot path in both static and dynamic complex environments. The effectiveness and efficiency of the proposed algorithm are demonstrated by simulation studies. The irreplaceable role of the specialized genetic operators in the proposed genetic algorithm for solving the robot path planning problem is demonstrated through a comparison study.

</p>
</details>

<details><summary><b>Machine learning for dynamically predicting the onset of renal replacement therapy in chronic kidney disease patients using claims data</b>
<a href="https://arxiv.org/abs/2209.01469">arxiv:2209.01469</a>
&#x1F4C8; 3 <br>
<p>Daniel Lopez-Martinez, Christina Chen, Ming-Jun Chen</p></summary>
<p>

**Abstract:** Chronic kidney disease (CKD) represents a slowly progressive disorder that can eventually require renal replacement therapy (RRT) including dialysis or renal transplantation. Early identification of patients who will require RRT (as much as 1 year in advance) improves patient outcomes, for example by allowing higher-quality vascular access for dialysis. Therefore, early recognition of the need for RRT by care teams is key to successfully managing the disease. Unfortunately, there is currently no commonly used predictive tool for RRT initiation. In this work, we present a machine learning model that dynamically identifies CKD patients at risk of requiring RRT up to one year in advance using only claims data. To evaluate the model, we studied approximately 3 million Medicare beneficiaries for which we made over 8 million predictions. We showed that the model can identify at risk patients with over 90% sensitivity and specificity. Although additional work is required before this approach is ready for clinical use, this study provides a basis for a screening tool to identify patients at risk within a time window that enables early proactive interventions intended to improve RRT outcomes.

</p>
</details>

<details><summary><b>Optimizing Partial Area Under the Top-k Curve: Theory and Practice</b>
<a href="https://arxiv.org/abs/2209.01398">arxiv:2209.01398</a>
&#x1F4C8; 3 <br>
<p>Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, Qingming Huang</p></summary>
<p>

**Abstract:** Top-k error has become a popular metric for large-scale classification benchmarks due to the inevitable semantic ambiguity among classes. Existing literature on top-k optimization generally focuses on the optimization method of the top-k objective, while ignoring the limitations of the metric itself. In this paper, we point out that the top-k objective lacks enough discrimination such that the induced predictions may give a totally irrelevant label a top rank. To fix this issue, we develop a novel metric named partial Area Under the top-k Curve (AUTKC). Theoretical analysis shows that AUTKC has a better discrimination ability, and its Bayes optimal score function could give a correct top-K ranking with respect to the conditional probability. This shows that AUTKC does not allow irrelevant labels to appear in the top list. Furthermore, we present an empirical surrogate risk minimization framework to optimize the proposed metric. Theoretically, we present (1) a sufficient condition for Fisher consistency of the Bayes optimal score function; (2) a generalization upper bound which is insensitive to the number of classes under a simple hyperparameter setting. Finally, the experimental results on four benchmark datasets validate the effectiveness of our proposed framework.

</p>
</details>

<details><summary><b>Masked Sinogram Model with Transformer for ill-Posed Computed Tomography Reconstruction: a Preliminary Study</b>
<a href="https://arxiv.org/abs/2209.01356">arxiv:2209.01356</a>
&#x1F4C8; 3 <br>
<p>Zhengchun Liu, Rajkumar Kettimuthu, Ian Foster</p></summary>
<p>

**Abstract:** Computed Tomography (CT) is an imaging technique where information about an object are collected at different angles (called projections or scans). Then the cross-sectional image showing the internal structure of the slice is produced by solving an inverse problem. Limited by certain factors such as radiation dosage, projection angles, the produced images can be noisy or contain artifacts. Inspired by the success of transformer for natural language processing, the core idea of this preliminary study is to consider a projection of tomography as a word token, and the whole scan of the cross-section (A.K.A. sinogram) as a sentence in the context of natural language processing. Then we explore the idea of foundation model by training a masked sinogram model (MSM) and fine-tune MSM for various downstream applications including CT reconstruction under data collections restriction (e.g., photon-budget) and a data-driven solution to approximate solutions of the inverse problem for CT reconstruction. Models and data used in this study are available at https://github.com/lzhengchun/TomoTx.

</p>
</details>

<details><summary><b>Federated XGBoost on Sample-Wise Non-IID Data</b>
<a href="https://arxiv.org/abs/2209.01340">arxiv:2209.01340</a>
&#x1F4C8; 3 <br>
<p>Katelinh Jones, Yuya Jeremy Ong, Yi Zhou, Nathalie Baracaldo</p></summary>
<p>

**Abstract:** Federated Learning (FL) is a paradigm for jointly training machine learning algorithms in a decentralized manner which allows for parties to communicate with an aggregator to create and train a model, without exposing the underlying raw data distribution of the local parties involved in the training process. Most research in FL has been focused on Neural Network-based approaches, however Tree-Based methods, such as XGBoost, have been underexplored in Federated Learning due to the challenges in overcoming the iterative and additive characteristics of the algorithm. Decision tree-based models, in particular XGBoost, can handle non-IID data, which is significant for algorithms used in Federated Learning frameworks since the underlying characteristics of the data are decentralized and have risks of being non-IID by nature. In this paper, we focus on investigating the effects of how Federated XGBoost is impacted by non-IID distributions by performing experiments on various sample size-based data skew scenarios and how these models perform under various non-IID scenarios. We conduct a set of extensive experiments across multiple different datasets and different data skew partitions. Our experimental results demonstrate that despite the various partition ratios, the performance of the models stayed consistent and performed close to or equally well against models that were trained in a centralized manner.

</p>
</details>

<details><summary><b>Noise-Robust Bidirectional Learning with Dynamic Sample Reweighting</b>
<a href="https://arxiv.org/abs/2209.01334">arxiv:2209.01334</a>
&#x1F4C8; 3 <br>
<p>Chen-Chen Zong, Zheng-Tao Cao, Hong-Tao Guo, Yun Du, Ming-Kun Xie, Shao-Yuan Li, Sheng-Jun Huang</p></summary>
<p>

**Abstract:** Deep neural networks trained with standard cross-entropy loss are more prone to memorize noisy labels, which degrades their performance. Negative learning using complementary labels is more robust when noisy labels intervene but with an extremely slow model convergence speed. In this paper, we first introduce a bidirectional learning scheme, where positive learning ensures convergence speed while negative learning robustly copes with label noise. Further, a dynamic sample reweighting strategy is proposed to globally weaken the effect of noise-labeled samples by exploiting the excellent discriminatory ability of negative learning on the sample probability distribution. In addition, we combine self-distillation to further improve the model performance. The code is available at \url{https://github.com/chenchenzong/BLDR}.

</p>
</details>

<details><summary><b>A Hybrid Tracking Control Strategy for an Unmanned Underwater Vehicle Aided with Bioinspired Neural Dynamics</b>
<a href="https://arxiv.org/abs/2209.01484">arxiv:2209.01484</a>
&#x1F4C8; 2 <br>
<p>Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden</p></summary>
<p>

**Abstract:** Tracking control has been a vital research topic in robotics. This paper presents a novel hybrid control strategy for an unmanned underwater vehicle (UUV) based on a bioinspired neural dynamics model. An enhanced backstepping kinematic control strategy is first developed to avoid sharp velocity jumps and provides smooth velocity commands relative to conventional methods. Then, a novel sliding mode control is proposed, which is capable of providing smooth and continuous torque commands free from chattering. In comparative studies, the proposed combined hybrid control strategy has ensured control signals smoothness, which is critical in real world applications, especially for an unmanned underwater vehicle that needs to operate in complex underwater environments.

</p>
</details>

<details><summary><b>Phishing URL Detection: A Network-based Approach Robust to Evasion</b>
<a href="https://arxiv.org/abs/2209.01454">arxiv:2209.01454</a>
&#x1F4C8; 2 <br>
<p>Taeri Kim, Noseong Park, Jiwon Hong, Sang-Wook Kim</p></summary>
<p>

**Abstract:** Many cyberattacks start with disseminating phishing URLs. When clicking these phishing URLs, the victim's private information is leaked to the attacker. There have been proposed several machine learning methods to detect phishing URLs. However, it still remains under-explored to detect phishing URLs with evasion, i.e., phishing URLs that pretend to be benign by manipulating patterns. In many cases, the attacker i) reuses prepared phishing web pages because making a completely brand-new set costs non-trivial expenses, ii) prefers hosting companies that do not require private information and are cheaper than others, iii) prefers shared hosting for cost efficiency, and iv) sometimes uses benign domains, IP addresses, and URL string patterns to evade existing detection methods. Inspired by those behavioral characteristics, we present a network-based inference method to accurately detect phishing URLs camouflaged with legitimate patterns, i.e., robust to evasion. In the network approach, a phishing URL will be still identified as phishy even after evasion unless a majority of its neighbors in the network are evaded at the same time. Our method consistently shows better detection performance throughout various experimental tests than state-of-the-art methods, e.g., F-1 of 0.89 for our method vs. 0.84 for the best feature-based method.

</p>
</details>

<details><summary><b>Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot</b>
<a href="https://arxiv.org/abs/2209.01434">arxiv:2209.01434</a>
&#x1F4C8; 2 <br>
<p>Yuxue Cao, Shengjie Wang, Xiang Zheng, Wenke Ma, Xinru Xie, Lei Liu</p></summary>
<p>

**Abstract:** Reinforcement learning methods as a promising technique have achieved superior results in the motion planning of free-floating space robots. However, due to the increase in planning dimension and the intensification of system dynamics coupling, the motion planning of dual-arm free-floating space robots remains an open challenge. In particular, the current study cannot handle the task of capturing a non-cooperative object due to the lack of the pose constraint of the end-effectors. To address the problem, we propose a novel algorithm, EfficientLPT, to facilitate RL-based methods to improve planning accuracy efficiently. Our core contributions are constructing a mixed policy with prior knowledge guidance and introducing infinite norm to build a more reasonable reward function. Furthermore, our method successfully captures a rotating object with different spinning speeds.

</p>
</details>

<details><summary><b>Classification of Breast Tumours Based on Histopathology Images Using Deep Features and Ensemble of Gradient Boosting Methods</b>
<a href="https://arxiv.org/abs/2209.01380">arxiv:2209.01380</a>
&#x1F4C8; 2 <br>
<p>Mohammad Reza Abbasniya, Sayed Ali Sheikholeslamzadeh, Hamid Nasiri, Samaneh Emami</p></summary>
<p>

**Abstract:** Breast cancer is the most common cancer among women worldwide. Early-stage diagnosis of breast cancer can significantly improve the efficiency of treatment. Computer-aided diagnosis (CAD) systems are widely adopted in this issue due to their reliability, accuracy and affordability. There are different imaging techniques for a breast cancer diagnosis; one of the most accurate ones is histopathology which is used in this paper. Deep feature transfer learning is used as the main idea of the proposed CAD system's feature extractor. Although 16 different pre-trained networks have been tested in this study, our main focus is on the classification phase. The Inception-ResNet-v2 which has both residual and inception networks profits together has shown the best feature extraction capability in the case of breast cancer histopathology images among all tested CNNs. In the classification phase, the ensemble of CatBoost, XGBoost and LightGBM has provided the best average accuracy. The BreakHis dataset was used to evaluate the proposed method. BreakHis contains 7909 histopathology images (2,480 benign and 5,429 malignant) in four magnification factors. The proposed method's accuracy (IRv2-CXL) using 70% of BreakHis dataset as training data in 40x, 100x, 200x and 400x magnification is 96.82%, 95.84%, 97.01% and 96.15%, respectively. Most studies on automated breast cancer detection have focused on feature extraction, which made us attend to the classification phase. IRv2-CXL has shown better or comparable results in all magnifications due to using the soft voting ensemble method which could combine the advantages of CatBoost, XGBoost and LightGBM together.

</p>
</details>

<details><summary><b>Semi-supervised Training for Knowledge Base Graph Self-attention Networks on Link Prediction</b>
<a href="https://arxiv.org/abs/2209.01350">arxiv:2209.01350</a>
&#x1F4C8; 2 <br>
<p>Shuanglong Yao, Dechang Pi, Junfu Chen, Yufei Liu, Zhiyuan Wu</p></summary>
<p>

**Abstract:** The task of link prediction aims to solve the problem of incomplete knowledge caused by the difficulty of collecting facts from the real world. GCNs-based models are widely applied to solve link prediction problems due to their sophistication, but GCNs-based models are suffering from two problems in the structure and training process. 1) The transformation methods of GCN layers become increasingly complex in GCN-based knowledge representation models; 2) Due to the incompleteness of the knowledge graph collection process, there are many uncollected true facts in the labeled negative samples. Therefore, this paper investigates the characteristic of the information aggregation coefficient (self-attention) of adjacent nodes and redesigns the self-attention mechanism of the GAT structure. Meanwhile, inspired by human thinking habits, we designed a semi-supervised self-training method over pre-trained models. Experimental results on the benchmark datasets FB15k-237 and WN18RR show that our proposed self-attention mechanism and semi-supervised self-training method can effectively improve the performance of the link prediction task. If you look at FB15k-237, for example, the proposed method improves Hits@1 by about 30%.

</p>
</details>

<details><summary><b>Quadratic Gradient: Uniting Gradient Algorithm and Newton Method as One</b>
<a href="https://arxiv.org/abs/2209.03282">arxiv:2209.03282</a>
&#x1F4C8; 1 <br>
<p>John Chiang</p></summary>
<p>

**Abstract:** It might be inadequate for the line search technique for Newton's method to use only one floating point number. A column vector of the same size as the gradient might be better than a mere float number to accelerate each of the gradient elements with different rates. Moreover, a square matrix of the same order as the Hessian matrix might be helpful to correct the Hessian matrix. Chiang applied something between a column vector and a square matrix, namely a diagonal matrix, to accelerate the gradient and further proposed a faster gradient variant called quadratic gradient. In this paper, we present a new way to build a new version of the quadratic gradient. This new quadratic gradient doesn't satisfy the convergence conditions of the fixed Hessian Newton's method. However, experimental results show that it sometimes has a better performance than the original one in convergence rate. Also, Chiang speculates that there might be a relation between the Hessian matrix and the learning rate for the first-order gradient descent method. We prove that the floating number $\frac{1}{ε+ \max \{| λ_i | \}}$ can be a good learning rate of the gradient methods, where $ε$ is a number to avoid division by zero and $λ_i$ the eigenvalues of the Hessian matrix.

</p>
</details>

<details><summary><b>StreamNet: A WAE for White Matter Streamline Analysis</b>
<a href="https://arxiv.org/abs/2209.01498">arxiv:2209.01498</a>
&#x1F4C8; 1 <br>
<p>Andrew Lizarraga, Katherine L. Narr, Kristy A. Donald, Shantanu H. Joshi</p></summary>
<p>

**Abstract:** We present StreamNet, an autoencoder architecture for the analysis of the highly heterogeneous geometry of large collections of white matter streamlines. This proposed framework takes advantage of geometry-preserving properties of the Wasserstein-1 metric in order to achieve direct encoding and reconstruction of entire bundles of streamlines. We show that the model not only accurately captures the distributive structures of streamlines in the population, but is also able to achieve superior reconstruction performance between real and synthetic streamlines. Experimental model performance is evaluated on white matter streamlines resulting from T1-weighted diffusion imaging of 40 healthy controls using recent state of the art bundle comparison metric that measures fiber-shape similarities.

</p>
</details>

<details><summary><b>Deep Live Video Ad Placement on the 5G Edge</b>
<a href="https://arxiv.org/abs/2209.01421">arxiv:2209.01421</a>
&#x1F4C8; 1 <br>
<p>Mohammad Hosseini</p></summary>
<p>

**Abstract:** The video broadcasting industry has been growing significantly in the recent years, specially on delivering personalized contents to the end users. While video broadcasting has continued to grow beyond TV, video adverting has become a key marketing tool to deliver targeted messages directly to the audience. However, unfortunately for broadband TV, a key problem is that the TV commercials target the broad audience, therefore lacking user-specific and personalized ad contents.
  In this paper, we propose a deep edge-cloud ad-placement system, and briefly describe our methodologies and the architecture of our designed ad placement system for delivering both the Video on Demand (VoD) and live broadcast TV contents over MMT streaming protocol. The aim of our paper is to showcase how to enable targeted, personalized, and user-specific advertising services deployed on the future 5G MEC platforms, which in turn can have high potentials to increase ad revenues for the mobile operator industry.

</p>
</details>

<details><summary><b>Deep learning automates bidimensional and volumetric tumor burden measurement from MRI in pre- and post-operative glioblastoma patients</b>
<a href="https://arxiv.org/abs/2209.01402">arxiv:2209.01402</a>
&#x1F4C8; 1 <br>
<p>Jakub Nalepa, Krzysztof Kotowski, Bartosz Machura, Szymon Adamski, Oskar Bozek, Bartosz Eksner, Bartosz Kokoszka, Tomasz Pekala, Mateusz Radom, Marek Strzelczak, Lukasz Zarudzki, Agata Krason, Filippo Arcadu, Jean Tessier</p></summary>
<p>

**Abstract:** Tumor burden assessment by magnetic resonance imaging (MRI) is central to the evaluation of treatment response for glioblastoma. This assessment is complex to perform and associated with high variability due to the high heterogeneity and complexity of the disease. In this work, we tackle this issue and propose a deep learning pipeline for the fully automated end-to-end analysis of glioblastoma patients. Our approach simultaneously identifies tumor sub-regions, including the enhancing tumor, peritumoral edema and surgical cavity in the first step, and then calculates the volumetric and bidimensional measurements that follow the current Response Assessment in Neuro-Oncology (RANO) criteria. Also, we introduce a rigorous manual annotation process which was followed to delineate the tumor sub-regions by the human experts, and to capture their segmentation confidences that are later used while training the deep learning models. The results of our extensive experimental study performed over 760 pre-operative and 504 post-operative adult patients with glioma obtained from the public database (acquired at 19 sites in years 2021-2020) and from a clinical treatment trial (47 and 69 sites for pre-/post-operative patients, 2009-2011) and backed up with thorough quantitative, qualitative and statistical analysis revealed that our pipeline performs accurate segmentation of pre- and post-operative MRIs in a fraction of the manual delineation time (up to 20 times faster than humans). The bidimensional and volumetric measurements were in strong agreement with expert radiologists, and we showed that RANO measurements are not always sufficient to quantify tumor burden.

</p>
</details>

<details><summary><b>SaleNet: A low-power end-to-end CNN accelerator for sustained attention level evaluation using EEG</b>
<a href="https://arxiv.org/abs/2209.01386">arxiv:2209.01386</a>
&#x1F4C8; 1 <br>
<p>Chao Zhang, Zijian Tang, Taoming Guo, Jiaxin Lei, Jiaxin Xiao, Anhe Wang, Shuo Bai, Milin Zhang</p></summary>
<p>

**Abstract:** This paper proposes SaleNet - an end-to-end convolutional neural network (CNN) for sustained attention level evaluation using prefrontal electroencephalogram (EEG). A bias-driven pruning method is proposed together with group convolution, global average pooling (GAP), near-zero pruning, weight clustering and quantization for the model compression, achieving a total compression ratio of 183.11x. The compressed SaleNet obtains a state-of-the-art subject-independent sustained attention level classification accuracy of 84.2% on the recorded 6-subject EEG database in this work. The SaleNet is implemented on a Artix-7 FPGA with a competitive power consumption of 0.11 W and an energy-efficiency of 8.19 GOps/W.

</p>
</details>


{% endraw %}
Prev: [2022.09.02]({{ '/2022/09/02/2022.09.02.html' | relative_url }})  Next: [2022.09.04]({{ '/2022/09/04/2022.09.04.html' | relative_url }})