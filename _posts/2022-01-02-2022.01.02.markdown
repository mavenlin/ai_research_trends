Prev: [2022.01.01]({{ '/2022/01/01/2022.01.01.html' | relative_url }})  Next: [2022.01.03]({{ '/2022/01/03/2022.01.03.html' | relative_url }})
{% raw %}
## Summary for 2022-01-02, created on 2022-01-12


<details><summary><b>DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents</b>
<a href="https://arxiv.org/abs/2201.00308">arxiv:2201.00308</a>
&#x1F4C8; 22 <br>
<p>Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar</p></summary>
<p>

**Abstract:** Diffusion Probabilistic models have been shown to generate state-of-the-art results on several competitive image synthesis benchmarks but lack a low-dimensional, interpretable latent space, and are slow at generation. On the other hand, Variational Autoencoders (VAEs) typically have access to a low-dimensional latent space but exhibit poor sample quality. Despite recent advances, VAEs usually require high-dimensional hierarchies of the latent codes to generate high-quality samples. We present DiffuseVAE, a novel generative framework that integrates VAE within a diffusion model framework, and leverage this to design a novel conditional parameterization for diffusion models. We show that the resulting model can improve upon the unconditional diffusion model in terms of sampling efficiency while also equipping diffusion models with the low-dimensional VAE inferred latent code. Furthermore, we show that the proposed model can generate high-resolution samples and exhibits synthesis quality comparable to state-of-the-art models on standard benchmarks. Lastly, we show that the proposed method can be used for controllable image synthesis and also exhibits out-of-the-box capabilities for downstream tasks like image super-resolution and denoising. For reproducibility, our source code is publicly available at \url{https://github.com/kpandey008/DiffuseVAE}.

</p>
</details>

<details><summary><b>The Introspective Agent: Interdependence of Strategy, Physiology, and Sensing for Embodied Agents</b>
<a href="https://arxiv.org/abs/2201.00411">arxiv:2201.00411</a>
&#x1F4C8; 6 <br>
<p>Sarah Pratt, Luca Weihs, Ali Farhadi</p></summary>
<p>

**Abstract:** The last few years have witnessed substantial progress in the field of embodied AI where artificial agents, mirroring biological counterparts, are now able to learn from interaction to accomplish complex tasks. Despite this success, biological organisms still hold one large advantage over these simulated agents: adaptation. While both living and simulated agents make decisions to achieve goals (strategy), biological organisms have evolved to understand their environment (sensing) and respond to it (physiology). The net gain of these factors depends on the environment, and organisms have adapted accordingly. For example, in a low vision aquatic environment some fish have evolved specific neurons which offer a predictable, but incredibly rapid, strategy to escape from predators. Mammals have lost these reactive systems, but they have a much larger fields of view and brain circuitry capable of understanding many future possibilities. While traditional embodied agents manipulate an environment to best achieve a goal, we argue for an introspective agent, which considers its own abilities in the context of its environment. We show that different environments yield vastly different optimal designs, and increasing long-term planning is often far less beneficial than other improvements, such as increased physical ability. We present these findings to broaden the definition of improvement in embodied AI passed increasingly complex models. Just as in nature, we hope to reframe strategy as one tool, among many, to succeed in an environment. Code is available at: https://github.com/sarahpratt/introspective.

</p>
</details>

<details><summary><b>ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions</b>
<a href="https://arxiv.org/abs/2201.00382">arxiv:2201.00382</a>
&#x1F4C8; 6 <br>
<p>Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, George H. Chen</p></summary>
<p>

**Abstract:** Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the "rare events" that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.

</p>
</details>

<details><summary><b>LSTM Architecture for Oil Stocks Prices Prediction</b>
<a href="https://arxiv.org/abs/2201.00350">arxiv:2201.00350</a>
&#x1F4C8; 5 <br>
<p>Javad T. Firouzjaee, Pouriya Khaliliyan</p></summary>
<p>

**Abstract:** Oil companies are among the largest companies in the world whose economic indicators in the global stock market have a great impact on the world economy and market due to their relation to gold, crude oil, and the dollar. To quantify these relations we use the correlation feature and the relationships between stocks with the dollar, crude oil, gold, and major oil company stock indices, we create datasets and compare the results of forecasts with real data. To predict the stocks of different companies, we use Recurrent Neural Networks (RNNs) and LSTM, because these stocks change in time series. We carry on empirical experiments and perform on the stock indices dataset to evaluate the prediction performance in terms of several common error metrics such as Mean Square Error (MSE), Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). The received results are promising and present a reasonably accurate prediction for the price of oil companies' stocks in the near future. The results show that RNNs do not have the interpretability, and we cannot improve the model by adding any correlated data.

</p>
</details>

<details><summary><b>maskGRU: Tracking Small Objects in the Presence of Large Background Motions</b>
<a href="https://arxiv.org/abs/2201.00467">arxiv:2201.00467</a>
&#x1F4C8; 4 <br>
<p>Constantine J. Roros, Avinash C. Kak</p></summary>
<p>

**Abstract:** We propose a recurrent neural network-based spatio-temporal framework named maskGRU for the detection and tracking of small objects in videos. While there have been many developments in the area of object tracking in recent years, tracking a small moving object amid other moving objects and actors (such as a ball amid moving players in sports footage) continues to be a difficult task. Existing spatio-temporal networks, such as convolutional Gated Recurrent Units (convGRUs), are difficult to train and have trouble accurately tracking small objects under such conditions. To overcome these difficulties, we developed the maskGRU framework that uses a weighted sum of the internal hidden state produced by a convGRU and a 3-channel mask of the tracked object's predicted bounding box as the hidden state to be used at the next time step of the underlying convGRU. We believe the technique of incorporating a mask into the hidden state through a weighted sum has two benefits: controlling the effect of exploding gradients and introducing an attention-like mechanism into the network by indicating where in the previous video frame the object is located. Our experiments show that maskGRU outperforms convGRU at tracking objects that are small relative to the video resolution even in the presence of other moving objects.

</p>
</details>

<details><summary><b>Actor-Critic Network for Q&A in an Adversarial Environment</b>
<a href="https://arxiv.org/abs/2201.00455">arxiv:2201.00455</a>
&#x1F4C8; 4 <br>
<p>Bejan Sadeghian</p></summary>
<p>

**Abstract:** Significant work has been placed in the Q&A NLP space to build models that are more robust to adversarial attacks. Two key areas of focus are in generating adversarial data for the purposes of training against these situations or modifying existing architectures to build robustness within. This paper introduces an approach that joins these two ideas together to train a critic model for use in an almost reinforcement learning framework. Using the Adversarial SQuAD "Add One Sent" dataset we show that there are some promising signs for this method in protecting against Adversarial attacks.

</p>
</details>

<details><summary><b>Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation</b>
<a href="https://arxiv.org/abs/2201.01196">arxiv:2201.01196</a>
&#x1F4C8; 3 <br>
<p>Mohammadamin Tavakoli, Alexander Shmakov, Francesco Ceccarelli, Pierre Baldi</p></summary>
<p>

**Abstract:** It is fundamental for science and technology to be able to predict chemical reactions and their properties. To achieve such skills, it is important to develop good representations of chemical reactions, or good deep learning architectures that can learn such representations automatically from the data. There is currently no universal and widely adopted method for robustly representing chemical reactions. Most existing methods suffer from one or more drawbacks, such as: (1) lacking universality; (2) lacking robustness; (3) lacking interpretability; or (4) requiring excessive manual pre-processing. Here we exploit graph-based representations of molecular structures to develop and test a hypergraph attention neural network approach to solve at once the reaction representation and property-prediction problems, alleviating the aforementioned drawbacks. We evaluate this hypergraph representation in three experiments using three independent data sets of chemical reactions. In all experiments, the hypergraph-based approach matches or outperforms other representations and their corresponding models of chemical reactions while yielding interpretable multi-level representations.

</p>
</details>

<details><summary><b>Super-resolution in Molecular Dynamics Trajectory Reconstruction with Bi-Directional Neural Networks</b>
<a href="https://arxiv.org/abs/2201.01195">arxiv:2201.01195</a>
&#x1F4C8; 3 <br>
<p>Ludwig Winkler, Klaus-Robert Müller, Huziel E. Sauceda</p></summary>
<p>

**Abstract:** Molecular dynamics simulations are a cornerstone in science, allowing to investigate from the system's thermodynamics to analyse intricate molecular interactions. In general, to create extended molecular trajectories can be a computationally expensive process, for example, when running $ab-initio$ simulations. Hence, repeating such calculations to either obtain more accurate thermodynamics or to get a higher resolution in the dynamics generated by a fine-grained quantum interaction can be time- and computationally-consuming. In this work, we explore different machine learning (ML) methodologies to increase the resolution of molecular dynamics trajectories on-demand within a post-processing step. As a proof of concept, we analyse the performance of bi-directional neural networks such as neural ODEs, Hamiltonian networks, recurrent neural networks and LSTMs, as well as the uni-directional variants as a reference, for molecular dynamics simulations (here: the MD17 dataset). We have found that Bi-LSTMs are the best performing models; by utilizing the local time-symmetry of thermostated trajectories they can even learn long-range correlations and display high robustness to noisy dynamics across molecular complexity. Our models can reach accuracies of up to 10$^{-4}$ angstroms in trajectory interpolation, while faithfully reconstructing several full cycles of unseen intricate high-frequency molecular vibrations, rendering the comparison between the learned and reference trajectories indistinguishable. The results reported in this work can serve (1) as a baseline for larger systems, as well as (2) for the construction of better MD integrators.

</p>
</details>

<details><summary><b>Adaptive Memory Networks with Self-supervised Learning for Unsupervised Anomaly Detection</b>
<a href="https://arxiv.org/abs/2201.00464">arxiv:2201.00464</a>
&#x1F4C8; 3 <br>
<p>Yuxin Zhang, Jindong Wang, Yiqiang Chen, Han Yu, Tao Qin</p></summary>
<p>

**Abstract:** Unsupervised anomaly detection aims to build models to effectively detect unseen anomalies by only training on the normal data. Although previous reconstruction-based methods have made fruitful progress, their generalization ability is limited due to two critical challenges. First, the training dataset only contains normal patterns, which limits the model generalization ability. Second, the feature representations learned by existing models often lack representativeness which hampers the ability to preserve the diversity of normal patterns. In this paper, we propose a novel approach called Adaptive Memory Network with Self-supervised Learning (AMSL) to address these challenges and enhance the generalization ability in unsupervised anomaly detection. Based on the convolutional autoencoder structure, AMSL incorporates a self-supervised learning module to learn general normal patterns and an adaptive memory fusion module to learn rich feature representations. Experiments on four public multivariate time series datasets demonstrate that AMSL significantly improves the performance compared to other state-of-the-art methods. Specifically, on the largest CAP sleep stage detection dataset with 900 million samples, AMSL outperforms the second-best baseline by \textbf{4}\%+ in both accuracy and F1 score. Apart from the enhanced generalization ability, AMSL is also more robust against input noise.

</p>
</details>

<details><summary><b>Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark</b>
<a href="https://arxiv.org/abs/2201.00458">arxiv:2201.00458</a>
&#x1F4C8; 3 <br>
<p>Parnian Afshar, Arash Mohammadi, Konstantinos N. Plataniotis, Keyvan Farahani, Justin Kirby, Anastasia Oikonomou, Amir Asif, Leonard Wee, Andre Dekker, Xin Wu, Mohammad Ariful Haque, Shahruk Hossain, Md. Kamrul Hasan, Uday Kamal, Winston Hsu, Jhih-Yuan Lin, M. Sohel Rahman, Nabil Ibtehaz, Sh. M. Amir Foisol, Kin-Man Lam, Zhong Guang, Runze Zhang, Sumohana S. Channappayya, Shashank Gupta, Chander Dev</p></summary>
<p>

**Abstract:** Lung cancer is one of the deadliest cancers, and in part its effective diagnosis and treatment depend on the accurate delineation of the tumor. Human-centered segmentation, which is currently the most common approach, is subject to inter-observer variability, and is also time-consuming, considering the fact that only experts are capable of providing annotations. Automatic and semi-automatic tumor segmentation methods have recently shown promising results. However, as different researchers have validated their algorithms using various datasets and performance metrics, reliably evaluating these methods is still an open challenge. The goal of the Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark created through 2018 IEEE Video and Image Processing (VIP) Cup competition, is to provide a unique dataset and pre-defined metrics, so that different researchers can develop and evaluate their methods in a unified fashion. The 2018 VIP Cup started with a global engagement from 42 countries to access the competition data. At the registration stage, there were 129 members clustered into 28 teams from 10 countries, out of which 9 teams made it to the final stage and 6 teams successfully completed all the required tasks. In a nutshell, all the algorithms proposed during the competition, are based on deep learning models combined with a false positive reduction technique. Methods developed by the three finalists show promising results in tumor segmentation, however, more effort should be put into reducing the false positive rate. This competition manuscript presents an overview of the VIP-Cup challenge, along with the proposed algorithms and results.

</p>
</details>

<details><summary><b>FUSeg: The Foot Ulcer Segmentation Challenge</b>
<a href="https://arxiv.org/abs/2201.00414">arxiv:2201.00414</a>
&#x1F4C8; 3 <br>
<p>Chuanbo Wang, Amirreza Mahbod, Isabella Ellinger, Adrian Galdran, Sandeep Gopalakrishnan, Jeffrey Niezgoda, Zeyun Yu</p></summary>
<p>

**Abstract:** Acute and chronic wounds with varying etiologies burden the healthcare systems economically. The advanced wound care market is estimated to reach $22 billion by 2024. Wound care professionals provide proper diagnosis and treatment with heavy reliance on images and image documentation. Segmentation of wound boundaries in images is a key component of the care and diagnosis protocol since it is important to estimate the area of the wound and provide quantitative measurement for the treatment. Unfortunately, this process is very time-consuming and requires a high level of expertise. Recently automatic wound segmentation methods based on deep learning have shown promising performance but require large datasets for training and it is unclear which methods perform better. To address these issues, we propose the Foot Ulcer Segmentation challenge (FUSeg) organized in conjunction with the 2021 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI). We built a wound image dataset containing 1,210 foot ulcer images collected over 2 years from 889 patients. It is pixel-wise annotated by wound care experts and split into a training set with 1010 images and a testing set with 200 images for evaluation. Teams around the world developed automated methods to predict wound segmentations on the testing set of which annotations were kept private. The predictions were evaluated and ranked based on the average Dice coefficient. The FUSeg challenge remains an open challenge as a benchmark for wound segmentation after the conference.

</p>
</details>

<details><summary><b>Fair Data Representation for Machine Learning at the Pareto Frontier</b>
<a href="https://arxiv.org/abs/2201.00292">arxiv:2201.00292</a>
&#x1F4C8; 3 <br>
<p>Shizhou Xu, Thomas Strohmer</p></summary>
<p>

**Abstract:** As machine learning powered decision making is playing an increasingly important role in our daily lives, it is imperative to strive for fairness of the underlying data processing and algorithms. We propose a pre-processing algorithm for fair data representation via which L2- objective supervised learning algorithms result in an estimation of the Pareto frontier between prediction error and statistical disparity. In particular, the present work applies the optimal positive definite affine transport maps to approach the post-processing Wasserstein barycenter characterization of the optimal fair L2-objective supervised learning via a pre-processing data deformation. We call the resulting data Wasserstein pseudo-barycenter. Furthermore, we show that the Wasserstein geodesics from the learning outcome marginals to the barycenter characterizes the Pareto frontier between L2-loss and total Wasserstein distance among learning outcome marginals. Thereby, an application of McCann interpolation generalizes the pseudo-barycenter to a family of data representations via which L2-objective supervised learning algorithms result in the Pareto frontier. Numerical simulations underscore the advantages of the proposed data representation: (1) the pre-processing step is compositive with arbitrary L2-objective supervised learning methods and unseen data; (2) the fair representation protects data privacy by preventing the training machine from direct or indirect access to the sensitive information of the data; (3) the optimal affine map results in efficient computation of fair supervised learning on high-dimensional data; (4) experimental results shed light on the fairness of L2-objective unsupervised learning via the proposed fair data representation.

</p>
</details>

<details><summary><b>Two-level Graph Neural Network</b>
<a href="https://arxiv.org/abs/2201.01190">arxiv:2201.01190</a>
&#x1F4C8; 2 <br>
<p>Xing Ai, Chengyu Sun, Zhihong Zhang, Edwin R Hancock</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNNs) are recently proposed neural network structures for the processing of graph-structured data. Due to their employed neighbor aggregation strategy, existing GNNs focus on capturing node-level information and neglect high-level information. Existing GNNs therefore suffer from representational limitations caused by the Local Permutation Invariance (LPI) problem. To overcome these limitations and enrich the features captured by GNNs, we propose a novel GNN framework, referred to as the Two-level GNN (TL-GNN). This merges subgraph-level information with node-level information. Moreover, we provide a mathematical analysis of the LPI problem which demonstrates that subgraph-level information is beneficial to overcoming the problems associated with LPI. A subgraph counting method based on the dynamic programming algorithm is also proposed, and this has time complexity is O(n^3), n is the number of nodes of a graph. Experiments show that TL-GNN outperforms existing GNNs and achieves state-of-the-art performance.

</p>
</details>

<details><summary><b>RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark</b>
<a href="https://arxiv.org/abs/2201.00466">arxiv:2201.00466</a>
&#x1F4C8; 2 <br>
<p>Zhuo Deng, Yuanhao Cai, Lu Chen, Zheng Gong, Qiqi Bao, Xue Yao, Dong Fang, Shaochong Zhang, Lan Ma</p></summary>
<p>

**Abstract:** Ophthalmologists have used fundus images to screen and diagnose eye diseases. However, different equipments and ophthalmologists pose large variations to the quality of fundus images. Low-quality (LQ) degraded fundus images easily lead to uncertainty in clinical screening and generally increase the risk of misdiagnosis. Thus, real fundus image restoration is worth studying. Unfortunately, real clinical benchmark has not been explored for this task so far. In this paper, we investigate the real clinical fundus image restoration problem. Firstly, We establish a clinical dataset, Real Fundus (RF), including 120 low- and high-quality (HQ) image pairs. Then we propose a novel Transformer-based Generative Adversarial Network (RFormer) to restore the real degradation of clinical fundus images. The key component in our network is the Window-based Self-Attention Block (WSAB) which captures non-local self-similarity and long-range dependencies. To produce more visually pleasant results, a Transformer-based discriminator is introduced. Extensive experiments on our clinical benchmark show that the proposed RFormer significantly outperforms the state-of-the-art (SOTA) methods. In addition, experiments of downstream tasks such as vessel segmentation and optic disc/cup detection demonstrate that our proposed RFormer benefits clinical fundus image analysis and applications. The dataset, code, and models will be released.

</p>
</details>

<details><summary><b>D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2201.00462">arxiv:2201.00462</a>
&#x1F4C8; 2 <br>
<p>Yixuan Wu, Kuanlun Liao, Jintai Chen, Jinhong Wang, Danny Z. Chen, Honghao Gao, Jian Wu</p></summary>
<p>

**Abstract:** Computer-aided medical image segmentation has been applied widely in diagnosis and treatment to obtain clinically useful information of shapes and volumes of target organs and tissues. In the past several years, convolutional neural network (CNN) based methods (e.g., U-Net) have dominated this area, but still suffered from inadequate long-range information capturing. Hence, recent work presented computer vision Transformer variants for medical image segmentation tasks and obtained promising performances. Such Transformers model long-range dependency by computing pair-wise patch relations. However, they incur prohibitive computational costs, especially on 3D medical images (e.g., CT and MRI). In this paper, we propose a new method called Dilated Transformer, which conducts self-attention for pair-wise patch relations captured alternately in local and global scopes. Inspired by dilated convolution kernels, we conduct the global self-attention in a dilated manner, enlarging receptive fields without increasing the patches involved and thus reducing computational costs. Based on this design of Dilated Transformer, we construct a U-shaped encoder-decoder hierarchical architecture called D-Former for 3D medical image segmentation. Experiments on the Synapse and ACDC datasets show that our D-Former model, trained from scratch, outperforms various competitive CNN-based or Transformer-based segmentation models at a low computational cost without time-consuming per-training process.

</p>
</details>

<details><summary><b>On the convex hull of convex quadratic optimization problems with indicators</b>
<a href="https://arxiv.org/abs/2201.00387">arxiv:2201.00387</a>
&#x1F4C8; 2 <br>
<p>Linchuan Wei, Alper Atamtürk, Andrés Gómez, Simge Küçükyavuz</p></summary>
<p>

**Abstract:** We consider the convex quadratic optimization problem with indicator variables and arbitrary constraints on the indicators. We show that a convex hull description of the associated mixed-integer set in an extended space with a quadratic number of additional variables consists of a single positive semidefinite constraint (explicitly stated) and linear constraints. In particular, convexification of this class of problems reduces to describing a polyhedral set in an extended formulation. We also give descriptions in the original space of variables: we provide a description based on an infinite number of conic-quadratic inequalities, which are "finitely generated." In particular, it is possible to characterize whether a given inequality is necessary to describe the convex-hull. The new theory presented here unifies several previously established results, and paves the way toward utilizing polyhedral methods to analyze the convex hull of mixed-integer nonlinear sets.

</p>
</details>

<details><summary><b>Parkour Spot ID: Feature Matching in Satellite and Street view images using Deep Learning</b>
<a href="https://arxiv.org/abs/2201.00377">arxiv:2201.00377</a>
&#x1F4C8; 2 <br>
<p>João Morais, Kaushal Rathi, Bhuvaneshwar Mohan, Shantanu Rajesh</p></summary>
<p>

**Abstract:** How to find places that are not indexed by Google Maps? We propose an intuitive method and framework to locate places based on their distinctive spatial features. The method uses satellite and street view images in machine vision approaches to classify locations. If we can classify locations, we just need to repeat for non-overlapping locations in our area of interest. We assess the proposed system in finding Parkour spots in the campus of Arizona State University. The results are very satisfactory, having found more than 25 new Parkour spots, with a rate of true positives above 60%.

</p>
</details>

<details><summary><b>Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference</b>
<a href="https://arxiv.org/abs/2201.00354">arxiv:2201.00354</a>
&#x1F4C8; 2 <br>
<p>Hao Sun</p></summary>
<p>

**Abstract:** Although it is well known that exploration plays a key role in Reinforcement Learning (RL), prevailing exploration strategies for continuous control tasks in RL are mainly based on naive isotropic Gaussian noise regardless of the causality relationship between action space and the task and consider all dimensions of actions equally important. In this work, we propose to conduct interventions on the primal action space to discover the causal relationship between the action space and the task reward. We propose the method of State-Wise Action Refined (SWAR), which addresses the issue of action space redundancy and promote causality discovery in RL. We formulate causality discovery in RL tasks as a state-dependent action space selection problem and propose two practical algorithms as solutions. The first approach, TD-SWAR, detects task-related actions during temporal difference learning, while the second approach, Dyn-SWAR, reveals important actions through dynamic model prediction. Empirically, both methods provide approaches to understand the decisions made by RL agents and improve learning efficiency in action-redundant tasks.

</p>
</details>

<details><summary><b>Riemannian Nearest-Regularized Subspace Classification for Polarimetric SAR images</b>
<a href="https://arxiv.org/abs/2201.00337">arxiv:2201.00337</a>
&#x1F4C8; 2 <br>
<p>Junfei Shi, Haiyan Jin</p></summary>
<p>

**Abstract:** As a representation learning method, nearest regularized subspace(NRS) algorithm is an effective tool to obtain both accuracy and speed for PolSAR image classification. However, existing NRS methods use the polarimetric feature vector but the PolSAR original covariance matrix(known as Hermitian positive definite(HPD)matrix) as the input. Without considering the matrix structure, existing NRS-based methods cannot learn correlation among channels. How to utilize the original covariance matrix to NRS method is a key problem. To address this limit, a Riemannian NRS method is proposed, which consider the HPD matrices endow in the Riemannian space. Firstly, to utilize the PolSAR original data, a Riemannian NRS method(RNRS) is proposed by constructing HPD dictionary and HPD distance metric. Secondly, a new Tikhonov regularization term is designed to reduce the differences within the same class. Finally, the optimal method is developed and the first-order derivation is inferred. During the experimental test, only T matrix is used in the proposed method, while multiple of features are utilized for compared methods. Experimental results demonstrate the proposed method can outperform the state-of-art algorithms even using less features.

</p>
</details>

<details><summary><b>On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations</b>
<a href="https://arxiv.org/abs/2201.00318">arxiv:2201.00318</a>
&#x1F4C8; 2 <br>
<p>Aamir Miyajiwala, Arnav Ladkat, Samiksha Jagadale, Raviraj Joshi</p></summary>
<p>

**Abstract:** Text classification is a fundamental Natural Language Processing task that has a wide variety of applications, where deep learning approaches have produced state-of-the-art results. While these models have been heavily criticized for their black-box nature, their robustness to slight perturbations in input text has been a matter of concern. In this work, we carry out a data-focused study evaluating the impact of systematic practical perturbations on the performance of the deep learning based text classification models like CNN, LSTM, and BERT-based algorithms. The perturbations are induced by the addition and removal of unwanted tokens like punctuation and stop-words that are minimally associated with the final performance of the model. We show that these deep learning approaches including BERT are sensitive to such legitimate input perturbations on four standard benchmark datasets SST2, TREC-6, BBC News, and tweet_eval. We observe that BERT is more susceptible to the removal of tokens as compared to the addition of tokens. Moreover, LSTM is slightly more sensitive to input perturbations as compared to CNN based model. The work also serves as a practical guide to assessing the impact of discrepancies in train-test conditions on the final performance of models.

</p>
</details>

<details><summary><b>Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers</b>
<a href="https://arxiv.org/abs/2201.03425">arxiv:2201.03425</a>
&#x1F4C8; 1 <br>
<p>Johannes Schneider, Robin Richner, Micha Riser</p></summary>
<p>

**Abstract:** Autograding short textual answers has become much more feasible due to the rise of NLP and the increased availability of question-answer pairs brought about by a shift to online education. Autograding performance is still inferior to human grading. The statistical and black-box nature of state-of-the-art machine learning models makes them untrustworthy, raising ethical concerns and limiting their practical utility. Furthermore, the evaluation of autograding is typically confined to small, monolingual datasets for a specific question type. This study uses a large dataset consisting of about 10 million question-answer pairs from multiple languages covering diverse fields such as math and language, and strong variation in question and answer syntax. We demonstrate the effectiveness of fine-tuning transformer models for autograding for such complex datasets. Our best hyperparameter-tuned model yields an accuracy of about 86.5\%, comparable to the state-of-the-art models that are less general and more tuned to a specific type of question, subject, and language. More importantly, we address trust and ethical concerns. By involving humans in the autograding process, we show how to improve the accuracy of automatically graded answers, achieving accuracy equivalent to that of teaching assistants. We also show how teachers can effectively control the type of errors made by the system and how they can validate efficiently that the autograder's performance on individual exams is close to the expected performance.

</p>
</details>

<details><summary><b>Transfer-learning-based Surrogate Model for Thermal Conductivity of Nanofluids</b>
<a href="https://arxiv.org/abs/2201.00435">arxiv:2201.00435</a>
&#x1F4C8; 1 <br>
<p>Saeel S. Pai, Abhijeet Banthiya</p></summary>
<p>

**Abstract:** Heat transfer characteristics of nanofluids have been extensively studied since the 1990s. Research investigations show that the suspended nanoparticles significantly alter the suspension's thermal properties. The thermal conductivity of nanofluids is one of the properties that is generally found to be greater than that of the base fluid. This increase in thermal conductivity is found to depend on several parameters. Several theories have been proposed to model the thermal conductivities of nanofluids, but there is no reliable universal theory yet to model the anomalous thermal conductivity of nanofluids. In recent years, supervised data-driven methods have been successfully employed to create surrogate models across various scientific disciplines, especially for modeling difficult-to-understand phenomena. These supervised learning methods allow the models to capture highly non-linear phenomena. In this work, we have taken advantage of existing correlations and used them concurrently with available experimental results to develop more robust surrogate models for predicting the thermal conductivity of nanofluids. Artificial neural networks are trained using the transfer learning approach to predict the thermal conductivity enhancement of nanofluids with spherical particles for 32 different particle-fluid combinations (8 particles materials and 4 fluids). The large amount of lower accuracy data generated from correlations is used to coarse-tune the model parameters, and the limited amount of more trustworthy experimental data is used to fine-tune the model parameters. The transfer learning-based models' results are compared with those from baseline models which are trained only on experimental data using a goodness of fit metric. It is found that the transfer learning models perform better with goodness of fit values of 0.93 as opposed to 0.83 from the baseline models.

</p>
</details>

<details><summary><b>Image Denoising with Control over Deep Network Hallucination</b>
<a href="https://arxiv.org/abs/2201.00429">arxiv:2201.00429</a>
&#x1F4C8; 1 <br>
<p>Qiyuan Liang, Florian Cassayre, Haley Owsianko, Majed El Helou, Sabine Süsstrunk</p></summary>
<p>

**Abstract:** Deep image denoisers achieve state-of-the-art results but with a hidden cost. As witnessed in recent literature, these deep networks are capable of overfitting their training distributions, causing inaccurate hallucinations to be added to the output and generalizing poorly to varying data. For better control and interpretability over a deep denoiser, we propose a novel framework exploiting a denoising network. We call it controllable confidence-based image denoising (CCID). In this framework, we exploit the outputs of a deep denoising network alongside an image convolved with a reliable filter. Such a filter can be a simple convolution kernel which does not risk adding hallucinated information. We propose to fuse the two components with a frequency-domain approach that takes into account the reliability of the deep network outputs. With our framework, the user can control the fusion of the two components in the frequency domain. We also provide a user-friendly map estimating spatially the confidence in the output that potentially contains network hallucination. Results show that our CCID not only provides more interpretability and control, but can even outperform both the quantitative performance of the deep denoiser and that of the reliable filter, especially when the test data diverge from the training data.

</p>
</details>

<details><summary><b>Integrating Artificial Intelligence and Augmented Reality in Robotic Surgery: An Initial dVRK Study Using a Surgical Education Scenario</b>
<a href="https://arxiv.org/abs/2201.00383">arxiv:2201.00383</a>
&#x1F4C8; 1 <br>
<p>Yonghao Long, Jianfeng Cao, Anton Deguet, Russell H. Taylor, Qi Dou</p></summary>
<p>

**Abstract:** The demand of competent robot assisted surgeons is progressively expanding, because robot-assisted surgery has become progressively more popular due to its clinical advantages. To meet this demand and provide a better surgical education for surgeon, we develop a novel robotic surgery education system by integrating artificial intelligence surgical module and augmented reality visualization. The artificial intelligence incorporates reinforcement leaning to learn from expert demonstration and then generate 3D guidance trajectory, providing surgical context awareness of the complete surgical procedure. The trajectory information is further visualized in stereo viewer in the dVRK along with other information such as text hint, where the user can perceive the 3D guidance and learn the procedure. The proposed system is evaluated through a preliminary experiment on surgical education task peg-transfer, which proves its feasibility and potential as the next generation of robot-assisted surgery education solution.

</p>
</details>

<details><summary><b>Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring Platforms</b>
<a href="https://arxiv.org/abs/2201.00378">arxiv:2201.00378</a>
&#x1F4C8; 1 <br>
<p>Pau Ferrer-Cid, Jose M. Barcelo-Ordinas, Jorge Garcia-Vidal</p></summary>
<p>

**Abstract:** Air pollution monitoring platforms play a very important role in preventing and mitigating the effects of pollution. Recent advances in the field of graph signal processing have made it possible to describe and analyze air pollution monitoring networks using graphs. One of the main applications is the reconstruction of the measured signal in a graph using a subset of sensors. Reconstructing the signal using information from sensor neighbors can help improve the quality of network data, examples are filling in missing data with correlated neighboring nodes, or correcting a drifting sensor with neighboring sensors that are more accurate. This paper compares the use of various types of graph signal reconstruction methods applied to real data sets of Spanish air pollution reference stations. The methods considered are Laplacian interpolation, graph signal processing low-pass based graph signal reconstruction, and kernel-based graph signal reconstruction, and are compared on actual air pollution data sets measuring O3, NO2, and PM10. The ability of the methods to reconstruct the signal of a pollutant is shown, as well as the computational cost of this reconstruction. The results indicate the superiority of methods based on kernel-based graph signal reconstruction, as well as the difficulties of the methods to scale in an air pollution monitoring network with a large number of low-cost sensors. However, we show that scalability can be overcome with simple methods, such as partitioning the network using a clustering algorithm.

</p>
</details>

<details><summary><b>Recurrent Feature Propagation and Edge Skip-Connections for Automatic Abdominal Organ Segmentation</b>
<a href="https://arxiv.org/abs/2201.00317">arxiv:2201.00317</a>
&#x1F4C8; 1 <br>
<p>Zefan Yang, Di Lin, Yi Wang</p></summary>
<p>

**Abstract:** Automatic segmentation of abdominal organs in computed tomography (CT) images can support radiation therapy and image-guided surgery workflows. Developing of such automatic solutions remains challenging mainly owing to complex organ interactions and blurry boundaries in CT images. To address these issues, we focus on effective spatial context modeling and explicit edge segmentation priors. Accordingly, we propose a 3D network with four main components trained end-to-end including shared encoder, edge detector, decoder with edge skip-connections (ESCs) and recurrent feature propagation head (RFP-Head). To capture wide-range spatial dependencies, the RFP-Head propagates and harvests local features through directed acyclic graphs (DAGs) formulated with recurrent connections in an efficient slice-wise manner, with regard to spatial arrangement of image units. To leverage edge information, the edge detector learns edge prior knowledge specifically tuned for semantic segmentation by exploiting intermediate features from the encoder with the edge supervision. The ESCs then aggregate the edge knowledge with multi-level decoder features to learn a hierarchy of discriminative features explicitly modeling complementarity between organs' interiors and edges for segmentation. We conduct extensive experiments on two challenging abdominal CT datasets with eight annotated organs. Experimental results show that the proposed network outperforms several state-of-the-art models, especially for the segmentation of small and complicated structures (gallbladder, esophagus, stomach, pancreas and duodenum). The code will be publicly available.

</p>
</details>

<details><summary><b>Testing the Robustness of a BiLSTM-based Structural Story Classifier</b>
<a href="https://arxiv.org/abs/2201.02733">arxiv:2201.02733</a>
&#x1F4C8; 0 <br>
<p>Aftab Hussain, Sai Durga Prasad Nanduri, Sneha Seenuvasavarathan</p></summary>
<p>

**Abstract:** The growing prevalence of counterfeit stories on the internet has fostered significant interest towards fast and scalable detection of fake news in the machine learning community. While several machine learning techniques for this purpose have emerged, we observe that there is a need to evaluate the impact of noise on these techniques' performance, where noise constitutes news articles being mistakenly labeled as fake (or real). This work takes a step in that direction, where we examine the impact of noise on a state-of-the-art, structural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake news detection, Hierarchical Discourse-level Structure for Fake News Detection by Karimi and Tang (Reference no. 9).

</p>
</details>

<details><summary><b>Biometrics in the Time of Pandemic: 40% Masked Face Recognition Degradation can be Reduced to 2%</b>
<a href="https://arxiv.org/abs/2201.00461">arxiv:2201.00461</a>
&#x1F4C8; 0 <br>
<p>Leonardo Queiroz, Kenneth Lai, Svetlana Yanushkevich, Vlad Shmerko</p></summary>
<p>

**Abstract:** In this study of the face recognition on masked versus unmasked faces generated using Flickr-Faces-HQ and SpeakingFaces datasets, we report 36.78% degradation of recognition performance caused by the mask-wearing at the time of pandemics, in particular, in border checkpoint scenarios. We have achieved better performance and reduced the degradation to 1.79% using advanced deep learning approaches in the cross-spectral domain.

</p>
</details>


{% endraw %}
Prev: [2022.01.01]({{ '/2022/01/01/2022.01.01.html' | relative_url }})  Next: [2022.01.03]({{ '/2022/01/03/2022.01.03.html' | relative_url }})