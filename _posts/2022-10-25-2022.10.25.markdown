Prev: [2022.10.24]({{ '/2022/10/24/2022.10.24.html' | relative_url }})  Next: [2022.10.26]({{ '/2022/10/26/2022.10.26.html' | relative_url }})
{% raw %}
## Summary for 2022-10-25, created on 2022-10-29


<details><summary><b>PlanT: Explainable Planning Transformers via Object-Level Representations</b>
<a href="https://arxiv.org/abs/2210.14222">arxiv:2210.14222</a>
&#x1F4C8; 478 <br>
<p>Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, A. Sophia Koepke, Zeynep Akata, Andreas Geiger</p></summary>
<p>

**Abstract:** Planning an optimal route in a complex environment requires efficient reasoning about the surrounding scene. While human drivers prioritize important objects and ignore details not relevant to the decision, learning-based planners typically extract features from dense, high-dimensional grid representations containing all vehicle and road context information. In this paper, we propose PlanT, a novel approach for planning in the context of self-driving that uses a standard transformer architecture. PlanT is based on imitation learning with a compact object-level input representation. On the Longest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the driving score of the expert) while being 5.3x faster than equivalent pixel-based planning baselines during inference. Combining PlanT with an off-the-shelf perception module provides a sensor-based driving system that is more than 10 points better in terms of driving score than the existing state of the art. Furthermore, we propose an evaluation protocol to quantify the ability of planners to identify relevant objects, providing insights regarding their decision-making. Our results indicate that PlanT can focus on the most relevant object in the scene, even when this object is geometrically distant.

</p>
</details>

<details><summary><b>In-context Reinforcement Learning with Algorithm Distillation</b>
<a href="https://arxiv.org/abs/2210.14215">arxiv:2210.14215</a>
&#x1F4C8; 333 <br>
<p>Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih</p></summary>
<p>

**Abstract:** We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.

</p>
</details>

<details><summary><b>Lafite2: Few-shot Text-to-Image Generation</b>
<a href="https://arxiv.org/abs/2210.14124">arxiv:2210.14124</a>
&#x1F4C8; 84 <br>
<p>Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, Jinhui Xu</p></summary>
<p>

**Abstract:** Text-to-image generation models have progressed considerably in recent years, which can now generate impressive realistic images from arbitrary text. Most of such models are trained on web-scale image-text paired datasets, which may not be affordable for many researchers. In this paper, we propose a novel method for pre-training text-to-image generation model on image-only datasets. It considers a retrieval-then-optimization procedure to synthesize pseudo text features: for a given image, relevant pseudo text features are first retrieved, then optimized for better alignment. The low requirement of the proposed method yields high flexibility and usability: it can be beneficial to a wide range of settings, including the few-shot, semi-supervised and fully-supervised learning; it can be applied on different models including generative adversarial networks (GANs) and diffusion models. Extensive experiments illustrate the effectiveness of the proposed method. On MS-COCO dataset, our GAN model obtains Fréchet Inception Distance (FID) of 6.78 which is the new state-of-the-art (SoTA) of GANs under fully-supervised setting. Our diffusion model obtains FID of 8.42 and 4.28 on zero-shot and supervised setting respectively, which are competitive to SoTA diffusion models with a much smaller model size.

</p>
</details>

<details><summary><b>Sim-to-Real via Sim-to-Seg: End-to-end Off-road Autonomous Driving Without Real Data</b>
<a href="https://arxiv.org/abs/2210.14721">arxiv:2210.14721</a>
&#x1F4C8; 13 <br>
<p>John So, Amber Xie, Sunggoo Jung, Jeffrey Edlund, Rohan Thakker, Ali Agha-mohammadi, Pieter Abbeel, Stephen James</p></summary>
<p>

**Abstract:** Autonomous driving is complex, requiring sophisticated 3D scene understanding, localization, mapping, and control. Rather than explicitly modelling and fusing each of these components, we instead consider an end-to-end approach via reinforcement learning (RL). However, collecting exploration driving data in the real world is impractical and dangerous. While training in simulation and deploying visual sim-to-real techniques has worked well for robot manipulation, deploying beyond controlled workspace viewpoints remains a challenge. In this paper, we address this challenge by presenting Sim2Seg, a re-imagining of RCAN that crosses the visual reality gap for off-road autonomous driving, without using any real-world data. This is done by learning to translate randomized simulation images into simulated segmentation and depth maps, subsequently enabling real-world images to also be translated. This allows us to train an end-to-end RL policy in simulation, and directly deploy in the real-world. Our approach, which can be trained in 48 hours on 1 GPU, can perform equally as well as a classical perception and control stack that took thousands of engineering hours over several months to build. We hope this work motivates future end-to-end autonomous driving research.

</p>
</details>

<details><summary><b>Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2210.13800">arxiv:2210.13800</a>
&#x1F4C8; 9 <br>
<p>Melanie Sclar, Peter West, Sachin Kumar, Yulia Tsvetkov, Yejin Choi</p></summary>
<p>

**Abstract:** We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation (West et al., 2022), where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models, further purified with three types of filters: length, fidelity, and Information Bottleneck. Moreover, we uniquely propose iterative distillation of knowledge, where student models from the previous iteration of distillation serve as teacher models in the next iteration. Starting off from a relatively modest set of GPT3-generated summaries, we demonstrate how iterative knowledge distillation can lead to considerably smaller, but better summarizers with sharper controllability. A useful by-product of this iterative distillation process is a high-quality dataset of sentence-summary pairs with varying degrees of compression ratios. Empirical results demonstrate that the final student models vastly outperform the much larger GPT3-Instruct model in terms of the controllability of compression ratios, without compromising the quality of resulting summarization.

</p>
</details>

<details><summary><b>Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding</b>
<a href="https://arxiv.org/abs/2210.14169">arxiv:2210.14169</a>
&#x1F4C8; 8 <br>
<p>Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Andy Rosenbaum, Seokhwan Kim, Yang Liu, Zhou Yu, Dilek Hakkani-Tur</p></summary>
<p>

**Abstract:** Dialogue understanding tasks often necessitate abundant annotated data to achieve good performance and that presents challenges in low-resource settings. To alleviate this barrier, we explore few-shot data augmentation for dialogue understanding by prompting large pre-trained language models and present a novel approach that iterates on augmentation quality by applying weakly-supervised filters. We evaluate our methods on the emotion and act classification tasks in DailyDialog and the intent classification task in Facebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our augmented data mixed with few-shot ground truth data are able to approach or surpass existing state-of-the-art performance on both datasets. For DailyDialog specifically, using 10% of the ground truth data we outperform the current state-of-the-art model which uses 100% of the data.

</p>
</details>

<details><summary><b>Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds</b>
<a href="https://arxiv.org/abs/2210.14051">arxiv:2210.14051</a>
&#x1F4C8; 8 <br>
<p>Hao Liang, Zhi-Quan Luo</p></summary>
<p>

**Abstract:** We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. We identify a key property of the EntRM, the monotonicity-preserving property, which enables the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.
  We prove that both of them attain $\tilde{\mathcal{O}}(\frac{\exp(|β| H)-1}{|β|H}H\sqrt{HS^2AT})$ regret upper bound, where $S$ is the number of states, $A$ the number of states, $H$ the time horizon and $T$ the number of total time steps. It matches RSVI2 proposed in \cite{fei2021exponential} with a much simpler regret analysis. To the best of our knowledge, this is the first regret analysis of DRL, which bridges DRL and RSRL in terms of sample complexity. Finally, we improve the existing lower bound by proving a tighter bound of $Ω(\frac{\exp(βH/6)-1}{βH}H\sqrt{SAT})$ for $β>0$ case, which recovers the tight lower bound $Ω(H\sqrt{SAT})$ in the risk-neutral setting.

</p>
</details>

<details><summary><b>Whitening Convergence Rate of Coupling-based Normalizing Flows</b>
<a href="https://arxiv.org/abs/2210.14032">arxiv:2210.14032</a>
&#x1F4C8; 8 <br>
<p>Felix Draxler, Christoph Schnörr, Ullrich Köthe</p></summary>
<p>

**Abstract:** Coupling-based normalizing flows (e.g. RealNVP) are a popular family of normalizing flow architectures that work surprisingly well in practice. This calls for theoretical understanding. Existing work shows that such flows weakly converge to arbitrary data distributions. However, they make no statement about the stricter convergence criterion used in practice, the maximum likelihood loss. For the first time, we make a quantitative statement about this kind of convergence: We prove that all coupling-based normalizing flows perform whitening of the data distribution (i.e. diagonalize the covariance matrix) and derive corresponding convergence bounds that show a linear convergence rate in the depth of the flow. Numerical experiments demonstrate the implications of our theory and point at open questions.

</p>
</details>

<details><summary><b>SciFact-Open: Towards open-domain scientific claim verification</b>
<a href="https://arxiv.org/abs/2210.13777">arxiv:2210.13777</a>
&#x1F4C8; 8 <br>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi</p></summary>
<p>

**Abstract:** While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.

</p>
</details>

<details><summary><b>A Database of Ultrastable MOFs Reassembled from Stable Fragments with Machine Learning Models</b>
<a href="https://arxiv.org/abs/2210.14191">arxiv:2210.14191</a>
&#x1F4C8; 7 <br>
<p>Aditya Nandy, Shuwen Yue, Changhwan Oh, Chenru Duan, Gianmarco G. Terrones, Yongchul G. Chung, Heather J. Kulik</p></summary>
<p>

**Abstract:** High-throughput screening of large hypothetical databases of metal-organic frameworks (MOFs) can uncover new materials, but their stability in real-world applications is often unknown. We leverage community knowledge and machine learning (ML) models to identify MOFs that are thermally stable and stable upon activation. We separate these MOFs into their building blocks and recombine them to make a new hypothetical MOF database of over 50,000 structures that samples orders of magnitude more connectivity nets and inorganic building blocks than prior databases. This database shows an order of magnitude enrichment of ultrastable MOF structures that are stable upon activation and more than one standard deviation more thermally stable than the average experimentally characterized MOF. For the nearly 10,000 ultrastable MOFs, we compute bulk elastic moduli to confirm these materials have good mechanical stability, and we report methane deliverable capacities. Our work identifies privileged metal nodes in ultrastable MOFs that optimize gas storage and mechanical stability simultaneously.

</p>
</details>

<details><summary><b>Influence Functions for Sequence Tagging Models</b>
<a href="https://arxiv.org/abs/2210.14177">arxiv:2210.14177</a>
&#x1F4C8; 7 <br>
<p>Sarthak Jain, Varun Manjunatha, Byron C. Wallace, Ani Nenkova</p></summary>
<p>

**Abstract:** Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging, and Semantic Role Labeling) are naturally framed as sequence tagging problems. However, there has been comparatively little work on interpretability methods for sequence tagging models. In this paper, we extend influence functions - which aim to trace predictions back to the training points that informed them - to sequence tagging tasks. We define the influence of a training instance segment as the effect that perturbing the labels within this segment has on a test segment level prediction. We provide an efficient approximation to compute this, and show that it tracks with the true segment influence, measured empirically. We show the practical utility of segment influence by using the method to identify systematic annotation errors in two named entity recognition corpora. Code to reproduce our results is available at https://github.com/successar/Segment_Influence_Functions.

</p>
</details>

<details><summary><b>Learning Explicit Object-Centric Representations with Vision Transformers</b>
<a href="https://arxiv.org/abs/2210.14139">arxiv:2210.14139</a>
&#x1F4C8; 7 <br>
<p>Oscar Vikström, Alexander Ilin</p></summary>
<p>

**Abstract:** With the recent successful adaptation of transformers to the vision domain, particularly when trained in a self-supervised fashion, it has been shown that vision transformers can learn impressive object-reasoning-like behaviour and features expressive for the task of object segmentation in images. In this paper, we build on the self-supervision task of masked autoencoding and explore its effectiveness for explicitly learning object-centric representations with transformers. To this end, we design an object-centric autoencoder using transformers only and train it end-to-end to reconstruct full images from unmasked patches. We show that the model efficiently learns to decompose simple scenes as measured by segmentation metrics on several multi-object benchmarks.

</p>
</details>

<details><summary><b>IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models</b>
<a href="https://arxiv.org/abs/2210.14128">arxiv:2210.14128</a>
&#x1F4C8; 7 <br>
<p>Chenguang Wang, Xiao Liu, Dawn Song</p></summary>
<p>

**Abstract:** We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer ``fill-in-the-blank'' questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets. Our code and datasets are available at https://github.com/cgraywang/IELM

</p>
</details>

<details><summary><b>Audio MFCC-gram Transformers for respiratory insufficiency detection in COVID-19</b>
<a href="https://arxiv.org/abs/2210.14085">arxiv:2210.14085</a>
&#x1F4C8; 7 <br>
<p>Marcelo Matheus Gauy, Marcelo Finger</p></summary>
<p>

**Abstract:** This work explores speech as a biomarker and investigates the detection of respiratory insufficiency (RI) by analyzing speech samples. Previous work \cite{spira2021} constructed a dataset of respiratory insufficiency COVID-19 patient utterances and analyzed it by means of a convolutional neural network achieving an accuracy of $87.04\%$, validating the hypothesis that one can detect RI through speech. Here, we study how Transformer neural network architectures can improve the performance on RI detection. This approach enables construction of an acoustic model. By choosing the correct pretraining technique, we generate a self-supervised acoustic model, leading to improved performance ($96.53\%$) of Transformers for RI detection.

</p>
</details>

<details><summary><b>Unsupervised Anomaly Detection for Auditing Data and Impact of Categorical Encodings</b>
<a href="https://arxiv.org/abs/2210.14056">arxiv:2210.14056</a>
&#x1F4C8; 7 <br>
<p>Ajay Chawda, Stefanie Grimm, Marius Kloft</p></summary>
<p>

**Abstract:** In this paper, we introduce the Vehicle Claims dataset, consisting of fraudulent insurance claims for automotive repairs. The data belongs to the more broad category of Auditing data, which includes also Journals and Network Intrusion data. Insurance claim data are distinctively different from other auditing data (such as network intrusion data) in their high number of categorical attributes. We tackle the common problem of missing benchmark datasets for anomaly detection: datasets are mostly confidential, and the public tabular datasets do not contain relevant and sufficient categorical attributes. Therefore, a large-sized dataset is created for this purpose and referred to as Vehicle Claims (VC) dataset. The dataset is evaluated on shallow and deep learning methods. Due to the introduction of categorical attributes, we encounter the challenge of encoding them for the large dataset. As One Hot encoding of high cardinal dataset invokes the "curse of dimensionality", we experiment with GEL encoding and embedding layer for representing categorical attributes. Our work compares competitive learning, reconstruction-error, density estimation and contrastive learning approaches for Label, One Hot, GEL encoding and embedding layer to handle categorical values.

</p>
</details>

<details><summary><b>LAB: Learnable Activation Binarizer for Binary Neural Networks</b>
<a href="https://arxiv.org/abs/2210.13858">arxiv:2210.13858</a>
&#x1F4C8; 7 <br>
<p>Sieger Falkena, Hadi Jamali-Rad, Jan van Gemert</p></summary>
<p>

**Abstract:** Binary Neural Networks (BNNs) are receiving an upsurge of attention for bringing power-hungry deep learning towards edge devices. The traditional wisdom in this space is to employ sign() for binarizing featuremaps. We argue and illustrate that sign() is a uniqueness bottleneck, limiting information propagation throughout the network. To alleviate this, we propose to dispense sign(), replacing it with a learnable activation binarizer (LAB), allowing the network to learn a fine-grained binarization kernel per layer - as opposed to global thresholding. LAB is a novel universal module that can seamlessly be integrated into existing architectures. To confirm this, we plug it into four seminal BNNs and show a considerable performance boost at the cost of tolerable increase in delay and complexity. Finally, we build an end-to-end BNN (coined as LAB-BNN) around LAB, and demonstrate that it achieves competitive performance on par with the state-of-the-art on ImageNet.

</p>
</details>

<details><summary><b>Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes</b>
<a href="https://arxiv.org/abs/2210.14410">arxiv:2210.14410</a>
&#x1F4C8; 6 <br>
<p>Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, Zico Kolter</p></summary>
<p>

**Abstract:** This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.

</p>
</details>

<details><summary><b>PolyHope: Dataset Creation for a Two-Level Hope Speech Detection Task from Tweets</b>
<a href="https://arxiv.org/abs/2210.14136">arxiv:2210.14136</a>
&#x1F4C8; 6 <br>
<p>Fazlourrahman Balouchzahi, Grigori Sidorov, Alexander Gelbukh</p></summary>
<p>

**Abstract:** Hope is characterized as openness of spirit toward the future, a desire, expectation, and wish for something to happen or to be true that remarkably affects human's state of mind, emotions, behaviors, and decisions. Hope is usually associated with concepts of desired expectations and possibility/probability concerning the future. Despite its importance, hope has rarely been studied as a social media analysis task. This paper presents a hope speech dataset that classifies each tweet first into "Hope" and "Not Hope", then into three fine-grained hope categories: "Generalized Hope", "Realistic Hope", and "Unrealistic Hope" (along with "Not Hope"). English tweets in the first half of 2022 were collected to build this dataset. Furthermore, we describe our annotation process and guidelines in detail and discuss the challenges of classifying hope and the limitations of the existing hope speech detection corpora. In addition, we reported several baselines based on different learning approaches, such as traditional machine learning, deep learning, and transformers, to benchmark our dataset. We evaluated our baselines using weighted-averaged and macro-averaged F1-scores. Observations show that a strict process for annotator selection and detailed annotation guidelines enhanced the dataset's quality. This strict annotation process resulted in promising performance for simple machine learning classifiers with only bi-grams; however, binary and multiclass hope speech detection results reveal that contextual embedding models have higher performance in this dataset.

</p>
</details>

<details><summary><b>UNIFY: a Unified Policy Designing Framework for Solving Constrained Optimization Problems with Machine Learning</b>
<a href="https://arxiv.org/abs/2210.14030">arxiv:2210.14030</a>
&#x1F4C8; 6 <br>
<p>Mattia Silvestri, Allegra De Filippo, Michele Lombardi, Michela Milano</p></summary>
<p>

**Abstract:** The interplay between Machine Learning (ML) and Constrained Optimization (CO) has recently been the subject of increasing interest, leading to a new and prolific research area covering (e.g.) Decision Focused Learning and Constrained Reinforcement Learning. Such approaches strive to tackle complex decision problems under uncertainty over multiple stages, involving both explicit (cost function, constraints) and implicit knowledge (from data), and possibly subject to execution time restrictions. While a good degree of success has been achieved, the existing methods still have limitations in terms of both applicability and effectiveness. For problems in this class, we propose UNIFY, a unified framework to design a solution policy for complex decision-making problems. Our approach relies on a clever decomposition of the policy in two stages, namely an unconstrained ML model and a CO problem, to take advantage of the strength of each approach while compensating for its weaknesses. With a little design effort, UNIFY can generalize several existing approaches, thus extending their applicability. We demonstrate the method effectiveness on two practical problems, namely an Energy Management System and the Set Multi-cover with stochastic coverage requirements. Finally, we highlight some current challenges of our method and future research directions that can benefit from the cross-fertilization of the two fields.

</p>
</details>

<details><summary><b>A White-Box Adversarial Attack Against a Digital Twin</b>
<a href="https://arxiv.org/abs/2210.14018">arxiv:2210.14018</a>
&#x1F4C8; 6 <br>
<p>Wilson Patterson, Ivan Fernandez, Subash Neupane, Milan Parmar, Sudip Mittal, Shahram Rahimi</p></summary>
<p>

**Abstract:** Recent research has shown that Machine Learning/Deep Learning (ML/DL) models are particularly vulnerable to adversarial perturbations, which are small changes made to the input data in order to fool a machine learning classifier. The Digital Twin, which is typically described as consisting of a physical entity, a virtual counterpart, and the data connections in between, is increasingly being investigated as a means of improving the performance of physical entities by leveraging computational techniques, which are enabled by the virtual counterpart. This paper explores the susceptibility of Digital Twin (DT), a virtual model designed to accurately reflect a physical object using ML/DL classifiers that operate as Cyber Physical Systems (CPS), to adversarial attacks. As a proof of concept, we first formulate a DT of a vehicular system using a deep neural network architecture and then utilize it to launch an adversarial attack. We attack the DT model by perturbing the input to the trained model and show how easily the model can be broken with white-box attacks.

</p>
</details>

<details><summary><b>I Prefer not to Say: Operationalizing Fair and User-guided Data Minimization</b>
<a href="https://arxiv.org/abs/2210.13954">arxiv:2210.13954</a>
&#x1F4C8; 6 <br>
<p>Tobias Leemann, Martin Pawelczyk, Christian Thomas Eberle, Gjergji Kasneci</p></summary>
<p>

**Abstract:** To grant users greater authority over their personal data, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle within these regulations is data minimization, which urges companies and institutions to only collect data that is relevant and adequate for the purpose of the data analysis. In this work, we take a user-centric perspective on this regulation, and let individual users decide which data they deem adequate and relevant to be processed by a machine-learned model. We require that users who decide to provide optional information should appropriately benefit from sharing their data, while users who rely on the mandate to leave their data undisclosed should not be penalized for doing so. This gives rise to the overlooked problem of fair treatment between individuals providing additional information and those choosing not to. While the classical fairness literature focuses on fair treatment between advantaged and disadvantaged groups, an initial look at this problem through the lens of classical fairness notions reveals that they are incompatible with these desiderata. We offer a solution to this problem by proposing the notion of Optional Feature Fairness (OFF) that follows from our requirements. To operationalize OFF, we derive a multi-model strategy and a tractable logistic regression model. We analyze the effect and the cost of applying OFF on several real-world data sets.

</p>
</details>

<details><summary><b>Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future Research Directions</b>
<a href="https://arxiv.org/abs/2210.13927">arxiv:2210.13927</a>
&#x1F4C8; 6 <br>
<p>Md. Haidar Sharif, Lei Jiao, Christian W. Omlin</p></summary>
<p>

**Abstract:** Crowd anomaly detection is one of the most popular topics in computer vision in the context of smart cities. A plethora of deep learning methods have been proposed that generally outperform other machine learning solutions. Our review primarily discusses algorithms that were published in mainstream conferences and journals between 2020 and 2022. We present datasets that are typically used for benchmarking, produce a taxonomy of the developed algorithms, and discuss and compare their performances. Our main findings are that the heterogeneities of pre-trained convolutional models have a negligible impact on crowd video anomaly detection performance. We conclude our discussion with fruitful directions for future research.

</p>
</details>

<details><summary><b>Connective Reconstruction-based Novelty Detection</b>
<a href="https://arxiv.org/abs/2210.13917">arxiv:2210.13917</a>
&#x1F4C8; 6 <br>
<p>Seyyed Morteza Hashemi, Parvaneh Aliniya, Parvin Razzaghi</p></summary>
<p>

**Abstract:** Detection of out-of-distribution samples is one of the critical tasks for real-world applications of computer vision. The advancement of deep learning has enabled us to analyze real-world data which contain unexplained samples, accentuating the need to detect out-of-distribution instances more than before. GAN-based approaches have been widely used to address this problem due to their ability to perform distribution fitting; however, they are accompanied by training instability and mode collapse. We propose a simple yet efficient reconstruction-based method that avoids adding complexities to compensate for the limitations of GAN models while outperforming them. Unlike previous reconstruction-based works that only utilize reconstruction error or generated samples, our proposed method simultaneously incorporates both of them in the detection task. Our model, which we call "Connective Novelty Detection" has two subnetworks, an autoencoder, and a binary classifier. The autoencoder learns the representation of the positive class by reconstructing them. Then, the model creates negative and connected positive examples using real and generated samples. Negative instances are generated via manipulating the real data, so their distribution is close to the positive class to achieve a more accurate boundary for the classifier. To boost the robustness of the detection to reconstruction error, connected positive samples are created by combining the real and generated samples. Finally, the binary classifier is trained using connected positive and negative examples. We demonstrate a considerable improvement in novelty detection over state-of-the-art methods on MNIST and Caltech-256 datasets.

</p>
</details>

<details><summary><b>Zero-Shot Learning of a Conditional Generative Adversarial Network for Data-Free Network Quantization</b>
<a href="https://arxiv.org/abs/2210.14392">arxiv:2210.14392</a>
&#x1F4C8; 5 <br>
<p>Yoojin Choi, Mostafa El-Khamy, Jungwon Lee</p></summary>
<p>

**Abstract:** We propose a novel method for training a conditional generative adversarial network (CGAN) without the use of training data, called zero-shot learning of a CGAN (ZS-CGAN). Zero-shot learning of a conditional generator only needs a pre-trained discriminative (classification) model and does not need any training data. In particular, the conditional generator is trained to produce labeled synthetic samples whose characteristics mimic the original training data by using the statistics stored in the batch normalization layers of the pre-trained model. We show the usefulness of ZS-CGAN in data-free quantization of deep neural networks. We achieved the state-of-the-art data-free network quantization of the ResNet and MobileNet classification models trained on the ImageNet dataset. Data-free quantization using ZS-CGAN showed a minimal loss in accuracy compared to that obtained by conventional data-dependent quantization.

</p>
</details>

<details><summary><b>Learning Ability of Interpolating Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2210.14184">arxiv:2210.14184</a>
&#x1F4C8; 5 <br>
<p>Tian-Yi Zhou, Xiaoming Huo</p></summary>
<p>

**Abstract:** It is frequently observed that overparameterized neural networks generalize well. Regarding such phenomena, existing theoretical work mainly devotes to linear settings or fully connected neural networks. This paper studies learning ability of an important family of deep neural networks, deep convolutional neural networks (DCNNs), under underparameterized and overparameterized settings. We establish the best learning rates of underparameterized DCNNs without parameter restrictions presented in the literature. We also show that, by adding well defined layers to an underparameterized DCNN, we can obtain some interpolating DCNNs that maintain the good learning rates of the underparameterized DCNN. This result is achieved by a novel network deepening scheme designed for DCNNs. Our work provides theoretical verification on how overfitted DCNNs generalize well.

</p>
</details>

<details><summary><b>Learning Individual Treatment Effects under Heterogeneous Interference in Networks</b>
<a href="https://arxiv.org/abs/2210.14080">arxiv:2210.14080</a>
&#x1F4C8; 5 <br>
<p>Ziyu Zhao, Kun Kuang, Ruoxuan Xiong, Fei Wu</p></summary>
<p>

**Abstract:** Estimates of individual treatment effects from networked observational data are attracting increasing attention these days. One major challenge in network scenarios is the violation of the stable unit treatment value assumption (SUTVA), which assumes that the treatment assignment of a unit does not influence others' outcomes. In network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). Furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). In this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. To address this issue, we propose a novel Dual Weighting Regression (DWR) algorithm by simultaneously learning attention weights that capture the heterogeneous interference and sample weights to eliminate the complex confounding bias in networks. We formulate the entire learning process as a bi-level optimization problem. In theory, we present generalization error bounds for individual treatment effect estimation. Extensive experiments on four benchmark datasets demonstrate that the proposed DWR algorithm outperforms state-of-the-art methods for estimating individual treatment effects under heterogeneous interference.

</p>
</details>

<details><summary><b>Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens</b>
<a href="https://arxiv.org/abs/2210.14011">arxiv:2210.14011</a>
&#x1F4C8; 5 <br>
<p>Nitish Joshi, Xiang Pan, He He</p></summary>
<p>

**Abstract:** The term `spurious correlations' has been used in NLP to informally denote any undesirable feature-label correlations. However, a correlation can be undesirable because (i) the feature is irrelevant to the label (e.g. punctuation in a review), or (ii) the feature's effect on the label depends on the context (e.g. negation words in a review), which is ubiquitous in language tasks. In case (i), we want the model to be invariant to the feature, which is neither necessary nor sufficient for prediction. But in case (ii), even an ideal model (e.g. humans) must rely on the feature, since it is necessary (but not sufficient) for prediction. Therefore, a more fine-grained treatment of spurious features is needed to specify the desired model behavior. We formalize this distinction using a causal model and probabilities of necessity and sufficiency, which delineates the causal relations between a feature and a label. We then show that this distinction helps explain results of existing debiasing methods on different spurious features, and demystifies surprising results such as the encoding of spurious features in model representations after debiasing.

</p>
</details>

<details><summary><b>Unsupervised domain-adaptive person re-identification with multi-camera constraints</b>
<a href="https://arxiv.org/abs/2210.13999">arxiv:2210.13999</a>
&#x1F4C8; 5 <br>
<p>S. Takeuchi, F. Li, S. Iwasaki, J. Ning, G. Suzuki</p></summary>
<p>

**Abstract:** Person re-identification is a key technology for analyzing video-based human behavior; however, its application is still challenging in practical situations due to the performance degradation for domains different from those in the training data. Here, we propose an environment-constrained adaptive network for reducing the domain gap. This network refines pseudo-labels estimated via a self-training scheme by imposing multi-camera constraints. The proposed method incorporates person-pair information without person identity labels obtained from the environment into the model training. In addition, we develop a method that appropriately selects a person from the pair that contributes to the performance improvement. We evaluate the performance of the network using public and private datasets and confirm the performance surpasses state-of-the-art methods in domains with overlapping camera views. To the best of our knowledge, this is the first study on domain-adaptive learning with multi-camera constraints that can be obtained in real environments.

</p>
</details>

<details><summary><b>Predicting Survival Outcomes in the Presence of Unlabeled Data</b>
<a href="https://arxiv.org/abs/2210.13891">arxiv:2210.13891</a>
&#x1F4C8; 5 <br>
<p>Fateme Nateghi Haredasht, Celine Vens</p></summary>
<p>

**Abstract:** Many clinical studies require the follow-up of patients over time. This is challenging: apart from frequently observed drop-out, there are often also organizational and financial challenges, which can lead to reduced data collection and, in turn, can complicate subsequent analyses. In contrast, there is often plenty of baseline data available of patients with similar characteristics and background information, e.g., from patients that fall outside the study time window. In this article, we investigate whether we can benefit from the inclusion of such unlabeled data instances to predict accurate survival times. In other words, we introduce a third level of supervision in the context of survival analysis, apart from fully observed and censored instances, we also include unlabeled instances. We propose three approaches to deal with this novel setting and provide an empirical comparison over fifteen real-life clinical and gene expression survival datasets. Our results demonstrate that all approaches are able to increase the predictive performance over independent test data. We also show that integrating the partial supervision provided by censored data in a semi-supervised wrapper approach generally provides the best results, often achieving high improvements, compared to not using unlabeled data.

</p>
</details>

<details><summary><b>A deep learning approach for brain tumor detection using magnetic resonance imaging</b>
<a href="https://arxiv.org/abs/2210.13882">arxiv:2210.13882</a>
&#x1F4C8; 5 <br>
<p>Al-Akhir Nayan, Ahamad Nokib Mozumder, Md. Rakibul Haque, Fahim Hossain Sifat, Khan Raqib Mahmud, Abul Kalam Al Azad, Muhammad Golam Kibria</p></summary>
<p>

**Abstract:** The growth of abnormal cells in the brain's tissue causes brain tumors. Brain tumors are considered one of the most dangerous disorders in children and adults. It develops quickly, and the patient's survival prospects are slim if not appropriately treated. Proper treatment planning and precise diagnoses are essential to improving a patient's life expectancy. Brain tumors are mainly diagnosed using magnetic resonance imaging (MRI). As part of a convolution neural network (CNN)-based illustration, an architecture containing five convolution layers, five max-pooling layers, a Flatten layer, and two dense layers has been proposed for detecting brain tumors from MRI images. The proposed model includes an automatic feature extractor, modified hidden layer architecture, and activation function. Several test cases were performed, and the proposed model achieved 98.6% accuracy and 97.8% precision score with a low cross-entropy rate. Compared with other approaches such as adjacent feature propagation network (AFPNet), mask region-based CNN (mask RCNN), YOLOv5, and Fourier CNN (FCNN), the proposed model has performed better in detecting brain tumors.

</p>
</details>

<details><summary><b>Stable deep MRI reconstruction using Generative Priors</b>
<a href="https://arxiv.org/abs/2210.13834">arxiv:2210.13834</a>
&#x1F4C8; 5 <br>
<p>Martin Zach, Florian Knoll, Thomas Pock</p></summary>
<p>

**Abstract:** Data-driven approaches recently achieved remarkable success in medical image reconstruction, but integration into clinical routine remains challenging due to a lack of generalizability and interpretability. Existing approaches usually require high-quality data-image pairs for training, but such data is not easily available for any imaging protocol and the reconstruction quality can quickly degrade even if only minor changes are made to the protocol. In addition, data-driven methods may create artificial features that can influence the clinicians decision-making. This is unacceptable if the clinician is unaware of the uncertainty associated with the reconstruction. In this paper, we address these challenges in a unified framework based on generative image priors. We propose a novel deep neural network based regularizer which is trained in an unsupervised setting on reference images without requiring any data-image pairs. After training, the regularizer can be used as part of a classical variational approach in combination with any acquisition protocols and shows stable behavior even if the test data deviates significantly from the training data. Furthermore, our probabilistic interpretation provides a distribution of reconstructions and hence allows uncertainty quantification. We demonstrate our approach on parallel magnetic resonance imaging, where results show competitive performance with SotA end-to-end deep learning methods, while preserving the flexibility of the acquisition protocol and allowing for uncertainty quantification.

</p>
</details>

<details><summary><b>Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning</b>
<a href="https://arxiv.org/abs/2210.14469">arxiv:2210.14469</a>
&#x1F4C8; 4 <br>
<p>Yifan Chen, Devamanyu Hazarika, Mahdi Namazifar, Yang Liu, Di Jin, Dilek Hakkani-Tur</p></summary>
<p>

**Abstract:** Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose to understand and further develop prefix-tuning through the kernel lens. Specifically, we make an analogy between \textit{prefixes} and \textit{inducing variables} in kernel methods and hypothesize that \textit{prefixes} serving as \textit{inducing variables} would improve their overall mechanism. From the kernel estimator perspective, we suggest a new variant of prefix-tuning -- \textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning while leveraging the residual form found in adapter-tuning. This mitigates the initialization issue in prefix-tuning. Through comprehensive empirical experiments on natural language understanding and generation tasks, we demonstrate that inducer-tuning can close the performance gap between prefix-tuning and fine-tuning.

</p>
</details>

<details><summary><b>D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning</b>
<a href="https://arxiv.org/abs/2210.14428">arxiv:2210.14428</a>
&#x1F4C8; 4 <br>
<p>Caroline Wang, Garrett Warnell, Peter Stone</p></summary>
<p>

**Abstract:** While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal policy in the presence of suboptimal demonstrations.

</p>
</details>

<details><summary><b>IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text</b>
<a href="https://arxiv.org/abs/2210.14395">arxiv:2210.14395</a>
&#x1F4C8; 4 <br>
<p>Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Alireza Dirafzoon, Aparajita Saraf, Amy Bearman, Babak Damavandi</p></summary>
<p>

**Abstract:** We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with video and text, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos -- while preserving the transitivity across these modalities.
  We explore several new IMU-based applications that IMU2CLIP enables, such as motion-based media retrieval and natural language reasoning tasks with motion data. In addition, we show that IMU2CLIP can significantly improve the downstream performance when fine-tuned for each application (e.g. activity recognition), demonstrating the universal usage of IMU2CLIP as a new pre-trained resource. Our code will be made publicly available.

</p>
</details>

<details><summary><b>Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation</b>
<a href="https://arxiv.org/abs/2210.14389">arxiv:2210.14389</a>
&#x1F4C8; 4 <br>
<p>Soyoung Yoon, Sungjoon Park, Gyuwan Kim, Junhee Cho, Kihyo Park, Gyu Tae Kim, Minjoon Seo, Alice Oh</p></summary>
<p>

**Abstract:** Research on Korean grammatical error correction (GEC) is limited compared to other major languages such as English and Chinese. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean. Thus, in this work, we first collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide range of error types and annotate them using our newly proposed tool called Korean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a carefully designed edit alignment & classification tool that considers the nature of Korean on generating an alignment between a source sentence and a target sentence, and identifies error types on each aligned edit. We also present baseline models fine-tuned over our datasets. We show that the model trained with our datasets significantly outperforms the public statistical GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets.

</p>
</details>

<details><summary><b>Streaming Submodular Maximization with Differential Privacy</b>
<a href="https://arxiv.org/abs/2210.14315">arxiv:2210.14315</a>
&#x1F4C8; 4 <br>
<p>Anamay Chaturvedi, Huy Lê Nguyen, Thy Nguyen</p></summary>
<p>

**Abstract:** In this work, we study the problem of privately maximizing a submodular function in the streaming setting. Extensive work has been done on privately maximizing submodular functions in the general case when the function depends upon the private data of individuals. However, when the size of the data stream drawn from the domain of the objective function is large or arrives very fast, one must privately optimize the objective within the constraints of the streaming setting. We establish fundamental differentially private baselines for this problem and then derive better trade-offs between privacy and utility for the special case of decomposable submodular functions. A submodular function is decomposable when it can be written as a sum of submodular functions; this structure arises naturally when each summand function models the utility of an individual and the goal is to study the total utility of the whole population as in the well-known Combinatorial Public Projects Problem. Finally, we complement our theoretical analysis with experimental corroboration.

</p>
</details>

<details><summary><b>pmuBAGE: The Benchmarking Assortment of Generated PMU Data for Power System Events</b>
<a href="https://arxiv.org/abs/2210.14204">arxiv:2210.14204</a>
&#x1F4C8; 4 <br>
<p>Brandon Foggo, Koji Yamashita, Nanpeng Yu</p></summary>
<p>

**Abstract:** This paper introduces pmuGE (phasor measurement unit Generator of Events), one of the first data-driven generative model for power system event data. We have trained this model on thousands of actual events and created a dataset denoted pmuBAGE (the Benchmarking Assortment of Generated PMU Events). The dataset consists of almost 1000 instances of labeled event data to encourage benchmark evaluations on phasor measurement unit (PMU) data analytics. PMU data are challenging to obtain, especially those covering event periods. Nevertheless, power system problems have recently seen phenomenal advancements via data-driven machine learning solutions. A highly accessible standard benchmarking dataset would enable a drastic acceleration of the development of successful machine learning techniques in this field. We propose a novel learning method based on the Event Participation Decomposition of Power System Events, which makes it possible to learn a generative model of PMU data during system anomalies. The model can create highly realistic event data without compromising the differential privacy of the PMUs used to train it. The dataset is available online for any researcher or practitioner to use at the pmuBAGE Github Repository: https://github.com/NanpengYu/pmuBAGE.

</p>
</details>

<details><summary><b>Exploring Mode Connectivity for Pre-trained Language Models</b>
<a href="https://arxiv.org/abs/2210.14102">arxiv:2210.14102</a>
&#x1F4C8; 4 <br>
<p>Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou</p></summary>
<p>

**Abstract:** Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM's mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM's task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation.

</p>
</details>

<details><summary><b>MEW-UNet: Multi-axis representation learning in frequency domain for medical image segmentation</b>
<a href="https://arxiv.org/abs/2210.14007">arxiv:2210.14007</a>
&#x1F4C8; 4 <br>
<p>Jiacheng Ruan, Mingye Xie, Suncheng Xiang, Ting Liu, Yuzhuo Fu</p></summary>
<p>

**Abstract:** Recently, Visual Transformer (ViT) has been widely used in various fields of computer vision due to applying self-attention mechanism in the spatial domain to modeling global knowledge. Especially in medical image segmentation (MIS), many works are devoted to combining ViT and CNN, and even some works directly utilize pure ViT-based models. However, recent works improved models in the aspect of spatial domain while ignoring the importance of frequency domain information. Therefore, we propose Multi-axis External Weights UNet (MEW-UNet) for MIS based on the U-shape architecture by replacing self-attention in ViT with our Multi-axis External Weights block. Specifically, our block performs a Fourier transform on the three axes of the input feature and assigns the external weight in the frequency domain, which is generated by our Weights Generator. Then, an inverse Fourier transform is performed to change the features back to the spatial domain. We evaluate our model on four datasets and achieve state-of-the-art performances. In particular, on the Synapse dataset, our method outperforms MT-UNet by 10.15mm in terms of HD95. Code is available at https://github.com/JCruan519/MEW-UNet.

</p>
</details>

<details><summary><b>Parametric PDF for Goodness of Fit</b>
<a href="https://arxiv.org/abs/2210.14005">arxiv:2210.14005</a>
&#x1F4C8; 4 <br>
<p>Natan Katz, Uri Itai</p></summary>
<p>

**Abstract:** The goodness of fit methods for classification problems relies traditionally on confusion matrices. This paper aims to enrich these methods with a risk evaluation and stability analysis tools. For this purpose, we present a parametric PDF framework.

</p>
</details>

<details><summary><b>Mitigating Health Data Poverty: Generative Approaches versus Resampling for Time-series Clinical Data</b>
<a href="https://arxiv.org/abs/2210.13958">arxiv:2210.13958</a>
&#x1F4C8; 4 <br>
<p>Raffaele Marchesi, Nicolo Micheletti, Giuseppe Jurman, Venet Osmani</p></summary>
<p>

**Abstract:** Several approaches have been developed to mitigate algorithmic bias stemming from health data poverty, where minority groups are underrepresented in training datasets. Augmenting the minority class using resampling (such as SMOTE) is a widely used approach due to the simplicity of the algorithms. However, these algorithms decrease data variability and may introduce correlations between samples, giving rise to the use of generative approaches based on GAN. Generation of high-dimensional, time-series, authentic data that provides a wide distribution coverage of the real data, remains a challenging task for both resampling and GAN-based approaches. In this work we propose CA-GAN architecture that addresses some of the shortcomings of the current approaches, where we provide a detailed comparison with both SMOTE and WGAN-GP*, using a high-dimensional, time-series, real dataset of 3343 hypotensive Caucasian and Black patients. We show that our approach is better at both generating authentic data of the minority class and remaining within the original distribution of the real data.

</p>
</details>

<details><summary><b>Discourse Context Predictability Effects in Hindi Word Order</b>
<a href="https://arxiv.org/abs/2210.13940">arxiv:2210.13940</a>
&#x1F4C8; 4 <br>
<p>Sidharth Ranjan, Marten van Schijndel, Sumeet Agarwal, Rajakrishnan Rajkumar</p></summary>
<p>

**Abstract:** We test the hypothesis that discourse predictability influences Hindi syntactic choice. While prior work has shown that a number of factors (e.g., information status, dependency length, and syntactic surprisal) influence Hindi word order preferences, the role of discourse predictability is underexplored in the literature. Inspired by prior work on syntactic priming, we investigate how the words and syntactic structures in a sentence influence the word order of the following sentences. Specifically, we extract sentences from the Hindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those sentences, and build a classifier to predict which sentences actually occurred in the corpus against artificially generated distractors. The classifier uses a number of discourse-based features and cognitive features to make its predictions, including dependency length, surprisal, and information status. We find that information status and LSTM-based discourse predictability influence word order choices, especially for non-canonical object-fronted orders. We conclude by situating our results within the broader syntactic priming literature.

</p>
</details>

<details><summary><b>Multi-Fidelity Bayesian Optimization with Unreliable Information Sources</b>
<a href="https://arxiv.org/abs/2210.13937">arxiv:2210.13937</a>
&#x1F4C8; 4 <br>
<p>Petrus Mikkola, Julien Martinelli, Louis Filstroff, Samuel Kaski</p></summary>
<p>

**Abstract:** Bayesian optimization (BO) is a powerful framework for optimizing black-box, expensive-to-evaluate functions. Over the past decade, many algorithms have been proposed to integrate cheaper, lower-fidelity approximations of the objective function into the optimization process, with the goal of converging towards the global optimum at a reduced cost. This task is generally referred to as multi-fidelity Bayesian optimization (MFBO). However, MFBO algorithms can lead to higher optimization costs than their vanilla BO counterparts, especially when the low-fidelity sources are poor approximations of the objective function, therefore defeating their purpose. To address this issue, we propose rMFBO (robust MFBO), a methodology to make any GP-based MFBO scheme robust to the addition of unreliable information sources. rMFBO comes with a theoretical guarantee that its performance can be bound to its vanilla BO analog, with high controllable probability. We demonstrate the effectiveness of the proposed methodology on a number of numerical benchmarks, outperforming earlier MFBO methods on unreliable sources. We expect rMFBO to be particularly useful to reliably include human experts with varying knowledge within BO processes.

</p>
</details>

<details><summary><b>One-shot, Offline and Production-Scalable PID Optimisation with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2210.13906">arxiv:2210.13906</a>
&#x1F4C8; 4 <br>
<p>Zacharaya Shabka, Michael Enrico, Nick Parsons, Georgios Zervas</p></summary>
<p>

**Abstract:** Proportional-integral-derivative (PID) control underlies more than $97\%$ of automated industrial processes. Controlling these processes effectively with respect to some specified set of performance goals requires finding an optimal set of PID parameters to moderate the PID loop. Tuning these parameters is a long and exhaustive process. A method (patent pending) based on deep reinforcement learning is presented that learns a relationship between generic system properties (e.g. resonance frequency), a multi-objective performance goal and optimal PID parameter values. Performance is demonstrated in the context of a real optical switching product of the foremost manufacturer of such devices globally. Switching is handled by piezoelectric actuators where switching time and optical loss are derived from the speed and stability of actuator-control processes respectively. The method achieves a $5\times$ improvement in the number of actuators that fall within the most challenging target switching speed, $\geq 20\%$ improvement in mean switching speed at the same optical loss and $\geq 75\%$ reduction in performance inconsistency when temperature varies between 5 and 73 degrees celcius. Furthermore, once trained (which takes $\mathcal{O}(hours)$), the model generates actuator-unique PID parameters in a one-shot inference process that takes $\mathcal{O}(ms)$ in comparison to up to $\mathcal{O}(week)$ required for conventional tuning methods, therefore accomplishing these performance improvements whilst achieving up to a $10^6\times$ speed-up. After training, the method can be applied entirely offline, incurring effectively zero optimisation-overhead in production.

</p>
</details>

<details><summary><b>Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multimodal Data</b>
<a href="https://arxiv.org/abs/2210.13889">arxiv:2210.13889</a>
&#x1F4C8; 4 <br>
<p>Huy Hoang Nguyen, Matthew B. Blaschko, Simo Saarakkala, Aleksei Tiulpin</p></summary>
<p>

**Abstract:** Deep neural networks are often applied to medical images to automate the problem of medical diagnosis. However, a more clinically relevant question that practitioners usually face is how to predict the future trajectory of a disease. Current methods for prognosis or disease trajectory forecasting often require domain knowledge and are complicated to apply. In this paper, we formulate the prognosis prediction problem as a one-to-many prediction problem. Inspired by a clinical decision-making process with two agents -- a radiologist and a general practitioner -- we predict prognosis with two transformer-based components that share information with each other. The first transformer in this framework aims to analyze the imaging data, and the second one leverages its internal states as inputs, also fusing them with auxiliary clinical data. The temporal nature of the problem is modeled within the transformer states, allowing us to treat the forecasting problem as a multi-task classification, for which we propose a novel loss. We show the effectiveness of our approach in predicting the development of structural knee osteoarthritis changes and forecasting Alzheimer's disease clinical status directly from raw multi-modal data. The proposed method outperforms multiple state-of-the-art baselines with respect to performance and calibration, both of which are needed for real-world applications. An open-source implementation of our method is made publicly available at \url{https://github.com/Oulu-IMEDS/CLIMATv2}.

</p>
</details>

<details><summary><b>Towards emotion recognition for virtual environments: an evaluation of EEG features on benchmark dataset</b>
<a href="https://arxiv.org/abs/2210.13876">arxiv:2210.13876</a>
&#x1F4C8; 4 <br>
<p>M. L. Menezes, A. Samara, L. Galway, A. Sant'anna, A. Verikas, F. Alonso-Fernandez, H. Wang, R. Bond</p></summary>
<p>

**Abstract:** One of the challenges in virtual environments is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the user's emotions. This paper investigates features extracted from electroencephalogram signals for the purpose of affective state modelling based on Russell's Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect to enhance interaction experience in virtual environments. The DEAP dataset was used within this work, along with a Support Vector Machine and Random Forest, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements and band power from the ź, \b{eta}, ź, and źź waves and High Order Crossing of the EEG signal.

</p>
</details>

<details><summary><b>Does Medical Imaging learn different Convolution Filters?</b>
<a href="https://arxiv.org/abs/2210.13799">arxiv:2210.13799</a>
&#x1F4C8; 4 <br>
<p>Paul Gavrikov, Janis Keuper</p></summary>
<p>

**Abstract:** Recent work has investigated the distributions of learned convolution filters through a large-scale study containing hundreds of heterogeneous image models. Surprisingly, on average, the distributions only show minor drifts in comparisons of various studied dimensions including the learned task, image domain, or dataset. However, among the studied image domains, medical imaging models appeared to show significant outliers through "spikey" distributions, and, therefore, learn clusters of highly specific filters different from other domains. Following this observation, we study the collected medical imaging models in more detail. We show that instead of fundamental differences, the outliers are due to specific processing in some architectures. Quite the contrary, for standardized architectures, we find that models trained on medical data do not significantly differ in their filter distributions from similar architectures trained on data from other domains. Our conclusions reinforce previous hypotheses stating that pre-training of imaging models can be done with any kind of diverse image data.

</p>
</details>

<details><summary><b>Towards Trustworthy Multi-label Sewer Defect Classification via Evidential Deep Learning</b>
<a href="https://arxiv.org/abs/2210.13782">arxiv:2210.13782</a>
&#x1F4C8; 4 <br>
<p>Chenyang Zhao, Chuanfei Hu, Hang Shao, Zhe Wang, Yongxiong Wang</p></summary>
<p>

**Abstract:** An automatic vision-based sewer inspection plays a key role of sewage system in a modern city. Recent advances focus on utilizing deep learning model to realize the sewer inspection system, benefiting from the capability of data-driven feature representation. However, the inherent uncertainty of sewer defects is ignored, resulting in the missed detection of serious unknown sewer defect categories. In this paper, we propose a trustworthy multi-label sewer defect classification (TMSDC) method, which can quantify the uncertainty of sewer defect prediction via evidential deep learning. Meanwhile, a novel expert base rate assignment (EBRA) is proposed to introduce the expert knowledge for describing reliable evidences in practical situations. Experimental results demonstrate the effectiveness of TMSDC and the superior capability of uncertainty estimation is achieved on the latest public benchmark.

</p>
</details>

<details><summary><b>GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion Estimates</b>
<a href="https://arxiv.org/abs/2210.13769">arxiv:2210.13769</a>
&#x1F4C8; 4 <br>
<p>Jerin Geo James, Devansh Jain, Ajit Rajwade</p></summary>
<p>

**Abstract:** Videos shot by laymen using hand-held cameras contain undesirable shaky motion. Estimating the global motion between successive frames, in a manner not influenced by moving objects, is central to many video stabilization techniques, but poses significant challenges. A large body of work uses 2D affine transformations or homography for the global motion. However, in this work, we introduce a more general representation scheme, which adapts any existing optical flow network to ignore the moving objects and obtain a spatially smooth approximation of the global motion between video frames. We achieve this by a knowledge distillation approach, where we first introduce a low pass filter module into the optical flow network to constrain the predicted optical flow to be spatially smooth. This becomes our student network, named as \textsc{GlobalFlowNet}. Then, using the original optical flow network as the teacher network, we train the student network using a robust loss function. Given a trained \textsc{GlobalFlowNet}, we stabilize videos using a two stage process. In the first stage, we correct the instability in affine parameters using a quadratic programming approach constrained by a user-specified cropping limit to control loss of field of view. In the second stage, we stabilize the video further by smoothing global motion parameters, expressed using a small number of discrete cosine transform coefficients. In extensive experiments on a variety of different videos, our technique outperforms state of the art techniques in terms of subjective quality and different quantitative measures of video stability. The source code is publicly available at \href{https://github.com/GlobalFlowNet/GlobalFlowNet}{https://github.com/GlobalFlowNet/GlobalFlowNet}

</p>
</details>

<details><summary><b>GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2210.13768">arxiv:2210.13768</a>
&#x1F4C8; 4 <br>
<p>Xingting Yao, Fanrong Li, Zitao Mo, Jian Cheng</p></summary>
<p>

**Abstract:** Spiking Neural Networks (SNNs) have been studied over decades to incorporate their biological plausibility and leverage their promising energy efficiency. Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly adopted to formulate the spiking neuron and evolves into numerous variants with different biological features. However, most LIF-based neurons support only single biological feature in different neuronal behaviors, limiting their expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF, a unified spiking neuron, to fuse different bio-features in different neuronal behaviors, enlarging the representation space of spiking neurons. In GLIF, gating factors, which are exploited to determine the proportion of the fused bio-features, are learnable during training. Combining all learnable membrane-related parameters, our method can make spiking neurons different and constantly changing, thus increasing the heterogeneity and adaptivity of spiking neurons. Extensive experiments on a variety of datasets demonstrate that our method obtains superior performance compared with other SNNs by simply changing their neuronal formulations to GLIF. In particular, we train a spiking ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on CIFAR-100, which has advanced the state-of-the-art. Codes are available at \url{https://github.com/Ikarosy/Gated-LIF}.

</p>
</details>

<details><summary><b>Automatic Extraction of Materials and Properties from Superconductors Scientific Literature</b>
<a href="https://arxiv.org/abs/2210.15600">arxiv:2210.15600</a>
&#x1F4C8; 3 <br>
<p>Luca Foppiano, Pedro Baptista de Castro, Pedro Ortiz Suarez, Kensei Terashima, Yoshihiko Takano, Masashi Ishii</p></summary>
<p>

**Abstract:** The automatic extraction of materials and related properties from the scientific literature is gaining attention in data-driven materials science (Materials Informatics). In this paper, we discuss Grobid-superconductors, our solution for automatically extracting superconductor material names and respective properties from text. Built as a Grobid module, it combines machine learning and heuristic approaches in a multi-step architecture that supports input data as raw text or PDF documents. Using Grobid-superconductors, we built SuperCon2, a database of 40324 materials and properties records from 37700 papers. The material (or sample) information is represented by name, chemical formula, and material class, and is characterized by shape, doping, substitution variables for components, and substrate as adjoined information. The properties include the Tc superconducting critical temperature and, when available, applied pressure with the Tc measurement method.

</p>
</details>

<details><summary><b>Learning versus Refutation in Noninteractive Local Differential Privacy</b>
<a href="https://arxiv.org/abs/2210.15439">arxiv:2210.15439</a>
&#x1F4C8; 3 <br>
<p>Alexander Edmonds, Aleksandar Nikolov, Toniann Pitassi</p></summary>
<p>

**Abstract:** We study two basic statistical tasks in non-interactive local differential privacy (LDP): learning and refutation. Learning requires finding a concept that best fits an unknown target function (from labelled samples drawn from a distribution), whereas refutation requires distinguishing between data distributions that are well-correlated with some concept in the class, versus distributions where the labels are random. Our main result is a complete characterization of the sample complexity of agnostic PAC learning for non-interactive LDP protocols. We show that the optimal sample complexity for any concept class is captured by the approximate $γ_2$~norm of a natural matrix associated with the class. Combined with previous work [Edmonds, Nikolov and Ullman, 2019] this gives an equivalence between learning and refutation in the agnostic setting.

</p>
</details>

<details><summary><b>Multi-view Representation Learning from Malware to Defend Against Adversarial Variants</b>
<a href="https://arxiv.org/abs/2210.15429">arxiv:2210.15429</a>
&#x1F4C8; 3 <br>
<p>James Lee Hu, Mohammadreza Ebrahimi, Weifeng Li, Xin Li, Hsinchun Chen</p></summary>
<p>

**Abstract:** Deep learning-based adversarial malware detectors have yielded promising results in detecting never-before-seen malware executables without relying on expensive dynamic behavior analysis and sandbox. Despite their abilities, these detectors have been shown to be vulnerable to adversarial malware variants - meticulously modified, functionality-preserving versions of original malware executables generated by machine learning. Due to the nature of these adversarial modifications, these adversarial methods often use a \textit{single view} of malware executables (i.e., the binary/hexadecimal view) to generate adversarial malware variants. This provides an opportunity for the defenders (i.e., malware detectors) to detect the adversarial variants by utilizing more than one view of a malware file (e.g., source code view in addition to the binary view). The rationale behind this idea is that while the adversary focuses on the binary view, certain characteristics of the malware file in the source code view remain untouched which leads to the detection of the adversarial malware variants. To capitalize on this opportunity, we propose Adversarially Robust Multiview Malware Defense (ARMD), a novel multi-view learning framework to improve the robustness of DL-based malware detectors against adversarial variants. Our experiments on three renowned open-source deep learning-based malware detectors across six common malware categories show that ARMD is able to improve the adversarial robustness by up to seven times on these malware detectors.

</p>
</details>

<details><summary><b>RedPen: Region- and Reason-Annotated Dataset of Unnatural Speech</b>
<a href="https://arxiv.org/abs/2210.14406">arxiv:2210.14406</a>
&#x1F4C8; 3 <br>
<p>Kyumin Park, Keon Lee, Daeyoung Kim, Dongyeop Kang</p></summary>
<p>

**Abstract:** Even with recent advances in speech synthesis models, the evaluation of such models is based purely on human judgement as a single naturalness score, such as the Mean Opinion Score (MOS). The score-based metric does not give any further information about which parts of speech are unnatural or why human judges believe they are unnatural. We present a novel speech dataset, RedPen, with human annotations on unnatural speech regions and their corresponding reasons. RedPen consists of 180 synthesized speeches with unnatural regions annotated by crowd workers; These regions are then reasoned and categorized by error types, such as voice trembling and background noise. We find that our dataset shows a better explanation for unnatural speech regions than the model-driven unnaturalness prediction. Our analysis also shows that each model includes different types of error types. Summing up, our dataset successfully shows the possibility that various error regions and types lie under the single naturalness score. We believe that our dataset will shed light on the evaluation and development of more interpretable speech models in the future. Our dataset will be publicly available upon acceptance.

</p>
</details>

<details><summary><b>Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport</b>
<a href="https://arxiv.org/abs/2210.14378">arxiv:2210.14378</a>
&#x1F4C8; 3 <br>
<p>Kelly Marchisio, Ali Saad-Eldin, Kevin Duh, Carey Priebe, Philipp Koehn</p></summary>
<p>

**Abstract:** Bilingual lexicons form a critical component of various natural language processing applications, including unsupervised and semisupervised machine translation and crosslingual information retrieval. We improve bilingual lexicon induction performance across 40 language pairs with a graph-matching method based on optimal transport. The method is especially strong with low amounts of supervision.

</p>
</details>

<details><summary><b>Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis</b>
<a href="https://arxiv.org/abs/2210.14377">arxiv:2210.14377</a>
&#x1F4C8; 3 <br>
<p>Niharika S. D'Souza, Hongzhi Wang, Andrea Giovannini, Antonio Foncubierta-Rodriguez, Kristen L. Beck, Orest Boyko, Tanveer Syeda-Mahmood</p></summary>
<p>

**Abstract:** In a complex disease such as tuberculosis, the evidence for the disease and its evolution may be present in multiple modalities such as clinical, genomic, or imaging data. Effective patient-tailored outcome prediction and therapeutic guidance will require fusing evidence from these modalities. Such multimodal fusion is difficult since the evidence for the disease may not be uniform across all modalities, not all modality features may be relevant, or not all modalities may be present for all patients. All these nuances make simple methods of early, late, or intermediate fusion of features inadequate for outcome prediction. In this paper, we present a novel fusion framework using multiplexed graphs and derive a new graph neural network for learning from such graphs. Specifically, the framework allows modalities to be represented through their targeted encodings, and models their relationship explicitly via multiplexed graphs derived from salient features in a combined latent space. We present results that show that our proposed method outperforms state-of-the-art methods of fusing modalities for multi-outcome prediction on a large Tuberculosis (TB) dataset.

</p>
</details>

<details><summary><b>On Robust Incremental Learning over Many Multilingual Steps</b>
<a href="https://arxiv.org/abs/2210.14307">arxiv:2210.14307</a>
&#x1F4C8; 3 <br>
<p>Karan Praharaj, Irina Matveeva</p></summary>
<p>

**Abstract:** Recent work in incremental learning has introduced diverse approaches to tackle catastrophic forgetting from data augmentation to optimized training regimes. However, most of them focus on very few training steps. We propose a method for robust incremental learning over dozens of fine-tuning steps using data from a variety of languages. We show that a combination of data-augmentation and an optimized training regime allows us to continue improving the model even for as many as fifty training steps. Crucially, our augmentation strategy does not require retaining access to previous training data and is suitable in scenarios with privacy constraints.

</p>
</details>

<details><summary><b>Learning in Multi-Player Stochastic Games</b>
<a href="https://arxiv.org/abs/2210.14280">arxiv:2210.14280</a>
&#x1F4C8; 3 <br>
<p>William Brown</p></summary>
<p>

**Abstract:** We consider the problem of simultaneous learning in stochastic games with many players in the finite-horizon setting. While the typical target solution for a stochastic game is a Nash equilibrium, this is intractable with many players. We instead focus on variants of {\it correlated equilibria}, such as those studied for extensive-form games. We begin with a hardness result for the adversarial MDP problem: even for a horizon of 3, obtaining sublinear regret against the best non-stationary policy is \textsf{NP}-hard when both rewards and transitions are adversarial. This implies that convergence to even the weakest natural solution concept -- normal-form coarse correlated equilbrium -- is not possible via black-box reduction to a no-regret algorithm even in stochastic games with constant horizon (unless $\textsf{NP}\subseteq\textsf{BPP}$). Instead, we turn to a different target: algorithms which {\it generate} an equilibrium when they are used by all players. Our main result is algorithm which generates an {\it extensive-form} correlated equilibrium, whose runtime is exponential in the horizon but polynomial in all other parameters. We give a similar algorithm which is polynomial in all parameters for "fast-mixing" stochastic games. We also show a method for efficiently reaching normal-form coarse correlated equilibria in "single-controller" stochastic games which follows the traditional no-regret approach. When shared randomness is available, the two generative algorithms can be extended to give simultaneous regret bounds and converge in the traditional sense.

</p>
</details>

<details><summary><b>'A net for everyone': fully personalized and unsupervised neural networks trained with longitudinal data from a single patient</b>
<a href="https://arxiv.org/abs/2210.14228">arxiv:2210.14228</a>
&#x1F4C8; 3 <br>
<p>Christian Strack, Kelsey L. Pomykala, Heinz-Peter Schlemmer, Jan Egger, Jens Kleesiek</p></summary>
<p>

**Abstract:** With the rise in importance of personalized medicine, we trained personalized neural networks to detect tumor progression in longitudinal datasets. The model was evaluated on two datasets with a total of 64 scans from 32 patients diagnosed with glioblastoma multiforme (GBM). Contrast-enhanced T1w sequences of brain magnetic resonance imaging (MRI) images were used in this study. For each patient, we trained their own neural network using just two images from different timepoints. Our approach uses a Wasserstein-GAN (generative adversarial network), an unsupervised network architecture, to map the differences between the two images. Using this map, the change in tumor volume can be evaluated. Due to the combination of data augmentation and the network architecture, co-registration of the two images is not needed. Furthermore, we do not rely on any additional training data, (manual) annotations or pre-training neural networks. The model received an AUC-score of 0.87 for tumor change. We also introduced a modified RANO criteria, for which an accuracy of 66% can be achieved. We show that using data from just one patient can be used to train deep neural networks to monitor tumor change.

</p>
</details>

<details><summary><b>FedClassAvg: Local Representation Learning for Personalized Federated Learning on Heterogeneous Neural Networks</b>
<a href="https://arxiv.org/abs/2210.14226">arxiv:2210.14226</a>
&#x1F4C8; 3 <br>
<p>Jaehee Jang, Heonseok Ha, Dahuin Jung, Sungroh Yoon</p></summary>
<p>

**Abstract:** Personalized federated learning is aimed at allowing numerous clients to train personalized models while participating in collaborative training in a communication-efficient manner without exchanging private data. However, many personalized federated learning algorithms assume that clients have the same neural network architecture, and those for heterogeneous models remain understudied. In this study, we propose a novel personalized federated learning method called federated classifier averaging (FedClassAvg). Deep neural networks for supervised learning tasks consist of feature extractor and classifier layers. FedClassAvg aggregates classifier weights as an agreement on decision boundaries on feature spaces so that clients with not independently and identically distributed (non-iid) data can learn about scarce labels. In addition, local feature representation learning is applied to stabilize the decision boundaries and improve the local feature extraction capabilities for clients. While the existing methods require the collection of auxiliary data or model weights to generate a counterpart, FedClassAvg only requires clients to communicate with a couple of fully connected layers, which is highly communication-efficient. Moreover, FedClassAvg does not require extra optimization problems such as knowledge transfer, which requires intensive computation overhead. We evaluated FedClassAvg through extensive experiments and demonstrated it outperforms the current state-of-the-art algorithms on heterogeneous personalized federated learning tasks.

</p>
</details>

<details><summary><b>Goal-Driven Context-Aware Next Service Recommendation for Mashup Composition</b>
<a href="https://arxiv.org/abs/2210.14127">arxiv:2210.14127</a>
&#x1F4C8; 3 <br>
<p>Xihao Xie, Jia Zhang, Rahul Ramachandran, Tsengdar J. Lee, Seungwon Lee</p></summary>
<p>

**Abstract:** As service-oriented architecture becoming one of the most prevalent techniques to rapidly deliver functionalities to customers, increasingly more reusable software components have been published online in forms of web services. To create a mashup, it gets not only time-consuming but also error-prone for developers to find suitable services from such a sea of services. Service discovery and recommendation has thus attracted significant momentum in both academia and industry. This paper proposes a novel incremental recommend-as-you-go approach to recommending next potential service based on the context of a mashup under construction, considering services that have been selected to the current step as well as its mashup goal. The core technique is an algorithm of learning the embedding of services, which learns their past goal-driven context-aware decision making behaviors in addition to their semantic descriptions and co-occurrence history. A goal exclusionary negative sampling mechanism tailored for mashup development is also developed to improve training performance. Extensive experiments on a real-world dataset demonstrate the effectiveness of our approach.

</p>
</details>

<details><summary><b>Bit Error and Block Error Rate Training for ML-Assisted Communication</b>
<a href="https://arxiv.org/abs/2210.14103">arxiv:2210.14103</a>
&#x1F4C8; 3 <br>
<p>Reinhard Wiesmayr, Gian Marti, Chris Dick, Haochuan Song, Christoph Studer</p></summary>
<p>

**Abstract:** Even though machine learning (ML) techniques are being widely used in communications, the question of how to train communication systems has received surprisingly little attention. In this paper, we show that the commonly used binary cross-entropy (BCE) loss is a sensible choice in uncoded systems, e.g., for training ML-assisted data detectors, but may not be optimal in coded systems. We propose new loss functions targeted at minimizing the block error rate and SNR de-weighting, a novel method that trains communication systems for optimal performance over a range of signal-to-noise ratios. The utility of the proposed loss functions as well as of SNR de-weighting is shown through simulations in NVIDIA Sionna.

</p>
</details>

<details><summary><b>Policy-Guided Lazy Search with Feedback for Task and Motion Planning</b>
<a href="https://arxiv.org/abs/2210.14055">arxiv:2210.14055</a>
&#x1F4C8; 3 <br>
<p>Mohamed Khodeir, Atharv Sonwane, Florian Shkurti</p></summary>
<p>

**Abstract:** PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP approach by comparing to existing solvers for PDDLStream problems on a range of simulated 7DoF rearrangement/manipulation problems.

</p>
</details>

<details><summary><b>SeismicNet: Physics-informed neural networks for seismic wave modeling in semi-infinite domain</b>
<a href="https://arxiv.org/abs/2210.14044">arxiv:2210.14044</a>
&#x1F4C8; 3 <br>
<p>Pu Ren, Chengping Rao, Hao Sun, Yang Liu</p></summary>
<p>

**Abstract:** There has been an increasing interest in integrating physics knowledge and machine learning for modeling dynamical systems. However, very limited studies have been conducted on seismic wave modeling tasks. A critical challenge is that these geophysical problems are typically defined in large domains (i.e., semi-infinite), which leads to high computational cost. In this paper, we present a novel physics-informed neural network (PINN) model for seismic wave modeling in semi-infinite domain without the nedd of labeled data. In specific, the absorbing boundary condition is introduced into the network as a soft regularizer for handling truncated boundaries. In terms of computational efficiency, we consider a sequential training strategy via temporal domain decomposition to improve the scalability of the network and solution accuracy. Moreover, we design a novel surrogate modeling strategy for parametric loading, which estimates the wave propagation in semin-infinite domain given the seismic loading at different locations. Various numerical experiments have been implemented to evaluate the performance of the proposed PINN model in the context of forward modeling of seismic wave propagation. In particular, we define diverse material distributions to test the versatility of this approach. The results demonstrate excellent solution accuracy under distinctive scenarios.

</p>
</details>

<details><summary><b>SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication</b>
<a href="https://arxiv.org/abs/2210.14026">arxiv:2210.14026</a>
&#x1F4C8; 3 <br>
<p>Marco Bornstein, Tahseen Rabbani, Evan Wang, Amrit Singh Bedi, Furong Huang</p></summary>
<p>

**Abstract:** The decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate $\mathcal{O}(1/\sqrt{T})$ of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations $T$). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to $T$ as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to run-time due to its wait-free structure. Our experimental results demonstrate that SWIFT's run-time is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50% faster than existing SOTA algorithms.

</p>
</details>

<details><summary><b>Faster Projection-Free Augmented Lagrangian Methods via Weak Proximal Oracle</b>
<a href="https://arxiv.org/abs/2210.13968">arxiv:2210.13968</a>
&#x1F4C8; 3 <br>
<p>Dan Garber, Tsur Livney, Shoham Sabac</p></summary>
<p>

**Abstract:** This paper considers a convex composite optimization problem with affine constraints, which includes problems that take the form of minimizing a smooth convex objective function over the intersection of (simple) convex sets, or regularized with multiple (simple) functions. Motivated by high-dimensional applications in which exact projection/proximal computations are not tractable, we propose a \textit{projection-free} augmented Lagrangian-based method, in which primal updates are carried out using a \textit{weak proximal oracle} (WPO). In an earlier work, WPO was shown to be more powerful than the standard \textit{linear minimization oracle} (LMO) that underlies conditional gradient-based methods (aka Frank-Wolfe methods). Moreover, WPO is computationally tractable for many high-dimensional problems of interest, including those motivated by recovery of low-rank matrices and tensors, and optimization over polytopes which admit efficient LMOs. The main result of this paper shows that under a certain curvature assumption (which is weaker than strong convexity), our WPO-based algorithm achieves an ergodic rate of convergence of $O(1/T)$ for both the objective residual and feasibility gap. This result, to the best of our knowledge, improves upon the $O(1/\sqrt{T})$ rate for existing LMO-based projection-free methods for this class of problems. Empirical experiments on a low-rank and sparse covariance matrix estimation task and the Max Cut semidefinite relaxation demonstrate the superiority of our method over state-of-the-art LMO-based Lagrangian-based methods.

</p>
</details>

<details><summary><b>KnowGL: Knowledge Generation and Linking from Text</b>
<a href="https://arxiv.org/abs/2210.13952">arxiv:2210.13952</a>
&#x1F4C8; 3 <br>
<p>Gaetano Rossiello, Faisal Chowdhury, Nandana Mihindukulasooriya, Owen Cornec, Alfio Gliozzo</p></summary>
<p>

**Abstract:** We propose KnowGL, a tool that allows converting text into structured relational data represented as a set of ABox assertions compliant with the TBox of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a sequence generation task by leveraging pre-trained sequence-to-sequence language models, e.g. BART. Given a sentence, we fine-tune such models to detect pairs of entity mentions and jointly generate a set of facts consisting of the full set of semantic annotations for a KG, such as entity labels, entity types, and their relationships. To showcase the capabilities of our tool, we build a web application consisting of a set of UI widgets that help users to navigate through the semantic data extracted from a given input text. We make the KnowGL model available at https://huggingface.co/ibm/knowgl-large.

</p>
</details>

<details><summary><b>Dual Mechanism Priming Effects in Hindi Word Order</b>
<a href="https://arxiv.org/abs/2210.13938">arxiv:2210.13938</a>
&#x1F4C8; 3 <br>
<p>Sidharth Ranjan, Marten van Schijndel, Sumeet Agarwal, Rajakrishnan Rajkumar</p></summary>
<p>

**Abstract:** Word order choices during sentence production can be primed by preceding sentences. In this work, we test the DUAL MECHANISM hypothesis that priming is driven by multiple different sources. Using a Hindi corpus of text productions, we model lexical priming with an n-gram cache model and we capture more abstract syntactic priming with an adaptive neural language model. We permute the preverbal constituents of corpus sentences, and then use a logistic regression model to predict which sentences actually occurred in the corpus against artificially generated meaning-equivalent variants. Our results indicate that lexical priming and lexically-independent syntactic priming affect complementary sets of verb classes. By showing that different priming influences are separable from one another, our results support the hypothesis that multiple different cognitive mechanisms underlie priming.

</p>
</details>

<details><summary><b>CoLoC: Conditioned Localizer and Classifier for Sound Event Localization and Detection</b>
<a href="https://arxiv.org/abs/2210.13932">arxiv:2210.13932</a>
&#x1F4C8; 3 <br>
<p>Sławomir Kapka, Jakub Tkaczuk</p></summary>
<p>

**Abstract:** In this article, we describe Conditioned Localizer and Classifier (CoLoC) which is a novel solution for Sound Event Localization and Detection (SELD). The solution constitutes of two stages: the localization is done first and is followed by classification conditioned by the output of the localizer. In order to resolve the problem of the unknown number of sources we incorporate the idea borrowed from Sequential Set Generation (SSG). Models from both stages are SELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that such two single-output models are fit for SELD task. We show that our solution improves on the baseline system in most metrics on the STARSS22 Dataset.

</p>
</details>

<details><summary><b>Proximal Mean Field Learning in Shallow Neural Networks</b>
<a href="https://arxiv.org/abs/2210.13879">arxiv:2210.13879</a>
&#x1F4C8; 3 <br>
<p>Alexis Teter, Iman Nodozi, Abhishek Halder</p></summary>
<p>

**Abstract:** Recent mean field interpretations of learning dynamics in over-parameterized neural networks offer theoretical insights on the empirical success of first order optimization algorithms in finding global minima of the nonconvex risk landscape. In this paper, we explore applying mean field learning dynamics as a computational algorithm, rather than as an analytical tool. Specifically, we design a Sinkhorn regularized proximal algorithm to approximate the distributional flow from the learning dynamics in the mean field regime over weighted point clouds. In this setting, a contractive fixed point recursion computes the time-varying weights, numerically realizing the interacting Wasserstein gradient flow of the parameter distribution supported over the neuronal ensemble. An appealing aspect of the proposed algorithm is that the measure-valued recursions allow meshless computation. We demonstrate the proposed computational framework of interacting weighted particle evolution on binary and multi-class classification. Our algorithm performs gradient descent of the free energy associated with the risk functional.

</p>
</details>

<details><summary><b>Multilingual Relation Classification via Efficient and Effective Prompting</b>
<a href="https://arxiv.org/abs/2210.13838">arxiv:2210.13838</a>
&#x1F4C8; 3 <br>
<p>Yuxuan Chen, David Harbecke, Leonhard Hennig</p></summary>
<p>

**Abstract:** Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the high cost of handcrafting multilingual prompts. In this paper, we present the first work on prompt-based multilingual relation classification (RC), by introducing an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels. We evaluate its performance in fully supervised, few-shot and zero-shot scenarios, and analyze its effectiveness across 14 languages, prompt variants, and English-task training in cross-lingual settings. We find that in both fully supervised and few-shot scenarios, our prompt method beats competitive baselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the random baseline by a large margin in zero-shot experiments. Our method requires little in-language knowledge and can be used as a strong baseline for similar multilingual classification tasks.

</p>
</details>

<details><summary><b>End-to-end Transformer for Compressed Video Quality Enhancement</b>
<a href="https://arxiv.org/abs/2210.13827">arxiv:2210.13827</a>
&#x1F4C8; 3 <br>
<p>Li Yu, Wenshuai Chang, Shiyu Wu, Moncef Gabbouj</p></summary>
<p>

**Abstract:** Convolutional neural networks have achieved excellent results in compressed video quality enhancement task in recent years. State-of-the-art methods explore the spatiotemporal information of adjacent frames mainly by deformable convolution. However, offset fields in deformable convolution are difficult to train, and its instability in training often leads to offset overflow, which reduce the efficiency of correlation modeling. In this work, we propose a transformer-based compressed video quality enhancement (TVQE) method, consisting of Swin-AutoEncoder based Spatio-Temporal feature Fusion (SSTF) module and Channel-wise Attention based Quality Enhancement (CAQE) module. The proposed SSTF module learns both local and global features with the help of Swin-AutoEncoder, which improves the ability of correlation modeling. Meanwhile, the window mechanism-based Swin Transformer and the encoderdecoder structure greatly improve the execution efficiency. On the other hand, the proposed CAQE module calculates the channel attention, which aggregates the temporal information between channels in the feature map, and finally achieves the efficient fusion of inter-frame information. Extensive experimental results on the JCT-VT test sequences show that the proposed method achieves better performance in average for both subjective and objective quality. Meanwhile, our proposed method outperforms existing ones in terms of both inference speed and GPU consumption.

</p>
</details>

<details><summary><b>Online model error correction with neural networks in the incremental 4D-Var framework</b>
<a href="https://arxiv.org/abs/2210.13817">arxiv:2210.13817</a>
&#x1F4C8; 3 <br>
<p>Alban Farchi, Marcin Chrust, Marc Bocquet, Patrick Laloyaux, Massimo Bonavita</p></summary>
<p>

**Abstract:** Recent studies have demonstrated that it is possible to combine machine learning with data assimilation to reconstruct the dynamics of a physical model partially and imperfectly observed. Data assimilation is used to estimate the system state from the observations, while machine learning computes a surrogate model of the dynamical system based on those estimated states. The surrogate model can be defined as an hybrid combination where a physical model based on prior knowledge is enhanced with a statistical model estimated by a neural network. The training of the neural network is typically done offline, once a large enough dataset of model state estimates is available. By contrast, with online approaches the surrogate model is improved each time a new system state estimate is computed. Online approaches naturally fit the sequential framework encountered in geosciences where new observations become available with time. In a recent methodology paper, we have developed a new weak-constraint 4D-Var formulation which can be used to train a neural network for online model error correction. In the present article, we develop a simplified version of that method, in the incremental 4D-Var framework adopted by most operational weather centres. The simplified method is implemented in the ECMWF Object-Oriented Prediction System, with the help of a newly developed Fortran neural network library, and tested with a two-layer two-dimensional quasi geostrophic model. The results confirm that online learning is effective and yields a more accurate model error correction than offline learning. Finally, the simplified method is compatible with future applications to state-of-the-art models such as the ECMWF Integrated Forecasting System.

</p>
</details>

<details><summary><b>Some Simulation and Empirical Results for Semi-Supervised Learning of the Bayes Rule of Allocation</b>
<a href="https://arxiv.org/abs/2210.13785">arxiv:2210.13785</a>
&#x1F4C8; 3 <br>
<p>Ziyang Lyu, Daniel Ahfock, Geoffrey J. McLachlan</p></summary>
<p>

**Abstract:** There has been increasing attention to semi-supervised learning (SSL) approaches in machine learning to forming a classifier in situations where the training data consists of some feature vectors that have their class labels missing. In this study, we consider the generative model approach proposed by Ahfock&McLachlan(2020) who introduced a framework with a missingness mechanism for the missing labels of the unclassified features. In the case of two multivariate normal classes with a common covariance matrix, they showed that the error rate of the estimated Bayes' rule formed by this SSL approach can actually have lower error rate than the one that could be formed from a completely classified sample. In this study we consider this rather surprising result in cases where there may be more than two normal classes with not necessarily common covariance matrices.

</p>
</details>

<details><summary><b>Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies</b>
<a href="https://arxiv.org/abs/2210.13783">arxiv:2210.13783</a>
&#x1F4C8; 3 <br>
<p>Eitan Wagner, Renana Keydar, Amit Pinchevski, Omri Abend</p></summary>
<p>

**Abstract:** The task of topical segmentation is well studied, but previous work has mostly addressed it in the context of structured, well-defined segments, such as segmentation into paragraphs, chapters, or segmenting text that originated from multiple sources. We tackle the task of segmenting running (spoken) narratives, which poses hitherto unaddressed challenges. As a test case, we address Holocaust survivor testimonies, given in English. Other than the importance of studying these testimonies for Holocaust research, we argue that they provide an interesting test case for topical segmentation, due to their unstructured surface level, relative abundance (tens of thousands of such testimonies were collected), and the relatively confined domain that they cover. We hypothesize that boundary points between segments correspond to low mutual information between the sentences proceeding and following the boundary. Based on this hypothesis, we explore a range of algorithmic approaches to the task, building on previous work on segmentation that uses generative Bayesian modeling and state-of-the-art neural machinery. Compared to manually annotated references, we find that the developed approaches show considerable improvements over previous work.

</p>
</details>

<details><summary><b>Detecting fake accounts through Generative Adversarial Network in online social media</b>
<a href="https://arxiv.org/abs/2210.15657">arxiv:2210.15657</a>
&#x1F4C8; 2 <br>
<p>Jinus Bordbar, Mohammadreza Mohammadrezaie, Saman Ardalan, Mohammad Ebrahim Shiri</p></summary>
<p>

**Abstract:** Nowadays, online social media has become an inseparable part of human life, also this phenomenon is being used by individuals to send messages and share files via videos and images. Twitter, Instagram, and Facebook are well-known samples of these networks. One of the main challenges of privacy for users in these networks is anomalies in security. Anomalies in online social networks can be attributed to illegal behavior, such deviance is done by malicious people like account forgers, online fraudsters, etc. This paper proposed a new method to identify fake user accounts by calculating the similarity measures among users, applying the Generative Adversarial Network (GAN) algorithm over the Twitter dataset. The results of the proposed method showed, accuracy was able to reach 98.1% for classifying and detecting fake user accounts.

</p>
</details>

<details><summary><b>PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits</b>
<a href="https://arxiv.org/abs/2210.15345">arxiv:2210.15345</a>
&#x1F4C8; 2 <br>
<p>Kyoungseok Jang, Chicheng Zhang, Kwang-Sung Jun</p></summary>
<p>

**Abstract:** In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in prior work.

</p>
</details>

<details><summary><b>A new Stack Autoencoder: Neighbouring Sample Envelope Embedded Stack Autoencoder Ensemble Model</b>
<a href="https://arxiv.org/abs/2210.14956">arxiv:2210.14956</a>
&#x1F4C8; 2 <br>
<p>Chuanyan Zhou, Jie Ma, Fan Li, Yongming Li, Pin Wang, Xiaoheng Zhang</p></summary>
<p>

**Abstract:** Stack autoencoder (SAE), as a representative deep network, has unique and excellent performance in feature learning, and has received extensive attention from researchers. However, existing deep SAEs focus on original samples without considering the hierarchical structural information between samples. To address this limitation, this paper proposes a new SAE model-neighbouring envelope embedded stack autoencoder ensemble (NE_ESAE). Firstly, the neighbouring sample envelope learning mechanism (NSELM) is proposed for preprocessing of input of SAE. NSELM constructs sample pairs by combining neighbouring samples. Besides, the NSELM constructs a multilayer sample spaces by multilayer iterative mean clustering, which considers the similar samples and generates layers of envelope samples with hierarchical structural information. Second, an embedded stack autoencoder (ESAE) is proposed and trained in each layer of sample space to consider the original samples during training and in the network structure, thereby better finding the relationship between original feature samples and deep feature samples. Third, feature reduction and base classifiers are conducted on the layers of envelope samples respectively, and output classification results of every layer of samples. Finally, the classification results of the layers of envelope sample space are fused through the ensemble mechanism. In the experimental section, the proposed algorithm is validated with over ten representative public datasets. The results show that our method significantly has better performance than existing traditional feature learning methods and the representative deep autoencoders.

</p>
</details>

<details><summary><b>An Intelligent Decision Support Ensemble Voting Model for Coronary Artery Disease Prediction in Smart Healthcare Monitoring Environments</b>
<a href="https://arxiv.org/abs/2210.14906">arxiv:2210.14906</a>
&#x1F4C8; 2 <br>
<p>Anas Maach, Jamila Elalami, Noureddine Elalami, El Houssine El Mazoudi</p></summary>
<p>

**Abstract:** Coronary artery disease (CAD) is one of the most common cardiac diseases worldwide and causes disability and economic burden. It is the world's leading and most serious cause of mortality, with approximately 80% of deaths reported in low- and middle-income countries. The preferred and most precise diagnostic tool for CAD is angiography, but it is invasive, expensive, and technically demanding. However, the research community is increasingly interested in the computer-aided diagnosis of CAD via the utilization of machine learning (ML) methods. The purpose of this work is to present an e-diagnosis tool based on ML algorithms that can be used in a smart healthcare monitoring system. We applied the most accurate machine learning methods that have shown superior results in the literature to different medical datasets such as RandomForest, XGboost, MLP, J48, AdaBoost, NaiveBayes, LogitBoost, KNN. Every single classifier can be efficient on a different dataset. Thus, an ensemble model using majority voting was designed to take advantage of the well-performed single classifiers, Ensemble learning aims to combine the forecasts of multiple individual classifiers to achieve higher performance than individual classifiers in terms of precision, specificity, sensitivity, and accuracy; furthermore, we have benchmarked our proposed model with the most efficient and well-known ensemble models, such as Bagging, Stacking methods based on the cross-validation technique, The experimental results confirm that the ensemble majority voting approach based on the top 3 classifiers: MultilayerPerceptron, RandomForest, and AdaBoost, achieves the highest accuracy of 88,12% and outperforms all other classifiers. This study demonstrates that the majority voting ensemble approach proposed above is the most accurate machine learning classification approach for the prediction and detection of coronary artery disease.

</p>
</details>

<details><summary><b>Improving Imbalanced Text Classification with Dynamic Curriculum Learning</b>
<a href="https://arxiv.org/abs/2210.14724">arxiv:2210.14724</a>
&#x1F4C8; 2 <br>
<p>Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</p></summary>
<p>

**Abstract:** Recent advances in pre-trained language models have improved the performance for text classification tasks. However, little attention is paid to the priority scheduling strategy on the samples during training. Humans acquire knowledge gradually from easy to complex concepts, and the difficulty of the same material can also vary significantly in different learning stages. Inspired by this insights, we proposed a novel self-paced dynamic curriculum learning (SPDCL) method for imbalanced text classification, which evaluates the sample difficulty by both linguistic character and model capacity. Meanwhile, rather than using static curriculum learning as in the existing research, our SPDCL can reorder and resample training data by difficulty criterion with an adaptive from easy to hard pace. The extensive experiments on several classification tasks show the effectiveness of SPDCL strategy, especially for the imbalanced dataset.

</p>
</details>

<details><summary><b>Semi-Supervised Learning Based on Reference Model for Low-resource TTS</b>
<a href="https://arxiv.org/abs/2210.14723">arxiv:2210.14723</a>
&#x1F4C8; 2 <br>
<p>Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</p></summary>
<p>

**Abstract:** Most previous neural text-to-speech (TTS) methods are mainly based on supervised learning methods, which means they depend on a large training dataset and hard to achieve comparable performance under low-resource conditions. To address this issue, we propose a semi-supervised learning method for neural TTS in which labeled target data is limited, which can also resolve the problem of exposure bias in the previous auto-regressive models. Specifically, we pre-train the reference model based on Fastspeech2 with much source data, fine-tuned on a limited target dataset. Meanwhile, pseudo labels generated by the original reference model are used to guide the fine-tuned model's training further, achieve a regularization effect, and reduce the overfitting of the fine-tuned model during training on the limited target data. Experimental results show that our proposed semi-supervised learning scheme with limited target data significantly improves the voice quality for test data to achieve naturalness and robustness in speech synthesis.

</p>
</details>

<details><summary><b>SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks</b>
<a href="https://arxiv.org/abs/2210.14474">arxiv:2210.14474</a>
&#x1F4C8; 2 <br>
<p>Vasily Zadorozhnyy, Qiang Ye, Kazuhito Koishida</p></summary>
<p>

**Abstract:** In recent years, Generative Adversarial Networks (GANs) have produced significantly improved results in speech enhancement (SE) tasks. They are difficult to train, however. In this work, we introduce several improvements to the GAN training schemes, which can be applied to most GAN-based SE models. We propose using consistency loss functions, which target the inconsistency in time and time-frequency domains caused by Fourier and Inverse Fourier Transforms. We also present self-correcting optimization for training a GAN discriminator on SE tasks, which helps avoid "harmful" training directions for parts of the discriminator loss function. We have tested our proposed methods on several state-of-the-art GAN-based SE models and obtained consistent improvements, including new state-of-the-art results for the Voice Bank+DEMAND dataset.

</p>
</details>

<details><summary><b>Discovering Design Concepts for CAD Sketches</b>
<a href="https://arxiv.org/abs/2210.14451">arxiv:2210.14451</a>
&#x1F4C8; 2 <br>
<p>Yuezhi Yang, Hao Pan</p></summary>
<p>

**Abstract:** Sketch design concepts are recurring patterns found in parametric CAD sketches. Though rarely explicitly formalized by the CAD designers, these concepts are implicitly used in design for modularity and regularity. In this paper, we propose a learning based approach that discovers the modular concepts by induction over raw sketches. We propose the dual implicit-explicit representation of concept structures that allows implicit detection and explicit generation, and the separation of structure generation and parameter instantiation for parameterized concept generation, to learn modular concepts by end-to-end training. We demonstrate the design concept learning on a large scale CAD sketch dataset and show its applications for design intent interpretation and auto-completion.

</p>
</details>

<details><summary><b>Residual Learning of Neural Text Generation with $n$-gram Language Model</b>
<a href="https://arxiv.org/abs/2210.14431">arxiv:2210.14431</a>
&#x1F4C8; 2 <br>
<p>Huayang Li, Deng Cai, Jin Xu, Taro Watanabe</p></summary>
<p>

**Abstract:** $N$-gram language models (LM) have been largely superseded by neural LMs as the latter exhibits better performance. However, we find that $n$-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an $n$-gram LM and the real-data distribution. The combination of $n$-gram and neural LMs not only allows the neural part to focus on the deeper understanding of language but also provides a flexible way to customize an LM by switching the underlying $n$-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific $n$-gram model, without any extra training. Our code is released at https://github.com/ghrua/NgramRes.

</p>
</details>

<details><summary><b>RBP-DIP: High-Quality CT Reconstruction Using an Untrained Neural Network with Residual Back Projection and Deep Image Prior</b>
<a href="https://arxiv.org/abs/2210.14416">arxiv:2210.14416</a>
&#x1F4C8; 2 <br>
<p>Ziyu Shu, Alireza Entezari</p></summary>
<p>

**Abstract:** Neural network related methods, due to their unprecedented success in image processing, have emerged as a new set of tools in CT reconstruction with the potential to change the field. However, the lack of high-quality training data and theoretical guarantees, together with increasingly complicated network structures, make its implementation impractical. In this paper, we present a new framework (RBP-DIP) based on Deep Image Prior (DIP) and a special residual back projection (RBP) connection to tackle these challenges. Comparing to other pre-trained neural network related algorithms, the proposed framework is closer to an iterative reconstruction (IR) algorithm as it requires no training data or training process. In that case, the proposed framework can be altered (e.g, different hyperparameters and constraints) on demand, adapting to different conditions (e.g, different imaged objects, imaging instruments, and noise levels) without retraining. Experiments show that the proposed framework has significant improvements over other state-of-the-art conventional methods, as well as pre-trained and untrained models with similar network structures, especially under sparse-view, limited-angle, and low-dose conditions.

</p>
</details>

<details><summary><b>Modeling the Graphotactics of Low-Resource Languages Using Sequential GANs</b>
<a href="https://arxiv.org/abs/2210.14409">arxiv:2210.14409</a>
&#x1F4C8; 2 <br>
<p>Isaac Wasserman</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) have been shown to aid in the creation of artificial data in situations where large amounts of real data are difficult to come by. This issue is especially salient in the computational linguistics space, where researchers are often tasked with modeling the complex morphologic and grammatical processes of low-resource languages. This paper will discuss the implementation and testing of a GAN that attempts to model and reproduce the graphotactics of a language using only 100 example strings. These artificial, yet graphotactically compliant, strings are meant to aid in modeling the morphological inflection of low-resource languages.

</p>
</details>

<details><summary><b>An Attention-based Long Short-Term Memory Framework for Detection of Bitcoin Scams</b>
<a href="https://arxiv.org/abs/2210.14408">arxiv:2210.14408</a>
&#x1F4C8; 2 <br>
<p>Puyang Zhao, Wei Tian, Lefu Xiao, Xinhui Liu, Jingjin Wu</p></summary>
<p>

**Abstract:** Bitcoin is the most common cryptocurrency involved in cyber scams. Cybercriminals often utilize pseudonymity and privacy protection mechanism associated with Bitcoin transactions to make their scams virtually untraceable. The Ponzi scheme has attracted particularly significant attention among Bitcoin fraudulent activities. This paper considers a multi-class classification problem to determine whether a transaction is involved in Ponzi schemes or other cyber scams, or is a non-scam transaction. We design a specifically designed crawler to collect data and propose a novel Attention-based Long Short-Term Memory (A-LSTM) method for the classification problem. The experimental results show that the proposed model has better efficiency and accuracy than existing approaches, including Random Forest, Extra Trees, Gradient Boosting, and classical LSTM. With correctly identified scam features, our proposed A-LSTM achieves an F1-score over 82% for the original data and outperforms the existing approaches.

</p>
</details>

<details><summary><b>Adversarially Robust Medical Classification via Attentive Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2210.14405">arxiv:2210.14405</a>
&#x1F4C8; 2 <br>
<p>Isaac Wasserman</p></summary>
<p>

**Abstract:** Convolutional neural network-based medical image classifiers have been shown to be especially susceptible to adversarial examples. Such instabilities are likely to be unacceptable in the future of automated diagnoses. Though statistical adversarial example detection methods have proven to be effective defense mechanisms, additional research is necessary that investigates the fundamental vulnerabilities of deep-learning-based systems and how best to build models that jointly maximize traditional and robust accuracy. This paper presents the inclusion of attention mechanisms in CNN-based medical image classifiers as a reliable and effective strategy for increasing robust accuracy without sacrifice. This method is able to increase robust accuracy by up to 16% in typical adversarial scenarios and up to 2700% in extreme cases.

</p>
</details>

<details><summary><b>Adaptive Test-Time Defense with the Manifold Hypothesis</b>
<a href="https://arxiv.org/abs/2210.14404">arxiv:2210.14404</a>
&#x1F4C8; 2 <br>
<p>Zhaoyuan Yang, Zhiwei Xu, Jing Zhang, Richard Hartley, Peter Tu</p></summary>
<p>

**Abstract:** In this work, we formulate a novel framework of adversarial robustness using the manifold hypothesis. Our framework provides sufficient conditions for defending against adversarial examples. We develop a test-time defense method with our formulation and variational inference. The developed approach combines manifold learning with the Bayesian framework to provide adversarial robustness without the need for adversarial training. We show that our proposed approach can provide adversarial robustness even if attackers are aware of existence of test-time defense. In additions, our approach can also serve as a test-time defense mechanism for variational autoencoders.

</p>
</details>

<details><summary><b>Federated Fuzzy Neural Network with Evolutionary Rule Learning</b>
<a href="https://arxiv.org/abs/2210.14393">arxiv:2210.14393</a>
&#x1F4C8; 2 <br>
<p>Leijie Zhang, Ye Shi, Yu-Cheng Chang, Chin-Teng Lin</p></summary>
<p>

**Abstract:** Distributed fuzzy neural networks (DFNNs) have attracted increasing attention recently due to their learning abilities in handling data uncertainties in distributed scenarios. However, it is challenging for DFNNs to handle cases in which the local data are non-independent and identically distributed (non-IID). In this paper, we propose a federated fuzzy neural network (FedFNN) with evolutionary rule learning (ERL) to cope with non-IID issues as well as data uncertainties. The FedFNN maintains a global set of rules in a server and a personalized subset of these rules for each local client. ERL is inspired by the theory of biological evolution; it encourages rule variations while activating superior rules and deactivating inferior rules for local clients with non-IID data. Specifically, ERL consists of two stages in an iterative procedure: a rule cooperation stage that updates global rules by aggregating local rules based on their activation statuses and a rule evolution stage that evolves the global rules and updates the activation statuses of the local rules. This procedure improves both the generalization and personalization of the FedFNN for dealing with non-IID issues and data uncertainties. Extensive experiments conducted on a range of datasets demonstrate the superiority of the FedFNN over state-of-the-art methods.

</p>
</details>

<details><summary><b>Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects?</b>
<a href="https://arxiv.org/abs/2210.14391">arxiv:2210.14391</a>
&#x1F4C8; 2 <br>
<p>Felicia Ruppel, Florian Faion, Claudius Gläser, Klaus Dietmayer</p></summary>
<p>

**Abstract:** Transformers have recently been utilized to perform object detection and tracking in the context of autonomous driving. One unique characteristic of these models is that attention weights are computed in each forward pass, giving insights into the model's interior, in particular, which part of the input data it deemed interesting for the given task. Such an attention matrix with the input grid is available for each detected (or tracked) object in every transformer decoder layer. In this work, we investigate the distribution of these attention weights: How do they change through the decoder layers and through the lifetime of a track? Can they be used to infer additional information about an object, such as a detection uncertainty? Especially in unstructured environments, or environments that were not common during training, a reliable measure of detection uncertainty is crucial to decide whether the system can still be trusted or not.

</p>
</details>

<details><summary><b>Auxiliary task discovery through generate-and-test</b>
<a href="https://arxiv.org/abs/2210.14361">arxiv:2210.14361</a>
&#x1F4C8; 2 <br>
<p>Banafsheh Rafiee, Sina Ghiassian, Jun Jin, Richard Sutton, Jun Luo, Adam White</p></summary>
<p>

**Abstract:** In this paper, we explore an approach to auxiliary task discovery in reinforcement learning based on ideas from representation learning. Auxiliary tasks tend to improve data efficiency by forcing the agent to learn auxiliary prediction and control objectives in addition to the main task of maximizing reward, and thus producing better representations. Typically these tasks are designed by people. Meta-learning offers a promising avenue for automatic task discovery; however, these methods are computationally expensive and challenging to tune in practice. In this paper, we explore a complementary approach to the auxiliary task discovery: continually generating new auxiliary tasks and preserving only those with high utility. We also introduce a new measure of auxiliary tasks usefulness based on how useful the features induced by them are for the main task. Our discovery algorithm significantly outperforms random tasks, hand-designed tasks, and learning without auxiliary tasks across a suite of environments.

</p>
</details>

<details><summary><b>LaundroGraph: Self-Supervised Graph Representation Learning for Anti-Money Laundering</b>
<a href="https://arxiv.org/abs/2210.14360">arxiv:2210.14360</a>
&#x1F4C8; 2 <br>
<p>Mário Cardoso, Pedro Saleiro, Pedro Bizarro</p></summary>
<p>

**Abstract:** Anti-money laundering (AML) regulations mandate financial institutions to deploy AML systems based on a set of rules that, when triggered, form the basis of a suspicious alert to be assessed by human analysts. Reviewing these cases is a cumbersome and complex task that requires analysts to navigate a large network of financial interactions to validate suspicious movements. Furthermore, these systems have very high false positive rates (estimated to be over 95\%). The scarcity of labels hinders the use of alternative systems based on supervised learning, reducing their applicability in real-world applications.
  In this work we present LaundroGraph, a novel self-supervised graph representation learning approach to encode banking customers and financial transactions into meaningful representations. These representations are used to provide insights to assist the AML reviewing process, such as identifying anomalous movements for a given customer. LaundroGraph represents the underlying network of financial interactions as a customer-transaction bipartite graph and trains a graph neural network on a fully self-supervised link prediction task. We empirically demonstrate that our approach outperforms other strong baselines on self-supervised link prediction using a real-world dataset, improving the best non-graph baseline by $12$ p.p. of AUC. The goal is to increase the efficiency of the reviewing process by supplying these AI-powered insights to the analysts upon review. To the best of our knowledge, this is the first fully self-supervised system within the context of AML detection.

</p>
</details>

<details><summary><b>Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations</b>
<a href="https://arxiv.org/abs/2210.14358">arxiv:2210.14358</a>
&#x1F4C8; 2 <br>
<p>Huaxiu Yao, Xinyu Yang, Allan Zhou, Chelsea Finn</p></summary>
<p>

**Abstract:** There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Existing long-tailed classification methods focus on the single-domain setting, where all examples are drawn from the same distribution. However, real-world scenarios often involve multiple domains with distinct imbalanced class distributions. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, which produces invariant predictors by balanced augmenting hidden representations over domains and classes. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on four long-tailed variants of classical domain generalization benchmarks and two real-world imbalanced multi-domain datasets. The results indicate that TALLY consistently outperforms other state-of-the-art methods in both subpopulation shift and domain shift.

</p>
</details>

<details><summary><b>Artificial ASMR: A Cyber-Psychological Study</b>
<a href="https://arxiv.org/abs/2210.14321">arxiv:2210.14321</a>
&#x1F4C8; 2 <br>
<p>Zexin Fang, Bin Han, C. Clark Cao, Hans. D. Schotten</p></summary>
<p>

**Abstract:** The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockteted over the past decade, but scientific studies on it are still few and immature. With our attention caught by the common acoustic patterns in ASMR audios, we investigate the correlation between the time-frequency and cyclic features of audio signals and their effectiveness in triggering ASMR effects. A cyber-psychological approach that combines signal processing, artificial intelligence, and experimental psychology is taken, with which we are able to identify ASMR-related acoustic features, and therewith synthesize random artificial ASMR audios.

</p>
</details>

<details><summary><b>Accelerating Certified Robustness Training via Knowledge Transfer</b>
<a href="https://arxiv.org/abs/2210.14283">arxiv:2210.14283</a>
&#x1F4C8; 2 <br>
<p>Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati</p></summary>
<p>

**Abstract:** Training deep neural network classifiers that are certifiably robust against adversarial attacks is critical to ensuring the security and reliability of AI-controlled systems. Although numerous state-of-the-art certified training methods have been developed, they are computationally expensive and scale poorly with respect to both dataset and network complexity. Widespread usage of certified training is further hindered by the fact that periodic retraining is necessary to incorporate new data and network improvements. In this paper, we propose Certified Robustness Transfer (CRT), a general-purpose framework for reducing the computational overhead of any certifiably robust training method through knowledge transfer. Given a robust teacher, our framework uses a novel training loss to transfer the teacher's robustness to the student. We provide theoretical and empirical validation of CRT. Our experiments on CIFAR-10 show that CRT speeds up certified robustness training by $8 \times$ on average across three different architecture generations while achieving comparable robustness to state-of-the-art methods. We also show that CRT can scale to large-scale datasets like ImageNet.

</p>
</details>

<details><summary><b>A Survey on 3D-aware Image Synthesis</b>
<a href="https://arxiv.org/abs/2210.14267">arxiv:2210.14267</a>
&#x1F4C8; 2 <br>
<p>Weihao Xia, Jing-Hao Xue</p></summary>
<p>

**Abstract:** Recent years have seen remarkable progress in deep learning powered visual content creation. This includes 3D-aware generative image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The 3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. The task of 3D-aware image synthesis has taken the field of computer vision by storm, with hundreds of papers accepted to top-tier journals and conferences in recent year (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at https://weihaox.github.io/projects/awesome-3d-aware.

</p>
</details>

<details><summary><b>CaloFlow for CaloChallenge Dataset 1</b>
<a href="https://arxiv.org/abs/2210.14245">arxiv:2210.14245</a>
&#x1F4C8; 2 <br>
<p>Claudius Krause, Ian Pang, David Shih</p></summary>
<p>

**Abstract:** CaloFlow is a new and promising approach to fast calorimeter simulation based on normalizing flows. Applying CaloFlow to the photon and charged pion Geant4 showers of Dataset 1 of the Fast Calorimeter Simulation Challenge 2022, we show how it can produce high-fidelity samples with a sampling time that is several orders of magnitude faster than Geant4. We demonstrate the fidelity of the samples using calorimeter shower images, histograms of high level features, and aggregate metrics such as a classifier trained to distinguish CaloFlow from Geant4 samples.

</p>
</details>

<details><summary><b>Causal Information Bottleneck Boosts Adversarial Robustness of Deep Neural Network</b>
<a href="https://arxiv.org/abs/2210.14229">arxiv:2210.14229</a>
&#x1F4C8; 2 <br>
<p>Huan Hua, Jun Yan, Xi Fang, Weiquan Huang, Huilin Yin, Wancheng Ge</p></summary>
<p>

**Abstract:** The information bottleneck (IB) method is a feasible defense solution against adversarial attacks in deep learning. However, this method suffers from the spurious correlation, which leads to the limitation of its further improvement of adversarial robustness. In this paper, we incorporate the causal inference into the IB framework to alleviate such a problem. Specifically, we divide the features obtained by the IB method into robust features (content information) and non-robust features (style information) via the instrumental variables to estimate the causal effects. With the utilization of such a framework, the influence of non-robust features could be mitigated to strengthen the adversarial robustness. We make an analysis of the effectiveness of our proposed method. The extensive experiments in MNIST, FashionMNIST, and CIFAR-10 show that our method exhibits the considerable robustness against multiple adversarial attacks. Our code would be released.

</p>
</details>

<details><summary><b>Modelling Residential Supply Tasks Based on Digital Orthophotography Using Machine Learning</b>
<a href="https://arxiv.org/abs/2210.14013">arxiv:2210.14013</a>
&#x1F4C8; 2 <br>
<p>Klemens Schumann, Luis Böttcher, Philipp Hälsig, Daniel Zelenak, Andreas Ulbig</p></summary>
<p>

**Abstract:** In order to achieve the climate targets, electrification of individual mobility is essential. However, grid integration of electrical vehicles poses challenges for the electrical distribution network due to high charging power and simultaneity. To investigate these challenges in research studies, the network-referenced supply task needs to be modeled. Previous research work utilizes data that is not always complete or sufficiently granular in space. This is why this paper presents a methodology which allows a holistic determination of residential supply tasks based on orthophotos. To do this, buildings are first identified from orthophotos, then residential building types are classified, and finally the electricity demand of each building is determined. In an exemplary case study, we validate the presented methodology and compare the results with another supply task methodology. The results show that the electricity demand deviates from the results of a reference method by an average 9%. Deviations result mainly from the parameterization of the selected residential building types. Thus, the presented methodology is able to model supply tasks similarly as other methods but more granular.

</p>
</details>

<details><summary><b>An Optimal Stochastic Algorithm for Decentralized Nonconvex Finite-sum Optimization</b>
<a href="https://arxiv.org/abs/2210.13931">arxiv:2210.13931</a>
&#x1F4C8; 2 <br>
<p>Luo Luo, Haishan Ye</p></summary>
<p>

**Abstract:** This paper studies the synchronized decentralized nonconvex optimization problem of the form $\min_{x\in{\mathbb R}^d} f(x)\triangleq \frac{1}{m}\sum_{i=1}^m f_i(x)$, where $f_i(x)\triangleq \frac{1}{n}\sum_{j=1}^n f_{i,j}(x)$ is the local function on $i$-th agent of the connected network. We propose a novel stochastic algorithm called DEcentralized probAbilistic Recursive gradiEnt deScenT (DEAREST), which integrates the techniques of variance reduction, gradient tracking and multi-consensus. We construct a Lyapunov function that simultaneously characterizes the function value, the gradient estimation error and the consensus error for the convergence analysis. Based on this measure, we provide a concise proof to show DEAREST requires at most ${\mathcal O}(mn+\sqrt{mn}L\varepsilon^{-2})$ incremental first-order oracle (IFO) calls and ${\mathcal O}(L\varepsilon^{-2}/\sqrt{1-λ_2(W)}\,)$ communication rounds to find an $\varepsilon$-stationary point in expectation, where $L$ is the smoothness parameter and $λ_2(W)$ is the second-largest eigenvalues of the gossip matrix $W$. We can verify both of the IFO complexity and communication complexity match the lower bounds. To the best of our knowledge, DEAREST is the first optimal algorithm for decentralized nonconvex finite-sum optimization.

</p>
</details>

<details><summary><b>LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels</b>
<a href="https://arxiv.org/abs/2210.15446">arxiv:2210.15446</a>
&#x1F4C8; 1 <br>
<p>Jiebao Zhang, Wenhua Qian, Rencan Nie, Jinde Cao, Dan Xu</p></summary>
<p>

**Abstract:** Deep neural networks are vulnerable to adversarial attacks. Most white-box attacks are based on the gradient of models to the input. Since the computation and memory budget, adversarial attacks based on the Hessian information are not paid enough attention. In this work, we study the attack performance and computation cost of the attack method based on the Hessian with a limited perturbation pixel number. Specifically, we propose the Limited Pixel BFGS (LP-BFGS) attack method by incorporating the BFGS algorithm. Some pixels are selected as perturbation pixels by the Integrated Gradient algorithm, which are regarded as optimization variables of the LP-BFGS attack. Experimental results across different networks and datasets with various perturbation pixel numbers demonstrate our approach has a comparable attack with an acceptable computation compared with existing solutions.

</p>
</details>

<details><summary><b>Neuro-symbolic partial differential equation solver</b>
<a href="https://arxiv.org/abs/2210.14907">arxiv:2210.14907</a>
&#x1F4C8; 1 <br>
<p>Pouria Mistani, Samira Pakravan, Rajesh Ilango, Sanjay Choudhry, Frederic Gibou</p></summary>
<p>

**Abstract:** We present a highly scalable strategy for developing mesh-free neuro-symbolic partial differential equation solvers from existing numerical discretizations found in scientific computing. This strategy is unique in that it can be used to efficiently train neural network surrogate models for the solution functions and the differential operators, while retaining the accuracy and convergence properties of state-of-the-art numerical solvers. This neural bootstrapping method is based on minimizing residuals of discretized differential systems on a set of random collocation points with respect to the trainable parameters of the neural network, achieving unprecedented resolution and optimal scaling for solving physical and biological systems.

</p>
</details>

<details><summary><b>Short Paper: Static and Microarchitectural ML-Based Approaches For Detecting Spectre Vulnerabilities and Attacks</b>
<a href="https://arxiv.org/abs/2210.14452">arxiv:2210.14452</a>
&#x1F4C8; 1 <br>
<p>Chidera Biringa, Gaspard Baye, Gökhan Kul</p></summary>
<p>

**Abstract:** Spectre intrusions exploit speculative execution design vulnerabilities in modern processors. The attacks violate the principles of isolation in programs to gain unauthorized private user information. Current state-of-the-art detection techniques utilize micro-architectural features or vulnerable speculative code to detect these threats. However, these techniques are insufficient as Spectre attacks have proven to be more stealthy with recently discovered variants that bypass current mitigation mechanisms. Side-channels generate distinct patterns in processor cache, and sensitive information leakage is dependent on source code vulnerable to Spectre attacks, where an adversary uses these vulnerabilities, such as branch prediction, which causes a data breach. Previous studies predominantly approach the detection of Spectre attacks using the microarchitectural analysis, a reactive approach. Hence, in this paper, we present the first comprehensive evaluation of static and microarchitectural analysis-assisted machine learning approaches to detect Spectre vulnerable code snippets (preventive) and Spectre attacks (reactive). We evaluate the performance trade-offs in employing classifiers for detecting Spectre vulnerabilities and attacks.

</p>
</details>

<details><summary><b>Optimizing Pessimism in Dynamic Treatment Regimes: A Bayesian Learning Approach</b>
<a href="https://arxiv.org/abs/2210.14420">arxiv:2210.14420</a>
&#x1F4C8; 1 <br>
<p>Yunzhe Zhou, Zhengling Qi, Chengchun Shi, Lexin Li</p></summary>
<p>

**Abstract:** In this article, we propose a novel pessimism-based Bayesian learning method for optimal dynamic treatment regimes in the offline setting. When the coverage condition does not hold, which is common for offline data, the existing solutions would produce sub-optimal policies. The pessimism principle addresses this issue by discouraging recommendation of actions that are less explored conditioning on the state. However, nearly all pessimism-based methods rely on a key hyper-parameter that quantifies the degree of pessimism, and the performance of the methods can be highly sensitive to the choice of this parameter. We propose to integrate the pessimism principle with Thompson sampling and Bayesian machine learning for optimizing the degree of pessimism. We derive a credible set whose boundary uniformly lower bounds the optimal Q-function, and thus does not require additional tuning of the degree of pessimism. We develop a general Bayesian learning method that works with a range of models, from Bayesian linear basis model to Bayesian neural network model. We develop the computational algorithm based on variational inference, which is highly efficient and scalable. We establish the theoretical guarantees of the proposed method, and show empirically that it outperforms the existing state-of-the-art solutions through both simulations and a real data example.

</p>
</details>

<details><summary><b>FedX: Federated Learning for Compositional Pairwise Risk Optimization</b>
<a href="https://arxiv.org/abs/2210.14396">arxiv:2210.14396</a>
&#x1F4C8; 1 <br>
<p>Zhishuai Guo, Rong Jin, Jiebo Luo, Tianbao Yang</p></summary>
<p>

**Abstract:** In this paper, we tackle a novel federated learning (FL) problem for optimizing a family of compositional pairwise risks, to which no existing FL algorithms are applicable. In particular, the objective has the form of $\mathbb E_{\mathbf z\sim \mathcal S_1} f(\mathbb E_{\mathbf z'\sim\mathcal S_2} \ell(\mathbf w, \mathbf z, \mathbf z'))$, where two sets of data $\mathcal S_1, \mathcal S_2$ are distributed over multiple machines, $\ell(\cdot; \cdot,\cdot)$ is a pairwise loss that only depends on the prediction outputs of the input data pairs $(\mathbf z, \mathbf z')$, and $f(\cdot)$ is possibly a non-linear non-convex function. This problem has important applications in machine learning, e.g., AUROC maximization with a pairwise loss, and partial AUROC maximization with a compositional loss. The challenges for designing an FL algorithm lie in the non-decomposability of the objective over multiple machines and the interdependency between different machines. We propose two provable FL algorithms (FedX) for handling linear and nonlinear $f$, respectively. To address the challenges, we decouple the gradient's components with two types, namely active parts and lazy parts, where the active parts depend on local data that are computed with the local model and the lazy parts depend on other machines that are communicated/computed based on historical models and samples. We develop a novel theoretical analysis to combat the latency of the lazy parts and the interdependency between the local model parameters and the involved data for computing local gradient estimators. We establish both iteration and communication complexities and show that using the historical samples and models for computing the lazy parts do not degrade the complexities. We conduct empirical studies of FedX for deep AUROC and partial AUROC maximization, and demonstrate their performance compared with several baselines.

</p>
</details>

<details><summary><b>Parameter-free Regret in High Probability with Heavy Tails</b>
<a href="https://arxiv.org/abs/2210.14355">arxiv:2210.14355</a>
&#x1F4C8; 1 <br>
<p>Jiujia Zhang, Ashok Cutkosky</p></summary>
<p>

**Abstract:** We present new algorithms for online convex optimization over unbounded domains that obtain parameter-free regret in high-probability given access only to potentially heavy-tailed subgradient estimates. Previous work in unbounded domains considers only in-expectation results for sub-exponential subgradients. Unlike in the bounded domain case, we cannot rely on straight-forward martingale concentration due to exponentially large iterates produced by the algorithm. We develop new regularization techniques to overcome these problems. Overall, with probability at most $δ$, for all comparators $\mathbf{u}$ our algorithm achieves regret $\tilde{O}(\| \mathbf{u} \| T^{1/\mathfrak{p}} \log (1/δ))$ for subgradients with bounded $\mathfrak{p}^{th}$ moments for some $\mathfrak{p} \in (1, 2]$.

</p>
</details>

<details><summary><b>Arc travel time and path choice model estimation subsumed</b>
<a href="https://arxiv.org/abs/2210.14351">arxiv:2210.14351</a>
&#x1F4C8; 1 <br>
<p>Sobhan Mohammadpour, Emma Frejinger</p></summary>
<p>

**Abstract:** We propose a method for maximum likelihood estimation of path choice model parameters and arc travel time using data of different levels of granularity. Hitherto these two tasks have been tackled separately under strong assumptions. Using a small example, we illustrate that this can lead to biased results. Results on both real (New York yellow cab) and simulated data show strong performance of our method compared to existing baselines.

</p>
</details>

<details><summary><b>Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures</b>
<a href="https://arxiv.org/abs/2210.14347">arxiv:2210.14347</a>
&#x1F4C8; 1 <br>
<p>Xin Bing, Marten Wegkamp</p></summary>
<p>

**Abstract:** This paper considers binary classification of high-dimensional features under a postulated model with a low-dimensional latent Gaussian mixture structure and non-vanishing noise. A generalized least squares estimator is used to estimate the direction of the optimal separating hyperplane. The estimated hyperplane is shown to interpolate on the training data. While the direction vector can be consistently estimated as could be expected from recent results in linear regression, a naive plug-in estimate fails to consistently estimate the intercept. A simple correction, that requires an independent hold-out sample, renders the procedure minimax optimal in many scenarios. The interpolation property of the latter procedure can be retained, but surprisingly depends on the way the labels are encoded.

</p>
</details>

<details><summary><b>A single-cell gene expression language model</b>
<a href="https://arxiv.org/abs/2210.14330">arxiv:2210.14330</a>
&#x1F4C8; 1 <br>
<p>William Connell, Umair Khan, Michael J. Keiser</p></summary>
<p>

**Abstract:** Gene regulation is a dynamic process that connects genotype and phenotype. Given the difficulty of physically mapping mammalian gene circuitry, we require new computational methods to learn regulatory rules. Natural language is a valuable analogy to the communication of regulatory control. Machine learning systems model natural language by explicitly learning context dependencies between words. We propose a similar system applied to single-cell RNA expression profiles to learn context dependencies between genes. Our model, Exceiver, is trained across a diversity of cell types using a self-supervised task formulated for discrete count data, accounting for feature sparsity. We found agreement between the similarity profiles of latent sample representations and learned gene embeddings with respect to biological annotations. We evaluated Exceiver on a new dataset and a downstream prediction task and found that pretraining supports transfer learning. Our work provides a framework to model gene regulation on a single-cell level and transfer knowledge to downstream tasks.

</p>
</details>

<details><summary><b>ANACONDA: An Improved Dynamic Regret Algorithm for Adaptive Non-Stationary Dueling Bandits</b>
<a href="https://arxiv.org/abs/2210.14322">arxiv:2210.14322</a>
&#x1F4C8; 1 <br>
<p>Thomas Kleine Buening, Aadirupa Saha</p></summary>
<p>

**Abstract:** We study the problem of non-stationary dueling bandits and provide the first adaptive dynamic regret algorithm for this problem. The only two existing attempts in this line of work fall short across multiple dimensions, including pessimistic measures of non-stationary complexity and non-adaptive parameter tuning that requires knowledge of the number of preference changes. We develop an elimination-based rescheduling algorithm to overcome these shortcomings and show a near-optimal $\tilde{O}(\sqrt{S^{\texttt{CW}} T})$ dynamic regret bound, where $S^{\texttt{CW}}$ is the number of times the Condorcet winner changes in $T$ rounds. This yields the first near-optimal dynamic regret algorithm for unknown $S^{\texttt{CW}}$. We further study other related notions of non-stationarity for which we also prove near-optimal dynamic regret guarantees under additional assumptions on the underlying preference model.

</p>
</details>

<details><summary><b>JAX-DIPS: Neural bootstrapping of finite discretization methods and application to elliptic problems with discontinuities</b>
<a href="https://arxiv.org/abs/2210.14312">arxiv:2210.14312</a>
&#x1F4C8; 1 <br>
<p>Pouria Mistani, Samira Pakravan, Rajesh Ilango, Frederic Gibou</p></summary>
<p>

**Abstract:** We present a scalable strategy for development of mesh-free hybrid neuro-symbolic partial differential equation solvers based on existing mesh-based numerical discretization methods. Particularly, this strategy can be used to efficiently train neural network surrogate models for the solution functions and operators of partial differential equations while retaining the accuracy and convergence properties of the state-of-the-art numerical solvers. The presented neural bootstrapping method (hereby dubbed NBM) is based on evaluation of the finite discretization residuals of the PDE system obtained on implicit Cartesian cells centered on a set of random collocation points with respect to trainable parameters of the neural network. We apply NBM to the important class of elliptic problems with jump conditions across irregular interfaces in three spatial dimensions. We show the method is convergent such that model accuracy improves by increasing number of collocation points in the domain. The algorithms presented here are implemented and released in a software package named JAX-DIPS (https://github.com/JAX-DIPS/JAX-DIPS), standing for differentiable interfacial PDE solver. JAX-DIPS is purely developed in JAX, offering end-to-end differentiability from mesh generation to the higher level discretization abstractions, geometric integrations, and interpolations, thus facilitating research into use of differentiable algorithms for developing hybrid PDE solvers.

</p>
</details>

<details><summary><b>Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming</b>
<a href="https://arxiv.org/abs/2210.14306">arxiv:2210.14306</a>
&#x1F4C8; 1 <br>
<p>Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz</p></summary>
<p>

**Abstract:** AI code-recommendation systems (CodeRec), such as Copilot, can assist programmers inside an IDE by suggesting and autocompleting arbitrary code; potentially improving their productivity. To understand how these AI improve programmers in a coding session, we need to understand how they affect programmers' behavior. To make progress, we studied GitHub Copilot, and developed CUPS -- a taxonomy of 12 programmer activities common to AI code completion systems. We then conducted a study with 21 programmers who completed coding tasks and used our labeling tool to retrospectively label their sessions with CUPS. We analyze over 3000 label instances, and visualize the results with timelines and state machines to profile programmer-CodeRec interaction. This reveals novel insights into the distribution and patterns of programmer behavior, as well as inefficiencies and time costs. Finally, we use these insights to inform future interventions to improve AI-assisted programming and human-AI interaction.

</p>
</details>

<details><summary><b>Wasserstein Archetypal Analysis</b>
<a href="https://arxiv.org/abs/2210.14298">arxiv:2210.14298</a>
&#x1F4C8; 1 <br>
<p>Katy Craig, Braxton Osting, Dong Wang, Yiming Xu</p></summary>
<p>

**Abstract:** Archetypal analysis is an unsupervised machine learning method that summarizes data using a convex polytope. In its original formulation, for fixed k, the method finds a convex polytope with k vertices, called archetype points, such that the polytope is contained in the convex hull of the data and the mean squared Euclidean distance between the data and the polytope is minimal.
  In the present work, we consider an alternative formulation of archetypal analysis based on the Wasserstein metric, which we call Wasserstein archetypal analysis (WAA). In one dimension, there exists a unique solution of WAA and, in two dimensions, we prove existence of a solution, as long as the data distribution is absolutely continuous with respect to Lebesgue measure. We discuss obstacles to extending our result to higher dimensions and general data distributions. We then introduce an appropriate regularization of the problem, via a Renyi entropy, which allows us to obtain existence of solutions of the regularized problem for general data distributions, in arbitrary dimensions. We prove a consistency result for the regularized problem, ensuring that if the data are iid samples from a probability measure, then as the number of samples is increased, a subsequence of the archetype points converges to the archetype points for the limiting data distribution, almost surely. Finally, we develop and implement a gradient-based computational approach for the two-dimensional problem, based on the semi-discrete formulation of the Wasserstein metric. Our analysis is supported by detailed computational experiments.

</p>
</details>

<details><summary><b>NAS-PRNet: Neural Architecture Search generated Phase Retrieval Net for Off-axis Quantitative Phase Imaging</b>
<a href="https://arxiv.org/abs/2210.14231">arxiv:2210.14231</a>
&#x1F4C8; 1 <br>
<p>Xin Shu, Mengxuan Niu, Yi Zhang, Renjie Zhou</p></summary>
<p>

**Abstract:** Single neural networks have achieved simultaneous phase retrieval with aberration compensation and phase unwrapping in off-axis Quantitative Phase Imaging (QPI). However, when designing the phase retrieval neural network architecture, the trade-off between computation latency and accuracy has been largely neglected. Here, we propose Neural Architecture Search (NAS) generated Phase Retrieval Net (NAS-PRNet), which is an encoder-decoder style neural network, automatically found from a large neural network architecture search space. The NAS scheme in NAS-PRNet is modified from SparseMask, in which the learning of skip connections between the encoder and the decoder is formulated as a differentiable NAS problem, and the gradient decent is applied to efficiently search the optimal skip connections. Using MobileNet-v2 as the encoder and a synthesized loss that incorporates phase reconstruction and network sparsity losses, NAS-PRNet has realized fast and accurate phase retrieval of biological cells. When tested on a cell dataset, NAS-PRNet has achieved a Peak Signal-to-Noise Ratio (PSNR) of 36.1 dB, outperforming the widely used U-Net and original SparseMask-generated neural network. Notably, the computation latency of NAS-PRNet is only 31 ms which is 12 times less than U-Net. Moreover, the connectivity scheme in NAS-PRNet, identified from one off-axis QPI system, can be well fitted to another with different fringe patterns.

</p>
</details>

<details><summary><b>Shortest Edit Path Crossover: A Theory-driven Solution to the Permutation Problem in Evolutionary Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2210.14016">arxiv:2210.14016</a>
&#x1F4C8; 1 <br>
<p>Xin Qiu, Risto Miikkulainen</p></summary>
<p>

**Abstract:** Evolutionary algorithms (EAs) have gained attention recently due to their success in neural architecture search (NAS). However, whereas traditional EAs draw much power from crossover operations, most evolutionary NAS methods deploy only mutation operators. The main reason is the permutation problem: The mapping between genotype and phenotype in traditional graph representations is many-to-one, leading to a disruptive effect of standard crossover. This work conducts the first theoretical analysis of the behaviors of crossover and mutation in the NAS context, and proposes a new crossover operator based on the shortest edit path (SEP) in graph space. The SEP crossover is shown to overcome the permutation problem, and as a result, offspring generated by the SEP crossover is theoretically proved to have a better expected improvement in terms of graph edit distance to global optimum, compared to mutation and standard crossover. Experiments further show that the SEP crossover significantly outperforms mutation and standard crossover on three state-of-the-art NAS benchmarks. The SEP crossover therefore allows taking full advantage of evolution in NAS, and potentially other similar design problems as well.

</p>
</details>

<details><summary><b>Deformation Theory of Boltzmann Distributions</b>
<a href="https://arxiv.org/abs/2210.13772">arxiv:2210.13772</a>
&#x1F4C8; 1 <br>
<p>Bálint Máté, François Fleuret</p></summary>
<p>

**Abstract:** Consider a one-parameter family of Boltzmann distributions $p_t(x) = \tfrac{1}{Z_t}e^{-S_t(x)}$. In this paper we study the problem of sampling from $p_{t_0}$ by first sampling from $p_{t_1}$ and then applying a transformation $Ψ_{t_1}^{t_0}$ to the samples so that to they follow $p_{t_0}$. We derive an equation relating $Ψ$ and the corresponding family of unnormalized log-likelihoods $S_t$. We demonstrate the utility of this idea on the $φ^4$ lattice field theory by extending its defining action $S_0$ to a family of actions $S_t$ and finding a $τ$ such that normalizing flows perform better at learning the Boltzmann distribution $p_τ$ than at learning $p_0$.

</p>
</details>

<details><summary><b>Networked Signal and Information Processing</b>
<a href="https://arxiv.org/abs/2210.13767">arxiv:2210.13767</a>
&#x1F4C8; 1 <br>
<p>Stefan Vlaski, Soummya Kar, Ali H. Sayed, José M. F. Moura</p></summary>
<p>

**Abstract:** The article reviews significant advances in networked signal and information processing, which have enabled in the last 25 years extending decision making and inference, optimization, control, and learning to the increasingly ubiquitous environments of distributed agents. As these interacting agents cooperate, new collective behaviors emerge from local decisions and actions. Moreover, and significantly, theory and applications show that networked agents, through cooperation and sharing, are able to match the performance of cloud or federated solutions, while preserving privacy, increasing resilience, and saving resources.

</p>
</details>

<details><summary><b>Deep nurbs -- admissible neural networks</b>
<a href="https://arxiv.org/abs/2210.13900">arxiv:2210.13900</a>
&#x1F4C8; 0 <br>
<p>Hamed Saidaoui, Luis Espath, Rául Tempone</p></summary>
<p>

**Abstract:** In this study, we propose a new numerical scheme for physics-informed neural networks (PINNs) that enables precise and inexpensive solution for partial differential equations (PDEs) in case of arbitrary geometries while strictly enforcing Dirichlet boundary conditions. The proposed approach combines admissible NURBS parametrizations required to define the physical domain and the Dirichlet boundary conditions with a PINN solver. The fundamental boundary conditions are automatically satisfied in this novel Deep NURBS framework. We verified our new approach using two-dimensional elliptic PDEs when considering arbitrary geometries, including non-Lipschitz domains. Compared to the classical PINN solver, the Deep NURBS estimator has a remarkably high convergence rate for all the studied problems. Moreover, a desirable accuracy was realized for most of the studied PDEs using only one hidden layer of neural networks. This novel approach is considered to pave the way for more effective solutions for high-dimensional problems by allowing for more realistic physics-informed statistical learning to solve PDE-based variational problems.

</p>
</details>


{% endraw %}
Prev: [2022.10.24]({{ '/2022/10/24/2022.10.24.html' | relative_url }})  Next: [2022.10.26]({{ '/2022/10/26/2022.10.26.html' | relative_url }})