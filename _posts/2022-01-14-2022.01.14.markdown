Prev: [2022.01.13]({{ '/2022/01/13/2022.01.13.html' | relative_url }})  Next: [2022.01.15]({{ '/2022/01/15/2022.01.15.html' | relative_url }})
{% raw %}
## Summary for 2022-01-14, created on 2022-01-24


<details><summary><b>CommonsenseQA 2.0: Exposing the Limits of AI through Gamification</b>
<a href="https://arxiv.org/abs/2201.05320">arxiv:2201.05320</a>
&#x1F4C8; 1040 <br>
<p>Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, Jonathan Berant</p></summary>
<p>

**Abstract:** Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction. The goal of players in the game is to compose questions that mislead a rival AI while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself. Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3 (52.9%) in a few-shot inference setup. Both score well below human performance which is at 94.1%.

</p>
</details>

<details><summary><b>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</b>
<a href="https://arxiv.org/abs/2201.05596">arxiv:2201.05596</a>
&#x1F4C8; 146 <br>
<p>Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He</p></summary>
<p>

**Abstract:** As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible.

</p>
</details>

<details><summary><b>When less is more: Simplifying inputs aids neural network understanding</b>
<a href="https://arxiv.org/abs/2201.05610">arxiv:2201.05610</a>
&#x1F4C8; 106 <br>
<p>Robin Tibor Schirrmeister, Rosanne Liu, Sara Hooker, Tonio Ball</p></summary>
<p>

**Abstract:** How do neural network image classifiers respond to simpler and simpler inputs? And what do such responses reveal about the learning process? To answer these questions, we need a clear measure of input simplicity (or inversely, complexity), an optimization objective that correlates with simplification, and a framework to incorporate such objective into training and inference. Lastly we need a variety of testbeds to experiment and evaluate the impact of such simplification on learning. In this work, we measure simplicity with the encoding bit size given by a pretrained generative model, and minimize the bit size to simplify inputs in training and inference. We investigate the effect of such simplification in several scenarios: conventional training, dataset condensation and post-hoc explanations. In all settings, inputs are simplified along with the original classification task, and we investigate the trade-off between input simplicity and task performance. For images with injected distractors, such simplification naturally removes superfluous information. For dataset condensation, we find that inputs can be simplified with almost no accuracy degradation. When used in post-hoc explanation, our learning-based simplification approach offers a valuable new tool to explore the basis of network decisions.

</p>
</details>

<details><summary><b>Smart Magnetic Microrobots Learn to Swim with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2201.05599">arxiv:2201.05599</a>
&#x1F4C8; 65 <br>
<p>Michael R. Behrens, Warren C. Ruder</p></summary>
<p>

**Abstract:** Swimming microrobots are increasingly developed with complex materials and dynamic shapes and are expected to operate in complex environments in which the system dynamics are difficult to model and positional control of the microrobot is not straightforward to achieve. Deep reinforcement learning is a promising method of autonomously developing robust controllers for creating smart microrobots, which can adapt their behavior to operate in uncharacterized environments without the need to model the system dynamics. Here, we report the development of a smart helical magnetic hydrogel microrobot that used the soft actor critic reinforcement learning algorithm to autonomously derive a control policy which allowed the microrobot to swim through an uncharacterized biomimetic fluidic environment under control of a time varying magnetic field generated from a three-axis array of electromagnets. The reinforcement learning agent learned successful control policies with fewer than 100,000 training steps, demonstrating sample efficiency for fast learning. We also demonstrate that we can fine tune the control policies learned by the reinforcement learning agent by fitting mathematical functions to the learned policy's action distribution via regression. Deep reinforcement learning applied to microrobot control is likely to significantly expand the capabilities of the next generation of microrobots.

</p>
</details>

<details><summary><b>CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks</b>
<a href="https://arxiv.org/abs/2201.05729">arxiv:2201.05729</a>
&#x1F4C8; 18 <br>
<p>Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Jianwei Yang, Xiyang Dai, Bin Xiao, Haoxuan You, Shih-Fu Chang, Lu Yuan</p></summary>
<p>

**Abstract:** Contrastive language-image pretraining (CLIP) links vision and language modalities into a unified embedding space, yielding the tremendous potential for vision-language (VL) tasks. While early concurrent works have begun to study this potential on a subset of tasks, important questions remain: 1) What is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches without impacting inference or pretraining complexity? In this work, we seek to answer these questions through two key contributions. First, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data availability constraints and conditions of domain shift. Second, we propose an approach, named CLIP Targeted Distillation (CLIP-TD), to intelligently distill knowledge from CLIP into existing architectures using a dynamically weighted objective applied to adaptively selected tokens per instance. Experiments demonstrate that our proposed CLIP-TD leads to exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to 71.3%) conditions of VCR, while simultaneously improving performance under standard fully-supervised conditions (up to 2%), achieving state-of-art performance on VCR compared to other single models that are pretrained with image-text data only. On SNLI-VE, CLIP-TD produces significant gains in low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works utilizing CLIP for finetuning, as well as baseline naive distillation approaches. Code will be made available.

</p>
</details>

<details><summary><b>Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings</b>
<a href="https://arxiv.org/abs/2201.05575">arxiv:2201.05575</a>
&#x1F4C8; 7 <br>
<p>Ningyu Zhang, Xin Xie, Xiang Chen, Shumin Deng, Chuanqi Tan, Fei Huang, Xu Cheng, Huajun Chen</p></summary>
<p>

**Abstract:** Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory.

</p>
</details>

<details><summary><b>De Rham compatible Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2201.05395">arxiv:2201.05395</a>
&#x1F4C8; 7 <br>
<p>Marcello Longo, Joost A. A. Opschoor, Nico Disch, Christoph Schwab, Jakob Zech</p></summary>
<p>

**Abstract:** We construct several classes of neural networks with ReLU and BiSU (Binary Step Unit) activations, which exactly emulate the lowest order Finite Element (FE) spaces on regular, simplicial partitions of polygonal and polyhedral domains $Ω\subset \mathbb{R}^d$, $d=2,3$. For continuous, piecewise linear (CPwL) functions, our constructions generalize previous results in that arbitrary, regular simplicial partitions of $Ω$ are admitted, also in arbitrary dimension $d\geq 2$.
  Vector-valued elements emulated include the classical Raviart-Thomas and the first family of Nédélec edge elements on triangles and tetrahedra. Neural Networks emulating these FE spaces are required in the correct approximation of boundary value problems of electromagnetism in nonconvex polyhedra $Ω\subset \mathbb{R}^3$, thereby constituting an essential ingredient in the application of e.g. the methodology of ``physics-informed NNs'' or ``deep Ritz methods'' to electromagnetic field simulation via deep learning techniques. They satisfy exact (De Rham) sequence properties, and also spawn discrete boundary complexes on $\partialΩ$ which satisfy exact sequence properties for the surface divergence and curl operators $\mathrm{div}_Γ$ and $\mathrm{curl}_Γ$, respectively, thereby enabling ``neural boundary elements'' for computational electromagnetism.
  We indicate generalizations of our constructions to higher-order compatible spaces and other, non-compatible classes of discretizations in particular the Crouzeix-Raviart elements and Hybridized, Higher Order (HHO) methods.

</p>
</details>

<details><summary><b>Zero-Shot Machine Unlearning</b>
<a href="https://arxiv.org/abs/2201.05629">arxiv:2201.05629</a>
&#x1F4C8; 6 <br>
<p>Vikram S Chundawat, Ayush K Tarun, Murari Mandal, Mohan Kankanhalli</p></summary>
<p>

**Abstract:** With the introduction of new privacy regulations, machine unlearning is becoming an emerging research problem due to an increasing need for regulatory compliance required for machine learning (ML) applications. Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. This necessitates deletion of data not only from storage archives but also from ML model. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch minus the deleted data. The few existing studies use the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. We also introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code will be made publicly available.

</p>
</details>

<details><summary><b>Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next</b>
<a href="https://arxiv.org/abs/2201.05624">arxiv:2201.05624</a>
&#x1F4C8; 6 <br>
<p>Salvatore Cuomo, Vincenzo Schiano di Cola, Fabio Giampaolo, Gianluigi Rozza, Maizar Raissi, Francesco Piccialli</p></summary>
<p>

**Abstract:** Physic-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, and integral-differential equations. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages, the review also attempts to incorporate publications on a larger variety of issues, including physics-constrained neural networks (PCNN), where the initial or boundary conditions are directly embedded in the NN structure rather than in the loss functions. The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.

</p>
</details>

<details><summary><b>A Novel Skeleton-Based Human Activity Discovery Technique Using Particle Swarm Optimization with Gaussian Mutation</b>
<a href="https://arxiv.org/abs/2201.05314">arxiv:2201.05314</a>
&#x1F4C8; 6 <br>
<p>Parham Hadikhani, Daphne Teck Ching Lai, Wee-Hong Ong</p></summary>
<p>

**Abstract:** Human activity discovery aims to distinguish the activities performed by humans, without any prior information of what defines each activity. Most methods presented in human activity recognition are supervised, where there are labeled inputs to train the system. In reality, it is difficult to label data because of its huge volume and the variety of activities performed by humans. In this paper, a novel unsupervised approach is proposed to perform human activity discovery in 3D skeleton sequences. First, important frames are selected based on kinetic energy. Next, the displacement of joints, set of statistical, angles, and orientation features are extracted to represent the activities information. Since not all extracted features have useful information, the dimension of features is reduced using PCA. Most human activity discovery proposed are not fully unsupervised. They use pre-segmented videos before categorizing activities. To deal with this, we used the fragmented sliding time window method to segment the time series of activities with some overlapping. Then, activities are discovered by a novel hybrid particle swarm optimization with a Gaussian mutation algorithm to avoid getting stuck in the local optimum. Finally, k-means is applied to the outcome centroids to overcome the slow rate of PSO. Experiments on three datasets have been presented and the results show the proposed method has superior performance in discovering activities in all evaluation parameters compared to the other state-of-the-art methods and has increased accuracy of at least 4 % on average. The code is available here: https://github.com/parhamhadikhani/Human-Activity-Discovery-HPGMK

</p>
</details>

<details><summary><b>Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions</b>
<a href="https://arxiv.org/abs/2201.05666">arxiv:2201.05666</a>
&#x1F4C8; 5 <br>
<p>Ignavier Ng, Yujia Zheng, Jiji Zhang, Kun Zhang</p></summary>
<p>

**Abstract:** Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-defined score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the super-structure. Numerical experiments validate the efficacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy.

</p>
</details>

<details><summary><b>Tools and Practices for Responsible AI Engineering</b>
<a href="https://arxiv.org/abs/2201.05647">arxiv:2201.05647</a>
&#x1F4C8; 5 <br>
<p>Ryan Soklaski, Justin Goodwin, Olivia Brown, Michael Yee, Jason Matterer</p></summary>
<p>

**Abstract:** Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.

</p>
</details>

<details><summary><b>HYLDA: End-to-end Hybrid Learning Domain Adaptation for LiDAR Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2201.05585">arxiv:2201.05585</a>
&#x1F4C8; 5 <br>
<p>Eduardo R. Corral-Soto, Mrigank Rochan, Yannis Y. He, Shubhra Aich, Yang Liu, Liu Bingbing</p></summary>
<p>

**Abstract:** In this paper we address the problem of training a LiDAR semantic segmentation network using a fully-labeled source dataset and a target dataset that only has a small number of labels. To this end, we develop a novel image-to-image translation engine, and couple it with a LiDAR semantic segmentation network, resulting in an integrated domain adaptation architecture we call HYLDA. To train the system end-to-end, we adopt a diverse set of learning paradigms, including 1) self-supervision on a simple auxiliary reconstruction task, 2) semi-supervised training using a few available labeled target domain frames, and 3) unsupervised training on the fake translated images generated by the image-to-image translation stage, together with the labeled frames from the source domain. In the latter case, the semantic segmentation network participates in the updating of the image-to-image translation engine. We demonstrate experimentally that HYLDA effectively addresses the challenging problem of improving generalization on validation data from the target domain when only a few target labeled frames are available for training. We perform an extensive evaluation where we compare HYLDA against strong baseline methods using two publicly available LiDAR semantic segmentation datasets.

</p>
</details>

<details><summary><b>Investigation of Data Augmentation Techniques for Disordered Speech Recognition</b>
<a href="https://arxiv.org/abs/2201.05562">arxiv:2201.05562</a>
&#x1F4C8; 5 <br>
<p>Mengzhe Geng, Xurong Xie, Shansong Liu, Jianwei Yu, Shoukang Hu, Xunying Liu, Helen Meng</p></summary>
<p>

**Abstract:** Disordered speech recognition is a highly challenging task. The underlying neuro-motor conditions of people with speech disorders, often compounded with co-occurring physical disabilities, lead to the difficulty in collecting large quantities of speech required for system development. This paper investigates a set of data augmentation techniques for disordered speech recognition, including vocal tract length perturbation (VTLP), tempo perturbation and speed perturbation. Both normal and disordered speech were exploited in the augmentation process. Variability among impaired speakers in both the original and augmented data was modeled using learning hidden unit contributions (LHUC) based speaker adaptive training. The final speaker adapted system constructed using the UASpeech corpus and the best augmentation approach based on speed perturbation produced up to 2.92% absolute (9.3% relative) word error rate (WER) reduction over the baseline system without data augmentation, and gave an overall WER of 26.37% on the test set containing 16 dysarthric speakers.

</p>
</details>

<details><summary><b>Probabilistic Mass Mapping with Neural Score Estimation</b>
<a href="https://arxiv.org/abs/2201.05561">arxiv:2201.05561</a>
&#x1F4C8; 5 <br>
<p>Benjamin Remy, Francois Lanusse, Niall Jeffrey, Jia Liu, Jean-Luc Starck, Ken Osato, Tim Schrabback</p></summary>
<p>

**Abstract:** Weak lensing mass-mapping is a useful tool to access the full distribution of dark matter on the sky, but because of intrinsic galaxy ellipticies and finite fields/missing data, the recovery of dark matter maps constitutes a challenging ill-posed inverse problem. We introduce a novel methodology allowing for efficient sampling of the high-dimensional Bayesian posterior of the weak lensing mass-mapping problem, and relying on simulations for defining a fully non-Gaussian prior. We aim to demonstrate the accuracy of the method on simulations, and then proceed to applying it to the mass reconstruction of the HST/ACS COSMOS field. The proposed methodology combines elements of Bayesian statistics, analytic theory, and a recent class of Deep Generative Models based on Neural Score Matching. This approach allows us to do the following: 1) Make full use of analytic cosmological theory to constrain the 2pt statistics of the solution. 2) Learn from cosmological simulations any differences between this analytic prior and full simulations. 3) Obtain samples from the full Bayesian posterior of the problem for robust Uncertainty Quantification. We demonstrate the method on the $κ$TNG simulations and find that the posterior mean significantly outperfoms previous methods (Kaiser-Squires, Wiener filter, Sparsity priors) both on root-mean-square error and in terms of the Pearson correlation. We further illustrate the interpretability of the recovered posterior by establishing a close correlation between posterior convergence values and SNR of clusters artificially introduced into a field. Finally, we apply the method to the reconstruction of the HST/ACS COSMOS field and yield the highest quality convergence map of this field to date.

</p>
</details>

<details><summary><b>Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition</b>
<a href="https://arxiv.org/abs/2201.05554">arxiv:2201.05554</a>
&#x1F4C8; 5 <br>
<p>Mengzhe Geng, Shansong Liu, Jianwei Yu, Xurong Xie, Shoukang Hu, Zi Ye, Zengrui Jin, Xunying Liu, Helen Meng</p></summary>
<p>

**Abstract:** Automatic recognition of disordered speech remains a highly challenging task to date. Sources of variability commonly found in normal speech including accent, age or gender, when further compounded with the underlying causes of speech impairment and varying severity levels, create large diversity among speakers. To this end, speaker adaptation techniques play a vital role in current speech recognition systems. Motivated by the spectro-temporal level differences between disordered and normal speech that systematically manifest in articulatory imprecision, decreased volume and clarity, slower speaking rates and increased dysfluencies, novel spectro-temporal subspace basis embedding deep features derived by SVD decomposition of speech spectrum are proposed to facilitate both accurate speech intelligibility assessment and auxiliary feature based speaker adaptation of state-of-the-art hybrid DNN and end-to-end disordered speech recognition systems. Experiments conducted on the UASpeech corpus suggest the proposed spectro-temporal deep feature adapted systems consistently outperformed baseline i-Vector adaptation by up to 2.63% absolute (8.6% relative) reduction in word error rate (WER) with or without data augmentation. Learning hidden unit contribution (LHUC) based speaker adaptation was further applied. The final speaker adapted system using the proposed spectral basis embedding features gave an overall WER of 25.6% on the UASpeech test set of 16 dysarthric speakers

</p>
</details>

<details><summary><b>Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks</b>
<a href="https://arxiv.org/abs/2201.05489">arxiv:2201.05489</a>
&#x1F4C8; 5 <br>
<p>Yuqi Wang, Xu-Yao Zhang, Cheng-Lin Liu, Zhaoxiang Zhang</p></summary>
<p>

**Abstract:** Representation is a core issue in artificial intelligence. Humans use discrete language to communicate and learn from each other, while machines use continuous features (like vector, matrix, or tensor in deep neural networks) to represent cognitive patterns. Discrete symbols are low-dimensional, decoupled, and have strong reasoning ability, while continuous features are high-dimensional, coupled, and have incredible abstracting capabilities. In recent years, deep learning has developed the idea of continuous representation to the extreme, using millions of parameters to achieve high accuracies. Although this is reasonable from the statistical perspective, it has other major problems like lacking interpretability, poor generalization, and is easy to be attacked. Since both paradigms have strengths and weaknesses, a better choice is to seek reconciliation. In this paper, we make an initial attempt towards this direction. Specifically, we propose to combine symbolism and connectionism principles by using neural networks to derive a discrete representation. This process is highly similar to human language, which is a natural combination of discrete symbols and neural systems, where the brain processes continuous signals and represents intelligence via discrete language. To mimic this functionality, we denote our approach as machine language. By designing an interactive environment and task, we demonstrated that machines could generate a spontaneous, flexible, and semantic language through cooperation. Moreover, through experiments we show that discrete language representation has several advantages compared with continuous feature representation, from the aspects of interpretability, generalization, and robustness.

</p>
</details>

<details><summary><b>Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval</b>
<a href="https://arxiv.org/abs/2201.05409">arxiv:2201.05409</a>
&#x1F4C8; 5 <br>
<p>Shitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang, Chaozhuo Li, Yingxia Shao, Defu Lian, Xing Xie, Hao Sun, Denvy Deng, Liangjie Zhang, Qi Zhang</p></summary>
<p>

**Abstract:** Ad-hoc search calls for the selection of appropriate answers from a massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a promising solution, where deep learning based document representation and ANN search techniques are allied to handle this task. However, a major challenge is that the ANN index can be too large to fit into memory, given the considerable size of answer corpus. In this work, we tackle this problem with Bi-Granular Document Representation, where the lightweight sparse embeddings are indexed and standby in memory for coarse-grained candidate search, and the heavyweight dense embeddings are hosted in disk for fine-grained post verification. For the best of retrieval accuracy, a Progressive Optimization framework is designed. The sparse embeddings are learned ahead for high-quality search of candidates. Conditioned on the candidate distribution induced by the sparse embeddings, the dense embeddings are continuously learned to optimize the discrimination of ground-truth from the shortlisted candidates. Besides, two techniques: the contrastive quantization and the locality-centric sampling are introduced for the learning of sparse and dense embeddings, which substantially contribute to their performances. Thanks to the above features, our method effectively handles massive-scale EBR with strong advantages in accuracy: with up to +4.3% recall gain on million-scale corpus, and up to +17.5% recall gain on billion-scale corpus. Besides, Our method is applied to a major sponsored search platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and CTR (+0.49%).

</p>
</details>

<details><summary><b>A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI</b>
<a href="https://arxiv.org/abs/2201.05373">arxiv:2201.05373</a>
&#x1F4C8; 5 <br>
<p>Mirza Mumtaz Zahoor, Shahzad Ahmad Qureshi, Saddam Hussain Khan, Asifullah Khan</p></summary>
<p>

**Abstract:** Brain tumors analysis is important in timely diagnosis and effective treatment to cure patients. Tumor analysis is challenging because of tumor morphology like size, location, texture, and heteromorphic appearance in the medical images. In this regard, a novel two-phase deep learning-based framework is proposed to detect and categorize brain tumors in magnetic resonance images (MRIs). In the first phase, a novel deep boosted features and ensemble classifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy individuals effectively. The deep boosted feature space is achieved through the customized and well-performing deep convolutional neural networks (CNNs), and consequently, fed into the ensemble of machine learning (ML) classifiers. While in the second phase, a new hybrid features fusion-based brain tumor classification approach is proposed, comprised of dynamic-static feature and ML classifier to categorize different tumor types. The dynamic features are extracted from the proposed BRAIN-RENet CNN, which carefully learns heteromorphic and inconsistent behavior of various tumors, while the static features are extracted using HOG. The effectiveness of the proposed two-phase brain tumor analysis framework is validated on two standard benchmark datasets; collected from Kaggle and Figshare containing different types of tumor, including glioma, meningioma, pituitary, and normal images. Experimental results proved that the proposed DBF-EC detection scheme outperforms and achieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score (0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme, the joint employment of the deep features fusion of proposed BRAIN-RENet and HOG features improves performance significantly in terms of recall (0.9913), precision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse datasets.

</p>
</details>

<details><summary><b>Polarity and Subjectivity Detection with Multitask Learning and BERT Embedding</b>
<a href="https://arxiv.org/abs/2201.05363">arxiv:2201.05363</a>
&#x1F4C8; 5 <br>
<p>Ranjan Satapathy, Shweta Pardeshi, Erik Cambria</p></summary>
<p>

**Abstract:** Multitask learning often helps improve the performance of related tasks as these often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multitask learning framework that jointly performs polarity and subjective detection. We propose an attention-based multitask model for predicting polarity and subjectivity. The input sentences are transformed into vectors using pre-trained BERT and Glove embeddings, and the results depict that BERT embedding based model works better than the Glove based model. We compare our approach with state-of-the-art models in both subjective and polarity classification single-task and multitask frameworks. The proposed approach reports baseline performances for both polarity detection and subjectivity detection.

</p>
</details>

<details><summary><b>Unsupervised Temporal Video Grounding with Deep Semantic Clustering</b>
<a href="https://arxiv.org/abs/2201.05307">arxiv:2201.05307</a>
&#x1F4C8; 5 <br>
<p>Daizong Liu, Xiaoye Qu, Yinzhen Wang, Xing Di, Kai Zou, Yu Cheng, Zichuan Xu, Pan Zhou</p></summary>
<p>

**Abstract:** Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive and time-consuming to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches.

</p>
</details>

<details><summary><b>Layerwise Geo-Distributed Computing between Cloud and IoT</b>
<a href="https://arxiv.org/abs/2201.07215">arxiv:2201.07215</a>
&#x1F4C8; 4 <br>
<p>Satoshi Kamo, Yiqiang Sheng</p></summary>
<p>

**Abstract:** In this paper, we propose a novel architecture for a deep learning system, named k-degree layer-wise network, to realize efficient geo-distributed computing between Cloud and Internet of Things (IoT). The geo-distributed computing extends Cloud to the geographical verge of the network in the neighbor of IoT. The basic ideas of the proposal include a k-degree constraint and a layer-wise constraint. The k-degree constraint is defined such that the degree of each vertex on the h-th layer is exactly k(h) to extend the existing deep belief networks and control the communication cost. The layer-wise constraint is defined such that the layer-wise degrees are monotonically decreasing in positive direction to gradually reduce the dimension of data. We prove the k-degree layer-wise network is sparse, while a typical deep neural network is dense. In an evaluation on the M-distributed MNIST database, the proposal is superior to a state-of-the-art model in terms of communication cost and learning time with scalability.

</p>
</details>

<details><summary><b>Block Policy Mirror Descent</b>
<a href="https://arxiv.org/abs/2201.05756">arxiv:2201.05756</a>
&#x1F4C8; 4 <br>
<p>Guanghui Lan, Yan Li, Tuo Zhao</p></summary>
<p>

**Abstract:** In this paper, we present a new class of policy gradient (PG) methods, namely the block policy mirror descent (BPMD) methods for solving a class of regularized reinforcement learning (RL) problems with (strongly) convex regularizers. Compared to the traditional PG methods with batch update rule, which visit and update the policy for every state, BPMD methods have cheap per-iteration computation via a partial update rule that performs the policy update on a sampled state. Despite the nonconvex nature of the problem and a partial update rule, BPMD methods achieve fast linear convergence to the global optimality. We further extend BPMD methods to the stochastic setting, by utilizing stochastic first-order information constructed from samples. We establish $\cO(1/ε)$ (resp. $\cO(1/ε^2)$) sample complexity for the strongly convex (resp. non-strongly convex) regularizers, with different procedures for constructing the stochastic first-order information, where $ε$ denotes the target accuracy. To the best of our knowledge, this is the first time that block coordinate descent methods have been developed and analyzed for policy optimization in reinforcement learning.

</p>
</details>

<details><summary><b>Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model</b>
<a href="https://arxiv.org/abs/2201.05748">arxiv:2201.05748</a>
&#x1F4C8; 4 <br>
<p>YeongHyeon Park</p></summary>
<p>

**Abstract:** Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for training not only the anomaly detection model but also the general generative neural network.

</p>
</details>

<details><summary><b>Deep Optimal Transport on SPD Manifolds for Domain Adaptation</b>
<a href="https://arxiv.org/abs/2201.05745">arxiv:2201.05745</a>
&#x1F4C8; 4 <br>
<p>Ce Ju, Cuntai Guan</p></summary>
<p>

**Abstract:** The domain adaption (DA) problem on symmetric positive definite (SPD) manifolds has raised interest in the machine learning community because of the growing potential for the SPD-matrix representations across many non-stationary applicable scenarios. This paper generalizes the joint distribution adaption (JDA) to align the source and target domains on SPD manifolds and proposes a deep network architecture, Deep Optimal Transport (DOT), using the generalized JDA and the existing deep network architectures on SPD manifolds. The specific architecture in DOT enables it to learn an approximate optimal transport (OT) solution to the DA problems on SPD manifolds. In the experiments, DOT exhibits a 2.32% and 2.92% increase on the average accuracy in two highly non-stationary cross-session scenarios in brain-computer interfaces (BCIs), respectively. The visualizational results of the source and target domains before and after the transformation also demonstrate the validity of DOT.

</p>
</details>

<details><summary><b>Estimating Gaussian Copulas with Missing Data</b>
<a href="https://arxiv.org/abs/2201.05565">arxiv:2201.05565</a>
&#x1F4C8; 4 <br>
<p>Maximilian Kertel, Markus Pauly</p></summary>
<p>

**Abstract:** In this work we present a rigorous application of the Expectation Maximization algorithm to determine the marginal distributions and the dependence structure in a Gaussian copula model with missing data. We further show how to circumvent a priori assumptions on the marginals with semiparametric modelling. The joint distribution learned through this algorithm is considerably closer to the underlying distribution than existing methods.

</p>
</details>

<details><summary><b>Reinforcement Learning in Time-Varying Systems: an Empirical Study</b>
<a href="https://arxiv.org/abs/2201.05560">arxiv:2201.05560</a>
&#x1F4C8; 4 <br>
<p>Pouya Hamadanian, Malte Schwarzkopf, Siddartha Sen, Mohammad Alizadeh</p></summary>
<p>

**Abstract:** Recent research has turned to Reinforcement Learning (RL) to solve challenging decision problems, as an alternative to hand-tuned heuristics. RL can learn good policies without the need for modeling the environment's dynamics. Despite this promise, RL remains an impractical solution for many real-world systems problems. A particularly challenging case occurs when the environment changes over time, i.e. it exhibits non-stationarity. In this work, we characterize the challenges introduced by non-stationarity and develop a framework for addressing them to train RL agents in live systems. Such agents must explore and learn new environments, without hurting the system's performance, and remember them over time. To this end, our framework (1) identifies different environments encountered by the live system, (2) explores and trains a separate expert policy for each environment, and (3) employs safeguards to protect the system's performance. We apply our framework to two systems problems: straggler mitigation and adaptive video streaming, and evaluate it against a variety of alternative approaches using real-world and synthetic data. We show that each component of our framework is necessary to cope with non-stationarity.

</p>
</details>

<details><summary><b>AWSnet: An Auto-weighted Supervision Attention Network for Myocardial Scar and Edema Segmentation in Multi-sequence Cardiac Magnetic Resonance Images</b>
<a href="https://arxiv.org/abs/2201.05344">arxiv:2201.05344</a>
&#x1F4C8; 4 <br>
<p>Kai-Ni Wang, Xin Yang, Juzheng Miao, Lei Li, Jing Yao, Ping Zhou, Wufeng Xue, Guang-Quan Zhou, Xiahai Zhuang, Dong Ni</p></summary>
<p>

**Abstract:** Multi-sequence cardiac magnetic resonance (CMR) provides essential pathology information (scar and edema) to diagnose myocardial infarction. However, automatic pathology segmentation can be challenging due to the difficulty of effectively exploring the underlying information from the multi-sequence CMR data. This paper aims to tackle the scar and edema segmentation from multi-sequence CMR with a novel auto-weighted supervision framework, where the interactions among different supervised layers are explored under a task-specific objective using reinforcement learning. Furthermore, we design a coarse-to-fine framework to boost the small myocardial pathology region segmentation with shape prior knowledge. The coarse segmentation model identifies the left ventricle myocardial structure as a shape prior, while the fine segmentation model integrates a pixel-wise attention strategy with an auto-weighted supervision model to learn and extract salient pathological structures from the multi-sequence CMR data. Extensive experimental results on a publicly available dataset from Myocardial pathology segmentation combining multi-sequence CMR (MyoPS 2020) demonstrate our method can achieve promising performance compared with other state-of-the-art methods. Our method is promising in advancing the myocardial pathology assessment on multi-sequence CMR data. To motivate the community, we have made our code publicly available via https://github.com/soleilssss/AWSnet/tree/master.

</p>
</details>

<details><summary><b>Semi-automated Virtual Unfolded View Generation Method of Stomach from CT Volumes</b>
<a href="https://arxiv.org/abs/2201.05331">arxiv:2201.05331</a>
&#x1F4C8; 4 <br>
<p>Masahiro Oda, Tomoaki Suito, Yuichiro Hayashi, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara, Yoshiki Hirooka, Hidemi Goto, Gen Iinuma, Kazunari Misawa, Shigeru Nawano, Kensaku Mori</p></summary>
<p>

**Abstract:** CT image-based diagnosis of the stomach is developed as a new way of diagnostic method. A virtual unfolded (VU) view is suitable for displaying its wall. In this paper, we propose a semi-automated method for generating VU views of the stomach. Our method requires minimum manual operations. The determination of the unfolding forces and the termination of the unfolding process are automated. The unfolded shape of the stomach is estimated based on its radius. The unfolding forces are determined so that the stomach wall is deformed to the expected shape. The iterative deformation process is terminated if the difference of the shapes between the deformed shape and expected shape is small. Our experiments using 67 CT volumes showed that our proposed method can generate good VU views for 76.1% cases.

</p>
</details>

<details><summary><b>Corrigendum and addendum to: How Populist are Parties? Measuring Degrees of Populism in Party Manifestos Using Supervised Machine Learning</b>
<a href="https://arxiv.org/abs/2201.07972">arxiv:2201.07972</a>
&#x1F4C8; 3 <br>
<p>Jessica Di Cocco, Bernardo Monechi</p></summary>
<p>

**Abstract:** This paper is a corrigendum and addendum to the previously published article: 'How Populist are Parties? Measuring Degrees of Populism in Party Manifestos Using Supervised Machine Learning' (Political Analysis, 1-17. doi:10.1017/pan.2021.29). These corrigendum and addendum were prepared to correct errors in data labelling and show some extra insights not included in the previously published paper. Here, we report these corrections and point to some additional conclusions by focusing on the effects of the label reshuffling per parties and years and presenting new figures wherever appropriate. We show that although the simplified labelling method proposed in the previously-published article can induce biases in the correlations with expert scores, random labelling reduces correlations significantly. We show that this is also true for correlations based on a manually-coded data set. These modifications are based on other evidence and results reported in detail in a future publication.

</p>
</details>

<details><summary><b>Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance</b>
<a href="https://arxiv.org/abs/2201.05739">arxiv:2201.05739</a>
&#x1F4C8; 3 <br>
<p>Justin Sanchez, Christopher Neff, Hamed Tabkhi</p></summary>
<p>

**Abstract:** Action recognition is a key algorithmic part of emerging on-the-edge smart video surveillance and security systems. Skeleton-based action recognition is an attractive approach which, instead of using RGB pixel data, relies on human pose information to classify appropriate actions. However, existing algorithms often assume ideal conditions that are not representative of real-world limitations, such as noisy input, latency requirements, and edge resource constraints.
  To address the limitations of existing approaches, this paper presents Real-World Graph Convolution Networks (RW-GCNs), an architecture-level solution for meeting the domain constraints of Real World Skeleton-based Action Recognition. Inspired by the presence of feedback connections in the human visual cortex, RW-GCNs leverage attentive feedback augmentation on existing near state-of-the-art (SotA) Spatial-Temporal Graph Convolution Networks (ST-GCNs). The ST-GCNs' design choices are derived from information theory-centric principles to address both the spatial and temporal noise typically encountered in end-to-end real-time and on-the-edge smart video systems. Our results demonstrate RW-GCNs' ability to serve these applications by achieving a new SotA accuracy on the NTU-RGB-D-120 dataset at 94.1%, and achieving 32X less latency than baseline ST-GCN applications while still achieving 90.4% accuracy on the Northwestern UCLA dataset in the presence of spatial keypoint noise. RW-GCNs further show system scalability by running on the 10X cost effective NVIDIA Jetson Nano (as opposed to NVIDIA Xavier NX), while still maintaining a respectful range of throughput (15.6 to 5.5 Actions per Second) on the resource constrained device. The code is available here: https://github.com/TeCSAR-UNCC/RW-GCN.

</p>
</details>

<details><summary><b>A unified algorithm framework for mean-variance optimization in discounted Markov decision processes</b>
<a href="https://arxiv.org/abs/2201.05737">arxiv:2201.05737</a>
&#x1F4C8; 3 <br>
<p>Shuai Ma, Xiaoteng Ma, Li Xia</p></summary>
<p>

**Abstract:** This paper studies the risk-averse mean-variance optimization in infinite-horizon discounted Markov decision processes (MDPs). The involved variance metric concerns reward variability during the whole process, and future deviations are discounted to their present values. This discounted mean-variance optimization yields a reward function dependent on a discounted mean, and this dependency renders traditional dynamic programming methods inapplicable since it suppresses a crucial property -- time consistency. To deal with this unorthodox problem, we introduce a pseudo mean to transform the untreatable MDP to a standard one with a redefined reward function in standard form and derive a discounted mean-variance performance difference formula. With the pseudo mean, we propose a unified algorithm framework with a bilevel optimization structure for the discounted mean-variance optimization. The framework unifies a variety of algorithms for several variance-related problems including, but not limited to, risk-averse variance and mean-variance optimizations in discounted and average MDPs. Furthermore, the convergence analyses missing from the literature can be complemented with the proposed framework as well. Taking the value iteration as an example, we develop a discounted mean-variance value iteration algorithm and prove its convergence to a local optimum with the aid of a Bellman local-optimality equation. Finally, we conduct a numerical experiment on portfolio management to validate the proposed algorithm.

</p>
</details>

<details><summary><b>Perspective Transformation Layer</b>
<a href="https://arxiv.org/abs/2201.05706">arxiv:2201.05706</a>
&#x1F4C8; 3 <br>
<p>Nishan Khatri, Agnibh Dasgupta, Yucong Shen, Xin Zhong, Frank Shih</p></summary>
<p>

**Abstract:** Incorporating geometric transformations that reflect the relative position changes between an observer and an object into computer vision and deep learning models has attracted much attention in recent years. However, the existing proposals mainly focus on affine transformations that cannot fully show viewpoint changes. Furthermore, current solutions often apply a neural network module to learn a single transformation matrix, which ignores the possibility for various viewpoints and creates extra to-be-trained module parameters. In this paper, a layer (PT layer) is proposed to learn the perspective transformations that not only model the geometries in affine transformation but also reflect the viewpoint changes. In addition, being able to be directly trained with gradient descent like traditional layers such as convolutional layers, a single proposed PT layer can learn an adjustable number of multiple viewpoints without training extra module parameters. The experiments and evaluations confirm the superiority of the proposed PT layer.

</p>
</details>

<details><summary><b>Diffusion Tensor Estimation with Transformer Neural Networks</b>
<a href="https://arxiv.org/abs/2201.05701">arxiv:2201.05701</a>
&#x1F4C8; 3 <br>
<p>Davood Karimi, Ali Gholipour</p></summary>
<p>

**Abstract:** Diffusion tensor imaging (DTI) is the most widely used tool for studying brain white matter development and degeneration. However, standard DTI estimation methods depend on a large number of high-quality measurements. This would require long scan times and can be particularly difficult to achieve with certain patient populations such as neonates. Here, we propose a method that can accurately estimate the diffusion tensor from only six diffusion-weighted measurements. Our method achieves this by learning to exploit the relationships between the diffusion signals and tensors in neighboring voxels. Our model is based on transformer networks, which represent the state of the art in modeling the relationship between signals in a sequence. In particular, our model consists of two such networks. The first network estimates the diffusion tensor based on the diffusion signals in a neighborhood of voxels. The second network provides more accurate tensor estimations by learning the relationships between the diffusion signals as well as the tensors estimated by the first network in neighboring voxels. Our experiments with three datasets show that our proposed method achieves highly accurate estimations of the diffusion tensor and is significantly superior to three competing methods. Estimations produced by our method with six measurements are comparable with those of standard estimation methods with 30-88 measurements. Hence, our method promises shorter scan times and more reliable assessment of brain white matter, particularly in non-cooperative patients such as neonates and infants.

</p>
</details>

<details><summary><b>Cost-Effective Training in Low-Resource Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2201.05700">arxiv:2201.05700</a>
&#x1F4C8; 3 <br>
<p>Sai Koneru, Danni Liu, Jan Niehues</p></summary>
<p>

**Abstract:** While Active Learning (AL) techniques are explored in Neural Machine Translation (NMT), only a few works focus on tackling low annotation budgets where a limited number of sentences can get translated. Such situations are especially challenging and can occur for endangered languages with few human annotators or having cost constraints to label large amounts of data. Although AL is shown to be helpful with large budgets, it is not enough to build high-quality translation systems in these low-resource conditions. In this work, we propose a cost-effective training procedure to increase the performance of NMT models utilizing a small number of annotated sentences and dictionary entries. Our method leverages monolingual data with self-supervised objectives and a small-scale, inexpensive dictionary for additional supervision to initialize the NMT model before applying AL. We show that improving the model using a combination of these knowledge sources is essential to exploit AL strategies and increase gains in low-resource conditions. We also present a novel AL strategy inspired by domain adaptation for NMT and show that it is effective for low budgets. We propose a new hybrid data-driven approach, which samples sentences that are diverse from the labelled data and also most similar to unlabelled data. Finally, we show that initializing the NMT model and further using our AL strategy can achieve gains of up to $13$ BLEU compared to conventional AL methods.

</p>
</details>

<details><summary><b>CLUE: Contextualised Unified Explainable Learning of User Engagement in Video Lectures</b>
<a href="https://arxiv.org/abs/2201.05651">arxiv:2201.05651</a>
&#x1F4C8; 3 <br>
<p>Sujit Roy, Gnaneswara Rao Gorle, Vishal Gaur, Haider Raza, Shoaib Jameel</p></summary>
<p>

**Abstract:** Predicting contextualised engagement in videos is a long-standing problem that has been popularly attempted by exploiting the number of views or the associated likes using different computational methods. The recent decade has seen a boom in online learning resources, and during the pandemic, there has been an exponential rise of online teaching videos without much quality control. The quality of the content could be improved if the creators could get constructive feedback on their content. Employing an army of domain expert volunteers to provide feedback on the videos might not scale. As a result, there has been a steep rise in developing computational methods to predict a user engagement score that is indicative of some form of possible user engagement, i.e., to what level a user would tend to engage with the content. A drawback in current methods is that they model various features separately, in a cascaded approach, that is prone to error propagation. Besides, most of them do not provide crucial explanations on how the creator could improve their content. In this paper, we have proposed a new unified model, CLUE for the educational domain, which learns from the features extracted from freely available public online teaching videos and provides explainable feedback on the video along with a user engagement score. Given the complexity of the task, our unified framework employs different pre-trained models working together as an ensemble of classifiers. Our model exploits various multi-modal features to model the complexity of language, context agnostic information, textual emotion of the delivered content, animation, speaker's pitch and speech emotions. Under a transfer learning setup, the overall model, in the unified space, is fine-tuned for downstream applications.

</p>
</details>

<details><summary><b>Disentanglement enables cross-domain Hippocampus Segmentation</b>
<a href="https://arxiv.org/abs/2201.05650">arxiv:2201.05650</a>
&#x1F4C8; 3 <br>
<p>John Kalkhof, Camila González, Anirban Mukhopadhyay</p></summary>
<p>

**Abstract:** Limited amount of labelled training data are a common problem in medical imaging. This makes it difficult to train a well-generalised model and therefore often leads to failure in unknown domains. Hippocampus segmentation from magnetic resonance imaging (MRI) scans is critical for the diagnosis and treatment of neuropsychatric disorders. Domain differences in contrast or shape can significantly affect segmentation. We address this issue by disentangling a T1-weighted MRI image into its content and domain. This separation enables us to perform a domain transfer and thus convert data from new sources into the training domain. This step thus simplifies the segmentation problem, resulting in higher quality segmentations. We achieve the disentanglement with the proposed novel methodology 'Content Domain Disentanglement GAN', and we propose to retrain the UNet on the transformed outputs to deal with GAN-specific artefacts. With these changes, we are able to improve performance on unseen domains by 6-13% and outperform state-of-the-art domain transfer methods.

</p>
</details>

<details><summary><b>OrchestRAN: Network Automation through Orchestrated Intelligence in the Open RAN</b>
<a href="https://arxiv.org/abs/2201.05632">arxiv:2201.05632</a>
&#x1F4C8; 3 <br>
<p>Salvatore D'Oro, Leonardo Bonati, Michele Polese, Tommaso Melodia</p></summary>
<p>

**Abstract:** The next generation of cellular networks will be characterized by softwarized, open, and disaggregated architectures exposing analytics and control knobs to enable network intelligence. How to realize this vision, however, is largely an open problem. In this paper, we take a decisive step forward by presenting and prototyping OrchestRAN, a novel orchestration framework that embraces and builds upon the Open RAN paradigm to provide a practical solution to these challenges. OrchestRAN has been designed to execute in the non-real-time RAN Intelligent Controller (RIC) and allows Network Operators (NOs) to specify high-level control/inference objectives (i.e., adapt scheduling, and forecast capacity in near-real-time for a set of base stations in Downtown New York). OrchestRAN automatically computes the optimal set of data-driven algorithms and their execution location to achieve intents specified by the NOs while meeting the desired timing requirements. We show that the problem of orchestrating intelligence in Open RAN is NP-hard, and design low-complexity solutions to support real-world applications. We prototype OrchestRAN and test it at scale on Colosseum. Our experimental results on a network with 7 base stations and 42 users demonstrate that OrchestRAN is able to instantiate data-driven services on demand with minimal control overhead and latency.

</p>
</details>

<details><summary><b>Reusing Auto-Schedules for Efficient DNN Compilation</b>
<a href="https://arxiv.org/abs/2201.05587">arxiv:2201.05587</a>
&#x1F4C8; 3 <br>
<p>Perry Gibson, José Cano</p></summary>
<p>

**Abstract:** Auto-scheduling is a process where a search algorithm automatically explores candidate schedules (program transformations) for a given tensor program on a given hardware platform to improve its performance. However this can be a very time consuming process, depending on the complexity of the tensor program, and capacity of the target device, with often many thousands of program variants being explored. To address this, in this paper we introduce and demonstrate the idea of \emph{tuning-reuse}, a novel approach to identify and re-use auto-schedules between tensor programs. We demonstrate this concept using Deep Neural Networks (DNNs), taking sets of auto-schedules from pre-tuned DNNs, and using them to reduce the inference time of a new DNN. Given a set of pre-tuned schedules, tuning-reuse provides its maximum speedup in less time than auto-scheduling using the state-of-the-art Ansor auto-scheduler. On a set of widely used DNN models, we apply tuning-reuse and achieve maximum speedups between $1.16\times$ and $4.76\times$, while outperforming Ansor when given limited tuning time.

</p>
</details>

<details><summary><b>Synthesising Electronic Health Records: Cystic Fibrosis Patient Group</b>
<a href="https://arxiv.org/abs/2201.05400">arxiv:2201.05400</a>
&#x1F4C8; 3 <br>
<p>Emily Muller, Xu Zheng, Jer Hayes</p></summary>
<p>

**Abstract:** Class imbalance can often degrade predictive performance of supervised learning algorithms. Balanced classes can be obtained by oversampling exact copies, with noise, or interpolation between nearest neighbours (as in traditional SMOTE methods). Oversampling tabular data using augmentation, as is typical in computer vision tasks, can be achieved with deep generative models. Deep generative models are effective data synthesisers due to their ability to capture complex underlying distributions. Synthetic data in healthcare can enhance interoperability between healthcare providers by ensuring patient privacy. Equipped with large synthetic datasets which do well to represent small patient groups, machine learning in healthcare can address the current challenges of bias and generalisability. This paper evaluates synthetic data generators ability to synthesise patient electronic health records. We test the utility of synthetic data for patient outcome classification, observing increased predictive performance when augmenting imbalanced datasets with synthetic data.

</p>
</details>

<details><summary><b>Reinforcement Learning to Solve NP-hard Problems: an Application to the CVRP</b>
<a href="https://arxiv.org/abs/2201.05393">arxiv:2201.05393</a>
&#x1F4C8; 3 <br>
<p>Leo Ardon</p></summary>
<p>

**Abstract:** In this paper, we evaluate the use of Reinforcement Learning (RL) to solve a classic combinatorial optimization problem: the Capacitated Vehicle Routing Problem (CVRP). We formalize this problem in the RL framework and compare two of the most promising RL approaches with traditional solving techniques on a set of benchmark instances. We measure the different approaches with the quality of the solution returned and the time required to return it. We found that despite not returning the best solution, the RL approach has many advantages over traditional solvers. First, the versatility of the framework allows the resolution of more complex combinatorial problems. Moreover, instead of trying to solve a specific instance of the problem, the RL algorithm learns the skills required to solve the problem. The trained policy can then quasi instantly provide a solution to an unseen problem without having to solve it from scratch. Finally, the use of trained models makes the RL solver by far the fastest, and therefore make this approach more suited for commercial use where the user experience is paramount. Techniques like Knowledge Transfer can also be used to improve the training efficiency of the algorithm and help solve bigger and more complex problems.

</p>
</details>

<details><summary><b>Prediction of Drug-Induced TdP Risks Using Machine Learning and Rabbit Ventricular Wedge Assay</b>
<a href="https://arxiv.org/abs/2201.05669">arxiv:2201.05669</a>
&#x1F4C8; 2 <br>
<p>Nan Miles Xi, Dalong Patrick Huang</p></summary>
<p>

**Abstract:** The evaluation of drug-induced Torsades de pointes (TdP) risks is crucial in drug safety assessment. In this study, we discuss machine learning approaches in the prediction of drug-induced TdP risks using preclinical data. Specifically, the random forest model was trained on the dataset generated by the rabbit ventricular wedge assay. The model prediction performance was measured on 28 drugs from the Comprehensive In Vitro Proarrhythmia Assay initiative. Leave-one-drug-out cross-validation provided an unbiased estimation of model performance. Stratified bootstrap revealed the uncertainty in the asymptotic model prediction. Our study validated the utility of machine learning approaches in predicting drug-induced TdP risks from preclinical data. Our methods can be extended to other preclinical protocols and serve as a supplementary evaluation in drug safety assessment.

</p>
</details>

<details><summary><b>Imputing Missing Observations with Time Sliced Synthetic Minority Oversampling Technique</b>
<a href="https://arxiv.org/abs/2201.05634">arxiv:2201.05634</a>
&#x1F4C8; 2 <br>
<p>Andrew Baumgartner, Sevda Molani, Qi Wei, Jennifer Hadlock</p></summary>
<p>

**Abstract:** We present a simple yet novel time series imputation technique with the goal of constructing an irregular time series that is uniform across every sample in a data set. Specifically, we fix a grid defined by the midpoints of non-overlapping bins (dubbed "slices") of observation times and ensure that each sample has values for all of the features at that given time. This allows one to both impute fully missing observations to allow uniform time series classification across the entire data and, in special cases, to impute individually missing features. To do so, we slightly generalize the well-known class imbalance algorithm SMOTE \cite{smote} to allow component wise nearest neighbor interpolation that preserves correlations when there are no missing features. We visualize the method in the simplified setting of 2-dimensional uncoupled harmonic oscillators. Next, we use tSMOTE to train an Encoder/Decoder long-short term memory (LSTM) model with Logistic Regression for predicting and classifying distinct trajectories of different 2D oscillators. After illustrating the the utility of tSMOTE in this context, we use the same architecture to train a clinical model for COVID-19 disease severity on an imputed data set. Our experiments show an improvement over standard mean and median imputation techniques by allowing a wider class of patient trajectories to be recognized by the model, as well as improvement over aggregated classification models.

</p>
</details>

<details><summary><b>The Dark Side of the Language: Pre-trained Transformers in the DarkNet</b>
<a href="https://arxiv.org/abs/2201.05613">arxiv:2201.05613</a>
&#x1F4C8; 2 <br>
<p>Leonardo Ranaldi, Aria Nourbakhsh, Arianna Patrizi, Elena Sofia Ruzzetti, Dario Onorati, Francesca Fallucchi Fabio Massimo Zanzotto</p></summary>
<p>

**Abstract:** Pre-trained Transformers are challenging human performances in many natural language processing tasks. The gigantic datasets used for pre-training seem to be the key for their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models perform on truly novel and unexplored data, provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks largely outperform pre-trained Transformers. This seems to suggest that pre-trained Transformers have serious difficulties in adapting to radically novel texts.

</p>
</details>

<details><summary><b>Precise Stock Price Prediction for Robust Portfolio Design from Selected Sectors of the Indian Stock Market</b>
<a href="https://arxiv.org/abs/2201.05570">arxiv:2201.05570</a>
&#x1F4C8; 2 <br>
<p>Jaydip Sen, Ashwin Kumar R S, Geetha Joseph, Kaushik Muthukrishnan, Koushik Tulasi, Praveen Varukolu</p></summary>
<p>

**Abstract:** Stock price prediction is a challenging task and a lot of propositions exist in the literature in this area. Portfolio construction is a process of choosing a group of stocks and investing in them optimally to maximize the return while minimizing the risk. Since the time when Markowitz proposed the Modern Portfolio Theory, several advancements have happened in the area of building efficient portfolios. An investor can get the best benefit out of the stock market if the investor invests in an efficient portfolio and could take the buy or sell decision in advance, by estimating the future asset value of the portfolio with a high level of precision. In this project, we have built an efficient portfolio and to predict the future asset value by means of individual stock price prediction of the stocks in the portfolio. As part of building an efficient portfolio we have studied multiple portfolio optimization methods beginning with the Modern Portfolio theory. We have built the minimum variance portfolio and optimal risk portfolio for all the five chosen sectors by using past daily stock prices over the past five years as the training data, and have also conducted back testing to check the performance of the portfolio. A comparative study of minimum variance portfolio and optimal risk portfolio with equal weight portfolio is done by backtesting.

</p>
</details>

<details><summary><b>SympOCnet: Solving optimal control problems with applications to high-dimensional multi-agent path planning problems</b>
<a href="https://arxiv.org/abs/2201.05475">arxiv:2201.05475</a>
&#x1F4C8; 2 <br>
<p>Tingwei Meng, Zhen Zhang, Jérôme Darbon, George Em Karniadakis</p></summary>
<p>

**Abstract:** Solving high-dimensional optimal control problems in real-time is an important but challenging problem, with applications to multi-agent path planning problems, which have drawn increased attention given the growing popularity of drones in recent years. In this paper, we propose a novel neural network method called SympOCnet that applies the Symplectic network to solve high-dimensional optimal control problems with state constraints. We present several numerical results on path planning problems in two-dimensional and three-dimensional spaces. Specifically, we demonstrate that our SympOCnet can solve a problem with more than 500 dimensions in 1.5 hours on a single GPU, which shows the effectiveness and efficiency of SympOCnet. The proposed method is scalable and has the potential to solve truly high-dimensional path planning problems in real-time.

</p>
</details>

<details><summary><b>Bayesian sense of time in biological and artificial brains</b>
<a href="https://arxiv.org/abs/2201.05464">arxiv:2201.05464</a>
&#x1F4C8; 2 <br>
<p>Zafeirios Fountas, Alexey Zakharov</p></summary>
<p>

**Abstract:** Enquiries concerning the underlying mechanisms and the emergent properties of a biological brain have a long history of theoretical postulates and experimental findings. Today, the scientific community tends to converge to a single interpretation of the brain's cognitive underpinnings -- that it is a Bayesian inference machine. This contemporary view has naturally been a strong driving force in recent developments around computational and cognitive neurosciences. Of particular interest is the brain's ability to process the passage of time -- one of the fundamental dimensions of our experience. How can we explain empirical data on human time perception using the Bayesian brain hypothesis? Can we replicate human estimation biases using Bayesian models? What insights can the agent-based machine learning models provide for the study of this subject? In this chapter, we review some of the recent advancements in the field of time perception and discuss the role of Bayesian processing in the construction of temporal models.

</p>
</details>

<details><summary><b>A causal model of safety assurance for machine learning</b>
<a href="https://arxiv.org/abs/2201.05451">arxiv:2201.05451</a>
&#x1F4C8; 2 <br>
<p>Simon Burton</p></summary>
<p>

**Abstract:** This paper proposes a framework based on a causal model of safety upon which effective safety assurance cases for ML-based applications can be built. In doing so, we build upon established principles of safety engineering as well as previous work on structuring assurance arguments for ML. The paper defines four categories of safety case evidence and a structured analysis approach within which these evidences can be effectively combined. Where appropriate, abstract formalisations of these contributions are used to illustrate the causalities they evaluate, their contributions to the safety argument and desirable properties of the evidences. Based on the proposed framework, progress in this area is re-evaluated and a set of future research directions proposed in order for tangible progress in this field to be made.

</p>
</details>

<details><summary><b>The Implicit Regularization of Momentum Gradient Descent with Early Stopping</b>
<a href="https://arxiv.org/abs/2201.05405">arxiv:2201.05405</a>
&#x1F4C8; 2 <br>
<p>Li Wang, Yingcong Zhou, Zhiguo Fu</p></summary>
<p>

**Abstract:** The study on the implicit regularization induced by gradient-based optimization is a longstanding pursuit. In the present paper, we characterize the implicit regularization of momentum gradient descent (MGD) with early stopping by comparing with the explicit $\ell_2$-regularization (ridge). In details, we study MGD in the continuous-time view, so-called momentum gradient flow (MGF), and show that its tendency is closer to ridge than the gradient descent (GD) [Ali et al., 2019] for least squares regression. Moreover, we prove that, under the calibration $t=\sqrt{2/λ}$, where $t$ is the time parameter in MGF and $λ$ is the tuning parameter in ridge regression, the risk of MGF is no more than 1.54 times that of ridge. In particular, the relative Bayes risk of MGF to ridge is between 1 and 1.035 under the optimal tuning. The numerical experiments support our theoretical results strongly.

</p>
</details>

<details><summary><b>Deep Learning for Agile Effort Estimation Have We Solved the Problem Yet?</b>
<a href="https://arxiv.org/abs/2201.05401">arxiv:2201.05401</a>
&#x1F4C8; 2 <br>
<p>Vali Tawosi, Rebecca Moussa, Federica Sarro</p></summary>
<p>

**Abstract:** In the last decade, several studies have proposed the use of automated techniques to estimate the effort of agile software development. In this paper we perform a close replication and extension of a seminal work proposing the use of Deep Learning for agile effort estimation (namely Deep-SE), which has set the state-of-the-art since. Specifically, we replicate three of the original research questions aiming at investigating the effectiveness of Deep-SE for both within-project and cross-project effort estimation. We benchmark Deep-SE against three baseline techniques (i.e., Random, Mean and Median effort prediction) and a previously proposed method to estimate agile software project development effort (dubbed TF/IDF-SE), as done in the original study. To this end, we use both the data from the original study and a new larger dataset of 31,960 issues, which we mined from 29 open-source projects. Using more data allows us to strengthen our confidence in the results and further mitigate the threat to the external validity of the study. We also extend the original study by investigating two additional research questions. One evaluates the accuracy of Deep-SE when the training set is augmented with issues from all other projects available in the repository at the time of estimation, and the other examines whether an expensive pre-training step used by the original Deep-SE, has any beneficial effect on its accuracy and convergence speed. The results of our replication show that Deep-SE outperforms the Median baseline estimator and TF/IDF-SE in only very few cases with statistical significance (8/42 and 9/32 cases, respectively), thus confounding previous findings on the efficacy of Deep-SE. The two additional RQs revealed that neither augmenting the training set nor pre-training Deep-SE play a role in improving its accuracy and convergence speed. ...

</p>
</details>

<details><summary><b>Artificial Intelligence in Software Testing : Impact, Problems, Challenges and Prospect</b>
<a href="https://arxiv.org/abs/2201.05371">arxiv:2201.05371</a>
&#x1F4C8; 2 <br>
<p>Zubair Khaliq, Sheikh Umar Farooq, Dawood Ashraf Khan</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) is making a significant impact in multiple areas like medical, military, industrial, domestic, law, arts as AI is capable to perform several roles such as managing smart factories, driving autonomous vehicles, creating accurate weather forecasts, detecting cancer and personal assistants, etc. Software testing is the process of putting the software to test for some abnormal behaviour of the software. Software testing is a tedious, laborious and most time-consuming process. Automation tools have been developed that help to automate some activities of the testing process to enhance quality and timely delivery. Over time with the inclusion of continuous integration and continuous delivery (CI/CD) pipeline, automation tools are becoming less effective. The testing community is turning to AI to fill the gap as AI is able to check the code for bugs and errors without any human intervention and in a much faster way than humans. In this study, we aim to recognize the impact of AI technologies on various software testing activities or facets in the STLC. Further, the study aims to recognize and explain some of the biggest challenges software testers face while applying AI to testing. The paper also proposes some key contributions of AI in the future to the domain of software testing.

</p>
</details>

<details><summary><b>Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?</b>
<a href="https://arxiv.org/abs/2201.05340">arxiv:2201.05340</a>
&#x1F4C8; 2 <br>
<p>Lena Schmid, Alexander Gerharz, Andreas Groll, Markus Pauly</p></summary>
<p>

**Abstract:** Tree-based ensembles such as the Random Forest are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether we separately fit univariate models or directly follow a multivariate approach. For the latter, several possibilities exist that are, e.g. based on modified splitting or stopping rules for multi-output regression. In this work we compare these methods in extensive simulations to help in answering the primary question when to use multivariate ensemble techniques.

</p>
</details>

<details><summary><b>A Kernel-Expanded Stochastic Neural Network</b>
<a href="https://arxiv.org/abs/2201.05319">arxiv:2201.05319</a>
&#x1F4C8; 2 <br>
<p>Yan Sun, Faming Liang</p></summary>
<p>

**Abstract:** The deep neural network suffers from many fundamental issues in machine learning. For example, it often gets trapped into a local minimum in training, and its prediction uncertainty is hard to be assessed. To address these issues, we propose the so-called kernel-expanded stochastic neural network (K-StoNet) model, which incorporates support vector regression (SVR) as the first hidden layer and reformulates the neural network as a latent variable model. The former maps the input vector into an infinite dimensional feature space via a radial basis function (RBF) kernel, ensuring absence of local minima on its training loss surface. The latter breaks the high-dimensional nonconvex neural network training problem into a series of low-dimensional convex optimization problems, and enables its prediction uncertainty easily assessed. The K-StoNet can be easily trained using the imputation-regularized optimization (IRO) algorithm. Compared to traditional deep neural networks, K-StoNet possesses a theoretical guarantee to asymptotically converge to the global optimum and enables the prediction uncertainty easily assessed. The performances of the new model in training, prediction and uncertainty quantification are illustrated by simulated and real data examples.

</p>
</details>

<details><summary><b>Wide Area Network Intelligence with Application to Multimedia Service</b>
<a href="https://arxiv.org/abs/2201.07216">arxiv:2201.07216</a>
&#x1F4C8; 1 <br>
<p>Satoshi Kamo, Yiqiang Sheng</p></summary>
<p>

**Abstract:** Network intelligence is a discipline that builds on the capabilities of network systems to act intelligently by the usage of network resources for delivering high-quality services in a changing environment. Wide area network intelligence is a class of network intelligence in wide area network which covers the core and the edge of Internet. In this paper, we propose a system based on machine learning for wide area network intelligence. The whole system consists of a core machine for pre-training and many terminal machines to accomplish faster responses. Each machine is one of dual-hemisphere models which are made of left and right hemispheres. The left hemisphere is used to improve latency by terminal response and the right hemisphere is used to improve communication by data generation. In an application on multimedia service, the proposed model is superior to the latest deep feed forward neural network in the data center with respect to the accuracy, latency and communication. Evaluation shows scalable improvement with regard to the number of terminal machines. Evaluation also shows the cost of improvement is longer learning time.

</p>
</details>

<details><summary><b>A Semantic Web Technology Index</b>
<a href="https://arxiv.org/abs/2201.07034">arxiv:2201.07034</a>
&#x1F4C8; 1 <br>
<p>Gongjin Lan, Ting Liu, Xu Wang, Xueli Pan, Zhisheng Huang</p></summary>
<p>

**Abstract:** Semantic Web (SW) technology has been widely applied to many domains such as medicine, health care, finance, geology. At present, researchers mainly rely on their experience and preferences to develop and evaluate the work of SW technology. Although the general architecture (e.g., Tim Berners-Lee's Semantic Web Layer Cake) of SW technology was proposed many years ago and has been well-known, it still lacks a concrete guideline for standardizing the development of SW technology. In this paper, we propose an SW technology index to standardize the development for ensuring that the work of SW technology is designed well and to quantitatively evaluate the quality of the work in SW technology. This index consists of 10 criteria that quantify the quality as a score of 0 ~ 10. We address each criterion in detail for a clear explanation from three aspects: 1) what is the criterion? 2) why do we consider this criterion and 3) how do the current studies meet this criterion? Finally, we present the validation of this index by providing some examples of how to apply the index to the validation cases. We conclude that the index is a useful standard to guide and evaluate the work in SW technology.

</p>
</details>

<details><summary><b>Digital Twin: From Concept to Practice</b>
<a href="https://arxiv.org/abs/2201.06912">arxiv:2201.06912</a>
&#x1F4C8; 1 <br>
<p>Ashwin Agrawal, Martin Fischer, Vishal Singh</p></summary>
<p>

**Abstract:** Recent technological developments and advances in Artificial Intelligence (AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT), virtually making it possible to introduce automation into all aspects of work processes. Given these possibilities that DT can offer, practitioners are facing increasingly difficult decisions regarding what capabilities to select while deploying a DT in practice. The lack of research in this field has not helped either. It has resulted in the rebranding and reuse of emerging technological capabilities like prediction, simulation, AI, and Machine Learning (ML) as necessary constituents of DT. Inappropriate selection of capabilities in a DT can result in missed opportunities, strategic misalignments, inflated expectations, and risk of it being rejected as just hype by the practitioners. To alleviate this challenge, this paper proposes the digitalization framework, designed and developed by following a Design Science Research (DSR) methodology over a period of 18 months. The framework can help practitioners select an appropriate level of sophistication in a DT by weighing the pros and cons for each level, deciding evaluation criteria for the digital twin system, and assessing the implications of the selected DT on the organizational processes and strategies, and value creation. Three real-life case studies illustrate the application and usefulness of the framework.

</p>
</details>

<details><summary><b>Wrapped Classifier with Dummy Teacher for training physics-based classifier at unlabeled radar data</b>
<a href="https://arxiv.org/abs/2201.05735">arxiv:2201.05735</a>
&#x1F4C8; 1 <br>
<p>Oleg I. Berngardt, Oleg A. Kusonsky, Alexey I. Poddelsky, Alexey V. Oinats</p></summary>
<p>

**Abstract:** In the paper a method for automatic classification of signals received by EKB and MAGW ISTP SB RAS coherent scatter radars (8-20MHz operating frequency) during 2021 is described. The method is suitable for automatic physical interpretation of the resulting classification of the experimental data in realtime. We called this algorithm Wrapped Classifier with Dummy Teacher. The method is trained on unlabeled dataset and is based on training optimal physics-based classification using clusterization results. The approach is close to optimal embedding search, where the embedding is interpreted as a vector of probabilities for soft classification. The approach allows to find optimal classification algorithm, based on physically interpretable parameters of the received data, both obtained during physics-based numerical simulation and measured experimentally. Dummy Teacher clusterer used for labeling unlabeled dataset is gaussian mixture clustering algorithm. For algorithm functioning we extended the parameters obtained by the radar with additional parameters, calculated during simulation of radiowave propagation using ray-tracing and IRI-2012 and IGRF models for ionosphere and Earth's magnetic field correspondingly. For clustering by Dummy Teacher we use the whole dataset of available parameters (measured and simulated ones). For classification by Wrapped Classifier we use only well physically interpreted parameters. As a result we trained the classification network and found 11 well-interpretable classes from physical point of view in the available data. Five other found classes are not interpretable from physical point of view, demonstrating the importance of taking into account radiowave propagation for correct classification.

</p>
</details>

<details><summary><b>IBAC: An Intelligent Dynamic Bandwidth Channel Access Avoiding Outside Warning Range Problem</b>
<a href="https://arxiv.org/abs/2201.05727">arxiv:2201.05727</a>
&#x1F4C8; 1 <br>
<p>Raja Karmakar, Georges Kaddoum</p></summary>
<p>

**Abstract:** IEEE 802.11ax uses the concept of primary and secondary channels, leading to the Dynamic Bandwidth Channel Access (DBCA) mechanism. By applying DBCA, a wireless station can select a wider channel bandwidth, such as 40/80/160 MHz, by applying the channel bonding feature. However, during channel bonding, inappropriate bandwidth selection can cause collisions. Therefore, to avoid collisions, a well-developed media access control (MAC) protocol is crucial to effectively utilize the channel bonding mechanism. In this paper, we address a collision scenario, called Outside Warning Range Problem (OWRP), that may occur during DBCA when a wireless station interferes with another wireless station after channel bonding is performed. Therefore, we propose a MAC layer mechanism, Intelligent Bonding Avoiding Collision (IBAC), that adapts the channel bonding level in DBCA in order to avoid the OWRP. We first design a theoretical model based on Markov chains for DBCA while avoiding the OWRP. Based on this model, we design a Thompson sampling based Bayesian approach to select the best possible channel bonding level intelligently. We analyze the performance of the IBAC through simulations where it is observed that, comparing to other competing mechanisms, the proposed approach can enhance the network performance significantly while avoiding the OWRP.

</p>
</details>

<details><summary><b>Transformers in Action:Weakly Supervised Action Segmentation</b>
<a href="https://arxiv.org/abs/2201.05675">arxiv:2201.05675</a>
&#x1F4C8; 1 <br>
<p>John Ridley, Huseyin Coskun, David Joseph Tan, Nassir Navab, Federico Tombari</p></summary>
<p>

**Abstract:** The video action segmentation task is regularly explored under weaker forms of supervision, such as transcript supervision, where a list of actions is easier to obtain than dense frame-wise labels. In this formulation, the task presents various challenges for sequence modeling approaches due to the emphasis on action transition points, long sequence lengths, and frame contextualization, making the task well-posed for transformers. Given developments enabling transformers to scale linearly, we demonstrate through our architecture how they can be applied to improve action alignment accuracy over the equivalent RNN-based models with the attention mechanism focusing around salient action transition regions. Additionally, given the recent focus on inference-time transcript selection, we propose a supplemental transcript embedding approach to select transcripts more quickly at inference-time. Furthermore, we subsequently demonstrate how this approach can also improve the overall segmentation performance. Finally, we evaluate our proposed methods across the benchmark datasets to better understand the applicability of transformers and the importance of transcript selection on this video-driven weakly-supervised task.

</p>
</details>

<details><summary><b>Waveform Learning for Reduced Out-of-Band Emissions Under a Nonlinear Power Amplifier</b>
<a href="https://arxiv.org/abs/2201.05524">arxiv:2201.05524</a>
&#x1F4C8; 1 <br>
<p>Dani Korpi, Mikko Honkala, Janne M. J. Huttunen, Fayçal Ait Aoudia, Jakob Hoydis</p></summary>
<p>

**Abstract:** Machine learning (ML) has shown great promise in optimizing various aspects of the physical layer processing in wireless communication systems. In this paper, we use ML to learn jointly the transmit waveform and the frequency-domain receiver. In particular, we consider a scenario where the transmitter power amplifier is operating in a nonlinear manner, and ML is used to optimize the waveform to minimize the out-of-band emissions. The system also learns a constellation shape that facilitates pilotless detection by the simultaneously learned receiver. The simulation results show that such an end-to-end optimized system can communicate data more accurately and with less out-of-band emissions than conventional systems, thereby demonstrating the potential of ML in optimizing the air interface. To the best of our knowledge, there are no prior works considering the power amplifier induced emissions in an end-to-end learned system. These findings pave the way towards an ML-native air interface, which could be one of the building blocks of 6G.

</p>
</details>

<details><summary><b>Security Orchestration, Automation, and Response Engine for Deployment of Behavioural Honeypots</b>
<a href="https://arxiv.org/abs/2201.05326">arxiv:2201.05326</a>
&#x1F4C8; 1 <br>
<p>Upendra Bartwal, Subhasis Mukhopadhyay, Rohit Negi, Sandeep Shukla</p></summary>
<p>

**Abstract:** Cyber Security is a critical topic for organizations with IT/OT networks as they are always susceptible to attack, whether insider or outsider. Since the cyber landscape is an ever-evolving scenario, one must keep upgrading its security systems to enhance the security of the infrastructure. Tools like Security Information and Event Management (SIEM), Endpoint Detection and Response (EDR), Threat Intelligence Platform (TIP), Information Technology Service Management (ITSM), along with other defensive techniques like Intrusion Detection System (IDS), Intrusion Protection System (IPS), and many others enhance the cyber security posture of the infrastructure. However, the proposed protection mechanisms have their limitations, they are insufficient to ensure security, and the attacker penetrates the network. Deception technology, along with Honeypots, provides a false sense of vulnerability in the target systems to the attackers. The attacker deceived reveals threat intel about their modus operandi. We have developed a Security Orchestration, Automation, and Response (SOAR) Engine that dynamically deploys custom honeypots inside the internal network infrastructure based on the attacker's behavior. The architecture is robust enough to support multiple VLANs connected to the system and used for orchestration. The presence of botnet traffic and DDOS attacks on the honeypots in the network is detected, along with a malware collection system. After being exposed to live traffic for four days, our engine dynamically orchestrated the honeypots 40 times, detected 7823 attacks, 965 DDOS attack packets, and three malicious samples. While our experiments with static honeypots show an average attacker engagement time of 102 seconds per instance, our SOAR Engine-based dynamic honeypots engage attackers on average 3148 seconds.

</p>
</details>


{% endraw %}
Prev: [2022.01.13]({{ '/2022/01/13/2022.01.13.html' | relative_url }})  Next: [2022.01.15]({{ '/2022/01/15/2022.01.15.html' | relative_url }})