Prev: [2021.02.25]({{ '/2021/02/25/2021.02.25.html' | relative_url }})  Next: [2021.02.27]({{ '/2021/02/27/2021.02.27.html' | relative_url }})
{% raw %}
## Summary for 2021-02-26, created on 2021-12-24


<details><summary><b>Transformer in Transformer</b>
<a href="https://arxiv.org/abs/2103.00112">arxiv:2103.00112</a>
&#x1F4C8; 2650 <br>
<p>Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang</p></summary>
<p>

**Abstract:** Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\times$16) as "visual sentences" and present to further divide them into smaller patches (e.g., 4$\times$4) as "visual words". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.

</p>
</details>

<details><summary><b>Learning Transferable Visual Models From Natural Language Supervision</b>
<a href="https://arxiv.org/abs/2103.00020">arxiv:2103.00020</a>
&#x1F4C8; 264 <br>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever</p></summary>
<p>

**Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.

</p>
</details>

<details><summary><b>3D Vessel Reconstruction in OCT-Angiography via Depth Map Estimation</b>
<a href="https://arxiv.org/abs/2102.13588">arxiv:2102.13588</a>
&#x1F4C8; 65 <br>
<p>Shuai Yu, Jianyang Xie, Jinkui Hao, Yalin Zheng, Jiong Zhang, Yan Hu, Jiang Liu, Yitian Zhao</p></summary>
<p>

**Abstract:** Optical Coherence Tomography Angiography (OCTA) has been increasingly used in the management of eye and systemic diseases in recent years. Manual or automatic analysis of blood vessel in 2D OCTA images (en face angiograms) is commonly used in clinical practice, however it may lose rich 3D spatial distribution information of blood vessels or capillaries that are useful for clinical decision-making. In this paper, we introduce a novel 3D vessel reconstruction framework based on the estimation of vessel depth maps from OCTA images. First, we design a network with structural constraints to predict the depth of blood vessels in OCTA images. In order to promote the accuracy of the predicted depth map at both the overall structure- and pixel- level, we combine MSE and SSIM loss as the training loss function. Finally, the 3D vessel reconstruction is achieved by utilizing the estimated depth map and 2D vessel segmentation results. Experimental results demonstrate that our method is effective in the depth prediction and 3D vessel reconstruction for OCTA images.% results may be used to guide subsequent vascular analysis

</p>
</details>

<details><summary><b>On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.13651">arxiv:2102.13651</a>
&#x1F4C8; 63 <br>
<p>Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, André Biedenkapp, Kurtland Chua, Frank Hutter, Roberto Calandra</p></summary>
<p>

**Abstract:** Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyperparameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.

</p>
</details>

<details><summary><b>NOMU: Neural Optimization-based Model Uncertainty</b>
<a href="https://arxiv.org/abs/2102.13640">arxiv:2102.13640</a>
&#x1F4C8; 56 <br>
<p>Jakob Heiss, Jakob Weissteiner, Hanna Wutte, Sven Seuken, Josef Teichmann</p></summary>
<p>

**Abstract:** We study methods for estimating model uncertainty for neural networks (NNs). To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access to its training data. We first experimentally study noiseless regression with scarce training data to highlight the deficiencies of the established benchmarks. Finally, we study the important task of Bayesian optimization (BO) with costly evaluations, where good model uncertainty estimates are essential. Our results show that NOMU performs as well or better than state-of-the-art benchmarks.

</p>
</details>

<details><summary><b>Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To Game</b>
<a href="https://arxiv.org/abs/2102.13647">arxiv:2102.13647</a>
&#x1F4C8; 45 <br>
<p>Alexander G. Reisach, Christof Seiler, Sebastian Weichwald</p></summary>
<p>

**Abstract:** Simulated DAG models may exhibit properties that, perhaps inadvertently, render their structure identifiable and unexpectedly affect structure learning algorithms. Here, we show that marginal variance tends to increase along the causal order for generically sampled additive noise models. We introduce varsortability as a measure of the agreement between the order of increasing marginal variance and the causal order. For commonly sampled graphs and model parameters, we show that the remarkable performance of some continuous structure learning algorithms can be explained by high varsortability and matched by a simple baseline method. Yet, this performance may not transfer to real-world data where varsortability may be moderate or dependent on the choice of measurement scales. On standardized data, the same algorithms fail to identify the ground-truth DAG or its Markov equivalence class. While standardization removes the pattern in marginal variance, we show that data generating processes that incur high varsortability also leave a distinct covariance pattern that may be exploited even after standardization. Our findings challenge the significance of generic benchmarks with independently drawn parameters. The code is available at https://github.com/Scriddie/Varsortability.

</p>
</details>

<details><summary><b>Distribution-Aware Testing of Neural Networks Using Generative Models</b>
<a href="https://arxiv.org/abs/2102.13602">arxiv:2102.13602</a>
&#x1F4C8; 36 <br>
<p>Swaroopa Dola, Matthew B. Dwyer, Mary Lou Soffa</p></summary>
<p>

**Abstract:** The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics.
  To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.

</p>
</details>

<details><summary><b>Knowledge-aware Zero-Shot Learning: Survey and Perspective</b>
<a href="https://arxiv.org/abs/2103.00070">arxiv:2103.00070</a>
&#x1F4C8; 28 <br>
<p>Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Ian Horrocks, Jeff Z. Pan, Huajun Chen</p></summary>
<p>

**Abstract:** Zero-shot learning (ZSL) which aims at predicting classes that have never appeared during the training using external knowledge (a.k.a. side information) has been widely investigated. In this paper we present a literature review towards ZSL in the perspective of external knowledge, where we categorize the external knowledge, review their methods and compare different external knowledge. With the literature review, we further discuss and outlook the role of symbolic knowledge in addressing ZSL and other machine learning sample shortage issues.

</p>
</details>

<details><summary><b>Why did the distribution change?</b>
<a href="https://arxiv.org/abs/2102.13384">arxiv:2102.13384</a>
&#x1F4C8; 25 <br>
<p>Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, Hoiyi Ng</p></summary>
<p>

**Abstract:** We describe a formal approach based on graphical causal models to identify the "root causes" of the change in the probability distribution of variables. After factorizing the joint distribution into conditional distributions of each variable, given its parents (the "causal mechanisms"), we attribute the change to changes of these causal mechanisms. This attribution analysis accounts for the fact that mechanisms often change independently and sometimes only some of them change. Through simulations, we study the performance of our distribution change attribution method. We then present a real-world case study identifying the drivers of the difference in the income distribution between men and women.

</p>
</details>

<details><summary><b>ASMNet: a Lightweight Deep Neural Network for Face Alignment and Pose Estimation</b>
<a href="https://arxiv.org/abs/2103.00119">arxiv:2103.00119</a>
&#x1F4C8; 21 <br>
<p>Ali Pourramezan Fard, Hojjat Abdollahi, Mohammad Mahoor</p></summary>
<p>

**Abstract:** Active Shape Model (ASM) is a statistical model of object shapes that represents a target structure. ASM can guide machine learning algorithms to fit a set of points representing an object (e.g., face) onto an image. This paper presents a lightweight Convolutional Neural Network (CNN) architecture with a loss function being assisted by ASM for face alignment and estimating head pose in the wild. We use ASM to first guide the network towards learning a smoother distribution of the facial landmark points. Inspired by transfer learning, during the training process, we gradually harden the regression problem and guide the network towards learning the original landmark points distribution. We define multi-tasks in our loss function that are responsible for detecting facial landmark points as well as estimating the face pose. Learning multiple correlated tasks simultaneously builds synergy and improves the performance of individual tasks. We compare the performance of our proposed model called ASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the face alignment and pose estimation tasks. Experimental results on challenging datasets show that by using the proposed ASM assisted loss function, the ASMNet performance is comparable with MobileNetV2 in the face alignment task. In addition, for face pose estimation, ASMNet performs much better than MobileNetV2. ASMNet achieves an acceptable performance for facial landmark points detection and pose estimation while having a significantly smaller number of parameters and floating-point operations compared to many CNN-based models.

</p>
</details>

<details><summary><b>Convolution-Free Medical Image Segmentation using Transformers</b>
<a href="https://arxiv.org/abs/2102.13645">arxiv:2102.13645</a>
&#x1F4C8; 21 <br>
<p>Davood Karimi, Serge Vasylechko, Ali Gholipour</p></summary>
<p>

**Abstract:** Like other applications in computer vision, medical image segmentation has been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. In this work we show that a different method, based entirely on self-attention between neighboring image patches and without any convolution operations, can achieve competitive or better results. Given a 3D image block, our network divides it into $n^3$ 3D patches, where $n=3 \text{ or } 5$ and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve segmentation accuracies that are better than the state of the art CNNs on three datasets. We also propose methods for pre-training this model on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.

</p>
</details>

<details><summary><b>Variance Reduction via Primal-Dual Accelerated Dual Averaging for Nonsmooth Convex Finite-Sums</b>
<a href="https://arxiv.org/abs/2102.13643">arxiv:2102.13643</a>
&#x1F4C8; 18 <br>
<p>Chaobing Song, Stephen J. Wright, Jelena Diakonikolas</p></summary>
<p>

**Abstract:** We study structured nonsmooth convex finite-sum optimization that appears widely in machine learning applications, including support vector machines and least absolute deviation. For the primal-dual formulation of this problem, we propose a novel algorithm called \emph{Variance Reduction via Primal-Dual Accelerated Dual Averaging (\vrpda)}. In the nonsmooth and general convex setting, \vrpda~has the overall complexity $O(nd\log\min \{1/ε, n\} + d/ε)$ in terms of the primal-dual gap, where $n$ denotes the number of samples, $d$ the dimension of the primal variables, and $ε$ the desired accuracy. In the nonsmooth and strongly convex setting, the overall complexity of \vrpda~becomes $O(nd\log\min\{1/ε, n\} + d/\sqrtε)$ in terms of both the primal-dual gap and the distance between iterate and optimal solution. Both these results for \vrpda~improve significantly on state-of-the-art complexity estimates, which are $O(nd\log \min\{1/ε, n\} + \sqrt{n}d/ε)$ for the nonsmooth and general convex setting and $O(nd\log \min\{1/ε, n\} + \sqrt{n}d/\sqrtε)$ for the nonsmooth and strongly convex setting, in a much more simple and straightforward way. Moreover, both complexities are better than \emph{lower} bounds for general convex finite sums that lack the particular (common) structure that we consider. Our theoretical results are supported by numerical experiments, which confirm the competitive performance of \vrpda~compared to state-of-the-art.

</p>
</details>

<details><summary><b>Natural Language Video Localization: A Revisit in Span-based Question Answering Framework</b>
<a href="https://arxiv.org/abs/2102.13558">arxiv:2102.13558</a>
&#x1F4C8; 18 <br>
<p>Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick Siow Mong Goh</p></summary>
<p>

**Abstract:** Natural Language Video Localization (NLVL) aims to locate a target moment from an untrimmed video that semantically corresponds to a text query. Existing approaches mainly solve the NLVL problem from the perspective of computer vision by formulating it as ranking, anchor, or regression tasks. These methods suffer from large performance degradation when localizing on long videos. In this work, we address the NLVL from a new perspective, i.e., span-based question answering (QA), by treating the input video as a text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the differences between NLVL and span-based QA through a simple yet effective query-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the matching video span within a highlighted region. To address the performance degradation on long videos, we further extend VSLNet to VSLNet-L by applying a multi-scale split-and-concatenation strategy. VSLNet-L first splits the untrimmed video into short clip segments; then, it predicts which clip segment contains the target moment and suppresses the importance of other segments. Finally, the clip segments are concatenated, with different confidences, to locate the target moment accurately. Extensive experiments on three benchmark datasets show that the proposed VSLNet and VSLNet-L outperform the state-of-the-art methods; VSLNet-L addresses the issue of performance degradation on long videos. Our study suggests that the span-based QA framework is an effective strategy to solve the NLVL problem.

</p>
</details>

<details><summary><b>Towards Robust and Reliable Algorithmic Recourse</b>
<a href="https://arxiv.org/abs/2102.13620">arxiv:2102.13620</a>
&#x1F4C8; 16 <br>
<p>Sohini Upadhyay, Shalmali Joshi, Himabindu Lakkaraju</p></summary>
<p>

**Abstract:** As predictive models are increasingly being deployed in high-stakes decision making (e.g., loan approvals), there has been growing interest in post hoc techniques which provide recourse to affected individuals. These techniques generate recourses under the assumption that the underlying predictive model does not change. However, in practice, models are often regularly updated for a variety of reasons (e.g., dataset shifts), thereby rendering previously prescribed recourses ineffective. To address this problem, we propose a novel framework, RObust Algorithmic Recourse (ROAR), that leverages adversarial training for finding recourses that are robust to model shifts. To the best of our knowledge, this work proposes the first solution to this critical problem. We also carry out detailed theoretical analysis which underscores the importance of constructing recourses that are robust to model shifts: 1) we derive a lower bound on the probability of invalidation of recourses generated by existing approaches which are not robust to model shifts. 2) we prove that the additional cost incurred due to the robust recourses output by our framework is bounded. Experimental evaluation on multiple synthetic and real-world datasets demonstrates the efficacy of the proposed framework and supports our theoretical findings.

</p>
</details>

<details><summary><b>On the Generalization of Stochastic Gradient Descent with Momentum</b>
<a href="https://arxiv.org/abs/2102.13653">arxiv:2102.13653</a>
&#x1F4C8; 10 <br>
<p>Ali Ramezani-Kebrya, Ashish Khisti, Ben Liang</p></summary>
<p>

**Abstract:** While momentum-based methods, in conjunction with stochastic gradient descent (SGD), are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which algorithmic stability fails to establish generalization guarantees when SGD with standard heavy-ball momentum (SGDM) is run for multiple epochs. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM), and show that it admits an upper-bound on the generalization error. Thus, our results show that machine learning models can be trained for multiple epochs of SGDEM with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper-bound on the expected true risk, in terms of the number of training steps, the size of the training set, and the momentum parameter. Experimental evaluations verify the consistency between the numerical results and our theoretical bounds and the effectiveness of SGDEM for smooth Lipschitz loss functions.

</p>
</details>

<details><summary><b>History-Augmented Collaborative Filtering for Financial Recommendations</b>
<a href="https://arxiv.org/abs/2102.13503">arxiv:2102.13503</a>
&#x1F4C8; 10 <br>
<p>Baptiste Barreau, Laurent Carlier</p></summary>
<p>

**Abstract:** In many businesses, and particularly in finance, the behavior of a client might drastically change over time. It is consequently crucial for recommender systems used in such environments to be able to adapt to these changes. In this study, we propose a novel collaborative filtering algorithm that captures the temporal context of a user-item interaction through the users' and items' recent interaction histories to provide dynamic recommendations. The algorithm, designed with issues specific to the financial world in mind, uses a custom neural network architecture that tackles the non-stationarity of users' and items' behaviors. The performance and properties of the algorithm are monitored in a series of experiments on a G10 bond request for quotation proprietary database from BNP Paribas Corporate and Institutional Banking.

</p>
</details>

<details><summary><b>Improving Longer-range Dialogue State Tracking</b>
<a href="https://arxiv.org/abs/2103.00109">arxiv:2103.00109</a>
&#x1F4C8; 9 <br>
<p>Ye Zhang, Yuan Cao, Mahdis Mahdieh, Jeffrey Zhao, Yonghui Wu</p></summary>
<p>

**Abstract:** Dialogue state tracking (DST) is a pivotal component in task-oriented dialogue systems. While it is relatively easy for a DST model to capture belief states in short conversations, the task of DST becomes more challenging as the length of a dialogue increases due to the injection of more distracting contexts. In this paper, we aim to improve the overall performance of DST with a special focus on handling longer dialogues. We tackle this problem from three perspectives: 1) A model designed to enable hierarchical slot status prediction; 2) Balanced training procedure for generic and task-specific language understanding; 3) Data perturbation which enhances the model's ability in handling longer conversations. We conduct experiments on the MultiWOZ benchmark, and demonstrate the effectiveness of each component via a set of ablation tests, especially on longer conversations.

</p>
</details>

<details><summary><b>Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability</b>
<a href="https://arxiv.org/abs/2103.00065">arxiv:2103.00065</a>
&#x1F4C8; 9 <br>
<p>Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, Ameet Talwalkar</p></summary>
<p>

**Abstract:** We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the numerical value $2 / \text{(step size)}$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability. Code is available at https://github.com/locuslab/edge-of-stability.

</p>
</details>

<details><summary><b>Beyond Convolutions: A Novel Deep Learning Approach for Raw Seismic Data Ingestion</b>
<a href="https://arxiv.org/abs/2102.13631">arxiv:2102.13631</a>
&#x1F4C8; 9 <br>
<p>Zhaozhuo Xu, Aditya Desai, Menal Gupta, Anu Chandran, Antoine Vial-Aussavy, Anshumali Shrivastava</p></summary>
<p>

**Abstract:** Traditional seismic processing workflows (SPW) are expensive, requiring over a year of human and computational effort. Deep learning (DL) based data-driven seismic workflows (DSPW) hold the potential to reduce these timelines to a few minutes. Raw seismic data (terabytes) and required subsurface prediction (gigabytes) are enormous. This large-scale, spatially irregular time-series data poses seismic data ingestion (SDI) as an unconventional yet fundamental problem in DSPW. Current DL research is limited to small-scale simplified synthetic datasets as they treat seismic data like images and process them with convolution networks. Real seismic data, however, is at least 5D. Applying 5D convolutions to this scale is computationally prohibitive. Moreover, raw seismic data is highly unstructured and hence inherently non-image like. We propose a fundamental shift to move away from convolutions and introduce SESDI: Set Embedding based SDI approach. SESDI first breaks down the mammoth task of large-scale prediction into an efficient compact auxiliary task. SESDI gracefully incorporates irregularities in data with its novel model architecture. We believe SESDI is the first successful demonstration of end-to-end learning on real seismic data. SESDI achieves SSIM of over 0.8 on velocity inversion task on real proprietary data from the Gulf of Mexico and outperforms the state-of-the-art U-Net model on synthetic datasets.

</p>
</details>

<details><summary><b>Learning Prediction Intervals for Regression: Generalization and Calibration</b>
<a href="https://arxiv.org/abs/2102.13625">arxiv:2102.13625</a>
&#x1F4C8; 9 <br>
<p>Haoxian Chen, Ziyi Huang, Henry Lam, Huajie Qian, Haofeng Zhang</p></summary>
<p>

**Abstract:** We study the generation of prediction intervals in regression for uncertainty quantification. This task can be formalized as an empirical constrained optimization problem that minimizes the average interval width while maintaining the coverage accuracy across data. We strengthen the existing literature by studying two aspects of this empirical optimization. First is a general learning theory to characterize the optimality-feasibility tradeoff that encompasses Lipschitz continuity and VC-subgraph classes, which are exemplified in regression trees and neural networks. Second is a calibration machinery and the corresponding statistical theory to optimally select the regularization parameter that manages this tradeoff, which bypasses the overfitting issues in previous approaches in coverage attainment. We empirically demonstrate the strengths of our interval generation and calibration algorithms in terms of testing performances compared to existing benchmarks.

</p>
</details>

<details><summary><b>A Meta-embedding-based Ensemble Approach for ICD Coding Prediction</b>
<a href="https://arxiv.org/abs/2102.13622">arxiv:2102.13622</a>
&#x1F4C8; 9 <br>
<p>Pavithra Rajendran, Alexandros Zenonos, Josh Spear, Rebecca Pope</p></summary>
<p>

**Abstract:** International Classification of Diseases (ICD) are the de facto codes used globally for clinical coding. These codes enable healthcare providers to claim reimbursement and facilitate efficient storage and retrieval of diagnostic information. The problem of automatically assigning ICD codes has been approached in literature as a multilabel classification, using neural models on unstructured data. Our proposed approach enhances the performance of neural models by effectively training word vectors using routine medical data as well as external knowledge from scientific articles. Furthermore, we exploit the geometric properties of the two sets of word vectors and combine them into a common dimensional space, using meta-embedding techniques. We demonstrate the efficacy of this approach for a multimodal setting, using unstructured and structured information. We empirically show that our approach improves the current state-of-the-art deep learning architectures and benefits ensemble models.

</p>
</details>

<details><summary><b>ECO: Enabling Energy-Neutral IoT Devices through Runtime Allocation of Harvested Energy</b>
<a href="https://arxiv.org/abs/2102.13605">arxiv:2102.13605</a>
&#x1F4C8; 9 <br>
<p>Yigit Tuncel, Ganapati Bhat, Jaehyun Park, Umit Ogras</p></summary>
<p>

**Abstract:** Energy harvesting offers an attractive and promising mechanism to power low-energy devices. However, it alone is insufficient to enable an energy-neutral operation, which can eliminate tedious battery charging and replacement requirements. Achieving an energy-neutral operation is challenging since the uncertainties in harvested energy undermine the quality of service requirements. To address this challenge, we present a runtime energy-allocation framework that optimizes the utility of the target device under energy constraints using a rollout algorithm, which is a sequential approach to solve dynamic optimization problems. The proposed framework uses an efficient iterative algorithm to compute initial energy allocations at the beginning of a day. The initial allocations are then corrected at every interval to compensate for the deviations from the expected energy harvesting pattern. We evaluate this framework using solar and motion energy harvesting modalities and American Time Use Survey data from 4772 different users. Compared to prior techniques, the proposed framework achieves up to 35% higher utility even under energy-limited scenarios. Moreover, measurements on a wearable device prototype show that the proposed framework has 1000x smaller energy overhead than iterative approaches with a negligible loss in utility.

</p>
</details>

<details><summary><b>Zoetrope Genetic Programming for Regression</b>
<a href="https://arxiv.org/abs/2102.13388">arxiv:2102.13388</a>
&#x1F4C8; 9 <br>
<p>Aurélie Boisbunon, Carlo Fanara, Ingrid Grenet, Jonathan Daeden, Alexis Vighi, Marc Schoenauer</p></summary>
<p>

**Abstract:** The Zoetrope Genetic Programming (ZGP) algorithm is based on an original representation for mathematical expressions, targeting evolutionary symbolic regression.The zoetropic representation uses repeated fusion operations between partial expressions, starting from the terminal set. Repeated fusions within an individual gradually generate more complex expressions, ending up in what can be viewed as new features. These features are then linearly combined to best fit the training data. ZGP individuals then undergo specific crossover and mutation operators, and selection takes place between parents and offspring. ZGP is validated using a large number of public domain regression datasets, and compared to other symbolic regression algorithms, as well as to traditional machine learning algorithms. ZGP reaches state-of-the-art performance with respect to both types of algorithms, and demonstrates a low computational time compared to other symbolic regression approaches.

</p>
</details>

<details><summary><b>Node Proximity Is All You Need: Unified Structural and Positional Node and Graph Embedding</b>
<a href="https://arxiv.org/abs/2102.13582">arxiv:2102.13582</a>
&#x1F4C8; 8 <br>
<p>Jing Zhu, Xingyu Lu, Mark Heimann, Danai Koutra</p></summary>
<p>

**Abstract:** While most network embedding techniques model the relative positions of nodes in a network, recently there has been significant interest in structural embeddings that model node role equivalences, irrespective of their distances to any specific nodes. We present PhUSION, a proximity-based unified framework for computing structural and positional node embeddings, which leverages well-established methods for calculating node proximity scores. Clarifying a point of contention in the literature, we show which step of PhUSION produces the different kinds of embeddings and what steps can be used by both. Moreover, by aggregating the PhUSION node embeddings, we obtain graph-level features that model information lost by previous graph feature learning and kernel methods. In a comprehensive empirical study with over 10 datasets, 4 tasks, and 35 methods, we systematically reveal successful design choices for node and graph-level machine learning with embeddings.

</p>
</details>

<details><summary><b>Sparse approximation in learning via neural ODEs</b>
<a href="https://arxiv.org/abs/2102.13566">arxiv:2102.13566</a>
&#x1F4C8; 8 <br>
<p>Carlos Esteve-Yagüe, Borjan Geshkovski</p></summary>
<p>

**Abstract:** We consider the neural ODE and optimal control perspective of supervised learning with $L^1(0,T;\mathbb{R}^{d_u})$ control penalties, where rather than only minimizing a final cost for the state, we integrate this cost over the entire time horizon. Under natural homogeneity assumptions on the nonlinear dynamics, we prove that any optimal control (for this cost) is sparse, in the sense that it vanishes beyond some positive stopping time. We also provide a polynomial stability estimate for the running cost of the state with respect to the time horizon. This can be seen as a \emph{turnpike property} result, for nonsmooth functionals and dynamics, and without any smallness assumptions on the data, both of which are new in the literature. In practical terms, the temporal sparsity and stability results could then be used to discard unnecessary layers in the corresponding residual neural network (ResNet), without removing relevant information.

</p>
</details>

<details><summary><b>Experiments with Rich Regime Training for Deep Learning</b>
<a href="https://arxiv.org/abs/2102.13522">arxiv:2102.13522</a>
&#x1F4C8; 8 <br>
<p>Xinyan Li, Arindam Banerjee</p></summary>
<p>

**Abstract:** In spite of advances in understanding lazy training, recent work attributes the practical success of deep learning to the rich regime with complex inductive bias. In this paper, we study rich regime training empirically with benchmark datasets, and find that while most parameters are lazy, there is always a small number of active parameters which change quite a bit during training. We show that re-initializing (resetting to their initial random values) the active parameters leads to worse generalization. Further, we show that most of the active parameters are in the bottom layers, close to the input, especially as the networks become wider. Based on such observations, we study static Layer-Wise Sparse (LWS) SGD, which only updates some subsets of layers. We find that only updating the top and bottom layers have good generalization and, as expected, only updating the top layers yields a fast algorithm. Inspired by this, we investigate probabilistic LWS-SGD, which mostly updates the top layers and occasionally updates the full network. We show that probabilistic LWS-SGD matches the generalization performance of vanilla SGD and the back-propagation time can be 2-5 times more efficient.

</p>
</details>

<details><summary><b>A Regret Minimization Approach to Iterative Learning Control</b>
<a href="https://arxiv.org/abs/2102.13478">arxiv:2102.13478</a>
&#x1F4C8; 8 <br>
<p>Naman Agarwal, Elad Hazan, Anirudha Majumdar, Karan Singh</p></summary>
<p>

**Abstract:** We consider the setting of iterative learning control, or model-based policy learning in the presence of uncertain, time-varying dynamics. In this setting, we propose a new performance metric, planning regret, which replaces the standard stochastic uncertainty assumptions with worst case regret. Based on recent advances in non-stochastic control, we design a new iterative algorithm for minimizing planning regret that is more robust to model mismatch and uncertainty. We provide theoretical and empirical evidence that the proposed algorithm outperforms existing methods on several benchmarks.

</p>
</details>

<details><summary><b>Federated Edge Learning with Misaligned Over-The-Air Computation</b>
<a href="https://arxiv.org/abs/2102.13604">arxiv:2102.13604</a>
&#x1F4C8; 7 <br>
<p>Yulin Shao, Deniz Gunduz, Soung Chang Liew</p></summary>
<p>

**Abstract:** Over-the-air computation (OAC) is a promising technique to realize fast model aggregation in the uplink of federated edge learning. OAC, however, hinges on accurate channel-gain precoding and strict synchronization among the edge devices, which are challenging in practice. As such, how to design the maximum likelihood (ML) estimator in the presence of residual channel-gain mismatch and asynchronies is an open problem. To fill this gap, this paper formulates the problem of misaligned OAC for federated edge learning and puts forth a whitened matched filtering and sampling scheme to obtain oversampled, but independent, samples from the misaligned and overlapped signals. Given the whitened samples, a sum-product ML estimator and an aligned-sample estimator are devised to estimate the arithmetic sum of the transmitted symbols. In particular, the computational complexity of our sum-product ML estimator is linear in the packet length and hence is significantly lower than the conventional ML estimator. Extensive simulations on the test accuracy versus the average received energy per symbol to noise power spectral density ratio (EsN0) yield two main results: 1) In the low EsN0 regime, the aligned-sample estimator can achieve superior test accuracy provided that the phase misalignment is non-severe. In contrast, the ML estimator does not work well due to the error propagation and noise enhancement in the estimation process. 2) In the high EsN0 regime, the ML estimator attains the optimal learning performance regardless of the severity of phase misalignment. On the other hand, the aligned-sample estimator suffers from a test-accuracy loss caused by phase misalignment.

</p>
</details>

<details><summary><b>The NPU System for the 2020 Personalized Voice Trigger Challenge</b>
<a href="https://arxiv.org/abs/2102.13552">arxiv:2102.13552</a>
&#x1F4C8; 7 <br>
<p>Jingyong Hou, Li Zhang, Yihui Fu, Qing Wang, Zhanheng Yang, Qijie Shao, Lei Xie</p></summary>
<p>

**Abstract:** This paper describes the system developed by the NPU team for the 2020 personalized voice trigger challenge. Our submitted system consists of two independently trained subsystems: a small footprint keyword spotting (KWS) system and a speaker verification (SV) system. For the KWS system, a multi-scale dilated temporal convolutional (MDTC) network is proposed to detect wake-up word (WuW). For SV system, Write something here. The KWS predicts posterior probabilities of whether an audio utterance contains WuW and estimates the location of WuW at the same time. When the posterior probability ofWuW reaches a predefined threshold, the identity information of triggered segment is determined by the SV system. On evaluation dataset, our submitted system obtains detection costs of 0.081and 0.091 in close talking and far-field tasks, respectively.

</p>
</details>

<details><summary><b>Robust Rational Polynomial Camera Modelling for SAR and Pushbroom Imaging</b>
<a href="https://arxiv.org/abs/2102.13423">arxiv:2102.13423</a>
&#x1F4C8; 7 <br>
<p>Roland Akiki, Roger Marí, Carlo de Franchis, Jean-Michel Morel, Gabriele Facciolo</p></summary>
<p>

**Abstract:** The Rational Polynomial Camera (RPC) model can be used to describe a variety of image acquisition systems in remote sensing, notably optical and Synthetic Aperture Radar (SAR) sensors. RPC functions relate 3D to 2D coordinates and vice versa, regardless of physical sensor specificities, which has made them an essential tool to harness satellite images in a generic way. This article describes a terrain-independent algorithm to accurately derive a RPC model from a set of 3D-2D point correspondences based on a regularized least squares fit. The performance of the method is assessed by varying the point correspondences and the size of the area that they cover. We test the algorithm on SAR and optical data, to derive RPCs from physical sensor models or from other RPC models after composition with corrective functions.

</p>
</details>

<details><summary><b>Class Knowledge Overlay to Visual Feature Learning for Zero-Shot Image Classification</b>
<a href="https://arxiv.org/abs/2102.13322">arxiv:2102.13322</a>
&#x1F4C8; 7 <br>
<p>Cheng Xie, Ting Zeng, Hongxin Xiang, Keqin Li, Yun Yang, Qing Liu</p></summary>
<p>

**Abstract:** New categories can be discovered by transforming semantic features into synthesized visual features without corresponding training samples in zero-shot image classification. Although significant progress has been made in generating high-quality synthesized visual features using generative adversarial networks, guaranteeing semantic consistency between the semantic features and visual features remains very challenging. In this paper, we propose a novel zero-shot learning approach, GAN-CST, based on class knowledge to visual feature learning to tackle the problem. The approach consists of three parts, class knowledge overlay, semi-supervised learning and triplet loss. It applies class knowledge overlay (CKO) to obtain knowledge not only from the corresponding class but also from other classes that have the knowledge overlay. It ensures that the knowledge-to-visual learning process has adequate information to generate synthesized visual features. The approach also applies a semi-supervised learning process to re-train knowledge-to-visual model. It contributes to reinforcing synthesized visual features generation as well as new category prediction. We tabulate results on a number of benchmark datasets demonstrating that the proposed model delivers superior performance over state-of-the-art approaches.

</p>
</details>

<details><summary><b>NEUROSPF: A tool for the Symbolic Analysis of Neural Networks</b>
<a href="https://arxiv.org/abs/2103.00124">arxiv:2103.00124</a>
&#x1F4C8; 6 <br>
<p>Muhammad Usman, Yannic Noller, Corina Pasareanu, Youcheng Sun, Divya Gopinath</p></summary>
<p>

**Abstract:** This paper presents NEUROSPF, a tool for the symbolic analysis of neural networks. Given a trained neural network model, the tool extracts the architecture and model parameters and translates them into a Java representation that is amenable for analysis using the Symbolic PathFinder symbolic execution tool. Notably, NEUROSPF encodes specialized peer classes for parsing the model's parameters, thereby enabling efficient analysis. With NEUROSPF the user has the flexibility to specify either the inputs or the network internal parameters as symbolic, promoting the application of program analysis and testing approaches from software engineering to the field of machine learning. For instance, NEUROSPF can be used for coverage-based testing and test generation, finding adversarial examples and also constraint-based repair of neural networks, thus improving the reliability of neural networks and of the applications that use them. Video URL: https://youtu.be/seal8fG78LI

</p>
</details>

<details><summary><b>Practical and Private (Deep) Learning without Sampling or Shuffling</b>
<a href="https://arxiv.org/abs/2103.00039">arxiv:2103.00039</a>
&#x1F4C8; 6 <br>
<p>Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, Zheng Xu</p></summary>
<p>

**Abstract:** We consider training models with differential privacy (DP) using mini-batch gradients. The existing state-of-the-art, Differentially Private Stochastic Gradient Descent (DP-SGD), requires privacy amplification by sampling or shuffling to obtain the best privacy/accuracy/computation trade-offs. Unfortunately, the precise requirements on exact sampling and shuffling can be hard to obtain in important practical scenarios, particularly federated learning (FL). We design and analyze a DP variant of Follow-The-Regularized-Leader (DP-FTRL) that compares favorably (both theoretically and empirically) to amplified DP-SGD, while allowing for much more flexible data access patterns. DP-FTRL does not use any form of privacy amplification.
  The code is available at https://github.com/google-research/federated/tree/master/dp_ftrl and https://github.com/google-research/DP-FTRL .

</p>
</details>

<details><summary><b>Texture-aware Video Frame Interpolation</b>
<a href="https://arxiv.org/abs/2102.13520">arxiv:2102.13520</a>
&#x1F4C8; 6 <br>
<p>Duolikun Danier, David Bull</p></summary>
<p>

**Abstract:** Temporal interpolation has the potential to be a powerful tool for video compression. Existing methods for frame interpolation do not discriminate between video textures and generally invoke a single general model capable of interpolating a wide range of video content. However, past work on video texture analysis and synthesis has shown that different textures exhibit vastly different motion characteristics and they can be divided into three classes (static, dynamic continuous and dynamic discrete). In this work, we study the impact of video textures on video frame interpolation, and propose a novel framework where, given an interpolation algorithm, separate models are trained on different textures. Our study shows that video texture has significant impact on the performance of frame interpolation models and it is beneficial to have separate models specifically adapted to these texture classes, instead of training a single model that tries to learn generic motion. Our results demonstrate that models fine-tuned using our framework achieve, on average, a 0.3dB gain in PSNR on the test set used.

</p>
</details>

<details><summary><b>Towards Explaining Expressive Qualities in Piano Recordings: Transfer of Explanatory Features via Acoustic Domain Adaptation</b>
<a href="https://arxiv.org/abs/2102.13479">arxiv:2102.13479</a>
&#x1F4C8; 6 <br>
<p>Shreyan Chowdhury, Gerhard Widmer</p></summary>
<p>

**Abstract:** Emotion and expressivity in music have been topics of considerable interest in the field of music information retrieval. In recent years, mid-level perceptual features have been suggested as means to explain computational predictions of musical emotion. We find that the diversity of musical styles and genres in the available dataset for learning these features is not sufficient for models to generalise well to specialised acoustic domains such as solo piano music. In this work, we show that by utilising unsupervised domain adaptation together with receptive-field regularised deep neural networks, it is possible to significantly improve generalisation to this domain. Additionally, we demonstrate that our domain-adapted models can better predict and explain expressive qualities in classical piano performances, as perceived and described by human listeners.

</p>
</details>

<details><summary><b>Panoramic annular SLAM with loop closure and global optimization</b>
<a href="https://arxiv.org/abs/2102.13400">arxiv:2102.13400</a>
&#x1F4C8; 6 <br>
<p>Hao Chen, Weijian Hu, Kailun Yang, Jian Bai, Kaiwei Wang</p></summary>
<p>

**Abstract:** In this paper, we propose panoramic annular simultaneous localization and mapping (PA-SLAM), a visual SLAM system based on panoramic annular lens. A hybrid point selection strategy is put forward in the tracking front-end, which ensures repeatability of keypoints and enables loop closure detection based on the bag-of-words approach. Every detected loop candidate is verified geometrically and the $Sim(3)$ relative pose constraint is estimated to perform pose graph optimization and global bundle adjustment in the back-end. A comprehensive set of experiments on real-world datasets demonstrates that the hybrid point selection strategy allows reliable loop closure detection, and the accumulated error and scale drift have been significantly reduced via global optimization, enabling PA-SLAM to reach state-of-the-art accuracy while maintaining high robustness and efficiency.

</p>
</details>

<details><summary><b>Underwater Acoustic Communication Receiver Using Deep Belief Network</b>
<a href="https://arxiv.org/abs/2102.13397">arxiv:2102.13397</a>
&#x1F4C8; 5 <br>
<p>Abigail Lee-Leon, Chau Yuen, Dorien Herremans</p></summary>
<p>

**Abstract:** Underwater environments create a challenging channel for communications. In this paper, we design a novel receiver system by exploring the machine learning technique--Deep Belief Network (DBN)-- to combat the signal distortion caused by the Doppler effect and multi-path propagation. We evaluate the performance of the proposed receiver system in both simulation experiments and sea trials. Our proposed receiver system comprises of DBN based de-noising and classification of the received signal. First, the received signal is segmented into frames before the each of these frames is individually pre-processed using a novel pixelization algorithm. Then, using the DBN based de-noising algorithm, features are extracted from these frames and used to reconstruct the received signal. Finally, DBN based classification of the reconstructed signal occurs. Our proposed DBN based receiver system does show better performance in channels influenced by the Doppler effect and multi-path propagation with a performance improvement of 13.2dB at $10^{-3}$ Bit Error Rate (BER).

</p>
</details>

<details><summary><b>Tails: Chasing Comets with the Zwicky Transient Facility and Deep Learning</b>
<a href="https://arxiv.org/abs/2102.13352">arxiv:2102.13352</a>
&#x1F4C8; 5 <br>
<p>Dmitry A. Duev, Bryce T. Bolin, Matthew J. Graham, Michael S. P. Kelley, Ashish Mahabal, Eric C. Bellm, Michael W. Coughlin, Richard Dekany, George Helou, Shrinivas R. Kulkarni, Frank J. Masci, Thomas A. Prince, Reed Riddle, Maayane T. Soumagnac, Stéfan J. van der Walt</p></summary>
<p>

**Abstract:** We present Tails, an open-source deep-learning framework for the identification and localization of comets in the image data of the Zwicky Transient Facility (ZTF), a robotic optical time-domain survey currently in operation at the Palomar Observatory in California, USA. Tails employs a custom EfficientDet-based architecture and is capable of finding comets in single images in near real time, rather than requiring multiple epochs as with traditional methods. The system achieves state-of-the-art performance with 99% recall, 0.01% false positive rate, and 1-2 pixel root mean square error in the predicted position. We report the initial results of the Tails efficiency evaluation in a production setting on the data of the ZTF Twilight survey, including the first AI-assisted discovery of a comet (C/2020 T2) and the recovery of a comet (P/2016 J3 = P/2021 A3).

</p>
</details>

<details><summary><b>Zero-Shot Learning Based on Knowledge Sharing</b>
<a href="https://arxiv.org/abs/2102.13326">arxiv:2102.13326</a>
&#x1F4C8; 5 <br>
<p>Zeng Ting, Xiang Hongxin, Xie Cheng, Yang Yun, Liu Qing</p></summary>
<p>

**Abstract:** Zero-Shot Learning (ZSL) is an emerging research that aims to solve the classification problems with very few training data. The present works on ZSL mainly focus on the mapping of learning semantic space to visual space. It encounters many challenges that obstruct the progress of ZSL research. First, the representation of the semantic feature is inadequate to represent all features of the categories. Second, the domain drift problem still exists during the transfer from semantic space to visual space. In this paper, we introduce knowledge sharing (KS) to enrich the representation of semantic features. Based on KS, we apply a generative adversarial network to generate pseudo visual features from semantic features that are very close to the real visual features. Abundant experimental results from two benchmark datasets of ZSL show that the proposed approach has a consistent improvement.

</p>
</details>

<details><summary><b>Semi-supervised Learning for COVID-19 Image Classification via ResNet</b>
<a href="https://arxiv.org/abs/2103.06140">arxiv:2103.06140</a>
&#x1F4C8; 4 <br>
<p>Lucy Nwosu, Xiangfang Li, Lijun Qian, Seungchan Kim, Xishuang Dong</p></summary>
<p>

**Abstract:** Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic in over 200 countries and territories, which has resulted in a great public health concern across the international community. Analysis of X-ray imaging data can play a critical role in timely and accurate screening and fighting against COVID-19. Supervised deep learning has been successfully applied to recognize COVID-19 pathology from X-ray imaging datasets. However, it requires a substantial amount of annotated X-ray images to train models, which is often not applicable to data analysis for emerging events such as COVID-19 outbreak, especially in the early stage of the outbreak. To address this challenge, this paper proposes a two-path semi-supervised deep learning model, ssResNet, based on Residual Neural Network (ResNet) for COVID-19 image classification, where two paths refer to a supervised path and an unsupervised path, respectively. Moreover, we design a weighted supervised loss that assigns higher weight for the minority classes in the training process to resolve the data imbalance. Experimental results on a large-scale of X-ray image dataset COVIDx demonstrate that the proposed model can achieve promising performance even when trained on very few labeled training images.

</p>
</details>

<details><summary><b>Neural Code Summarization</b>
<a href="https://arxiv.org/abs/2103.01025">arxiv:2103.01025</a>
&#x1F4C8; 4 <br>
<p>Piyush Shrivastava</p></summary>
<p>

**Abstract:** Code summarization is the task of generating readable summaries that are semantically meaningful and can accurately describe the presumed task of a software. Program comprehension has become one of the most tedious tasks for knowledge transfer. As the codebase evolves over time, the description needs to be manually updated each time with the changes made. An automatic approach is proposed to infer such captions based on benchmarked and custom datasets with comparison between the original and generated results.

</p>
</details>

<details><summary><b>CURE: Code-Aware Neural Machine Translation for Automatic Program Repair</b>
<a href="https://arxiv.org/abs/2103.00073">arxiv:2103.00073</a>
&#x1F4C8; 4 <br>
<p>Nan Jiang, Thibaud Lutellier, Lin Tan</p></summary>
<p>

**Abstract:** Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.
  We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.
  Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.

</p>
</details>

<details><summary><b>Multi-fidelity regression using artificial neural networks: efficient approximation of parameter-dependent output quantities</b>
<a href="https://arxiv.org/abs/2102.13403">arxiv:2102.13403</a>
&#x1F4C8; 4 <br>
<p>Mengwu Guo, Andrea Manzoni, Maurice Amendt, Paolo Conti, Jan S. Hesthaven</p></summary>
<p>

**Abstract:** Highly accurate numerical or physical experiments are often time-consuming or expensive to obtain. When time or budget restrictions prohibit the generation of additional data, the amount of available samples may be too limited to provide satisfactory model results. Multi-fidelity methods deal with such problems by incorporating information from other sources, which are ideally well-correlated with the high-fidelity data, but can be obtained at a lower cost. By leveraging correlations between different data sets, multi-fidelity methods often yield superior generalization when compared to models based solely on a small amount of high-fidelity data. In this work, we present the use of artificial neural networks applied to multi-fidelity regression problems. By elaborating a few existing approaches, we propose new neural network architectures for multi-fidelity regression. The introduced models are compared against a traditional multi-fidelity scheme, co-kriging. A collection of artificial benchmarks are presented to measure the performance of the analyzed models. The results show that cross-validation in combination with Bayesian optimization consistently leads to neural network models that outperform the co-kriging scheme. Additionally, we show an application of multi-fidelity regression to an engineering problem. The propagation of a pressure wave into an acoustic horn with parametrized shape and frequency is considered, and the index of reflection intensity is approximated using the multi-fidelity models. A finite element model and a reduced basis model are adopted as the high- and low-fidelity, respectively. It is shown that the multi-fidelity neural network returns outputs that achieve a comparable accuracy to those from the expensive, full-order model, using only very few full-order evaluations combined with a larger amount of inaccurate but cheap evaluations of a reduced order model.

</p>
</details>

<details><summary><b>MDA for random forests: inconsistency, and a practical solution via the Sobol-MDA</b>
<a href="https://arxiv.org/abs/2102.13347">arxiv:2102.13347</a>
&#x1F4C8; 4 <br>
<p>Clément Bénard, Sébastien da Veiga, Erwan Scornet</p></summary>
<p>

**Abstract:** Variable importance measures are the main tools to analyze the black-box mechanisms of random forests. Although the mean decrease accuracy (MDA) is widely accepted as the most efficient variable importance measure for random forests, little is known about its statistical properties. In fact, the exact MDA definition varies across the main random forest software. In this article, our objective is to rigorously analyze the behavior of the main MDA implementations. Consequently, we mathematically formalize the various implemented MDA algorithms, and then establish their limits when the sample size increases. In particular, we break down these limits in three components: the first one is related to Sobol indices, which are well-defined measures of a covariate contribution to the response variance, widely used in the sensitivity analysis field, as opposed to thethird term, whose value increases with dependence within covariates. Thus, we theoretically demonstrate that the MDA does not target the right quantity when covariates are dependent, a fact that has already been noticed experimentally. To address this issue, we define a new importance measure for random forests, the Sobol-MDA, which fixes the flaws of the original MDA. We prove the consistency of the Sobol-MDA and show thatthe Sobol-MDA empirically outperforms its competitors on both simulated and real data. An open source implementation in R and C++ is available online.

</p>
</details>

<details><summary><b>CXR-Net: An Artificial Intelligence Pipeline for Quick Covid-19 Screening of Chest X-Rays</b>
<a href="https://arxiv.org/abs/2103.00087">arxiv:2103.00087</a>
&#x1F4C8; 3 <br>
<p>Haikal Abdulah, Benjamin Huber, Sinan Lal, Hassan Abdallah, Luigi L. Palese, Hamid Soltanian-Zadeh, Domenico L. Gatti</p></summary>
<p>

**Abstract:** CXR-Net is a two-module Artificial Intelligence pipeline for the quick detection of SARS-CoV-2 from chest X-rays (CXRs). Module 1 was trained on a public dataset of 6395 CXRs with radiologist annotated lung contours to generate masks of the lungs that overlap the heart and large vasa. Module 2 is a hybrid convnet in which the first convolutional layer with learned coefficients is replaced by a layer with fixed coefficients provided by the Wavelet Scattering Transform (WST). Module 2 takes as inputs the patients CXRs and corresponding lung masks calculated by Module 1, and produces as outputs a class assignment (Covid vs. non-Covid) and high resolution heat maps that identify the SARS associated lung regions. Module 2 was trained on a dataset of CXRs from non-Covid and RT-PCR confirmed Covid patients acquired at the Henry Ford Health System (HFHS) Hospital in Detroit. All non-Covid CXRs were from pre-Covid era (2018-2019), and included images from both normal lungs and lungs affected by non-Covid pathologies. Training and test sets consisted of 2265 CXRs (1417 Covid negative, 848 Covid positive), and 1532 CXRs (945 Covid negative, 587 Covid positive), respectively. Six distinct cross-validation models, each trained on 1887 images and validated against 378 images, were combined into an ensemble model that was used to classify the CXR images of the test set with resulting Accuracy = 0.789, Precision = 0.739, Recall = 0.693, F1 score = 0.715, ROC(AUC) = 0.852.

</p>
</details>

<details><summary><b>Partitioned Graph Convolution Using Adversarial and Regression Networks for Road Travel Speed Prediction</b>
<a href="https://arxiv.org/abs/2103.00067">arxiv:2103.00067</a>
&#x1F4C8; 3 <br>
<p>Jakob Meldgaard Kjær, Lasse Kristensen, Mads Alberg Christensen</p></summary>
<p>

**Abstract:** Access to quality travel time information for roads in a road network has become increasingly important with the rising demand for real-time travel time estimation for paths within road networks. In the context of the Danish road network (DRN) dataset used in this paper, the data coverage is sparse and skewed towards arterial roads, with a coverage of 23.88% across 850,980 road segments, which makes travel time estimation difficult. Existing solutions for graph-based data processing often neglect the size of the graph, which is an apparent problem for road networks with a large amount of connected road segments. To this end, we propose a framework for predicting road segment travel speed histograms for dataless edges, based on a latent representation generated by an adversarially regularized convolutional network. We apply a partitioning algorithm to divide the graph into dense subgraphs, and then train a model for each subgraph to predict speed histograms for the nodes. The framework achieves an accuracy of 71.5% intersection and 78.5% correlation on predicting travel speed histograms using the DRN dataset. Furthermore, experiments show that partitioning the dataset into clusters increases the performance of the framework. Specifically, partitioning the road network dataset into 100 clusters, with approximately 500 road segments in each cluster, achieves a better performance than when using 10 and 20 clusters.

</p>
</details>

<details><summary><b>Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances</b>
<a href="https://arxiv.org/abs/2103.00034">arxiv:2103.00034</a>
&#x1F4C8; 3 <br>
<p>Hunter Lang, Aravind Reddy, David Sontag, Aravindan Vijayaraghavan</p></summary>
<p>

**Abstract:** Several works have shown that perturbation stable instances of the MAP inference problem in Potts models can be solved exactly using a natural linear programming (LP) relaxation. However, most of these works give few (or no) guarantees for the LP solutions on instances that do not satisfy the relatively strict perturbation stability definitions. In this work, we go beyond these stability results by showing that the LP approximately recovers the MAP solution of a stable instance even after the instance is corrupted by noise. This "noisy stable" model realistically fits with practical MAP inference problems: we design an algorithm for finding "close" stable instances, and show that several real-world instances from computer vision have nearby instances that are perturbation stable. These results suggest a new theoretical explanation for the excellent performance of this LP relaxation in practice.

</p>
</details>

<details><summary><b>Moreau-Yosida $f$-divergences</b>
<a href="https://arxiv.org/abs/2102.13416">arxiv:2102.13416</a>
&#x1F4C8; 3 <br>
<p>Dávid Terjék</p></summary>
<p>

**Abstract:** Variational representations of $f$-divergences are central to many machine learning algorithms, with Lipschitz constrained variants recently gaining attention. Inspired by this, we define the Moreau-Yosida approximation of $f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding variational formulas provide a generalization of a number of recent results, novel special cases of interest and a relaxation of the hard Lipschitz constraint. Additionally, we prove that the so-called tight variational representation of $f$-divergences can be to be taken over the quotient space of Lipschitz functions, and give a characterization of functions achieving the supremum in the variational representation. On the practical side, we propose an algorithm to calculate the tight convex conjugate of $f$-divergences compatible with automatic differentiation frameworks. As an application of our results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler, $χ^2$, reverse $χ^2$, squared Hellinger, Jensen-Shannon, Jeffreys, triangular discrimination and total variation divergences as GANs trained on CIFAR-10, leading to competitive results and a simple solution to the problem of uniqueness of the optimal critic.

</p>
</details>

<details><summary><b>Batch Bayesian Optimization on Permutations using Acquisition Weighted Kernels</b>
<a href="https://arxiv.org/abs/2102.13382">arxiv:2102.13382</a>
&#x1F4C8; 3 <br>
<p>Changyong Oh, Roberto Bondesan, Efstratios Gavves, Max Welling</p></summary>
<p>

**Abstract:** In this work we propose a batch Bayesian optimization method for combinatorial problems on permutations, which is well suited for expensive cost functions on permutations. We introduce LAW, a new efficient batch acquisition method based on the determinantal point process, using an acquisition weighted kernel. Relying on multiple parallel evaluations, LAW accelerates the search for the optimal permutation. We provide a regret analysis for our method to gain insight in its theoretical properties. We then apply the framework to permutation problems, which have so far received little attention in the Bayesian Optimization literature, despite their practical importance. We call this method LAW2ORDER. We evaluate the method on several standard combinatorial problems involving permutations such as quadratic assignment, flowshop scheduling and the traveling salesman, as well as on a structure learning task.

</p>
</details>

<details><summary><b>Knowledge Distillation Circumvents Nonlinearity for Optical Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2102.13323">arxiv:2102.13323</a>
&#x1F4C8; 3 <br>
<p>Jinlin Xiang, Shane Colburn, Arka Majumdar, Eli Shlizerman</p></summary>
<p>

**Abstract:** In recent years, Convolutional Neural Networks (CNNs) have enabled ubiquitous image processing applications. As such, CNNs require fast runtime (forward propagation) to process high-resolution visual streams in real time. This is still a challenging task even with state-of-the-art graphics and tensor processing units. The bottleneck in computational efficiency primarily occurs in the convolutional layers. Performing operations in the Fourier domain is a promising way to accelerate forward propagation since it transforms convolutions into elementwise multiplications, which are considerably faster to compute for large kernels. Furthermore, such computation could be implemented using an optical 4f system with orders of magnitude faster operation. However, a major challenge in using this spectral approach, as well as in an optical implementation of CNNs, is the inclusion of a nonlinearity between each convolutional layer, without which CNN performance drops dramatically. Here, we propose a Spectral CNN Linear Counterpart (SCLC) network architecture and develop a Knowledge Distillation (KD) approach to circumvent the need for a nonlinearity and successfully train such networks. While the KD approach is known in machine learning as an effective process for network pruning, we adapt the approach to transfer the knowledge from a nonlinear network (teacher) to a linear counterpart (student). We show that the KD approach can achieve performance that easily surpasses the standard linear version of a CNN and could approach the performance of the nonlinear network. Our simulations show that the possibility of increasing the resolution of the input image allows our proposed 4f optical linear network to perform more efficiently than a nonlinear network with the same accuracy on two fundamental image processing tasks: (i) object classification and (ii) semantic segmentation.

</p>
</details>

<details><summary><b>Visual diagnosis of the Varroa destructor parasitic mite in honeybees using object detector techniques</b>
<a href="https://arxiv.org/abs/2103.03133">arxiv:2103.03133</a>
&#x1F4C8; 2 <br>
<p>Simon Bilik, Lukas Kratochvila, Adam Ligocki, Ondrej Bostik, Tomas Zemcik, Matous Hybl, Karel Horak, Ludek Zalud</p></summary>
<p>

**Abstract:** The Varroa destructor mite is one of the most dangerous Honey Bee (Apis mellifera) parasites worldwide and the bee colonies have to be regularly monitored in order to control its spread. Here we present an object detector based method for health state monitoring of bee colonies. This method has the potential for online measurement and processing. In our experiment, we compare the YOLO and SSD object detectors along with the Deep SVDD anomaly detector. Based on the custom dataset with 600 ground-truth images of healthy and infected bees in various scenes, the detectors reached a high F1 score up to 0.874 in the infected bee detection and up to 0.727 in the detection of the Varroa Destructor mite itself. The results demonstrate the potential of this approach, which will be later used in the real-time computer vision based honey bee inspection system. To the best of our knowledge, this study is the first one using object detectors for this purpose. We expect that performance of those object detectors will enable us to inspect the health status of the honey bee colonies.

</p>
</details>

<details><summary><b>Between Post-Flaneur and Smartphone Zombie Smartphone Users Altering Visual Attention and Walking Behavior in Public Space</b>
<a href="https://arxiv.org/abs/2103.01217">arxiv:2103.01217</a>
&#x1F4C8; 2 <br>
<p>Gorsev Argin, Burak Pak, Handan Turkoglu</p></summary>
<p>

**Abstract:** The extensive use of smartphones in our everyday lives has created new modes of appropriation and behavior in public spaces. Recognition of these are essential for urban design and planning practices which help us to improve the relationship between humans, technologies, and urban environment. This study aims to research smartphone users in public space by observing their altering visual attention and walking behavior, and, in this way, to reveal the emergent new figures. For this purpose, Korenmarkt square in Ghent, Belgium, was observed for seven days in 10-min time intervals. The gaze and walking behavior of smartphone users were encoded as geo-located and temporal data, analyzed and mapped using statistical and spatial analysis methods. Developing and implementing new methods for identifying the characteristics of smartphone users, this study resulted in a nuanced characterization of novel spatial appropriations. The findings led to a better understanding and knowledge of the different behavior patterns of emergent figures such as post-flaneurs and smartphone zombies while uncovering their altering visual interactions with and movements in the public space. The results evoked questions on how researchers and designers can make use of spatial analysis methods and rethink the public space of the future as a hybrid construct integrating the virtual and the physical.

</p>
</details>

<details><summary><b>If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques</b>
<a href="https://arxiv.org/abs/2103.01035">arxiv:2103.01035</a>
&#x1F4C8; 2 <br>
<p>Mark T Keane, Eoin M Kenny, Eoin Delaney, Barry Smyth</p></summary>
<p>

**Abstract:** In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.

</p>
</details>

<details><summary><b>Deep Learning-based Compressive Beam Alignment in mmWave Vehicular Systems</b>
<a href="https://arxiv.org/abs/2103.00125">arxiv:2103.00125</a>
&#x1F4C8; 2 <br>
<p>Yuyang Wang, Nitin Jonathan Myers, Nuria González-Prelcic, Robert W. Heath Jr</p></summary>
<p>

**Abstract:** Millimeter wave vehicular channels exhibit structure that can be exploited for beam alignment with fewer channel measurements compared to exhaustive beam search. With fixed layouts of roadside buildings and regular vehicular moving trajectory, the dominant path directions of channels will likely be among a subset of beam directions instead of distributing randomly over the whole beamspace. In this paper, we propose a deep learning-based technique to design a structured compressed sensing (CS) matrix that is well suited to the underlying channel distribution for mmWave vehicular beam alignment. The proposed approach leverages both sparsity and the particular spatial structure that appears in vehicular channels. We model the compressive channel acquisition by a two-dimensional (2D) convolutional layer followed by dropout. We design fully-connected layers to optimize channel acquisition and beam alignment. We incorporate the low-resolution phase shifter constraint during neural network training by using projected gradient descent for weight updates. Furthermore, we exploit channel spectral structure to optimize the power allocated for different subcarriers. Simulations indicate that our deep learning-based approach achieves better beam alignment than standard CS techniques which use random phase shift-based design. Numerical experiments also show that one single subcarrier is sufficient to provide necessary information for beam alignment.

</p>
</details>

<details><summary><b>Revisiting Peng's Q($λ$) for Modern Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2103.00107">arxiv:2103.00107</a>
&#x1F4C8; 2 <br>
<p>Tadashi Kozuno, Yunhao Tang, Mark Rowland, Rémi Munos, Steven Kapturowski, Will Dabney, Michal Valko, David Abel</p></summary>
<p>

**Abstract:** Off-policy multi-step reinforcement learning algorithms consist of conservative and non-conservative algorithms: the former actively cut traces, whereas the latter do not. Recently, Munos et al. (2016) proved the convergence of conservative algorithms to an optimal Q-function. In contrast, non-conservative algorithms are thought to be unsafe and have a limited or no theoretical guarantee. Nonetheless, recent studies have shown that non-conservative algorithms empirically outperform conservative ones. Motivated by the empirical results and the lack of theory, we carry out theoretical analyses of Peng's Q($λ$), a representative example of non-conservative algorithms. We prove that it also converges to an optimal policy provided that the behavior policy slowly tracks a greedy policy in a way similar to conservative policy iteration. Such a result has been conjectured to be true but has not been proven. We also experiment with Peng's Q($λ$) in complex continuous control tasks, confirming that Peng's Q($λ$) often outperforms conservative algorithms despite its simplicity. These results indicate that Peng's Q($λ$), which was thought to be unsafe, is a theoretically-sound and practically effective algorithm.

</p>
</details>

<details><summary><b>Deep Quantile Aggregation</b>
<a href="https://arxiv.org/abs/2103.00083">arxiv:2103.00083</a>
&#x1F4C8; 2 <br>
<p>Taesup Kim, Rasool Fakoor, Jonas Mueller, Ryan J. Tibshirani, Alexander J. Smola</p></summary>
<p>

**Abstract:** Conditional quantile estimation is a key statistical learning challenge motivated by the need to quantify uncertainty in predictions or to model a diverse population without being overly reductive. As such, many models have been developed for this problem. Adopting a meta viewpoint, we propose a general framework (inspired by neural network optimization) for aggregating any number of conditional quantile models in order to boost predictive accuracy. We consider weighted ensembling strategies of increasing flexibility where the weights may vary over individual models, quantile levels, and feature values. An appeal of our approach is its portability: we ensure that estimated quantiles at adjacent levels do not cross by applying simple transformations through which gradients can be backpropagated, and this allows us to leverage the modern deep learning toolkit for building quantile ensembles. Our experiments confirm that ensembling can lead to big gains in accuracy, even when the constituent models are themselves powerful and flexible.

</p>
</details>

<details><summary><b>PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2103.00053">arxiv:2103.00053</a>
&#x1F4C8; 2 <br>
<p>Reyhan Kevser Keser, Aydin Ayanzadeh, Omid Abdollahi Aghdam, Caglar Kilcioglu, Behcet Ugur Toreyin, Nazim Kemal Ure</p></summary>
<p>

**Abstract:** We propose a novel knowledge distillation methodology for compressing deep neural networks. One of the most efficient methods for knowledge distillation is hint distillation, where the student model is injected with information (hints) from several different layers of the teacher model. Although the selection of hint points can drastically alter the compression performance, there is no systematic approach for selecting them, other than brute-force hyper-parameter search. We propose a clustering based hint selection methodology, where the layers of teacher model are clustered with respect to several metrics and the cluster centers are used as the hint points. The proposed approach is validated in CIFAR-100 dataset, where ResNet-110 network was used as the teacher model. Our results show that hint points selected by our algorithm results in superior compression performance with respect to state-of-the-art knowledge distillation algorithms on the same student models and datasets.

</p>
</details>

<details><summary><b>Constructing Dampened LTI Systems Generating Polynomial Bases</b>
<a href="https://arxiv.org/abs/2103.00051">arxiv:2103.00051</a>
&#x1F4C8; 2 <br>
<p>Andreas Stöckel</p></summary>
<p>

**Abstract:** We present an alternative derivation of the LTI system underlying the Legendre Delay Network (LDN). To this end, we first construct an LTI system that generates the Legendre polynomials. We then dampen the system by approximating a windowed impulse response, using what we call a "delay re-encoder". The resulting LTI system is equivalent to the LDN system. This technique can be applied to arbitrary polynomial bases, although there typically is no closed-form equation that describes the state-transition matrix.

</p>
</details>

<details><summary><b>TEC: Tensor Ensemble Classifier for Big Data</b>
<a href="https://arxiv.org/abs/2103.00025">arxiv:2103.00025</a>
&#x1F4C8; 2 <br>
<p>Peide Li, Rejaul Karim, Tapabrata Maiti</p></summary>
<p>

**Abstract:** Tensor (multidimensional array) classification problem has become very popular in modern applications such as image recognition and high dimensional spatio-temporal data analysis. Support Tensor Machine (STM) classifier, which is extended from the support vector machine, takes CANDECOMP / Parafac (CP) form of tensor data as input and predicts the data labels. The distribution-free and statistically consistent properties of STM highlight its potential in successfully handling wide varieties of data applications. Training a STM can be computationally expensive with high-dimensional tensors. However, reducing the size of tensor with a random projection technique can reduce the computational time and cost, making it feasible to handle large size tensors on regular machines. We name an STM estimated with randomly projected tensor as Random Projection-based Support Tensor Machine (RPSTM). In this work, we propose a Tensor Ensemble Classifier (TEC), which aggregates multiple RPSTMs for big tensor classification. TEC utilizes the ensemble idea to minimize the excessive classification risk brought by random projection, providing statistically consistent predictions while taking the computational advantage of RPSTM. Since each RPSTM can be estimated independently, TEC can further take advantage of parallel computing techniques and be more computationally efficient. The theoretical and numerical results demonstrate the decent performance of TEC model in high-dimensional tensor classification problems. The model prediction is statistically consistent as its risk is shown to converge to the optimal Bayes risk. Besides, we highlight the trade-off between the computational cost and the prediction risk for TEC model. The method is validated by extensive simulation and a real data example. We prepare a python package for applying TEC, which is available at our GitHub.

</p>
</details>

<details><summary><b>Heterogeneous Objectives: State-of-the-Art and Future Research</b>
<a href="https://arxiv.org/abs/2103.15546">arxiv:2103.15546</a>
&#x1F4C8; 1 <br>
<p>Richard Allmendinger, Joshua Knowles</p></summary>
<p>

**Abstract:** Multiobjective optimization problems with heterogeneous objectives are defined as those that possess significantly different types of objective function components (not just incommensurable in units or scale). For example, in a heterogeneous problem the objective function components may differ in formal computational complexity, practical evaluation effort (time, costs, or resources), determinism (stochastic vs deterministic), or some combination of all three. A particularly challenging variety of heterogeneity may occur by the combination of a time-consuming laboratory-based objective with other objectives that are evaluated using faster computer-based calculations. Perhaps more commonly, all objectives may be evaluated computationally, but some may require a lengthy simulation process while others are computed from a relatively simple closed-form calculation. In this chapter, we motivate the need for more work on the topic of heterogeneous objectives (with reference to real-world examples), expand on a basic taxonomy of heterogeneity types, and review the state of the art in tackling these problems. We give special attention to heterogeneity in evaluation time (latency) as this requires sophisticated approaches. We also present original experimental work on estimating the amount of heterogeneity in evaluation time expected in many-objective problems, given reasonable assumptions, and survey related research threads that could contribute to this area in future.

</p>
</details>

<details><summary><b>A New K means Grey Wolf Algorithm for Engineering Problems</b>
<a href="https://arxiv.org/abs/2103.05760">arxiv:2103.05760</a>
&#x1F4C8; 1 <br>
<p>Hardi M. Mohammed, Zrar Kh. Abdul, Tarik A. Rashid, Abeer Alsadoon, Nebojsa Bacanin</p></summary>
<p>

**Abstract:** Purpose: The development of metaheuristic algorithms has increased by researchers to use them extensively in the field of business, science, and engineering. One of the common metaheuristic optimization algorithms is called Grey Wolf Optimization (GWO). The algorithm works based on imitation of the wolves' searching and the process of attacking grey wolves. The main purpose of this paper to overcome the GWO problem which is trapping into local optima.
  Design or Methodology or Approach: In this paper, the K-means clustering algorithm is used to enhance the performance of the original Grey Wolf Optimization by dividing the population into different parts. The proposed algorithm is called K-means clustering Grey Wolf Optimization (KMGWO).
  Findings: Results illustrate the efficiency of KMGWO is superior to GWO. To evaluate the performance of the KMGWO, KMGWO applied to solve 10 CEC2019 benchmark test functions. Results prove that KMGWO is better compared to GWO. KMGWO is also compared to Cat Swarm Optimization (CSO), Whale Optimization Algorithm-Bat Algorithm (WOA-BAT), and WOA, so, KMGWO achieves the first rank in terms of performance. Statistical results proved that KMGWO achieved a higher significant value compared to the compared algorithms. Also, the KMGWO is used to solve a pressure vessel design problem and it has outperformed results.
  Originality/value: Results prove that KMGWO is superior to GWO. KMGWO is also compared to cat swarm optimization (CSO), whale optimization algorithm-bat algorithm (WOA-BAT), WOA, and GWO so KMGWO achieved the first rank in terms of performance. Also, the KMGWO is used to solve a classical engineering problem and it is superior

</p>
</details>

<details><summary><b>Genetic Algorithm based hyper-parameters optimization for transfer Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2103.03875">arxiv:2103.03875</a>
&#x1F4C8; 1 <br>
<p>Chen Li, JinZhe Jiang, YaQian Zhao, RenGang Li, EnDong Wang, Xin Zhang, Kun Zhao</p></summary>
<p>

**Abstract:** Hyperparameter optimization is a challenging problem in developing deep neural networks. Decision of transfer layers and trainable layers is a major task for design of the transfer convolutional neural networks (CNN). Conventional transfer CNN models are usually manually designed based on intuition. In this paper, a genetic algorithm is applied to select trainable layers of the transfer model. The filter criterion is constructed by accuracy and the counts of the trainable layers. The results show that the method is competent in this task. The system will converge with a precision of 97% in the classification of Cats and Dogs datasets, in no more than 15 generations. Moreover, backward inference according the results of the genetic algorithm shows that our method can capture the gradient features in network layers, which plays a part on understanding of the transfer AI models.

</p>
</details>

<details><summary><b>Secure Evaluation of Knowledge Graph Merging Gain</b>
<a href="https://arxiv.org/abs/2103.00082">arxiv:2103.00082</a>
&#x1F4C8; 1 <br>
<p>Leandro Eichenberger, Michael Cochez, Benjamin Heitmann, Stefan Decker</p></summary>
<p>

**Abstract:** Finding out the differences and commonalities between the knowledge of two parties is an important task. Such a comparison becomes necessary, when one party wants to determine how much it is worth to acquire the knowledge of the second party, or similarly when two parties try to determine, whether a collaboration could be beneficial. When these two parties cannot trust each other (for example, due to them being competitors) performing such a comparison is challenging as neither of them would be willing to share any of their assets. This paper addresses this problem for knowledge graphs, without a need for non-disclosure agreements nor a third party during the protocol.
  During the protocol, the intersection between the two knowledge graphs is determined in a privacy preserving fashion. This is followed by the computation of various metrics, which give an indication of the potential gain from obtaining the other parties knowledge graph, while still keeping the actual knowledge graph contents secret. The protocol makes use of blind signatures and (counting) Bloom filters to reduce the amount of leaked information. Finally, the party who wants to obtain the other's knowledge graph can get a part of such in a way that neither party is able to know beforehand which parts of the graph are obtained (i.e., they cannot choose to only get or share the good parts). After inspection of the quality of this part, the Buyer can decide to proceed with the transaction.
  The analysis of the protocol indicates that the developed protocol is secure against malicious participants. Further experimental analysis shows that the resource consumption scales linear with the number of statements in the knowledge graph.

</p>
</details>

<details><summary><b>Variation Control and Evaluation for Generative SlateRecommendations</b>
<a href="https://arxiv.org/abs/2102.13302">arxiv:2102.13302</a>
&#x1F4C8; 1 <br>
<p>Shuchang Liu, Fei Sun, Yingqiang Ge, Changhua Pei, Yongfeng Zhang</p></summary>
<p>

**Abstract:** Slate recommendation generates a list of items as a whole instead of ranking each item individually, so as to better model the intra-list positional biases and item relations. In order to deal with the enormous combinatorial space of slates, recent work considers a generative solution so that a slate distribution can be directly modeled. However, we observe that such approaches -- despite their proved effectiveness in computer vision -- suffer from a trade-off dilemma in recommender systems: when focusing on reconstruction, they easily over-fit the data and hardly generate satisfactory recommendations; on the other hand, when focusing on satisfying the user interests, they get trapped in a few items and fail to cover the item variation in slates. In this paper, we propose to enhance the accuracy-based evaluation with slate variation metrics to estimate the stochastic behavior of generative models. We illustrate that instead of reaching to one of the two undesirable extreme cases in the dilemma, a valid generative solution resides in a narrow "elbow" region in between. And we show that item perturbation can enforce slate variation and mitigate the over-concentration of generated slates, which expand the "elbow" performance to an easy-to-find region. We further propose to separate a pivot selection phase from the generation process so that the model can apply perturbation before generation. Empirical results show that this simple modification can provide even better variance with the same level of accuracy compared to post-generation perturbation methods.

</p>
</details>

<details><summary><b>What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors</b>
<a href="https://arxiv.org/abs/2102.13624">arxiv:2102.13624</a>
&#x1F4C8; 0 <br>
<p>Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, Tom Goldstein</p></summary>
<p>

**Abstract:** Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.

</p>
</details>

<details><summary><b>PredDiff: Explanations and Interactions from Conditional Expectations</b>
<a href="https://arxiv.org/abs/2102.13519">arxiv:2102.13519</a>
&#x1F4C8; 0 <br>
<p>Stefan Blücher, Johanna Vielhaben, Nils Strodthoff</p></summary>
<p>

**Abstract:** PredDiff is a model-agnostic, local attribution method that is firmly rooted in probability theory. Its simple intuition is to measure prediction changes while marginalizing features. In this work, we clarify properties of PredDiff and its connection to Shapley values. We stress important differences between classification and regression, which require a specific treatment within both formalisms. We extend PredDiff by introducing a new, well-founded measure for interaction effects between arbitrary feature subsets. The study of interaction effects represents an inevitable step towards a comprehensive understanding of black-box models and is particularly important for science applications. As opposed to Shapley values, our novel measure maintains the original linear scaling and is thus generally applicable to real-world problems.

</p>
</details>

<details><summary><b>Iterative SE(3)-Transformers</b>
<a href="https://arxiv.org/abs/2102.13419">arxiv:2102.13419</a>
&#x1F4C8; 0 <br>
<p>Fabian B. Fuchs, Edward Wagstaff, Justas Dauparas, Ingmar Posner</p></summary>
<p>

**Abstract:** When manipulating three-dimensional data, it is possible to ensure that rotational and translational symmetries are respected by applying so-called SE(3)-equivariant models. Protein structure prediction is a prominent example of a task which displays these symmetries. Recent work in this area has successfully made use of an SE(3)-equivariant model, applying an iterative SE(3)-equivariant attention mechanism. Motivated by this application, we implement an iterative version of the SE(3)-Transformer, an SE(3)-equivariant attention-based model for graph data. We address the additional complications which arise when applying the SE(3)-Transformer in an iterative fashion, compare the iterative and single-pass versions on a toy problem, and consider why an iterative model may be beneficial in some problem settings. We make the code for our implementation available to the community.

</p>
</details>

<details><summary><b>A novel notion of barycenter for probability distributions based on optimal weak mass transport</b>
<a href="https://arxiv.org/abs/2102.13380">arxiv:2102.13380</a>
&#x1F4C8; 0 <br>
<p>Elsa Cazelles, Felipe Tobar, Joaquín Fontbona</p></summary>
<p>

**Abstract:** We introduce weak barycenters of a family of probability distributions, based on the recently developed notion of optimal weak transport of mass by Gozlanet al. (2017) and Backhoff-Veraguas et al. (2020). We provide a theoretical analysis of this object and discuss its interpretation in the light of convex ordering between probability measures. In particular, we show that, rather than averaging the input distributions in a geometric way (as the Wasserstein barycenter based on classic optimal transport does) weak barycenters extract common geometric information shared by all the input distributions, encoded as a latent random variable that underlies all of them. We also provide an iterative algorithm to compute a weak barycenter for a finite family of input distributions, and a stochastic algorithm that computes them for arbitrary populations of laws. The latter approach is particularly well suited for the streaming setting, i.e., when distributions are observed sequentially. The notion of weak barycenter and our approaches to compute it are illustrated on synthetic examples, validated on 2D real-world data and compared to standard Wasserstein barycenters.

</p>
</details>

<details><summary><b>Collective Intelligence: Decentralized Learning for Android Malware Detection in IoT with Blockchain</b>
<a href="https://arxiv.org/abs/2102.13376">arxiv:2102.13376</a>
&#x1F4C8; 0 <br>
<p>Rajesh Kumar, WenYong Wang, Jay Kumar,  Zakria, Ting Yang, Waqar Ali</p></summary>
<p>

**Abstract:** The widespread significance of Android IoT devices is due to its flexibility and hardware support features which revolutionized the digital world by introducing exciting applications almost in all walks of daily life, such as healthcare, smart cities, smart environments, safety, remote sensing, and many more. Such versatile applicability gives incentive for more malware attacks. In this paper, we propose a framework which continuously aggregates multiple user trained models on non-overlapping data into single model. Specifically for malware detection task, (i) we propose a novel user (local) neural network (LNN) which trains on local distribution and (ii) then to assure the model authenticity and quality, we propose a novel smart contract which enable aggregation process over blokchain platform. The LNN model analyzes various static and dynamic features of both malware and benign whereas the smart contract verifies the malicious applications both for uploading and downloading processes in the network using stored aggregated features of local models. In this way, the proposed model not only improves malware detection accuracy using decentralized model network but also model efficacy with blockchain. We evaluate our approach with three state-of-the-art models and performed deep analyses of extracted features of the relative model.

</p>
</details>


{% endraw %}
Prev: [2021.02.25]({{ '/2021/02/25/2021.02.25.html' | relative_url }})  Next: [2021.02.27]({{ '/2021/02/27/2021.02.27.html' | relative_url }})