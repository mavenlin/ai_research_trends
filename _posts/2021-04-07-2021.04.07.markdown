## Summary for 2021-04-07, created on 2021-12-22


<details><summary><b>Regularizing Generative Adversarial Networks under Limited Data</b>
<a href="https://arxiv.org/abs/2104.03310">arxiv:2104.03310</a>
&#x1F4C8; 27 <br>
<p>Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang</p></summary>
<p>

**Abstract:** Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-the-art performance when only limited training data of the ImageNet benchmark is available.

</p>
</details>

<details><summary><b>Hollow-tree Super: a directional and scalable approach for feature importance in boosted tree models</b>
<a href="https://arxiv.org/abs/2104.03088">arxiv:2104.03088</a>
&#x1F4C8; 24 <br>
<p>Stephane Doyen, Hugh Taylor, Peter Nicholas, Lewis Crawford, Isabella Young, Michael Sughrue</p></summary>
<p>

**Abstract:** Current limitations in boosted tree modelling prevent the effective scaling to datasets with a large feature number, particularly when investigating the magnitude and directionality of various features on classification. We present a novel methodology, Hollow-tree Super (HOTS), to resolve and visualize feature importance in boosted tree models involving a large number of features. Further, HOTS allows for investigation of the directionality and magnitude various features have on classification. Using the Iris dataset, we first compare HOTS to Gini Importance, Partial Dependence Plots, and Permutation Importance, and demonstrate how HOTS resolves the weaknesses present in these methods. We then show how HOTS can be utilized in high dimensional neuroscientific data, by taking 60 Schizophrenic subjects and applying the method to determine which brain regions were most important for classification of schizophrenia as determined by the PANSS. HOTS effectively replicated and supported the findings of Gini importance, Partial Dependence Plots and Permutation importance within the Iris dataset. When applied to the schizophrenic brain dataset, HOTS was able to resolve the top 10 most important features for classification, as well as their directionality for classification and magnitude compared to other features. Cross-validation supported that these same 10 features were consistently used in the decision-making process across multiple trees, and these features were localised primarily to the occipital and parietal cortices, commonly disturbed brain regions in those with Schizophrenia. It is imperative that a methodology is developed that is able to handle the demands of working with large datasets that contain a large number of features. HOTS represents a unique way to investigate both the directionality and magnitude of feature importance when working at scale with boosted-tree modelling.

</p>
</details>

<details><summary><b>Prism: Private Verifiable Set Computation over Multi-Owner Outsourced Databases</b>
<a href="https://arxiv.org/abs/2104.03354">arxiv:2104.03354</a>
&#x1F4C8; 23 <br>
<p>Yin Li, Dhrubajyoti Ghosh, Peeyush Gupta, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma</p></summary>
<p>

**Abstract:** This paper proposes Prism, a secret sharing based approach to compute private set operations (i.e., intersection and union), as well as aggregates over outsourced databases belonging to multiple owners. Prism enables data owners to pre-load the data onto non-colluding servers and exploits the additive and multiplicative properties of secret-shares to compute the above-listed operations in (at most) two rounds of communication between the servers (storing the secret-shares) and the querier, resulting in a very efficient implementation. Also, Prism does not require communication among the servers and supports result verification techniques for each operation to detect malicious adversaries. Experimental results show that Prism scales both in terms of the number of data owners and database sizes, to which prior approaches do not scale.

</p>
</details>

<details><summary><b>Differentiable Patch Selection for Image Recognition</b>
<a href="https://arxiv.org/abs/2104.03059">arxiv:2104.03059</a>
&#x1F4C8; 20 <br>
<p>Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, Thomas Unterthiner</p></summary>
<p>

**Abstract:** Neural Networks require large amounts of memory and compute to process high resolution images, even when only a small part of the image is actually informative for the task at hand. We propose a method based on a differentiable Top-K operator to select the most relevant parts of the input to efficiently process high resolution images. Our method may be interfaced with any downstream neural network, is able to aggregate information from different patches in a flexible way, and allows the whole model to be trained end-to-end using backpropagation. We show results for traffic sign recognition, inter-patch relationship reasoning, and fine-grained recognition without using object/part bounding box annotations during training.

</p>
</details>

<details><summary><b>TenSEAL: A Library for Encrypted Tensor Operations Using Homomorphic Encryption</b>
<a href="https://arxiv.org/abs/2104.03152">arxiv:2104.03152</a>
&#x1F4C8; 15 <br>
<p>Ayoub Benaissa, Bilal Retiat, Bogdan Cebere, Alaa Eddine Belfedhal</p></summary>
<p>

**Abstract:** Machine learning algorithms have achieved remarkable results and are widely applied in a variety of domains. These algorithms often rely on sensitive and private data such as medical and financial records. Therefore, it is vital to draw further attention regarding privacy threats and corresponding defensive techniques applied to machine learning models. In this paper, we present TenSEAL, an open-source library for Privacy-Preserving Machine Learning using Homomorphic Encryption that can be easily integrated within popular machine learning frameworks. We benchmark our implementation using MNIST and show that an encrypted convolutional neural network can be evaluated in less than a second, using less than half a megabyte of communication.

</p>
</details>

<details><summary><b>Pushing the Limits of Non-Autoregressive Speech Recognition</b>
<a href="https://arxiv.org/abs/2104.03416">arxiv:2104.03416</a>
&#x1F4C8; 13 <br>
<p>Edwin G. Ng, Chung-Cheng Chiu, Yu Zhang, William Chan</p></summary>
<p>

**Abstract:** We combine recent advancements in end-to-end speech recognition to non-autoregressive automatic speech recognition. We push the limits of non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech, Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC on giant Conformer neural network architectures with SpecAugment and wav2vec2 pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets, 5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without a language model.

</p>
</details>

<details><summary><b>Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering</b>
<a href="https://arxiv.org/abs/2104.03149">arxiv:2104.03149</a>
&#x1F4C8; 10 <br>
<p>Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord</p></summary>
<p>

**Abstract:** We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer "What is the color of the sky" with "blue" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.

</p>
</details>

<details><summary><b>Utilizing Self-supervised Representations for MOS Prediction</b>
<a href="https://arxiv.org/abs/2104.03017">arxiv:2104.03017</a>
&#x1F4C8; 10 <br>
<p>Wei-Cheng Tseng, Chien-yu Huang, Wei-Tsung Kao, Yist Y. Lin, Hung-yi Lee</p></summary>
<p>

**Abstract:** Speech quality assessment has been a critical issue in speech processing for decades. Existing automatic evaluations usually require clean references or parallel ground truth data, which is infeasible when the amount of data soars. Subjective tests, on the other hand, do not need any additional clean or parallel data and correlates better to human perception. However, such a test is expensive and time-consuming because crowd work is necessary. It thus becomes highly desired to develop an automatic evaluation approach that correlates well with human perception while not requiring ground truth data. In this paper, we use self-supervised pre-trained models for MOS prediction. We show their representations can distinguish between clean and noisy audios. Then, we fine-tune these pre-trained models followed by simple linear layers in an end-to-end manner. The experiment results showed that our framework outperforms the two previous state-of-the-art models by a significant improvement on Voice Conversion Challenge 2018 and achieves comparable or superior performance on Voice Conversion Challenge 2016. We also conducted an ablation study to further investigate how each module benefits the task. The experiment results are implemented and reproducible with publicly available toolkits.

</p>
</details>

<details><summary><b>Reinforcement Learning with a Disentangled Universal Value Function for Item Recommendation</b>
<a href="https://arxiv.org/abs/2104.02981">arxiv:2104.02981</a>
&#x1F4C8; 10 <br>
<p>Kai Wang, Zhene Zou, Qilin Deng, Runze Wu, Jianrong Tao, Changjie Fan, Liang Chen, Peng Cui</p></summary>
<p>

**Abstract:** In recent years, there are great interests as well as challenges in applying reinforcement learning (RL) to recommendation systems (RS). In this paper, we summarize three key practical challenges of large-scale RL-based recommender systems: massive state and action spaces, high-variance environment, and the unspecific reward setting in recommendation. All these problems remain largely unexplored in the existing literature and make the application of RL challenging. We develop a model-based reinforcement learning framework, called GoalRec. Inspired by the ideas of world model (model-based), value function estimation (model-free), and goal-based RL, a novel disentangled universal value function designed for item recommendation is proposed. It can generalize to various goals that the recommender may have, and disentangle the stochastic environmental dynamics and high-variance reward signals accordingly. As a part of the value function, free from the sparse and high-variance reward signals, a high-capacity reward-independent world model is trained to simulate complex environmental dynamics under a certain goal. Based on the predicted environmental dynamics, the disentangled universal value function is related to the user's future trajectory instead of a monolithic state and a scalar reward. We demonstrate the superiority of GoalRec over previous approaches in terms of the above three practical challenges in a series of simulations and a real application.

</p>
</details>

<details><summary><b>DeepI2P: Image-to-Point Cloud Registration via Deep Classification</b>
<a href="https://arxiv.org/abs/2104.03501">arxiv:2104.03501</a>
&#x1F4C8; 9 <br>
<p>Jiaxin Li, Gim Hee Lee</p></summary>
<p>

**Abstract:** This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point cloud. Given an image (e.g. from a rgb-camera) and a general point cloud (e.g. from a 3D Lidar scanner) captured at different locations in the same scene, our method estimates the relative rigid transformation between the coordinate frames of the camera and Lidar. Learning common feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difficulty by converting the registration problem into a classification and inverse camera projection optimization problem. A classification neural network is designed to label whether the projection of each point in the point cloud is within or beyond the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Oxford Robotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P

</p>
</details>

<details><summary><b>Prediction with Missing Data</b>
<a href="https://arxiv.org/abs/2104.03158">arxiv:2104.03158</a>
&#x1F4C8; 9 <br>
<p>Dimitris Bertsimas, Arthur Delarue, Jean Pauphilet</p></summary>
<p>

**Abstract:** Missing information is inevitable in real-world data sets. While imputation is well-suited and theoretically sound for statistical inference, its relevance and practical implementation for out-of-sample prediction remains unsettled. We provide a theoretical analysis of widely used data imputation methods and highlight their key deficiencies in making accurate predictions. Alternatively, we propose adaptive linear regression, a new class of models that can be directly trained and evaluated on partially observed data, adapting to the set of available features. In particular, we show that certain adaptive regression models are equivalent to impute-then-regress methods where the imputation and the regression models are learned simultaneously instead of sequentially. We validate our theoretical findings and adaptive regression approach with numerical results with real-world data sets.

</p>
</details>

<details><summary><b>Multimodal Fusion Refiner Networks</b>
<a href="https://arxiv.org/abs/2104.03435">arxiv:2104.03435</a>
&#x1F4C8; 8 <br>
<p>Sethuraman Sankaran, David Yang, Ser-Nam Lim</p></summary>
<p>

**Abstract:** Tasks that rely on multi-modal information typically include a fusion module that combines information from different modalities. In this work, we develop a Refiner Fusion Network (ReFNet) that enables fusion modules to combine strong unimodal representation with strong multimodal representations. ReFNet combines the fusion network with a decoding/defusing module, which imposes a modality-centric responsibility condition. This approach addresses a big gap in existing multimodal fusion frameworks by ensuring that both unimodal and fused representations are strongly encoded in the latent fusion space. We demonstrate that the Refiner Fusion Network can improve upon performance of powerful baseline fusion modules such as multimodal transformers. The refiner network enables inducing graphical representations of the fused embeddings in the latent space, which we prove under certain conditions and is supported by strong empirical results in the numerical experiments. These graph structures are further strengthened by combining the ReFNet with a Multi-Similarity contrastive loss function. The modular nature of Refiner Fusion Network lends itself to be combined with different fusion architectures easily, and in addition, the refiner step can be applied for pre-training on unlabeled datasets, thus leveraging unsupervised data towards improving performance. We demonstrate the power of Refiner Fusion Networks on three datasets, and further show that they can maintain performance with only a small fraction of labeled data.

</p>
</details>

<details><summary><b>Few-Shot Incremental Learning with Continually Evolved Classifiers</b>
<a href="https://arxiv.org/abs/2104.03047">arxiv:2104.03047</a>
&#x1F4C8; 8 <br>
<p>Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu</p></summary>
<p>

**Abstract:** Few-shot class-incremental learning (FSCIL) aims to design machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difficulty lies in that limited data from new classes not only lead to significant overfitting issues but also exacerbate the notorious catastrophic forgetting problems. Moreover, as training data come in sequence in FSCIL, the learned classifier can only provide discriminative information in individual sessions, while FSCIL requires all classes to be involved for evaluation. In this paper, we address the FSCIL problem from two aspects. First, we adopt a simple but effective decoupled learning strategy of representations and classifiers that only the classifiers are updated in each incremental session, which avoids knowledge forgetting in the representations. By doing so, we demonstrate that a pre-trained backbone plus a non-parametric class mean classifier can beat state-of-the-art methods. Second, to make the classifiers learned on individual sessions applicable to all classes, we propose a Continually Evolved Classifier (CEC) that employs a graph model to propagate context information between classifiers for adaptation. To enable the learning of CEC, we design a pseudo incremental learning paradigm that episodically constructs a pseudo incremental learning task to optimize the graph parameters by sampling data from the base dataset. Experiments on three popular benchmark datasets, including CIFAR100, miniImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our method significantly outperforms the baselines and sets new state-of-the-art results with remarkable advantages.

</p>
</details>

<details><summary><b>Deep Interpretable Models of Theory of Mind</b>
<a href="https://arxiv.org/abs/2104.02938">arxiv:2104.02938</a>
&#x1F4C8; 8 <br>
<p>Ini Oguntola, Dana Hughes, Katia Sycara</p></summary>
<p>

**Abstract:** When developing AI systems that interact with humans, it is essential to design both a system that can understand humans, and a system that humans can understand. Most deep network based agent-modeling approaches are 1) not interpretable and 2) only model external behavior, ignoring internal mental states, which potentially limits their capability for assistance, interventions, discovering false beliefs, etc. To this end, we develop an interpretable modular neural framework for modeling the intentions of other observed entities. We demonstrate the efficacy of our approach with experiments on data from human participants on a search and rescue task in Minecraft, and show that incorporating interpretability can significantly increase predictive performance under the right conditions.

</p>
</details>

<details><summary><b>Dynabench: Rethinking Benchmarking in NLP</b>
<a href="https://arxiv.org/abs/2104.14337">arxiv:2104.14337</a>
&#x1F4C8; 7 <br>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, Adina Williams</p></summary>
<p>

**Abstract:** We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.

</p>
</details>

<details><summary><b>Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model</b>
<a href="https://arxiv.org/abs/2104.03189">arxiv:2104.03189</a>
&#x1F4C8; 7 <br>
<p>Tunazzina Islam, Dan Goldwasser</p></summary>
<p>

**Abstract:** Multiview representation learning of data can help construct coherent and contextualized users' representations on social media. This paper suggests a joint embedding model, incorporating users' social and textual information to learn contextualized user representations used for understanding their lifestyle choices. We apply our model to tweets related to two lifestyle activities, `Yoga' and `Keto diet' and use it to analyze users' activity type and motivation. We explain the data collection and annotation process in detail and provide an in-depth analysis of users from different classes based on their Twitter content. Our experiments show that our model results in performance improvements in both domains.

</p>
</details>

<details><summary><b>Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Models</b>
<a href="https://arxiv.org/abs/2104.04580">arxiv:2104.04580</a>
&#x1F4C8; 6 <br>
<p>Jian Wu, Rajal Nivargi, Sree Sai Teja Lanka, Arjun Manoj Menon, Sai Ajay Modukuri, Nishanth Nakshatri, Xin Wei, Zhuoer Wang, James Caverlee, Sarah M. Rajtmajer, C. Lee Giles</p></summary>
<p>

**Abstract:** In recent years, significant effort has been invested verifying the reproducibility and robustness of research claims in social and behavioral sciences (SBS), much of which has involved resource-intensive replication projects. In this paper, we investigate prediction of the reproducibility of SBS papers using machine learning methods based on a set of features. We propose a framework that extracts five types of features from scholarly work that can be used to support assessments of reproducibility of published research claims. Bibliometric features, venue features, and author features are collected from public APIs or extracted using open source machine learning libraries with customized parsers. Statistical features, such as p-values, are extracted by recognizing patterns in the body text. Semantic features, such as funding information, are obtained from public APIs or are extracted using natural language processing models. We analyze pairwise correlations between individual features and their importance for predicting a set of human-assessed ground truth labels. In doing so, we identify a subset of 9 top features that play relatively more important roles in predicting the reproducibility of SBS papers in our corpus. Results are verified by comparing performances of 10 supervised predictive classifiers trained on different sets of features.

</p>
</details>

<details><summary><b>Automated User Experience Testing through Multi-Dimensional Performance Impact Analysis</b>
<a href="https://arxiv.org/abs/2104.03453">arxiv:2104.03453</a>
&#x1F4C8; 6 <br>
<p>Chidera Biringa, Gokhan Kul</p></summary>
<p>

**Abstract:** Although there are many automated software testing suites, they usually focus on unit, system, and interface testing. However, especially software updates such as new security features have the potential to diminish user experience. In this paper, we propose a novel automated user experience testing methodology that learns how code changes impact the time unit and system tests take, and extrapolate user experience changes based on this information. Such a tool can be integrated into existing continuous integration pipelines, and it provides software teams immediate user experience feedback. We construct a feature set from lexical, layout, and syntactic characteristics of the code, and using Abstract Syntax Tree-Based Embeddings, we can calculate the approximate semantic distance to feed into a machine learning algorithm. In our experiments, we use several regression methods to estimate the time impact of software updates. Our open-source tool achieved 3.7% mean absolute error rate with a random forest regressor.

</p>
</details>

<details><summary><b>PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics</b>
<a href="https://arxiv.org/abs/2104.03311">arxiv:2104.03311</a>
&#x1F4C8; 6 <br>
<p>Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, Chuang Gan</p></summary>
<p>

**Abstract:** Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks.

</p>
</details>

<details><summary><b>Streaming Self-Training via Domain-Agnostic Unlabeled Images</b>
<a href="https://arxiv.org/abs/2104.03309">arxiv:2104.03309</a>
&#x1F4C8; 6 <br>
<p>Zhiqiu Lin, Deva Ramanan, Aayush Bansal</p></summary>
<p>

**Abstract:** We present streaming self-training (SST) that aims to democratize the process of learning visual recognition models such that a non-expert user can define a new task depending on their needs via a few labeled examples and minimal domain knowledge. Key to SST are two crucial observations: (1) domain-agnostic unlabeled images enable us to learn better models with a few labeled examples without any additional knowledge or supervision; and (2) learning is a continuous process and can be done by constructing a schedule of learning updates that iterates between pre-training on novel segments of the streams of unlabeled data, and fine-tuning on the small and fixed labeled dataset. This allows SST to overcome the need for a large number of domain-specific labeled and unlabeled examples, exorbitant computational resources, and domain/task-specific knowledge. In this setting, classical semi-supervised approaches require a large amount of domain-specific labeled and unlabeled examples, immense resources to process data, and expert knowledge of a particular task. Due to these reasons, semi-supervised learning has been restricted to a few places that can house required computational and human resources. In this work, we overcome these challenges and demonstrate our findings for a wide range of visual recognition tasks including fine-grained image classification, surface normal estimation, and semantic segmentation. We also demonstrate our findings for diverse domains including medical, satellite, and agricultural imagery, where there does not exist a large amount of labeled or unlabeled data.

</p>
</details>

<details><summary><b>Improving Robustness of Deep Reinforcement Learning Agents: Environment Attacks based on Critic Networks</b>
<a href="https://arxiv.org/abs/2104.03154">arxiv:2104.03154</a>
&#x1F4C8; 6 <br>
<p>Lucas Schott, Manon Césaire, Hatem Hajri, Sylvain Lamprier</p></summary>
<p>

**Abstract:** To improve policy robustness of deep reinforcement learning agents, a line of recent works focus on producing disturbances of the environment. Existing approaches of the literature to generate meaningful disturbances of the environment are adversarial reinforcement learning methods. These methods set the problem as a two-player game between the protagonist agent, which learns to perform a task in an environment, and the adversary agent, which learns to disturb the protagonist via modifications of the considered environment. Both protagonist and adversary are trained with deep reinforcement learning algorithms. Alternatively, we propose in this paper to build on gradient-based adversarial attacks, usually used for classification tasks for instance, that we apply on the critic network of the protagonist to identify efficient disturbances of the environment. Rather than learning an attacker policy, which usually reveals as very complex and unstable, we leverage the knowledge of the critic network of the protagonist, to dynamically complexify the task at each step of the learning process. We show that our method, while being faster and lighter, leads to significantly better improvements in policy robustness than existing methods of the literature.

</p>
</details>

<details><summary><b>Librispeech Transducer Model with Internal Language Model Prior Correction</b>
<a href="https://arxiv.org/abs/2104.03006">arxiv:2104.03006</a>
&#x1F4C8; 6 <br>
<p>Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, Hermann Ney</p></summary>
<p>

**Abstract:** We present our transducer model on Librispeech. We study variants to include an external language model (LM) with shallow fusion and subtract an estimated internal LM. This is justified by a Bayesian interpretation where the transducer model prior is given by the estimated internal LM. The subtraction of the internal LM gives us over 14% relative improvement over normal shallow fusion. Our transducer has a separate probability distribution for the non-blank labels which allows for easier combination with the external LM, and easier estimation of the internal LM. We additionally take care of including the end-of-sentence (EOS) probability of the external LM in the last blank probability which further improves the performance. All our code and setups are published.

</p>
</details>

<details><summary><b>Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings</b>
<a href="https://arxiv.org/abs/2104.03502">arxiv:2104.03502</a>
&#x1F4C8; 5 <br>
<p>Leonardo Pepino, Pablo Riera, Luciana Ferrer</p></summary>
<p>

**Abstract:** Emotion recognition datasets are relatively small, making the use of the more sophisticated deep learning approaches challenging. In this work, we propose a transfer learning method for speech emotion recognition where features extracted from pre-trained wav2vec 2.0 models are modeled using simple neural networks. We propose to combine the output of several layers from the pre-trained model using trainable weights which are learned jointly with the downstream model. Further, we compare performance using two different wav2vec 2.0 models, with and without finetuning for speech recognition. We evaluate our proposed approaches on two standard emotion databases IEMOCAP and RAVDESS, showing superior performance compared to results in the literature.

</p>
</details>

<details><summary><b>Convolutional Neural Network Pruning with Structural Redundancy Reduction</b>
<a href="https://arxiv.org/abs/2104.03438">arxiv:2104.03438</a>
&#x1F4C8; 5 <br>
<p>Zi Wang, Chengcheng Li, Xiangyang Wang</p></summary>
<p>

**Abstract:** Convolutional neural network (CNN) pruning has become one of the most successful network compression approaches in recent years. Existing works on network pruning usually focus on removing the least important filters in the network to achieve compact architectures. In this study, we claim that identifying structural redundancy plays a more essential role than finding unimportant filters, theoretically and empirically. We first statistically model the network pruning problem in a redundancy reduction perspective and find that pruning in the layer(s) with the most structural redundancy outperforms pruning the least important filters across all layers. Based on this finding, we then propose a network pruning approach that identifies structural redundancy of a CNN and prunes filters in the selected layer(s) with the most redundancy. Experiments on various benchmark network architectures and datasets show that our proposed approach significantly outperforms the previous state-of-the-art.

</p>
</details>

<details><summary><b>Evolutionary rates of information gain and decay in fluctuating environments</b>
<a href="https://arxiv.org/abs/2104.03406">arxiv:2104.03406</a>
&#x1F4C8; 5 <br>
<p>Nicholas Guttenberg</p></summary>
<p>

**Abstract:** In this paper, we wish to investigate the dynamics of information transfer in evolutionary dynamics. We use information theoretic tools to track how much information an evolving population has obtained and managed to retain about different environments that it is exposed to. By understanding the dynamics of information gain and loss in a static environment, we predict how that same evolutionary system would behave when the environment is fluctuating. Specifically, we anticipate a cross-over between the regime in which fluctuations improve the ability of the evolutionary system to capture environmental information and the regime in which the fluctuations inhibit it, governed by a cross-over in the timescales of information gain and decay.

</p>
</details>

<details><summary><b>Distilling and Transferring Knowledge via cGAN-generated Samples for Image Classification and Regression</b>
<a href="https://arxiv.org/abs/2104.03164">arxiv:2104.03164</a>
&#x1F4C8; 5 <br>
<p>Xin Ding, Yongwei Wang, Zuheng Xu, Z. Jane Wang, William J. Welch</p></summary>
<p>

**Abstract:** Knowledge distillation (KD) has been actively studied for image classification tasks in deep learning, aiming to improve the performance of a student model based on the knowledge from a teacher model. However, there have been very few efforts for applying KD in image regression with a scalar response, and there is no KD method applicable to both tasks. Moreover, existing KD methods often require a practitioner to carefully choose or adjust the teacher and student architectures, making these methods less scalable in practice. Furthermore, although KD is usually conducted in scenarios with limited labeled data, very few techniques are developed to alleviate such data insufficiency. To solve the above problems in an all-in-one manner, we propose in this paper a unified KD framework based on conditional generative adversarial networks (cGANs), termed cGAN-KD. Fundamentally different from existing KD methods, cGAN-KD distills and transfers knowledge from a teacher model to a student model via cGAN-generated samples. This unique mechanism makes cGAN-KD suitable for both classification and regression tasks, compatible with other KD methods, and insensitive to the teacher and student architectures. Also, benefiting from the recent advances in cGAN methodology and our specially designed subsampling and filtering procedures, cGAN-KD also performs well when labeled data are scarce. An error bound of a student model trained in the cGAN-KD framework is derived in this work, which theoretically explains why cGAN-KD takes effect and guides the implementation of cGAN-KD in practice. Extensive experiments on CIFAR-10 and Tiny-ImageNet show that we can incorporate state-of-the-art KD methods into the cGAN-KD framework to reach a new state of the art. Also, experiments on RC-49 and UTKFace demonstrate the effectiveness of cGAN-KD in image regression tasks, where existing KD methods are inapplicable.

</p>
</details>

<details><summary><b>Distributional Robustness Loss for Long-tail Learning</b>
<a href="https://arxiv.org/abs/2104.03066">arxiv:2104.03066</a>
&#x1F4C8; 5 <br>
<p>Dvir Samuel, Gal Chechik</p></summary>
<p>

**Abstract:** Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.

</p>
</details>

<details><summary><b>Theoretically Improving Graph Neural Networks via Anonymous Walk Graph Kernels</b>
<a href="https://arxiv.org/abs/2104.02995">arxiv:2104.02995</a>
&#x1F4C8; 5 <br>
<p>Qingqing Long, Yilun Jin, Yi Wu, Guojie Song</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) have achieved tremendous success in graph mining. However, the inability of GNNs to model substructures in graphs remains a significant drawback. Specifically, message-passing GNNs (MPGNNs), as the prevailing type of GNNs, have been theoretically shown unable to distinguish, detect or count many graph substructures. While efforts have been paid to complement the inability, existing works either rely on pre-defined substructure sets, thus being less flexible, or are lacking in theoretical insights. In this paper, we propose GSKN, a GNN model with a theoretically stronger ability to distinguish graph structures. Specifically, we design GSKN based on anonymous walks (AWs), flexible substructure units, and derive it upon feature mappings of graph kernels (GKs). We theoretically show that GSKN provably extends the 1-WL test, and hence the maximally powerful MPGNNs from both graph-level and node-level viewpoints. Correspondingly, various experiments are leveraged to evaluate GSKN, where GSKN outperforms a wide range of baselines, endorsing the analysis.

</p>
</details>

<details><summary><b>Unsupervised Visual Attention and Invariance for Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2104.02921">arxiv:2104.02921</a>
&#x1F4C8; 5 <br>
<p>Xudong Wang, Long Lian, Stella X. Yu</p></summary>
<p>

**Abstract:** Vision-based reinforcement learning (RL) is successful, but how to generalize it to unknown test environments remains challenging. Existing methods focus on training an RL policy that is universal to changing visual domains, whereas we focus on extracting visual foreground that is universal, feeding clean invariant vision to the RL policy learner. Our method is completely unsupervised, without manual annotations or access to environment internals.
  Given videos of actions in a training environment, we learn how to extract foregrounds with unsupervised keypoint detection, followed by unsupervised visual attention to automatically generate a foreground mask per video frame. We can then introduce artificial distractors and train a model to reconstruct the clean foreground mask from noisy observations. Only this learned model is needed during test to provide distraction-free visual input to the RL policy learner.
  Our Visual Attention and Invariance (VAI) method significantly outperforms the state-of-the-art on visual domain generalization, gaining 15 to 49% (61 to 229%) more cumulative rewards per episode on DeepMind Control (our DrawerWorld Manipulation) benchmarks. Our results demonstrate that it is not only possible to learn domain-invariant vision without any supervision, but freeing RL from visual distractions also makes the policy more focused and thus far better.

</p>
</details>

<details><summary><b>Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs</b>
<a href="https://arxiv.org/abs/2104.05752">arxiv:2104.05752</a>
&#x1F4C8; 4 <br>
<p>Sujeong Cha, Wangrui Hou, Hyun Jung, My Phung, Michael Picheny, Hong-Kwang Kuo, Samuel Thomas, Edmilson Morais</p></summary>
<p>

**Abstract:** A major focus of recent research in spoken language understanding (SLU) has been on the end-to-end approach where a single model can predict intents directly from speech inputs without intermediate transcripts. However, this approach presents some challenges. First, since speech can be considered as personally identifiable information, in some cases only automatic speech recognition (ASR) transcripts are accessible. Second, intent-labeled speech data is scarce. To address the first challenge, we propose a novel system that can predict intents from flexible types of inputs: speech, ASR transcripts, or both. We demonstrate strong performance for either modality separately, and when both speech and ASR transcripts are available, through system combination, we achieve better results than using a single input modality. To address the second challenge, we leverage a semantically robust pre-trained BERT model and adopt a cross-modal system that co-trains text embeddings and acoustic embeddings in a shared latent space. We further enhance this system by utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the text module on our target datasets. Our experiments show significant advantages for these pre-training and fine-tuning strategies, resulting in a system that achieves competitive intent-classification performance on Snips SLU and Fluent Speech Commands datasets.

</p>
</details>

<details><summary><b>Question-Driven Design Process for Explainable AI User Experiences</b>
<a href="https://arxiv.org/abs/2104.03483">arxiv:2104.03483</a>
&#x1F4C8; 4 <br>
<p>Q. Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, Daby Sow</p></summary>
<p>

**Abstract:** A pervasive design issue of AI systems is their explainability--how to provide appropriate information to help users understand the AI. The technical field of explainable AI (XAI) has produced a rich toolbox of techniques. Designers are now tasked with the challenges of how to select the most suitable XAI techniques and translate them into UX solutions. Informed by our previous work studying design challenges around XAI UX, this work proposes a design process to tackle these challenges. We review our and related prior work to identify requirements that the process should fulfill, and accordingly, propose a Question-Driven Design Process that grounds the user needs, choices of XAI techniques, design, and evaluation of XAI UX all in the user questions. We provide a mapping guide between prototypical user questions and exemplars of XAI techniques to reframe the technical space of XAI, also serving as boundary objects to support collaboration between designers and AI engineers. We demonstrate it with a use case of designing XAI for healthcare adverse events prediction, and discuss lessons learned for tackling design challenges of AI systems.

</p>
</details>

<details><summary><b>Quantum Enhanced Filter: QFilter</b>
<a href="https://arxiv.org/abs/2104.03418">arxiv:2104.03418</a>
&#x1F4C8; 4 <br>
<p>Parfait Atchade-Adelomou, Guillermo Alonso-Linaje</p></summary>
<p>

**Abstract:** Convolutional Neural Networks (CNN) are used mainly to treat problems with many images characteristic of Deep Learning. In this work, we propose a hybrid image classification model to take advantage of quantum and classical computing. The method will use the potential that convolutional networks have shown in artificial intelligence by replacing classical filters with variational quantum filters. Similarly, this work will compare with other classification methods and the system's execution on different servers. The algorithm's quantum feasibility is modelled and tested on Amazon Braket Notebook instances and experimented on the Pennylane's philosophy and framework.

</p>
</details>

<details><summary><b>Evaluating the state-of-the-art in mapping research spaces: a Brazilian case study</b>
<a href="https://arxiv.org/abs/2104.03338">arxiv:2104.03338</a>
&#x1F4C8; 4 <br>
<p>Francisco Galuppo Azevedo, Fabricio Murai</p></summary>
<p>

**Abstract:** Scientific knowledge cannot be seen as a set of isolated fields, but as a highly connected network. Understanding how research areas are connected is of paramount importance for adequately allocating funding and human resources (e.g., assembling teams to tackle multidisciplinary problems). The relationship between disciplines can be drawn from data on the trajectory of individual scientists, as researchers often make contributions in a small set of interrelated areas. Two recent works propose methods for creating research maps from scientists' publication records: by using a frequentist approach to create a transition probability matrix; and by learning embeddings (vector representations). Surprisingly, these models were evaluated on different datasets and have never been compared in the literature. In this work, we compare both models in a systematic way, using a large dataset of publication records from Brazilian researchers. We evaluate these models' ability to predict whether a given entity (scientist, institution or region) will enter a new field w.r.t. the area under the ROC curve. Moreover, we analyze how sensitive each method is to the number of publications and the number of fields associated to one entity. Last, we conduct a case study to showcase how these models can be used to characterize science dynamics in the context of Brazil.

</p>
</details>

<details><summary><b>DoubleML -- An Object-Oriented Implementation of Double Machine Learning in Python</b>
<a href="https://arxiv.org/abs/2104.03220">arxiv:2104.03220</a>
&#x1F4C8; 4 <br>
<p>Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler</p></summary>
<p>

**Abstract:** DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.

</p>
</details>

<details><summary><b>Active learning using weakly supervised signals for quality inspection</b>
<a href="https://arxiv.org/abs/2104.02973">arxiv:2104.02973</a>
&#x1F4C8; 4 <br>
<p>Antoine Cordier, Deepan Das, Pierre Gutierrez</p></summary>
<p>

**Abstract:** Because manufacturing processes evolve fast, and since production visual aspect can vary significantly on a daily basis, the ability to rapidly update machine vision based inspection systems is paramount. Unfortunately, supervised learning of convolutional neural networks requires a significant amount of annotated images for being able to learn effectively from new data. Acknowledging the abundance of continuously generated images coming from the production line and the cost of their annotation, we demonstrate it is possible to prioritize and accelerate the annotation process. In this work, we develop a methodology for learning actively, from rapidly mined, weakly (i.e. partially) annotated data, enabling a fast, direct feedback from the operators on the production line and tackling a big machine vision weakness: false positives. We also consider the problem of covariate shift, which arises inevitably due to changing conditions during data acquisition. In that regard, we show domain-adversarial training to be an efficient way to address this issue.

</p>
</details>

<details><summary><b>Deep Features for training Support Vector Machine</b>
<a href="https://arxiv.org/abs/2104.03488">arxiv:2104.03488</a>
&#x1F4C8; 3 <br>
<p>Loris Nanni, Stefano Ghidoni, Sheryl Brahnam</p></summary>
<p>

**Abstract:** Features play a crucial role in computer vision. Initially designed to detect salient elements by means of handcrafted algorithms, features are now often learned by different layers in Convolutional Neural Networks (CNNs). This paper develops a generic computer vision system based on features extracted from trained CNNs. Multiple learned features are combined into a single structure to work on different image classification tasks. The proposed system was experimentally derived by testing several approaches for extracting features from the inner layers of CNNs and using them as inputs to SVMs that are then combined by sum rule. Dimensionality reduction techniques are used to reduce the high dimensionality of inner layers. The resulting vision system is shown to significantly boost the performance of standard CNNs across a large and diverse collection of image data sets. An ensemble of different topologies using the same approach obtains state-of-the-art results on a virus data set.

</p>
</details>

<details><summary><b>Lower Bounds from Fitness Levels Made Easy</b>
<a href="https://arxiv.org/abs/2104.03372">arxiv:2104.03372</a>
&#x1F4C8; 3 <br>
<p>Benjamin Doerr, Timo Kötzing</p></summary>
<p>

**Abstract:** One of the first and easy to use techniques for proving run time bounds for evolutionary algorithms is the so-called method of fitness levels by Wegener. It uses a partition of the search space into a sequence of levels which are traversed by the algorithm in increasing order, possibly skipping levels. An easy, but often strong upper bound for the run time can then be derived by adding the reciprocals of the probabilities to leave the levels (or upper bounds for these). Unfortunately, a similarly effective method for proving lower bounds has not yet been established. The strongest such method, proposed by Sudholt (2013), requires a careful choice of the viscosity parameters $γ_{i,j}$, $0 \le i < j \le n$.
  In this paper we present two new variants of the method, one for upper and one for lower bounds. Besides the level leaving probabilities, they only rely on the probabilities that levels are visited at all. We show that these can be computed or estimated without greater difficulties and apply our method to reprove the following known results in an easy and natural way. (i) The precise run time of the (1+1) EA on \textsc{LeadingOnes}. (ii) A lower bound for the run time of the (1+1) EA on \textsc{OneMax}, tight apart from an $O(n)$ term. (iii) A lower bound for the run time of the (1+1) EA on long $k$-paths. We also prove a tighter lower bound for the run time of the (1+1) EA on jump functions by showing that, regardless of the jump size, only with probability $O(2^{-n})$ the algorithm can avoid to jump over the valley of low fitness.

</p>
</details>

<details><summary><b>Enabling Integration and Interaction for Decentralized Artificial Intelligence in Airline Disruption Management</b>
<a href="https://arxiv.org/abs/2104.03349">arxiv:2104.03349</a>
&#x1F4C8; 3 <br>
<p>Kolawole Ogunsina, Daniel DeLaurentis</p></summary>
<p>

**Abstract:** Airline disruption management traditionally seeks to address three problem dimensions: aircraft scheduling, crew scheduling, and passenger scheduling, in that order. However, current efforts have, at most, only addressed the first two problem dimensions concurrently and do not account for the propagative effects that uncertain scheduling outcomes in one dimension can have on another dimension. In addition, existing approaches for airline disruption management include human specialists who decide on necessary corrective actions for airline schedule disruptions on the day of operation. However, human specialists are limited in their ability to process copious amounts of information imperative for making robust decisions that simultaneously address all problem dimensions during disruption management. Therefore, there is a need to augment the decision-making capabilities of a human specialist with quantitative and qualitative tools that can rationalize complex interactions amongst all dimensions in airline disruption management, and provide objective insights to the specialists in the airline operations control center. To that effect, we provide a discussion and demonstration of an agnostic and systematic paradigm for enabling expeditious simultaneously-integrated recovery of all problem dimensions during airline disruption management, through an intelligent multi-agent system that employs principles from artificial intelligence and distributed ledger technology. Results indicate that our paradigm for simultaneously-integrated recovery executes in polynomial time and is effective when all the flights in the airline route network are disrupted.

</p>
</details>

<details><summary><b>Automatic Generation of Descriptive Titles for Video Clips Using Deep Learning</b>
<a href="https://arxiv.org/abs/2104.03337">arxiv:2104.03337</a>
&#x1F4C8; 3 <br>
<p>Soheyla Amirian, Khaled Rasheed, Thiab R. Taha, Hamid R. Arabnia</p></summary>
<p>

**Abstract:** Over the last decade, the use of Deep Learning in many applications produced results that are comparable to and in some cases surpassing human expert performance. The application domains include diagnosing diseases, finance, agriculture, search engines, robot vision, and many others. In this paper, we are proposing an architecture that utilizes image/video captioning methods and Natural Language Processing systems to generate a title and a concise abstract for a video. Such a system can potentially be utilized in many application domains, including, the cinema industry, video search engines, security surveillance, video databases/warehouses, data centers, and others. The proposed system functions and operates as followed: it reads a video; representative image frames are identified and selected; the image frames are captioned; NLP is applied to all generated captions together with text summarization; and finally, a title and an abstract are generated for the video. All functions are performed automatically. Preliminary results are provided in this paper using publicly available datasets. This paper is not concerned about the efficiency of the system at the execution time. We hope to be able to address execution efficiency issues in our subsequent publications.

</p>
</details>

<details><summary><b>Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images</b>
<a href="https://arxiv.org/abs/2104.03225">arxiv:2104.03225</a>
&#x1F4C8; 3 <br>
<p>Yanwen Li, Luyang Luo, Huangjing Lin, Hao Chen, Pheng-Ann Heng</p></summary>
<p>

**Abstract:** The novel coronavirus disease 2019 (COVID-19) characterized by atypical pneumonia has caused millions of deaths worldwide. Automatically segmenting lesions from chest Computed Tomography (CT) is a promising way to assist doctors in COVID-19 screening, treatment planning, and follow-up monitoring. However, voxel-wise annotations are extremely expert-demanding and scarce, especially when it comes to novel diseases, while an abundance of unlabeled data could be available. To tackle the challenge of limited annotations, in this paper, we propose an uncertainty-guided dual-consistency learning network (UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images. Specifically, we present a dual-consistency learning scheme that simultaneously imposes image transformation equivalence and feature perturbation invariance to effectively harness the knowledge from unlabeled data. We then quantify the segmentation uncertainty in two forms and employ them together to guide the consistency regularization for more reliable unsupervised learning. Extensive experiments showed that our proposed UDC-Net improves the fully supervised method by 6.3% in Dice and outperforms other competitive semi-supervised approaches by significant margins, demonstrating high potential in real-world clinical practice.

</p>
</details>

<details><summary><b>The Use of Video Captioning for Fostering Physical Activity</b>
<a href="https://arxiv.org/abs/2104.03207">arxiv:2104.03207</a>
&#x1F4C8; 3 <br>
<p>Soheyla Amirian, Abolfazl Farahani, Hamid R. Arabnia, Khaled Rasheed, Thiab R. Taha</p></summary>
<p>

**Abstract:** Video Captioning is considered to be one of the most challenging problems in the field of computer vision. Video Captioning involves the combination of different deep learning models to perform object detection, action detection, and localization by processing a sequence of image frames. It is crucial to consider the sequence of actions in a video in order to generate a meaningful description of the overall action event. A reliable, accurate, and real-time video captioning method can be used in many applications. However, this paper focuses on one application: video captioning for fostering and facilitating physical activities. In broad terms, the work can be considered to be assistive technology. Lack of physical activity appears to be increasingly widespread in many nations due to many factors, the most important being the convenience that technology has provided in workplaces. The adopted sedentary lifestyle is becoming a significant public health issue. Therefore, it is essential to incorporate more physical movements into our daily lives. Tracking one's daily physical activities would offer a base for comparison with activities performed in subsequent days. With the above in mind, this paper proposes a video captioning framework that aims to describe the activities in a video and estimate a person's daily physical activity level. This framework could potentially help people trace their daily movements to reduce an inactive lifestyle's health risks. The work presented in this paper is still in its infancy. The initial steps of the application are outlined in this paper. Based on our preliminary research, this project has great merit.

</p>
</details>

<details><summary><b>A matrix math facility for Power ISA(TM) processors</b>
<a href="https://arxiv.org/abs/2104.03142">arxiv:2104.03142</a>
&#x1F4C8; 3 <br>
<p>José E. Moreira, Kit Barton, Steven Battle, Peter Bergner, Ramon Bertran, Puneeth Bhat, Pedro Caldeira, David Edelsohn, Gordon Fossum, Brad Frey, Nemanja Ivanovic, Chip Kerchner, Vincent Lim, Shakti Kapoor, Tulio Machado Filho, Silvia Melitta Mueller, Brett Olsson, Satish Sadasivam, Baptiste Saleil, Bill Schmidt, Rajalakshmi Srinivasaraghavan, Shricharan Srivatsan, Brian Thompto, Andreas Wagner, Nelson Wu</p></summary>
<p>

**Abstract:** Power ISA(TM) Version 3.1 has introduced a new family of matrix math instructions, collectively known as the Matrix-Multiply Assist (MMA) facility. The instructions in this facility implement numerical linear algebra operations on small matrices and are meant to accelerate computation-intensive kernels, such as matrix multiplication, convolution and discrete Fourier transform. These instructions have led to a power- and area-efficient implementation of a high throughput math engine in the future POWER10 processor. Performance per core is 4 times better, at constant frequency, than the previous generation POWER9 processor. We also advocate the use of compiler built-ins as the preferred way of leveraging these instructions, which we illustrate through case studies covering matrix multiplication and convolution.

</p>
</details>

<details><summary><b>Which Neural Network to Choose for Post-Fault Localization, Dynamic State Estimation and Optimal Measurement Placement in Power Systems?</b>
<a href="https://arxiv.org/abs/2104.03115">arxiv:2104.03115</a>
&#x1F4C8; 3 <br>
<p>Andrei Afonin, Michael Chertkov</p></summary>
<p>

**Abstract:** We consider a power transmission system monitored with Phasor Measurement Units (PMUs) placed at significant, but not all, nodes of the system. Assuming that a sufficient number of distinct single-line faults, specifically pre-fault state and (not cleared) post-fault state, are recorded by the PMUs and are available for training, we, first, design a comprehensive sequence of Neural Networks (NNs) locating the faulty line. Performance of different NNs in the sequence, including Linear Regression, Feed-Forward NN, AlexNet, Graphical Convolutional NN, Neural Linear ODE and Neural Graph-based ODE, ordered according to the type and amount of the power flow physics involved, are compared for different levels of observability. Second, we build a sequence of advanced Power-System-Dynamics-Informed and Neural-ODE based Machine Learning schemes trained, given pre-fault state, to predict the post-fault state and also, in parallel, to estimate system parameters. Finally, third, and continuing to work with the first (fault localization) setting we design a (NN-based) algorithm which discovers optimal PMU placement.

</p>
</details>

<details><summary><b>Risk-Conditioned Distributional Soft Actor-Critic for Risk-Sensitive Navigation</b>
<a href="https://arxiv.org/abs/2104.03111">arxiv:2104.03111</a>
&#x1F4C8; 3 <br>
<p>Jinyoung Choi, Christopher R. Dance, Jung-eun Kim, Seulbin Hwang, Kyung-sik Park</p></summary>
<p>

**Abstract:** Modern navigation algorithms based on deep reinforcement learning (RL) show promising efficiency and robustness. However, most deep RL algorithms operate in a risk-neutral manner, making no special attempt to shield users from relatively rare but serious outcomes, even if such shielding might cause little loss of performance. Furthermore, such algorithms typically make no provisions to ensure safety in the presence of inaccuracies in the models on which they were trained, beyond adding a cost-of-collision and some domain randomization while training, in spite of the formidable complexity of the environments in which they operate. In this paper, we present a novel distributional RL algorithm that not only learns an uncertainty-aware policy, but can also change its risk measure without expensive fine-tuning or retraining. Our method shows superior performance and safety over baselines in partially-observed navigation tasks. We also demonstrate that agents trained using our method can adapt their policies to a wide range of risk measures at run-time.

</p>
</details>

<details><summary><b>Representative & Fair Synthetic Data</b>
<a href="https://arxiv.org/abs/2104.03007">arxiv:2104.03007</a>
&#x1F4C8; 3 <br>
<p>Paul Tiwald, Alexandra Ebert, Daniel T. Soukup</p></summary>
<p>

**Abstract:** Algorithms learn rules and associations based on the training data that they are exposed to. Yet, the very same data that teaches machines to understand and predict the world, contains societal and historic biases, resulting in biased algorithms with the risk of further amplifying these once put into use for decision support. Synthetic data, on the other hand, emerges with the promise to provide an unlimited amount of representative, realistic training samples, that can be shared further without disclosing the privacy of individual subjects. We present a framework to incorporate fairness constraints into the self-supervised learning process, that allows to then simulate an unlimited amount of representative as well as fair synthetic data. This framework provides a handle to govern and control for privacy as well as for bias within AI at its very source: the training data. We demonstrate the proposed approach by amending an existing generative model architecture and generating a representative as well as fair version of the UCI Adult census data set. While the relationships between attributes are faithfully retained, the gender and racial biases inherent in the original data are controlled for. This is further validated by comparing propensity scores of downstream predictive models that are trained on the original data versus the fair synthetic data. We consider representative & fair synthetic data a promising future building block to teach algorithms not on historic worlds, but rather on the worlds that we strive to live in.

</p>
</details>

<details><summary><b>Optimal Algorithms for Differentially Private Stochastic Monotone Variational Inequalities and Saddle-Point Problems</b>
<a href="https://arxiv.org/abs/2104.02988">arxiv:2104.02988</a>
&#x1F4C8; 3 <br>
<p>Digvijay Boob, Cristóbal Guzmán</p></summary>
<p>

**Abstract:** In this work, we conduct the first systematic study of stochastic variational inequality (SVI) and stochastic saddle point (SSP) problems under the constraint of differential privacy-(DP). We propose two algorithms: Noisy Stochastic Extragradient (NSEG) and Noisy Inexact Stochastic Proximal Point (NISPP). We show that sampling with replacement variants of these algorithms attain the optimal risk for DP-SVI and DP-SSP. Key to our analysis is the investigation of algorithmic stability bounds, both of which are new even in the nonprivate case, together with a novel "stability implies generalization" result for the gap functions for SVI and SSP problems. The dependence of the running time of these algorithms, with respect to the dataset size $n$, is $n^2$ for NSEG and $\widetilde{O}(n^{3/2})$ for NISPP.

</p>
</details>

<details><summary><b>Synthetic training data generation for deep learning based quality inspection</b>
<a href="https://arxiv.org/abs/2104.02980">arxiv:2104.02980</a>
&#x1F4C8; 3 <br>
<p>Pierre Gutierrez, Maria Luschkova, Antoine Cordier, Mustafa Shukor, Mona Schappert, Tim Dahmen</p></summary>
<p>

**Abstract:** Deep learning is now the gold standard in computer vision-based quality inspection systems. In order to detect defects, supervised learning is often utilized, but necessitates a large amount of annotated images, which can be costly: collecting, cleaning, and annotating the data is tedious and limits the speed at which a system can be deployed as everything the system must detect needs to be observed first. This can impede the inspection of rare defects, since very few samples can be collected by the manufacturer. In this work, we focus on simulations to solve this issue. We first present a generic simulation pipeline to render images of defective or healthy (non defective) parts. As metallic parts can be highly textured with small defects like holes, we design a texture scanning and generation method. We assess the quality of the generated images by training deep learning networks and by testing them on real data from a manufacturer. We demonstrate that we can achieve encouraging results on real defect detection using purely simulated data. Additionally, we are able to improve global performances by concatenating simulated and real data, showing that simulations can complement real images to boost performances. Lastly, using domain adaptation techniques helps improving slightly our final results.

</p>
</details>

<details><summary><b>A Question-answering Based Framework for Relation Extraction Validation</b>
<a href="https://arxiv.org/abs/2104.02934">arxiv:2104.02934</a>
&#x1F4C8; 3 <br>
<p>Jiayang Cheng, Haiyun Jiang, Deqing Yang, Yanghua Xiao</p></summary>
<p>

**Abstract:** Relation extraction is an important task in knowledge acquisition and text understanding. Existing works mainly focus on improving relation extraction by extracting effective features or designing reasonable model structures. However, few works have focused on how to validate and correct the results generated by the existing relation extraction models. We argue that validation is an important and promising direction to further improve the performance of relation extraction. In this paper, we explore the possibility of using question answering as validation. Specifically, we propose a novel question-answering based framework to validate the results from relation extraction models. Our proposed framework can be easily applied to existing relation classifiers without any additional information. We conduct extensive experiments on the popular NYT dataset to evaluate the proposed framework, and observe consistent improvements over five strong baselines.

</p>
</details>

<details><summary><b>Pretrained equivariant features improve unsupervised landmark discovery</b>
<a href="https://arxiv.org/abs/2104.02925">arxiv:2104.02925</a>
&#x1F4C8; 3 <br>
<p>Rahul Rahaman, Atin Ghosh, Alexandre H. Thiery</p></summary>
<p>

**Abstract:** Locating semantically meaningful landmark points is a crucial component of a large number of computer vision pipelines. Because of the small number of available datasets with ground truth landmark annotations, it is important to design robust unsupervised and semi-supervised methods for landmark detection.
  Many of the recent unsupervised learning methods rely on the equivariance properties of landmarks to synthetic image deformations. Our work focuses on such widely used methods and sheds light on its core problem, its inability to produce equivariant intermediate convolutional features. This finding leads us to formulate a two-step unsupervised approach that overcomes this challenge by first learning powerful pixel-based features and then use the pre-trained features to learn a landmark detector by the traditional equivariance method. Our method produces state-of-the-art results in several challenging landmark detection datasets such as the BBC Pose dataset and the Cat-Head dataset. It performs comparably on a range of other benchmarks.

</p>
</details>

<details><summary><b>Py-Feat: Python Facial Expression Analysis Toolbox</b>
<a href="https://arxiv.org/abs/2104.03509">arxiv:2104.03509</a>
&#x1F4C8; 2 <br>
<p>Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Luke J. Chang</p></summary>
<p>

**Abstract:** Studying facial expressions is a notoriously difficult endeavor. Recent advances in the field of affective computing have yielded impressive progress in automatically detecting facial expressions from pictures and videos. However, much of this work has yet to be widely disseminated in social science domains such as psychology. Current state of the art models require considerable domain expertise that is not traditionally incorporated into social science training programs. Furthermore, there is a notable absence of user-friendly and open-source software that provides a comprehensive set of tools and functions that support facial expression research. In this paper, we introduce Py-Feat, an open-source Python toolbox that provides support for detecting, preprocessing, analyzing, and visualizing facial expression data. Py-Feat makes it easy for domain experts to disseminate and benchmark computer vision models and also for end users to quickly process, analyze, and visualize face expression data. We hope this platform will facilitate increased use of facial expression data in human behavior research.

</p>
</details>

<details><summary><b>Lone Pine at SemEval-2021 Task 5: Fine-Grained Detection of Hate Speech Using BERToxic</b>
<a href="https://arxiv.org/abs/2104.03506">arxiv:2104.03506</a>
&#x1F4C8; 2 <br>
<p>Yakoob Khan, Weicheng Ma, Soroush Vosoughi</p></summary>
<p>

**Abstract:** This paper describes our approach to the Toxic Spans Detection problem (SemEval-2021 Task 5). We propose BERToxic, a system that fine-tunes a pre-trained BERT model to locate toxic text spans in a given text and utilizes additional post-processing steps to refine the boundaries. The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic. Through experiments, we show that these two post-processing steps improve the performance of our model by 4.16% on the test set. We also studied the effects of data augmentation and ensemble modeling strategies on our system. Our system significantly outperformed the provided baseline and achieved an F1-score of 0.683, placing Lone Pine in the 17th place out of 91 teams in the competition. Our code is made available at https://github.com/Yakoob-Khan/Toxic-Spans-Detection

</p>
</details>

<details><summary><b>Prototypical Region Proposal Networks for Few-Shot Localization and Classification</b>
<a href="https://arxiv.org/abs/2104.03496">arxiv:2104.03496</a>
&#x1F4C8; 2 <br>
<p>Elliott Skomski, Aaron Tuor, Andrew Avila, Lauren Phillips, Zachary New, Henry Kvinge, Courtney D. Corley, Nathan Hodas</p></summary>
<p>

**Abstract:** Recently proposed few-shot image classification methods have generally focused on use cases where the objects to be classified are the central subject of images. Despite success on benchmark vision datasets aligned with this use case, these methods typically fail on use cases involving densely-annotated, busy images: images common in the wild where objects of relevance are not the central subject, instead appearing potentially occluded, small, or among other incidental objects belonging to other classes of potential interest. To localize relevant objects, we employ a prototype-based few-shot segmentation model which compares the encoded features of unlabeled query images with support class centroids to produce region proposals indicating the presence and location of support set classes in a query image. These region proposals are then used as additional conditioning input to few-shot image classifiers. We develop a framework to unify the two stages (segmentation and classification) into an end-to-end classification model -- PRoPnet -- and empirically demonstrate that our methods improve accuracy on image datasets with natural scenes containing multiple object classes.

</p>
</details>

<details><summary><b>Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics</b>
<a href="https://arxiv.org/abs/2104.03408">arxiv:2104.03408</a>
&#x1F4C8; 2 <br>
<p>Tae Min Hong, Benjamin Carlson, Brandon Eubanks, Stephen Racz, Stephen Roche, Joerg Stelzer, Daniel Stumpp</p></summary>
<p>

**Abstract:** We present a novel implementation of classification using the machine learning / artificial intelligence method called boosted decision trees (BDT) on field programmable gate arrays (FPGA). The firmware implementation of binary classification requiring 100 training trees with a maximum depth of 4 using four input variables gives a latency value of about 10 ns, independent of the clock speed from 100 to 320 MHz in our setup. The low timing values are achieved by restructuring the BDT layout and reconfiguring its parameters. The FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our setup. A software package called fwXmachina achieves this implementation. Our intended user is an expert of custom electronics-based trigger systems in high energy physics experiments or anyone that needs decisions at the lowest latency values for real-time event classification. Two problems from high energy physics are considered, in the separation of electrons vs. photons and in the selection of vector boson fusion-produced Higgs bosons vs. the rejection of the multijet processes.

</p>
</details>

<details><summary><b>Bootstrapping of memetic from genetic evolution via inter-agent selection pressures</b>
<a href="https://arxiv.org/abs/2104.03404">arxiv:2104.03404</a>
&#x1F4C8; 2 <br>
<p>Nicholas Guttenberg, Marek Rosa</p></summary>
<p>

**Abstract:** We create an artificial system of agents (attention-based neural networks) which selectively exchange messages with each-other in order to study the emergence of memetic evolution and how memetic evolutionary pressures interact with genetic evolution of the network weights. We observe that the ability of agents to exert selection pressures on each-other is essential for memetic evolution to bootstrap itself into a state which has both high-fidelity replication of memes, as well as continuing production of new memes over time. However, in this system there is very little interaction between this memetic 'ecology' and underlying tasks driving individual fitness - the emergent meme layer appears to be neither helpful nor harmful to agents' ability to learn to solve tasks. Sourcecode for these experiments is available at https://github.com/GoodAI/memes

</p>
</details>

<details><summary><b>Monitoring Social-distance in Wide Areas during Pandemics: a Density Map and Segmentation Approach</b>
<a href="https://arxiv.org/abs/2104.03361">arxiv:2104.03361</a>
&#x1F4C8; 2 <br>
<p>Javier A. González-Trejo, Diego A. Mercado-Ravell</p></summary>
<p>

**Abstract:** With the relaxation of the containment measurements around the globe, monitoring the social distancing in crowded public places is of grate importance to prevent a new massive wave of COVID-19 infections. Recent works in that matter have limited themselves by detecting social distancing in corridors up to small crowds by detecting each person individually considering the full body in the image. In this work, we propose a new framework for monitoring the social-distance using end-to-end Deep Learning, to detect crowds violating the social-distance in wide areas where important occlusions may be present. Our framework consists in the creation of a new ground truth based on the ground truth density maps and the proposal of two different solutions, a density-map-based and a segmentation-based, to detect the crowds violating the social-distance constrain. We assess the results of both approaches by using the generated ground truth from the PET2009 and CityStreet datasets. We show that our framework performs well at providing the zones where people are not following the social-distance even when heavily occluded or far away from one camera.

</p>
</details>

<details><summary><b>Minimax Estimation of Linear Functions of Eigenvectors in the Face of Small Eigen-Gaps</b>
<a href="https://arxiv.org/abs/2104.03298">arxiv:2104.03298</a>
&#x1F4C8; 2 <br>
<p>Gen Li, Changxiao Cai, Yuantao Gu, H. Vincent Poor, Yuxin Chen</p></summary>
<p>

**Abstract:** Eigenvector perturbation analysis plays a vital role in various statistical data science applications. A large body of prior works, however, focused on establishing $\ell_{2}$ eigenvector perturbation bounds, which are often highly inadequate in addressing tasks that rely on fine-grained behavior of an eigenvector. This paper makes progress on this by studying the perturbation of linear functions of an unknown eigenvector. Focusing on two fundamental problems -- matrix denoising and principal component analysis -- in the presence of Gaussian noise, we develop a suite of statistical theory that characterizes the perturbation of arbitrary linear functions of an unknown eigenvector. In order to mitigate a non-negligible bias issue inherent to the natural "plug-in" estimator, we develop de-biased estimators that (1) achieve minimax lower bounds for a family of scenarios (modulo some logarithmic factor), and (2) can be computed in a data-driven manner without sample splitting. Noteworthily, the proposed estimators are nearly minimax optimal even when the associated eigen-gap is substantially smaller than what is required in prior theory.

</p>
</details>

<details><summary><b>Evaluation of Time Series Forecasting Models for Estimation of PM2.5 Levels in Air</b>
<a href="https://arxiv.org/abs/2104.03226">arxiv:2104.03226</a>
&#x1F4C8; 2 <br>
<p>Satvik Garg, Himanshu Jindal</p></summary>
<p>

**Abstract:** Air contamination in urban areas has risen consistently over the past few years. Due to expanding industrialization and increasing concentration of toxic gases in the climate, the air is getting more poisonous step by step at an alarming rate. Since the arrival of the Coronavirus pandemic, it is getting more critical to lessen air contamination to reduce its impact. The specialists and environmentalists are making a valiant effort to gauge air contamination levels. However, its genuinely unpredictable to mimic subatomic communication in the air, which brings about off base outcomes. There has been an ascent in using machine learning and deep learning models to foresee the results on time series data. This study adopts ARIMA, FBProphet, and deep learning models such as LSTM, 1D CNN, to estimate the concentration of PM2.5 in the environment. Our predicted results convey that all adopted methods give comparative outcomes in terms of average root mean squared error. However, the LSTM outperforms all other models with reference to mean absolute percentage error.

</p>
</details>

<details><summary><b>Adversarial Robustness Guarantees for Gaussian Processes</b>
<a href="https://arxiv.org/abs/2104.03180">arxiv:2104.03180</a>
&#x1F4C8; 2 <br>
<p>Andrea Patane, Arno Blaas, Luca Laurenti, Luca Cardelli, Stephen Roberts, Marta Kwiatkowska</p></summary>
<p>

**Abstract:** Gaussian processes (GPs) enable principled computation of model uncertainty, making them attractive for safety-critical applications. Such scenarios demand that GP decisions are not only accurate, but also robust to perturbations. In this paper we present a framework to analyse adversarial robustness of GPs, defined as invariance of the model's decision to bounded perturbations. Given a compact subset of the input space $T\subseteq \mathbb{R}^d$, a point $x^*$ and a GP, we provide provable guarantees of adversarial robustness of the GP by computing lower and upper bounds on its prediction range in $T$. We develop a branch-and-bound scheme to refine the bounds and show, for any $ε> 0$, that our algorithm is guaranteed to converge to values $ε$-close to the actual values in finitely many iterations. The algorithm is anytime and can handle both regression and classification tasks, with analytical formulation for most kernels used in practice. We evaluate our methods on a collection of synthetic and standard benchmark datasets, including SPAM, MNIST and FashionMNIST. We study the effect of approximate inference techniques on robustness and demonstrate how our method can be used for interpretability. Our empirical results suggest that the adversarial robustness of GPs increases with accurate posterior estimation.

</p>
</details>

<details><summary><b>Dense Dilated UNet: Deep Learning for 3D Photoacoustic Tomography Image Reconstruction</b>
<a href="https://arxiv.org/abs/2104.03130">arxiv:2104.03130</a>
&#x1F4C8; 2 <br>
<p>Steven Guan, Ko-Tsung Hsu, Matthias Eyassu, Parag V. Chitnis</p></summary>
<p>

**Abstract:** In photoacoustic tomography (PAT), the acoustic pressure waves produced by optical excitation are measured by an array of detectors and used to reconstruct an image. Sparse spatial sampling and limited-view detection are two common challenges faced in PAT. Reconstructing from incomplete data using standard methods results in severe streaking artifacts and blurring. We propose a modified convolutional neural network (CNN) architecture termed Dense Dilation UNet (DD-UNet) for correcting artifacts in 3D PAT. The DD-Net leverages the benefits of dense connectivity and dilated convolutions to improve CNN performance. We compare the proposed CNN in terms of image quality as measured by the multiscale structural similarity index metric to the Fully Dense UNet (FD-UNet). Results demonstrate that the DD-Net consistently outperforms the FD-UNet and is able to more reliably reconstruct smaller image features.

</p>
</details>

<details><summary><b>Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution</b>
<a href="https://arxiv.org/abs/2104.03078">arxiv:2104.03078</a>
&#x1F4C8; 2 <br>
<p>Xiu Li, Jinli Suo, Weihang Zhang, Xin Yuan, Qionghai Dai</p></summary>
<p>

**Abstract:** High quality imaging usually requires bulky and expensive lenses to compensate geometric and chromatic aberrations. This poses high constraints on the optical hash or low cost applications. Although one can utilize algorithmic reconstruction to remove the artifacts of low-end lenses, the degeneration from optical aberrations is spatially varying and the computation has to trade off efficiency for performance. For example, we need to conduct patch-wise optimization or train a large set of local deep neural networks to achieve high reconstruction performance across the whole image. In this paper, we propose a PSF aware plug-and-play deep network, which takes the aberrant image and PSF map as input and produces the latent high quality version via incorporating lens-specific deep priors, thus leading to a universal and flexible optical aberration correction method. Specifically, we pre-train a base model from a set of diverse lenses and then adapt it to a given lens by quickly refining the parameters, which largely alleviates the time and memory consumption of model learning. The approach is of high efficiency in both training and testing stages. Extensive results verify the promising applications of our proposed approach for compact low-end cameras.

</p>
</details>

<details><summary><b>Multimodal Continuous Visual Attention Mechanisms</b>
<a href="https://arxiv.org/abs/2104.03046">arxiv:2104.03046</a>
&#x1F4C8; 2 <br>
<p>António Farinhas, André F. T. Martins, Pedro M. Q. Aguiar</p></summary>
<p>

**Abstract:** Visual attention mechanisms are a key component of neural network models for computer vision. By focusing on a discrete set of objects or image regions, these mechanisms identify the most relevant features and use them to build more powerful representations. Recently, continuous-domain alternatives to discrete attention models have been proposed, which exploit the continuity of images. These approaches model attention as simple unimodal densities (e.g. a Gaussian), making them less suitable to deal with images whose region of interest has a complex shape or is composed of multiple non-contiguous patches. In this paper, we introduce a new continuous attention mechanism that produces multimodal densities, in the form of mixtures of Gaussians. We use the EM algorithm to obtain a clustering of relevant regions in the image, and a description length penalty to select the number of components in the mixture. Our densities decompose as a linear combination of unimodal attention mechanisms, enabling closed-form Jacobians for the backpropagation step. Experiments on visual question answering in the VQA-v2 dataset show competitive accuracies and a selection of regions that mimics human attention more closely in VQA-HAT. We present several examples that suggest how multimodal attention maps are naturally more interpretable than their unimodal counterparts, showing the ability of our model to automatically segregate objects from ground in complex scenes.

</p>
</details>

<details><summary><b>On-device Federated Learning with Flower</b>
<a href="https://arxiv.org/abs/2104.03042">arxiv:2104.03042</a>
&#x1F4C8; 2 <br>
<p>Akhil Mathur, Daniel J. Beutel, Pedro Porto Buarque de Gusmão, Javier Fernandez-Marques, Taner Topal, Xinchi Qiu, Titouan Parcollet, Yan Gao, Nicholas D. Lane</p></summary>
<p>

**Abstract:** Federated Learning (FL) allows edge devices to collaboratively learn a shared prediction model while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store data in the cloud. Despite the algorithmic advancements in FL, the support for on-device training of FL algorithms on edge devices remains poor. In this paper, we present an exploration of on-device FL on various smartphones and embedded devices using the Flower framework. We also evaluate the system costs of on-device FL and discuss how this quantification could be used to design more efficient FL algorithms.

</p>
</details>

<details><summary><b>Graph-based Normalizing Flow for Human Motion Generation and Reconstruction</b>
<a href="https://arxiv.org/abs/2104.03020">arxiv:2104.03020</a>
&#x1F4C8; 2 <br>
<p>Wenjie Yin, Hang Yin, Danica Kragic, Mårten Björkman</p></summary>
<p>

**Abstract:** Data-driven approaches for modeling human skeletal motion have found various applications in interactive media and social robotics. Challenges remain in these fields for generating high-fidelity samples and robustly reconstructing motion from imperfect input data, due to e.g. missed marker detection. In this paper, we propose a probabilistic generative model to synthesize and reconstruct long horizon motion sequences conditioned on past information and control signals, such as the path along which an individual is moving. Our method adapts the existing work MoGlow by introducing a new graph-based model. The model leverages the spatial-temporal graph convolutional network (ST-GCN) to effectively capture the spatial structure and temporal correlation of skeletal motion data at multiple scales. We evaluate the models on a mixture of motion capture datasets of human locomotion with foot-step and bone-length analysis. The results demonstrate the advantages of our model in reconstructing missing markers and achieving comparable results on generating realistic future poses. When the inputs are imperfect, our model shows improvements on robustness of generation.

</p>
</details>

<details><summary><b>The AS-NU System for the M2VoC Challenge</b>
<a href="https://arxiv.org/abs/2104.03009">arxiv:2104.03009</a>
&#x1F4C8; 2 <br>
<p>Cheng-Hung Hu, Yi-Chiao Wu, Wen-Chin Huang, Yu-Huai Peng, Yu-Wen Chen, Pin-Jui Ku, Tomoki Toda, Yu Tsao, Hsin-Min Wang</p></summary>
<p>

**Abstract:** This paper describes the AS-NU systems for two tracks in MultiSpeaker Multi-Style Voice Cloning Challenge (M2VoC). The first track focuses on using a small number of 100 target utterances for voice cloning, while the second track focuses on using only 5 target utterances for voice cloning. Due to the serious lack of data in the second track, we selected the speaker most similar to the target speaker from the training data of the TTS system, and used the speaker's utterances and the given 5 target utterances to fine-tune our model. The evaluation results show that our systems on the two tracks perform similarly in terms of quality, but there is still a clear gap between the similarity score of the second track and the similarity score of the first track.

</p>
</details>

<details><summary><b>Siamese Neural Network with Joint Bayesian Model Structure for Speaker Verification</b>
<a href="https://arxiv.org/abs/2104.03004">arxiv:2104.03004</a>
&#x1F4C8; 2 <br>
<p>Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai</p></summary>
<p>

**Abstract:** Generative probability models are widely used for speaker verification (SV). However, the generative models are lack of discriminative feature selection ability. As a hypothesis test, the SV can be regarded as a binary classification task which can be designed as a Siamese neural network (SiamNN) with discriminative training. However, in most of the discriminative training for SiamNN, only the distribution of pair-wised sample distances is considered, and the additional discriminative information in joint distribution of samples is ignored. In this paper, we propose a novel SiamNN with consideration of the joint distribution of samples. The joint distribution of samples is first formulated based on a joint Bayesian (JB) based generative model, then a SiamNN is designed with dense layers to approximate the factorized affine transforms as used in the JB model. By initializing the SiamNN with the learned model parameters of the JB model, we further train the model parameters with the pair-wised samples as a binary discrimination task for SV. We carried out SV experiments on data corpus of speakers in the wild (SITW) and VoxCeleb. Experimental results showed that our proposed model improved the performance with a large margin compared with state of the art models for SV.

</p>
</details>

<details><summary><b>CNN Based Segmentation of Infarcted Regions in Acute Cerebral Stroke Patients From Computed Tomography Perfusion Imaging</b>
<a href="https://arxiv.org/abs/2104.03002">arxiv:2104.03002</a>
&#x1F4C8; 2 <br>
<p>Luca Tomasetti, Kjersti Engan, Mahdieh Khanmohammadi, Kathinka Dæhli Kurz</p></summary>
<p>

**Abstract:** More than 13 million people suffer from ischemic cerebral stroke worldwide each year. Thrombolytic treatment can reduce brain damage but has a narrow treatment window. Computed Tomography Perfusion imaging is a commonly used primary assessment tool for stroke patients, and typically the radiologists will evaluate resulting parametric maps to estimate the affected areas, dead tissue (core), and the surrounding tissue at risk (penumbra), to decide further treatments. Different work has been reported, suggesting thresholds, and semi-automated methods, and in later years deep neural networks, for segmenting infarction areas based on the parametric maps. However, there is no consensus in terms of which thresholds to use, or how to combine the information from the parametric maps, and the presented methods all have limitations in terms of both accuracy and reproducibility.
  We propose a fully automated convolutional neural network based segmentation method that uses the full four-dimensional computed tomography perfusion dataset as input, rather than the pre-filtered parametric maps. The suggested network is tested on an available dataset as a proof-of-concept, with very encouraging results. Cross-validated results show averaged Dice score of 0.78 and 0.53, and an area under the receiver operating characteristic curve of 0.97 and 0.94 for penumbra and core respectively

</p>
</details>

<details><summary><b>Universal Adversarial Training with Class-Wise Perturbations</b>
<a href="https://arxiv.org/abs/2104.03000">arxiv:2104.03000</a>
&#x1F4C8; 2 <br>
<p>Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon</p></summary>
<p>

**Abstract:** Despite their overwhelming success on a wide range of applications, convolutional neural networks (CNNs) are widely recognized to be vulnerable to adversarial examples. This intriguing phenomenon led to a competition between adversarial attacks and defense techniques. So far, adversarial training is the most widely used method for defending against adversarial attacks. It has also been extended to defend against universal adversarial perturbations (UAPs). The SOTA universal adversarial training (UAT) method optimizes a single perturbation for all training samples in the mini-batch. In this work, we find that a UAP does not attack all classes equally. Inspired by this observation, we identify it as the source of the model having unbalanced robustness. To this end, we improve the SOTA UAT by proposing to utilize class-wise UAPs during adversarial training. On multiple benchmark datasets, our class-wise UAT leads superior performance for both clean accuracy and adversarial robustness against universal attack.

</p>
</details>

<details><summary><b>Few-Shot Meta-Learning on Point Cloud for Semantic Segmentation</b>
<a href="https://arxiv.org/abs/2104.02979">arxiv:2104.02979</a>
&#x1F4C8; 2 <br>
<p>Xudong Li, Li Feng, Lei Li, Chen Wang</p></summary>
<p>

**Abstract:** The promotion of construction robots can solve the problem of human resource shortage and improve the quality of decoration. To help the construction robots obtain environmental information, we need to use 3D point cloud, which is widely used in robotics, autonomous driving, and so on. With a good understanding of environmental information, construction robots can work better. However, the dynamic changes of 3D point cloud data may bring difficulties for construction robots to understand environmental information, such as when construction robots renovate houses. The paper proposes a semantic segmentation method for point cloud based on meta-learning. The method includes a basic learning module and a meta-learning module. The basic learning module is responsible for learning data features and evaluating the model, while the meta-learning module is responsible for updating the parameters of the model and improving the model generalization ability. In our work, we pioneered the method of producing datasets for meta-learning in 3D scenes, as well as demonstrated that the Model-Agnostic Meta-Learning (MAML) algorithm can be applied to process 3D point cloud data. At the same time, experiments show that our method can allow the model to be quickly applied to new environments with a few samples. Our method has important applications.

</p>
</details>

<details><summary><b>MPN: Multimodal Parallel Network for Audio-Visual Event Localization</b>
<a href="https://arxiv.org/abs/2104.02971">arxiv:2104.02971</a>
&#x1F4C8; 2 <br>
<p>Jiashuo Yu, Ying Cheng, Rui Feng</p></summary>
<p>

**Abstract:** Audio-visual event localization aims to localize an event that is both audible and visible in the wild, which is a widespread audio-visual scene analysis task for unconstrained videos. To address this task, we propose a Multimodal Parallel Network (MPN), which can perceive global semantics and unmixed local information parallelly. Specifically, our MPN framework consists of a classification subnetwork to predict event categories and a localization subnetwork to predict event boundaries. The classification subnetwork is constructed by the Multimodal Co-attention Module (MCM) and obtains global contexts. The localization subnetwork consists of Multimodal Bottleneck Attention Module (MBAM), which is designed to extract fine-grained segment-level contents. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance both in fully supervised and weakly supervised settings on the Audio-Visual Event (AVE) dataset.

</p>
</details>

<details><summary><b>The art of defense: letting networks fool the attacker</b>
<a href="https://arxiv.org/abs/2104.02963">arxiv:2104.02963</a>
&#x1F4C8; 2 <br>
<p>Jinlai Zhang, Binbin Liu, Lyvjie Chen, Bo Ouyang, Jihong Zhu, Minchi Kuang, Houqing Wang, Yanmei Meng</p></summary>
<p>

**Abstract:** Some deep neural networks are invariant to some input transformations, such as Pointnet is permutation invariant to the input point cloud. In this paper, we demonstrated this property could be powerful in defense of gradient-based attacks. Specifically, we apply random input transformation which is invariant to the networks we want to defend. Extensive experiments demonstrate that the proposed scheme defeats various gradient-based attackers in the targeted attack setting, and breaking the attack accuracy into nearly zero. Our code is available at: {\footnotesize{\url{https://github.com/cuge1995/IT-Defense}}}.

</p>
</details>

<details><summary><b>The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL</b>
<a href="https://arxiv.org/abs/2104.02959">arxiv:2104.02959</a>
&#x1F4C8; 2 <br>
<p>Badr AlKhamissi, Muhammad ElNokrashy, Michael Spranger</p></summary>
<p>

**Abstract:** In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to reveal two classes of neurons that emerge in the agent's working memory (an epLSTM cell) when trained using episodic meta-RL on an episodic variant of the Harlow visual fixation task. Specifically, Abstract neurons encode knowledge shared across tasks, while Episodic neurons carry information relevant for a specific episode's task.

</p>
</details>

<details><summary><b>Bootstrapping Your Own Positive Sample: Contrastive Learning With Electronic Health Record Data</b>
<a href="https://arxiv.org/abs/2104.02932">arxiv:2104.02932</a>
&#x1F4C8; 2 <br>
<p>Tingyi Wanyan, Jing Zhang, Ying Ding, Ariful Azad, Zhangyang Wang, Benjamin S Glicksberg</p></summary>
<p>

**Abstract:** Electronic Health Record (EHR) data has been of tremendous utility in Artificial Intelligence (AI) for healthcare such as predicting future clinical events. These tasks, however, often come with many challenges when using classical machine learning models due to a myriad of factors including class imbalance and data heterogeneity (i.e., the complex intra-class variances). To address some of these research gaps, this paper leverages the exciting contrastive learning framework and proposes a novel contrastive regularized clinical classification model. The contrastive loss is found to substantially augment EHR-based prediction: it effectively characterizes the similar/dissimilar patterns (by its "push-and-pull" form), meanwhile mitigating the highly skewed class distribution by learning more balanced feature spaces (as also echoed by recent findings). In particular, when naively exporting the contrastive learning to the EHR data, one hurdle is in generating positive samples, since EHR data is not as amendable to data augmentation as image data. To this end, we have introduced two unique positive sampling strategies specifically tailored for EHR data: a feature-based positive sampling that exploits the feature space neighborhood structure to reinforce the feature learning; and an attribute-based positive sampling that incorporates pre-generated patient similarity metrics to define the sample proximity. Both sampling approaches are designed with an awareness of unique high intra-class variance in EHR data. Our overall framework yields highly competitive experimental results in predicting the mortality risk on real-world COVID-19 EHR data with a total of 5,712 patients admitted to a large, urban health system. Specifically, our method reaches a high AUROC prediction score of 0.959, which outperforms other baselines and alternatives: cross-entropy(0.873) and focal loss(0.931).

</p>
</details>

<details><summary><b>Sparse Oblique Decision Trees: A Tool to Understand and Manipulate Neural Net Features</b>
<a href="https://arxiv.org/abs/2104.02922">arxiv:2104.02922</a>
&#x1F4C8; 2 <br>
<p>Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán, Arman Zharmagambetov</p></summary>
<p>

**Abstract:** The widespread deployment of deep nets in practical applications has lead to a growing desire to understand how and why such black-box methods perform prediction. Much work has focused on understanding what part of the input pattern (an image, say) is responsible for a particular class being predicted, and how the input may be manipulated to predict a different class. We focus instead on understanding which of the internal features computed by the neural net are responsible for a particular class. We achieve this by mimicking part of the neural net with an oblique decision tree having sparse weight vectors at the decision nodes. Using the recently proposed Tree Alternating Optimization (TAO) algorithm, we are able to learn trees that are both highly accurate and interpretable. Such trees can faithfully mimic the part of the neural net they replaced, and hence they can provide insights into the deep net black box. Further, we show we can easily manipulate the neural net features in order to make the net predict, or not predict, a given class, thus showing that it is possible to carry out adversarial attacks at the level of the features. These insights and manipulations apply globally to the entire training and test set, not just at a local (single-instance) level. We demonstrate this robustly in the MNIST and ImageNet datasets with LeNet5 and VGG networks.

</p>
</details>

<details><summary><b>Zero-bias Deep Learning Enabled Quick and Reliable Abnormality Detection in IoT</b>
<a href="https://arxiv.org/abs/2105.15098">arxiv:2105.15098</a>
&#x1F4C8; 1 <br>
<p>Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song</p></summary>
<p>

**Abstract:** Abnormality detection is essential to the performance of safety-critical and latency-constrained systems. However, as systems are becoming increasingly complicated with a large quantity of heterogeneous data, conventional statistical change point detection methods are becoming less effective and efficient. Although Deep Learning (DL) and Deep Neural Networks (DNNs) are increasingly employed to handle heterogeneous data, they still lack theoretic assurable performance and explainability. This paper integrates zero-bias DNN and Quickest Event Detection algorithms to provide a holistic framework for quick and reliable detection of both abnormalities and time-dependent abnormal events in the Internet of Things (IoT). We first use the zero-bias dense layer to increase the explainability of DNN. We provide a solution to convert zero-bias DNN classifiers into performance assured binary abnormality detectors. Using the converted abnormality detector, we then present a sequential quickest detection scheme that provides the theoretically assured lowest abnormal event detection delay under false alarm constraints. Finally, we demonstrate the effectiveness of the framework using both massive signal records from real-world aviation communication systems and simulated data. Code and data of our work is available at \url{https://github.com/pcwhy/AbnormalityDetectionInZbDNN}

</p>
</details>

<details><summary><b>An Object Detection based Solver for Google's Image reCAPTCHA v2</b>
<a href="https://arxiv.org/abs/2104.03366">arxiv:2104.03366</a>
&#x1F4C8; 1 <br>
<p>Md Imran Hossen, Yazhou Tu, Md Fazle Rabby, Md Nazmul Islam, Hui Cao, Xiali Hei</p></summary>
<p>

**Abstract:** Previous work showed that reCAPTCHA v2's image challenges could be solved by automated programs armed with Deep Neural Network (DNN) image classifiers and vision APIs provided by off-the-shelf image recognition services. In response to emerging threats, Google has made significant updates to its image reCAPTCHA v2 challenges that can render the prior approaches ineffective to a great extent. In this paper, we investigate the robustness of the latest version of reCAPTCHA v2 against advanced object detection based solvers. We propose a fully automated object detection based system that breaks the most advanced challenges of reCAPTCHA v2 with an online success rate of 83.25%, the highest success rate to date, and it takes only 19.93 seconds (including network delays) on average to crack a challenge. We also study the updated security features of reCAPTCHA v2, such as anti-recognition mechanisms, improved anti-bot detection techniques, and adjustable security preferences. Our extensive experiments show that while these security features can provide some resistance against automated attacks, adversaries can still bypass most of them. Our experimental findings indicate that the recent advances in object detection technologies pose a severe threat to the security of image captcha designs relying on simple object detection as their underlying AI problem.

</p>
</details>

<details><summary><b>Single-Qubit Fidelity Assessment of Quantum Annealing Hardware</b>
<a href="https://arxiv.org/abs/2104.03335">arxiv:2104.03335</a>
&#x1F4C8; 1 <br>
<p>Jon Nelson, Marc Vuffray, Andrey Y. Lokhov, Carleton Coffrin</p></summary>
<p>

**Abstract:** As a wide variety of quantum computing platforms become available, methods for assessing and comparing the performance of these devices are of increasing interest and importance. Inspired by the success of single-qubit error rate computations for tracking the progress of gate-based quantum computers, this work proposes a Quantum Annealing Single-qubit Assessment (QASA) protocol for quantifying the performance of individual qubits in quantum annealing computers. The proposed protocol scales to large quantum annealers with thousands of qubits and provides unique insights into the distribution of qubit properties within a particular hardware device. The efficacy of the QASA protocol is demonstrated by analyzing the properties of a D-Wave 2000Q system, revealing unanticipated correlations in the qubit performance of that device. A study repeating the QASA protocol at different annealing times highlights how the method can be utilized to understand the impact of annealing parameters on qubit performance. Overall, the proposed QASA protocol provides a useful tool for assessing the performance of current and emerging quantum annealing devices.

</p>
</details>

<details><summary><b>Semi-Supervised Classification of Social Media Posts: Identifying Sex-Industry Posts to Enable Better Support for Those Experiencing Sex-Trafficking</b>
<a href="https://arxiv.org/abs/2104.03233">arxiv:2104.03233</a>
&#x1F4C8; 1 <br>
<p>Ellie Simonson</p></summary>
<p>

**Abstract:** Social media is both helpful and harmful to the work against sex trafficking. On one hand, social workers carefully use social media to support people experiencing sex trafficking. On the other hand, traffickers use social media to groom and recruit people into trafficking situations. There is the opportunity to use social media data to better provide support for people experiencing trafficking.
  While AI and Machine Learning (ML) have been used in work against sex trafficking, they predominantly focus on detecting Child Sexual Abuse Material. Work using social media data has not been done with the intention to provide community level support to people of all ages experiencing trafficking. Within this context, this thesis explores the use of semi-supervised classification to identify social media posts that are a part of the sex industry.
  Several methods were explored for ML. However, the primary method used was semi-supervised learning, which has the benefit of providing automated classification with a limited set of labelled data. Social media posts were embedded into low-dimensional vectors using FastText and Doc2Vec models. The data were then clustered using k-means clustering, and cross-validation was used to determine label propagation accuracy.
  The results of the semi-supervised algorithm were encouraging. The FastText CBOW model provided 98.6% accuracy to over 12,000 posts in clusters where label propagation was applied. The results of this thesis suggest that further semi-supervised learning, in conjunction with manual labeling, may allow for the entire dataset containing over 50,000 posts to be accurately labeled.
  A fully labeled dataset could be used to develop a tool to identify an overview of where and when social media is used within the sex industry. This could be used to help determine better ways to provide support to people experiencing trafficking.

</p>
</details>

<details><summary><b>Efficient and Accurate In-Database Machine Learning with SQL Code Generation in Python</b>
<a href="https://arxiv.org/abs/2104.03224">arxiv:2104.03224</a>
&#x1F4C8; 1 <br>
<p>Michael Kaufmann, Gabriel Stechschulte, Anna Huber</p></summary>
<p>

**Abstract:** Following an analysis of the advantages of SQL-based Machine Learning (ML) and a short literature survey of the field, we describe a novel method for In-Database Machine Learning (IDBML). We contribute a process for SQL-code generation in Python using template macros in Jinja2 as well as the prototype implementation of the process. We describe our implementation of the process to compute multidimensional histogram (MDH) probability estimation in SQL. For this, we contribute and implement a novel discretization method called equal quantized rank binning (EQRB) and equal-width binning (EWB). Based on this, we provide data gathered in a benchmarking experiment for the quantitative empirical evaluation of our method and system using the Covertype dataset. We measured accuracy and computation time and compared it to Scikit Learn state of the art classification algorithms. Using EWB, our multidimensional probability estimation was the fastest of all tested algorithms, while being only 1-2% less accurate than the best state of the art methods found (decision trees and random forests). Our method was significantly more accurate than Naive Bayes, which assumes independent one-dimensional probabilities and/or densities. Also, our method was significantly more accurate and faster than logistic regression. This motivates for further research in accuracy improvement and in IDBML with SQL code generation for big data and larger-than-memory datasets.

</p>
</details>

<details><summary><b>Empowering Prosumer Communities in Smart Grid with Wireless Communications and Federated Edge Learning</b>
<a href="https://arxiv.org/abs/2104.03169">arxiv:2104.03169</a>
&#x1F4C8; 1 <br>
<p>Afaf Taik, Boubakr Nour, Soumaya Cherkaoui</p></summary>
<p>

**Abstract:** The exponential growth of distributed energy resources is enabling the transformation of traditional consumers in the smart grid into prosumers. Such transition presents a promising opportunity for sustainable energy trading. Yet, the integration of prosumers in the energy market imposes new considerations in designing unified and sustainable frameworks for efficient use of the power and communication infrastructure. Furthermore, several issues need to be tackled to adequately promote the adoption of decentralized renewable-oriented systems, such as communication overhead, data privacy, scalability, and sustainability. In this article, we present the different aspects and challenges to be addressed for building efficient energy trading markets in relation to communication and smart decision-making. Accordingly, we propose a multi-level pro-decision framework for prosumer communities to achieve collective goals. Since the individual decisions of prosumers are mainly driven by individual self-sufficiency goals, the framework prioritizes the individual prosumers' decisions and relies on 5G wireless network for fast coordination among community members. In fact, each prosumer predicts energy production and consumption to make proactive trading decisions as a response to collective-level requests. Moreover, the collaboration of the community is further extended by including the collaborative training of prediction models using Federated Learning, assisted by edge servers and prosumer home-area equipment. In addition to preserving prosumers' privacy, we show through evaluations that training prediction models using Federated Learning yields high accuracy for different energy resources while reducing the communication overhead.

</p>
</details>

<details><summary><b>HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks</b>
<a href="https://arxiv.org/abs/2104.03090">arxiv:2104.03090</a>
&#x1F4C8; 1 <br>
<p>Firoj Alam, Umair Qazi, Muhammad Imran, Ferda Ofli</p></summary>
<p>

**Abstract:** Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with ~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available. https://crisisnlp.qcri.org/humaid_dataset.html

</p>
</details>

<details><summary><b>Plinius: Secure and Persistent Machine Learning Model Training</b>
<a href="https://arxiv.org/abs/2104.02987">arxiv:2104.02987</a>
&#x1F4C8; 1 <br>
<p>Peterson Yuhala, Pascal Felber, Valerio Schiavoni, Alain Tchana</p></summary>
<p>

**Abstract:** With the increasing popularity of cloud based machine learning (ML) techniques there comes a need for privacy and integrity guarantees for ML data. In addition, the significant scalability challenges faced by DRAM coupled with the high access-times of secondary storage represent a huge performance bottleneck for ML systems. While solutions exist to tackle the security aspect, performance remains an issue. Persistent memory (PM) is resilient to power loss (unlike DRAM), provides fast and fine-granular access to memory (unlike disk storage) and has latency and bandwidth close to DRAM (in the order of ns and GB/s, respectively). We present PLINIUS, a ML framework using Intel SGX enclaves for secure training of ML models and PM for fault tolerance guarantees. P LINIUS uses a novel mirroring mechanism to create and maintain (i) encrypted mirror copies of ML models on PM, and (ii) encrypted training data in byte-addressable PM, for near-instantaneous data recovery after a system failure. Compared to disk-based checkpointing systems,PLINIUS is 3.2x and 3.7x faster respectively for saving and restoring models on real PM hardware, achieving robust and secure ML model training in SGX enclaves.

</p>
</details>

<details><summary><b>Contrastive Learning of Global-Local Video Representations</b>
<a href="https://arxiv.org/abs/2104.05418">arxiv:2104.05418</a>
&#x1F4C8; 0 <br>
<p>Shuang Ma, Zhaoyang Zeng, Daniel McDuff, Yale Song</p></summary>
<p>

**Abstract:** Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning representations specific to downstream scenarios, i.e., \textit{global} representations suitable for tasks such as classification or \textit{local} representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require local fine-grained spatio-temporal information (e.g., localization). We achieve this by optimizing two contrastive objectives that together encourage our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, significantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classification, lip reading, deepfake detection, event and sound localization (https://github.com/yunyikristy/global\_local).

</p>
</details>

<details><summary><b>Learning to Coordinate via Multiple Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2104.03503">arxiv:2104.03503</a>
&#x1F4C8; 0 <br>
<p>Zhiwei Xu, Bin Zhang, Yunpeng Bai, Dapeng Li, Guoliang Fan</p></summary>
<p>

**Abstract:** The collaboration between agents has gradually become an important topic in multi-agent systems. The key is how to efficiently solve the credit assignment problems. This paper introduces MGAN for collaborative multi-agent reinforcement learning, a new algorithm that combines graph convolutional networks and value-decomposition methods. MGAN learns the representation of agents from different perspectives through multiple graph networks, and realizes the proper allocation of attention between all agents. We show the amazing ability of the graph network in representation learning by visualizing the output of the graph network, and therefore improve interpretability for the actions of each agent in the multi-agent system.

</p>
</details>

<details><summary><b>PrivateSNN: Privacy-Preserving Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2104.03414">arxiv:2104.03414</a>
&#x1F4C8; 0 <br>
<p>Youngeun Kim, Yeshwanth Venkatesha, Priyadarshini Panda</p></summary>
<p>

**Abstract:** How can we bring both privacy and energy-efficiency to a neural system? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage is caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using the generated images. However, converted SNNs remain vulnerable to class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes SNNs difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN eliminates data and class leakage issues with a slight performance drop (less than ~2) and significant energy-efficiency gain (about 55x) compared to the standard ANN. We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training.

</p>
</details>

<details><summary><b>Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction</b>
<a href="https://arxiv.org/abs/2104.03279">arxiv:2104.03279</a>
&#x1F4C8; 0 <br>
<p>Philipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas Verhoeven, Marwin Segler, Jörg K. Wegner, Sepp Hochreiter, Günter Klambauer</p></summary>
<p>

**Abstract:** Finding synthesis routes for molecules of interest is an essential step in the discovery of new drugs and materials. To find such routes, computer-assisted synthesis planning (CASP) methods are employed which rely on a model of chemical reactivity. In this study, we model single-step retrosynthesis in a template-based approach using modern Hopfield networks (MHNs). We adapt MHNs to associate different modalities, reaction templates and molecules, which allows the model to leverage structural information about reaction templates. This approach significantly improves the performance of template relevance prediction, especially for templates with few or zero training examples. With inference speed several times faster than that of baseline methods, we improve predictive performance for top-k exact match accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.

</p>
</details>

<details><summary><b>The Proper Use of Google Trends in Forecasting Models</b>
<a href="https://arxiv.org/abs/2104.03065">arxiv:2104.03065</a>
&#x1F4C8; 0 <br>
<p>Marcelo C. Medeiros, Henrique F. Pires</p></summary>
<p>

**Abstract:** It is widely known that Google Trends have become one of the most popular free tools used by forecasters both in academics and in the private and public sectors. There are many papers, from several different fields, concluding that Google Trends improve forecasts' accuracy. However, what seems to be widely unknown, is that each sample of Google search data is different from the other, even if you set the same search term, data and location. This means that it is possible to find arbitrary conclusions merely by chance. This paper aims to show why and when it can become a problem and how to overcome this obstacle.

</p>
</details>

<details><summary><b>Minimax Kernel Machine Learning for a Class of Doubly Robust Functionals with Application to Proximal Causal Inference</b>
<a href="https://arxiv.org/abs/2104.02929">arxiv:2104.02929</a>
&#x1F4C8; 0 <br>
<p>AmirEmad Ghassami, Andrew Ying, Ilya Shpitser, Eric Tchetgen Tchetgen</p></summary>
<p>

**Abstract:** A moment function is called doubly robust if it is comprised of two nuisance functions and has the desired property that the estimator based on it is a consistent estimator of the target parameter even if one of the nuisance functions is misspecified. A common approach for obtaining such a moment function is based on using the influence function (IF) of the parameter of interest. Robins et al. (2008) introduced a large class of doubly robust IFs. However, that class does not include the IF of functionals for which the nuisance functions are solutions to integral equations. Such functionals are particularly important in the field of causal inference, specifically in the recently proposed proximal inference framework of (Miao et al., 2018; Tchetgen Tchetgen et al., 2020), which allows for estimating the average causal effect when unobserved confounders are present in the system. Motivated by the proximal inference framework, in this paper, we first extend the class of Robins et al. to include doubly robust IFs in which the nuisance functions are solutions to integral equations. Then we demonstrate that the double robustness property of these IFs can be leveraged to construct estimating equations for the nuisance functions, which enables us to solve the integral equations without resorting to parametric models. The main idea is to choose each nuisance function such that it minimizes the dependency of the expected value of the moment function to the other nuisance function. We frame this idea as a minimax optimization problem and use RKHSes as the function spaces. We provide convergence rates for the nuisance functions and conditions required for asymptotic linearity of the estimator of the functional of interest. The experiment results demonstrate that our proposed methodology leads to robust and high-performance estimators for average causal effect in the proximal inference framework.

</p>
</details>


[Next Page]({{ '/2021/04/06/2021.04.06.html' | relative_url }})
