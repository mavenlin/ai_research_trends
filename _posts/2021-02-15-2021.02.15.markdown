Prev: [2021.02.14]({{ '/2021/02/14/2021.02.14.html' | relative_url }})  Next: [2021.02.16]({{ '/2021/02/16/2021.02.16.html' | relative_url }})
{% raw %}
## Summary for 2021-02-15, created on 2021-12-24


<details><summary><b>Momentum Residual Neural Networks</b>
<a href="https://arxiv.org/abs/2102.07870">arxiv:2102.07870</a>
&#x1F4C8; 253 <br>
<p>Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr√©</p></summary>
<p>

**Abstract:** The training of deep residual neural networks (ResNets) with backpropagation has a memory cost that increases linearly with respect to the depth of the network. A way to circumvent this issue is to use reversible architectures. In this paper, we propose to change the forward rule of a ResNet by adding a momentum term. The resulting networks, momentum residual neural networks (Momentum ResNets), are invertible. Unlike previous invertible architectures, they can be used as a drop-in replacement for any existing ResNet block. We show that Momentum ResNets can be interpreted in the infinitesimal step size regime as second-order ordinary differential equations (ODEs) and exactly characterize how adding momentum progressively increases the representation capabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets can learn any linear mapping up to a multiplicative factor, while ResNets cannot. In a learning to optimize setting, where convergence to a fixed point is required, we show theoretically and empirically that our method succeeds while existing invertible architectures fail. We show on CIFAR and ImageNet that Momentum ResNets have the same accuracy as ResNets, while having a much smaller memory footprint, and show that pre-trained Momentum ResNets are promising for fine-tuning models.

</p>
</details>

<details><summary><b>ELIXIR: Learning from User Feedback on Explanations to Improve Recommender Models</b>
<a href="https://arxiv.org/abs/2102.09388">arxiv:2102.09388</a>
&#x1F4C8; 60 <br>
<p>Azin Ghazimatin, Soumajit Pramanik, Rishiraj Saha Roy, Gerhard Weikum</p></summary>
<p>

**Abstract:** System-provided explanations for recommendations are an important component towards transparent and trustworthy AI. In state-of-the-art research, this is a one-way signal, though, to improve user acceptance. In this paper, we turn the role of explanations around and investigate how they can contribute to enhancing the quality of the generated recommendations themselves. We devise a human-in-the-loop framework, called ELIXIR, where user feedback on explanations is leveraged for pairwise learning of user preferences. ELIXIR leverages feedback on pairs of recommendations and explanations to learn user-specific latent preference vectors, overcoming sparseness by label propagation with item-similarity-based neighborhoods. Our framework is instantiated using generalized graph recommendation via Random Walk with Restart. Insightful experiments with a real user study show significant improvements in movie and book recommendations over item-level feedback.

</p>
</details>

<details><summary><b>Topological Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2102.07835">arxiv:2102.07835</a>
&#x1F4C8; 52 <br>
<p>Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, Karsten Borgwardt</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive in terms of the Weisfeiler-Lehman graph isomorphism test. Augmenting GNNs with our layer leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets (which can be classified by humans using their topology but not by ordinary GNNs) and on real-world data.

</p>
</details>

<details><summary><b>Meta Back-translation</b>
<a href="https://arxiv.org/abs/2102.07847">arxiv:2102.07847</a>
&#x1F4C8; 43 <br>
<p>Hieu Pham, Xinyi Wang, Yiming Yang, Graham Neubig</p></summary>
<p>

**Abstract:** Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results. In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudo-parallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. Our code will be made available.

</p>
</details>

<details><summary><b>End-to-End Egospheric Spatial Memory</b>
<a href="https://arxiv.org/abs/2102.07764">arxiv:2102.07764</a>
&#x1F4C8; 43 <br>
<p>Daniel Lenton, Stephen James, Ronald Clark, Andrew J. Davison</p></summary>
<p>

**Abstract:** Spatial memory, or the ability to remember and recall specific locations and objects, is central to autonomous agents' ability to carry out tasks in real environments. However, most existing artificial memory modules are not very adept at storing spatial information. We propose a parameter-free module, Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations. ESM can be trained end-to-end via either imitation or reinforcement learning, and improves both training efficiency and final performance against other memory baselines on both drone and manipulator visuomotor control tasks. The explicit egocentric geometry also enables us to seamlessly combine the learned controller with other non-learned modalities, such as local obstacle avoidance. We further show applications to semantic segmentation on the ScanNet dataset, where ESM naturally combines image-level and map-level inference modalities. Through our broad set of experiments, we show that ESM provides a general computation graph for embodied spatial reasoning, and the module forms a bridge between real-time mapping systems and differentiable memory architectures. Implementation at: https://github.com/ivy-dl/memory.

</p>
</details>

<details><summary><b>Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing</b>
<a href="https://arxiv.org/abs/2102.07475">arxiv:2102.07475</a>
&#x1F4C8; 43 <br>
<p>Filippos Christianos, Georgios Papoudakis, Arrasy Rahman, Stefano V. Albrecht</p></summary>
<p>

**Abstract:** Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.

</p>
</details>

<details><summary><b>Hierarchical Transformer-based Large-Context End-to-end ASR with Large-Context Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2102.07935">arxiv:2102.07935</a>
&#x1F4C8; 30 <br>
<p>Ryo Masumura, Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro Tanaka, Shota Orihashi</p></summary>
<p>

**Abstract:** We present a novel large-context end-to-end automatic speech recognition (E2E-ASR) model and its effective training method based on knowledge distillation. Common E2E-ASR models have mainly focused on utterance-level processing in which each utterance is independently transcribed. On the other hand, large-context E2E-ASR models, which take into account long-range sequential contexts beyond utterance boundaries, well handle a sequence of utterances such as discourses and conversations. However, the transformer architecture, which has recently achieved state-of-the-art ASR performance among utterance-level ASR systems, has not yet been introduced into the large-context ASR systems. We can expect that the transformer architecture can be leveraged for effectively capturing not only input speech contexts but also long-range sequential contexts beyond utterance boundaries. Therefore, this paper proposes a hierarchical transformer-based large-context E2E-ASR model that combines the transformer architecture with hierarchical encoder-decoder based large-context modeling. In addition, in order to enable the proposed model to use long-range sequential contexts, we also propose a large-context knowledge distillation that distills the knowledge from a pre-trained large-context language model in the training phase. We evaluate the effectiveness of the proposed model and proposed training method on Japanese discourse ASR tasks.

</p>
</details>

<details><summary><b>Self-Supervised Features Improve Open-World Learning</b>
<a href="https://arxiv.org/abs/2102.07848">arxiv:2102.07848</a>
&#x1F4C8; 28 <br>
<p>Akshay Raj Dhamija, Touqeer Ahmad, Jonathan Schwan, Mohsen Jafarzadeh, Chunchun Li, Terrance E. Boult</p></summary>
<p>

**Abstract:** This paper identifies the flaws in existing open-world learning approaches and attempts to provide a complete picture in the form of \textbf{True Open-World Learning}. We accomplish this by proposing a comprehensive generalize-able open-world learning protocol capable of evaluating various components of open-world learning in an operational setting. We argue that in true open-world learning, the underlying feature representation should be learned in a self-supervised manner. Under this self-supervised feature representation, we introduce the problem of detecting unknowns as samples belonging to Out-of-Label space. We differentiate between Out-of-Label space detection and the conventional Out-of-Distribution detection depending upon whether the unknowns being detected belong to the native-world (same as feature representation) or a new-world, respectively. Our unifying open-world learning framework combines three individual research dimensions, which typically have been explored independently, i.e., Incremental Learning, Out-of-Distribution detection and Open-World Learning. Starting from a self-supervised feature space, an open-world learner has the ability to adapt and specialize its feature space to the classes in each incremental phase and hence perform better without incurring any significant overhead, as demonstrated by our experimental results. The incremental learning component of our pipeline provides the new state-of-the-art on established ImageNet-100 protocol. We also demonstrate the adaptability of our approach by showing how it can work as a plug-in with any of the self-supervised feature representation methods.

</p>
</details>

<details><summary><b>A Deep-Learning Approach For Direct Whole-Heart Mesh Reconstruction</b>
<a href="https://arxiv.org/abs/2102.07899">arxiv:2102.07899</a>
&#x1F4C8; 27 <br>
<p>Fanwei Kong, Nathan Wilson, Shawn C. Shadden</p></summary>
<p>

**Abstract:** Automated construction of surface geometries of cardiac structures from volumetric medical images is important for a number of clinical applications. While deep-learning-based approaches have demonstrated promising reconstruction precision, these approaches have mostly focused on voxel-wise segmentation followed by surface reconstruction and post-processing techniques. However, such approaches suffer from a number of limitations including disconnected regions or incorrect surface topology due to erroneous segmentation and stair-case artifacts due to limited segmentation resolution. We propose a novel deep-learning-based approach that directly predicts whole heart surface meshes from volumetric CT and MR image data. Our approach leverages a graph convolutional neural network to predict deformation on mesh vertices from a pre-defined mesh template to reconstruct multiple anatomical structures in a 3D image volume. Our method demonstrated promising performance of generating whole heart reconstructions with as good or better accuracy than prior deep-learning-based methods on both CT and MR data. Furthermore, by deforming a template mesh, our method can generate whole heart geometries with better anatomical consistency and produce high-resolution geometries from lower resolution input image data. Our method was also able to produce temporally consistent surface mesh predictions for heart motion from CT or MR cine sequences, and therefore can potentially be applied for efficiently constructing 4D whole heart dynamics. Our code and pre-trained networks are available at https://github.com/fkong7/MeshDeformNet

</p>
</details>

<details><summary><b>Detection and severity classification of COVID-19 in CT images using deep learning</b>
<a href="https://arxiv.org/abs/2102.07726">arxiv:2102.07726</a>
&#x1F4C8; 27 <br>
<p>Yazan Qiblawey, Anas Tahir, Muhammad E. H. Chowdhury, Amith Khandakar, Serkan Kiranyaz, Tawsifur Rahman, Nabil Ibtehaz, Sakib Mahmud, Somaya Al-Madeed, Farayi Musharavati</p></summary>
<p>

**Abstract:** Since the breakout of coronavirus disease (COVID-19), the computer-aided diagnosis has become a necessity to prevent the spread of the virus. Detecting COVID-19 at an early stage is essential to reduce the mortality risk of the patients. In this study, a cascaded system is proposed to segment the lung, detect, localize, and quantify COVID-19 infections from computed tomography (CT) images Furthermore, the system classifies the severity of COVID-19 as mild, moderate, severe, or critical based on the percentage of infected lungs. An extensive set of experiments were performed using state-of-the-art deep Encoder-Decoder Convolutional Neural Networks (ED-CNNs), UNet, and Feature Pyramid Network (FPN), with different backbone (encoder) structures using the variants of DenseNet and ResNet. The conducted experiments showed the best performance for lung region segmentation with Dice Similarity Coefficient (DSC) of 97.19% and Intersection over Union (IoU) of 95.10% using U-Net model with the DenseNet 161 encoder. Furthermore, the proposed system achieved an elegant performance for COVID-19 infection segmentation with a DSC of 94.13% and IoU of 91.85% using the FPN model with the DenseNet201 encoder. The achieved performance is significantly superior to previous methods for COVID-19 lesion localization. Besides, the proposed system can reliably localize infection of various shapes and sizes, especially small infection regions, which are rarely considered in recent studies. Moreover, the proposed system achieved high COVID-19 detection performance with 99.64% sensitivity and 98.72% specificity. Finally, the system was able to discriminate between different severity levels of COVID-19 infection over a dataset of 1,110 subjects with sensitivity values of 98.3%, 71.2%, 77.8%, and 100% for mild, moderate, severe, and critical infections, respectively.

</p>
</details>

<details><summary><b>Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</b>
<a href="https://arxiv.org/abs/2102.07350">arxiv:2102.07350</a>
&#x1F4C8; 27 <br>
<p>Laria Reynolds, Kyle McDonell</p></summary>
<p>

**Abstract:** Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.

</p>
</details>

<details><summary><b>Training Larger Networks for Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.07920">arxiv:2102.07920</a>
&#x1F4C8; 24 <br>
<p>Kei Ota, Devesh K. Jha, Asako Kanezaki</p></summary>
<p>

**Abstract:** The success of deep learning in the computer vision and natural language processing communities can be attributed to training of very deep neural networks with millions or billions of parameters which can then be trained with massive amounts of data. However, similar trend has largely eluded training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of 1) wider networks with DenseNet connection, 2) decoupling representation learning from training of RL, 3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.

</p>
</details>

<details><summary><b>CHARET: Character-centered Approach to Emotion Tracking in Stories</b>
<a href="https://arxiv.org/abs/2102.07537">arxiv:2102.07537</a>
&#x1F4C8; 24 <br>
<p>Diogo S. Carvalho, Joana Campos, Manuel Guimar√£es, Ana Antunes, Jo√£o Dias, Pedro A. Santos</p></summary>
<p>

**Abstract:** Autonomous agents that can engage in social interactions witha human is the ultimate goal of a myriad of applications. A keychallenge in the design of these applications is to define the socialbehavior of the agent, which requires extensive content creation.In this research, we explore how we can leverage current state-of-the-art tools to make inferences about the emotional state ofa character in a story as events unfold, in a coherent way. Wepropose a character role-labelling approach to emotion tracking thataccounts for the semantics of emotions. We show that by identifyingactors and objects of events and considering the emotional stateof the characters, we can achieve better performance in this task,when compared to end-to-end approaches.

</p>
</details>

<details><summary><b>UserReg: A Simple but Strong Model for Rating Prediction</b>
<a href="https://arxiv.org/abs/2102.07601">arxiv:2102.07601</a>
&#x1F4C8; 23 <br>
<p>Haiyang Zhang, Ivan Ganchev, Nikola S. Nikolov, Mark Stevenson</p></summary>
<p>

**Abstract:** Collaborative filtering (CF) has achieved great success in the field of recommender systems. In recent years, many novel CF models, particularly those based on deep learning or graph techniques, have been proposed for a variety of recommendation tasks, such as rating prediction and item ranking. These newly published models usually demonstrate their performance in comparison to baselines or existing models in terms of accuracy improvements. However, others have pointed out that many newly proposed models are not as strong as expected and are outperformed by very simple baselines.
  This paper proposes a simple linear model based on Matrix Factorization (MF), called UserReg, which regularizes users' latent representations with explicit feedback information for rating prediction. We compare the effectiveness of UserReg with three linear CF models that are widely-used as baselines, and with a set of recently proposed complex models that are based on deep learning or graph techniques. Experimental results show that UserReg achieves overall better performance than the fine-tuned baselines considered and is highly competitive when compared with other recently proposed models. We conclude that UserReg can be used as a strong baseline for future CF research.

</p>
</details>

<details><summary><b>Network of Tensor Time Series</b>
<a href="https://arxiv.org/abs/2102.07736">arxiv:2102.07736</a>
&#x1F4C8; 22 <br>
<p>Baoyu Jing, Hanghang Tong, Yada Zhu</p></summary>
<p>

**Abstract:** Co-evolving time series appears in a multitude of applications such as environmental monitoring, financial analysis, and smart transportation. This paper aims to address the following challenges, including (C1) how to incorporate explicit relationship networks of the time series; (C2) how to model the implicit relationship of the temporal dynamics. We propose a novel model called Network of Tensor Time Series, which is comprised of two modules, including Tensor Graph Convolutional Network (TGCN) and Tensor Recurrent Neural Network (TRNN). TGCN tackles the first challenge by generalizing Graph Convolutional Network (GCN) for flat graphs to tensor graphs, which captures the synergy between multiple graphs associated with the tensors. TRNN leverages tensor decomposition to model the implicit relationships among co-evolving time series. The experimental results on five real-world datasets demonstrate the efficacy of the proposed method.

</p>
</details>

<details><summary><b>Annealed Flow Transport Monte Carlo</b>
<a href="https://arxiv.org/abs/2102.07501">arxiv:2102.07501</a>
&#x1F4C8; 21 <br>
<p>Michael Arbel, Alexander G. D. G. Matthews, Arnaud Doucet</p></summary>
<p>

**Abstract:** Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC) extensions are state-of-the-art methods for estimating normalizing constants of probability distributions. We propose here a novel Monte Carlo algorithm, Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them with normalizing flows (NFs) for improved performance. This method transports a set of particles using not only importance sampling (IS), Markov chain Monte Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are learned sequentially to push particles towards the successive annealed targets. We provide limit theorems for the resulting Monte Carlo estimates of the normalizing constant and expectations with respect to the target distribution. Additionally, we show that a continuous-time scaling limit of the population version of AFT is given by a Feynman--Kac measure which simplifies to the law of a controlled diffusion for expressive NFs. We demonstrate experimentally the benefits and limitations of our methodology on a variety of applications.

</p>
</details>

<details><summary><b>DAC: Deep Autoencoder-based Clustering, a General Deep Learning Framework of Representation Learning</b>
<a href="https://arxiv.org/abs/2102.07472">arxiv:2102.07472</a>
&#x1F4C8; 21 <br>
<p>Si Lu, Ruisi Li</p></summary>
<p>

**Abstract:** Clustering performs an essential role in many real world applications, such as market research, pattern recognition, data analysis, and image processing. However, due to the high dimensionality of the input feature values, the data being fed to clustering algorithms usually contains noise and thus could lead to in-accurate clustering results. While traditional dimension reduction and feature selection algorithms could be used to address this problem, the simple heuristic rules used in those algorithms are based on some particular assumptions. When those assumptions does not hold, these algorithms then might not work. In this paper, we propose DAC, Deep Autoencoder-based Clustering, a generalized data-driven framework to learn clustering representations using deep neuron networks. Experiment results show that our approach could effectively boost performance of the K-Means clustering algorithm on a variety types of datasets.

</p>
</details>

<details><summary><b>Anomalous Sound Detection with Machine Learning: A Systematic Review</b>
<a href="https://arxiv.org/abs/2102.07820">arxiv:2102.07820</a>
&#x1F4C8; 18 <br>
<p>Eduardo C. Nunes</p></summary>
<p>

**Abstract:** Anomalous sound detection (ASD) is the task of identifying whether the sound emitted from an object is normal or anomalous. In some cases, early detection of this anomaly can prevent several problems. This article presents a Systematic Review (SR) about studies related to Anamolous Sound Detection using Machine Learning (ML) techniques. This SR was conducted through a selection of 31 (accepted studies) studies published in journals and conferences between 2010 and 2020. The state of the art was addressed, collecting data sets, methods for extracting features in audio, ML models, and evaluation methods used for ASD. The results showed that the ToyADMOS, MIMII, and Mivia datasets, the Mel-frequency cepstral coefficients (MFCC) method for extracting features, the Autoencoder (AE) and Convolutional Neural Network (CNN) models of ML, the AUC and F1-score evaluation methods were most cited.

</p>
</details>

<details><summary><b>DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning</b>
<a href="https://arxiv.org/abs/2102.07936">arxiv:2102.07936</a>
&#x1F4C8; 15 <br>
<p>Wei-Fang Sun, Cheng-Kuang Lee, Chun-Yi Lee</p></summary>
<p>

**Abstract:** In fully cooperative multi-agent reinforcement learning (MARL) settings, the environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of the other agents. To address the above issues, we integrate distributional RL and value function factorization methods by proposing a Distributional Value Function Factorization (DFAC) framework to generalize expected value function factorization methods to their DFAC variants. DFAC extends the individual utility functions from deterministic variables to random variables, and models the quantile function of the total return as a quantile mixture. To validate DFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game with stochastic rewards and perform experiments on all Super Hard tasks of StarCraft Multi-Agent Challenge, showing that DFAC is able to outperform expected value function factorization baselines.

</p>
</details>

<details><summary><b>REST: Relational Event-driven Stock Trend Forecasting</b>
<a href="https://arxiv.org/abs/2102.07372">arxiv:2102.07372</a>
&#x1F4C8; 14 <br>
<p>Wentao Xu, Weiqing Liu, Chang Xu, Jiang Bian, Jian Yin, Tie-Yan Liu</p></summary>
<p>

**Abstract:** Stock trend forecasting, aiming at predicting the stock future trends, is crucial for investors to seek maximized profits from the stock market. Many event-driven methods utilized the events extracted from news, social media, and discussion board to forecast the stock trend in recent years. However, existing event-driven methods have two main shortcomings: 1) overlooking the influence of event information differentiated by the stock-dependent properties; 2) neglecting the effect of event information from other related stocks. In this paper, we propose a relational event-driven stock trend forecasting (REST) framework, which can address the shortcoming of existing methods. To remedy the first shortcoming, we propose to model the stock context and learn the effect of event information on the stocks under different contexts. To address the second shortcoming, we construct a stock graph and design a new propagation layer to propagate the effect of event information from related stocks. The experimental studies on the real-world data demonstrate the efficiency of our REST framework. The results of investment simulation show that our framework can achieve a higher return of investment than baselines.

</p>
</details>

<details><summary><b>GraphGallery: A Platform for Fast Benchmarking and Easy Development of Graph Neural Networks Based Intelligent Software</b>
<a href="https://arxiv.org/abs/2102.07933">arxiv:2102.07933</a>
&#x1F4C8; 13 <br>
<p>Jintang Li, Kun Xu, Liang Chen, Zibin Zheng, Xiao Liu</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNNs) have recently shown to be powerful tools for representing and analyzing graph data. So far GNNs is becoming an increasingly critical role in software engineering including program analysis, type inference, and code representation. In this paper, we introduce GraphGallery, a platform for fast benchmarking and easy development of GNNs based software. GraphGallery is an easy-to-use platform that allows developers to automatically deploy GNNs even with less domain-specific knowledge. It offers a set of implementations of common GNN models based on mainstream deep learning frameworks. In addition, existing GNNs toolboxes such as PyG and DGL can be easily incorporated into the platform. Experiments demonstrate the reliability of implementations and superiority in fast coding. The official source code of GraphGallery is available at https://github.com/EdisonLeeeee/GraphGallery and a demo video can be found at https://youtu.be/mv7Zs1YeaYo.

</p>
</details>

<details><summary><b>Translational Equivariance in Kernelizable Attention</b>
<a href="https://arxiv.org/abs/2102.07680">arxiv:2102.07680</a>
&#x1F4C8; 11 <br>
<p>Max Horn, Kumar Shridhar, Elrich Groenewald, Philipp F. M. Baumann</p></summary>
<p>

**Abstract:** While Transformer architectures have show remarkable success, they are bound to the computation of all pairwise interactions of input element and thus suffer from limited scalability. Recent work has been successful by avoiding the computation of the complete attention matrix, yet leads to problems down the line. The absence of an explicit attention matrix makes the inclusion of inductive biases relying on relative interactions between elements more challenging. An extremely powerful inductive bias is translational equivariance, which has been conjectured to be responsible for much of the success of Convolutional Neural Networks on image recognition tasks. In this work we show how translational equivariance can be implemented in efficient Transformers based on kernelizable attention - Performers. Our experiments highlight that the devised approach significantly improves robustness of Performers to shifts of input images compared to their naive application. This represents an important step on the path of replacing Convolutional Neural Networks with more expressive Transformer architectures and will help to improve sample efficiency and robustness in this realm.

</p>
</details>

<details><summary><b>Causal Markov Decision Processes: Learning Good Interventions Efficiently</b>
<a href="https://arxiv.org/abs/2102.07663">arxiv:2102.07663</a>
&#x1F4C8; 10 <br>
<p>Yangyi Lu, Amirhossein Meisami, Ambuj Tewari</p></summary>
<p>

**Abstract:** We introduce causal Markov Decision Processes (C-MDPs), a new formalism for sequential decision making which combines the standard MDP formulation with causal structures over state transition and reward functions. Many contemporary and emerging application areas such as digital healthcare and digital marketing can benefit from modeling with C-MDPs due to the causal mechanisms underlying the relationship between interventions and states/rewards. We propose the causal upper confidence bound value iteration (C-UCBVI) algorithm that exploits the causal structure in C-MDPs and improves the performance of standard reinforcement learning algorithms that do not take causal knowledge into account. We prove that C-UCBVI satisfies an $\tilde{O}(HS\sqrt{ZT})$ regret bound, where $T$ is the the total time steps, $H$ is the episodic horizon, and $S$ is the cardinality of the state space. Notably, our regret bound does not scale with the size of actions/interventions ($A$), but only scales with a causal graph dependent quantity $Z$ which can be exponentially smaller than $A$. By extending C-UCBVI to the factored MDP setting, we propose the causal factored UCBVI (CF-UCBVI) algorithm, which further reduces the regret exponentially in terms of $S$. Furthermore, we show that RL algorithms for linear MDP problems can also be incorporated in C-MDPs. We empirically show the benefit of our causal approaches in various settings to validate our algorithms and theoretical results.

</p>
</details>

<details><summary><b>ResNet-LDDMM: Advancing the LDDMM Framework Using Deep Residual Networks</b>
<a href="https://arxiv.org/abs/2102.07951">arxiv:2102.07951</a>
&#x1F4C8; 9 <br>
<p>Boulbaba Ben Amor, Sylvain Arguill√®re, Ling Shao</p></summary>
<p>

**Abstract:** In deformable registration, the geometric framework - large deformation diffeomorphic metric mapping or LDDMM, in short - has inspired numerous techniques for comparing, deforming, averaging and analyzing shapes or images. Grounded in flows, which are akin to the equations of motion used in fluid dynamics, LDDMM algorithms solve the flow equation in the space of plausible deformations, i.e. diffeomorphisms. In this work, we make use of deep residual neural networks to solve the non-stationary ODE (flow equation) based on a Euler's discretization scheme. The central idea is to represent time-dependent velocity fields as fully connected ReLU neural networks (building blocks) and derive optimal weights by minimizing a regularized loss function. Computing minimizing paths between deformations, thus between shapes, turns to find optimal network parameters by back-propagating over the intermediate building blocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal partition of the space into multiple polytopes, and then computes optimal velocity vectors as affine transformations on each of these polytopes. As a result, different parts of the shape, even if they are close (such as two fingers of a hand), can be made to belong to different polytopes, and therefore be moved in different directions without costing too much energy. Importantly, we show how diffeomorphic transformations, or more precisely bilipshitz transformations, are predicted by our algorithm. We illustrate these ideas on diverse registration problems of 3D shapes under complex topology-preserving transformations. We thus provide essential foundations for more advanced shape variability analysis under a novel joint geometric-neural networks Riemannian-like framework, i.e. ResNet-LDDMM.

</p>
</details>

<details><summary><b>How to Learn when Data Reacts to Your Model: Performative Gradient Descent</b>
<a href="https://arxiv.org/abs/2102.07698">arxiv:2102.07698</a>
&#x1F4C8; 9 <br>
<p>Zachary Izzo, Lexing Ying, James Zou</p></summary>
<p>

**Abstract:** Performative distribution shift captures the setting where the choice of which ML model is deployed changes the data distribution. For example, a bank which uses the number of open credit lines to determine a customer's risk of default on a loan may induce customers to open more credit lines in order to improve their chances of being approved. Because of the interactions between the model and data distribution, finding the optimal model parameters is challenging. Works in this area have focused on finding stable points, which can be far from optimal. Here we introduce performative gradient descent (PerfGD), which is the first algorithm which provably converges to the performatively optimal point. PerfGD explicitly captures how changes in the model affects the data distribution and is simple to use. We support our findings with theory and experiments.

</p>
</details>

<details><summary><b>Differentiable Particle Filtering via Entropy-Regularized Optimal Transport</b>
<a href="https://arxiv.org/abs/2102.07850">arxiv:2102.07850</a>
&#x1F4C8; 8 <br>
<p>Adrien Corenflos, James Thornton, George Deligiannidis, Arnaud Doucet</p></summary>
<p>

**Abstract:** Particle Filtering (PF) methods are an established class of procedures for performing inference in non-linear state-space models. Resampling is a key ingredient of PF, necessary to obtain low variance likelihood and states estimates. However, traditional resampling methods result in PF-based loss functions being non-differentiable with respect to model and PF parameters. In a variational inference context, resampling also yields high variance gradient estimates of the PF-based evidence lower bound. By leveraging optimal transport ideas, we introduce a principled differentiable particle filter and provide convergence results. We demonstrate this novel method on a variety of applications.

</p>
</details>

<details><summary><b>How Convolutional Neural Networks Deal with Aliasing</b>
<a href="https://arxiv.org/abs/2102.07757">arxiv:2102.07757</a>
&#x1F4C8; 8 <br>
<p>Ant√¥nio H. Ribeiro, Thomas B. Sch√∂n</p></summary>
<p>

**Abstract:** The convolutional neural network (CNN) remains an essential tool in solving computer vision problems. Standard convolutional architectures consist of stacked layers of operations that progressively downscale the image. Aliasing is a well-known side-effect of downsampling that may take place: it causes high-frequency components of the original signal to become indistinguishable from its low-frequency components. While downsampling takes place in the max-pooling layers or in the strided-convolutions in these models, there is no explicit mechanism that prevents aliasing from taking place in these layers. Due to the impressive performance of these models, it is natural to suspect that they, somehow, implicitly deal with this distortion. The question we aim to answer in this paper is simply: "how and to what extent do CNNs counteract aliasing?" We explore the question by means of two examples: In the first, we assess the CNNs capability of distinguishing oscillations at the input, showing that the redundancies in the intermediate channels play an important role in succeeding at the task; In the second, we show that an image classifier CNN while, in principle, capable of implementing anti-aliasing filters, does not prevent aliasing from taking place in the intermediate layers.

</p>
</details>

<details><summary><b>Learning from Demonstrations using Signal Temporal Logic</b>
<a href="https://arxiv.org/abs/2102.07730">arxiv:2102.07730</a>
&#x1F4C8; 8 <br>
<p>Aniruddh G. Puranic, Jyotirmoy V. Deshmukh, Stefanos Nikolaidis</p></summary>
<p>

**Abstract:** Learning-from-demonstrations is an emerging paradigm to obtain effective robot control policies for complex tasks via reinforcement learning without the need to explicitly design reward functions. However, it is susceptible to imperfections in demonstrations and also raises concerns of safety and interpretability in the learned control policies. To address these issues, we use Signal Temporal Logic to evaluate and rank the quality of demonstrations. Temporal logic-based specifications allow us to create non-Markovian rewards, and also define interesting causal dependencies between tasks such as sequential task specifications. We validate our approach through experiments on discrete-world and OpenAI Gym environments, and show that our approach outperforms the state-of-the-art Maximum Causal Entropy Inverse Reinforcement Learning.

</p>
</details>

<details><summary><b>Nonintrusive Uncertainty Quantification for automotive crash problems with VPS/Pamcrash</b>
<a href="https://arxiv.org/abs/2102.07673">arxiv:2102.07673</a>
&#x1F4C8; 8 <br>
<p>Marc Rocas, Alberto Garc√≠a-Gonz√°lez, Sergio Zlotnik, Xabier Larr√°yoz, Pedro D√≠ez</p></summary>
<p>

**Abstract:** Uncertainty Quantification (UQ) is a key discipline for computational modeling of complex systems, enhancing reliability of engineering simulations. In crashworthiness, having an accurate assessment of the behavior of the model uncertainty allows reducing the number of prototypes and associated costs. Carrying out UQ in this framework is especially challenging because it requires highly expensive simulations. In this context, surrogate models (metamodels) allow drastically reducing the computational cost of Monte Carlo process. Different techniques to describe the metamodel are considered, Ordinary Kriging, Polynomial Response Surfaces and a novel strategy (based on Proper Generalized Decomposition) denoted by Separated Response Surface (SRS). A large number of uncertain input parameters may jeopardize the efficiency of the metamodels. Thus, previous to define a metamodel, kernel Principal Component Analysis (kPCA) is found to be effective to simplify the model outcome description. A benchmark crash test is used to show the efficiency of combining metamodels with kPCA.

</p>
</details>

<details><summary><b>Neuro-algorithmic Policies enable Fast Combinatorial Generalization</b>
<a href="https://arxiv.org/abs/2102.07456">arxiv:2102.07456</a>
&#x1F4C8; 7 <br>
<p>Marin Vlastelica, Michal Rol√≠nek, Georg Martius</p></summary>
<p>

**Abstract:** Although model-based and model-free approaches to learning the control of systems have achieved impressive results on standard benchmarks, generalization to task variations is still lacking. Recent results suggest that generalization for standard architectures improves only after obtaining exhaustive amounts of data. We give evidence that generalization capabilities are in many cases bottlenecked by the inability to generalize on the combinatorial aspects of the problem. Furthermore, we show that for a certain subclass of the MDP framework, this can be alleviated by neuro-algorithmic architectures.
  Many control problems require long-term planning that is hard to solve generically with neural networks alone. We introduce a neuro-algorithmic policy architecture consisting of a neural network and an embedded time-dependent shortest path solver. These policies can be trained end-to-end by blackbox differentiation. We show that this type of architecture generalizes well to unseen variations in the environment already after seeing a few examples.

</p>
</details>

<details><summary><b>One Line To Rule Them All: Generating LO-Shot Soft-Label Prototypes</b>
<a href="https://arxiv.org/abs/2102.07834">arxiv:2102.07834</a>
&#x1F4C8; 6 <br>
<p>Ilia Sucholutsky, Nam-Hwui Kim, Ryan P. Browne, Matthias Schonlau</p></summary>
<p>

**Abstract:** Increasingly large datasets are rapidly driving up the computational costs of machine learning. Prototype generation methods aim to create a small set of synthetic observations that accurately represent a training dataset but greatly reduce the computational cost of learning from it. Assigning soft labels to prototypes can allow increasingly small sets of prototypes to accurately represent the original training dataset. Although foundational work on `less than one'-shot learning has proven the theoretical plausibility of learning with fewer than one observation per class, developing practical algorithms for generating such prototypes remains an unexplored territory. We propose a novel, modular method for generating soft-label prototypical lines that still maintains representational accuracy even when there are fewer prototypes than the number of classes in the data. In addition, we propose the Hierarchical Soft-Label Prototype k-Nearest Neighbor classification algorithm based on these prototypical lines. We show that our method maintains high classification accuracy while greatly reducing the number of prototypes required to represent a dataset, even when working with severely imbalanced and difficult data. Our code is available at https://github.com/ilia10000/SLkNN.

</p>
</details>

<details><summary><b>Efficient Learning with Arbitrary Covariate Shift</b>
<a href="https://arxiv.org/abs/2102.07802">arxiv:2102.07802</a>
&#x1F4C8; 6 <br>
<p>Adam Kalai, Varun Kanade</p></summary>
<p>

**Abstract:** We give an efficient algorithm for learning a binary function in a given class C of bounded VC dimension, with training data distributed according to P and test data according to Q, where P and Q may be arbitrary distributions over X. This is the generic form of what is called covariate shift, which is impossible in general as arbitrary P and Q may not even overlap. However, recently guarantees were given in a model called PQ-learning (Goldwasser et al., 2020) where the learner has: (a) access to unlabeled test examples from Q (in addition to labeled samples from P, i.e., semi-supervised learning); and (b) the option to reject any example and abstain from classifying it (i.e., selective classification). The algorithm of Goldwasser et al. (2020) requires an (agnostic) noise tolerant learner for C. The present work gives a polynomial-time PQ-learning algorithm that uses an oracle to a "reliable" learner for C, where reliable learning (Kalai et al., 2012) is a model of learning with one-sided noise. Furthermore, our reduction is optimal in the sense that we show the equivalence of reliable and PQ learning.

</p>
</details>

<details><summary><b>PeriodNet: A non-autoregressive waveform generation model with a structure separating periodic and aperiodic components</b>
<a href="https://arxiv.org/abs/2102.07786">arxiv:2102.07786</a>
&#x1F4C8; 6 <br>
<p>Yukiya Hono, Shinji Takaki, Kei Hashimoto, Keiichiro Oura, Yoshihiko Nankaku, Keiichi Tokuda</p></summary>
<p>

**Abstract:** We propose PeriodNet, a non-autoregressive (non-AR) waveform generation model with a new model structure for modeling periodic and aperiodic components in speech waveforms. The non-AR waveform generation models can generate speech waveforms parallelly and can be used as a speech vocoder by conditioning an acoustic feature. Since a speech waveform contains periodic and aperiodic components, both components should be appropriately modeled to generate a high-quality speech waveform. However, it is difficult to decompose the components from a natural speech waveform in advance. To address this issue, we propose a parallel model and a series model structure separating periodic and aperiodic components. The features of our proposed models are that explicit periodic and aperiodic signals are taken as input, and external periodic/aperiodic decomposition is not needed in training. Experiments using a singing voice corpus show that our proposed structure improves the naturalness of the generated waveform. We also show that the speech waveforms with a pitch outside of the training data range can be generated with more naturalness.

</p>
</details>

<details><summary><b>Overview of the TREC 2020 deep learning track</b>
<a href="https://arxiv.org/abs/2102.07662">arxiv:2102.07662</a>
&#x1F4C8; 6 <br>
<p>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos</p></summary>
<p>

**Abstract:** This is the second year of the TREC Deep Learning Track, with the goal of studying ad hoc ranking in the large training data regime. We again have a document retrieval task and a passage retrieval task, each with hundreds of thousands of human-labeled training queries. We evaluate using single-shot TREC-style evaluation, to give us a picture of which ranking methods work best when large data is available, with much more comprehensive relevance labeling on the small number of test queries. This year we have further evidence that rankers with BERT-style pretraining outperform other rankers in the large data regime.

</p>
</details>

<details><summary><b>Certifiably Robust Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2102.07559">arxiv:2102.07559</a>
&#x1F4C8; 6 <br>
<p>Ben Barrett, Alexander Camuto, Matthew Willetts, Tom Rainforth</p></summary>
<p>

**Abstract:** We introduce an approach for training Variational Autoencoders (VAEs) that are certifiably robust to adversarial attack. Specifically, we first derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount, with these bounds depending on certain key parameters such as the Lipschitz constants of the encoder and decoder. We then show how these parameters can be controlled, thereby providing a mechanism to ensure \textit{a priori} that a VAE will attain a desired level of robustness. Moreover, we extend this to a complete practical approach for training such VAEs to ensure our criteria are met. Critically, our method allows one to specify a desired level of robustness \emph{upfront} and then train a VAE that is guaranteed to achieve this robustness. We further demonstrate that these Lipschitz--constrained VAEs are more robust to attack than standard VAEs in practice.

</p>
</details>

<details><summary><b>One-shot learning for the long term: consolidation with an artificial hippocampal algorithm</b>
<a href="https://arxiv.org/abs/2102.07503">arxiv:2102.07503</a>
&#x1F4C8; 6 <br>
<p>Gideon Kowadlo, Abdelrahman Ahmed, David Rawlinson</p></summary>
<p>

**Abstract:** Standard few-shot experiments involve learning to efficiently match previously unseen samples by class. We claim that few-shot learning should be long term, assimilating knowledge for the future, without forgetting previous concepts. In the mammalian brain, the hippocampus is understood to play a significant role in this process, by learning rapidly and consolidating knowledge to the neocortex incrementally over a short period. In this research we tested whether an artificial hippocampal algorithm (AHA), could be used with a conventional Machine Learning (ML) model that learns incrementally analogous to the neocortex, to achieve one-shot learning both short and long term. The results demonstrated that with the addition of AHA, the system could learn in one-shot and consolidate the knowledge for the long term without catastrophic forgetting. This study is one of the first examples of using a CLS model of hippocampus to consolidate memories, and it constitutes a step toward few-shot continual learning.

</p>
</details>

<details><summary><b>Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification</b>
<a href="https://arxiv.org/abs/2102.07856">arxiv:2102.07856</a>
&#x1F4C8; 5 <br>
<p>Yu Bai, Song Mei, Huan Wang, Caiming Xiong</p></summary>
<p>

**Abstract:** Modern machine learning models with high accuracy are often miscalibrated -- the predicted top probability does not reflect the actual accuracy, and tends to be over-confident. It is commonly believed that such over-confidence is mainly due to over-parametrization, in particular when the model is large enough to memorize the training data and maximize the confidence.
  In this paper, we show theoretically that over-parametrization is not the only reason for over-confidence. We prove that logistic regression is inherently over-confident, in the realizable, under-parametrized setting where the data is generated from the logistic model, and the sample size is much larger than the number of parameters. Further, this over-confidence happens for general well-specified binary classification problems as long as the activation is symmetric and concave on the positive part. Perhaps surprisingly, we also show that over-confidence is not always the case -- there exists another activation function (and a suitable loss function) under which the learned classifier is under-confident at some probability values. Overall, our theory provides a precise characterization of calibration in realizable binary classification, which we verify on simulations and real data experiments.

</p>
</details>

<details><summary><b>Secure-UCB: Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification</b>
<a href="https://arxiv.org/abs/2102.07711">arxiv:2102.07711</a>
&#x1F4C8; 5 <br>
<p>Anshuka Rangi, Long Tran-Thanh, Haifeng Xu, Massimo Franceschetti</p></summary>
<p>

**Abstract:** This paper studies bandit algorithms under data poisoning attacks in a bounded reward setting. We consider a strong attacker model in which the attacker can observe both the selected actions and their corresponding rewards, and can contaminate the rewards with additive noise. We show that \emph{any} bandit algorithm with regret $O(\log T)$ can be forced to suffer a regret $Œ©(T)$ with an expected amount of contamination $O(\log T)$. This amount of contamination is also necessary, as we prove that there exists an $O(\log T)$ regret bandit algorithm, specifically the classical UCB, that requires $Œ©(\log T)$ amount of contamination to suffer regret $Œ©(T)$. To combat such poising attacks, our second main contribution is to propose a novel algorithm, Secure-UCB, which uses limited \emph{verification} to access a limited number of uncontaminated rewards. We show that with $O(\log T)$ expected number of verifications, Secure-UCB can restore the order optimal $O(\log T)$ regret \emph{irrespective of the amount of contamination} used by the attacker. Finally, we prove that for any bandit algorithm, this number of verifications $O(\log T)$ is necessary to recover the order-optimal regret. We can then conclude that Secure-UCB is order-optimal in terms of both the expected regret and the expected number of verifications, and can save stochastic bandits from any data poisoning attack.

</p>
</details>

<details><summary><b>Improved Customer Transaction Classification using Semi-Supervised Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2102.07635">arxiv:2102.07635</a>
&#x1F4C8; 5 <br>
<p>Rohan Sukumaran</p></summary>
<p>

**Abstract:** In pickup and delivery services, transaction classification based on customer provided free text is a challenging problem. It involves the association of a wide variety of customer inputs to a fixed set of categories while adapting to the various customer writing styles. This categorization is important for the business: it helps understand the market needs and trends, and also assist in building a personalized experience for different segments of the customers. Hence, it is vital to capture these category information trends at scale, with high precision and recall. In this paper, we focus on a specific use-case where a single category drives each transaction. We propose a cost-effective transaction classification approach based on semi-supervision and knowledge distillation frameworks. The approach identifies the category of a transaction using free text input given by the customer. We use weak labelling and notice that the performance gains are similar to that of using human-annotated samples. On a large internal dataset and on 20Newsgroup dataset, we see that RoBERTa performs the best for the categorization tasks. Further, using an ALBERT model (it has 33x fewer parameters vis-a-vis parameters of RoBERTa), with RoBERTa as the Teacher, we see a performance similar to that of RoBERTa and better performance over unadapted ALBERT. This framework, with ALBERT as a student and RoBERTa as teacher, is further referred to as R-ALBERT in this paper. The model is in production and is used by business to understand changing trends and take appropriate decisions.

</p>
</details>

<details><summary><b>Learning image quality assessment by reinforcing task amenable data selection</b>
<a href="https://arxiv.org/abs/2102.07615">arxiv:2102.07615</a>
&#x1F4C8; 5 <br>
<p>Shaheer U. Saeed, Yunguan Fu, Zachary M. C. Baum, Qianye Yang, Mirabela Rusu, Richard E. Fan, Geoffrey A. Sonn, Dean C. Barratt, Yipeng Hu</p></summary>
<p>

**Abstract:** In this paper, we consider a type of image quality assessment as a task-specific measurement, which can be used to select images that are more amenable to a given target task, such as image classification or segmentation. We propose to train simultaneously two neural networks for image selection and a target task using reinforcement learning. A controller network learns an image selection policy by maximising an accumulated reward based on the target task performance on the controller-selected validation set, whilst the target task predictor is optimised using the training set. The trained controller is therefore able to reject those images that lead to poor accuracy in the target task. In this work, we show that the controller-predicted image quality can be significantly different from the task-specific image quality labels that are manually defined by humans. Furthermore, we demonstrate that it is possible to learn effective image quality assessment without using a ``clean'' validation set, thereby avoiding the requirement for human labelling of images with respect to their amenability for the task. Using $6712$, labelled and segmented, clinical ultrasound images from $259$ patients, experimental results on holdout data show that the proposed image quality assessment achieved a mean classification accuracy of $0.94\pm0.01$ and a mean segmentation Dice of $0.89\pm0.02$, by discarding $5\%$ and $15\%$ of the acquired images, respectively. The significantly improved performance was observed for both tested tasks, compared with the respective $0.90\pm0.01$ and $0.82\pm0.02$ from networks without considering task amenability. This enables image quality feedback during real-time ultrasound acquisition among many other medical imaging applications.

</p>
</details>

<details><summary><b>Tractable structured natural gradient descent using local parameterizations</b>
<a href="https://arxiv.org/abs/2102.07405">arxiv:2102.07405</a>
&#x1F4C8; 5 <br>
<p>Wu Lin, Frank Nielsen, Mohammad Emtiyaz Khan, Mark Schmidt</p></summary>
<p>

**Abstract:** Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank covariances) is computationally challenging due to difficult Fisher-matrix computations. We address this issue by using \emph{local-parameter coordinates} to obtain a flexible and efficient NGD method that works well for a wide-variety of structured parameterizations. We show four applications where our method (1) generalizes the exponential natural evolutionary strategy, (2) recovers existing Newton-like algorithms, (3) yields new structured second-order algorithms via matrix groups, and (4) gives new algorithms to learn covariances of Gaussian and Wishart-based distributions. We show results on a range of problems from deep learning, variational inference, and evolution strategies. Our work opens a new direction for scalable structured geometric methods.

</p>
</details>

<details><summary><b>TI-Capsule: Capsule Network for Stock Exchange Prediction</b>
<a href="https://arxiv.org/abs/2102.07718">arxiv:2102.07718</a>
&#x1F4C8; 4 <br>
<p>Ramin Mousa, Sara Nazari, Ali Karhe Abadi, Reza Shoukhcheshm, Mohammad Niknam Pirzadeh, Leila Safari</p></summary>
<p>

**Abstract:** Today, the use of social networking data has attracted a lot of academic and commercial attention in predicting the stock market. In most studies in this area, the sentiment analysis of the content of user posts on social networks is used to predict market fluctuations. Predicting stock marketing is challenging because of the variables involved. In the short run, the market behaves like a voting machine, but in the long run, it acts like a weighing machine. The purpose of this study is to predict EUR/USD stock behavior using Capsule Network on finance texts and Candlestick images. One of the most important features of Capsule Network is the maintenance of features in a vector, which also takes into account the space between features. The proposed model, TI-Capsule (Text and Image information based Capsule Neural Network), is trained with both the text and image information simultaneously. Extensive experiments carried on the collected dataset have demonstrated the effectiveness of TI-Capsule in solving the stock exchange prediction problem with 91% accuracy.

</p>
</details>

<details><summary><b>Scalable nonparametric Bayesian learning for heterogeneous and dynamic velocity fields</b>
<a href="https://arxiv.org/abs/2102.07695">arxiv:2102.07695</a>
&#x1F4C8; 4 <br>
<p>Sunrit Chakraborty, Aritra Guha, Rayleigh Lei, XuanLong Nguyen</p></summary>
<p>

**Abstract:** Analysis of heterogeneous patterns in complex spatio-temporal data finds usage across various domains in applied science and engineering, including training autonomous vehicles to navigate in complex traffic scenarios. Motivated by applications arising in the transportation domain, in this paper we develop a model for learning heterogeneous and dynamic patterns of velocity field data. We draw from basic nonparameric Bayesian modeling elements such as hierarchical Dirichlet process and infinite hidden Markov model, while the smoothness of each homogeneous velocity field element is captured with a Gaussian process prior. Of particular focus is a scalable approximate inference method for the proposed model; this is achieved by employing sequential MAP estimates from the infinite HMM model and an efficient sequential GP posterior computation technique, which is shown to work effectively on simulated data sets. Finally, we demonstrate the effectiveness of our techniques to the NGSIM dataset of complex multi-vehicle interactions.

</p>
</details>

<details><summary><b>A generalized quadratic loss for SVM and Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2102.07606">arxiv:2102.07606</a>
&#x1F4C8; 4 <br>
<p>Filippo Portera</p></summary>
<p>

**Abstract:** We consider some supervised binary classification tasks and a regression task, whereas SVM and Deep Learning, at present, exhibit the best generalization performances. We extend the work [3] on a generalized quadratic loss for learning problems that examines pattern correlations in order to concentrate the learning problem into input space regions where patterns are more densely distributed. From a shallow methods point of view (e.g.: SVM), since the following mathematical derivation of problem (9) in [3] is incorrect, we restart from problem (8) in [3] and we try to solve it with one procedure that iterates over the dual variables until the primal and dual objective functions converge. In addition we propose another algorithm that tries to solve the classification problem directly from the primal problem formulation. We make also use of Multiple Kernel Learning to improve generalization performances. Moreover, we introduce for the first time a custom loss that takes in consideration pattern correlation for a shallow and a Deep Learning task. We propose some pattern selection criteria and the results on 4 UCI data-sets for the SVM method. We also report the results on a larger binary classification data-set based on Twitter, again drawn from UCI, combined with shallow Learning Neural Networks, with and without the generalized quadratic loss. At last, we test our loss with a Deep Neural Network within a larger regression task taken from UCI. We compare the results of our optimizers with the well known solver SVMlight and with Keras Multi-Layers Neural Networks with standard losses and with a parameterized generalized quadratic loss, and we obtain comparable results.

</p>
</details>

<details><summary><b>On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers</b>
<a href="https://arxiv.org/abs/2102.07346">arxiv:2102.07346</a>
&#x1F4C8; 4 <br>
<p>Kenji Kawaguchi</p></summary>
<p>

**Abstract:** A deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.

</p>
</details>

<details><summary><b>KNH: Multi-View Modeling with K-Nearest Hyperplanes Graph for Misinformation Detection</b>
<a href="https://arxiv.org/abs/2102.07857">arxiv:2102.07857</a>
&#x1F4C8; 3 <br>
<p>Sara Abdali, Neil Shah, Evangelos E. Papalexakis</p></summary>
<p>

**Abstract:** Graphs are one of the most efficacious structures for representing datapoints and their relations, and they have been largely exploited for different applications. Previously, the higher-order relations between the nodes have been modeled by a generalization of graphs known as hypergraphs. In hypergraphs, the edges are defined by a set of nodes i.e., hyperedges to demonstrate the higher order relationships between the data. However, there is no explicit higher-order generalization for nodes themselves. In this work, we introduce a novel generalization of graphs i.e., K-Nearest Hyperplanes graph (KNH) where the nodes are defined by higher order Euclidean subspaces for multi-view modeling of the nodes. In fact, in KNH, nodes are hyperplanes or more precisely m-flats instead of datapoints. We experimentally evaluate the KNH graph on two multi-aspect datasets for misinformation detection. The experimental results suggest that multi-view modeling of articles using KNH graph outperforms the classic KNN graph in terms of classification performance.

</p>
</details>

<details><summary><b>Identifying Misinformation from Website Screenshots</b>
<a href="https://arxiv.org/abs/2102.07849">arxiv:2102.07849</a>
&#x1F4C8; 3 <br>
<p>Sara Abdali, Rutuja Gurav, Siddharth Menon, Daniel Fonseca, Negin Entezari, Neil Shah, Evangelos E. Papalexakis</p></summary>
<p>

**Abstract:** Can the look and the feel of a website give information about the trustworthiness of an article? In this paper, we propose to use a promising, yet neglected aspect in detecting the misinformativeness: the overall look of the domain webpage. To capture this overall look, we take screenshots of news articles served by either misinformative or trustworthy web domains and leverage a tensor decomposition based semi-supervised classification technique. The proposed approach i.e., VizFake is insensitive to a number of image transformations such as converting the image to grayscale, vectorizing the image and losing some parts of the screenshots. VizFake leverages a very small amount of known labels, mirroring realistic and practical scenarios, where labels (especially for known misinformative articles), are scarce and quickly become dated. The F1 score of VizFake on a dataset of 50k screenshots of news articles spanning more than 500 domains is roughly 85% using only 5% of ground truth labels. Furthermore, tensor representations of VizFake, obtained in an unsupervised manner, allow for exploratory analysis of the data that provides valuable insights into the problem. Finally, we compare VizFake with deep transfer learning, since it is a very popular black-box approach for image classification and also well-known text text-based methods. VizFake achieves competitive accuracy with deep transfer learning models while being two orders of magnitude faster and not requiring laborious hyper-parameter tuning.

</p>
</details>

<details><summary><b>Universal Adversarial Examples and Perturbations for Quantum Classifiers</b>
<a href="https://arxiv.org/abs/2102.07788">arxiv:2102.07788</a>
&#x1F4C8; 3 <br>
<p>Weiyuan Gong, Dong-Ling Deng</p></summary>
<p>

**Abstract:** Quantum machine learning explores the interplay between machine learning and quantum physics, which may lead to unprecedented perspectives for both fields. In fact, recent works have shown strong evidences that quantum computers could outperform classical computers in solving certain notable machine learning tasks. Yet, quantum learning systems may also suffer from the vulnerability problem: adding a tiny carefully-crafted perturbation to the legitimate input data would cause the systems to make incorrect predictions at a notably high confidence level. In this paper, we study the universality of adversarial examples and perturbations for quantum classifiers. Through concrete examples involving classifications of real-life images and quantum phases of matter, we show that there exist universal adversarial examples that can fool a set of different quantum classifiers. We prove that for a set of $k$ classifiers with each receiving input data of $n$ qubits, an $O(\frac{\ln k} {2^n})$ increase of the perturbation strength is enough to ensure a moderate universal adversarial risk. In addition, for a given quantum classifier we show that there exist universal adversarial perturbations, which can be added to different legitimate samples and make them to be adversarial examples for the classifier. Our results reveal the universality perspective of adversarial attacks for quantum machine learning systems, which would be crucial for practical applications of both near-term and future quantum technologies in solving machine learning problems.

</p>
</details>

<details><summary><b>Posterior-Aided Regularization for Likelihood-Free Inference</b>
<a href="https://arxiv.org/abs/2102.07770">arxiv:2102.07770</a>
&#x1F4C8; 3 <br>
<p>Dongjun Kim, Kyungwoo Song, Seungjae Shin, Wanmo Kang, Il-Chul Moon</p></summary>
<p>

**Abstract:** The recent development of likelihood-free inference aims training a flexible density estimator for the target posterior with a set of input-output pairs from simulation. Given the diversity of simulation structures, it is difficult to find a single unified inference method for each simulation model. This paper proposes a universally applicable regularization technique, called Posterior-Aided Regularization (PAR), which is applicable to learning the density estimator, regardless of the model structure. Particularly, PAR solves the mode collapse problem that arises as the output dimension of the simulation increases. PAR resolves this posterior mode degeneracy through a mixture of 1) the reverse KL divergence with the mode seeking property; and 2) the mutual information for the high quality representation on likelihood. Because of the estimation intractability of PAR, we provide a unified estimation method of PAR to estimate both reverse KL term and mutual information term with a single neural network. Afterwards, we theoretically prove the asymptotic convergence of the regularized optimal solution to the unregularized optimal solution as the regularization magnitude converges to zero. Additionally, we empirically show that past sequential neural likelihood inferences in conjunction with PAR present the statistically significant gains on diverse simulation tasks.

</p>
</details>

<details><summary><b>Fair and Optimal Cohort Selection for Linear Utilities</b>
<a href="https://arxiv.org/abs/2102.07684">arxiv:2102.07684</a>
&#x1F4C8; 3 <br>
<p>Konstantina Bairaktari, Huy Le Nguyen, Jonathan Ullman</p></summary>
<p>

**Abstract:** The rise of algorithmic decision-making has created an explosion of research around the fairness of those algorithms. While there are many compelling notions of individual fairness, beginning with the work of Dwork et al., these notions typically do not satisfy desirable composition properties. To this end, Dwork and Ilvento introduced the fair cohort selection problem, which captures a specific application where a single fair classifier is composed with itself to pick a group of candidates of size exactly $k$. In this work we introduce a specific instance of cohort selection where the goal is to choose a cohort maximizing a linear utility function. We give approximately optimal polynomial-time algorithms for this problem in both an offline setting where the entire fair classifier is given at once, or an online setting where candidates arrive one at a time and are classified as they arrive.

</p>
</details>

<details><summary><b>On Riemannian Stochastic Approximation Schemes with Fixed Step-Size</b>
<a href="https://arxiv.org/abs/2102.07586">arxiv:2102.07586</a>
&#x1F4C8; 3 <br>
<p>Alain Durmus, Pablo Jim√©nez, √âric Moulines, Salem Said</p></summary>
<p>

**Abstract:** This paper studies fixed step-size stochastic approximation (SA) schemes, including stochastic gradient schemes, in a Riemannian framework. It is motivated by several applications, where geodesics can be computed explicitly, and their use accelerates crude Euclidean methods. A fixed step-size scheme defines a family of time-homogeneous Markov chains, parametrized by the step-size. Here, using this formulation, non-asymptotic performance bounds are derived, under Lyapunov conditions. Then, for any step-size, the corresponding Markov chain is proved to admit a unique stationary distribution, and to be geometrically ergodic. This result gives rise to a family of stationary distributions indexed by the step-size, which is further shown to converge to a Dirac measure, concentrated at the solution of the problem at hand, as the step-size goes to 0. Finally, the asymptotic rate of this convergence is established, through an asymptotic expansion of the bias, and a central limit theorem.

</p>
</details>

<details><summary><b>Generating Structured Adversarial Attacks Using Frank-Wolfe Method</b>
<a href="https://arxiv.org/abs/2102.07360">arxiv:2102.07360</a>
&#x1F4C8; 3 <br>
<p>Ehsan Kazemi, Thomas Kerdreux, Liquang Wang</p></summary>
<p>

**Abstract:** White box adversarial perturbations are generated via iterative optimization algorithms most often by minimizing an adversarial loss on a $\ell_p$ neighborhood of the original image, the so-called distortion set. Constraining the adversarial search with different norms results in disparately structured adversarial examples. Here we explore several distortion sets with structure-enhancing algorithms. These new structures for adversarial examples might provide challenges for provable and empirical robust mechanisms. Because adversarial robustness is still an empirical field, defense mechanisms should also reasonably be evaluated against differently structured attacks. Besides, these structured adversarial perturbations may allow for larger distortions size than their $\ell_p$ counter-part while remaining imperceptible or perceptible as natural distortions of the image. We will demonstrate in this work that the proposed structured adversarial examples can significantly bring down the classification accuracy of adversarialy trained classifiers while showing low $\ell_2$ distortion rate. For instance, on ImagNet dataset the structured attacks drop the accuracy of adversarial model to near zero with only 50\% of $\ell_2$ distortion generated using white-box attacks like PGD. As a byproduct, our finding on structured adversarial examples can be used for adversarial regularization of models to make models more robust or improve their generalization performance on datasets which are structurally different.

</p>
</details>

<details><summary><b>Within-Document Event Coreference with BERT-Based Contextualized Representations</b>
<a href="https://arxiv.org/abs/2102.09600">arxiv:2102.09600</a>
&#x1F4C8; 2 <br>
<p>Shafiuddin Rehan Ahmed, James H. Martin</p></summary>
<p>

**Abstract:** Event coreference continues to be a challenging problem in information extraction. With the absence of any external knowledge bases for events, coreference becomes a clustering task that relies on effective representations of the context in which event mentions appear. Recent advances in contextualized language representations have proven successful in many tasks, however, their use in event linking been limited. Here we present a three part approach that (1) uses representations derived from a pretrained BERT model to (2) train a neural classifier to (3) drive a simple clustering algorithm to create coreference chains. We achieve state of the art results with this model on two standard datasets for within-document event coreference task and establish a new standard on a third newer dataset.

</p>
</details>

<details><summary><b>AlphaNet: Improved Training of Supernets with Alpha-Divergence</b>
<a href="https://arxiv.org/abs/2102.07954">arxiv:2102.07954</a>
&#x1F4C8; 2 <br>
<p>Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, Vikas Chandra</p></summary>
<p>

**Abstract:** Weight-sharing neural architecture search (NAS) is an effective technique for automating efficient neural architecture design. Weight-sharing NAS builds a supernet that assembles all the architectures as its sub-networks and jointly trains the supernet with the sub-networks. The success of weight-sharing NAS heavily relies on distilling the knowledge of the supernet to the sub-networks. However, we find that the widely used distillation divergence, i.e., KL divergence, may lead to student sub-networks that over-estimate or under-estimate the uncertainty of the teacher supernet, leading to inferior performance of the sub-networks. In this work, we propose to improve the supernet training with a more generalized alpha-divergence. By adaptively selecting the alpha-divergence, we simultaneously prevent the over-estimation or under-estimation of the uncertainty of the teacher model. We apply the proposed alpha-divergence based supernets training to both slimmable neural networks and weight-sharing NAS, and demonstrate significant improvements. Specifically, our discovered model family, AlphaNet, outperforms prior-art models on a wide range of FLOPs regimes, including BigNAS, Once-for-All networks, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0% with only 444M FLOPs. Our code and pretrained models are available at https://github.com/facebookresearch/AlphaNet.

</p>
</details>

<details><summary><b>Structured Graph Learning for Scalable Subspace Clustering: From Single-view to Multi-view</b>
<a href="https://arxiv.org/abs/2102.07943">arxiv:2102.07943</a>
&#x1F4C8; 2 <br>
<p>Zhao Kang, Zhiping Lin, Xiaofeng Zhu, Wenbo Xu</p></summary>
<p>

**Abstract:** Graph-based subspace clustering methods have exhibited promising performance. However, they still suffer some of these drawbacks: encounter the expensive time overhead, fail in exploring the explicit clusters, and cannot generalize to unseen data points. In this work, we propose a scalable graph learning framework, seeking to address the above three challenges simultaneously. Specifically, it is based on the ideas of anchor points and bipartite graph. Rather than building a $n\times n$ graph, where $n$ is the number of samples, we construct a bipartite graph to depict the relationship between samples and anchor points. Meanwhile, a connectivity constraint is employed to ensure that the connected components indicate clusters directly. We further establish the connection between our method and the K-means clustering. Moreover, a model to process multi-view data is also proposed, which is linear scaled with respect to $n$. Extensive experiments demonstrate the efficiency and effectiveness of our approach with respect to many state-of-the-art clustering methods.

</p>
</details>

<details><summary><b>Top-$k$ eXtreme Contextual Bandits with Arm Hierarchy</b>
<a href="https://arxiv.org/abs/2102.07800">arxiv:2102.07800</a>
&#x1F4C8; 2 <br>
<p>Rajat Sen, Alexander Rakhlin, Lexing Ying, Rahul Kidambi, Dean Foster, Daniel Hill, Inderjit Dhillon</p></summary>
<p>

**Abstract:** Motivated by modern applications, such as online advertisement and recommender systems, we study the top-$k$ extreme contextual bandits problem, where the total number of arms can be enormous, and the learner is allowed to select $k$ arms and observe all or some of the rewards for the chosen arms. We first propose an algorithm for the non-extreme realizable setting, utilizing the Inverse Gap Weighting strategy for selecting multiple arms. We show that our algorithm has a regret guarantee of $O(k\sqrt{(A-k+1)T \log (|\mathcal{F}|T)})$, where $A$ is the total number of arms and $\mathcal{F}$ is the class containing the regression function, while only requiring $\tilde{O}(A)$ computation per time step. In the extreme setting, where the total number of arms can be in the millions, we propose a practically-motivated arm hierarchy model that induces a certain structure in mean rewards to ensure statistical and computational efficiency. The hierarchical structure allows for an exponential reduction in the number of relevant arms for each context, thus resulting in a regret guarantee of $O(k\sqrt{(\log A-k+1)T \log (|\mathcal{F}|T)})$. Finally, we implement our algorithm using a hierarchical linear function class and show superior performance with respect to well-known benchmarks on simulated bandit feedback experiments using extreme multi-label classification datasets. On a dataset with three million arms, our reduction scheme has an average inference time of only 7.9 milliseconds, which is a 100x improvement.

</p>
</details>

<details><summary><b>Fast End-to-End Speech Recognition via Non-Autoregressive Models and Cross-Modal Knowledge Transferring from BERT</b>
<a href="https://arxiv.org/abs/2102.07594">arxiv:2102.07594</a>
&#x1F4C8; 2 <br>
<p>Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi Wen, Shuai Zhang</p></summary>
<p>

**Abstract:** Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because the decoder predicts text tokens (such as characters or words) in an autoregressive manner, it is difficult for an AED model to predict all tokens in parallel. This makes the inference speed relatively slow. We believe that because the encoder already captures the whole speech utterance, which has the token-level relationship implicitly, we can predict a token without explicitly autoregressive language modeling. When the prediction of a token does not rely on other tokens, the parallel prediction of all tokens in the sequence is realizable. Based on this idea, we propose a non-autoregressive speech recognition model called LASO (Listen Attentively, and Spell Once). The model consists of an encoder, a decoder, and a position dependent summarizer (PDS). The three modules are based on basic attention blocks. The encoder extracts high-level representations from the speech. The PDS uses positional encodings corresponding to tokens to convert the acoustic representations into token-level representations. The decoder further captures token-level relationships with the self-attention mechanism. At last, the probability distribution on the vocabulary is computed for each token position. Therefore, speech recognition is re-formulated as a position-wise classification problem. Further, we propose a cross-modal transfer learning method to refine semantics from a large-scale pre-trained language model BERT for improving the performance.

</p>
</details>

<details><summary><b>Machine Learning Model Development from a Software Engineering Perspective: A Systematic Literature Review</b>
<a href="https://arxiv.org/abs/2102.07574">arxiv:2102.07574</a>
&#x1F4C8; 2 <br>
<p>Giuliano Lorenzoni, Paulo Alencar, Nathalia Nascimento, Donald Cowan</p></summary>
<p>

**Abstract:** Data scientists often develop machine learning models to solve a variety of problems in the industry and academy but not without facing several challenges in terms of Model Development. The problems regarding Machine Learning Development involves the fact that such professionals do not realize that they usually perform ad-hoc practices that could be improved by the adoption of activities presented in the Software Engineering Development Lifecycle. Of course, since machine learning systems are different from traditional Software systems, some differences in their respective development processes are to be expected. In this context, this paper is an effort to investigate the challenges and practices that emerge during the development of ML models from the software engineering perspective by focusing on understanding how software developers could benefit from applying or adapting the traditional software engineering process to the Machine Learning workflow.

</p>
</details>

<details><summary><b>Cooperation and Reputation Dynamics with Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.07523">arxiv:2102.07523</a>
&#x1F4C8; 2 <br>
<p>Nicolas Anastassacos, Julian Garc√≠a, Stephen Hailes, Mirco Musolesi</p></summary>
<p>

**Abstract:** Creating incentives for cooperation is a challenge in natural and artificial systems. One potential answer is reputation, whereby agents trade the immediate cost of cooperation for the future benefits of having a good reputation. Game theoretical models have shown that specific social norms can make cooperation stable, but how agents can independently learn to establish effective reputation mechanisms on their own is less understood. We use a simple model of reinforcement learning to show that reputation mechanisms generate two coordination problems: agents need to learn how to coordinate on the meaning of existing reputations and collectively agree on a social norm to assign reputations to others based on their behavior. These coordination problems exhibit multiple equilibria, some of which effectively establish cooperation. When we train agents with a standard Q-learning algorithm in an environment with the presence of reputation mechanisms, convergence to undesirable equilibria is widespread. We propose two mechanisms to alleviate this: (i) seeding a proportion of the system with fixed agents that steer others towards good equilibria; and (ii), intrinsic rewards based on the idea of introspection, i.e., augmenting agents' rewards by an amount proportionate to the performance of their own strategy against themselves. A combination of these simple mechanisms is successful in stabilizing cooperation, even in a fully decentralized version of the problem where agents learn to use and assign reputations simultaneously. We show how our results relate to the literature in Evolutionary Game Theory, and discuss implications for artificial, human and hybrid systems, where reputations can be used as a way to establish trust and cooperation.

</p>
</details>

<details><summary><b>Distributed Online Learning for Joint Regret with Communication Constraints</b>
<a href="https://arxiv.org/abs/2102.07521">arxiv:2102.07521</a>
&#x1F4C8; 2 <br>
<p>Dirk van der Hoeven, H√©di Hadiji, Tim van Erven</p></summary>
<p>

**Abstract:** We consider distributed online learning for joint regret with communication constraints. In this setting, there are multiple agents that are connected in a graph. Each round, an adversary first activates one of the agents to issue a prediction and provides a corresponding gradient, and then the agents are allowed to send a $b$-bit message to their neighbors in the graph. All agents cooperate to control the joint regret, which is the sum of the losses of the activated agents minus the losses evaluated at the best fixed common comparator parameters $u$. We observe that it is suboptimal for agents to wait for gradients that take too long to arrive. Instead, the graph should be partitioned into local clusters that communicate among themselves. Our main result is a new method that can adapt to the optimal graph partition for the adversarial activations and gradients, where the graph partition is selected from a set of candidate partitions. A crucial building block along the way is a new algorithm for online convex optimization with delayed gradient information that is comparator-adaptive, meaning that its joint regret scales with the norm of the comparator $||u||$. We further provide near-optimal gradient compression schemes depending on the ratio of $b$ and the dimension times the diameter of the graph.

</p>
</details>

<details><summary><b>Fast and accurate optimization on the orthogonal manifold without retraction</b>
<a href="https://arxiv.org/abs/2102.07432">arxiv:2102.07432</a>
&#x1F4C8; 2 <br>
<p>Pierre Ablin, Gabriel Peyr√©</p></summary>
<p>

**Abstract:** We consider the problem of minimizing a function over the manifold of orthogonal matrices. The majority of algorithms for this problem compute a direction in the tangent space, and then use a retraction to move in that direction while staying on the manifold. Unfortunately, the numerical computation of retractions on the orthogonal manifold always involves some expensive linear algebra operation, such as matrix inversion or matrix square-root. These operations quickly become expensive as the dimension of the matrices grows. To bypass this limitation, we propose the landing algorithm which does not involve retractions. The algorithm is not constrained to stay on the manifold but its evolution is driven by a potential energy which progressively attracts it towards the manifold. One iteration of the landing algorithm only involves matrix multiplications, which makes it cheap compared to its retraction counterparts. We provide an analysis of the convergence of the algorithm, and demonstrate its promises on large-scale problems, where it is faster and less prone to numerical errors than retraction-based methods.

</p>
</details>

<details><summary><b>DiffCo: Auto-Differentiable Proxy Collision Detection with Multi-class Labels for Safety-Aware Trajectory Optimization</b>
<a href="https://arxiv.org/abs/2102.07413">arxiv:2102.07413</a>
&#x1F4C8; 2 <br>
<p>Yuheng Zhi, Nikhil Das, Michael Yip</p></summary>
<p>

**Abstract:** The objective of trajectory optimization algorithms is to achieve an optimal collision-free path between a start and goal state. In real-world scenarios where environments can be complex and non-homogeneous, a robot needs to be able to gauge whether a state will be in collision with various objects in order to meet some safety metrics. The collision detector should be computationally efficient and, ideally, analytically differentiable to facilitate stable and rapid gradient descent during optimization. However, methods today lack an elegant approach to detect collision differentiably, relying rather on numerical gradients that can be unstable. We present DiffCo, the first, fully auto-differentiable, non-parametric model for collision detection. Its non-parametric behavior allows one to compute collision boundaries on-the-fly and update them, requiring no pre-training and allowing it to update continuously in dynamic environments. It provides robust gradients for trajectory optimization via backpropagation and is often 10-100x faster to compute than its geometric counterparts. DiffCo also extends trivially to modeling different object collision classes for semantically informed trajectory optimization.

</p>
</details>

<details><summary><b>Almost Optimal Algorithms for Two-player Markov Games with Linear Function Approximation</b>
<a href="https://arxiv.org/abs/2102.07404">arxiv:2102.07404</a>
&#x1F4C8; 2 <br>
<p>Zixiang Chen, Dongruo Zhou, Quanquan Gu</p></summary>
<p>

**Abstract:** We study reinforcement learning for two-player zero-sum Markov games with simultaneous moves in the finite-horizon setting, where the transition kernel of the underlying Markov games can be parameterized by a linear function over the current state, both players' actions and the next state. In particular, we assume that we can control both players and aim to find the Nash Equilibrium by minimizing the duality gap. We propose an algorithm Nash-UCRL-VTR based on the principle "Optimism-in-Face-of-Uncertainty". Our algorithm only needs to find a Coarse Correlated Equilibrium (CCE), which is computationally very efficient. Specifically, we show that Nash-UCRL-VTR can provably achieve an $\tilde{O}(dH\sqrt{T})$ regret, where $d$ is the linear function dimension, $H$ is the length of the game and $T$ is the total number of steps in the game. To access the optimality of our algorithm, we also prove an $\tildeŒ©( dH\sqrt{T})$ lower bound on the regret. Our upper bound matches the lower bound up to logarithmic factors, which suggests the optimality of our algorithm.

</p>
</details>

<details><summary><b>And/or trade-off in artificial neurons: impact on adversarial robustness</b>
<a href="https://arxiv.org/abs/2102.07389">arxiv:2102.07389</a>
&#x1F4C8; 2 <br>
<p>Alessandro Fontana</p></summary>
<p>

**Abstract:** Since its discovery in 2013, the phenomenon of adversarial examples has attracted a growing amount of attention from the machine learning community. A deeper understanding of the problem could lead to a better comprehension of how information is processed and encoded in neural networks and, more in general, could help to solve the issue of interpretability in machine learning. Our idea to increase adversarial resilience starts with the observation that artificial neurons can be divided in two broad categories: AND-like neurons and OR-like neurons. Intuitively, the former are characterised by a relatively low number of combinations of input values which trigger neuron activation, while for the latter the opposite is true. Our hypothesis is that the presence in a network of a sufficiently high number of OR-like neurons could lead to classification "brittleness" and increase the network's susceptibility to adversarial attacks. After constructing an operational definition of a neuron AND-like behaviour, we proceed to introduce several measures to increase the proportion of AND-like neurons in the network: L1 norm weight normalisation; application of an input filter; comparison between the neuron output's distribution obtained when the network is fed with the actual data set and the distribution obtained when the network is fed with a randomised version of the former called "scrambled data set". Tests performed on the MNIST data set hint that the proposed measures could represent an interesting direction to explore.

</p>
</details>

<details><summary><b>Win-Fail Action Recognition</b>
<a href="https://arxiv.org/abs/2102.07355">arxiv:2102.07355</a>
&#x1F4C8; 2 <br>
<p>Paritosh Parmar, Brendan Morris</p></summary>
<p>

**Abstract:** Current video/action understanding systems have demonstrated impressive performance on large recognition tasks. However, they might be limiting themselves to learning to recognize spatiotemporal patterns, rather than attempting to thoroughly understand the actions. To spur progress in the direction of a truer, deeper understanding of videos, we introduce the task of win-fail action recognition -- differentiating between successful and failed attempts at various activities. We introduce a first of its kind paired win-fail action understanding dataset with samples from the following domains: "General Stunts," "Internet Wins-Fails," "Trick Shots," and "Party Games." Unlike existing action recognition datasets, intra-class variation is high making the task challenging, yet feasible. We systematically analyze the characteristics of the win-fail task/dataset with prototypical action recognition networks and a novel video retrieval task. While current action recognition methods work well on our task/dataset, they still leave a large gap to achieve high performance. We hope to motivate more work towards the true understanding of actions/videos. Dataset will be available from https://github.com/ParitoshParmar/Win-Fail-Action-Recognition.

</p>
</details>

<details><summary><b>Surface Warping Incorporating Machine Learning Assisted Domain Likelihood Estimation: A New Paradigm in Mine Geology Modelling and Automation</b>
<a href="https://arxiv.org/abs/2103.03923">arxiv:2103.03923</a>
&#x1F4C8; 1 <br>
<p>Raymond Leung, Mehala Balamurali, Alexander Lowe</p></summary>
<p>

**Abstract:** This paper illustrates an application of machine learning (ML) within a complex system that performs grade estimation. In surface mining, assay measurements taken from production drilling often provide useful information that allows initially inaccurate surfaces created using sparse exploration data to be revised and subsequently improved. Recently, a Bayesian warping technique has been proposed to reshape modeled surfaces using geochemical and spatial constraints imposed by newly acquired blasthole data. This paper focuses on incorporating machine learning into this warping framework to make the likelihood computation generalizable. The technique works by adjusting the position of vertices on the surface to maximize the integrity of modeled geological boundaries with respect to sparse geochemical observations. Its foundation is laid by a Bayesian derivation in which the geological domain likelihood given the chemistry, p(g|c), plays a similar role to p(y(c)|g). This observation allows a manually calibrated process centered around the latter to be automated since ML techniques may be used to estimate the former in a data-driven way. Machine learning performance is evaluated for gradient boosting, neural network, random forest and other classifiers in a binary and multi-class context using precision and recall rates. Once ML likelihood estimators are integrated in the surface warping framework, surface shaping performance is evaluated using unseen data by examining the categorical distribution of test samples located above and below the warped surface. Large-scale validation experiments are performed to assess the overall efficacy of ML assisted surface warping as a fully integrated component within an ore grade estimation system where the posterior mean is obtained via Gaussian Process inference with a Matern 3/2 kernel.

</p>
</details>

<details><summary><b>HSR: Hyperbolic Social Recommender</b>
<a href="https://arxiv.org/abs/2102.09389">arxiv:2102.09389</a>
&#x1F4C8; 1 <br>
<p>Anchen Li, Bo Yang</p></summary>
<p>

**Abstract:** With the prevalence of online social media, users' social connections have been widely studied and utilized to enhance the performance of recommender systems. In this paper, we explore the use of hyperbolic geometry for social recommendation. We present Hyperbolic Social Recommender (HSR), a novel social recommendation framework that utilizes hyperbolic geometry to boost the performance. With the help of hyperbolic spaces, HSR can learn high-quality user and item representations for better modeling user-item interaction and user-user social relations. Via a series of extensive experiments, we show that our proposed HSR outperforms its Euclidean counterpart and state-of-the-art social recommenders in click-through rate prediction and top-K recommendation, demonstrating the effectiveness of social recommendation in the hyperbolic space.

</p>
</details>

<details><summary><b>Efficient Discretizations of Optimal Transport</b>
<a href="https://arxiv.org/abs/2102.07956">arxiv:2102.07956</a>
&#x1F4C8; 1 <br>
<p>Junqi Wang, Pei Wang, Patrick Shafto</p></summary>
<p>

**Abstract:** Obtaining solutions to Optimal Transportation (OT) problems is typically intractable when the marginal spaces are continuous. Recent research has focused on approximating continuous solutions with discretization methods based on i.i.d. sampling, and has proven convergence as the sample size increases. However, obtaining OT solutions with large sample sizes requires intensive computation effort, that can be prohibitive in practice. In this paper, we propose an algorithm for calculating discretizations with a given number of points for marginal distributions, by minimizing the (entropy-regularized) Wasserstein distance, and result in plans that are comparable to those obtained with much larger numbers of i.i.d. samples. Moreover, a local version of such discretizations which is parallelizable for large scale applications is proposed. We prove bounds for our approximation and demonstrate performance on a wide range of problems.

</p>
</details>

<details><summary><b>A Hidden Challenge of Link Prediction: Which Pairs to Check?</b>
<a href="https://arxiv.org/abs/2102.07878">arxiv:2102.07878</a>
&#x1F4C8; 1 <br>
<p>Caleb Belth, Alican B√ºy√ºk√ßakƒ±r, Danai Koutra</p></summary>
<p>

**Abstract:** The traditional setup of link prediction in networks assumes that a test set of node pairs, which is usually balanced, is available over which to predict the presence of links. However, in practice, there is no test set: the ground-truth is not known, so the number of possible pairs to predict over is quadratic in the number of nodes in the graph. Moreover, because graphs are sparse, most of these possible pairs will not be links. Thus, link prediction methods, which often rely on proximity-preserving embeddings or heuristic notions of node similarity, face a vast search space, with many pairs that are in close proximity, but that should not be linked. To mitigate this issue, we introduce LinkWaldo, a framework for choosing from this quadratic, massively-skewed search space of node pairs, a concise set of candidate pairs that, in addition to being in close proximity, also structurally resemble the observed edges. This allows it to ignore some high-proximity but low-resemblance pairs, and also identify high-resemblance, lower-proximity pairs. Our framework is built on a model that theoretically combines Stochastic Block Models (SBMs) with node proximity models. The block structure of the SBM maps out where in the search space new links are expected to fall, and the proximity identifies the most plausible links within these blocks, using locality sensitive hashing to avoid expensive exhaustive search. LinkWaldo can use any node representation learning or heuristic definition of proximity, and can generate candidate pairs for any link prediction method, allowing the representation power of current and future methods to be realized for link prediction in practice. We evaluate LinkWaldo on 13 networks across multiple domains, and show that on average it returns candidate sets containing 7-33% more missing and future links than both embedding-based and heuristic baselines' sets.

</p>
</details>

<details><summary><b>Corneal Pachymetry by AS-OCT after Descemet's Membrane Endothelial Keratoplasty</b>
<a href="https://arxiv.org/abs/2102.07846">arxiv:2102.07846</a>
&#x1F4C8; 1 <br>
<p>Friso G. Heslinga, Ruben T. Lucassen, Myrthe A. van den Berg, Luuk van der Hoek, Josien P. W. Pluim, Javier Cabrerizo, Mark Alberti, Mitko Veta</p></summary>
<p>

**Abstract:** Corneal thickness (pachymetry) maps can be used to monitor restoration of corneal endothelial function, for example after Descemet's membrane endothelial keratoplasty (DMEK). Automated delineation of the corneal interfaces in anterior segment optical coherence tomography (AS-OCT) can be challenging for corneas that are irregularly shaped due to pathology, or as a consequence of surgery, leading to incorrect thickness measurements. In this research, deep learning is used to automatically delineate the corneal interfaces and measure corneal thickness with high accuracy in post-DMEK AS-OCT B-scans. Three different deep learning strategies were developed based on 960 B-scans from 50 patients. On an independent test set of 320 B-scans, corneal thickness could be measured with an error of 13.98 to 15.50 micrometer for the central 9 mm range, which is less than 3% of the average corneal thickness. The accurate thickness measurements were used to construct detailed pachymetry maps. Moreover, follow-up scans could be registered based on anatomical landmarks to obtain differential pachymetry maps. These maps may enable a more comprehensive understanding of the restoration of the endothelial function after DMEK, where thickness often varies throughout different regions of the cornea, and subsequently contribute to a standardized postoperative regime.

</p>
</details>

<details><summary><b>Controlling False Discovery Rates under Cross-Sectional Correlations</b>
<a href="https://arxiv.org/abs/2102.07826">arxiv:2102.07826</a>
&#x1F4C8; 1 <br>
<p>Junpei Komiyama, Masaya Abe, Kei Nakagawa, Kenichiro McAlinn</p></summary>
<p>

**Abstract:** We consider controlling the false discovery rate for testing many time series with an unknown cross-sectional correlation structure. Given a large number of hypotheses, false and missing discoveries can plague an analysis. While many procedures have been proposed to control false discovery, most of them either assume independent hypotheses or lack statistical power. A problem of particular interest is in financial asset pricing, where the goal is to determine which ``factors" lead to excess returns out of a large number of potential factors. Our contribution is two-fold. First, we show the consistency of Fama and French's prominent method under multiple testing. Second, we propose a novel method for false discovery control using double bootstrapping. We achieve superior statistical power to existing methods and prove that the false discovery rate is controlled. Simulations and a real data application illustrate the efficacy of our method over existing methods.

</p>
</details>

<details><summary><b>KnowledgeCheckR: Intelligent Techniques for Counteracting Forgetting</b>
<a href="https://arxiv.org/abs/2102.07825">arxiv:2102.07825</a>
&#x1F4C8; 1 <br>
<p>Martin Stettinger, Trang Tran, Ingo Pribik, Gerhard Leitner, Alexander Felfernig, Ralph Samer, Muesluem Atas, Manfred Wundara</p></summary>
<p>

**Abstract:** Existing e-learning environments primarily focus on the aspect of providing intuitive learning contents and to recommend learning units in a personalized fashion. The major focus of the KnowledgeCheckR environment is to take into account forgetting processes which immediately start after a learning unit has been completed. In this context, techniques are needed that are able to predict which learning units are the most relevant ones to be repeated in future learning sessions. In this paper, we provide an overview of the recommendation approaches integrated in KnowledgeCheckR. Examples thereof are utility-based recommendation that helps to identify learning contents to be repeated in the future, collaborative filtering approaches that help to implement session-based recommendation, and content-based recommendation that supports intelligent question answering. In order to show the applicability of the presented techniques, we provide an overview of the results of empirical studies that have been conducted in real-world scenarios.

</p>
</details>

<details><summary><b>Enhancing the Spatio-temporal Observability of Grid-Edge Resources in Distribution Grids</b>
<a href="https://arxiv.org/abs/2102.07801">arxiv:2102.07801</a>
&#x1F4C8; 1 <br>
<p>Shanny Lin, Hao Zhu</p></summary>
<p>

**Abstract:** Enhancing the spatio-temporal observability of distributed energy resources (DERs) is crucial for achieving secure and efficient operations in distribution grids. This paper puts forth a joint recovery framework for residential loads by leveraging the complimentary strengths of heterogeneous measurements in real time. The proposed framework integrates low-resolution smart meter data collected at every load node with fast-sampled feeder-level measurements from limited number of distribution phasor measurement units. To address the lack of data, we exploit two key characteristics for the loads and DERs, namely the sparse changes due to infrequent activities of appliances and electric vehicles (EVs) and the locational dependence of solar photovoltaic (PV) generation. Accordingly, meaningful regularization terms are introduced to cast a convex load recovery problem, which will be further simplified to reduce the computational complexity. The load recovery solutions can be utilized to identify the EV charging events at each load node and to infer the total behind-the-meter PV output. Numerical tests using real-world data have demonstrated the effectiveness of the proposed approaches in enhancing the visibility of grid-edge DERs.

</p>
</details>

<details><summary><b>Colored Kimia Path24 Dataset: Configurations and Benchmarks with Deep Embeddings</b>
<a href="https://arxiv.org/abs/2102.07611">arxiv:2102.07611</a>
&#x1F4C8; 1 <br>
<p>Sobhan Shafiei, Morteza Babaie, Shivam Kalra, H. R. Tizhoosh</p></summary>
<p>

**Abstract:** The Kimia Path24 dataset has been introduced as a classification and retrieval dataset for digital pathology. Although it provides multi-class data, the color information has been neglected in the process of extracting patches. The staining information plays a major role in the recognition of tissue patterns. To address this drawback, we introduce the color version of Kimia Path24 by recreating sample patches from all 24 scans to propose Kimia Path24C. We run extensive experiments to determine the best configuration for selected patches. To provide preliminary results for setting a benchmark for the new dataset, we utilize VGG16, InceptionV3 and DenseNet-121 model as feature extractors. Then, we use these feature vectors to retrieve test patches. The accuracy of image retrieval using DenseNet was 95.92% while the highest accuracy using InceptionV3 and VGG16 reached 92.45% and 92%, respectively. We also experimented with "deep barcodes" and established that with a small loss in accuracy (e.g., 93.43% for binarized features for DenseNet instead of 95.92% when the features themselves are used), the search operations can be significantly accelerated.

</p>
</details>

<details><summary><b>A Reference Model for IoT Embodied Agents Controlled by Neural Networks</b>
<a href="https://arxiv.org/abs/2102.07589">arxiv:2102.07589</a>
&#x1F4C8; 1 <br>
<p>Nathalia Nascimento, Paulo Alencar, Donald Cowan, Carlos Lucena</p></summary>
<p>

**Abstract:** Embodied agents is a term used to denote intelligent agents, which are a component of devices belonging to the Internet of Things (IoT) domain. Each agent is provided with sensors and actuators to interact with the environment, and with a 'controller' that usually contains an artificial neural network (ANN). In previous publications, we introduced three software approaches to design, implement and test IoT embodied agents. In this paper, we propose a reference model based on statecharts that offers abstractions tailored to the development of IoT applications. The model represents embodied agents that are controlled by neural networks. Our model includes the ANN training process, represented as a reconfiguration step such as changing agent features or neural net connections. Our contributions include the identification of the main characteristics of IoT embodied agents, a reference model specification based on statecharts, and an illustrative application of the model to support autonomous street lights. The proposal aims to support the design and implementation of IoT applications by providing high-level design abstractions and models, thus enabling the designer to have a uniform approach to conceiving, designing and explaining such applications.

</p>
</details>

<details><summary><b>User Embedding based Neighborhood Aggregation Method for Inductive Recommendation</b>
<a href="https://arxiv.org/abs/2102.07575">arxiv:2102.07575</a>
&#x1F4C8; 1 <br>
<p>Rahul Ragesh, Sundararajan Sellamanickam, Vijay Lingam, Arun Iyer, Ramakrishna Bairi</p></summary>
<p>

**Abstract:** We consider the problem of learning latent features (aka embedding) for users and items in a recommendation setting. Given only a user-item interaction graph, the goal is to recommend items for each user. Traditional approaches employ matrix factorization-based collaborative filtering methods. Recent methods using graph convolutional networks (e.g., LightGCN) achieve state-of-the-art performance. They learn both user and item embedding. One major drawback of most existing methods is that they are not inductive; they do not generalize for users and items unseen during training. Besides, existing network models are quite complex, difficult to train and scale. Motivated by LightGCN, we propose a graph convolutional network modeling approach for collaborative filtering CF-GCN. We solely learn user embedding and derive item embedding using light variant CF-LGCN-U performing neighborhood aggregation, making it scalable due to reduced model complexity. CF-LGCN-U models naturally possess the inductive capability for new items, and we propose a simple solution to generalize for new users. We show how the proposed models are related to LightGCN. As a by-product, we suggest a simple solution to make LightGCN inductive. We perform comprehensive experiments on several benchmark datasets and demonstrate the capabilities of the proposed approach. Experimental results show that similar or better generalization performance is achievable than the state of the art methods in both transductive and inductive settings.

</p>
</details>

<details><summary><b>Scalable Vector Gaussian Information Bottleneck</b>
<a href="https://arxiv.org/abs/2102.07525">arxiv:2102.07525</a>
&#x1F4C8; 1 <br>
<p>Mohammad Mahdi Mahvari, Mari Kobayashi, Abdellatif Zaidi</p></summary>
<p>

**Abstract:** In the context of statistical learning, the Information Bottleneck method seeks a right balance between accuracy and generalization capability through a suitable tradeoff between compression complexity, measured by minimum description length, and distortion evaluated under logarithmic loss measure. In this paper, we study a variation of the problem, called scalable information bottleneck, in which the encoder outputs multiple descriptions of the observation with increasingly richer features. The model, which is of successive-refinement type with degraded side information streams at the decoders, is motivated by some application scenarios that require varying levels of accuracy depending on the allowed (or targeted) level of complexity. We establish an analytic characterization of the optimal relevance-complexity region for vector Gaussian sources. Then, we derive a variational inference type algorithm for general sources with unknown distribution; and show means of parametrizing it using neural networks. Finally, we provide experimental results on the MNIST dataset which illustrate that the proposed method generalizes better to unseen data during the training phase.

</p>
</details>

<details><summary><b>Short- and long-term prediction of a chaotic flow: A physics-constrained reservoir computing approach</b>
<a href="https://arxiv.org/abs/2102.07514">arxiv:2102.07514</a>
&#x1F4C8; 1 <br>
<p>Nguyen Anh Khoa Doan, Wolfgang Polifke, Luca Magri</p></summary>
<p>

**Abstract:** We propose a physics-constrained machine learning method-based on reservoir computing- to time-accurately predict extreme events and long-term velocity statistics in a model of turbulent shear flow. The method leverages the strengths of two different approaches: empirical modelling based on reservoir computing, which it learns the chaotic dynamics from data only, and physical modelling based on conservation laws, which extrapolates the dynamics when training data becomes unavailable. We show that the combination of the two approaches is able to accurately reproduce the velocity statistics and to predict the occurrence and amplitude of extreme events in a model of self-sustaining process in turbulence. In this flow, the extreme events are abrupt transitions from turbulent to quasi-laminar states, which are deterministic phenomena that cannot be traditionally predicted because of chaos. Furthermore, the physics-constrained machine learning method is shown to be robust with respect to noise. This work opens up new possibilities for synergistically enhancing data-driven methods with physical knowledge for the time-accurate prediction of chaotic flows.

</p>
</details>

<details><summary><b>ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.07495">arxiv:2102.07495</a>
&#x1F4C8; 1 <br>
<p>Naichen Shi, Ruichen Li, Sun Youran</p></summary>
<p>

**Abstract:** People have made remarkable progress in game AIs, especially in domain of perfect information game. However, trick-taking poker game, as a popular form of imperfect information game, has been regarded as a challenge for a long time. Since trick-taking game requires high level of not only reasoning, but also inference to excel, it can be a new milestone for imperfect information game AI. We study Gongzhu, a trick-taking game analogous to, but slightly simpler than contract bridge. Nonetheless, the strategies of Gongzhu are complex enough for both human and computer players. We train a strong Gongzhu AI ScrofaZero from \textit{tabula rasa} by deep reinforcement learning, while few previous efforts on solving trick-taking poker game utilize the representation power of neural networks. Also, we introduce new techniques for imperfect information game including stratified sampling, importance weighting, integral over equivalent class, Bayesian inference, etc. Our AI can achieve human expert level performance. The methodologies in building our program can be easily transferred into a wide range of trick-taking games.

</p>
</details>

<details><summary><b>Holographic Cell Stiffness Mapping Using Acoustic Stimulation</b>
<a href="https://arxiv.org/abs/2102.07480">arxiv:2102.07480</a>
&#x1F4C8; 1 <br>
<p>Rahmetullah Varol, Sevde Omeroglu, Zeynep Karavelioglu, Gizem Aydemir, Aslihan Karadag, Hanife Ecenur Meco, Gizem Calibasi Kocal, Muhammed Enes Oruc, Gokhan Bora Esmer, Yasemin Basbinar, Huseyin Uvet</p></summary>
<p>

**Abstract:** Accurate assessment of stiffness distribution is essential due to the critical role of single cell mechanobiology in the regulation of many vital cellular processes such as proliferation, adhesion, migration, and motility. Cell stiffness is one of the fundamental mechanical properties of the cell and is greatly affected by the intracellular tensional forces, cytoskeletal prestress, and cytoskeleton structure. Herein, we propose a novel holographic single-cell stiffness measurement technique that can obtain the stiffness distribution over a cell membrane at high resolution and in real-time. The proposed imaging method coupled with acoustic signals allows us to assess the cell stiffness distribution with a low error margin and label-free manner. We demonstrate the proposed technique on HCT116 (Human Colorectal Carcinoma) cells and CTC-mimicked HCT116 cells by induction with transforming growth factor-beta (TGF-\b{eta}). Validation studies of the proposed approach were carried out on certified polystyrene microbeads with known stiffness levels. Its performance was evaluated in comparison with the AFM results obtained for the relevant cells. When the experimental results were examined, the proposed methodology shows utmost performance over average cell stiffness values for HCT116, and CTC-mimicked HCT116 cells were found as 1.08 kPa, and 0.88 kPa, respectively. The results confirm that CTC-mimicked HCT116 cells lose their adhesion ability to enter the vascular circulation and metastasize. They also exhibit a softer stiffness profile compared to adherent forms of the cancer cells. Hence, the proposed technique is a significant, reliable, and faster alternative for in-vitro cell stiffness characterization tools. It can be utilized for various applications where single-cell analysis is required, such as disease modeling, drug testing, diagnostics, and many more.

</p>
</details>

<details><summary><b>Federated Dropout Learning for Hybrid Beamforming With Spatial Path Index Modulation In Multi-User mmWave-MIMO Systems</b>
<a href="https://arxiv.org/abs/2102.07450">arxiv:2102.07450</a>
&#x1F4C8; 1 <br>
<p>Ahmet M. Elbir, Sinem Coleri, Kumar Vijay Mishra</p></summary>
<p>

**Abstract:** Millimeter wave multiple-input multiple-output (mmWave-MIMO) systems with small number of radio-frequency (RF) chains have limited multiplexing gain. Spatial path index modulation (SPIM) is helpful in improving this gain by utilizing additional signal bits modulated by the indices of spatial paths. In this paper, we introduce model-based and model-free frameworks for beamformer design in multi-user SPIM-MIMO systems. We first design the beamformers via model-based manifold optimization algorithm. Then, we leverage federated learning (FL) with dropout learning (DL) to train a learning model on the local dataset of users, who estimate the beamformers by feeding the model with their channel data. The DL randomly selects different set of model parameters during training, thereby further reducing the transmission overhead compared to conventional FL. Numerical experiments show that the proposed framework exhibits higher spectral efficiency than the state-of-the-art SPIM-MIMO methods and mmWave-MIMO, which relies on the strongest propagation path. Furthermore, the proposed FL approach provides at least 10 times lower transmission overhead than the centralized learning techniques.

</p>
</details>

<details><summary><b>Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator</b>
<a href="https://arxiv.org/abs/2102.07358">arxiv:2102.07358</a>
&#x1F4C8; 1 <br>
<p>Shichao Xu, Lixu Wang, Yixuan Wang, Qi Zhu</p></summary>
<p>

**Abstract:** Data quantity and quality are crucial factors for data-driven learning methods. In some target problem domains, there are not many data samples available, which could significantly hinder the learning process. While data from similar domains may be leveraged to help through domain adaptation, obtaining high-quality labeled data for those source domains themselves could be difficult or costly. To address such challenges on data insufficiency for classification problem in a target domain, we propose a weak adaptation learning (WAL) approach that leverages unlabeled data from a similar source domain, a low-cost weak annotator that produces labels based on task-specific heuristics, labeling rules, or other methods (albeit with inaccuracy), and a small amount of labeled data in the target domain. Our approach first conducts a theoretical analysis on the error bound of the trained classifier with respect to the data quantity and the performance of the weak annotator, and then introduces a multi-stage weak adaptation learning method to learn an accurate classifier by lowering the error bound. Our experiments demonstrate the effectiveness of our approach in learning an accurate classifier with limited labeled data in the target domain and unlabeled data in the source domain.

</p>
</details>

<details><summary><b>A Machine Learning Approach for Early Detection of Fish Diseases by Analyzing Water Quality</b>
<a href="https://arxiv.org/abs/2102.09390">arxiv:2102.09390</a>
&#x1F4C8; 0 <br>
<p>Al-Akhir Nayan, Ahamad Nokib Mozumder, Joyeta Saha, Khan Raqib Mahmud, Abul Kalam Al Azad, Muhammad Golam Kibria</p></summary>
<p>

**Abstract:** Early detection of fish diseases and identifying the underlying causes are crucial for farmers to take necessary steps to mitigate the potential outbreak and thus to avert financial losses with apparent negative implications to the national economy. Typically, fish diseases are caused by viruses and bacteria; according to biochemical studies, the presence of certain bacteria and viruses may affect the level of pH, DO, BOD, COD, TSS, TDS, EC, PO43-, NO3-N, and NH3-N in water, resulting in the death of fishes. Besides, natural processes, e.g., photosynthesis, respiration, and decomposition, also contribute to the alteration of water quality that adversely affects fish health. Being motivated by the recent successes of machine learning techniques, a state-of-art machine learning algorithm has been adopted in this paper to detect and predict the degradation of water quality timely and accurately. Thus, it helps to take preemptive steps against potential fish diseases. The experimental results show high accuracy in detecting fish diseases specific to water quality based on the algorithm with real datasets.

</p>
</details>

<details><summary><b>Deep Equilibrium Architectures for Inverse Problems in Imaging</b>
<a href="https://arxiv.org/abs/2102.07944">arxiv:2102.07944</a>
&#x1F4C8; 0 <br>
<p>Davis Gilton, Gregory Ongie, Rebecca Willett</p></summary>
<p>

**Abstract:** Recent efforts on solving inverse problems in imaging via deep neural networks use architectures inspired by a fixed number of iterations of an optimization method. The number of iterations is typically quite small due to difficulties in training networks corresponding to more iterations; the resulting solvers cannot be run for more iterations at test time without incurring significant errors. This paper describes an alternative approach corresponding to an infinite number of iterations, yielding a consistent improvement in reconstruction accuracy above state-of-the-art alternatives and where the computational budget can be selected at test time to optimize context-dependent trade-offs between accuracy and computation. The proposed approach leverages ideas from Deep Equilibrium Models, where the fixed-point iteration is constructed to incorporate a known forward model and insights from classical optimization-based reconstruction methods.

</p>
</details>

<details><summary><b>Inverse Reinforcement Learning in a Continuous State Space with Formal Guarantees</b>
<a href="https://arxiv.org/abs/2102.07937">arxiv:2102.07937</a>
&#x1F4C8; 0 <br>
<p>Gregory Dexter, Kevin Bello, Jean Honorio</p></summary>
<p>

**Abstract:** Inverse Reinforcement Learning (IRL) is the problem of finding a reward function which describes observed/known expert behavior. The IRL setting is remarkably useful for automated control, in situations where the reward function is difficult to specify manually or as a means to extract agent preference. In this work, we provide a new IRL algorithm for the continuous state space setting with unknown transition dynamics by modeling the system using a basis of orthonormal functions. Moreover, we provide a proof of correctness and formal guarantees on the sample and time complexity of our algorithm. Finally, we present synthetic experiments to corroborate our theoretical guarantees.

</p>
</details>

<details><summary><b>Structured Dropout Variational Inference for Bayesian Neural Networks</b>
<a href="https://arxiv.org/abs/2102.07927">arxiv:2102.07927</a>
&#x1F4C8; 0 <br>
<p>Son Nguyen, Duong Nguyen, Khai Nguyen, Khoat Than, Hung Bui, Nhat Ho</p></summary>
<p>

**Abstract:** Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high fidelity posterior approximations while maintaining computational efficiency and scalability. We tackle this challenge by introducing a novel variational structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inflexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complexity, and consequently induces statistical dependencies in the approximate posterior. Theoretically, VSD successfully addresses the pathologies of previous Variational Dropout methods and thus offers a standard Bayesian justification. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection.

</p>
</details>

<details><summary><b>Deep Neural Networks for Radar Waveform Classification</b>
<a href="https://arxiv.org/abs/2102.07827">arxiv:2102.07827</a>
&#x1F4C8; 0 <br>
<p>Michael Wharton, Anne M. Pavy, Philip Schniter</p></summary>
<p>

**Abstract:** We consider the problem of classifying radar pulses given raw I/Q waveforms in the presence of noise and absence of synchronization. We also consider the problem of classifying multiple superimposed radar pulses. For both, we design deep neural networks (DNNs) that are robust to synchronization, pulse width, and SNR. Our designs yield more than 100x reduction in error-rate over the previous state-of-the-art.

</p>
</details>

<details><summary><b>Zero-Shot Self-Supervised Learning for MRI Reconstruction</b>
<a href="https://arxiv.org/abs/2102.07737">arxiv:2102.07737</a>
&#x1F4C8; 0 <br>
<p>Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Mehmet Ak√ßakaya</p></summary>
<p>

**Abstract:** Deep learning (DL) has emerged as a powerful tool for accelerated MRI reconstruction, but these methods often necessitate a database of fully-sampled measurements for training. Recent self-supervised and unsupervised learning approaches enable training without fully-sampled data. However, a database of undersampled measurements may not be available in many scenarios, especially for scans involving contrast or recently developed translational acquisitions. Moreover, database-trained models may not generalize well when the unseen measurements differ in terms of sampling pattern, acceleration rate, SNR, image contrast, and anatomy. Such challenges necessitate a new methodology that can enable scan-specific DL MRI reconstruction without any external training datasets. In this work, we propose a zero-shot self-supervised learning approach to perform scan-specific accelerated MRI reconstruction to tackle these issues. The proposed approach splits available measurements for each scan into three disjoint sets. Two of these sets are used to enforce data consistency and define loss during training, while the last set is used to establish an early stopping criterion. In the presence of models pre-trained on a database with different image characteristics, we show that the proposed approach can be combined with transfer learning to further improve reconstruction quality.

</p>
</details>

<details><summary><b>Does the Adam Optimizer Exacerbate Catastrophic Forgetting?</b>
<a href="https://arxiv.org/abs/2102.07686">arxiv:2102.07686</a>
&#x1F4C8; 0 <br>
<p>Dylan R. Ashley, Sina Ghiassian, Richard S. Sutton</p></summary>
<p>

**Abstract:** Catastrophic forgetting remains a severe hindrance to the broad application of artificial neural networks (ANNs), however, it continues to be a poorly understood phenomenon. Despite the extensive amount of work on catastrophic forgetting, we argue that it is still unclear how exactly the phenomenon should be quantified, and, moreover, to what degree all of the choices we make when designing learning systems affect the amount of catastrophic forgetting. We use various testbeds from the reinforcement learning and supervised learning literature to (1) provide evidence that the choice of which modern gradient-based optimization algorithm is used to train an ANN has a significant impact on the amount of catastrophic forgetting and show that-surprisingly-in many instances classical algorithms such as vanilla SGD experience less catastrophic forgetting than the more modern algorithms such as Adam. We empirically compare four different existing metrics for quantifying catastrophic forgetting and (2) show that the degree to which the learning systems experience catastrophic forgetting is sufficiently sensitive to the metric used that a change from one principled metric to another is enough to change the conclusions of a study dramatically. Our results suggest that a much more rigorous experimental methodology is required when looking at catastrophic forgetting. Based on our results, we recommend inter-task forgetting in supervised learning must be measured with both retention and relearning metrics concurrently, and intra-task forgetting in reinforcement learning must-at the very least-be measured with pairwise interference.

</p>
</details>

<details><summary><b>Plug-and-Play gradient-based denoisers applied to CT image enhancement</b>
<a href="https://arxiv.org/abs/2102.07510">arxiv:2102.07510</a>
&#x1F4C8; 0 <br>
<p>Pasquale Cascarano, Elena Loli Piccolomini, Elena Morotti, Andrea Sebastiani</p></summary>
<p>

**Abstract:** Blur and noise corrupting Computed Tomography (CT) images can hide or distort small but important details, negatively affecting the diagnosis. In this paper, we present a novel gradient-based Plug-and-Play algorithm, constructed on the Half-Quadratic Splitting scheme, and we apply it to restore CT images. In particular, we consider different schemes encompassing external and internal denoisers as priors, defined on the image gradient domain. The internal prior is based on the Total Variation functional. The external denoiser is implemented by a deep Convolutional Neural Network (CNN) trained on the gradient domain (and not on the image one, as in state-of-the-art works). We also prove a general fixed-point convergence theorem under weak assumptions on both internal and external denoisers. The experiments confirm the effectiveness of the proposed framework in restoring blurred noisy CT images, both in simulated and real medical settings. The achieved enhancements in the restored images are really remarkable, if compared to the results of many state-of-the-art methods.

</p>
</details>

<details><summary><b>CLNet: Complex Input Lightweight Neural Network designed for Massive MIMO CSI Feedback</b>
<a href="https://arxiv.org/abs/2102.07507">arxiv:2102.07507</a>
&#x1F4C8; 0 <br>
<p>Sijie Ji, Mo Li</p></summary>
<p>

**Abstract:** The Massive Multiple Input Multiple Output (MIMO) system is a core technology of the next generation communication. With the growing complexity of CSI, CSI feedback in massive MIMO system has become a bottleneck problem, the traditional compressive sensing based CSI feedback approaches have limited performance. Recently, numerous deep learning based CSI feedback approaches demonstrate their efficiency and potential. However, most existing methods improve accuracy at the cost of computational complexity and the accuracy decreases significantly as the CSI compression rate increases. This paper presents a novel neural network CLNet tailored for CSI feedback problem based on the intrinsic properties of CSI. The experiment result shows that CLNet outperforms the state-of-the-art method by average accuracy improvement of 5.41% in both outdoor and indoor scenarios with average 24.1% less computational overhead. Codes are available at GitHub.

</p>
</details>

<details><summary><b>A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum</b>
<a href="https://arxiv.org/abs/2102.07367">arxiv:2102.07367</a>
&#x1F4C8; 0 <br>
<p>Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, Zhuoran Yang</p></summary>
<p>

**Abstract:** This paper proposes a new algorithm -- the \underline{S}ingle-timescale Do\underline{u}ble-momentum \underline{St}ochastic \underline{A}pprox\underline{i}matio\underline{n} (SUSTAIN) -- for tackling stochastic unconstrained bilevel optimization problems. We focus on bilevel problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike prior works which rely on \emph{two-timescale} or \emph{double loop} techniques, we design a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. The latter allows us to control the error in the stochastic gradient updates due to inaccurate solution to both subproblems. If the upper objective function is smooth but possibly non-convex, we show that {\aname}~requires $\mathcal{O}(Œµ^{-3/2})$ iterations (each using ${\cal O}(1)$ samples) to find an $Œµ$-stationary solution. The $Œµ$-stationary solution is defined as the point whose squared norm of the gradient of the outer function is less than or equal to $Œµ$. The total number of stochastic gradient samples required for the upper and lower level objective functions matches the best-known complexity for single-level stochastic gradient algorithms. We also analyze the case when the upper level objective function is strongly-convex.

</p>
</details>

<details><summary><b>A Unified Batch Selection Policy for Active Metric Learning</b>
<a href="https://arxiv.org/abs/2102.07365">arxiv:2102.07365</a>
&#x1F4C8; 0 <br>
<p>Priyadarshini K, Siddhartha Chaudhuri, Vivek Borkar, Subhasis Chaudhuri</p></summary>
<p>

**Abstract:** Active metric learning is the problem of incrementally selecting high-utility batches of training data (typically, ordered triplets) to annotate, in order to progressively improve a learned model of a metric over some input domain as rapidly as possible. Standard approaches, which independently assess the informativeness of each triplet in a batch, are susceptible to highly correlated batches with many redundant triplets and hence low overall utility. While a recent work \cite{kumari2020batch} proposes batch-decorrelation strategies for metric learning, they rely on ad hoc heuristics to estimate the correlation between two triplets at a time. We present a novel batch active metric learning method that leverages the Maximum Entropy Principle to learn the least biased estimate of triplet distribution for a given set of prior constraints. To avoid redundancy between triplets, our method collectively selects batches with maximum joint entropy, which simultaneously captures both informativeness and diversity. We take advantage of the submodularity of the joint entropy function to construct a tractable solution using an efficient greedy algorithm based on Gram-Schmidt orthogonalization that is provably $\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch active metric learning method to define a unified score that balances informativeness and diversity for an entire batch of triplets. Experiments with several real-world datasets demonstrate that our algorithm is robust, generalizes well to different applications and input modalities, and consistently outperforms the state-of-the-art.

</p>
</details>


{% endraw %}
Prev: [2021.02.14]({{ '/2021/02/14/2021.02.14.html' | relative_url }})  Next: [2021.02.16]({{ '/2021/02/16/2021.02.16.html' | relative_url }})