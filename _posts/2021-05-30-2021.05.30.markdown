## Summary for 2021-05-30, created on 2021-12-21


<details><summary><b>Introducing "Neuromorphic Computing and Engineering"</b>
<a href="https://arxiv.org/abs/2106.01329">arxiv:2106.01329</a>
&#x1F4C8; 1760 <br>
<p>Giacomo Indiveri</p></summary>
<p>

**Abstract:** The standard nature of computing is currently being challenged by a range of problems that start to hinder technological progress. One of the strategies being proposed to address some of these problems is to develop novel brain-inspired processing methods and technologies, and apply them to a wide range of application scenarios. This is an extremely challenging endeavor that requires researchers in multiple disciplines to combine their efforts and co-design at the same time the processing methods, the supporting computing architectures, and their underlying technologies. The journal ``Neuromorphic Computing and Engineering'' (NCE) has been launched to support this new community in this effort and provide a forum and repository for presenting and discussing its latest advances. Through close collaboration with our colleagues on the editorial team, the scope and characteristics of NCE have been designed to ensure it serves a growing transdisciplinary and dynamic community across academia and industry.

</p>
</details>

<details><summary><b>Cascaded Diffusion Models for High Fidelity Image Generation</b>
<a href="https://arxiv.org/abs/2106.15282">arxiv:2106.15282</a>
&#x1F4C8; 45 <br>
<p>Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans</p></summary>
<p>

**Abstract:** We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.

</p>
</details>

<details><summary><b>Embedding Principle of Loss Landscape of Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2105.14573">arxiv:2105.14573</a>
&#x1F4C8; 9 <br>
<p>Yaoyu Zhang, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu</p></summary>
<p>

**Abstract:** Understanding the structure of loss landscape of deep neural networks (DNNs)is obviously important. In this work, we prove an embedding principle that the loss landscape of a DNN "contains" all the critical points of all the narrower DNNs. More precisely, we propose a critical embedding such that any critical point, e.g., local or global minima, of a narrower DNN can be embedded to a critical point/hyperplane of the target DNN with higher degeneracy and preserving the DNN output function. The embedding structure of critical points is independent of loss function and training data, showing a stark difference from other nonconvex problems such as protein-folding. Empirically, we find that a wide DNN is often attracted by highly-degenerate critical points that are embedded from narrow DNNs. The embedding principle provides an explanation for the general easy optimization of wide DNNs and unravels a potential implicit low-complexity regularization during the training. Overall, our work provides a skeleton for the study of loss landscape of DNNs and its implication, by which a more exact and comprehensive understanding can be anticipated in the near

</p>
</details>

<details><summary><b>NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2105.14444">arxiv:2105.14444</a>
&#x1F4C8; 9 <br>
<p>Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, Tie-Yan Liu</p></summary>
<p>

**Abstract:** While pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.

</p>
</details>

<details><summary><b>Diversifying Dialog Generation via Adaptive Label Smoothing</b>
<a href="https://arxiv.org/abs/2105.14556">arxiv:2105.14556</a>
&#x1F4C8; 8 <br>
<p>Yida Wang, Yinhe Zheng, Yong Jiang, Minlie Huang</p></summary>
<p>

**Abstract:** Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an end-to-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.

</p>
</details>

<details><summary><b>MLPruning: A Multilevel Structured Pruning Framework for Transformer-based Models</b>
<a href="https://arxiv.org/abs/2105.14636">arxiv:2105.14636</a>
&#x1F4C8; 7 <br>
<p>Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, Michael W. Mahoney</p></summary>
<p>

**Abstract:** Pruning is an effective method to reduce the memory footprint and computational cost associated with large natural language processing models. However, current approaches either only explore head pruning, which has a limited pruning ratio, or only focus on unstructured pruning, which has negligible effects on the real inference time and/or power consumption. To address these challenges, we develop a novel MultiLevel structured Pruning (MLPruning) framework, which uses three different levels of structured pruning: head pruning, row pruning, and block-wise sparse pruning. We propose using a learnable Top-k threshold, which employs an adaptive regularization to adjust the regularization magnitude adaptively, to select appropriate pruning ratios for different weight matrices. We also propose a two-step pipeline to combine block-wise pruning with head/row pruning to achieve high structured pruning ratios with minimum accuracy degradation. Our empirical results show that for \bertbase, with \textapprox20\% of remaining weights, \OURS can achieve an accuracy that is comparable to the full model on QQP/MNLI/\squad, with up to \textapprox3.69x speedup. Our framework has been open sourced~\cite{codebase}.

</p>
</details>

<details><summary><b>Pre-training Universal Language Representation</b>
<a href="https://arxiv.org/abs/2105.14478">arxiv:2105.14478</a>
&#x1F4C8; 7 <br>
<p>Yian Li, Hai Zhao</p></summary>
<p>

**Abstract:** Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.

</p>
</details>

<details><summary><b>A Compression-Compilation Framework for On-mobile Real-time BERT Applications</b>
<a href="https://arxiv.org/abs/2106.00526">arxiv:2106.00526</a>
&#x1F4C8; 6 <br>
<p>Wei Niu, Zhenglun Kong, Geng Yuan, Weiwen Jiang, Jiexiong Guan, Caiwen Ding, Pu Zhao, Sijia Liu, Bin Ren, Yanzhi Wang</p></summary>
<p>

**Abstract:** Transformer-based deep learning models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. In this paper, we propose a compression-compilation co-design framework that can guarantee the identified model to meet both resource and real-time specifications of mobile devices. Our framework applies a compiler-aware neural architecture optimization method (CANAO), which can generate the optimal compressed model that balances both accuracy and latency. We are able to achieve up to 7.8x speedup compared with TensorFlow-Lite with only minor accuracy loss. We present two types of BERT applications on mobile devices: Question Answering (QA) and Text Generation. Both can be executed in real-time with latency as low as 45ms. Videos for demonstrating the framework can be found on https://www.youtube.com/watch?v=_WIRvK_2PZI

</p>
</details>

<details><summary><b>DAAIN: Detection of Anomalous and Adversarial Input using Normalizing Flows</b>
<a href="https://arxiv.org/abs/2105.14638">arxiv:2105.14638</a>
&#x1F4C8; 6 <br>
<p>Samuel von Baußnern, Johannes Otterbach, Adrian Loy, Mathieu Salzmann, Thomas Wollmann</p></summary>
<p>

**Abstract:** Despite much recent work, detecting out-of-distribution (OOD) inputs and adversarial attacks (AA) for computer vision models remains a challenge. In this work, we introduce a novel technique, DAAIN, to detect OOD inputs and AA for image segmentation in a unified setting. Our approach monitors the inner workings of a neural network and learns a density estimator of the activation distribution. We equip the density estimator with a classification head to discriminate between regular and anomalous inputs. To deal with the high-dimensional activation-space of typical segmentation networks, we subsample them to obtain a homogeneous spatial and layer-wise coverage. The subsampling pattern is chosen once per monitored model and kept fixed for all inputs. Since the attacker has access to neither the detection model nor the sampling key, it becomes harder for them to attack the segmentation network, as the attack cannot be backpropagated through the detector. We demonstrate the effectiveness of our approach using an ESPNet trained on the Cityscapes dataset as segmentation model, an affine Normalizing Flow as density estimator and use blue noise to ensure homogeneous sampling. Our model can be trained on a single GPU making it compute efficient and deployable without requiring specialized accelerators.

</p>
</details>

<details><summary><b>Sparse Uncertainty Representation in Deep Learning with Inducing Weights</b>
<a href="https://arxiv.org/abs/2105.14594">arxiv:2105.14594</a>
&#x1F4C8; 6 <br>
<p>Hippolyt Ritter, Martin Kukla, Cheng Zhang, Yingzhen Li</p></summary>
<p>

**Abstract:** Bayesian neural networks and deep ensembles represent two modern paradigms of uncertainty quantification in deep learning. Yet these approaches struggle to scale mainly due to memory inefficiency issues, since they require parameter storage several times higher than their deterministic counterparts. To address this, we augment the weight matrix of each layer with a small number of inducing weights, thereby projecting the uncertainty quantification into such low dimensional spaces. We further extend Matheron's conditional Gaussian sampling rule to enable fast weight sampling, which enables our inference method to maintain reasonable run-time as compared with ensembles. Importantly, our approach achieves competitive performance to the state-of-the-art in prediction and uncertainty estimation tasks with fully connected neural networks and ResNets, while reducing the parameter size to $\leq 24.3\%$ of that of a $single$ neural network.

</p>
</details>

<details><summary><b>Training Domain-invariant Object Detector Faster with Feature Replay and Slow Learner</b>
<a href="https://arxiv.org/abs/2105.14693">arxiv:2105.14693</a>
&#x1F4C8; 5 <br>
<p>Chaehyeon Lee, Junghoon Seo, Heechul Jung</p></summary>
<p>

**Abstract:** In deep learning-based object detection on remote sensing domain, nuisance factors, which affect observed variables while not affecting predictor variables, often matters because they cause domain changes. Previously, nuisance disentangled feature transformation (NDFT) was proposed to build domain-invariant feature extractor with with knowledge of nuisance factors. However, NDFT requires enormous time in a training phase, so it has been impractical. In this paper, we introduce our proposed method, A-NDFT, which is an improvement to NDFT. A-NDFT utilizes two acceleration techniques, feature replay and slow learner. Consequently, on a large-scale UAVDT benchmark, it is shown that our framework can reduce the training time of NDFT from 31 hours to 3 hours while still maintaining the performance. The code will be made publicly available online.

</p>
</details>

<details><summary><b>Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation</b>
<a href="https://arxiv.org/abs/2105.14462">arxiv:2105.14462</a>
&#x1F4C8; 5 <br>
<p>Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao</p></summary>
<p>

**Abstract:** A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models' interpretability, and discuss how our findings will benefit future research.

</p>
</details>

<details><summary><b>A Bytecode-based Approach for Smart Contract Classification</b>
<a href="https://arxiv.org/abs/2106.15497">arxiv:2106.15497</a>
&#x1F4C8; 4 <br>
<p>Chaochen Shi, Yong Xiang, Robin Ram Mohan Doss, Jiangshan Yu, Keshav Sood, Longxiang Gao</p></summary>
<p>

**Abstract:** With the development of blockchain technologies, the number of smart contracts deployed on blockchain platforms is growing exponentially, which makes it difficult for users to find desired services by manual screening. The automatic classification of smart contracts can provide blockchain users with keyword-based contract searching and helps to manage smart contracts effectively. Current research on smart contract classification focuses on Natural Language Processing (NLP) solutions which are based on contract source code. However, more than 94% of smart contracts are not open-source, so the application scenarios of NLP methods are very limited. Meanwhile, NLP models are vulnerable to adversarial attacks. This paper proposes a classification model based on features from contract bytecode instead of source code to solve these problems. We also use feature selection and ensemble learning to optimize the model. Our experimental studies on over 3,300 real-world Ethereum smart contracts show that our model can classify smart contracts without source code and has better performance than baseline models. Our model also has good resistance to adversarial attacks compared with NLP-based models. In addition, our analysis reveals that account features used in many smart contract classification models have little effect on classification and can be excluded.

</p>
</details>

<details><summary><b>Kolmogorov-Smirnov Test-Based Actively-Adaptive Thompson Sampling for Non-Stationary Bandits</b>
<a href="https://arxiv.org/abs/2105.14586">arxiv:2105.14586</a>
&#x1F4C8; 4 <br>
<p>Gourab Ghatak, Hardhik Mohanty, Aniq Ur Rahman</p></summary>
<p>

**Abstract:** We consider the non-stationary multi-armed bandit (MAB) framework and propose a Kolmogorov-Smirnov (KS) test based Thompson Sampling (TS) algorithm named TS-KS, that actively detects change points and resets the TS parameters once a change is detected. In particular, for the two-armed bandit case, we derive bounds on the number of samples of the reward distribution to detect the change once it occurs. Consequently, we show that the proposed algorithm has sub-linear regret. Contrary to existing works, our algorithm is able to detect a change when the underlying reward distribution changes even though the mean reward remains the same. Finally, to test the efficacy of the proposed algorithm, we employ it in two case-studies: i) task-offloading scenario in wireless edge-computing, and ii) portfolio optimization. Our results show that the proposed TS-KS algorithm outperforms not only the static TS algorithm but also it performs better than other bandit algorithms designed for non-stationary environments. Moreover, the performance of TS-KS is at par with the state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA.

</p>
</details>

<details><summary><b>Polygonal Point Set Tracking</b>
<a href="https://arxiv.org/abs/2105.14584">arxiv:2105.14584</a>
&#x1F4C8; 4 <br>
<p>Gunhee Nam, Miran Heo, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim</p></summary>
<p>

**Abstract:** In this paper, we propose a novel learning-based polygonal point set tracking method. Compared to existing video object segmentation~(VOS) methods that propagate pixel-wise object mask information, we propagate a polygonal point set over frames.
  Specifically, the set is defined as a subset of points in the target contour, and our goal is to track corresponding points on the target contour. Those outputs enable us to apply various visual effects such as motion tracking, part deformation, and texture mapping. To this end, we propose a new method to track the corresponding points between frames by the global-local alignment with delicately designed losses and regularization terms. We also introduce a novel learning strategy using synthetic and VOS datasets that makes it possible to tackle the problem without developing the point correspondence dataset. Since the existing datasets are not suitable to validate our method, we build a new polygonal point set tracking dataset and demonstrate the superior performance of our method over the baselines and existing contour-based VOS methods. In addition, we present visual-effects applications of our method on part distortion and text mapping.

</p>
</details>

<details><summary><b>Scalable and Interpretable Marked Point Processes</b>
<a href="https://arxiv.org/abs/2105.14574">arxiv:2105.14574</a>
&#x1F4C8; 4 <br>
<p>Aristeidis Panos, Ioannis Kosmidis, Petros Dellaportas</p></summary>
<p>

**Abstract:** We introduce a novel inferential framework for marked point processes that enjoys both scalability and interpretability. The framework is based on variational inference and it aims to speed up inference for a flexible family of marked point processes where the joint distribution of times and marks can be specified in terms of the conditional distribution of times given the process filtration, and of the conditional distribution of marks given the process filtration and the current time. We assess the predictive ability of our proposed method over four real-world datasets where results show its competitive performance against other baselines. The attractiveness of our framework for the modelling of marked point processes is illustrated through a case study of association football data where scalability and interpretability are exploited for extracting useful informative patterns.

</p>
</details>

<details><summary><b>Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization</b>
<a href="https://arxiv.org/abs/2107.00462">arxiv:2107.00462</a>
&#x1F4C8; 3 <br>
<p>Skylar W. Wurster, Han-Wei Shen, Hanqi Guo, Thomas Peterka, Mukund Raj, Jiayi Xu</p></summary>
<p>

**Abstract:** We present an approach for hierarchical super resolution (SR) using neural networks on an octree data representation. We train a hierarchy of neural networks, each capable of 2x upscaling in each spatial dimension between two levels of detail, and use these networks in tandem to facilitate large scale factor super resolution, scaling with the number of trained networks. We utilize these networks in a hierarchical super resolution algorithm that upscales multiresolution data to a uniform high resolution without introducing seam artifacts on octree node boundaries. We evaluate application of this algorithm in a data reduction framework by dynamically downscaling input data to an octree-based data structure to represent the multiresolution data before compressing for additional storage reduction. We demonstrate that our approach avoids seam artifacts common to multiresolution data formats, and show how neural network super resolution assisted data reduction can preserve global features better than compressors alone at the same compression ratios.

</p>
</details>

<details><summary><b>Neural Models for Offensive Language Detection</b>
<a href="https://arxiv.org/abs/2106.14609">arxiv:2106.14609</a>
&#x1F4C8; 3 <br>
<p>Ehab Hamdy</p></summary>
<p>

**Abstract:** Offensive language detection is an ever-growing natural language processing (NLP) application. This growth is mainly because of the widespread usage of social networks, which becomes a mainstream channel for people to communicate, work, and enjoy entertainment content. Many incidents of sharing aggressive and offensive content negatively impacted society to a great extend. We believe contributing to improving and comparing different machine learning models to fight such harmful contents is an important and challenging goal for this thesis. We targeted the problem of offensive language detection for building efficient automated models for offensive language detection. With the recent advancements of NLP models, specifically, the Transformer model, which tackled many shortcomings of the standard seq-to-seq techniques. The BERT model has shown state-of-the-art results on many NLP tasks. Although the literature still exploring the reasons for the BERT achievements in the NLP field. Other efficient variants have been developed to improve upon the standard BERT, such as RoBERTa and ALBERT. Moreover, due to the multilingual nature of text on social media that could affect the model decision on a given tween, it is becoming essential to examine multilingual models such as XLM-RoBERTa trained on 100 languages and how did it compare to unilingual models. The RoBERTa based model proved to be the most capable model and achieved the highest F1 score for the tasks. Another critical aspect of a well-rounded offensive language detection system is the speed at which a model can be trained and make inferences. In that respect, we have considered the model run-time and fine-tuned the very efficient implementation of FastText called BlazingText that achieved good results, which is much faster than BERT-based models.

</p>
</details>

<details><summary><b>Fully Hyperbolic Neural Networks</b>
<a href="https://arxiv.org/abs/2105.14686">arxiv:2105.14686</a>
&#x1F4C8; 3 <br>
<p>Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou</p></summary>
<p>

**Abstract:** Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in a hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic space. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research.

</p>
</details>

<details><summary><b>Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage Time-distributed Capsule Network</b>
<a href="https://arxiv.org/abs/2105.14656">arxiv:2105.14656</a>
&#x1F4C8; 3 <br>
<p>Parnian Afshar, Moezedin Javad Rafiee, Farnoosh Naderkhani, Shahin Heidarian, Nastaran Enshaei, Anastasia Oikonomou, Faranak Babaki Fard, Reut Anconina, Keyvan Farahani, Konstantinos N. Plataniotis, Arash Mohammadi</p></summary>
<p>

**Abstract:** Reverse transcription-polymerase chain reaction (RT-PCR) is currently the gold standard in COVID-19 diagnosis. It can, however, take days to provide the diagnosis, and false negative rate is relatively high. Imaging, in particular chest computed tomography (CT), can assist with diagnosis and assessment of this disease. Nevertheless, it is shown that standard dose CT scan gives significant radiation burden to patients, especially those in need of multiple scans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT) scan protocols that reduce the radiation exposure close to that of a single X-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since thoracic radiology expertise may not be widely available during the pandemic, we develop an Artificial Intelligence (AI)-based framework using a collected dataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can provide human-level performance. The AI model uses a two stage capsule network architecture and can rapidly classify COVID-19, community acquired pneumonia (CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19 sensitivity of 89.5% +\- 0.11, CAP sensitivity of 95% +\- 0.11, normal cases sensitivity (specificity) of 85.7% +\- 0.16, and accuracy of 90% +\- 0.06. By incorporating clinical data (demographic and symptoms), the performance further improves to COVID-19 sensitivity of 94.3% +\- pm 0.05, CAP sensitivity of 96.7% +\- 0.07, normal cases sensitivity (specificity) of 91% +\- 0.09 , and accuracy of 94.1% +\- 0.03. The proposed AI model achieves human-level diagnosis based on the LDCT/ULDCT scans with reduced radiation exposure. We believe that the proposed AI model has the potential to assist the radiologists to accurately and promptly diagnose COVID-19 infection and help control the transmission chain during the pandemic.

</p>
</details>

<details><summary><b>Shaped Policy Search for Evolutionary Strategies using Waypoints</b>
<a href="https://arxiv.org/abs/2105.14639">arxiv:2105.14639</a>
&#x1F4C8; 3 <br>
<p>Kiran Lekkala, Laurent Itti</p></summary>
<p>

**Abstract:** In this paper, we try to improve exploration in Blackbox methods, particularly Evolution strategies (ES), when applied to Reinforcement Learning (RL) problems where intermediate waypoints/subgoals are available. Since Evolutionary strategies are highly parallelizable, instead of extracting just a scalar cumulative reward, we use the state-action pairs from the trajectories obtained during rollouts/evaluations, to learn the dynamics of the agent. The learnt dynamics are then used in the optimization procedure to speed-up training. Lastly, we show how our proposed approach is universally applicable by presenting results from experiments conducted on Carla driving and UR5 robotic arm simulators.

</p>
</details>

<details><summary><b>On the geometry of generalization and memorization in deep neural networks</b>
<a href="https://arxiv.org/abs/2105.14602">arxiv:2105.14602</a>
&#x1F4C8; 3 <br>
<p>Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin Tang, SueYeon Chung</p></summary>
<p>

**Abstract:** Understanding how large neural networks avoid memorizing training data is key to explaining their high generalization performance. To examine the structure of when and where memorization occurs in a deep network, we use a recently developed replica-based mean field theoretic geometric analysis method. We find that all layers preferentially learn from examples which share features, and link this behavior to generalization performance. Memorization predominately occurs in the deeper layers, due to decreasing object manifolds' radius and dimension, whereas early layers are minimally affected. This predicts that generalization can be restored by reverting the final few layer weights to earlier epochs before significant memorization occurred, which is confirmed by the experiments. Additionally, by studying generalization under different model sizes, we reveal the connection between the double descent phenomenon and the underlying model geometry. Finally, analytical analysis shows that networks avoid memorization early in training because close to initialization, the gradient contribution from permuted examples are small. These findings provide quantitative evidence for the structure of memorization across layers of a deep neural network, the drivers for such structure, and its connection to manifold geometric properties.

</p>
</details>

<details><summary><b>Robust Dynamic Network Embedding via Ensembles</b>
<a href="https://arxiv.org/abs/2105.14557">arxiv:2105.14557</a>
&#x1F4C8; 3 <br>
<p>Chengbin Hou, Guoji Fu, Peng Yang, Zheng Hu, Shan He, Ke Tang</p></summary>
<p>

**Abstract:** Dynamic Network Embedding (DNE) has recently attracted considerable attention due to the advantage of network embedding in various fields and the dynamic nature of many real-world networks. An input dynamic network to DNE is often assumed to have smooth changes over snapshots, which however would not hold for all real-world scenarios. It is natural to ask if existing DNE methods can perform well for an input dynamic network without smooth changes. To quantify it, an index called Degree of Changes (DoCs) is suggested so that the smaller DoCs indicates the smoother changes. Our comparative study shows several DNE methods are not robust enough to different DoCs even if the corresponding input dynamic networks come from the same dataset, which would make these methods unreliable and hard to use for unknown real-world applications. To propose an effective and more robust DNE method, we follow the notion of ensembles where each base learner adopts an incremental Skip-Gram embedding model. To further boost the performance, a simple yet effective strategy is designed to enhance the diversity among base learners at each timestep by capturing different levels of local-global topology. Extensive experiments demonstrate the superior effectiveness and robustness of the proposed method compared to state-of-the-art DNE methods, as well as the benefits of special designs in the proposed method and its scalability.

</p>
</details>

<details><summary><b>Longer Version for "Deep Context-Encoding Network for Retinal Image Captioning"</b>
<a href="https://arxiv.org/abs/2105.14538">arxiv:2105.14538</a>
&#x1F4C8; 3 <br>
<p>Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, Marcel Worring</p></summary>
<p>

**Abstract:** Automatically generating medical reports for retinal images is one of the promising ways to help ophthalmologists reduce their workload and improve work efficiency. In this work, we propose a new context-driven encoding network to automatically generate medical reports for retinal images. The proposed model is mainly composed of a multi-modal input encoder and a fused-feature decoder. Our experimental results show that our proposed method is capable of effectively leveraging the interactive information between the input image and context, i.e., keywords in our case. The proposed method creates more accurate and meaningful reports for retinal images than baseline models and achieves state-of-the-art performance. This performance is shown in several commonly used metrics for the medical report generation task: BLEU-avg (+16%), CIDEr (+10.2%), and ROUGE (+8.6%).

</p>
</details>

<details><summary><b>Generating Ten BCI Commands Using Four Simple Motor Imageries</b>
<a href="https://arxiv.org/abs/2105.14493">arxiv:2105.14493</a>
&#x1F4C8; 3 <br>
<p>Nuri Korkan, Tamer Olmez, Zumray Dokur</p></summary>
<p>

**Abstract:** The brain computer interface (BCI) systems are utilized for transferring information among humans and computers by analyzing electroencephalogram (EEG) recordings.The process of mentally previewing a motor movement without generating the corporal output can be described as motor imagery (MI).In this emerging research field, the number of commands is also limited in relation to the number of MI tasks; in the current literature, mostly two or four commands (classes) are studied. As a solution to this problem, it is recommended to use mental tasks as well as MI tasks. Unfortunately, the use of this approach reduces the classification performance of MI EEG signals. The fMRI analyses show that the resources in the brain associated with the motor imagery can be activated independently. It is assumed that the brain activity induced by the MI of the combination of body parts corresponds to the superposition of the activities generated during each body parts's simple MI. In this study, in order to create more than four BCI commands, we suggest to generate combined MI EEG signals artificially by using left hand, right hand, tongue, and feet motor imageries in pairs. A maximum of ten different BCI commands can be generated by using four motor imageries in pairs.This study aims to achieve high classification performances for BCI commands produced from four motor imageries by implementing a small-sized deep neural network (DNN).The presented method is evaluated on the four-class datasets of BCI Competitions III and IV, and an average classification performance of 81.8% is achieved for ten classes. The above assumption is also validated on a different dataset which consists of simple and combined MI EEG signals acquired in real time. Trained with the artificially generated combined MI EEG signals, DivFE resulted in an average of 76.5% success rate for the combined MI EEG signals acquired in real-time.

</p>
</details>

<details><summary><b>High Performance Hyperspectral Image Classification using Graphics Processing Units</b>
<a href="https://arxiv.org/abs/2106.12942">arxiv:2106.12942</a>
&#x1F4C8; 2 <br>
<p>Mahmoud Hossam</p></summary>
<p>

**Abstract:** Real-time remote sensing applications like search and rescue missions, military target detection, environmental monitoring, hazard prevention and other time-critical applications require onboard real time processing capabilities or autonomous decision making. Some unmanned remote systems like satellites are physically remote from their operators, and all control of the spacecraft and data returned by the spacecraft must be transmitted over a wireless radio link. This link may not be available for extended periods when the satellite is out of line of sight of its ground station. Therefore, lightweight, small size and low power consumption hardware is essential for onboard real time processing systems. With increasing dimensionality, size and resolution of recent hyperspectral imaging sensors, additional challenges are posed upon remote sensing processing systems and more capable computing architectures are needed. Graphical Processing Units (GPUs) emerged as promising architecture for light weight high performance computing that can address these computational requirements for onboard systems. The goal of this study is to build high performance methods for onboard hyperspectral analysis. We propose accelerated methods for the well-known recursive hierarchical segmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a GPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the National Aeronautics and Space Administration (NASA), which is designed to provide rich classification information with several output levels. The achieved speedups by parallel solutions compared to CPU sequential implementations are 21x for parallel single GPU and 240x for hybrid multi-node computer clusters with 16 computing nodes. The energy consumption is reduced to 74% using a single GPU compared to the equivalent parallel CPU cluster.

</p>
</details>

<details><summary><b>Deep-Learning Discovers Macroscopic Governing Equations for Viscous Gravity Currents from Microscopic Simulation Data</b>
<a href="https://arxiv.org/abs/2106.00009">arxiv:2106.00009</a>
&#x1F4C8; 2 <br>
<p>Junsheng Zeng, Hao Xu, Yuntian Chen, Dongxiao Zhang</p></summary>
<p>

**Abstract:** Although deep-learning has been successfully applied in a variety of science and engineering problems owing to its strong high-dimensional nonlinear mapping capability, it is of limited use in scientific knowledge discovery. In this work, we propose a deep-learning based framework to discover the macroscopic governing equation of viscous gravity current based on high-resolution microscopic simulation data without the need for prior knowledge of underlying terms. For two typical scenarios with different viscosity ratios, the deep-learning based equations exactly capture the same dominated terms as the theoretically derived equations for describing long-term asymptotic behaviors, which validates the proposed framework. Unknown macroscopic equations are then obtained for describing short-term behaviors, and additional deep-learned compensation terms are eventually discovered. Comparison of posterior tests shows that the deep-learning based PDEs actually perform better than the theoretically derived PDEs in predicting evolving viscous gravity currents for both long-term and short-term regimes. Moreover, the proposed framework is proven to be very robust against non-biased data noise for training, which is up to 20%. Consequently, the presented deep-learning framework shows considerable potential for discovering unrevealed intrinsic laws in scientific semantic space from raw experimental or simulation results in data space.

</p>
</details>

<details><summary><b>Robust discovery of partial differential equations in complex situations</b>
<a href="https://arxiv.org/abs/2106.00008">arxiv:2106.00008</a>
&#x1F4C8; 2 <br>
<p>Hao Xu, Dongxiao Zhang</p></summary>
<p>

**Abstract:** Data-driven discovery of partial differential equations (PDEs) has achieved considerable development in recent years. Several aspects of problems have been resolved by sparse regression-based and neural network-based methods. However, the performances of existing methods lack stability when dealing with complex situations, including sparse data with high noise, high-order derivatives and shock waves, which bring obstacles to calculating derivatives accurately. Therefore, a robust PDE discovery framework, called the robust deep learning-genetic algorithm (R-DLGA), that incorporates the physics-informed neural network (PINN), is proposed in this work. In the framework, a preliminary result of potential terms provided by the deep learning-genetic algorithm is added into the loss function of the PINN as physical constraints to improve the accuracy of derivative calculation. It assists to optimize the preliminary result and obtain the ultimately discovered PDE by eliminating the error compensation terms. The stability and accuracy of the proposed R-DLGA in several complex situations are examined for proof-and-concept, and the results prove that the proposed framework is able to calculate derivatives accurately with the optimization of PINN and possesses surprising robustness to complex situations, including sparse data with high noise, high-order derivatives, and shock waves.

</p>
</details>

<details><summary><b>A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression</b>
<a href="https://arxiv.org/abs/2105.14673">arxiv:2105.14673</a>
&#x1F4C8; 2 <br>
<p>Batoul Taki, Mohsen Ghassemi, Anand D. Sarwate, Waheed U. Bajwa</p></summary>
<p>

**Abstract:** This paper considers the problem of matrix-variate logistic regression. The fundamental error threshold on estimating coefficient matrices in the logistic regression problem is found by deriving a lower bound on the minimax risk. The focus of this paper is on derivation of a minimax risk lower bound for low-rank coefficient matrices. The bound depends explicitly on the dimensions and distribution of the covariates, the rank and energy of the coefficient matrix, and the number of samples. The resulting bound is proportional to the intrinsic degrees of freedom in the problem, which suggests the sample complexity of the low-rank matrix logistic regression problem can be lower than that for vectorized logistic regression. \color{red}\color{black} The proof techniques utilized in this work also set the stage for development of minimax lower bounds for tensor-variate logistic regression problems.

</p>
</details>

<details><summary><b>Sharper bounds for online learning of smooth functions of a single variable</b>
<a href="https://arxiv.org/abs/2105.14648">arxiv:2105.14648</a>
&#x1F4C8; 2 <br>
<p>Jesse Geneson</p></summary>
<p>

**Abstract:** We investigate the generalization of the mistake-bound model to continuous real-valued single variable functions. Let $\mathcal{F}_q$ be the class of absolutely continuous functions $f: [0, 1] \rightarrow \mathbb{R}$ with $||f'||_q \le 1$, and define $opt_p(\mathcal{F}_q)$ as the best possible bound on the worst-case sum of the $p^{th}$ powers of the absolute prediction errors over any number of trials. Kimber and Long (Theoretical Computer Science, 1995) proved for $q \ge 2$ that $opt_p(\mathcal{F}_q) = 1$ when $p \ge 2$ and $opt_p(\mathcal{F}_q) = \infty$ when $p = 1$. For $1 < p < 2$ with $p = 1+ε$, the only known bound was $opt_p(\mathcal{F}_{q}) = O(ε^{-1})$ from the same paper. We show for all $ε\in (0, 1)$ and $q \ge 2$ that $opt_{1+ε}(\mathcal{F}_q) = Θ(ε^{-\frac{1}{2}})$, where the constants in the bound do not depend on $q$. We also show that $opt_{1+ε}(\mathcal{F}_{\infty}) = Θ(ε^{-\frac{1}{2}})$.

</p>
</details>

<details><summary><b>Non-local Patch-based Low-rank Tensor Ring Completion for Visual Data</b>
<a href="https://arxiv.org/abs/2105.14620">arxiv:2105.14620</a>
&#x1F4C8; 2 <br>
<p>Yicong He, George K. Atia</p></summary>
<p>

**Abstract:** Tensor completion is the problem of estimating the missing entries of a partially observed tensor with a certain low-rank structure. It improves on matrix completion for image and video data by capturing additional structural information intrinsic to such data. % With more inherent information involving in tensor structure than matrix, tensor completion has shown better performance compared with matrix completion especially in image and video data. Traditional completion algorithms treat the entire visual data as a tensor, which may not always work well especially when camera or object motion exists. In this paper, we develop a novel non-local patch-based tensor ring completion algorithm. In the proposed approach, similar patches are extracted for each reference patch along both the spatial and temporal domains of the visual data. The collected patches are then formed into a high-order tensor and a tensor ring completion algorithm is proposed to recover the completed tensor. A novel interval sampling-based block matching (ISBM) strategy and a hybrid completion strategy are also proposed to improve efficiency and accuracy. Further, we develop an online patch-based completion algorithm to deal with streaming video data. An efficient online tensor ring completion algorithm is proposed to reduce the time cost. Extensive experimental results demonstrate the superior performance of the proposed algorithms compared with state-of-the-art methods.

</p>
</details>

<details><summary><b>How effective are Graph Neural Networks in Fraud Detection for Network Data?</b>
<a href="https://arxiv.org/abs/2105.14568">arxiv:2105.14568</a>
&#x1F4C8; 2 <br>
<p>Ronald D. R. Pereira, Fabrício Murai</p></summary>
<p>

**Abstract:** Graph-based Neural Networks (GNNs) are recent models created for learning representations of nodes (and graphs), which have achieved promising results when detecting patterns that occur in large-scale data relating different entities. Among these patterns, financial fraud stands out for its socioeconomic relevance and for presenting particular challenges, such as the extreme imbalance between the positive (fraud) and negative (legitimate transactions) classes, and the concept drift (i.e., statistical properties of the data change over time). Since GNNs are based on message propagation, the representation of a node is strongly impacted by its neighbors and by the network's hubs, amplifying the imbalance effects. Recent works attempt to adapt undersampling and oversampling strategies for GNNs in order to mitigate this effect without, however, accounting for concept drift. In this work, we conduct experiments to evaluate existing techniques for detecting network fraud, considering the two previous challenges. For this, we use real data sets, complemented by synthetic data created from a new methodology introduced here. Based on this analysis, we propose a series of improvement points that should be investigated in future research.

</p>
</details>

<details><summary><b>BABA: Beta Approximation for Bayesian Active Learning</b>
<a href="https://arxiv.org/abs/2105.14559">arxiv:2105.14559</a>
&#x1F4C8; 2 <br>
<p>Jae Oh Woo</p></summary>
<p>

**Abstract:** This paper introduces a new acquisition function under the Bayesian active learning framework, namely BABA. It is motivated by previously well-established works BALD, and BatchBALD which capture the mutual information between the model parameters and the predictive outputs of the data. Our proposed measure, BABA, endeavors to quantify the normalized mutual information by approximating the stochasticity of predictive probabilities using Beta distributions. BABA outperforms the well-known family of acquisition functions, including BALD and BatchBALD. We demonstrate this by showing extensive experimental results obtained from MNIST and EMNIST datasets.

</p>
</details>

<details><summary><b>Z2P: Instant Rendering of Point Clouds</b>
<a href="https://arxiv.org/abs/2105.14548">arxiv:2105.14548</a>
&#x1F4C8; 2 <br>
<p>Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or</p></summary>
<p>

**Abstract:** We present a technique for rendering point clouds using a neural network. Existing point rendering techniques either use splatting, or first reconstruct a surface mesh that can then be rendered. Both of these techniques require solving for global point normal orientation, which is a challenging problem on its own. Furthermore, splatting techniques result in holes and overlaps, whereas mesh reconstruction is particularly challenging, especially in the cases of thin surfaces and sheets.
  We cast the rendering problem as a conditional image-to-image translation problem. In our formulation, Z2P, i.e., depth-augmented point features as viewed from target camera view, are directly translated by a neural network to rendered images, conditioned on control variables (e.g., color, light). We avoid inevitable issues with splatting (i.e., holes and overlaps), and bypass solving the notoriously challenging surface reconstruction problem or estimating oriented normals. Yet, our approach results in a rendered image as if a surface mesh was reconstructed. We demonstrate that our framework produces a plausible image, and can effectively handle noise, non-uniform sampling, thin surfaces / sheets, and is fast.

</p>
</details>

<details><summary><b>On the benefits of representation regularization in invariance based domain generalization</b>
<a href="https://arxiv.org/abs/2105.14529">arxiv:2105.14529</a>
&#x1F4C8; 2 <br>
<p>Changjian Shui, Boyu Wang, Christian Gagné</p></summary>
<p>

**Abstract:** A crucial aspect in reliable machine learning is to design a deployable system in generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning invariant representation is vulnerable to the unseen environment. To this end, we derive novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. Our regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms for invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various dataset and invariance criteria.

</p>
</details>

<details><summary><b>Parameter Estimation for the SEIR Model Using Recurrent Nets</b>
<a href="https://arxiv.org/abs/2105.14524">arxiv:2105.14524</a>
&#x1F4C8; 2 <br>
<p>Chun Fan, Yuxian Meng, Xiaofei Sun, Fei Wu, Tianwei Zhang, Jiwei Li</p></summary>
<p>

**Abstract:** The standard way to estimate the parameters $Θ_\text{SEIR}$ (e.g., the transmission rate $β$) of an SEIR model is to use grid search, where simulations are performed on each set of parameters, and the parameter set leading to the least $L_2$ distance between predicted number of infections and observed infections is selected. This brute-force strategy is not only time consuming, as simulations are slow when the population is large, but also inaccurate, since it is impossible to enumerate all parameter combinations. To address these issues, in this paper, we propose to transform the non-differentiable problem of finding optimal $Θ_\text{SEIR}$ to a differentiable one, where we first train a recurrent net to fit a small number of simulation data. Next, based on this recurrent net that is able to generalize SEIR simulations, we are able to transform the objective to a differentiable one with respect to $Θ_\text{SEIR}$, and straightforwardly obtain its optimal value. The proposed strategy is both time efficient as it only relies on a small number of SEIR simulations, and accurate as we are able to find the optimal $Θ_\text{SEIR}$ based on the differentiable objective. On two COVID-19 datasets, we observe that the proposed strategy leads to significantly better parameter estimations with a smaller number of simulations.

</p>
</details>

<details><summary><b>Human Interpretable AI: Enhancing Tsetlin Machine Stochasticity with Drop Clause</b>
<a href="https://arxiv.org/abs/2105.14506">arxiv:2105.14506</a>
&#x1F4C8; 2 <br>
<p>Jivitesh Sharma, Rohan Yadav, Ole-Christoffer Granmo, Lei Jiao</p></summary>
<p>

**Abstract:** In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to 4x faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are \emph{human}-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.

</p>
</details>

<details><summary><b>2.5-dimensional distributed model training</b>
<a href="https://arxiv.org/abs/2105.14500">arxiv:2105.14500</a>
&#x1F4C8; 2 <br>
<p>Boxiang Wang, Qifan Xu, Zhengda Bian, Yang You</p></summary>
<p>

**Abstract:** Data parallelism does a good job in speeding up the training. However, when it comes to the case when the memory of a single device can not host a whole model, data parallelism would not have the chance to do anything. Another option is to split the model by operator, or horizontally. Megatron-LM introduced a 1-Dimensional distributed method to use GPUs to speed up the training process. Optimus is a 2D solution for distributed tensor parallelism. However, these methods have a high communication overhead and a low scaling efficiency on large-scale computing clusters. To solve this problem, we investigate the 2.5-Dimensional distributed tensor parallelism.Introduced by Solomonik et al., 2.5-Dimensional Matrix Multiplication developed an effective method to perform multiple Cannon's algorithm at the same time to increase the efficiency. With many restrictions of Cannon's Algorithm and a huge amount of shift operation, we need to invent a new method of 2.5-dimensional matrix multiplication to enhance the performance. Absorbing the essence from both SUMMA and 2.5-Dimensional Matrix Multiplication, we introduced SUMMA2.5-LM for language models to overcome the abundance of unnecessary transmission loss result from the increasing size of language model parallelism. Compared to previous 1D and 2D model parallelization of language models, our SUMMA2.5-LM managed to reduce the transmission cost on each layer, which could get a 1.45X efficiency according to our weak scaling result between 2.5-D [4,4,4] arrangement and 2-D [8,8,1] arrangement.

</p>
</details>

<details><summary><b>Institutionalising Ethics in AI through Broader Impact Requirements</b>
<a href="https://arxiv.org/abs/2106.11039">arxiv:2106.11039</a>
&#x1F4C8; 1 <br>
<p>Carina Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, Allan Dafoe</p></summary>
<p>

**Abstract:** Turning principles into practice is one of the most pressing challenges of artificial intelligence (AI) governance. In this article, we reflect on a novel governance initiative by one of the world's largest AI conferences. In 2020, the Conference on Neural Information Processing Systems (NeurIPS) introduced a requirement for submitting authors to include a statement on the broader societal impacts of their research. Drawing insights from similar governance initiatives, including institutional review boards (IRBs) and impact requirements for funding applications, we investigate the risks, challenges and potential benefits of such an initiative. Among the challenges, we list a lack of recognised best practice and procedural transparency, researcher opportunity costs, institutional and social pressures, cognitive biases, and the inherently difficult nature of the task. The potential benefits, on the other hand, include improved anticipation and identification of impacts, better communication with policy and governance experts, and a general strengthening of the norms around responsible research. To maximise the chance of success, we recommend measures to increase transparency, improve guidance, create incentives to engage earnestly with the process, and facilitate public deliberation on the requirement's merits and future. Perhaps the most important contribution from this analysis are the insights we can gain regarding effective community-based governance and the role and responsibility of the AI research community more broadly.

</p>
</details>

<details><summary><b>Multi-Objectivizing Software Configuration Tuning (for a single performance concern)</b>
<a href="https://arxiv.org/abs/2106.01331">arxiv:2106.01331</a>
&#x1F4C8; 1 <br>
<p>Tao Chen, Miqing Li</p></summary>
<p>

**Abstract:** Automatically tuning software configuration for optimizing a single performance attribute (e.g., minimizing latency) is not trivial, due to the nature of the configuration systems (e.g., complex landscape and expensive measurement). To deal with the problem, existing work has been focusing on developing various effective optimizers. However, a prominent issue that all these optimizers need to take care of is how to avoid the search being trapped in local optima -- a hard nut to crack for software configuration tuning due to its rugged and sparse landscape, and neighboring configurations tending to behave very differently. Overcoming such in an expensive measurement setting is even more challenging. In this paper, we take a different perspective to tackle this issue. Instead of focusing on improving the optimizer, we work on the level of optimization model. We do this by proposing a meta multi-objectivization model (MMO) that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model unique is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima.
  Experiments on eight real-world software systems/environments with diverse performance attributes reveal that our MMO model is statistically more effective than state-of-the-art single-objective counterparts in overcoming local optima (up to 42% gain), while using as low as 24% of their measurements to achieve the same (or better) performance result.

</p>
</details>

<details><summary><b>A Compact and Interpretable Convolutional Neural Network for Cross-Subject Driver Drowsiness Detection from Single-Channel EEG</b>
<a href="https://arxiv.org/abs/2106.00613">arxiv:2106.00613</a>
&#x1F4C8; 1 <br>
<p>Jian Cui, Zirui Lan, Yisi Liu, Ruilin Li, Fan Li, Olga Sourina, Wolfgang Mueller-Wittig</p></summary>
<p>

**Abstract:** Driver drowsiness is one of main factors leading to road fatalities and hazards in the transportation industry. Electroencephalography (EEG) has been considered as one of the best physiological signals to detect drivers drowsy states, since it directly measures neurophysiological activities in the brain. However, designing a calibration-free system for driver drowsiness detection with EEG is still a challenging task, as EEG suffers from serious mental and physical drifts across different subjects. In this paper, we propose a compact and interpretable Convolutional Neural Network (CNN) to discover shared EEG features across different subjects for driver drowsiness detection. We incorporate the Global Average Pooling (GAP) layer in the model structure, allowing the Class Activation Map (CAM) method to be used for localizing regions of the input signal that contribute most for classification. Results show that the proposed model can achieve an average accuracy of 73.22% on 11 subjects for 2-class cross-subject EEG signal classification, which is higher than conventional machine learning methods and other state-of-art deep learning methods. It is revealed by the visualization technique that the model has learned biologically explainable features, e.g., Alpha spindles and Theta burst, as evidence for the drowsy state. It is also interesting to see that the model uses artifacts that usually dominate the wakeful EEG, e.g., muscle artifacts and sensor drifts, to recognize the alert state. The proposed model illustrates a potential direction to use CNN models as a powerful tool to discover shared features related to different mental states across different subjects from EEG signals.

</p>
</details>

<details><summary><b>Dynamic-Deep: ECG Task-Aware Compression</b>
<a href="https://arxiv.org/abs/2106.00606">arxiv:2106.00606</a>
&#x1F4C8; 1 <br>
<p>Eli Brosh, Elad Wasserstein, Anat Bremler-Barr</p></summary>
<p>

**Abstract:** Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common application of Internet of Things (IoT) devices. Compression methods are often applied on the massive amounts of sensor data generated before sending it to the Cloud to reduce storage and delivery costs. A lossy compression provides high compression gain (CG) but may reduce the performance of an ECG application (downstream task) due to information loss. Previous works on ECG monitoring focus either on optimizing the signal reconstruction or the task's performance. Instead, we advocate a lossy compression solution that allows configuring a desired performance level on the downstream tasks while maintaining an optimized CG.
  We propose Dynamic-Deep, a task-aware compression that uses convolutional autoencoders. The compression level is dynamically selected to yield an optimized compression without violating tasks' performance requirements. We conduct an extensive evaluation of our approach on common ECG datasets using two popular ECG applications, which includes heart rate (HR) arrhythmia classification. We demonstrate that Dynamic-Deep improves HR classification F1-score by a factor of 3 and increases CG by up to 83% compared to the previous state-of-the-art (autoencoder-based) compressor. Additionally, Dynamic-Deep has a 67% lower memory footprint. Analyzing Dynamic-Deep on the Google Cloud Platform, we observe a 97% reduction in cloud costs compared to a no compression solution.
  To the best of our knowledge, Dynamic-Deep is the first proposal to focus on balancing the need for high performance of cloud-based downstream tasks and the desire to achieve optimized compression in IoT ECG monitoring settings.

</p>
</details>

<details><summary><b>Quantum Federated Learning with Quantum Data</b>
<a href="https://arxiv.org/abs/2106.00005">arxiv:2106.00005</a>
&#x1F4C8; 1 <br>
<p>Mahdi Chehimi, Walid Saad</p></summary>
<p>

**Abstract:** Quantum machine learning (QML) has emerged as a promising field that leans on the developments in quantum computing to explore large complex machine learning problems. Recently, some purely quantum machine learning models were proposed such as the quantum convolutional neural networks (QCNN) to perform classification on quantum data. However, all of the existing QML models rely on centralized solutions that cannot scale well for large-scale and distributed quantum networks. Hence, it is apropos to consider more practical quantum federated learning (QFL) solutions tailored towards emerging quantum network architectures. Indeed, developing QFL frameworks for quantum networks is critical given the fragile nature of computing qubits and the difficulty of transferring them. On top of its practical momentousness, QFL allows for distributed quantum learning by leveraging existing wireless communication infrastructure. This paper proposes the first fully quantum federated learning framework that can operate over quantum data and, thus, share the learning of quantum circuit parameters in a decentralized manner. First, given the lack of existing quantum federated datasets in the literature, the proposed framework begins by generating the first quantum federated dataset, with a hierarchical data format, for distributed quantum networks. Then, clients sharing QCNN models are fed with the quantum data to perform a classification task. Subsequently, the server aggregates the learnable quantum circuit parameters from clients and performs federated averaging. Extensive experiments are conducted to evaluate and validate the effectiveness of the proposed QFL solution. This work is the first to combine Google's TensorFlow Federated and TensorFlow Quantum in a practical implementation.

</p>
</details>

<details><summary><b>Zero-shot Fact Verification by Claim Generation</b>
<a href="https://arxiv.org/abs/2105.14682">arxiv:2105.14682</a>
&#x1F4C8; 1 <br>
<p>Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang Wang</p></summary>
<p>

**Abstract:** Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.

</p>
</details>

<details><summary><b>Empirical Models for Multidimensional Regression of Fission Systems</b>
<a href="https://arxiv.org/abs/2105.14645">arxiv:2105.14645</a>
&#x1F4C8; 1 <br>
<p>Akshay J. Dave, Jiankai Yu, Jarod Wilson, Bren Phillips, Kaichao Sun, Benoit Forget</p></summary>
<p>

**Abstract:** The development of next-generation autonomous control of fission systems, such as nuclear power plants, will require leveraging advancements in machine learning. For fission systems, accurate prediction of nuclear transport is important to quantify the safety margin and optimize performance. The state-of-the-art approach to this problem is costly Monte Carlo (MC) simulations to approximate solutions of the neutron transport equation. Such an approach is feasible for offline calculations e.g., for design or licensing, but is precluded from use as a model-based controller. In this work, we explore the use of Artificial Neural Networks (ANN), Gradient Boosting Regression (GBR), Gaussian Process Regression (GPR) and Support Vector Regression (SVR) to generate empirical models. The empirical model can then be deployed, e.g., in a model predictive controller. Two fission systems are explored: the subcritical MIT Graphite Exponential Pile (MGEP), and the critical MIT Research Reactor (MITR).
  Findings from this work establish guidelines for developing empirical models for multidimensional regression of neutron transport. An assessment of the accuracy and precision finds that the SVR, followed closely by ANN, performs the best. For both MGEP and MITR, the optimized SVR model exhibited a domain-averaged, test, mean absolute percentage error of 0.17 %. A spatial distribution of performance metrics indicates that physical regions of poor performance coincide with locations of largest neutron flux perturbation -- this outcome is mitigated by ANN and SVR. Even at local maxima, ANN and SVR bias is within experimental uncertainty bounds. A comparison of the performance vs. training dataset size found that SVR is more data-efficient than ANN. Both ANN and SVR achieve a greater than 7 order reduction in evaluation time vs. a MC simulation.

</p>
</details>

<details><summary><b>Power and Performance Efficient SDN-Enabled Fog Architecture</b>
<a href="https://arxiv.org/abs/2105.14607">arxiv:2105.14607</a>
&#x1F4C8; 1 <br>
<p>Adnan Akhunzada, Sherali Zeadally, Saif ul Islam</p></summary>
<p>

**Abstract:** Software Defined Networks (SDNs) have dramatically simplified network management. However, enabling pure SDNs to respond in real-time while handling massive amounts of data still remains a challenging task. In contrast, fog computing has strong potential to serve large surges of data in real-time. SDN control plane enables innovation, and greatly simplifies network operations and management thereby providing a promising solution to implement energy and performance aware SDN-enabled fog computing. Besides, power efficiency and performance evaluation in SDN-enabled fog computing is an area that has not yet been fully explored by the research community. We present a novel SDN-enabled fog architecture to improve power efficacy and performance by leveraging cooperative and non-cooperative policy-based computing. Preliminary results from extensive simulation demonstrate an improvement in the power utilization as well as the overall performance (i.e., processing time, response time). Finally, we discuss several open research issues that need further investigation in the future.

</p>
</details>

<details><summary><b>Evaluating Resilience of Encrypted Traffic Classification Against Adversarial Evasion Attacks</b>
<a href="https://arxiv.org/abs/2105.14564">arxiv:2105.14564</a>
&#x1F4C8; 1 <br>
<p>Ramy Maarouf, Danish Sattar, Ashraf Matrawy</p></summary>
<p>

**Abstract:** Machine learning and deep learning algorithms can be used to classify encrypted Internet traffic. Classification of encrypted traffic can become more challenging in the presence of adversarial attacks that target the learning algorithms. In this paper, we focus on investigating the effectiveness of different evasion attacks and see how resilient machine and deep learning algorithms are. Namely, we test C4.5 Decision Tree, K-Nearest Neighbor (KNN), Artificial Neural Network (ANN), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In most of our experimental results, deep learning shows better resilience against the adversarial samples in comparison to machine learning. Whereas, the impact of the attack varies depending on the type of attack.

</p>
</details>

<details><summary><b>A logic for binary classifiers and their explanation</b>
<a href="https://arxiv.org/abs/2105.14452">arxiv:2105.14452</a>
&#x1F4C8; 1 <br>
<p>Xinghan Liu, Emiliano Lorini</p></summary>
<p>

**Abstract:** Recent years have witnessed a renewed interest in Boolean function in explaining binary classifiers in the field of explainable AI (XAI). The standard approach of Boolean function is propositional logic. We study a family of classifier models, axiomatize it and show completeness of our axiomatics. Moreover, we prove that satisfiability checking for our modal language relative to such a class of models is NP-complete. We leverage the language to formalize counterfactual conditional as well as a variety of notions of explanation including abductive, contrastive and counterfactual explanations, and biases. Finally, we present two extensions of our language: a dynamic extension by the notion of assignment enabling classifier change and an epistemic extension in which the classifier's uncertainty about the actual input can be represented.

</p>
</details>

<details><summary><b>Maximizing Parallelism in Distributed Training for Huge Neural Networks</b>
<a href="https://arxiv.org/abs/2105.14450">arxiv:2105.14450</a>
&#x1F4C8; 1 <br>
<p>Zhengda Bian, Qifan Xu, Boxiang Wang, Yang You</p></summary>
<p>

**Abstract:** The recent Natural Language Processing techniques have been refreshing the state-of-the-art performance at an incredible speed. Training huge language models is therefore an imperative demand in both industry and academy. However, huge language models impose challenges to both hardware and software. Graphical processing units (GPUs) are iterated frequently to meet the exploding demand, and a variety of ASICs like TPUs are spawned. However, there is still a tension between the fast growth of the extremely huge models and the fact that Moore's law is approaching the end. To this end, many model parallelism techniques are proposed to distribute the model parameters to multiple devices, so as to alleviate the tension on both memory and computation. Our work is the first to introduce a 3-dimensional model parallelism for expediting huge language models. By reaching a perfect load balance, our approach presents smaller memory and communication cost than existing state-of-the-art 1-D and 2-D model parallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D parallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x speedup, respectively.

</p>
</details>

<details><summary><b>EEG-based Cross-Subject Driver Drowsiness Recognition with an Interpretable Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2107.09507">arxiv:2107.09507</a>
&#x1F4C8; 0 <br>
<p>Jian Cui, Zirui Lan, Olga Sourina, Wolfgang Müller-Wittig</p></summary>
<p>

**Abstract:** In the context of electroencephalogram (EEG)-based driver drowsiness recognition, it is still challenging to design a calibration-free system, since EEG signals vary significantly among different subjects and recording sessions. Many efforts have been made to use deep learning methods for mental state recognition from EEG signals. However, existing work mostly treats deep learning models as black-box classifiers, while what have been learned by the models and to which extent they are affected by the noise in EEG data are still underexplored. In this paper, we develop a novel convolutional neural network combined with an interpretation technique that allows sample-wise analysis of important features for classification. The network has a compact structure and takes advantage of separable convolutions to process the EEG signals in a spatial-temporal sequence. Results show that the model achieves an average accuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness recognition, which is higher than the conventional baseline methods of 53.40%-72.68% and state-of-the-art deep learning methods of 71.75%-75.19%. Interpretation results indicate the model has learned to recognize biologically meaningful features from EEG signals, e.g., Alpha spindles, as strong indicators of drowsiness across different subjects. In addition, we also explore reasons behind some wrongly classified samples with the interpretation technique and discuss potential ways to improve the recognition accuracy. Our work illustrates a promising direction on using interpretable deep learning models to discover meaningful patterns related to different mental states from complex EEG signals.

</p>
</details>

<details><summary><b>DikpolaSat Mission: Improvement of Space Flight Performance and Optimal Control Using Trained Deep Neural Network -- Trajectory Controller for Space Objects Collision Avoidance</b>
<a href="https://arxiv.org/abs/2106.00007">arxiv:2106.00007</a>
&#x1F4C8; 0 <br>
<p>Manuel Ntumba, Saurabh Gore, Jean Baptiste Awanyo</p></summary>
<p>

**Abstract:** This paper introduced the space mission DikpolaSat Mission, how this research fits into the mission, and the importance of having a trained DNN model instead of the usual GN&C functionality. This paper shows how the controller demonstration is carried out by having the spacecraft follow a desired path, specified in the referenced model. Increases can be made by examining the route used to construct a DNN and understanding the effects of various activating functions on system efficiency. The obstacle avoidance algorithm is built into the control features to respond spontaneously using inputs from the neural network for collision avoidance while optimizing the modified trajectory. The action of a neural network to control the adaptive nature of the nonlinear mechanisms in the controller will make the control system capable of handling multiple nonlinear events and also uncertainties that have not been induced in the control algorithm. Multiple algorithms for optimizing flight controls and fuel consumption can be implemented using knowledge of flight dynamics in trajectory and also in the event of obstacle avoidance. This paper also explains how a DNN can learn to control the flight path and make the system more reliable with each launch, thereby improving the chances of predicting collisions of space objects. The data released from this research is used to design more advanced DNN model capable of predicting other orbital events as well.

</p>
</details>

<details><summary><b>Characterization of Generalizability of Spike Timing Dependent Plasticity trained Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2105.14677">arxiv:2105.14677</a>
&#x1F4C8; 0 <br>
<p>Biswadeep Chakraborty, Saibal Mukhopadhyay</p></summary>
<p>

**Abstract:** A Spiking Neural Network (SNN) is trained with Spike Timing Dependent Plasticity (STDP), which is a neuro-inspired unsupervised learning method for various machine learning applications. This paper studies the generalizability properties of the STDP learning processes using the Hausdorff dimension of the trajectories of the learning algorithm. The paper analyzes the effects of STDP learning models and associated hyper-parameters on the generalizability properties of an SNN. The analysis is used to develop a Bayesian optimization approach to optimize the hyper-parameters for an STDP model for improving the generalizability properties of an SNN.

</p>
</details>

<details><summary><b>$\ell_2$-norm Flow Diffusion in Near-Linear Time</b>
<a href="https://arxiv.org/abs/2105.14629">arxiv:2105.14629</a>
&#x1F4C8; 0 <br>
<p>Li Chen, Richard Peng, Di Wang</p></summary>
<p>

**Abstract:** Diffusion is a fundamental graph procedure and has been a basic building block in a wide range of theoretical and empirical applications such as graph partitioning and semi-supervised learning on graphs. In this paper, we study computationally efficient diffusion primitives beyond random walk.
  We design an $\widetilde{O}(m)$-time randomized algorithm for the $\ell_2$-norm flow diffusion problem, a recently proposed diffusion model based on network flow with demonstrated graph clustering related applications both in theory and in practice. Examples include finding locally-biased low conductance cuts. Using a known connection between the optimal dual solution of the flow diffusion problem and the local cut structure, our algorithm gives an alternative approach for finding such cuts in nearly linear time.
  From a technical point of view, our algorithm contributes a novel way of dealing with inequality constraints in graph optimization problems. It adapts the high-level algorithmic framework of nearly linear time Laplacian system solvers, but requires several new tools: vertex elimination under constraints, a new family of graph ultra-sparsifiers, and accelerated proximal gradient methods with inexact proximal mapping computation.

</p>
</details>


[Next Page]({{ '/2021/05/29/2021.05.29.html' | relative_url }})
