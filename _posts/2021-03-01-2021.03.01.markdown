## Summary for 2021-03-01, created on 2021-12-14


<details><summary><b>Generative Adversarial Transformers</b>
<a href="https://arxiv.org/abs/2103.01209">arxiv:2103.01209</a>
&#x1F4C8; 107 <br>
<p>Drew A. Hudson, C. Lawrence Zitnick</p></summary>
<p>

**Abstract:** We introduce the GANformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linear efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at https://github.com/dorarad/gansformer.

</p>
</details>

<details><summary><b>Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2103.01315">arxiv:2103.01315</a>
&#x1F4C8; 49 <br>
<p>Mamshad Nayeem Rizve, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah</p></summary>
<p>

**Abstract:** In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the objective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predominantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the significance of powerful feature representations with a simple embedding network that can outperform existing sophisticated FSL algorithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been employed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Simultaneous optimization for both of these contrasting objectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transformations. These complementary sets of features help generalize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive experimentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on five popular benchmark datasets.

</p>
</details>

<details><summary><b>Challenges and Opportunities in High-dimensional Variational Inference</b>
<a href="https://arxiv.org/abs/2103.01085">arxiv:2103.01085</a>
&#x1F4C8; 49 <br>
<p>Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael Riis Andersen, Jonathan Huggins, Aki Vehtari</p></summary>
<p>

**Abstract:** Current black-box variational inference (BBVI) methods require the user to make numerous design choices -- such as the selection of variational objective and approximating family -- yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. Our framework and supporting experiments help to distinguish between the behavior of BBVI methods for approximating low-dimensional versus moderate-to-high-dimensional posteriors. In the latter case, we show that mass-covering variational objectives are difficult to optimize and do not improve accuracy, but flexible variational families can improve accuracy and the effectiveness of importance sampling -- at the cost of additional optimization challenges. Therefore, for moderate-to-high-dimensional posteriors we recommend using the (mode-seeking) exclusive KL divergence since it is the easiest to optimize, and improving the variational family or using model parameter transformations to make the posterior and optimal variational approximation more similar. On the other hand, in low-dimensional settings, we show that heavy-tailed variational families and mass-covering divergences are effective and can increase the chances that the approximation can be improved by importance sampling.

</p>
</details>

<details><summary><b>Learners' languages</b>
<a href="https://arxiv.org/abs/2103.01189">arxiv:2103.01189</a>
&#x1F4C8; 48 <br>
<p>David I. Spivak</p></summary>
<p>

**Abstract:** In "Backprop as functor", the authors show that the fundamental elements of deep learning -- gradient descent and backpropagation -- can be conceptualized as a strong monoidal functor $\mathbf{Para}(\mathbf{Euc})\to\mathbf{Learn}$ from the category of parameterized Euclidean spaces to that of learners, a category developed explicitly to capture parameter update and backpropagation. It was soon realized that there is an isomorphism $\mathbf{Learn}\cong\mathbf{Para}(\mathbf{SLens})$, where $\mathbf{SLens}$ is the symmetric monoidal category of simple lenses as used in functional programming.
  In this note, we observe that $\mathbf{SLens}$ is a full subcategory of $\mathbf{Poly}$, the category of polynomial functors in one variable, via the functor $A\mapsto Ay^A$. Using the fact that $(\mathbf{Poly},\otimes)$ is monoidal closed, we show that a map $A\to B$ in $\mathbf{Para}(\mathbf{SLens})$ has a natural interpretation in terms of dynamical systems (more precisely, generalized Moore machines) whose interface is the internal-hom type $[Ay^A,By^B]$.
  Finally, we review the fact that the category $p\text{-}\mathbf{Coalg}$ of dynamical systems on any $p\in\mathbf{Poly}$ forms a topos, and consider the logical propositions that can be stated in its internal language. We give gradient descent as an example, and we conclude by discussing some directions for future work.

</p>
</details>

<details><summary><b>Cross Modal Focal Loss for RGBD Face Anti-Spoofing</b>
<a href="https://arxiv.org/abs/2103.00948">arxiv:2103.00948</a>
&#x1F4C8; 46 <br>
<p>Anjith George, Sebastien Marcel</p></summary>
<p>

**Abstract:** Automatic methods for detecting presentation attacks are essential to ensure the reliable use of facial recognition technology. Most of the methods available in the literature for presentation attack detection (PAD) fails in generalizing to unseen attacks. In recent years, multi-channel methods have been proposed to improve the robustness of PAD systems. Often, only a limited amount of data is available for additional channels, which limits the effectiveness of these methods. In this work, we present a new framework for PAD that uses RGB and depth channels together with a novel loss function. The new architecture uses complementary information from the two modalities while reducing the impact of overfitting. Essentially, a cross-modal focal loss function is proposed to modulate the loss contribution of each channel as a function of the confidence of individual channels. Extensive evaluations in two publicly available datasets demonstrate the effectiveness of the proposed approach.

</p>
</details>

<details><summary><b>Accounting for Variance in Machine Learning Benchmarks</b>
<a href="https://arxiv.org/abs/2103.03098">arxiv:2103.03098</a>
&#x1F4C8; 44 <br>
<p>Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Dmitriy Serdyuk, Tal Arbel, Chris Pal, GaÃ«l Varoquaux, Pascal Vincent</p></summary>
<p>

**Abstract:** Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, data augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51 times reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on five different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.

</p>
</details>

<details><summary><b>Fast Adaptation with Linearized Neural Networks</b>
<a href="https://arxiv.org/abs/2103.01439">arxiv:2103.01439</a>
&#x1F4C8; 29 <br>
<p>Wesley J. Maddox, Shuai Tang, Pablo Garcia Moreno, Andrew Gordon Wilson, Andreas Damianou</p></summary>
<p>

**Abstract:** The inductive biases of trained neural networks are difficult to understand and, consequently, to adapt to new settings. We study the inductive biases of linearizations of neural networks, which we show to be surprisingly good summaries of the full network functions. Inspired by this finding, we propose a technique for embedding these inductive biases into Gaussian processes through a kernel designed from the Jacobian of the network. In this setting, domain adaptation takes the form of interpretable posterior inference, with accompanying uncertainty estimation. This inference is analytic and free of local optima issues found in standard techniques such as fine-tuning neural network weights to a new task. We develop significant computational speed-ups based on matrix multiplies, including a novel implementation for scalable Fisher vector products. Our experiments on both image classification and regression demonstrate the promise and convenience of this framework for transfer learning, compared to neural network fine-tuning. Code is available at https://github.com/amzn/xfer/tree/master/finite_ntk.

</p>
</details>

<details><summary><b>Coordination Among Neural Modules Through a Shared Global Workspace</b>
<a href="https://arxiv.org/abs/2103.01197">arxiv:2103.01197</a>
&#x1F4C8; 29 <br>
<p>Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, Yoshua Bengio</p></summary>
<p>

**Abstract:** Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.

</p>
</details>

<details><summary><b>OmniNet: Omnidirectional Representations from Transformers</b>
<a href="https://arxiv.org/abs/2103.01075">arxiv:2103.01075</a>
&#x1F4C8; 29 <br>
<p>Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler</p></summary>
<p>

**Abstract:** This paper proposes Omnidirectional Representations from Transformers (OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer et al.) as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.

</p>
</details>

<details><summary><b>Statistically Significant Stopping of Neural Network Training</b>
<a href="https://arxiv.org/abs/2103.01205">arxiv:2103.01205</a>
&#x1F4C8; 28 <br>
<p>J. K. Terry, Mario Jayakumar, Kusal De Alwis</p></summary>
<p>

**Abstract:** The general approach taken when training deep learning classifiers is to save the parameters after every few iterations, train until either a human observer or a simple metric-based heuristic decides the network isn't learning anymore, and then backtrack and pick the saved parameters with the best validation accuracy. Simple methods are used to determine if a neural network isn't learning anymore because, as long as it's well after the optimal values are found, the condition doesn't impact the final accuracy of the model. However from a runtime perspective, this is of great significance to the many cases where numerous neural networks are trained simultaneously (e.g. hyper-parameter tuning). Motivated by this, we introduce a statistical significance test to determine if a neural network has stopped learning. This stopping criterion appears to represent a happy medium compared to other popular stopping criterions, achieving comparable accuracy to the criterions that achieve the highest final accuracies in 77% or fewer epochs, while the criterions which stop sooner do so with an appreciable loss to final accuracy. Additionally, we use this as the basis of a new learning rate scheduler, removing the need to manually choose learning rate schedules and acting as a quasi-line search, achieving superior or comparable empirical performance to existing methods.

</p>
</details>

<details><summary><b>Contrastive Explanations for Model Interpretability</b>
<a href="https://arxiv.org/abs/2103.01378">arxiv:2103.01378</a>
&#x1F4C8; 24 <br>
<p>Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg</p></summary>
<p>

**Abstract:** Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision.

</p>
</details>

<details><summary><b>Persistent Message Passing</b>
<a href="https://arxiv.org/abs/2103.01043">arxiv:2103.01043</a>
&#x1F4C8; 22 <br>
<p>Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, Petar VeliÄkoviÄ</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) are a powerful inductive bias for modelling algorithmic reasoning procedures and data structures. Their prowess was mainly demonstrated on tasks featuring Markovian dynamics, where querying any associated data structure depends only on its latest state. For many tasks of interest, however, it may be highly beneficial to support efficient data structure queries dependent on previous states. This requires tracking the data structure's evolution through time, placing significant pressure on the GNN's latent representations. We introduce Persistent Message Passing (PMP), a mechanism which endows GNNs with capability of querying past state by explicitly persisting it: rather than overwriting node representations, it creates new nodes whenever required. PMP generalises out-of-distribution to more than 2x larger test inputs on dynamic temporal range queries, significantly outperforming GNNs which overwrite states.

</p>
</details>

<details><summary><b>AdaSpeech: Adaptive Text to Speech for Custom Voice</b>
<a href="https://arxiv.org/abs/2103.00993">arxiv:2103.00993</a>
&#x1F4C8; 21 <br>
<p>Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, Tie-Yan Liu</p></summary>
<p>

**Abstract:** Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.

</p>
</details>

<details><summary><b>Counterfactual Zero-Shot and Open-Set Visual Recognition</b>
<a href="https://arxiv.org/abs/2103.00887">arxiv:2103.00887</a>
&#x1F4C8; 14 <br>
<p>Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua</p></summary>
<p>

**Abstract:** We present a novel counterfactual framework for both Zero-Shot Learning (ZSL) and Open-Set Recognition (OSR), whose common challenge is generalizing to the unseen-classes by only training on the seen-classes. Our idea stems from the observation that the generated samples for unseen-classes are often out of the true distribution, which causes severe recognition rate imbalance between the seen-class (high) and unseen-class (low). We show that the key reason is that the generation is not Counterfactual Faithful, and thus we propose a faithful one, whose generation is from the sample-specific counterfactual question: What would the sample look like, if we set its class attribute to a certain class, while keeping its sample attribute unchanged? Thanks to the faithfulness, we can apply the Consistency Rule to perform unseen/seen binary classification, by asking: Would its counterfactual still look like itself? If ``yes'', the sample is from a certain class, and ``no'' otherwise. Through extensive experiments on ZSL and OSR, we demonstrate that our framework effectively mitigates the seen/unseen imbalance and hence significantly improves the overall performance. Note that this framework is orthogonal to existing methods, thus, it can serve as a new baseline to evaluate how ZSL/OSR models generalize. Codes are available at https://github.com/yue-zhongqi/gcm-cf.

</p>
</details>

<details><summary><b>There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge</b>
<a href="https://arxiv.org/abs/2103.01353">arxiv:2103.01353</a>
&#x1F4C8; 12 <br>
<p>Francisco Rivera Valverde, Juana Valeria Hurtado, Abhinav Valada</p></summary>
<p>

**Abstract:** Attributes of sound inherent to objects can provide valuable cues to learn rich representations for object detection and tracking. Furthermore, the co-occurrence of audiovisual events in videos can be exploited to localize objects over the image field by solely monitoring the sound in the environment. Thus far, this has only been feasible in scenarios where the camera is static and for single object detection. Moreover, the robustness of these methods has been limited as they primarily rely on RGB images which are highly susceptible to illumination and weather changes. In this work, we present the novel self-supervised MM-DistillNet framework consisting of multiple teachers that leverage diverse modalities including RGB, depth and thermal images, to simultaneously exploit complementary cues and distill knowledge into a single audio student network. We propose the new MTA loss function that facilitates the distillation of information from multimodal teachers in a self-supervised manner. Additionally, we propose a novel self-supervised pretext task for the audio student that enables us to not rely on labor-intensive manual annotations. We introduce a large-scale multimodal dataset with over 113,000 time-synchronized frames of RGB, depth, thermal, and audio modalities. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods while being able to detect multiple objects using only sound during inference and even while moving.

</p>
</details>

<details><summary><b>Scalable Scene Flow from Point Clouds in the Real World</b>
<a href="https://arxiv.org/abs/2103.01306">arxiv:2103.01306</a>
&#x1F4C8; 12 <br>
<p>Philipp Jund, Chris Sweeney, Nichola Abdo, Zhifeng Chen, Jonathon Shlens</p></summary>
<p>

**Abstract:** Autonomous vehicles operate in highly dynamic environments necessitating an accurate assessment of which aspects of a scene are moving and where they are moving to. A popular approach to 3D motion estimation, termed scene flow, is to employ 3D point cloud data from consecutive LiDAR scans, although such approaches have been limited by the small size of real-world, annotated LiDAR data. In this work, we introduce a new large-scale dataset for scene flow estimation derived from corresponding tracked 3D objects, which is $\sim$1,000$\times$ larger than previous real-world datasets in terms of the number of annotated frames. We demonstrate how previous works were bounded based on the amount of real LiDAR data available, suggesting that larger datasets are required to achieve state-of-the-art predictive performance. Furthermore, we show how previous heuristics for operating on point clouds such as down-sampling heavily degrade performance, motivating a new class of models that are tractable on the full point cloud. To address this issue, we introduce the FastFlow3D architecture which provides real time inference on the full point cloud. Additionally, we design human-interpretable metrics that better capture real world aspects by accounting for ego-motion and providing breakdowns per object type. We hope that this dataset may provide new opportunities for developing real world scene flow systems.

</p>
</details>

<details><summary><b>Assessing deep learning methods for the identification of kidney stones in endoscopic images</b>
<a href="https://arxiv.org/abs/2103.01146">arxiv:2103.01146</a>
&#x1F4C8; 12 <br>
<p>Francisco Lopez, Andres Varela, Oscar Hinojosa, Mauricio Mendez, Dinh-Hoan Trinh, Jonathan ElBeze, Jacques Hubert, Vincent Estrade, Miguel Gonzalez, Gilberto Ochoa, Christian Daul</p></summary>
<p>

**Abstract:** Knowing the type (i.e., the biochemical composition) of kidney stones is crucial to prevent relapses with an appropriate treatment. During ureteroscopies, kidney stones are fragmented, extracted from the urinary tract, and their composition is determined using a morpho-constitutional analysis. This procedure is time consuming (the morpho-constitutional analysis results are only available after some days) and tedious (the fragment extraction lasts up to an hour). Identifying the kidney stone type only with the in-vivo endoscopic images would allow for the dusting of the fragments, while the morpho-constitutional analysis could be avoided. Only few contributions dealing with the in vivo identification of kidney stones were published. This paper discusses and compares five classification methods including deep convolutional neural networks (DCNN)-based approaches and traditional (non DCNN-based) ones. Even if the best method is a DCCN approach with a precision and recall of 98% and 97% over four classes, this contribution shows that a XGBoost classifier exploiting well-chosen feature vectors can closely approach the performances of DCNN classifiers for a medical application with a limited number of annotated data.

</p>
</details>

<details><summary><b>Kernel Interpolation for Scalable Online Gaussian Processes</b>
<a href="https://arxiv.org/abs/2103.01454">arxiv:2103.01454</a>
&#x1F4C8; 10 <br>
<p>Samuel Stanton, Wesley J. Maddox, Ian Delbridge, Andrew Gordon Wilson</p></summary>
<p>

**Abstract:** Gaussian processes (GPs) provide a gold standard for performance in online settings, such as sample-efficient control and black box optimization, where we need to update a posterior distribution as we acquire data in a sequential fashion. However, updating a GP posterior to accommodate even a single new observation after having observed $n$ points incurs at least $O(n)$ computations in the exact setting. We show how to use structured kernel interpolation to efficiently recycle computations for constant-time $O(1)$ online updates with respect to the number of points $n$, while retaining exact inference. We demonstrate the promise of our approach in a range of online regression and classification settings, Bayesian optimization, and active sampling to reduce error in malaria incidence forecasting. Code is available at https://github.com/wjmaddox/online_gp.

</p>
</details>

<details><summary><b>Posterior Meta-Replay for Continual Learning</b>
<a href="https://arxiv.org/abs/2103.01133">arxiv:2103.01133</a>
&#x1F4C8; 9 <br>
<p>Christian Henning, Maria R. Cervera, Francesco D'Angelo, Johannes von Oswald, Regina Traber, Benjamin Ehret, Seijin Kobayashi, Benjamin F. Grewe, JoÃ£o Sacramento</p></summary>
<p>

**Abstract:** Learning a sequence of tasks without access to i.i.d. observations is a widely studied form of continual learning (CL) that remains challenging. In principle, Bayesian learning directly applies to this setting, since recursive and one-off Bayesian updates yield the same result. In practice, however, recursive updating often leads to poor trade-off solutions across tasks because approximate inference is necessary for most models of interest. Here, we describe an alternative Bayesian approach where task-conditioned parameter distributions are continually inferred from data. We offer a practical deep learning implementation of our framework based on probabilistic task-conditioned hypernetworks, an approach we term posterior meta-replay. Experiments on standard benchmarks show that our probabilistic hypernetworks compress sequences of posterior parameter distributions with virtually no forgetting. We obtain considerable performance gains compared to existing Bayesian CL methods, and identify task inference as our major limiting factor. This limitation has several causes that are independent of the considered sequential setting, opening up new avenues for progress in CL.

</p>
</details>

<details><summary><b>Diverse Critical Interaction Generation for Planning and Planner Evaluation</b>
<a href="https://arxiv.org/abs/2103.00906">arxiv:2103.00906</a>
&#x1F4C8; 9 <br>
<p>Zhao-Heng Yin, Lingfeng Sun, Liting Sun, Masayoshi Tomizuka, Wei Zhan</p></summary>
<p>

**Abstract:** Generating diverse and comprehensive interacting agents to evaluate the decision-making modules is essential for the safe and robust planning of autonomous vehicles~(AV). Due to efficiency and safety concerns, most researchers choose to train interactive adversary~(competitive or weakly competitive) agents in simulators and generate test cases to interact with evaluated AVs. However, most existing methods fail to provide both natural and critical interaction behaviors in various traffic scenarios. To tackle this problem, we propose a styled generative model RouteGAN that generates diverse interactions by controlling the vehicles separately with desired styles. By altering its style coefficients, the model can generate trajectories with different safety levels serve as an online planner. Experiments show that our model can generate diverse interactions in various scenarios. We evaluate different planners with our model by testing their collision rate in interaction with RouteGAN planners of multiple critical levels.

</p>
</details>

<details><summary><b>Multi-Objective Evolutionary Design of Composite Data-Driven Models</b>
<a href="https://arxiv.org/abs/2103.01301">arxiv:2103.01301</a>
&#x1F4C8; 8 <br>
<p>Iana S. Polonskaia, Nikolay O. Nikitin, Ilia Revin, Pavel Vychuzhanin, Anna V. Kalyuzhnaya</p></summary>
<p>

**Abstract:** In this paper, a multi-objective approach for the design of composite data-driven mathematical models is proposed. It allows automating the identification of graph-based heterogeneous pipelines that consist of different blocks: machine learning models, data preprocessing blocks, etc. The implemented approach is based on a parameter-free genetic algorithm (GA) for model design called GPComp@Free. It is developed to be part of automated machine learning solutions and to increase the efficiency of the modeling pipeline automation. A set of experiments was conducted to verify the correctness and efficiency of the proposed approach and substantiate the selected solutions. The experimental results confirm that a multi-objective approach to the model design allows achieving better diversity and quality of obtained models. The implemented approach is available as a part of the open-source AutoML framework FEDOT.

</p>
</details>

<details><summary><b>The Healthy States of America: Creating a Health Taxonomy with Social Media</b>
<a href="https://arxiv.org/abs/2103.01169">arxiv:2103.01169</a>
&#x1F4C8; 8 <br>
<p>Sanja Scepanovic, Luca Maria Aiello, Ke Zhou, Sagar Joglekar, Daniele Quercia</p></summary>
<p>

**Abstract:** Since the uptake of social media, researchers have mined online discussions to track the outbreak and evolution of specific diseases or chronic conditions such as influenza or depression. To broaden the set of diseases under study, we developed a Deep Learning tool for Natural Language Processing that extracts mentions of virtually any medical condition or disease from unstructured social media text. With that tool at hand, we processed Reddit and Twitter posts, analyzed the clusters of the two resulting co-occurrence networks of conditions, and discovered that they correspond to well-defined categories of medical conditions. This resulted in the creation of the first comprehensive taxonomy of medical conditions automatically derived from online discussions. We validated the structure of our taxonomy against the official International Statistical Classification of Diseases and Related Health Problems (ICD-11), finding matches of our clusters with 20 official categories, out of 22. Based on the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced in the U.S., we were then able to compute disease-specific health scores. As opposed to counts of disease mentions or counts with no knowledge of our taxonomy's structure, we found that our disease-specific health scores are causally linked with the officially reported prevalence of 18 conditions.

</p>
</details>

<details><summary><b>Domain Generalization via Inference-time Label-Preserving Target Projections</b>
<a href="https://arxiv.org/abs/2103.01134">arxiv:2103.01134</a>
&#x1F4C8; 8 <br>
<p>Prashant Pandey, Mrigank Raman, Sumanth Varambally, Prathosh AP</p></summary>
<p>

**Abstract:** Generalization of machine learning models trained on a set of source domains on unseen target domains with different statistics, is a challenging problem. While many approaches have been proposed to solve this problem, they only utilize source data during training but do not take advantage of the fact that a single target example is available at the time of inference. Motivated by this, we propose a method that effectively uses the target sample during inference beyond mere classification. Our method has three components - (i) A label-preserving feature or metric transformation on source data such that the source samples are clustered in accordance with their class irrespective of their domain (ii) A generative model trained on the these features (iii) A label-preserving projection of the target point on the source-feature manifold during inference via solving an optimization problem on the input space of the generative model using the learned metric. Finally, the projected target is used in the classifier. Since the projected target feature comes from the source manifold and has the same label as the real target by design, the classifier is expected to perform better on it than the true target. We demonstrate that our method outperforms the state-of-the-art Domain Generalization methods on multiple datasets and tasks.

</p>
</details>

<details><summary><b>Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack</b>
<a href="https://arxiv.org/abs/2103.00847">arxiv:2103.00847</a>
&#x1F4C8; 8 <br>
<p>Shahroz Tariq, Sowon Jeon, Simon S. Woo</p></summary>
<p>

**Abstract:** Recently, significant advancements have been made in face recognition technologies using Deep Neural Networks. As a result, companies such as Microsoft, Amazon, and Naver offer highly accurate commercial face recognition web services for diverse applications to meet the end-user needs. Naturally, however, such technologies are threatened persistently, as virtually any individual can quickly implement impersonation attacks. In particular, these attacks can be a significant threat for authentication and identification services, which heavily rely on their underlying face recognition technologies' accuracy and robustness. Despite its gravity, the issue regarding deepfake abuse using commercial web APIs and their robustness has not yet been thoroughly investigated. This work provides a measurement study on the robustness of black-box commercial face recognition APIs against Deepfake Impersonation (DI) attacks using celebrity recognition APIs as an example case study. We use five deepfake datasets, two of which are created by us and planned to be released. More specifically, we measure attack performance based on two scenarios (targeted and non-targeted) and further analyze the differing system behaviors using fidelity, confidence, and similarity metrics. Accordingly, we demonstrate how vulnerable face recognition technologies from popular companies are to DI attack, achieving maximum success rates of 78.0% and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with any celebrity) attacks, respectively. Moreover, we propose practical defense strategies to mitigate DI attacks, reducing the attack success rates to as low as 0% and 0.02% for targeted and non-targeted attacks, respectively.

</p>
</details>

<details><summary><b>Generating Probabilistic Safety Guarantees for Neural Network Controllers</b>
<a href="https://arxiv.org/abs/2103.01203">arxiv:2103.01203</a>
&#x1F4C8; 7 <br>
<p>Sydney M. Katz, Kyle D. Julian, Christopher A. Strong, Mykel J. Kochenderfer</p></summary>
<p>

**Abstract:** Neural networks serve as effective controllers in a variety of complex settings due to their ability to represent expressive policies. The complex nature of neural networks, however, makes their output difficult to verify and predict, which limits their use in safety-critical applications. While simulations provide insight into the performance of neural network controllers, they are not enough to guarantee that the controller will perform safely in all scenarios. To address this problem, recent work has focused on formal methods to verify properties of neural network outputs. For neural network controllers, we can use a dynamics model to determine the output properties that must hold for the controller to operate safely. In this work, we develop a method to use the results from neural network verification tools to provide probabilistic safety guarantees on a neural network controller. We develop an adaptive verification approach to efficiently generate an overapproximation of the neural network policy. Next, we modify the traditional formulation of Markov decision process (MDP) model checking to provide guarantees on the overapproximated policy given a stochastic dynamics model. Finally, we incorporate techniques in state abstraction to reduce overapproximation error during the model checking process. We show that our method is able to generate meaningful probabilistic safety guarantees for aircraft collision avoidance neural networks that are loosely inspired by Airborne Collision Avoidance System X (ACAS X), a family of collision avoidance systems that formulates the problem as a partially observable Markov decision process (POMDP).

</p>
</details>

<details><summary><b>Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues</b>
<a href="https://arxiv.org/abs/2103.00820">arxiv:2103.00820</a>
&#x1F4C8; 7 <br>
<p>Hung Le, Nancy F. Chen, Steven C. H. Hoi</p></summary>
<p>

**Abstract:** Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.

</p>
</details>

<details><summary><b>AdeNet: Deep learning architecture that identifies damaged electrical insulators in power lines</b>
<a href="https://arxiv.org/abs/2103.01426">arxiv:2103.01426</a>
&#x1F4C8; 6 <br>
<p>Ademola Okerinde, Lior Shamir, William Hsu, Tom Theis</p></summary>
<p>

**Abstract:** Ceramic insulators are important to electronic systems, designed and installed to protect humans from the danger of high voltage electric current. However, insulators are not immortal, and natural deterioration can gradually damage them. Therefore, the condition of insulators must be continually monitored, which is normally done using UAVs. UAVs collect many images of insulators, and these images are then analyzed to identify those that are damaged. Here we describe AdeNet as a deep neural network designed to identify damaged insulators, and test multiple approaches to automatic analysis of the condition of insulators. Several deep neural networks were tested, as were shallow learning methods. The best results (88.8\%) were achieved using AdeNet without transfer learning. AdeNet also reduced the false negative rate to $\sim$7\%. While the method cannot fully replace human inspection, its high throughput can reduce the amount of labor required to monitor lines for damaged insulators and provide early warning to replace damaged insulators.

</p>
</details>

<details><summary><b>Acceleration via Fractal Learning Rate Schedules</b>
<a href="https://arxiv.org/abs/2103.01338">arxiv:2103.01338</a>
&#x1F4C8; 6 <br>
<p>Naman Agarwal, Surbhi Goel, Cyril Zhang</p></summary>
<p>

**Abstract:** In practical applications of iterative first-order optimization, the learning rate schedule remains notoriously difficult to understand and expensive to tune. We demonstrate the presence of these subtleties even in the innocuous case when the objective is a convex quadratic. We reinterpret an iterative algorithm from the numerical analysis literature as what we call the Chebyshev learning rate schedule for accelerating vanilla gradient descent, and show that the problem of mitigating instability leads to a fractal ordering of step sizes. We provide some experiments to challenge conventional beliefs about stable learning rates in deep learning: the fractal schedule enables training to converge with locally unstable updates which make negative progress on the objective.

</p>
</details>

<details><summary><b>Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</b>
<a href="https://arxiv.org/abs/2103.01242">arxiv:2103.01242</a>
&#x1F4C8; 6 <br>
<p>Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy</p></summary>
<p>

**Abstract:** Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%).

</p>
</details>

<details><summary><b>Computing the Information Content of Trained Neural Networks</b>
<a href="https://arxiv.org/abs/2103.01045">arxiv:2103.01045</a>
&#x1F4C8; 6 <br>
<p>Jeremy Bernstein, Yisong Yue</p></summary>
<p>

**Abstract:** How much information does a learning algorithm extract from the training data and store in a neural network's weights? Too much, and the network would overfit to the training data. Too little, and the network would not fit to anything at all. NaÃ¯vely, the amount of information the network stores should scale in proportion to the number of trainable weights. This raises the question: how can neural networks with vastly more weights than training data still generalise? A simple resolution to this conundrum is that the number of weights is usually a bad proxy for the actual amount of information stored. For instance, typical weight vectors may be highly compressible. Then another question occurs: is it possible to compute the actual amount of information stored? This paper derives both a consistent estimator and a closed-form upper bound on the information content of infinitely wide neural networks. The derivation is based on an identification between neural information content and the negative log probability of a Gaussian orthant. This identification yields bounds that analytically control the generalisation behaviour of the entire solution space of infinitely wide networks. The bounds have a simple dependence on both the network architecture and the training data. Corroborating the findings of Valle-PÃ©rez et al. (2019), who conducted a similar analysis using approximate Gaussian integration techniques, the bounds are found to be both non-vacuous and correlated with the empirical generalisation behaviour at finite width.

</p>
</details>

<details><summary><b>DR-TANet: Dynamic Receptive Temporal Attention Network for Street Scene Change Detection</b>
<a href="https://arxiv.org/abs/2103.00879">arxiv:2103.00879</a>
&#x1F4C8; 6 <br>
<p>Shuo Chen, Kailun Yang, Rainer Stiefelhagen</p></summary>
<p>

**Abstract:** Street scene change detection continues to capture researchers' interests in the computer vision community. It aims to identify the changed regions of the paired street-view images captured at different times. The state-of-the-art network based on the encoder-decoder architecture leverages the feature maps at the corresponding level between two channels to gain sufficient information of changes. Still, the efficiency of feature extraction, feature correlation calculation, even the whole network requires further improvement. This paper proposes the temporal attention and explores the impact of the dependency-scope size of temporal attention on the performance of change detection. In addition, based on the Temporal Attention Module (TAM), we introduce a more efficient and light-weight version - Dynamic Receptive Temporal Attention Module (DRTAM) and propose the Concurrent Horizontal and Vertical Attention (CHVA) to improve the accuracy of the network on specific challenging entities. On street scene datasets `GSV', `TSUNAMI' and `VL-CMU-CD', our approach gains excellent performance, establishing new state-of-the-art scores without bells and whistles, while maintaining high efficiency applicable in autonomous vehicles.

</p>
</details>

<details><summary><b>Tune-In: Training Under Negative Environments with Interference for Attention Networks Simulating Cocktail Party Effect</b>
<a href="https://arxiv.org/abs/2103.01461">arxiv:2103.01461</a>
&#x1F4C8; 5 <br>
<p>Jun Wang, Max W. Y. Lam, Dan Su, Dong Yu</p></summary>
<p>

**Abstract:** We study the cocktail party problem and propose a novel attention network called Tune-In, abbreviated for training under negative environments with interference. It firstly learns two separate spaces of speaker-knowledge and speech-stimuli based on a shared feature space, where a new block structure is designed as the building block for all spaces, and then cooperatively solves different tasks. Between the two spaces, information is cast towards each other via a novel cross- and dual-attention mechanism, mimicking the bottom-up and top-down processes of a human's cocktail party effect. It turns out that substantially discriminative and generalizable speaker representations can be learnt in severely interfered conditions via our self-supervised training. The experimental results verify this seeming paradox. The learnt speaker embedding has superior discriminative power than a standard speaker verification method; meanwhile, Tune-In achieves remarkably better speech separation performances in terms of SI-SNRi and SDRi consistently in all test modes, and especially at lower memory and computational consumption, than state-of-the-art benchmark systems.

</p>
</details>

<details><summary><b>Deep Bag-of-Sub-Emotions for Depression Detection in Social Media</b>
<a href="https://arxiv.org/abs/2103.01334">arxiv:2103.01334</a>
&#x1F4C8; 5 <br>
<p>Juan S. Lara, Mario Ezra Aragon, Fabio A. Gonzalez, Manuel Montes-y-Gomez</p></summary>
<p>

**Abstract:** This paper presents the Deep Bag-of-Sub-Emotions (DeepBoSE), a novel deep learning model for depression detection in social media. The model is formulated such that it internally computes a differentiable Bag-of-Features (BoF) representation that incorporates emotional information. This is achieved by a reinterpretation of classical weighting schemes like term frequency-inverse document frequency into probabilistic deep learning operations. An important advantage of the proposed method is that it can be trained under the transfer learning paradigm, which is useful to enhance conventional BoF models that cannot be directly integrated into deep learning architectures. Experiments were performed in the eRisk17 and eRisk18 datasets for the depression detection task; results show that DeepBoSE outperforms conventional BoF representations and it is competitive with the state of the art, achieving a F1-score over the positive class of 0.64 in eRisk17 and 0.65 in eRisk18.

</p>
</details>

<details><summary><b>Audio-Visual Speech Separation Using Cross-Modal Correspondence Loss</b>
<a href="https://arxiv.org/abs/2103.01463">arxiv:2103.01463</a>
&#x1F4C8; 4 <br>
<p>Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro Tanaka, Shota Orihashi, Ryo Masumura</p></summary>
<p>

**Abstract:** We present an audio-visual speech separation learning method that considers the correspondence between the separated signals and the visual signals to reflect the speech characteristics during training. Audio-visual speech separation is a technique to estimate the individual speech signals from a mixture using the visual signals of the speakers. Conventional studies on audio-visual speech separation mainly train the separation model on the audio-only loss, which reflects the distance between the source signals and the separated signals. However, conventional losses do not reflect the characteristics of the speech signals, including the speaker's characteristics and phonetic information, which leads to distortion or remaining noise. To address this problem, we propose the cross-modal correspondence (CMC) loss, which is based on the cooccurrence of the speech signal and the visual signal. Since the visual signal is not affected by background noise and contains speaker and phonetic information, using the CMC loss enables the audio-visual speech separation model to remove noise while preserving the speech characteristics. Experimental results demonstrate that the proposed method learns the cooccurrence on the basis of CMC loss, which improves separation performance.

</p>
</details>

<details><summary><b>DeepMerge II: Building Robust Deep Learning Algorithms for Merging Galaxy Identification Across Domains</b>
<a href="https://arxiv.org/abs/2103.01373">arxiv:2103.01373</a>
&#x1F4C8; 4 <br>
<p>A. ÄiprijanoviÄ, D. Kafkes, K. Downey, S. Jenkins, G. N. Perdue, S. Madireddy, T. Johnston, G. F. Snyder, B. Nord</p></summary>
<p>

**Abstract:** In astronomy, neural networks are often trained on simulation data with the prospect of being used on telescope observations. Unfortunately, training a model on simulation data and then applying it to instrument data leads to a substantial and potentially even detrimental decrease in model accuracy on the new target dataset. Simulated and instrument data represent different data domains, and for an algorithm to work in both, domain-invariant learning is necessary. Here we employ domain adaptation techniques$-$ Maximum Mean Discrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural Networks (DANNs)$-$ and demonstrate their viability to extract domain-invariant features within the astronomical context of classifying merging and non-merging galaxies. Additionally, we explore the use of Fisher loss and entropy minimization to enforce better in-domain class discriminability. We show that the addition of each domain adaptation technique improves the performance of a classifier when compared to conventional deep learning algorithms. We demonstrate this on two examples: between two Illustris-1 simulated datasets of distant merging galaxies, and between Illustris-1 simulated data of nearby merging galaxies and observed data from the Sloan Digital Sky Survey. The use of domain adaptation techniques in our experiments leads to an increase of target domain classification accuracy of up to ${\sim}20\%$. With further development, these techniques will allow astronomers to successfully implement neural network models trained on simulation data to efficiently detect and study astrophysical objects in current and future large-scale astronomical surveys.

</p>
</details>

<details><summary><b>Understanding & Predicting User Lifetime with Machine Learning in an Anonymous Location-Based Social Network</b>
<a href="https://arxiv.org/abs/2103.01300">arxiv:2103.01300</a>
&#x1F4C8; 4 <br>
<p>Jens Helge Reelfs, Max Bergmann, Oliver Hohlfeld, Niklas Henckell</p></summary>
<p>

**Abstract:** In this work, we predict the user lifetime within the anonymous and location-based social network Jodel in the Kingdom of Saudi Arabia. Jodel's location-based nature yields to the establishment of disjoint communities country-wide and enables for the first time the study of user lifetime in the case of a large set of disjoint communities. A user's lifetime is an important measurement for evaluating and steering customer bases as it can be leveraged to predict churn and possibly apply suitable methods to circumvent potential user losses. We train and test off the shelf machine learning techniques with 5-fold crossvalidation to predict user lifetime as a regression and classification problem; identifying the Random Forest to provide very strong results. Discussing model complexity and quality trade-offs, we also dive deep into a time-dependent feature subset analysis, which does not work very well; Easing up the classification problem into a binary decision (lifetime longer than timespan $x$) enables a practical lifetime predictor with very good performance. We identify implicit similarities across community models according to strong correlations in feature importance. A single countrywide model generalizes the problem and works equally well for any tested community; the overall model internally works similar to others also indicated by its feature importances.

</p>
</details>

<details><summary><b>Geometry-Based Grasping of Vine Tomatoes</b>
<a href="https://arxiv.org/abs/2103.01272">arxiv:2103.01272</a>
&#x1F4C8; 4 <br>
<p>Taeke de Haan, Padmaja Kulkarni, Robert Babuska</p></summary>
<p>

**Abstract:** We propose a geometry-based grasping method for vine tomatoes. It relies on a computer-vision pipeline to identify the required geometric features of the tomatoes and of the truss stem. The grasping method then uses a geometric model of the robotic hand and the truss to determine a suitable grasping location on the stem. This approach allows for grasping tomato trusses without requiring delicate contact sensors or complex mechanistic models and under minimal risk of damaging the tomatoes. Lab experiments were conducted to validate the proposed methods, using an RGB-D camera and a low-cost robotic manipulator. The success rate was 83% to 92%, depending on the type of truss.

</p>
</details>

<details><summary><b>Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels</b>
<a href="https://arxiv.org/abs/2103.01210">arxiv:2103.01210</a>
&#x1F4C8; 4 <br>
<p>Eran Malach, Pritish Kamath, Emmanuel Abbe, Nathan Srebro</p></summary>
<p>

**Abstract:** We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.

</p>
</details>

<details><summary><b>BERT based patent novelty search by training claims to their own description</b>
<a href="https://arxiv.org/abs/2103.01126">arxiv:2103.01126</a>
&#x1F4C8; 4 <br>
<p>Michael Freunek, AndrÃ© Bodmer</p></summary>
<p>

**Abstract:** In this paper we present a method to concatenate patent claims to their own description. By applying this method, BERT trains suitable descriptions for claims. Such a trained BERT (claim-to-description- BERT) could be able to identify novelty relevant descriptions for patents. In addition, we introduce a new scoring scheme, relevance scoring or novelty scoring, to process the output of BERT in a meaningful way. We tested the method on patent applications by training BERT on the first claims of patents and corresponding descriptions. BERT's output has been processed according to the relevance score and the results compared with the cited X documents in the search reports. The test showed that BERT has scored some of the cited X documents as highly relevant.

</p>
</details>

<details><summary><b>On the Fairness of Generative Adversarial Networks (GANs)</b>
<a href="https://arxiv.org/abs/2103.00950">arxiv:2103.00950</a>
&#x1F4C8; 4 <br>
<p>Patrik Joslin Kenfack, Daniil Dmitrievich Arapov, Rasheed Hussain, S. M. Ahsan Kazmi, Adil Mehmood Khan</p></summary>
<p>

**Abstract:** Generative adversarial networks (GANs) are one of the greatest advances in AI in recent years. With their ability to directly learn the probability distribution of data, and then sample synthetic realistic data. Many applications have emerged, using GANs to solve classical problems in machine learning, such as data augmentation, class unbalance problems, and fair representation learning. In this paper, we analyze and highlight fairness concerns of GANs model. In this regard, we show empirically that GANs models may inherently prefer certain groups during the training process and therefore they're not able to homogeneously generate data from different groups during the testing phase. Furthermore, we propose solutions to solve this issue by conditioning the GAN model towards samples' group or using ensemble method (boosting) to allow the GAN model to leverage distributed structure of data during the training phase and generate groups at equal rate during the testing phase.

</p>
</details>

<details><summary><b>Mitigating Edge Machine Learning Inference Bottlenecks: An Empirical Study on Accelerating Google Edge Models</b>
<a href="https://arxiv.org/abs/2103.00768">arxiv:2103.00768</a>
&#x1F4C8; 4 <br>
<p>Amirali Boroumand, Saugata Ghose, Berkin Akin, Ravi Narayanaswami, Geraldo F. Oliveira, Xiaoyu Ma, Eric Shiu, Onur Mutlu</p></summary>
<p>

**Abstract:** As the need for edge computing grows, many modern consumer devices now contain edge machine learning (ML) accelerators that can compute a wide range of neural network (NN) models while still fitting within tight resource constraints. We analyze a commercial Edge TPU using 24 Google edge NN models (including CNNs, LSTMs, transducers, and RCNNs), and find that the accelerator suffers from three shortcomings, in terms of computational throughput, energy efficiency, and memory access handling. We comprehensively study the characteristics of each NN layer in all of the Google edge models, and find that these shortcomings arise from the one-size-fits-all approach of the accelerator, as there is a high amount of heterogeneity in key layer characteristics both across different models and across different layers in the same model.
  We propose a new acceleration framework called Mensa. Mensa incorporates multiple heterogeneous ML edge accelerators (including both on-chip and near-data accelerators), each of which caters to the characteristics of a particular subset of models. At runtime, Mensa schedules each layer to run on the best-suited accelerator, accounting for both efficiency and inter-layer dependencies. As we analyze the Google edge NN models, we discover that all of the layers naturally group into a small number of clusters, which allows us to design an efficient implementation of Mensa for these models with only three specialized accelerators. Averaged across all 24 Google edge models, Mensa improves energy efficiency and throughput by 3.0x and 3.1x over the Edge TPU, and by 2.4x and 4.3x over Eyeriss v2, a state-of-the-art accelerator.

</p>
</details>

<details><summary><b>Efficient Optimal Selection for Composited Advertising Creatives with Tree Structure</b>
<a href="https://arxiv.org/abs/2103.01453">arxiv:2103.01453</a>
&#x1F4C8; 3 <br>
<p>Jin Chen, Tiezheng Ge, Gangwei Jiang, Zhiqiang Zhang, Defu Lian, Kai Zheng</p></summary>
<p>

**Abstract:** Ad creatives are one of the prominent mediums for online e-commerce advertisements. Ad creatives with enjoyable visual appearance may increase the click-through rate (CTR) of products. Ad creatives are typically handcrafted by advertisers and then delivered to the advertising platforms for advertisement. In recent years, advertising platforms are capable of instantly compositing ad creatives with arbitrarily designated elements of each ingredient, so advertisers are only required to provide basic materials. While facilitating the advertisers, a great number of potential ad creatives can be composited, making it difficult to accurately estimate CTR for them given limited real-time feedback. To this end, we propose an Adaptive and Efficient ad creative Selection (AES) framework based on a tree structure. The tree structure on compositing ingredients enables dynamic programming for efficient ad creative selection on the basis of CTR. Due to limited feedback, the CTR estimator is usually of high variance. Exploration techniques based on Thompson sampling are widely used for reducing variances of the CTR estimator, alleviating feedback sparsity. Based on the tree structure, Thompson sampling is adapted with dynamic programming, leading to efficient exploration for potential ad creatives with the largest CTR. We finally evaluate the proposed algorithm on the synthetic dataset and the real-world dataset. The results show that our approach can outperform competing baselines in terms of convergence rate and overall CTR.

</p>
</details>

<details><summary><b>Interpretable Hyperspectral AI: When Non-Convex Modeling meets Hyperspectral Remote Sensing</b>
<a href="https://arxiv.org/abs/2103.01449">arxiv:2103.01449</a>
&#x1F4C8; 3 <br>
<p>Danfeng Hong, Wei He, Naoto Yokoya, Jing Yao, Lianru Gao, Liangpei Zhang, Jocelyn Chanussot, Xiao Xiang Zhu</p></summary>
<p>

**Abstract:** Hyperspectral imaging, also known as image spectrometry, is a landmark technique in geoscience and remote sensing (RS). In the past decade, enormous efforts have been made to process and analyze these hyperspectral (HS) products mainly by means of seasoned experts. However, with the ever-growing volume of data, the bulk of costs in manpower and material resources poses new challenges on reducing the burden of manual labor and improving efficiency. For this reason, it is, therefore, urgent to develop more intelligent and automatic approaches for various HS RS applications. Machine learning (ML) tools with convex optimization have successfully undertaken the tasks of numerous artificial intelligence (AI)-related applications. However, their ability in handling complex practical problems remains limited, particularly for HS data, due to the effects of various spectral variabilities in the process of HS imaging and the complexity and redundancy of higher dimensional HS signals. Compared to the convex models, non-convex modeling, which is capable of characterizing more complex real scenes and providing the model interpretability technically and theoretically, has been proven to be a feasible solution to reduce the gap between challenging HS vision tasks and currently advanced intelligent data processing models.

</p>
</details>

<details><summary><b>A Survey of Deep Learning Techniques for Weed Detection from Images</b>
<a href="https://arxiv.org/abs/2103.01415">arxiv:2103.01415</a>
&#x1F4C8; 3 <br>
<p>A S M Mahmudul Hasan, Ferdous Sohel, Dean Diepeveen, Hamid Laga, Michael G. K. Jones</p></summary>
<p>

**Abstract:** The rapid advances in Deep Learning (DL) techniques have enabled rapid detection, localisation, and recognition of objects from images or videos. DL techniques are now being used in many applications related to agriculture and farming. Automatic detection and classification of weeds can play an important role in weed management and so contribute to higher yields. Weed detection in crops from imagery is inherently a challenging problem because both weeds and crops have similar colours ('green-on-green'), and their shapes and texture can be very similar at the growth phase. Also, a crop in one setting can be considered a weed in another. In addition to their detection, the recognition of specific weed species is essential so that targeted controlling mechanisms (e.g. appropriate herbicides and correct doses) can be applied. In this paper, we review existing deep learning-based weed detection and classification techniques. We cover the detailed literature on four main procedures, i.e., data acquisition, dataset preparation, DL techniques employed for detection, location and classification of weeds in crops, and evaluation metrics approaches. We found that most studies applied supervised learning techniques, they achieved high classification accuracy by fine-tuning pre-trained models on any plant dataset, and past experiments have already achieved high accuracy when a large amount of labelled data is available.

</p>
</details>

<details><summary><b>A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics</b>
<a href="https://arxiv.org/abs/2103.01403">arxiv:2103.01403</a>
&#x1F4C8; 3 <br>
<p>Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu</p></summary>
<p>

**Abstract:** Inspired by humans' remarkable ability to master arithmetic and generalize to unseen problems, we present a new dataset, HINT, to study machines' capability of learning generalizable concepts at three different levels: perception, syntax, and semantics. In particular, concepts in HINT, including both digits and operators, are required to learn in a weakly-supervised fashion: Only the final results of handwriting expressions are provided as supervision. Learning agents need to reckon how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics). With a focus on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts. To tackle this challenging problem, we propose a neural-symbolic system by integrating neural networks with grammar parsing and program synthesis, learned by a novel deduction--abduction strategy. In experiments, the proposed neural-symbolic system demonstrates strong generalization capability and significantly outperforms end-to-end neural methods like RNN and Transformer. The results also indicate the significance of recursive priors for extrapolation on syntax and semantics.

</p>
</details>

<details><summary><b>Adversarial training in communication constrained federated learning</b>
<a href="https://arxiv.org/abs/2103.01319">arxiv:2103.01319</a>
&#x1F4C8; 3 <br>
<p>Devansh Shah, Parijat Dube, Supriyo Chakraborty, Ashish Verma</p></summary>
<p>

**Abstract:** Federated learning enables model training over a distributed corpus of agent data. However, the trained model is vulnerable to adversarial examples, designed to elicit misclassification. We study the feasibility of using adversarial training (AT) in the federated learning setting. Furthermore, we do so assuming a fixed communication budget and non-iid data distribution between participating agents. We observe a significant drop in both natural and adversarial accuracies when AT is used in the federated setting as opposed to centralized training. We attribute this to the number of epochs of AT performed locally at the agents, which in turn effects (i) drift between local models; and (ii) convergence time (measured in number of communication rounds). Towards this end, we propose FedDynAT, a novel algorithm for performing AT in federated setting. Through extensive experimentation we show that FedDynAT significantly improves both natural and adversarial accuracy, as well as model convergence time by reducing the model drift.

</p>
</details>

<details><summary><b>UCB Momentum Q-learning: Correcting the bias without forgetting</b>
<a href="https://arxiv.org/abs/2103.01312">arxiv:2103.01312</a>
&#x1F4C8; 3 <br>
<p>Pierre Menard, Omar Darwiche Domingues, Xuedong Shang, Michal Valko</p></summary>
<p>

**Abstract:** We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm for reinforcement learning in tabular and possibly stage-dependent, episodic Markov decision process. UCBMQ is based on Q-learning where we add a momentum term and rely on the principle of optimism in face of uncertainty to deal with exploration. Our new technical ingredient of UCBMQ is the use of momentum to correct the bias that Q-learning suffers while, at the same time, limiting the impact it has on the second-order term of the regret. For UCBMQ , we are able to guarantee a regret of at most $O(\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the length of an episode, $S$ the number of states, $A$ the number of actions, $T$ the number of episodes and ignoring terms in poly$log(SAHT)$. Notably, UCBMQ is the first algorithm that simultaneously matches the lower bound of $Î©(\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with respect to the horizon $T$) that scales only linearly with the number of states $S$.

</p>
</details>

<details><summary><b>Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers</b>
<a href="https://arxiv.org/abs/2103.01208">arxiv:2103.01208</a>
&#x1F4C8; 3 <br>
<p>Francesco Croce, Matthias Hein</p></summary>
<p>

**Abstract:** We show that when taking into account also the image domain $[0,1]^d$, established $l_1$-projected gradient descent (PGD) attacks are suboptimal as they do not consider that the effective threat model is the intersection of the $l_1$-ball and $[0,1]^d$. We study the expected sparsity of the steepest descent step for this effective threat model and show that the exact projection onto this set is computationally feasible and yields better performance. Moreover, we propose an adaptive form of PGD which is highly effective even with a small budget of iterations. Our resulting $l_1$-APGD is a strong white-box attack showing that prior works overestimated their $l_1$-robustness. Using $l_1$-APGD for adversarial training we get a robust classifier with SOTA $l_1$-robustness. Finally, we combine $l_1$-APGD and an adaptation of the Square Attack to $l_1$ into $l_1$-AutoAttack, an ensemble of attacks which reliably assesses adversarial robustness for the threat model of $l_1$-ball intersected with $[0,1]^d$.

</p>
</details>

<details><summary><b>Explainable AI in Credit Risk Management</b>
<a href="https://arxiv.org/abs/2103.00949">arxiv:2103.00949</a>
&#x1F4C8; 3 <br>
<p>Branka Hadji Misheva, Joerg Osterrieder, Ali Hirsa, Onkar Kulkarni, Stephen Fung Lin</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) has created the single biggest technology revolution the world has ever seen. For the finance sector, it provides great opportunities to enhance customer experience, democratize financial services, ensure consumer protection and significantly improve risk management. While it is easier than ever to run state-of-the-art machine learning models, designing and implementing systems that support real-world finance applications have been challenging. In large part because they lack transparency and explainability which are important factors in establishing reliable technology and the research on this topic with a specific focus on applications in credit risk management. In this paper, we implement two advanced post-hoc model agnostic explainability techniques called Local Interpretable Model Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) to machine learning (ML)-based credit scoring models applied to the open-access data set offered by the US-based P2P Lending Platform, Lending Club. Specifically, we use LIME to explain instances locally and SHAP to get both local and global explanations. We discuss the results in detail and present multiple comparison scenarios by using various kernels available for explaining graphs generated using SHAP values. We also discuss the practical challenges associated with the implementation of these state-of-art eXplainabale AI (XAI) methods and document them for future reference. We have made an effort to document every technical aspect of this research, while at the same time providing a general summary of the conclusions.

</p>
</details>

<details><summary><b>Panoramic Panoptic Segmentation: Towards Complete Surrounding Understanding via Unsupervised Contrastive Learning</b>
<a href="https://arxiv.org/abs/2103.00868">arxiv:2103.00868</a>
&#x1F4C8; 3 <br>
<p>Alexander Jaus, Kailun Yang, Rainer Stiefelhagen</p></summary>
<p>

**Abstract:** In this work, we introduce panoramic panoptic segmentation as the most holistic scene understanding both in terms of field of view and image level understanding for standard camera based input. A complete surrounding understanding provides a maximum of information to the agent, which is essential for any intelligent vehicle in order to make informed decisions in a safety-critical dynamic environment such as real-world traffic. In order to overcome the lack of annotated panoramic images, we propose a framework which allows model training on standard pinhole images and transfers the learned features to a different domain. Using our proposed method, we manage to achieve significant improvements of over 5% measured in PQ over non-adapted models on our Wild Panoramic Panoptic Segmentation (WildPPS) dataset. We show that our proposed Panoramic Robust Feature (PRF) framework is not only suitable to improve performance on panoramic images but can be beneficial whenever model training and deployment are executed on data taken from different distributions. As an additional contribution, we publish WildPPS: The first panoramic panoptic image dataset to foster progress in surrounding perception.

</p>
</details>

<details><summary><b>CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation</b>
<a href="https://arxiv.org/abs/2103.00852">arxiv:2103.00852</a>
&#x1F4C8; 3 <br>
<p>Aly Magassouba, Komei Sugiura, Hisashi Kawai</p></summary>
<p>

**Abstract:** Navigation guided by natural language instructions is particularly suitable for Domestic Service Robots that interacts naturally with users. This task involves the prediction of a sequence of actions that leads to a specified destination given a natural language navigation instruction. The task thus requires the understanding of instructions, such as ``Walk out of the bathroom and wait on the stairs that are on the right''. The Visual and Language Navigation remains challenging, notably because it requires the exploration of the environment and at the accurate following of a path specified by the instructions to model the relationship between language and vision. To address this, we propose the CrossMap Transformer network, which encodes the linguistic and visual features to sequentially generate a path. The CrossMap transformer is tied to a Transformer-based speaker that generates navigation instructions. The two networks share common latent features, for mutual enhancement through a double back translation model: Generated paths are translated into instructions while generated instructions are translated into path The experimental results show the benefits of our approach in terms of instruction understanding and instruction generation.

</p>
</details>

<details><summary><b>Sandglasset: A Light Multi-Granularity Self-attentive Network For Time-Domain Speech Separation</b>
<a href="https://arxiv.org/abs/2103.00819">arxiv:2103.00819</a>
&#x1F4C8; 3 <br>
<p>Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu</p></summary>
<p>

**Abstract:** One of the leading single-channel speech separation (SS) models is based on a TasNet with a dual-path segmentation technique, where the size of each segment remains unchanged throughout all layers. In contrast, our key finding is that multi-granularity features are essential for enhancing contextual modeling and computational efficiency. We introduce a self-attentive network with a novel sandglass-shape, namely Sandglasset, which advances the state-of-the-art (SOTA) SS performance at significantly smaller model size and computational cost. Forward along each block inside Sandglasset, the temporal granularity of the features gradually becomes coarser until reaching half of the network blocks, and then successively turns finer towards the raw signal level. We also unfold that residual connections between features with the same granularity are critical for preserving information after passing through the bottleneck layer. Experiments show our Sandglasset with only 2.3M parameters has achieved the best results on two benchmark SS datasets -- WSJ0-2mix and WSJ0-3mix, where the SI-SNRi scores have been improved by absolute 0.8 dB and 2.4 dB, respectively, comparing to the prior SOTA results.

</p>
</details>

<details><summary><b>Contrastive Separative Coding for Self-supervised Representation Learning</b>
<a href="https://arxiv.org/abs/2103.00816">arxiv:2103.00816</a>
&#x1F4C8; 3 <br>
<p>Jun Wang, Max W. Y. Lam, Dan Su, Dong Yu</p></summary>
<p>

**Abstract:** To extract robust deep representations from long sequential modeling of speech data, we propose a self-supervised learning approach, namely Contrastive Separative Coding (CSC). Our key finding is to learn such representations by separating the target signal from contrastive interfering signals. First, a multi-task separative encoder is built to extract shared separable and discriminative embedding; secondly, we propose a powerful cross-attention mechanism performed over speaker representations across various interfering conditions, allowing the model to focus on and globally aggregate the most critical information to answer the "query" (current bottom-up embedding) while paying less attention to interfering, noisy, or irrelevant parts; lastly, we form a new probabilistic contrastive loss which estimates and maximizes the mutual information between the representations and the global speaker vector. While most prior unsupervised methods have focused on predicting the future, neighboring, or missing samples, we take a different perspective of predicting the interfered samples. Moreover, our contrastive separative loss is free from negative sampling. The experiment demonstrates that our approach can learn useful representations achieving a strong speaker verification performance in adverse conditions.

</p>
</details>

<details><summary><b>Towards Unbiased COVID-19 Lesion Localisation and Segmentation via Weakly Supervised Learning</b>
<a href="https://arxiv.org/abs/2103.00780">arxiv:2103.00780</a>
&#x1F4C8; 3 <br>
<p>Yang Yang, Jiancong Chen, Ruixuan Wang, Ting Ma, Lingwei Wang, Jie Chen, Wei-Shi Zheng, Tong Zhang</p></summary>
<p>

**Abstract:** Despite tremendous efforts, it is very challenging to generate a robust model to assist in the accurate quantification assessment of COVID-19 on chest CT images. Due to the nature of blurred boundaries, the supervised segmentation methods usually suffer from annotation biases. To support unbiased lesion localisation and to minimise the labeling costs, we propose a data-driven framework supervised by only image-level labels. The framework can explicitly separate potential lesions from original images, with the help of a generative adversarial network and a lesion-specific decoder. Experiments on two COVID-19 datasets demonstrate the effectiveness of the proposed framework and its superior performance to several existing methods.

</p>
</details>

<details><summary><b>Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines</b>
<a href="https://arxiv.org/abs/2106.07719">arxiv:2106.07719</a>
&#x1F4C8; 2 <br>
<p>Mahdi Hajiaghayi, Monir Hajiaghayi, Mark Bolin</p></summary>
<p>

**Abstract:** In this paper, we present a multi-lingual sentence encoder that can be used in search engines as a query and document encoder. This embedding enables a semantic similarity score between queries and documents that can be an important feature in document ranking and relevancy. To train such a customized sentence encoder, it is beneficial to leverage users search data in the form of query-document clicked pairs however, we must avoid relying too much on search click data as it is biased and does not cover many unseen cases. The search data is heavily skewed towards short queries and for long queries is small and often noisy. The goal is to design a universal multi-lingual encoder that works for all cases and covers both short and long queries. We select a number of public NLI datasets in different languages and translation data and together with user search data we train a language model using a multi-task approach. A challenge is that these datasets are not homogeneous in terms of content, size and the balance ratio. While the public NLI datasets are usually two-sentence based with the same portion of positive and negative pairs, the user search data can contain multi-sentence documents and only positive pairs. We show how multi-task training enables us to leverage all these datasets and exploit knowledge sharing across these tasks.

</p>
</details>

<details><summary><b>Significance tests of feature relevance for a blackbox learner</b>
<a href="https://arxiv.org/abs/2103.04985">arxiv:2103.04985</a>
&#x1F4C8; 2 <br>
<p>Ben Dai, Xiaotong Shen, Wei Pan</p></summary>
<p>

**Abstract:** An exciting recent development is the uptake of deep learning in many scientific fields, where the objective is seeking novel scientific insights and discoveries. To interpret a learning outcome, researchers perform hypothesis testing for explainable features to advance scientific domain knowledge. In such a situation, testing for a blackbox learner poses a severe challenge because of intractable models, unknown limiting distributions of parameter estimates, and high computational constraints. In this article, we derive two consistent tests for the feature relevance of a blackbox learner. The first one evaluates a loss difference with perturbation on an inference sample, which is independent of an estimation sample used for parameter estimation in model fitting. The second further splits the inference sample into two but does not require data perturbation. Also, we develop their combined versions by aggregating the order statistics of the $p$-values based on repeated sample splitting. To estimate the splitting ratio and the perturbation size, we develop adaptive splitting schemes for suitably controlling the Type \rom{1} error subject to computational constraints. By deflating the \textit{bias-sd-ratio}, we establish asymptotic null distributions of the test statistics and their consistency in terms of statistical power. Our theoretical power analysis and simulations indicate that the one-split test is more powerful than the two-split test, though the latter is easier to apply for large datasets. Moreover, the combined tests are more stable while compensating for a power loss by repeated sample splitting. Numerically, we demonstrate the utility of the proposed tests on two benchmark examples. Accompanying this paper is our Python library {\tt dnn-inference} https://dnn-inference.readthedocs.io/en/latest/ that implements the proposed tests.

</p>
</details>

<details><summary><b>Safe Learning of Uncertain Environments</b>
<a href="https://arxiv.org/abs/2103.01413">arxiv:2103.01413</a>
&#x1F4C8; 2 <br>
<p>Farhad Farokhi, Alex Leong, Iman Shames, Mohammad Zamani</p></summary>
<p>

**Abstract:** In many learning based control methodologies, learning the unknown dynamic model precedes the control phase, while the aim is to control the system such that it remains in some safe region of the state space. In this work, our aim is to guarantee safety while learning and control proceed simultaneously. Specifically, we consider the problem of safe learning in nonlinear control-affine systems subject to unknown additive uncertainty. We first model the uncertainty as a Gaussian noise and use state measurements to learn its mean and covariance. We provide rigorous time-varying bounds on the mean and covariance of the uncertainty and employ them to modify the control input via an optimization program with potentially time-varying safety constraints. We show that with an arbitrarily large probability we can guarantee that the state will remain in the safe set, while learning and control are carried out simultaneously, provided that a feasible solution exists for the optimization problem. We provide a secondary formulation of this optimization that is computationally more efficient. This is based on tightening the safety constraints to counter the uncertainty about the learned mean and covariance. The magnitude of the tightening can be decreased as our confidence in the learned mean and covariance increases (i.e., as we gather more measurements about the environment). Extensions of the method are provided for non-Gaussian process noise with unknown mean and covariance as well as Gaussian uncertainties with state-dependent mean and covariance to accommodate more general environments.

</p>
</details>

<details><summary><b>Practical Privacy Filters and Odometers with RÃ©nyi Differential Privacy and Applications to Differentially Private Deep Learning</b>
<a href="https://arxiv.org/abs/2103.01379">arxiv:2103.01379</a>
&#x1F4C8; 2 <br>
<p>Mathias LÃ©cuyer</p></summary>
<p>

**Abstract:** Differential Privacy (DP) is the leading approach to privacy preserving deep learning. As such, there are multiple efforts to provide drop-in integration of DP into popular frameworks. These efforts, which add noise to each gradient computation to make it DP, rely on composition theorems to bound the total privacy loss incurred over this sequence of DP computations.
  However, existing composition theorems present a tension between efficiency and flexibility. Most theorems require all computations in the sequence to have a predefined DP parameter, called the privacy budget. This prevents the design of training algorithms that adapt the privacy budget on the fly, or that terminate early to reduce the total privacy loss. Alternatively, the few existing composition results for adaptive privacy budgets provide complex bounds on the privacy loss, with constants too large to be practical.
  In this paper, we study DP composition under adaptive privacy budgets through the lens of RÃ©nyi Differential Privacy, proving a simpler composition theorem with smaller constants, making it practical enough to use in algorithm design. We demonstrate two applications of this theorem for DP deep learning: adapting the noise or batch size online to improve a model's accuracy within a fixed total privacy loss, and stopping early when fine-tuning a model to reduce total privacy loss.

</p>
</details>

<details><summary><b>Robust 3D U-Net Segmentation of Macular Holes</b>
<a href="https://arxiv.org/abs/2103.01299">arxiv:2103.01299</a>
&#x1F4C8; 2 <br>
<p>Jonathan Frawley, Chris G. Willcocks, Maged Habib, Caspar Geenen, David H. Steel, Boguslaw Obara</p></summary>
<p>

**Abstract:** Macular holes are a common eye condition which result in visual impairment. We look at the application of deep convolutional neural networks to the problem of macular hole segmentation. We use the 3D U-Net architecture as a basis and experiment with a number of design variants. Manually annotating and measuring macular holes is time consuming and error prone. Previous automated approaches to macular hole segmentation take minutes to segment a single 3D scan. Our proposed model generates significantly more accurate segmentations in less than a second. We found that an approach of architectural simplification, by greatly simplifying the network capacity and depth, exceeds both expert performance and state-of-the-art models such as residual 3D U-Nets.

</p>
</details>

<details><summary><b>Generative Particle Variational Inference via Estimation of Functional Gradients</b>
<a href="https://arxiv.org/abs/2103.01291">arxiv:2103.01291</a>
&#x1F4C8; 2 <br>
<p>Neale Ratzlaff, Qinxun Bai, Li Fuxin, Wei Xu</p></summary>
<p>

**Abstract:** Recently, particle-based variational inference (ParVI) methods have gained interest because they can avoid arbitrary parametric assumptions that are common in variational inference. However, many ParVI approaches do not allow arbitrary sampling from the posterior, and the few that do allow such sampling suffer from suboptimality. This work proposes a new method for learning to approximately sample from the posterior distribution. We construct a neural sampler that is trained with the functional gradient of the KL-divergence between the empirical sampling distribution and the target distribution, assuming the gradient resides within a reproducing kernel Hilbert space. Our generative ParVI (GPVI) approach maintains the asymptotic performance of ParVI methods while offering the flexibility of a generative sampler. Through carefully constructed experiments, we show that GPVI outperforms previous generative ParVI methods such as amortized SVGD, and is competitive with ParVI as well as gold-standard approaches like Hamiltonian Monte Carlo for fitting both exactly known and intractable target distributions.

</p>
</details>

<details><summary><b>Performance Variability in Zero-Shot Classification</b>
<a href="https://arxiv.org/abs/2103.01284">arxiv:2103.01284</a>
&#x1F4C8; 2 <br>
<p>MatÃ­as Molina, Jorge SÃ¡nchez</p></summary>
<p>

**Abstract:** Zero-shot classification (ZSC) is the task of learning predictors for classes not seen during training. Although the different methods in the literature are evaluated using the same class splits, little is known about their stability under different class partitions. In this work we show experimentally that ZSC performance exhibits strong variability under changing training setups. We propose the use ensemble learning as an attempt to mitigate this phenomena.

</p>
</details>

<details><summary><b>Non-Euclidean Differentially Private Stochastic Convex Optimization</b>
<a href="https://arxiv.org/abs/2103.01278">arxiv:2103.01278</a>
&#x1F4C8; 2 <br>
<p>Raef Bassily, CristÃ³bal GuzmÃ¡n, Anupama Nandi</p></summary>
<p>

**Abstract:** Differentially private (DP) stochastic convex optimization (SCO) is a fundamental problem, where the goal is to approximately minimize the population risk with respect to a convex loss function, given a dataset of i.i.d. samples from a distribution, while satisfying differential privacy with respect to the dataset. Most of the existing works in the literature of private convex optimization focus on the Euclidean (i.e., $\ell_2$) setting, where the loss is assumed to be Lipschitz (and possibly smooth) w.r.t. the $\ell_2$ norm over a constraint set with bounded $\ell_2$ diameter. Algorithms based on noisy stochastic gradient descent (SGD) are known to attain the optimal excess risk in this setting.
  In this work, we conduct a systematic study of DP-SCO for $\ell_p$-setups. For $p=1$, under a standard smoothness assumption, we give a new algorithm with nearly optimal excess risk. This result also extends to general polyhedral norms and feasible sets. For $p\in(1, 2)$, we give two new algorithms, whose central building block is a novel privacy mechanism, which generalizes the Gaussian mechanism. Moreover, we establish a lower bound on the excess risk for this range of $p$, showing a necessary dependence on $\sqrt{d}$, where $d$ is the dimension of the space. Our lower bound implies a sudden transition of the excess risk at $p=1$, where the dependence on $d$ changes from logarithmic to polynomial, resolving an open question in prior work [TTZ15] . For $p\in (2, \infty)$, noisy SGD attains optimal excess risk in the low-dimensional regime; in particular, this proves the optimality of noisy SGD for $p=\infty$. Our work draws upon concepts from the geometry of normed spaces, such as the notions of regularity, uniform convexity, and uniform smoothness.

</p>
</details>

<details><summary><b>A Multiclass Boosting Framework for Achieving Fast and Provable Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2103.01276">arxiv:2103.01276</a>
&#x1F4C8; 2 <br>
<p>Jacob Abernethy, Pranjal Awasthi, Satyen Kale</p></summary>
<p>

**Abstract:** Alongside the well-publicized accomplishments of deep neural networks there has emerged an apparent bug in their success on tasks such as object recognition: with deep models trained using vanilla methods, input images can be slightly corrupted in order to modify output predictions, even when these corruptions are practically invisible. This apparent lack of robustness has led researchers to propose methods that can help to prevent an adversary from having such capabilities. The state-of-the-art approaches have incorporated the robustness requirement into the loss function, and the training process involves taking stochastic gradient descent steps not using original inputs but on adversarially-corrupted ones. In this paper we propose a multiclass boosting framework to ensure adversarial robustness. Boosting algorithms are generally well-suited for adversarial scenarios, as they were classically designed to satisfy a minimax guarantee. We provide a theoretical foundation for this methodology and describe conditions under which robustness can be achieved given a weak training oracle. We show empirically that adversarially-robust multiclass boosting not only outperforms the state-of-the-art methods, it does so at a fraction of the training time.

</p>
</details>

<details><summary><b>Unsupervised Classification of Voiced Speech and Pitch Tracking Using Forward-Backward Kalman Filtering</b>
<a href="https://arxiv.org/abs/2103.01173">arxiv:2103.01173</a>
&#x1F4C8; 2 <br>
<p>Benedikt Boenninghoff, Robert M. Nickel, Steffen Zeiler, Dorothea Kolossa</p></summary>
<p>

**Abstract:** The detection of voiced speech, the estimation of the fundamental frequency, and the tracking of pitch values over time are crucial subtasks for a variety of speech processing techniques. Many different algorithms have been developed for each of the three subtasks. We present a new algorithm that integrates the three subtasks into a single procedure. The algorithm can be applied to pre-recorded speech utterances in the presence of considerable amounts of background noise. We combine a collection of standard metrics, such as the zero-crossing rate, for example, to formulate an unsupervised voicing classifier. The estimation of pitch values is accomplished with a hybrid autocorrelation-based technique. We propose a forward-backward Kalman filter to smooth the estimated pitch contour. In experiments, we are able to show that the proposed method compares favorably with current, state-of-the-art pitch detection algorithms.

</p>
</details>

<details><summary><b>Class Means as an Early Exit Decision Mechanism</b>
<a href="https://arxiv.org/abs/2103.01148">arxiv:2103.01148</a>
&#x1F4C8; 2 <br>
<p>Alperen Gormez, Erdem Koyuncu</p></summary>
<p>

**Abstract:** State-of-the-art neural networks with early exit mechanisms often need considerable amount of training and fine-tuning to achieve good performance with low computational cost. We propose a novel early exit technique based on the class means of samples. Unlike most existing schemes, our method does not require gradient-based training of internal classifiers. This makes our method particularly useful for neural network training in low-power devices, as in wireless edge networks. In particular, given a fixed training time budget, our scheme achieves higher accuracy as compared to existing early exit mechanisms. Moreover, if there are no limitations on the training time budget, our method can be combined with an existing early exit scheme to boost its performance, achieving a better trade-off between computational cost and network accuracy.

</p>
</details>

<details><summary><b>Optimal Linear Combination of Classifiers</b>
<a href="https://arxiv.org/abs/2103.01109">arxiv:2103.01109</a>
&#x1F4C8; 2 <br>
<p>Georgi Nalbantov, Svetoslav Ivanov</p></summary>
<p>

**Abstract:** The question of whether to use one classifier or a combination of classifiers is a central topic in Machine Learning. We propose here a method for finding an optimal linear combination of classifiers derived from a bias-variance framework for the classification task.

</p>
</details>

<details><summary><b>Visualizing Rule Sets: Exploration and Validation of a Design Space</b>
<a href="https://arxiv.org/abs/2103.01022">arxiv:2103.01022</a>
&#x1F4C8; 2 <br>
<p>Jun Yuan, Oded Nov, Enrico Bertini</p></summary>
<p>

**Abstract:** Rule sets are often used in Machine Learning (ML) as a way to communicate the model logic in settings where transparency and intelligibility are necessary. Rule sets are typically presented as a text-based list of logical statements (rules). Surprisingly, to date there has been limited work on exploring visual alternatives for presenting rules. In this paper, we explore the idea of designing alternative representations of rules, focusing on a number of visual factors we believe have a positive impact on rule readability and understanding. The paper presents an initial design space for visualizing rule sets and a user study exploring their impact. The results show that some design factors have a strong impact on how efficiently readers can process the rules while having minimal impact on accuracy. This work can help practitioners employ more effective solutions when using rules as a communication strategy to understand ML models.

</p>
</details>

<details><summary><b>Machine learning on small size samples: A synthetic knowledge synthesis</b>
<a href="https://arxiv.org/abs/2103.01002">arxiv:2103.01002</a>
&#x1F4C8; 2 <br>
<p>Peter Kokol, Marko Kokol, SaÅ¡o Zagoranski</p></summary>
<p>

**Abstract:** One of the increasingly important technologies dealing with the growing complexity of the digitalization of almost all human activities is Artificial intelligence, more precisely machine learning Despite the fact, that we live in a Big data world where almost everything is digitally stored, there are many real-world situations, where researchers are faced with small data samples. The present study aim is to answer the following research question namely What is the small data problem in machine learning and how it is solved?. Our bibliometric study showed a positive trend in the number of research publications concerning the use of small datasets and substantial growth of the research community dealing with the small dataset problem, indicating that the research field is moving toward higher maturity levels. Despite notable international cooperation, the regional concentration of research literature production in economically more developed countries was observed.

</p>
</details>

<details><summary><b>Moment-Based Variational Inference for Stochastic Differential Equations</b>
<a href="https://arxiv.org/abs/2103.00988">arxiv:2103.00988</a>
&#x1F4C8; 2 <br>
<p>Christian Wildner, Heinz Koeppl</p></summary>
<p>

**Abstract:** Existing deterministic variational inference approaches for diffusion processes use simple proposals and target the marginal density of the posterior. We construct the variational process as a controlled version of the prior process and approximate the posterior by a set of moment functions. In combination with moment closure, the smoothing problem is reduced to a deterministic optimal control problem. Exploiting the path-wise Fisher information, we propose an optimization procedure that corresponds to a natural gradient descent in the variational parameters. Our approach allows for richer variational approximations that extend to state-dependent diffusion terms. The classical Gaussian process approximation is recovered as a special case.

</p>
</details>

<details><summary><b>Listening to the city, attentively: A Spatio-Temporal Attention Boosted Autoencoder for the Short-Term Flow Prediction Problem</b>
<a href="https://arxiv.org/abs/2103.00983">arxiv:2103.00983</a>
&#x1F4C8; 2 <br>
<p>Stefano Fiorini, Michele Ciavotta, Andrea Maurino</p></summary>
<p>

**Abstract:** In recent years, studying and predicting alternative mobility (e.g., sharing services) patterns in urban environments has become increasingly important as accurate and timely information on current and future vehicle flows can successfully increase the quality and availability of transportation services. This need is aggravated during the current pandemic crisis, which pushes policymakers and private citizens to seek social-distancing compliant urban mobility services, such as electric bikes and scooter sharing offerings. However, predicting the number of incoming and outgoing vehicles for different city areas is challenging due to the nonlinear spatial and temporal dependencies typical of urban mobility patterns. In this work, we propose STREED-Net, a novel deep learning network with a multi-attention (spatial and temporal) mechanism that effectively captures and exploits complex spatial and temporal patterns in mobility data. The results of a thorough experimental analysis using real-life data are reported, indicating that the proposed model improves the state-of-the-art for this task.

</p>
</details>

<details><summary><b>Collaborative Recognition of Feasible Region with Aerial and Ground Robots through DPCN</b>
<a href="https://arxiv.org/abs/2103.00947">arxiv:2103.00947</a>
&#x1F4C8; 2 <br>
<p>Yunshuang Li, Zheyuan Huang, Zexi chen, Yue Wang, Rong Xiong</p></summary>
<p>

**Abstract:** Ground robots always get collision in that only if they get close to the obstacles, can they sense the danger and take actions, which is usually too late to avoid the crash, causing severe damage to the robots. To address this issue, we present collaboration of aerial and ground robots in recognition of feasible region. Taking the aerial robots' advantages of having large scale variance of view points of the same route which the ground robots is on, the collaboration work provides global information of road segmentation for the ground robot, thus enabling it to obtain feasible region and adjust its pose ahead of time. Under normal circumstance, the transformation between these two devices can be obtained by GPS yet with much error, directly causing inferior influence on recognition of feasible region. Thereby, we utilize the state-of-the-art research achievements in matching heterogeneous sensor measurements called deep phase correlation network(DPCN), which has excellent performance on heterogeneous mapping, to refine the transformation. The network is light-weighted and promising for better generalization. We use Aero-Ground dataset which consists of heterogeneous sensor images and aerial road segmentation images. The results show that our collaborative system has great accuracy, speed and stability.

</p>
</details>

<details><summary><b>LADMM-Net: An Unrolled Deep Network For Spectral Image Fusion From Compressive Data</b>
<a href="https://arxiv.org/abs/2103.00940">arxiv:2103.00940</a>
&#x1F4C8; 2 <br>
<p>Juan Marcos RamÃ­rez, JosÃ© Ignacio MartÃ­nez Torre, Henry Arguello Fuentes</p></summary>
<p>

**Abstract:** Image fusion aims at estimating a high-resolution spectral image from a low-spatial-resolution hyperspectral image and a low-spectral-resolution multispectral image. In this regard, compressive spectral imaging (CSI) has emerged as an acquisition framework that captures the relevant information of spectral images using a reduced number of measurements. Recently, various image fusion methods from CSI measurements have been proposed. However, these methods exhibit high running times and face the challenging task of choosing sparsity-inducing bases. In this paper, a deep network under the algorithm unrolling approach is proposed for fusing spectral images from compressive measurements. This architecture, dubbed LADMM-Net, casts each iteration of a linearized version of the alternating direction method of multipliers into a processing layer whose concatenation deploys a deep network. The linearized approach enables obtaining fusion estimates without resorting to costly matrix inversions. Furthermore, this approach exploits the benefits of learnable transforms to estimate the image details included in both the auxiliary variable and the Lagrange multiplier. Finally, the performance of the proposed technique is evaluated on two spectral image databases and one dataset captured at the laboratory. Extensive simulations show that the proposed method outperforms the state-of-the-art approaches that fuse spectral images from compressive measurements.

</p>
</details>

<details><summary><b>Latent linear dynamics in spatiotemporal medical data</b>
<a href="https://arxiv.org/abs/2103.00930">arxiv:2103.00930</a>
&#x1F4C8; 2 <br>
<p>Niklas Gunnarsson, Jens SjÃ¶lund, Thomas B. SchÃ¶n</p></summary>
<p>

**Abstract:** Spatiotemporal imaging is common in medical imaging, with applications in e.g. cardiac diagnostics, surgical guidance and radiotherapy monitoring. In this paper, we present an unsupervised model that identifies the underlying dynamics of the system, only based on the sequential images. The model maps the input to a low-dimensional latent space wherein a linear relationship holds between a hidden state process and the observed latent process. Knowledge of the system dynamics enables denoising, imputation of missing values and extrapolation of future image frames. We use a Variational Auto-Encoder (VAE) for the dimensionality reduction and a Linear Gaussian State Space Model (LGSSM) for the latent dynamics. The model, known as a Kalman Variational Auto-Encoder, is end-to-end trainable and the weights, both in the VAE and LGSSM, are simultaneously updated by maximizing the evidence lower bound of the marginal log likelihood. Our experiment, on cardiac ultrasound time series, shows that the dynamical model provide better reconstructions than a similar model without dynamics. And also possibility to impute and extrapolate for missing samples.

</p>
</details>

<details><summary><b>STUDD: A Student-Teacher Method for Unsupervised Concept Drift Detection</b>
<a href="https://arxiv.org/abs/2103.00903">arxiv:2103.00903</a>
&#x1F4C8; 2 <br>
<p>Vitor Cerqueira, Heitor Murilo Gomes, Albert Bifet, Luis Torgo</p></summary>
<p>

**Abstract:** Concept drift detection is a crucial task in data stream evolving environments. Most of state of the art approaches designed to tackle this problem monitor the loss of predictive models. However, this approach falls short in many real-world scenarios, where the true labels are not readily available to compute the loss. In this context, there is increasing attention to approaches that perform concept drift detection in an unsupervised manner, i.e., without access to the true labels. We propose a novel approach to unsupervised concept drift detection based on a student-teacher learning paradigm. Essentially, we create an auxiliary model (student) to mimic the behaviour of the primary model (teacher). At run-time, our approach is to use the teacher for predicting new instances and monitoring the mimicking loss of the student for concept drift detection. In a set of experiments using 19 data streams, we show that the proposed approach can detect concept drift and present a competitive behaviour relative to the state of the art approaches.

</p>
</details>

<details><summary><b>A Bioinspired Approach-Sensitive Neural Network for Collision Detection in Cluttered and Dynamic Backgrounds</b>
<a href="https://arxiv.org/abs/2103.00857">arxiv:2103.00857</a>
&#x1F4C8; 2 <br>
<p>Xiao Huang, Hong Qiao, Hui Li, Zhihong Jiang</p></summary>
<p>

**Abstract:** Rapid, accurate and robust detection of looming objects in cluttered moving backgrounds is a significant and challenging problem for robotic visual systems to perform collision detection and avoidance tasks. Inspired by the neural circuit of elementary motion vision in the mammalian retina, this paper proposes a bioinspired approach-sensitive neural network (ASNN) that contains three main contributions. Firstly, a direction-selective visual processing module is built based on the spatiotemporal energy framework, which can estimate motion direction accurately via only two mutually perpendicular spatiotemporal filtering channels. Secondly, a novel approach-sensitive neural network is modeled as a push-pull structure formed by ON and OFF pathways, which responds strongly to approaching motion while insensitivity to lateral motion. Finally, a method of directionally selective inhibition is introduced, which is able to suppress the translational backgrounds effectively. Extensive synthetic and real robotic experiments show that the proposed model is able to not only detect collision accurately and robustly in cluttered and dynamic backgrounds but also extract more collision information like position and direction, for guiding rapid decision making.

</p>
</details>

<details><summary><b>A Brief Summary of Interactions Between Meta-Learning and Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2103.00845">arxiv:2103.00845</a>
&#x1F4C8; 2 <br>
<p>Huimin Peng</p></summary>
<p>

**Abstract:** This paper briefly reviews the connections between meta-learning and self-supervised learning. Meta-learning can be applied to improve model generalization capability and to construct general AI algorithms. Self-supervised learning utilizes self-supervision from original data and extracts higher-level generalizable features through unsupervised pre-training or optimization of contrastive loss objectives. In self-supervised learning, data augmentation techniques are widely applied and data labels are not required since pseudo labels can be estimated from trained models on similar tasks. Meta-learning aims to adapt trained deep models to solve diverse tasks and to develop general AI algorithms. We review the associations of meta-learning with both generative and contrastive self-supervised learning models. Unlabeled data from multiple sources can be jointly considered even when data sources are vastly different. We show that an integration of meta-learning and self-supervised learning models can best contribute to the improvement of model generalization capability. Self-supervised learning guided by meta-learner and general meta-learning algorithms under self-supervision are both examples of possible combinations.

</p>
</details>

<details><summary><b>GEBT: Drawing Early-Bird Tickets in Graph Convolutional Network Training</b>
<a href="https://arxiv.org/abs/2103.00794">arxiv:2103.00794</a>
&#x1F4C8; 2 <br>
<p>Haoran You, Zhihan Lu, Zijian Zhou, Yingyan Lin</p></summary>
<p>

**Abstract:** Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. However, it remains notoriously challenging to train and inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because as the graph size grows, the sheer number of node features and the large adjacency matrix can easily explode the required memory and data movements. To tackle the aforementioned challenge, we explore the possibility of drawing lottery tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency matrix yet are capable of achieving accuracy comparable to or even better than their corresponding full graphs. Specifically, we for the first time discover the existence of graph early-bird (GEB) tickets that emerge at the very early stage when sparsifying GCN graphs, and propose a simple yet effective detector to automatically identify the emergence of such GEB tickets. Furthermore, we develop a generic efficient GCN training framework dubbed GEBT that can significantly boost the efficiency of GCN training by (1) drawing joint early-bird tickets between the GCN graphs and models and (2) enabling simultaneously sparsifying both GCN graphs and models, paving the way for training and inferencing large GCN graphs to handle real-world graph datasets. Experiments on various GCN models and datasets consistently validate our GEB finding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2% ~ 85.6% and 84.6% ~ 87.5% savings of GCN training and inference costs while leading to a comparable or even better accuracy as compared to state-of-the-art methods. Code available at https://github.com/RICE-EIC/GEBT

</p>
</details>

<details><summary><b>Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network</b>
<a href="https://arxiv.org/abs/2103.00793">arxiv:2103.00793</a>
&#x1F4C8; 2 <br>
<p>Qi Zhao, Shuchang Lyu, Zhiwei Zhang, Ting-Bing Xu, Guangliang Cheng</p></summary>
<p>

**Abstract:** In real applications, different computation-resource devices need different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually, existing methods either design multiple networks and train them independently, or construct depth-level/width-level dynamic neural networks which is hard to prove the accuracy of each sub-net. In this article, we propose an elegant Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets of similar architectures. To improve the generalization of sub-nets, we design the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to implement knowledge transfer from the teacher (full-net) to multiple students (sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to constrain the posterior class probability consistency between full-net and sub-nets, and self-attention distillation on the same resolution feature of different depth is addressed to drive more abundant feature representations of sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in a DDNN via the online knowledge distillation in each training iteration without extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet datasets demonstrate that sub-nets in DDNN with EKD training achieve better performance than individually training networks while preserving the original performance of full-nets.

</p>
</details>

<details><summary><b>RAGA: Relation-aware Graph Attention Networks for Global Entity Alignment</b>
<a href="https://arxiv.org/abs/2103.00791">arxiv:2103.00791</a>
&#x1F4C8; 2 <br>
<p>Renbo Zhu, Meng Ma, Ping Wang</p></summary>
<p>

**Abstract:** Entity alignment (EA) is the task to discover entities referring to the same real-world object from different knowledge graphs (KGs), which is the most crucial step in integrating multi-source KGs. The majority of the existing embeddings-based entity alignment methods embed entities and relations into a vector space based on relation triples of KGs for local alignment. As these methods insufficiently consider the multiple relations between entities, the structure information of KGs has not been fully leveraged. In this paper, we propose a novel framework based on Relation-aware Graph Attention Networks to capture the interactions between entities and relations. Our framework adopts the self-attention mechanism to spread entity information to the relations and then aggregate relation information back to entities. Furthermore, we propose a global alignment algorithm to make one-to-one entity alignments with a fine-grained similarity matrix. Experiments on three real-world cross-lingual datasets show that our framework outperforms the state-of-the-art methods.

</p>
</details>

<details><summary><b>Self-Supervised Multi-View Learning via Auto-Encoding 3D Transformations</b>
<a href="https://arxiv.org/abs/2103.00787">arxiv:2103.00787</a>
&#x1F4C8; 2 <br>
<p>Xiang Gao, Wei Hu, Guo-Jun Qi</p></summary>
<p>

**Abstract:** 3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learn multi-view feature representations without involving labeled data. To this end, we propose a novel self-supervised paradigm to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views. Specifically, we perform a 3D transformation on a 3D object, and obtain multiple views before and after the transformation via projection. Then, we self-train a representation to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposed MV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks, and show the generalization to real-world datasets.

</p>
</details>

<details><summary><b>A Machine Learning Approach for Predicting Human Preference for Graph Layouts</b>
<a href="https://arxiv.org/abs/2103.03665">arxiv:2103.03665</a>
&#x1F4C8; 1 <br>
<p>Shijun Cai, Seok-Hee Hong, Jialiang Shen, Tongliang Liu</p></summary>
<p>

**Abstract:** Understanding what graph layout human prefer and why they prefer is significant and challenging due to the highly complex visual perception and cognition system in human brain. In this paper, we present the first machine learning approach for predicting human preference for graph layouts.
  In general, the data sets with human preference labels are limited and insufficient for training deep networks. To address this, we train our deep learning model by employing the transfer learning method, e.g., exploiting the quality metrics, such as shape-based metrics, edge crossing and stress, which are shown to be correlated to human preference on graph layouts. Experimental results using the ground truth human preference data sets show that our model can successfully predict human preference for graph layouts. To our best knowledge, this is the first approach for predicting qualitative evaluation of graph layouts using human preference experiment data.

</p>
</details>

<details><summary><b>Adaptive Transmission Scheduling in Wireless Networks for Asynchronous Federated Learning</b>
<a href="https://arxiv.org/abs/2103.01422">arxiv:2103.01422</a>
&#x1F4C8; 1 <br>
<p>Hyun-Suk Lee, Jang-Won Lee</p></summary>
<p>

**Abstract:** In this paper, we study asynchronous federated learning (FL) in a wireless distributed learning network (WDLN). To allow each edge device to use its local data more efficiently via asynchronous FL, transmission scheduling in the WDLN for asynchronous FL should be carefully determined considering system uncertainties, such as time-varying channel and stochastic data arrivals, and the scarce radio resources in the WDLN. To address this, we propose a metric, called an effectivity score, which represents the amount of learning from asynchronous FL. We then formulate an Asynchronous Learning-aware transmission Scheduling (ALS) problem to maximize the effectivity score and develop three ALS algorithms, called ALSA-PI, BALSA, and BALSA-PO, to solve it. If the statistical information about the uncertainties is known, the problem can be optimally and efficiently solved by ALSA-PI. Even if not, it can be still optimally solved by BALSA that learns the uncertainties based on a Bayesian approach using the state information reported from devices. BALSA-PO suboptimally solves the problem, but it addresses a more restrictive WDLN in practice, where the AP can observe a limited state information compared with the information used in BALSA. We show via simulations that the models trained by our ALS algorithms achieve performances close to that by an ideal benchmark and outperform those by other state-of-the-art baseline scheduling algorithms in terms of model accuracy, training loss, learning speed, and robustness of learning. These results demonstrate that the adaptive scheduling strategy in our ALS algorithms is effective to asynchronous FL.

</p>
</details>

<details><summary><b>Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image</b>
<a href="https://arxiv.org/abs/2103.01263">arxiv:2103.01263</a>
&#x1F4C8; 1 <br>
<p>Alon Mamistvalov, Yonina C. Eldar</p></summary>
<p>

**Abstract:** The most common technique for generating B-mode ultrasound (US) images is delay and sum (DAS) beamforming, where the signals received at the transducer array are sampled before an appropriate delay is applied. This necessitates sampling rates exceeding the Nyquist rate and the use of a large number of antenna elements to ensure sufficient image quality. Recently we proposed methods to reduce the sampling rate and the array size relying on image recovery using iterative algorithms, based on compressed sensing (CS) and the finite rate of innovation (FRI) frameworks. Iterative algorithms typically require a large number of iterations, making them difficult to use in real-time. Here, we propose a reconstruction method from sub-Nyquist samples in the time and spatial domain, that is based on unfolding the ISTA algorithm, resulting in an efficient and interpretable deep network. The inputs to our network are the subsampled beamformed signals after summation and delay in the frequency domain, requiring only a subset of the US signal to be stored for recovery. Our method allows reducing the number of array elements, sampling rate, and computational time while ensuring high quality imaging performance. Using \emph{in vivo} data we demonstrate that the proposed method yields high-quality images while reducing the data volume traditionally used up to 36 times. In terms of image resolution and contrast, our technique outperforms previously suggested methods as well as DAS and minimum-variance (MV) beamforming, paving the way to real-time applicable recovery methods.

</p>
</details>

<details><summary><b>Offshore Software Maintenance Outsourcing Predicting Clients Proposal using Supervised Learning</b>
<a href="https://arxiv.org/abs/2103.01223">arxiv:2103.01223</a>
&#x1F4C8; 1 <br>
<p>Atif Ikram, Masita Abdul Jalil, Amir Bin Ngah, Ahmad Salman Khan, Tahir Iqbal</p></summary>
<p>

**Abstract:** In software engineering, software maintenance is the process of correction, updating, and improvement of software products after handed over to the customer. Through offshore software maintenance outsourcing clients can get advantages like reduce cost, save time, and improve quality. In most cases, the OSMO vendor generates considerable revenue. However, the selection of an appropriate proposal among multiple clients is one of the critical problems for OSMO vendors. The purpose of this paper is to suggest an effective machine learning technique that can be used by OSMO vendors to assess or predict the OSMO client proposal. The dataset is generated through a survey of OSMO vendors working in a developing country. The results showed that supervised learning-based classifiers like NaÃ¯ve Bayesian, SMO, Logistics apprehended 69.75, 81.81, and 87.27 percent testing accuracy respectively. This study concludes that supervised learning is the most suitable technique to predict the OSMO client's proposal.

</p>
</details>

<details><summary><b>Gradient Coding with Dynamic Clustering for Straggler-Tolerant Distributed Learning</b>
<a href="https://arxiv.org/abs/2103.01206">arxiv:2103.01206</a>
&#x1F4C8; 1 <br>
<p>Baturalp Buyukates, Emre Ozfatura, Sennur Ulukus, Deniz Gunduz</p></summary>
<p>

**Abstract:** Distributed implementations are crucial in speeding up large scale machine learning applications. Distributed gradient descent (GD) is widely employed to parallelize the learning task by distributing the dataset across multiple workers. A significant performance bottleneck for the per-iteration completion time in distributed synchronous GD is $straggling$ workers. Coded distributed computation techniques have been introduced recently to mitigate stragglers and to speed up GD iterations by assigning redundant computations to workers. In this paper, we consider gradient coding (GC), and propose a novel dynamic GC scheme, which assigns redundant data to workers to acquire the flexibility to dynamically choose from among a set of possible codes depending on the past straggling behavior. In particular, we consider GC with clustering, and regulate the number of stragglers in each cluster by dynamically forming the clusters at each iteration; hence, the proposed scheme is called $GC$ $with$ $dynamic$ $clustering$ (GC-DC). Under a time-correlated straggling behavior, GC-DC gains from adapting to the straggling behavior over time such that, at each iteration, GC-DC aims at distributing the stragglers across clusters as uniformly as possible based on the past straggler behavior. For both homogeneous and heterogeneous worker models, we numerically show that GC-DC provides significant improvements in the average per-iteration completion time without an increase in the communication load compared to the original GC scheme.

</p>
</details>

<details><summary><b>Noncoding RNAs and deep learning neural network discriminate multi-cancer types</b>
<a href="https://arxiv.org/abs/2103.01179">arxiv:2103.01179</a>
&#x1F4C8; 1 <br>
<p>Anyou Wang, Rong Hai, Paul J Rider, Qianchuan He</p></summary>
<p>

**Abstract:** Detecting cancers at early stages can dramatically reduce mortality rates. Therefore, practical cancer screening at the population level is needed. Here, we develop a comprehensive detection system to classify all common cancer types. By integrating artificial intelligence deep learning neural network and noncoding RNA biomarkers selected from massive data, our system can accurately detect cancer vs healthy object with 96.3% of AUC of ROC (Area Under Curve of a Receiver Operating Characteristic curve). Intriguinely, with no more than 6 biomarkers, our approach can easily discriminate any individual cancer type vs normal with 99% to 100% AUC. Furthermore, a comprehensive marker panel can simultaneously multi-classify all common cancers with a stable 78% of accuracy at heterological cancerous tissues and conditions. This provides a valuable framework for large scale cancer screening. The AI models and plots of results were available in https://combai.org/ai/cancerdetection/

</p>
</details>

<details><summary><b>Deep Learning with a Classifier System: Initial Results</b>
<a href="https://arxiv.org/abs/2103.01118">arxiv:2103.01118</a>
&#x1F4C8; 1 <br>
<p>Richard J. Preen, Larry Bull</p></summary>
<p>

**Abstract:** This article presents the first results from using a learning classifier system capable of performing adaptive computation with deep neural networks. Individual classifiers within the population are composed of two neural networks. The first acts as a gating or guarding component, which enables the conditional computation of an associated deep neural network on a per instance basis. Self-adaptive mutation is applied upon reproduction and prediction networks are refined with stochastic gradient descent during lifetime learning. The use of fully-connected and convolutional layers are evaluated on handwritten digit recognition tasks where evolution adapts (i) the gradient descent learning rate applied to each layer (ii) the number of units within each layer, i.e., the number of fully-connected neurons and the number of convolutional kernel filters (iii) the connectivity of each layer, i.e., whether each weight is active (iv) the weight magnitudes, enabling escape from local optima. The system automatically reduces the number of weights and units while maintaining performance after achieving a maximum prediction error.

</p>
</details>

<details><summary><b>Blockchain-Based Federated Learning in Mobile Edge Networks with Application in Internet of Vehicles</b>
<a href="https://arxiv.org/abs/2103.01116">arxiv:2103.01116</a>
&#x1F4C8; 1 <br>
<p>Rui Wang, Heju Li, Erwu Liu</p></summary>
<p>

**Abstract:** The rapid increase of the data scale in Internet of Vehicles (IoV) system paradigm, hews out new possibilities in boosting the service quality for the emerging applications through data sharing. Nevertheless, privacy concerns are major bottlenecks for data providers to share private data in traditional IoV networks. To this end, federated learning (FL) as an emerging learning paradigm, where data providers only send local model updates trained on their local raw data rather than upload any raw data, has been recently proposed to build a privacy-preserving data sharing models. Unfortunately, by analyzing on the differences of uploaded local model updates from data providers, private information can still be divulged, and performance of the system cannot be guaranteed when partial federated nodes executes malicious behavior. Additionally, traditional cloud-based FL poses challenges to the communication overhead with the rapid increase of terminal equipment in IoV system. All these issues inspire us to propose an autonomous blockchain empowered privacy-preserving FL framework in this paper, where the mobile edge computing (MEC) technology was naturally integrated in IoV system.

</p>
</details>

<details><summary><b>A survey on Variational Autoencoders from a GreenAI perspective</b>
<a href="https://arxiv.org/abs/2103.01071">arxiv:2103.01071</a>
&#x1F4C8; 1 <br>
<p>A. Asperti, D. Evangelista, E. Loli Piccolomini</p></summary>
<p>

**Abstract:** Variational AutoEncoders (VAEs) are powerful generative models that merge elements from statistics and information theory with the flexibility offered by deep neural networks to efficiently solve the generation problem for high dimensional data. The key insight of VAEs is to learn the latent distribution of data in such a way that new meaningful samples can be generated from it. This approach led to tremendous research and variations in the architectural design of VAEs, nourishing the recent field of research known as unsupervised representation learning. In this article, we provide a comparative evaluation of some of the most successful, recent variations of VAEs. We particularly focus the analysis on the energetic efficiency of the different models, in the spirit of the so called Green AI, aiming both to reduce the carbon footprint and the financial cost of generative techniques. For each architecture we provide its mathematical formulation, the ideas underlying its design, a detailed model description, a running implementation and quantitative results.

</p>
</details>

<details><summary><b>A Hybrid Quantum-Classical Hamiltonian Learning Algorithm</b>
<a href="https://arxiv.org/abs/2103.01061">arxiv:2103.01061</a>
&#x1F4C8; 1 <br>
<p>Youle Wang, Guangxi Li, Xin Wang</p></summary>
<p>

**Abstract:** Hamiltonian learning is crucial to the certification of quantum devices and quantum simulators. In this paper, we propose a hybrid quantum-classical Hamiltonian learning algorithm to find the coefficients of the Pauli operator components of the Hamiltonian. Its main subroutine is the practical log-partition function estimation algorithm, which is based on the minimization of the free energy of the system. Concretely, we devise a stochastic variational quantum eigensolver (SVQE) to diagonalize the Hamiltonians and then exploit the obtained eigenvalues to compute the free energy's global minimum using convex optimization. Our approach not only avoids the challenge of estimating von Neumann entropy in free energy minimization, but also reduces the quantum resources via importance sampling in Hamiltonian diagonalization, facilitating the implementation of our method on near-term quantum devices. Finally, we demonstrate our approach's validity by conducting numerical experiments with Hamiltonians of interest in quantum many-body physics.

</p>
</details>

<details><summary><b>Information Discrepancy in Strategic Learning</b>
<a href="https://arxiv.org/abs/2103.01028">arxiv:2103.01028</a>
&#x1F4C8; 1 <br>
<p>Yahav Bechavod, Chara Podimata, Zhiwei Steven Wu, Juba Ziani</p></summary>
<p>

**Abstract:** We study the effects of information discrepancy across sub-populations on their ability to simultaneously improve their features in strategic learning settings. Specifically, we consider a game where a principal deploys a decision rule in an attempt to optimize the whole population's welfare, and agents strategically adapt to it to receive better scores. Inspired by real-life settings, such as loan approvals and college admissions, we remove the typical assumption made in the strategic learning literature that the decision rule is fully known to the agents, and focus on settings where it is inaccessible. In their lack of knowledge, individuals try to infer this rule by learning from their peers (e.g., friends and acquaintances who previously applied for a loan), naturally forming groups in the population, each with possibly different type and level of information about the decision rule. In our equilibrium analysis, we show that the principal's decision rule optimizing the welfare across subgroups may cause a surprising negative externality; the true quality of some of the subgroups can actually deteriorate. On the positive side, we show that in many natural cases, optimal improvement is guaranteed simultaneously for all subgroups in equilibrium. We also characterize the disparity in improvements across subgroups via a measure of their informational overlap. Finally, we complement our theoretical analysis with experiments on real-world datasets.

</p>
</details>

<details><summary><b>Error Estimates for the Variational Training of Neural Networks with Boundary Penalty</b>
<a href="https://arxiv.org/abs/2103.01007">arxiv:2103.01007</a>
&#x1F4C8; 1 <br>
<p>Johannes MÃ¼ller, Marius Zeinhofer</p></summary>
<p>

**Abstract:** We establish estimates on the error made by the Ritz method for quadratic energies on the space $H^1(Î©)$ in the approximation of the solution of variational problems with different boundary conditions. Special attention is paid to the case of Dirichlet boundary values which are treated with the boundary penalty method. We consider arbitrary and in general non linear classes $V\subseteq H^1(Î©)$ of ansatz functions and estimate the error in dependence of the optimisation accuracy, the approximation capabilities of the ansatz class and - in the case of Dirichlet boundary values - the penalisation strength $Î»$. For non-essential boundary conditions the error of the Ritz method decays with the same rate as the approximation rate of the ansatz classes. For the boundary penalty method we obtain that given an approximation rate of $r$ in $H^1(Î©)$ and an approximation rate of $s$ in $L^2(\partialÎ©)$ of the ansatz classes, the optimal decay rate of the estimated error is $\min(s/2, r) \in [r/2, r]$ and achieved by choosing $Î»_n\sim n^{s}$. We discuss how this rate can be improved, the relation to existing estimates for finite element functions as well as the implications for ansatz classes which are given through ReLU networks. Finally, we use the notion of $Î$-convergence to show that the Ritz method converges for a wide class of energies including nonlinear stationary PDEs like the $p$-Laplace.

</p>
</details>

<details><summary><b>CogDL: Toolkit for Deep Learning on Graphs</b>
<a href="https://arxiv.org/abs/2103.00959">arxiv:2103.00959</a>
&#x1F4C8; 1 <br>
<p>Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Yang Yang, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, Jie Tang</p></summary>
<p>

**Abstract:** Deep learning on graphs has attracted tremendous attention from the graph learning community in recent years. It has been widely used in several real-world applications such as social network analysis and recommender systems. In this paper, we introduce CogDL, an extensive toolkit for deep learning on graphs that allows researchers and developers to easily conduct experiments and build applications. It provides standard training and evaluation for the most important tasks in the graph domain, including node classification, graph classification, etc. For each task, it provides implementations of state-of-the-art models. The models in our toolkit are divided into two major parts, graph embedding methods and graph neural networks. Most of the graph embedding methods learn node-level or graph-level representations in an unsupervised way and preserves the graph properties such as structural information, while graph neural networks capture node features and work in semi-supervised or self-supervised settings. All models implemented in our toolkit can be easily reproducible for leaderboard results. Most models in CogDL are developed on top of PyTorch, and users can leverage the advantages of PyTorch to implement their own models. Furthermore, we demonstrate the effectiveness of CogDL for real-world applications in AMiner, a large academic mining system.

</p>
</details>

<details><summary><b>CARMI: A Cache-Aware Learned Index with a Cost-based Construction Algorithm</b>
<a href="https://arxiv.org/abs/2103.00858">arxiv:2103.00858</a>
&#x1F4C8; 1 <br>
<p>Jiaoyi Zhang, Yihan Gao</p></summary>
<p>

**Abstract:** Learned indexes, which use machine learning models to replace traditional index structures, have shown promising results in recent studies. However, our understanding of this new type of index structure is still at an early stage with many details that need to be carefully examined and improved. In this paper, we propose a cache-aware learned index (CARMI) design to improve the efficiency of the Recursive Model Index (RMI) framework proposed by Kraska et al. and a cost-based construction algorithm to construct the optimal indexes in a wide variety of application scenarios. We formulate the problem of finding the optimal design of a learned index as an optimization problem and propose a dynamic programming algorithm for solving it and a partial greedy step to speed up. Experiments show that our index construction strategy can construct indexes with significantly better performance compared to baselines under various data distribution and workload requirements. Among them, CARMI can obtain an average of 2.52X speedup compared to B-tree, while using only about 0.56X memory space of B-tree on average.

</p>
</details>

<details><summary><b>Query Rewriting via Cycle-Consistent Translation for E-Commerce Search</b>
<a href="https://arxiv.org/abs/2103.00800">arxiv:2103.00800</a>
&#x1F4C8; 1 <br>
<p>Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, Wen-Yun Yang</p></summary>
<p>

**Abstract:** Nowadays e-commerce search has become an integral part of many people's shopping routines. One critical challenge in today's e-commerce search is the semantic matching problem where the relevant items may not contain the exact terms in the user query. In this paper, we propose a novel deep neural network based approach to query rewriting, in order to tackle this problem. Specifically, we formulate query rewriting into a cyclic machine translation problem to leverage abundant click log data. Then we introduce a novel cyclic consistent training algorithm in conjunction with state-of-the-art machine translation models to achieve the optimal performance in terms of query rewriting accuracy. In order to make it practical in industrial scenarios, we optimize the syntax tree construction to reduce computational cost and online serving latency. Offline experiments show that the proposed method is able to rewrite hard user queries into more standard queries that are more appropriate for the inverted index to retrieve. Comparing with human curated rule-based method, the proposed model significantly improves query rewriting diversity while maintaining good relevancy. Online A/B experiments show that it improves core e-commerce business metrics significantly. Since the summer of 2020, the proposed model has been launched into our search engine production, serving hundreds of millions of users.

</p>
</details>

<details><summary><b>Knowledge-Guided Dynamic Systems Modeling: A Case Study on Modeling River Water Quality</b>
<a href="https://arxiv.org/abs/2103.00792">arxiv:2103.00792</a>
&#x1F4C8; 1 <br>
<p>Namyong Park, MinHyeok Kim, Nguyen Xuan Hoai, R. I.,  McKay, Dong-Kyun Kim</p></summary>
<p>

**Abstract:** Modeling real-world phenomena is a focus of many science and engineering efforts, such as ecological modeling and financial forecasting, to name a few. Building an accurate model for complex and dynamic systems improves understanding of underlying processes and leads to resource efficiency. Towards this goal, knowledge-driven modeling builds a model based on human expertise, yet is often suboptimal. At the opposite extreme, data-driven modeling learns a model directly from data, requiring extensive data and potentially generating overfitting. We focus on an intermediate approach, model revision, in which prior knowledge and data are combined to achieve the best of both worlds. In this paper, we propose a genetic model revision framework based on tree-adjoining grammar (TAG) guided genetic programming (GP), using the TAG formalism and GP operators in an effective mechanism to incorporate prior knowledge and make data-driven revisions in a way that complies with prior knowledge. Our framework is designed to address the high computational cost of evolutionary modeling of complex systems. Via a case study on the challenging problem of river water quality modeling, we show that the framework efficiently learns an interpretable model, with higher modeling accuracy than existing methods.

</p>
</details>

<details><summary><b>Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation</b>
<a href="https://arxiv.org/abs/2103.03102">arxiv:2103.03102</a>
&#x1F4C8; 0 <br>
<p>Wei Dai, Daniel Berleant</p></summary>
<p>

**Abstract:** This paper adds to the fundamental body of work on benchmarking the robustness of deep learning (DL) classifiers. We innovate a new benchmarking methodology to evaluate robustness of DL classifiers. Also, we introduce a new four-quadrant statistical visualization tool, including minimum accuracy, maximum accuracy, mean accuracy, and coefficient of variation, for benchmarking robustness of DL classifiers. To measure robust DL classifiers, we created a comprehensive 69 benchmarking image set, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. After collecting experimental results, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. The two-factor perturbation includes (1) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences, and (2) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in both sequences. All source codes, related image sets, and preliminary data, figures are shared on a GitHub website to support future academic research and industry projects. The web resources locate at https://github.com/caperock/robustai

</p>
</details>

<details><summary><b>Smoothness Analysis of Adversarial Training</b>
<a href="https://arxiv.org/abs/2103.01400">arxiv:2103.01400</a>
&#x1F4C8; 0 <br>
<p>Sekitoshi Kanai, Masanori Yamada, Hiroshi Takahashi, Yuki Yamanaka, Yasutoshi Ida</p></summary>
<p>

**Abstract:** Deep neural networks are vulnerable to adversarial attacks. Recent studies about adversarial robustness focus on the loss landscape in the parameter space since it is related to optimization and generalization performance. These studies conclude that the difficulty of adversarial training is caused by the non-smoothness of the loss function: i.e., its gradient is not Lipschitz continuous. However, this analysis ignores the dependence of adversarial attacks on model parameters. Since adversarial attacks are optimized for models, they should depend on the parameters. Considering this dependence, we analyze the smoothness of the loss function of adversarial training using the optimal attacks for the model parameter in more detail. We reveal that the constraint of adversarial attacks is one cause of the non-smoothness and that the smoothness depends on the types of the constraints. Specifically, the $L_\infty$ constraint can cause non-smoothness more than the $L_2$ constraint. Moreover, our analysis implies that if we flatten the loss function with respect to input data, the Lipschitz constant of the gradient of adversarial loss tends to increase. To address the non-smoothness, we show that EntropySGD smoothens the non-smooth loss and improves the performance of adversarial training.

</p>
</details>

<details><summary><b>Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation</b>
<a href="https://arxiv.org/abs/2103.01391">arxiv:2103.01391</a>
&#x1F4C8; 0 <br>
<p>Semih Cayci, Siddhartha Satpathi, Niao He, R. Srikant</p></summary>
<p>

**Abstract:** In this paper, we study the dynamics of temporal difference learning with neural network-based value function approximation over a general state space, namely, \emph{Neural TD learning}. We consider two practically used algorithms, projection-free and max-norm regularized Neural TD learning, and establish the first convergence bounds for these algorithms. An interesting observation from our results is that max-norm regularization can dramatically improve the performance of TD learning algorithms, both in terms of sample complexity and overparameterization. In particular, we prove that max-norm regularization improves state-of-the-art sample complexity and overparameterization bounds. The results in this work rely on a novel Lyapunov drift analysis of the network parameters as a stopped and controlled random process.

</p>
</details>

<details><summary><b>Anticipation Next -- System-sensitive technology development and integration in work contexts</b>
<a href="https://arxiv.org/abs/2103.00923">arxiv:2103.00923</a>
&#x1F4C8; 0 <br>
<p>Sarah Janboecke, Susanne Zajitschek</p></summary>
<p>

**Abstract:** When discussing future concerns within socio-technical systems in work contexts, we often find descriptions of missed technology development and integration. The experience of technology that fails whilst being integrated is often rooted in dysfunctional epistemological approaches within the research and development process. Thus, ultimately leading to sustainable technology-distrust in work contexts. This is true for organizations that integrate new technologies and for organizations that invent them. Organizations in which we find failed technology development and integrations are, in their very nature, social systems. Nowadays, those complex social systems act within an even more complex environment. This urges the development of new anticipation methods for technology development and integration. Gathering of and dealing with complex information in the described context is what we call Anticipation Next. This explorative work uses existing literature from the adjoining research fields of system theory, organizational theory, and socio-technical research to combine various concepts. We deliberately aim at a networked way of thinking in scientific contexts and thus combine multidisciplinary subject areas in one paper to present an innovative way to deal with multi-faceted problems in a human-centred way. We end with suggesting a conceptual framework that should be used in the very early stages of technology development and integration in work contexts.

</p>
</details>


[Next Page]({{ '/2021/02/28/2021.02.28.html' | relative_url }})
