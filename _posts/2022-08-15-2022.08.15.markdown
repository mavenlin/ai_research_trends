Prev: [2022.08.14]({{ '/2022/08/14/2022.08.14.html' | relative_url }})  Next: [2022.08.16]({{ '/2022/08/16/2022.08.16.html' | relative_url }})
{% raw %}
## Summary for 2022-08-15, created on 2022-08-22


<details><summary><b>Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection</b>
<a href="https://arxiv.org/abs/2208.07084">arxiv:2208.07084</a>
&#x1F4C8; 25 <br>
<p>Daniele Comi, Dimitrios Christofidellis, Pier Francesco Piazza, Matteo Manica</p></summary>
<p>

**Abstract:** Intent discovery is a fundamental task in NLP, and it is increasingly relevant for a variety of industrial applications (Quarteroni 2018). The main challenge resides in the need to identify from input utterances novel unseen in-tents. Herein, we propose Z-BERT-A, a two-stage method for intent discovery relying on a Transformer architecture (Vaswani et al. 2017; Devlin et al. 2018), fine-tuned with Adapters (Pfeiffer et al. 2020), initially trained for Natural Language Inference (NLI), and later applied for unknown in-tent classification in a zero-shot setting. In our evaluation, we firstly analyze the quality of the model after adaptive fine-tuning on known classes. Secondly, we evaluate its performance casting intent classification as an NLI task. Lastly, we test the zero-shot performance of the model on unseen classes, showing how Z-BERT-A can effectively perform in-tent discovery by generating intents that are semantically similar, if not equal, to the ground truth ones. Our experiments show how Z-BERT-A is outperforming a wide variety of baselines in two zero-shot settings: known intents classification and unseen intent discovery. The proposed pipeline holds the potential to be widely applied in a variety of application for customer care. It enables automated dynamic triage using a lightweight model that, unlike large language models, can be easily deployed and scaled in a wide variety of business scenarios. Especially when considering a setting with limited hardware availability and performance whereon-premise or low resource cloud deployments are imperative. Z-BERT-A, predicting novel intents from a single utterance, represents an innovative approach for intent discovery, enabling online generation of novel intents. The pipeline is available as an installable python package at the following link: https://github.com/GT4SD/zberta.

</p>
</details>

<details><summary><b>Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer</b>
<a href="https://arxiv.org/abs/2208.07282">arxiv:2208.07282</a>
&#x1F4C8; 20 <br>
<p>Shahan Nercessian</p></summary>
<p>

**Abstract:** In this paper, we propose a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks such as (singing) voice conversion and the DDSP timbre transfer task. Accordingly, our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We can extend the baseline synthesizer by appending lightweight black-box postnets which apply further processing to the baseline output in order to improve fidelity. An alternative differentiable approach considers extraction of the source excitation spectrum directly, which can improve naturalness albeit for a narrower class of style transfer applications. The acoustic feature parameterization used by our approaches has the added benefit that it naturally disentangles pitch and timbral information so that they can be modeled separately. Moreover, as there exists a robust means of estimating these acoustic features from monophonic audio sources, it allows for parameter loss terms to be added to an end-to-end objective function, which can help convergence and/or further stabilize (adversarial) training.

</p>
</details>

<details><summary><b>DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images</b>
<a href="https://arxiv.org/abs/2208.07227">arxiv:2208.07227</a>
&#x1F4C8; 16 <br>
<p>Bing Wang, Lu Chen, Bo Yang</p></summary>
<p>

**Abstract:** In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.

</p>
</details>

<details><summary><b>Bias amplification in experimental social networks is reduced by resampling</b>
<a href="https://arxiv.org/abs/2208.07261">arxiv:2208.07261</a>
&#x1F4C8; 12 <br>
<p>Mathew D. Hardy, Bill D. Thompson, P. M. Krafft, Thomas L. Griffiths</p></summary>
<p>

**Abstract:** Large-scale social networks are thought to contribute to polarization by amplifying people's biases. However, the complexity of these technologies makes it difficult to identify the mechanisms responsible and to evaluate mitigation strategies. Here we show under controlled laboratory conditions that information transmission through social networks amplifies motivational biases on a simple perceptual decision-making task. Participants in a large behavioral experiment showed increased rates of biased decision-making when part of a social network relative to asocial participants, across 40 independently evolving populations. Drawing on techniques from machine learning and Bayesian statistics, we identify a simple adjustment to content-selection algorithms that is predicted to mitigate bias amplification. This algorithm generates a sample of perspectives from within an individual's network that is more representative of the population as a whole. In a second large experiment, this strategy reduced bias amplification while maintaining the benefits of information sharing.

</p>
</details>

<details><summary><b>Benchmarking Validation Methods for Unsupervised Domain Adaptation</b>
<a href="https://arxiv.org/abs/2208.07360">arxiv:2208.07360</a>
&#x1F4C8; 10 <br>
<p>Kevin Musgrave, Serge Belongie, Ser-Nam Lim</p></summary>
<p>

**Abstract:** This paper compares and ranks 11 UDA validation methods. Validators estimate model accuracy, which makes them an essential component of any UDA train-test pipeline. We rank these validators to indicate which of them are most useful for the purpose of selecting optimal models, checkpoints, and hyperparameters. In addition, we propose and compare new effective validators and significantly improved versions of existing validators. To the best of our knowledge, this large-scale benchmark study is the first of its kind in the UDA field.

</p>
</details>

<details><summary><b>Unsupervised Video Domain Adaptation: A Disentanglement Perspective</b>
<a href="https://arxiv.org/abs/2208.07365">arxiv:2208.07365</a>
&#x1F4C8; 8 <br>
<p>Pengfei Wei, Lingdong Kong, Xinghua Qu, Xiang Yin, Zhiqiang Xu, Jing Jiang, Zejun Ma</p></summary>
<p>

**Abstract:** Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to disentangle the domain-related information from the data during the adaptation process. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static domain-related information and another encoding the temporal and semantic-related information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we further propose several objectives to constrain the latent factors in TranSVAE. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared with several state-of-the-art methods. Code is publicly available at https://github.com/ldkong1205/TranSVAE.

</p>
</details>

<details><summary><b>Multi-modal Transformer Path Prediction for Autonomous Vehicle</b>
<a href="https://arxiv.org/abs/2208.07256">arxiv:2208.07256</a>
&#x1F4C8; 8 <br>
<p>Chia Hong Tseng, Jie Zhang, Min-Te Sun, Kazuya Sakai, Wei-Shinn Ku</p></summary>
<p>

**Abstract:** Reasoning about vehicle path prediction is an essential and challenging problem for the safe operation of autonomous driving systems. There exist many research works for path prediction. However, most of them do not use lane information and are not based on the Transformer architecture. By utilizing different types of data collected from sensors equipped on the self-driving vehicles, we propose a path prediction system named Multi-modal Transformer Path Prediction (MTPP) that aims to predict long-term future trajectory of target agents. To achieve more accurate path prediction, the Transformer architecture is adopted in our model. To better utilize the lane information, the lanes which are in opposite direction to target agent are not likely to be taken by the target agent and are consequently filtered out. In addition, consecutive lane chunks are combined to ensure the lane input to be long enough for path prediction. An extensive evaluation is conducted to show the efficacy of the proposed system using nuScene, a real-world trajectory forecasting dataset.

</p>
</details>

<details><summary><b>Active Bucketized Learning for ACOPF Optimization Proxies</b>
<a href="https://arxiv.org/abs/2208.07497">arxiv:2208.07497</a>
&#x1F4C8; 7 <br>
<p>Michael Klamkin, Mathieu Tanneau, Terrence W. K. Mak, Pascal Van Hentenryck</p></summary>
<p>

**Abstract:** This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Active Bucketized Sampling (ABS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. ABS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. It relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of ABS.

</p>
</details>

<details><summary><b>Knowledge-Injected Federated Learning</b>
<a href="https://arxiv.org/abs/2208.07530">arxiv:2208.07530</a>
&#x1F4C8; 6 <br>
<p>Zhenan Fan, Zirui Zhou, Jian Pei, Michael P. Friedlander, Jiajie Hu, Chengliang Li, Yong Zhang</p></summary>
<p>

**Abstract:** Federated learning is an emerging technique for training models from decentralized data sets. In many applications, data owners participating in the federated learning system hold not only the data but also a set of domain knowledge. Such knowledge includes human know-how and craftsmanship that can be extremely helpful to the federated learning task. In this work, we propose a federated learning framework that allows the injection of participants' domain knowledge, where the key idea is to refine the global model with knowledge locally. The scenario we consider is motivated by a real industry-level application, and we demonstrate the effectiveness of our approach to this application.

</p>
</details>

<details><summary><b>An Efficient Multi-Scale Fusion Network for 3D Organ at Risk (OAR) Segmentation</b>
<a href="https://arxiv.org/abs/2208.07417">arxiv:2208.07417</a>
&#x1F4C8; 6 <br>
<p>Abhishek Srivastava, Debesh Jha, Elif Keles, Bulent Aydogan, Mohamed Abazeed, Ulas Bagci</p></summary>
<p>

**Abstract:** Accurate segmentation of organs-at-risks (OARs) is a precursor for optimizing radiation therapy planning. Existing deep learning-based multi-scale fusion architectures have demonstrated a tremendous capacity for 2D medical image segmentation. The key to their success is aggregating global context and maintaining high resolution representations. However, when translated into 3D segmentation problems, existing multi-scale fusion architectures might underperform due to their heavy computation overhead and substantial data diet. To address this issue, we propose a new OAR segmentation framework, called OARFocalFuseNet, which fuses multi-scale features and employs focal modulation for capturing global-local context across multiple scales. Each resolution stream is enriched with features from different resolution scales, and multi-scale information is aggregated to model diverse contextual ranges. As a result, feature representations are further boosted. The comprehensive comparisons in our experimental setup with OAR segmentation as well as multi-organ segmentation show that our proposed OARFocalFuseNet outperforms the recent state-of-the-art methods on publicly available OpenKBP datasets and Synapse multi-organ segmentation. Both of the proposed methods (3D-MSF and OARFocalFuseNet) showed promising performance in terms of standard evaluation metrics. Our best performing method (OARFocalFuseNet) obtained a dice coefficient of 0.7995 and hausdorff distance of 5.1435 on OpenKBP datasets and dice coefficient of 0.8137 on Synapse multi-organ segmentation dataset.

</p>
</details>

<details><summary><b>Private Query Release via the Johnson-Lindenstrauss Transform</b>
<a href="https://arxiv.org/abs/2208.07410">arxiv:2208.07410</a>
&#x1F4C8; 6 <br>
<p>Aleksandar Nikolov</p></summary>
<p>

**Abstract:** We introduce a new method for releasing answers to statistical queries with differential privacy, based on the Johnson-Lindenstrauss lemma. The key idea is to randomly project the query answers to a lower dimensional space so that the distance between any two vectors of feasible query answers is preserved up to an additive error. Then we answer the projected queries using a simple noise-adding mechanism, and lift the answers up to the original dimension. Using this method, we give, for the first time, purely differentially private mechanisms with optimal worst case sample complexity under average error for answering a workload of $k$ queries over a universe of size $N$. As other applications, we give the first purely private efficient mechanisms with optimal sample complexity for computing the covariance of a bounded high-dimensional distribution, and for answering 2-way marginal queries. We also show that, up to the dependence on the error, a variant of our mechanism is nearly optimal for every given query workload.

</p>
</details>

<details><summary><b>Reward Design For An Online Reinforcement Learning Algorithm Supporting Oral Self-Care</b>
<a href="https://arxiv.org/abs/2208.07406">arxiv:2208.07406</a>
&#x1F4C8; 6 <br>
<p>Anna L. Trella, Kelly W. Zhang, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy</p></summary>
<p>

**Abstract:** Dental disease is one of the most common chronic diseases despite being largely preventable. However, professional advice on optimal oral hygiene practices is often forgotten or abandoned by patients. Therefore patients may benefit from timely and personalized encouragement to engage in oral self-care behaviors. In this paper, we develop an online reinforcement learning (RL) algorithm for use in optimizing the delivery of mobile-based prompts to encourage oral hygiene behaviors. One of the main challenges in developing such an algorithm is ensuring that the algorithm considers the impact of the current action on the effectiveness of future actions (i.e., delayed effects), especially when the algorithm has been made simple in order to run stably and autonomously in a constrained, real-world setting (i.e., highly noisy, sparse data). We address this challenge by designing a quality reward which maximizes the desired health outcome (i.e., high-quality brushing) while minimizing user burden. We also highlight a procedure for optimizing the hyperparameters of the reward by building a simulation environment test bed and evaluating candidates using the test bed. The RL algorithm discussed in this paper will be deployed in Oralytics, an oral self-care app that provides behavioral strategies to boost patient engagement in oral hygiene practices.

</p>
</details>

<details><summary><b>Combining Predictions under Uncertainty: The Case of Random Decision Trees</b>
<a href="https://arxiv.org/abs/2208.07403">arxiv:2208.07403</a>
&#x1F4C8; 6 <br>
<p>Florian Busch, Moritz Kulessa, Eneldo Loza Mencía, Hendrik Blockeel</p></summary>
<p>

**Abstract:** A common approach to aggregate classification estimates in an ensemble of decision trees is to either use voting or to average the probabilities for each class. The latter takes uncertainty into account, but not the reliability of the uncertainty estimates (so to say, the "uncertainty about the uncertainty"). More generally, much remains unknown about how to best combine probabilistic estimates from multiple sources. In this paper, we investigate a number of alternative prediction methods. Our methods are inspired by the theories of probability, belief functions and reliable classification, as well as a principle that we call evidence accumulation. Our experiments on a variety of data sets are based on random decision trees which guarantees a high diversity in the predictions to be combined. Somewhat unexpectedly, we found that taking the average over the probabilities is actually hard to beat. However, evidence accumulation showed consistently better results on all but very small leafs.

</p>
</details>

<details><summary><b>Computational Empathy Counteracts the Negative Effects of Anger on Creative Problem Solving</b>
<a href="https://arxiv.org/abs/2208.07178">arxiv:2208.07178</a>
&#x1F4C8; 6 <br>
<p>Matthew Groh, Craig Ferguson, Robert Lewis, Rosalind Picard</p></summary>
<p>

**Abstract:** How does empathy influence creative problem solving? We introduce a computational empathy intervention based on context-specific affective mimicry and perspective taking by a virtual agent appearing in the form of a well-dressed polar bear. In an online experiment with 1,006 participants randomly assigned to an emotion elicitation intervention (with a control elicitation condition and anger elicitation condition) and a computational empathy intervention (with a control virtual agent and an empathic virtual agent), we examine how anger and empathy influence participants' performance in solving a word game based on Wordle. We find participants who are assigned to the anger elicitation condition perform significantly worse on multiple performance metrics than participants assigned to the control condition. However, we find the empathic virtual agent counteracts the drop in performance induced by the anger condition such that participants assigned to both the empathic virtual agent and the anger condition perform no differently than participants in the control elicitation condition and significantly better than participants assigned to the control virtual agent and the anger elicitation condition. While empathy reduces the negative effects of anger, we do not find evidence that the empathic virtual agent influences performance of participants who are assigned to the control elicitation condition. By introducing a framework for computational empathy interventions and conducting a two-by-two factorial design randomized experiment, we provide rigorous, empirical evidence that computational empathy can counteract the negative effects of anger on creative problem solving.

</p>
</details>

<details><summary><b>Memory-Driven Text-to-Image Generation</b>
<a href="https://arxiv.org/abs/2208.07022">arxiv:2208.07022</a>
&#x1F4C8; 6 <br>
<p>Bowen Li, Philip H. S. Torr, Thomas Lukasiewicz</p></summary>
<p>

**Abstract:** We introduce a memory-driven semi-parametric approach to text-to-image generation, which is based on both parametric and non-parametric techniques. The non-parametric component is a memory bank of image features constructed from a training set of images. The parametric component is a generative adversarial network. Given a new text description at inference time, the memory bank is used to selectively retrieve image features that are provided as basic information of target images, which enables the generator to produce realistic synthetic results. We also incorporate the content information into the discriminator, together with semantic features, allowing the discriminator to make a more reliable prediction. Experimental results demonstrate that the proposed memory-driven semi-parametric approach produces more realistic images than purely parametric approaches, in terms of both visual fidelity and text-image semantic consistency.

</p>
</details>

<details><summary><b>CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models</b>
<a href="https://arxiv.org/abs/2208.07476">arxiv:2208.07476</a>
&#x1F4C8; 5 <br>
<p>Chuyen Nguyen, Caleb Morgan, Sudip Mittal</p></summary>
<p>

**Abstract:** As the practicality of Artificial Intelligence (AI) and Machine Learning (ML) based techniques grow, there is an ever increasing threat of adversarial attacks. There is a need to red team this ecosystem to identify system vulnerabilities, potential threats, characterize properties that will enhance system robustness, and encourage the creation of effective defenses. A secondary need is to share this AI security threat intelligence between different stakeholders like, model developers, users, and AI/ML security professionals. In this paper, we create and describe a prototype system CTI4AI, to overcome the need to methodically identify and share AI/ML specific vulnerabilities and threat intelligence.

</p>
</details>

<details><summary><b>MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control</b>
<a href="https://arxiv.org/abs/2208.07363">arxiv:2208.07363</a>
&#x1F4C8; 5 <br>
<p>Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, Matthew Hausknecht</p></summary>
<p>

**Abstract:** Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.
  Videos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct.

</p>
</details>

<details><summary><b>Predicting from Predictions</b>
<a href="https://arxiv.org/abs/2208.07331">arxiv:2208.07331</a>
&#x1F4C8; 5 <br>
<p>Celestine Mendler-Dünner, Frances Ding, Yixin Wang</p></summary>
<p>

**Abstract:** Predictions about people, such as their expected educational achievement or their credit risk, can be performative and shape the outcome that they aim to predict. Understanding the causal effect of these predictions on the eventual outcomes is crucial for foreseeing the implications of future predictive models and selecting which models to deploy. However, this causal estimation task poses unique challenges: model predictions are usually deterministic functions of input features and highly correlated with outcomes, which can make the causal effects of predictions impossible to disentangle from the direct effect of the covariates. We study this problem through the lens of causal identifiability, and despite the hardness of this problem in full generality, we highlight three natural scenarios where the causal effect of predictions on outcomes can be identified from observational data: randomization in predictions or prediction-based decisions, overparameterization of the predictive model deployed during data collection, and discrete prediction outputs. We show empirically that, under suitable identifiability conditions, standard variants of supervised learning that predict from predictions can find transferable functional relationships between features, predictions, and outcomes, allowing for conclusions about newly deployed prediction models. Our positive results fundamentally rely on model predictions being recorded during data collection, bringing forward the importance of rethinking standard data collection practices to enable progress towards a better understanding of social outcomes and performative feedback loops.

</p>
</details>

<details><summary><b>Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task</b>
<a href="https://arxiv.org/abs/2208.07097">arxiv:2208.07097</a>
&#x1F4C8; 5 <br>
<p>Radostin Cholakov, Todor Kolev</p></summary>
<p>

**Abstract:** The adoption of pre-trained language models in task-oriented dialogue systems has resulted in significant enhancements of their text generation abilities. However, these architectures are slow to use because of the large number of trainable parameters and can sometimes fail to generate diverse responses. To address these limitations, we propose two models with auxiliary tasks for response selection - (1) distinguishing distractors from ground truth responses and (2) distinguishing synthetic responses from ground truth labels. They achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined scores of 107.5 and 108.3 and outperform a baseline with three times more parameters. We publish reproducible code and checkpoints and discuss the effects of applying auxiliary tasks to T5-based architectures.

</p>
</details>

<details><summary><b>Generating Pixel Art Character Sprites using GANs</b>
<a href="https://arxiv.org/abs/2208.06413">arxiv:2208.06413</a>
&#x1F4C8; 5 <br>
<p>Flávio Coutinho, Luiz Chaimowicz</p></summary>
<p>

**Abstract:** Iterating on creating pixel art character sprite sheets is essential to the game development process. However, it can take a lot of effort until the final versions containing different poses and animation clips are achieved. This paper investigates using conditional generative adversarial networks to aid the designers in creating such sprite sheets. We propose an architecture based on Pix2Pix to generate images of characters facing a target side (e.g., right) given sprites of them in a source pose (e.g., front). Experiments with small pixel art datasets yielded promising results, resulting in models with varying degrees of generalization, sometimes capable of generating images very close to the ground truth. We analyze the results through visual inspection and quantitatively with FID.

</p>
</details>

<details><summary><b>Towards Informed Design and Validation Assistance in Computer Games Using Imitation Learning</b>
<a href="https://arxiv.org/abs/2208.07811">arxiv:2208.07811</a>
&#x1F4C8; 4 <br>
<p>Alessandro Sestini, Joakim Bergdahl, Konrad Tollera, Andrew D. Bagdanov, Linus Gisslén</p></summary>
<p>

**Abstract:** In games, as in and many other domains, design validation and testing is a huge challenge as systems are growing in size and manual testing is becoming infeasible. This paper proposes a new approach to automated game validation and testing. Our method leverages a data-driven imitation learning technique, which requires little effort and time and no knowledge of machine learning or programming, that designers can use to efficiently train game testing agents. We investigate the validity of our approach through a user study with industry experts. The survey results show that our method is indeed a valid approach to game validation and that data-driven programming would be a useful aid to reducing effort and increasing quality of modern playtesting. The survey also highlights several open challenges. With the help of the most recent literature, we analyze the identified challenges and propose future research directions suitable for supporting and maximizing the utility of our approach.

</p>
</details>

<details><summary><b>An Overview and Prospective Outlook on Robust Training and Certification of Machine Learning Models</b>
<a href="https://arxiv.org/abs/2208.07464">arxiv:2208.07464</a>
&#x1F4C8; 4 <br>
<p>Brendon G. Anderson, Tanmay Gautam, Somayeh Sojoudi</p></summary>
<p>

**Abstract:** In this discussion paper, we survey recent research surrounding robustness of machine learning models. As learning algorithms become increasingly more popular in data-driven control systems, their robustness to data uncertainty must be ensured in order to maintain reliable safety-critical operations. We begin by reviewing common formalisms for such robustness, and then move on to discuss popular and state-of-the-art techniques for training robust machine learning models as well as methods for provably certifying such robustness. From this unification of robust machine learning, we identify and discuss pressing directions for future research in the area.

</p>
</details>

<details><summary><b>Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives</b>
<a href="https://arxiv.org/abs/2208.07422">arxiv:2208.07422</a>
&#x1F4C8; 4 <br>
<p>Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-Won Kang, Jonghye Woo</p></summary>
<p>

**Abstract:** Deep learning has become the method of choice to tackle real-world problems in different domains, partly because of its ability to learn from data and achieve impressive performance on a wide range of applications. However, its success usually relies on two assumptions: (i) vast troves of labeled datasets are required for accurate model fitting, and (ii) training and testing data are independent and identically distributed. Its performance on unseen target domains, thus, is not guaranteed, especially when encountering out-of-distribution data at the adaptation stage. The performance drop on data in a target domain is a critical problem in deploying deep neural networks that are successfully trained on data in a source domain. Unsupervised domain adaptation (UDA) is proposed to counter this, by leveraging both labeled source domain data and unlabeled target domain data to carry out various tasks in the target domain. UDA has yielded promising results on natural image processing, video analysis, natural language processing, time-series data analysis, medical image analysis, etc. In this review, as a rapidly evolving topic, we provide a systematic comparison of its methods and applications. In addition, the connection of UDA with its closely related tasks, e.g., domain generalization and out-of-distribution detection, has also been discussed. Furthermore, deficiencies in current methods and possible promising directions are highlighted.

</p>
</details>

<details><summary><b>A Survey of Recommender System Techniques and the Ecommerce Domain</b>
<a href="https://arxiv.org/abs/2208.07399">arxiv:2208.07399</a>
&#x1F4C8; 4 <br>
<p>Imran Hossain, Md Aminul Haque Palash, Anika Tabassum Sejuty, Noor A Tanjim, MD Abdullah AL Nasim, Sarwar Saif, Abu Bokor Suraj</p></summary>
<p>

**Abstract:** In this big data era, it is hard for the current generation to find the right data from the huge amount of data contained within online platforms. In such a situation, there is a need for an information filtering system that might help them find the information they are looking for. In recent years, a research field has emerged known as recommender systems. Recommenders have become important as they have many real-life applications. This paper reviews the different techniques and developments of recommender systems in e-commerce, e-tourism, e-resources, e-government, e-learning, and e-library. By analyzing recent work on this topic, we will be able to provide a detailed overview of current developments and identify existing difficulties in recommendation systems. The final results give practitioners and researchers the necessary guidance and insights into the recommendation system and its application.

</p>
</details>

<details><summary><b>Domain-aware Control-oriented Neural Models for Autonomous Underwater Vehicles</b>
<a href="https://arxiv.org/abs/2208.07333">arxiv:2208.07333</a>
&#x1F4C8; 4 <br>
<p>Wenceslao Shaw Cortez, Soumya Vasisht, Aaron Tuor, Ján Drgoňa, Draguna Vrabie</p></summary>
<p>

**Abstract:** Conventional physics-based modeling is a time-consuming bottleneck in control design for complex nonlinear systems like autonomous underwater vehicles (AUVs). In contrast, purely data-driven models, though convenient and quick to obtain, require a large number of observations and lack operational guarantees for safety-critical systems. Data-driven models leveraging available partially characterized dynamics have potential to provide reliable systems models in a typical data-limited scenario for high value complex systems, thereby avoiding months of expensive expert modeling time. In this work we explore this middle-ground between expert-modeled and pure data-driven modeling. We present control-oriented parametric models with varying levels of domain-awareness that exploit known system structure and prior physics knowledge to create constrained deep neural dynamical system models. We employ universal differential equations to construct data-driven blackbox and graybox representations of the AUV dynamics. In addition, we explore a hybrid formulation that explicitly models the residual error related to imperfect graybox models. We compare the prediction performance of the learned models for different distributions of initial conditions and control inputs to assess their accuracy, generalization, and suitability for control.

</p>
</details>

<details><summary><b>Cross-scale Attention Guided Multi-instance Learning for Crohn's Disease Diagnosis with Pathological Images</b>
<a href="https://arxiv.org/abs/2208.07322">arxiv:2208.07322</a>
&#x1F4C8; 4 <br>
<p>Ruining Deng, Can Cui, Lucas W. Remedios, Shunxing Bao, R. Michael Womick, Sophie Chiron, Jia Li, Joseph T. Roland, Ken S. Lau, Qi Liu, Keith T. Wilson, Yaohong Wang, Lori A. Coburn, Bennett A. Landman, Yuankai Huo</p></summary>
<p>

**Abstract:** Multi-instance learning (MIL) is widely used in the computer-aided interpretation of pathological Whole Slide Images (WSIs) to solve the lack of pixel-wise or patch-wise annotations. Often, this approach directly applies "natural image driven" MIL algorithms which overlook the multi-scale (i.e. pyramidal) nature of WSIs. Off-the-shelf MIL algorithms are typically deployed on a single-scale of WSIs (e.g., 20x magnification), while human pathologists usually aggregate the global and local patterns in a multi-scale manner (e.g., by zooming in and out between different magnifications). In this study, we propose a novel cross-scale attention mechanism to explicitly aggregate inter-scale interactions into a single MIL network for Crohn's Disease (CD), which is a form of inflammatory bowel disease. The contribution of this paper is two-fold: (1) a cross-scale attention mechanism is proposed to aggregate features from different resolutions with multi-scale interaction; and (2) differential multi-scale attention visualizations are generated to localize explainable lesion patterns. By training ~250,000 H&E-stained Ascending Colon (AC) patches from 20 CD patient and 30 healthy control samples at different scales, our approach achieved a superior Area under the Curve (AUC) score of 0.8924 compared with baseline models. The official implementation is publicly available at https://github.com/hrlblab/CS-MIL.

</p>
</details>

<details><summary><b>Analysis of impact of emotions on target speech extraction and speech separation</b>
<a href="https://arxiv.org/abs/2208.07091">arxiv:2208.07091</a>
&#x1F4C8; 4 <br>
<p>Ján Švec, Kateřina Žmolíková, Martin Kocour, Marc Delcroix, Tsubasa Ochiai, Ladislav Mošner, Jan Černocký</p></summary>
<p>

**Abstract:** Recently, the performance of blind speech separation (BSS) and target speech extraction (TSE) has greatly progressed. Most works, however, focus on relatively well-controlled conditions using, e.g., read speech. The performance may degrade in more realistic situations. One of the factors causing such degradation may be intrinsic speaker variability, such as emotions, occurring commonly in realistic speech. In this paper, we investigate the influence of emotions on TSE and BSS. We create a new test dataset of emotional mixtures for the evaluation of TSE and BSS. This dataset combines LibriSpeech and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). Through controlled experiments, we can analyze the impact of different emotions on the performance of BSS and TSE. We observe that BSS is relatively robust to emotions, while TSE, which requires identifying and extracting the speech of a target speaker, is much more sensitive to emotions. On comparative speaker verification experiments we show that identifying the target speaker may be particularly challenging when dealing with emotional speech. Using our findings, we outline potential future directions that could improve the robustness of BSS and TSE systems toward emotional speech.

</p>
</details>

<details><summary><b>Semidefinite Programming versus Burer-Monteiro Factorization for Matrix Sensing</b>
<a href="https://arxiv.org/abs/2208.07469">arxiv:2208.07469</a>
&#x1F4C8; 3 <br>
<p>Baturalp Yalcin, Ziye Ma, Javad Lavaei, Somayeh Sojoudi</p></summary>
<p>

**Abstract:** Many fundamental low-rank optimization problems, such as matrix completion, phase synchronization/retrieval, power system state estimation, and robust PCA, can be formulated as the matrix sensing problem. Two main approaches for solving matrix sensing are based on semidefinite programming (SDP) and Burer-Monteiro (B-M) factorization. The SDP method suffers from high computational and space complexities, whereas the B-M method may return a spurious solution due to the non-convexity of the problem. The existing theoretical guarantees for the success of these methods have led to similar conservative conditions, which may wrongly imply that these methods have comparable performances. In this paper, we shed light on some major differences between these two methods. First, we present a class of structured matrix completion problems for which the B-M methods fail with an overwhelming probability, while the SDP method works correctly. Second, we identify a class of highly sparse matrix completion problems for which the B-M method works and the SDP method fails. Third, we prove that although the B-M method exhibits the same performance independent of the rank of the unknown solution, the success of the SDP method is correlated to the rank of the solution and improves as the rank increases. Unlike the existing literature that has mainly focused on those instances of matrix sensing for which both SDP and B-M work, this paper offers the first result on the unique merit of each method over the alternative approach.

</p>
</details>

<details><summary><b>Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets</b>
<a href="https://arxiv.org/abs/2208.07463">arxiv:2208.07463</a>
&#x1F4C8; 3 <br>
<p>Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Wei Ye, Jindong Wang, Guosheng Hu, Marios Savvides</p></summary>
<p>

**Abstract:** While parameter efficient tuning (PET) methods have shown great potential with transformer architecture on Natural Language Processing (NLP) tasks, their effectiveness is still under-studied with large-scale ConvNets on Computer Vision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for ConvNets. Conv-Adapter is light-weight, domain-transferable, and architecture-agnostic with generalized performance on different tasks. When transferring on downstream tasks, Conv-Adapter learns tasks-specific feature modulation to the intermediate representations of backbone while keeping the pre-trained parameters frozen. By introducing only a tiny amount of learnable parameters, e.g., only 3.5% full fine-tuning parameters of ResNet50, Conv-Adapter outperforms previous PET baseline methods and achieves comparable or surpasses the performance of full fine-tuning on 23 classification tasks of various domains. It also presents superior performance on few-shot classifications, with an average margin of 3.39%. Beyond classification, Conv-Adapter can generalize to detection and segmentation tasks with more than 50% reduction of parameters but comparable performance to the traditional full fine-tuning.

</p>
</details>

<details><summary><b>ROLAND: Graph Learning Framework for Dynamic Graphs</b>
<a href="https://arxiv.org/abs/2208.07239">arxiv:2208.07239</a>
&#x1F4C8; 3 <br>
<p>Jiaxuan You, Tianyu Du, Jure Leskovec</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNNs) have been successfully applied to many real-world static graphs. However, the success of static graphs has not fully translated to dynamic graphs due to the limitations in model design, evaluation settings, and training strategies. Concretely, existing dynamic GNNs do not incorporate state-of-the-art designs from static GNNs, which limits their performance. Current evaluation settings for dynamic GNNs do not fully reflect the evolving nature of dynamic graphs. Finally, commonly used training methods for dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph representation learning framework for real-world dynamic graphs. At its core, the ROLAND framework can help researchers easily repurpose any static GNN to dynamic graphs. Our insight is to view the node embeddings at different GNN layers as hierarchical node states and then recurrently update them over time. We then introduce a live-update evaluation setting for dynamic graphs that mimics real-world use cases, where GNNs are making predictions and being updated on a rolling basis. Finally, we propose a scalable and efficient training approach for dynamic GNNs via incremental training and meta-learning. We conduct experiments over eight different dynamic graph datasets on future link prediction tasks. Models built using the ROLAND framework achieve on average 62.7% relative mean reciprocal rank (MRR) improvement over state-of-the-art baselines under the standard evaluation settings on three datasets. We find state-of-the-art baselines experience out-of-memory errors for larger datasets, while ROLAND can easily scale to dynamic graphs with 56 million edges. After re-implementing these baselines using the ROLAND training strategy, ROLAND models still achieve on average 15.5% relative MRR improvement over the baselines.

</p>
</details>

<details><summary><b>Energy and Spectrum Efficient Federated Learning via High-Precision Over-the-Air Computation</b>
<a href="https://arxiv.org/abs/2208.07237">arxiv:2208.07237</a>
&#x1F4C8; 3 <br>
<p>Liang Li, Chenpei Huang, Dian Shi, Hao Wang, Xiangwei Zhou, Minglei Shu, Miao Pan</p></summary>
<p>

**Abstract:** Federated learning (FL) enables mobile devices to collaboratively learn a shared prediction model while keeping data locally. However, there are two major research challenges to practically deploy FL over mobile devices: (i) frequent wireless updates of huge size gradients v.s. limited spectrum resources, and (ii) energy-hungry FL communication and local computing during training v.s. battery-constrained mobile devices. To address those challenges, in this paper, we propose a novel multi-bit over-the-air computation (M-AirComp) approach for spectrum-efficient aggregation of local model updates in FL and further present an energy-efficient FL design for mobile devices. Specifically, a high-precision digital modulation scheme is designed and incorporated in the M-AirComp, allowing mobile devices to upload model updates at the selected positions simultaneously in the multi-access channel. Moreover, we theoretically analyze the convergence property of our FL algorithm. Guided by FL convergence analysis, we formulate a joint transmission probability and local computing control optimization, aiming to minimize the overall energy consumption (i.e., iterative local computing + multi-round communications) of mobile devices in FL. Extensive simulation results show that our proposed scheme outperforms existing ones in terms of spectrum utilization, energy efficiency, and learning accuracy.

</p>
</details>

<details><summary><b>Man-in-the-Middle Attack against Object Detection Systems</b>
<a href="https://arxiv.org/abs/2208.07174">arxiv:2208.07174</a>
&#x1F4C8; 3 <br>
<p>Han Wu, Sareh Rowlands, Johan Wahlstrom</p></summary>
<p>

**Abstract:** Is deep learning secure for robots? As embedded systems have access to more powerful CPUs and GPUs, deep-learning-enabled object detection systems become pervasive in robotic applications. Meanwhile, prior research unveils that deep learning models are vulnerable to adversarial attacks. Does this put real-world robots at threat? Our research borrows the idea of the Main-in-the-Middle attack from Cryptography to attack an object detection system. Our experimental results prove that we can generate a strong Universal Adversarial Perturbation (UAP) within one minute and then use the perturbation to attack a detection system via the Man-in-the-Middle attack. Our findings raise a serious concern over the applications of deep learning models in safety-critical systems such as autonomous driving.

</p>
</details>

<details><summary><b>Deception for Cyber Defence: Challenges and Opportunities</b>
<a href="https://arxiv.org/abs/2208.07127">arxiv:2208.07127</a>
&#x1F4C8; 3 <br>
<p>David Liebowitz, Surya Nepal, Kristen Moore, Cody J. Christopher, Salil S. Kanhere, David Nguyen, Roelien C. Timmer, Michael Longland, Keerth Rathakumar</p></summary>
<p>

**Abstract:** Deception is rapidly growing as an important tool for cyber defence, complementing existing perimeter security measures to rapidly detect breaches and data theft. One of the factors limiting the use of deception has been the cost of generating realistic artefacts by hand. Recent advances in Machine Learning have, however, created opportunities for scalable, automated generation of realistic deceptions. This vision paper describes the opportunities and challenges involved in developing models to mimic many common elements of the IT stack for deception effects.

</p>
</details>

<details><summary><b>Grasping Core Rules of Time Series through Pure Models</b>
<a href="https://arxiv.org/abs/2208.07105">arxiv:2208.07105</a>
&#x1F4C8; 3 <br>
<p>Gedi Liu, Yifeng Jiang, Yi Ouyang, Keyang Zhong, Yang Wang</p></summary>
<p>

**Abstract:** Time series underwent the transition from statistics to deep learning, as did many other machine learning fields. Although it appears that the accuracy has been increasing as the model is updated in a number of publicly available datasets, it typically only increases the scale by several times in exchange for a slight difference in accuracy. Through this experiment, we point out a different line of thinking, time series, especially long-term forecasting, may differ from other fields. It is not necessary to use extensive and complex models to grasp all aspects of time series, but to use pure models to grasp the core rules of time series changes. With this simple but effective idea, we created PureTS, a network with three pure linear layers that achieved state-of-the-art in 80% of the long sequence prediction tasks while being nearly the lightest model and having the fastest running speed. On this basis, we discuss the potential of pure linear layers in both phenomena and essence. The ability to understand the core law contributes to the high precision of long-distance prediction, and reasonable fluctuation prevents it from distorting the curve in multi-step prediction like mainstream deep learning models, which is summarized as a pure linear neural network that avoids over-fluctuating. Finally, we suggest the fundamental design standards for lightweight long-step time series tasks: input and output should try to have the same dimension, and the structure avoids fragmentation and complex operations.

</p>
</details>

<details><summary><b>Online Pole Segmentation on Range Images for Long-term LiDAR Localization in Urban Environments</b>
<a href="https://arxiv.org/abs/2208.07364">arxiv:2208.07364</a>
&#x1F4C8; 2 <br>
<p>Hao Dong, Xieyuanli Chen, Simo Särkkä, Cyrill Stachniss</p></summary>
<p>

**Abstract:** Robust and accurate localization is a basic requirement for mobile autonomous systems. Pole-like objects, such as traffic signs, poles, and lamps are frequently used landmarks for localization in urban environments due to their local distinctiveness and long-term stability. In this paper, we present a novel, accurate, and fast pole extraction approach based on geometric features that runs online and has little computational demands. Our method performs all computations directly on range images generated from 3D LiDAR scans, which avoids processing 3D point clouds explicitly and enables fast pole extraction for each scan. We further use the extracted poles as pseudo labels to train a deep neural network for online range image-based pole segmentation. We test both our geometric and learning-based pole extraction methods for localization on different datasets with different LiDAR scanners, routes, and seasonal changes. The experimental results show that our methods outperform other state-of-the-art approaches. Moreover, boosted with pseudo pole labels extracted from multiple datasets, our learning-based method can run across different datasets and achieve even better localization results compared to our geometry-based method. We released our pole datasets to the public for evaluating the performance of pole extractors, as well as the implementation of our approach.

</p>
</details>

<details><summary><b>Can a latent Hawkes process be used for epidemiological modelling?</b>
<a href="https://arxiv.org/abs/2208.07340">arxiv:2208.07340</a>
&#x1F4C8; 2 <br>
<p>Stamatina Lamprinakou, Axel Gandy, Emma McCoy</p></summary>
<p>

**Abstract:** Understanding the spread of COVID-19 has been the subject of numerous studies, highlighting the significance of reliable epidemic models. Here, we introduce a novel epidemic model using a latent Hawkes process with temporal covariates for modelling the infections. Unlike other models, we model the reported cases via a probability distribution driven by the underlying Hawkes process. Modelling the infections via a Hawkes process allows us to estimate by whom an infected individual was infected. We propose a Kernel Density Particle Filter (KDPF) for inference of both latent cases and reproduction number and for predicting the new cases in the near future. The computational effort is proportional to the number of infections making it possible to use particle filter type algorithms, such as the KDPF. We demonstrate the performance of the proposed algorithm on synthetic data sets and COVID-19 reported cases in various local authorities in the UK, and benchmark our model to alternative approaches.

</p>
</details>

<details><summary><b>MENLI: Robust Evaluation Metrics from Natural Language Inference</b>
<a href="https://arxiv.org/abs/2208.07316">arxiv:2208.07316</a>
&#x1F4C8; 2 <br>
<p>Yanran Chen, Steffen Eger</p></summary>
<p>

**Abstract:** Recently proposed BERT-based evaluation metrics perform well on standard evaluation benchmarks but are vulnerable to adversarial attacks, e.g., relating to factuality errors. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when we combine existing metrics with our NLI metrics, we obtain both higher adversarial robustness (+20% to +30%) and higher quality metrics as measured on standard benchmarks (+5% to +25%).

</p>
</details>

<details><summary><b>Learn2Trust: A video and streamlit-based educational programme for AI-based medical image analysis targeted towards medical students</b>
<a href="https://arxiv.org/abs/2208.07314">arxiv:2208.07314</a>
&#x1F4C8; 2 <br>
<p>Hanna Siebert, Marian Himstedt, Mattias Heinrich</p></summary>
<p>

**Abstract:** In order to be able to use artificial intelligence (AI) in medicine without scepticism and to recognise and assess its growing potential, a basic understanding of this topic is necessary among current and future medical staff. Under the premise of "trust through understanding", we developed an innovative online course as a learning opportunity within the framework of the German KI Campus (AI campus) project, which is a self-guided course that teaches the basics of AI for the analysis of medical image data. The main goal is to provide a learning environment for a sufficient understanding of AI in medical image analysis so that further interest in this topic is stimulated and inhibitions towards its use can be overcome by means of positive application experience. The focus was on medical applications and the fundamentals of machine learning. The online course was divided into consecutive lessons, which include theory in the form of explanatory videos, practical exercises in the form of Streamlit and practical exercises and/or quizzes to check learning progress. A survey among the participating medical students in the first run of the course was used to analyse our research hypotheses quantitatively.

</p>
</details>

<details><summary><b>Task Oriented Video Coding: A Survey</b>
<a href="https://arxiv.org/abs/2208.07313">arxiv:2208.07313</a>
&#x1F4C8; 2 <br>
<p>Daniel Wood</p></summary>
<p>

**Abstract:** Video coding technology has been continuously improved for higher compression ratio with higher resolution. However, the state-of-the-art video coding standards, such as H.265/HEVC and Versatile Video Coding, are still designed with the assumption the compressed video will be watched by humans. With the tremendous advance and maturation of deep neural networks in solving computer vision tasks, more and more videos are directly analyzed by deep neural networks without humans' involvement. Such a conventional design for video coding standard is not optimal when the compressed video is used by computer vision applications. While the human visual system is consistently sensitive to the content with high contrast, the impact of pixels on computer vision algorithms is driven by specific computer vision tasks. In this paper, we explore and summarize recent progress on computer vision task oriented video coding and emerging video coding standard, Video Coding for Machines.

</p>
</details>

<details><summary><b>Convergence Rates for Stochastic Approximation on a Boundary</b>
<a href="https://arxiv.org/abs/2208.07243">arxiv:2208.07243</a>
&#x1F4C8; 2 <br>
<p>Kody Law, Neil Walton, Shangda Yang</p></summary>
<p>

**Abstract:** We analyze the behavior of projected stochastic gradient descent focusing on the case where the optimum is on the boundary of the constraint set and the gradient does not vanish at the optimum. Here iterates may in expectation make progress against the objective at each step. When this and an appropriate moment condition on noise holds, we prove that the convergence rate to the optimum of the constrained stochastic gradient descent will be different and typically be faster than the unconstrained stochastic gradient descent algorithm. Our results argue that the concentration around the optimum is exponentially distributed rather than normally distributed, which typically determines the limiting convergence in the unconstrained case. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains by Hajek (1982) to the area of stochastic approximation algorithms. As examples, we show how the results apply to linear programming and tabular reinforcement learning.

</p>
</details>

<details><summary><b>Where is VALDO? VAscular Lesions Detection and segmentatiOn challenge at MICCAI 2021</b>
<a href="https://arxiv.org/abs/2208.07167">arxiv:2208.07167</a>
&#x1F4C8; 2 <br>
<p>Carole H. Sudre, Kimberlin Van Wijnen, Florian Dubost, Hieab Adams, David Atkinson, Frederik Barkhof, Mahlet A. Birhanu, Esther E. Bron, Robin Camarasa, Nish Chaturvedi, Yuan Chen, Zihao Chen, Shuai Chen, Qi Dou, Tavia Evans, Ivan Ezhov, Haojun Gao, Marta Girones Sanguesa, Juan Domingo Gispert, Beatriz Gomez Anson, Alun D. Hughes, M. Arfan Ikram, Silvia Ingala, H. Rolf Jaeger, Florian Kofler</p></summary>
<p>

**Abstract:** Imaging markers of cerebral small vessel disease provide valuable information on brain health, but their manual assessment is time-consuming and hampered by substantial intra- and interrater variability. Automated rating may benefit biomedical research, as well as clinical assessment, but diagnostic reliability of existing algorithms is unknown. Here, we present the results of the \textit{VAscular Lesions DetectiOn and Segmentation} (\textit{Where is VALDO?}) challenge that was run as a satellite event at the international conference on Medical Image Computing and Computer Aided Intervention (MICCAI) 2021. This challenge aimed to promote the development of methods for automated detection and segmentation of small and sparse imaging markers of cerebral small vessel disease, namely enlarged perivascular spaces (EPVS) (Task 1), cerebral microbleeds (Task 2) and lacunes of presumed vascular origin (Task 3) while leveraging weak and noisy labels. Overall, 12 teams participated in the challenge proposing solutions for one or more tasks (4 for Task 1 - EPVS, 9 for Task 2 - Microbleeds and 6 for Task 3 - Lacunes). Multi-cohort data was used in both training and evaluation. Results showed a large variability in performance both across teams and across tasks, with promising results notably for Task 1 - EPVS and Task 2 - Microbleeds and not practically useful results yet for Task 3 - Lacunes. It also highlighted the performance inconsistency across cases that may deter use at an individual level, while still proving useful at a population level.

</p>
</details>

<details><summary><b>Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling</b>
<a href="https://arxiv.org/abs/2208.07131">arxiv:2208.07131</a>
&#x1F4C8; 2 <br>
<p>Ki-Ung Song</p></summary>
<p>

**Abstract:** Compared to the existing function-based models in deep generative modeling, the recently proposed diffusion models have achieved outstanding performance with a stochastic-process-based approach. But a long sampling time is required for this approach due to many timesteps for discretization. Schrödinger bridge (SB)-based models attempt to tackle this problem by training bidirectional stochastic processes between distributions. However, they still have a slow sampling speed compared to generative models such as generative adversarial networks. And due to the training of the bidirectional stochastic processes, they require a relatively long training time. Therefore, this study tried to reduce the number of timesteps and training time required and proposed regularization terms to the existing SB models to make the bidirectional stochastic processes consistent and stable with a reduced number of timesteps. Each regularization term was integrated into a single term to enable more efficient training in computation time and memory usage. Applying this regularized stochastic process to various generation tasks, the desired translations between different distributions were obtained, and accordingly, the possibility of generative modeling based on a stochastic process with faster sampling speed could be confirmed. The code is available at https://github.com/KiUngSong/RSB.

</p>
</details>

<details><summary><b>Online 3D Bin Packing Reinforcement Learning Solution with Buffer</b>
<a href="https://arxiv.org/abs/2208.07123">arxiv:2208.07123</a>
&#x1F4C8; 2 <br>
<p>Aaron Valero Puche, Sukhan Lee</p></summary>
<p>

**Abstract:** The 3D Bin Packing Problem (3D-BPP) is one of the most demanded yet challenging problems in industry, where an agent must pack variable size items delivered in sequence into a finite bin with the aim to maximize the space utilization. It represents a strongly NP-Hard optimization problem such that no solution has been offered to date with high performance in space utilization. In this paper, we present a new reinforcement learning (RL) framework for a 3D-BPP solution for improving performance. First, a buffer is introduced to allow multi-item action selection. By increasing the degree of freedom in action selection, a more complex policy that results in better packing performance can be derived. Second, we propose an agnostic data augmentation strategy that exploits both bin item symmetries for improving sample efficiency. Third, we implement a model-based RL method adapted from the popular algorithm AlphaGo, which has shown superhuman performance in zero-sum games. Our adaptation is capable of working in single-player and score based environments. In spite of the fact that AlphaGo versions are known to be computationally heavy, we manage to train the proposed framework with a single thread and GPU, while obtaining a solution that outperforms the state-of-the-art results in space utilization.

</p>
</details>

<details><summary><b>Seminaive Materialisation in DatalogMTL</b>
<a href="https://arxiv.org/abs/2208.07100">arxiv:2208.07100</a>
&#x1F4C8; 2 <br>
<p>Dingmin Wang, Przemysław Andrzej Wałęga, Bernardo Cuenca Grau</p></summary>
<p>

**Abstract:** DatalogMTL is an extension of Datalog with metric temporal operators that has found applications in temporal ontology-based data access and query answering, as well as in stream reasoning. Practical algorithms for DatalogMTL are reliant on materialisation-based reasoning, where temporal facts are derived in a forward chaining manner in successive rounds of rule applications. Current materialisation-based procedures are, however, based on a naive evaluation strategy, where the main source of inefficiency stems from redundant computations.
  In this paper, we propose a materialisation-based procedure which, analogously to the classical seminaive algorithm in Datalog, aims at minimising redundant computation by ensuring that each temporal rule instance is considered at most once during the execution of the algorithm. Our experiments show that our optimised seminaive strategy for DatalogMTL is able to significantly reduce materialisation times.

</p>
</details>

<details><summary><b>Predictive Data Calibration for Linear Correlation Significance Testing</b>
<a href="https://arxiv.org/abs/2208.07081">arxiv:2208.07081</a>
&#x1F4C8; 2 <br>
<p>Kaustubh R. Patil, Simon B. Eickhoff, Robert Langner</p></summary>
<p>

**Abstract:** Inferring linear relationships lies at the heart of many empirical investigations. A measure of linear dependence should correctly evaluate the strength of the relationship as well as qualify whether it is meaningful for the population. Pearson's correlation coefficient (PCC), the \textit{de-facto} measure for bivariate relationships, is known to lack in both regards. The estimated strength $r$ maybe wrong due to limited sample size, and nonnormality of data. In the context of statistical significance testing, erroneous interpretation of a $p$-value as posterior probability leads to Type I errors -- a general issue with significance testing that extends to PCC. Such errors are exacerbated when testing multiple hypotheses simultaneously. To tackle these issues, we propose a machine-learning-based predictive data calibration method which essentially conditions the data samples on the expected linear relationship. Calculating PCC using calibrated data yields a calibrated $p$-value that can be interpreted as posterior probability together with a calibrated $r$ estimate, a desired outcome not provided by other methods. Furthermore, the ensuing independent interpretation of each test might eliminate the need for multiple testing correction. We provide empirical evidence favouring the proposed method using several simulations and application to real-world data.

</p>
</details>

<details><summary><b>Self-Supervised Vision Transformers for Malware Detection</b>
<a href="https://arxiv.org/abs/2208.07049">arxiv:2208.07049</a>
&#x1F4C8; 2 <br>
<p>Sachith Seneviratne, Ridwan Shariffdeen, Sanka Rasnayaka, Nuran Kasthuriarachchi</p></summary>
<p>

**Abstract:** Malware detection plays a crucial role in cyber-security with the increase in malware growth and advancements in cyber-attacks. Previously unseen malware which is not determined by security vendors are often used in these attacks and it is becoming inevitable to find a solution that can self-learn from unlabeled sample data. This paper presents SHERLOCK, a self-supervision based deep learning model to detect malware based on the Vision Transformer (ViT) architecture. SHERLOCK is a novel malware detection method which learns unique features to differentiate malware from benign programs with the use of image-based binary representation. Experimental results using 1.2 million Android applications across a hierarchy of 47 types and 696 families, shows that self-supervised learning can achieve an accuracy of 97% for the binary classification of malware which is higher than existing state-of-the-art techniques. Our proposed model is also able to outperform state-of-the-art techniques for multi-class malware classification of types and family with macro-F1 score of .497 and .491 respectively.

</p>
</details>

<details><summary><b>Pyramidal Predictive Network: A Model for Visual-frame Prediction Based on Predictive Coding Theory</b>
<a href="https://arxiv.org/abs/2208.07021">arxiv:2208.07021</a>
&#x1F4C8; 2 <br>
<p>Chaofan Ling, Junpei Zhong, Weihua Li</p></summary>
<p>

**Abstract:** Inspired by the well-known predictive coding theory in cognitive science, we propose a novel neural network model for the task of visual-frame prediction. In this paper, our main work is to combine the theoretical framework of predictive coding and deep learning architectures, to design an efficient predictive network model for visual-frame prediction. The model is composed of a series of recurrent and convolutional units forming the top-down and bottom-up streams, respectively. It learns to predict future frames in a visual sequence, with ConvLSTMs on each layer in the network making local prediction from top to down. The main innovation of our model is that the update frequency of neural units on each of the layer decreases with the increasing of network levels, which results in the model appears like a pyramid from the perspective of time dimension, so we call it the Pyramid Predictive Network (PPNet). Particularly, this pyramid-like design is consistent to the neuronal activities in the neuroscience findings involved in the predictive coding framework. According to the experimental results, this model shows better compactness and comparable predictive performance with existing works, implying lower computational cost and higher prediction accuracy. Code will be available at https://github.com/Ling-CF/PPNet.

</p>
</details>

<details><summary><b>Transformer Networks for Predictive Group Elevator Control</b>
<a href="https://arxiv.org/abs/2208.08948">arxiv:2208.08948</a>
&#x1F4C8; 1 <br>
<p>Jing Zhang, Athanasios Tsiligkaridis, Hiroshi Taguchi, Arvind Raghunathan, Daniel Nikovski</p></summary>
<p>

**Abstract:** We propose a Predictive Group Elevator Scheduler by using predictive information of passengers arrivals from a Transformer based destination predictor and a linear regression model that predicts remaining time to destinations. Through extensive empirical evaluation, we find that the savings of Average Waiting Time (AWT) could be as high as above 50% for light arrival streams and around 15% for medium arrival streams in afternoon down-peak traffic regimes. Such results can be obtained after carefully setting the Predicted Probability of Going to Elevator (PPGE) threshold, thus avoiding a majority of false predictions for people heading to the elevator, while achieving as high as 80% of true predictive elevator landings as early as after having seen only 60% of the whole trajectory of a passenger.

</p>
</details>

<details><summary><b>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</b>
<a href="https://arxiv.org/abs/2208.07339">arxiv:2208.07339</a>
&#x1F4C8; 1 <br>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer</p></summary>
<p>

**Abstract:** Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs.

</p>
</details>

<details><summary><b>Transformer-based Value Function Decomposition for Cooperative Multi-agent Reinforcement Learning in StarCraft</b>
<a href="https://arxiv.org/abs/2208.07298">arxiv:2208.07298</a>
&#x1F4C8; 1 <br>
<p>Muhammad Junaid Khan, Syed Hammad Ahmed, Gita Sukthankar</p></summary>
<p>

**Abstract:** The StarCraft II Multi-Agent Challenge (SMAC) was created to be a challenging benchmark problem for cooperative multi-agent reinforcement learning (MARL). SMAC focuses exclusively on the problem of StarCraft micromanagement and assumes that each unit is controlled individually by a learning agent that acts independently and only possesses local information; centralized training is assumed to occur with decentralized execution (CTDE). To perform well in SMAC, MARL algorithms must handle the dual problems of multi-agent credit assignment and joint action evaluation.
  This paper introduces a new architecture TransMix, a transformer-based joint action-value mixing network which we show to be efficient and scalable as compared to the other state-of-the-art cooperative MARL solutions. TransMix leverages the ability of transformers to learn a richer mixing function for combining the agents' individual value functions. It achieves comparable performance to previous work on easy SMAC scenarios and outperforms other techniques on hard scenarios, as well as scenarios that are corrupted with Gaussian noise to simulate fog of war.

</p>
</details>

<details><summary><b>Debiased Recommendation with Neural Stratification</b>
<a href="https://arxiv.org/abs/2208.07281">arxiv:2208.07281</a>
&#x1F4C8; 1 <br>
<p>Quanyu Dai, Zhenhua Dong, Xu Chen</p></summary>
<p>

**Abstract:** Debiased recommender models have recently attracted increasing attention from the academic and industry communities. Existing models are mostly based on the technique of inverse propensity score (IPS). However, in the recommendation domain, IPS can be hard to estimate given the sparse and noisy nature of the observed user-item exposure data. To alleviate this problem, in this paper, we assume that the user preference can be dominated by a small amount of latent factors, and propose to cluster the users for computing more accurate IPS via increasing the exposure densities. Basically, such method is similar with the spirit of stratification models in applied statistics. However, unlike previous heuristic stratification strategy, we learn the cluster criterion by presenting the users with low ranking embeddings, which are future shared with the user representations in the recommender model. At last, we find that our model has strong connections with the previous two types of debiased recommender models. We conduct extensive experiments based on real-world datasets to demonstrate the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>Estimating Personal Model Parameters from Utterances in Model-based Reminiscence</b>
<a href="https://arxiv.org/abs/2208.07087">arxiv:2208.07087</a>
&#x1F4C8; 1 <br>
<p>Shoki Sakai, Kazuki Itabashi, Junya Morita</p></summary>
<p>

**Abstract:** Reminiscence therapy is mental health care based on the recollection of memories. However, the effectiveness of this method varies amongst individuals. To solve this problem, it is necessary to provide more personalized support; therefore, this study utilized a computational model of personal memory recollection based on a cognitive architecture adaptive control of thought-rational (ACT-R). An ACT-R memory model reflecting the state of users is expected to facilitate personal recollection. In this study, we proposed a method for estimating the internal states of users through repeated interactions with the memory model. The model, which contains the lifelog of the user, presents a memory item (stimulus) to the user, and receives the response of the user to the stimulus, based on which it adjusts the internal parameters of the model. Through the repetition of these processes, the parameters of the model will reflect the internal states of the user. To confirm the feasibility of the proposed method, we analyzed utterances of users when using a system that incorporates this model. The results confirmed the ability of the method to estimate the memory retrieval parameters of the model from the utterances of the user. In addition, the ability of the method to estimate changes in the mood of the user caused by using the system was confirmed. These results support the feasibility of the interactive method for estimating human internal states, which will eventually contribute to the ability to induce memory recall and emotions for our well-being.

</p>
</details>

<details><summary><b>Combinatorial optimization solving by coherent Ising machines based on spiking neural networks</b>
<a href="https://arxiv.org/abs/2208.07502">arxiv:2208.07502</a>
&#x1F4C8; 0 <br>
<p>Bo Lu, Yong-Pan Gao, Kai Wen, Chuan Wang</p></summary>
<p>

**Abstract:** Spiking neural network is a kind of neuromorphic computing which is believed to improve on the level of intelligence and provide advabtages for quantum computing. In this work, we address this issue by designing an optical spiking neural network and prove that it can be used to accelerate the speed of computation, especially on the combinatorial optimization problems. Here the spiking neural network is constructed by the antisymmetrically coupled degenerate optical parametric oscillator pulses and dissipative pulses. A nonlinear transfer function is chosen to mitigate amplitude inhomogeneities and destabilize the resulting local minima according to the dynamical behavior of spiking neurons. It is numerically proved that the spiking neural network-coherent Ising machines has excellent performance on combinatorial optimization problems, for which is expected to offer a new applications for neural computing and optical computing.

</p>
</details>

<details><summary><b>Dürfen Maschinen denken (können)? Warum Künstliche Intelligenz eine Ethik braucht. (Are Machines Allowed to (be able to) Think? Why Artificial Intelligence Needs Ethics)</b>
<a href="https://arxiv.org/abs/2208.07402">arxiv:2208.07402</a>
&#x1F4C8; 0 <br>
<p>Karsten Wendland</p></summary>
<p>

**Abstract:** Speech manuscript (German + English) of the impulse lecture for the panel discussion "May machines (be able to) think?" at the 102nd Katholikentag on May 28, 2022 in Stuttgart. Panel: Winfried Kretschmann (MdL, Prime Minister Baden-Württemberg, Stuttgart), Ursula Nothelle-Wildfeuer (Freiburg), Michael Resch (Stuttgart),Karsten Wendland (Aalen). Moderation: Stefanie Rentsch (Fulda). Advocate of the audience: Verena Neuhausen (Stuttgart).

</p>
</details>


{% endraw %}
Prev: [2022.08.14]({{ '/2022/08/14/2022.08.14.html' | relative_url }})  Next: [2022.08.16]({{ '/2022/08/16/2022.08.16.html' | relative_url }})