Prev: [2022.02.01]({{ '/2022/02/01/2022.02.01.html' | relative_url }})  Next: [2022.02.03]({{ '/2022/02/03/2022.02.03.html' | relative_url }})
{% raw %}
## Summary for 2022-02-02, created on 2022-02-12


<details><summary><b>VOS: Learning What You Don't Know by Virtual Outlier Synthesis</b>
<a href="https://arxiv.org/abs/2202.01197">arxiv:2202.01197</a>
&#x1F4C8; 1300 <br>
<p>Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li</p></summary>
<p>

**Abstract:** Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos.

</p>
</details>

<details><summary><b>Unified Scaling Laws for Routed Language Models</b>
<a href="https://arxiv.org/abs/2202.01169">arxiv:2202.01169</a>
&#x1F4C8; 242 <br>
<p>Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu</p></summary>
<p>

**Abstract:** The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.

</p>
</details>

<details><summary><b>Technology Ethics in Action: Critical and Interdisciplinary Perspectives</b>
<a href="https://arxiv.org/abs/2202.01351">arxiv:2202.01351</a>
&#x1F4C8; 201 <br>
<p>Ben Green</p></summary>
<p>

**Abstract:** This special issue interrogates the meaning and impacts of "tech ethics": the embedding of ethics into digital technology research, development, use, and governance. In response to concerns about the social harms associated with digital technologies, many individuals and institutions have articulated the need for a greater emphasis on ethics in digital technology. Yet as more groups embrace the concept of ethics, critical discourses have emerged questioning whose ethics are being centered, whether "ethics" is the appropriate frame for improving technology, and what it means to develop "ethical" technology in practice. This interdisciplinary issue takes up these questions, interrogating the relationships among ethics, technology, and society in action. This special issue engages with the normative and contested notions of ethics itself, how ethics has been integrated with technology across domains, and potential paths forward to support more just and egalitarian technology. Rather than starting from philosophical theories, the authors in this issue orient their articles around the real-world discourses and impacts of tech ethics--i.e., tech ethics in action.

</p>
</details>

<details><summary><b>Maintaining fairness across distribution shift: do we have viable solutions for real-world applications?</b>
<a href="https://arxiv.org/abs/2202.01034">arxiv:2202.01034</a>
&#x1F4C8; 59 <br>
<p>Jessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, Awa Dieng, Yuan Liu, Vivek Natarajan, Alan Karthikesalingam, Katherine Heller, Silvia Chiappa, Alexander D'Amour</p></summary>
<p>

**Abstract:** Fairness and robustness are often considered as orthogonal dimensions when evaluating machine learning models. However, recent work has revealed interactions between fairness and robustness, showing that fairness properties are not necessarily maintained under distribution shift. In healthcare settings, this can result in e.g. a model that performs fairly according to a selected metric in "hospital A" showing unfairness when deployed in "hospital B". While a nascent field has emerged to develop provable fair and robust models, it typically relies on strong assumptions about the shift, limiting its impact for real-world applications. In this work, we explore the settings in which recently proposed mitigation strategies are applicable by referring to a causal framing. Using examples of predictive models in dermatology and electronic health records, we show that real-world applications are complex and often invalidate the assumptions of such methods. Our work hence highlights technical, practical, and engineering gaps that prevent the development of robustly fair machine learning models for real-world applications. Finally, we discuss potential remedies at each step of the machine learning pipeline.

</p>
</details>

<details><summary><b>mSLAM: Massively multilingual joint pre-training for speech and text</b>
<a href="https://arxiv.org/abs/2202.01374">arxiv:2202.01374</a>
&#x1F4C8; 57 <br>
<p>Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja, Jason Riesa, Alexis Conneau</p></summary>
<p>

**Abstract:** We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.

</p>
</details>

<details><summary><b>Robust Training of Neural Networks using Scale Invariant Architectures</b>
<a href="https://arxiv.org/abs/2202.00980">arxiv:2202.00980</a>
&#x1F4C8; 43 <br>
<p>Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank J. Reddi, Sanjiv Kumar</p></summary>
<p>

**Abstract:** In contrast to SGD, adaptive gradient methods like Adam allow robust training of modern deep networks, especially large language models. However, the use of adaptivity not only comes at the cost of extra memory but also raises the fundamental question: can non-adaptive methods like SGD enjoy similar benefits? In this paper, we provide an affirmative answer to this question by proposing to achieve both robust and memory-efficient training via the following general recipe: (1) modify the architecture and make it scale invariant, i.e. the scale of parameter doesn't affect the output of the network, (2) train with SGD and weight decay, and optionally (3) clip the global gradient norm proportional to weight norm multiplied by $\sqrt{\tfrac{2λ}η}$, where $η$ is learning rate and $λ$ is weight decay. We show that this general approach is robust to rescaling of parameter and loss by proving that its convergence only depends logarithmically on the scale of initialization and loss, whereas the standard SGD might not even converge for many initializations. Following our recipe, we design a scale invariant version of BERT, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks.

</p>
</details>

<details><summary><b>Formal Mathematics Statement Curriculum Learning</b>
<a href="https://arxiv.org/abs/2202.01344">arxiv:2202.01344</a>
&#x1F4C8; 30 <br>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever</p></summary>
<p>

**Abstract:** We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.

</p>
</details>

<details><summary><b>AI Research Associate for Early-Stage Scientific Discovery</b>
<a href="https://arxiv.org/abs/2202.03199">arxiv:2202.03199</a>
&#x1F4C8; 24 <br>
<p>Morad Behandish, John Maxwell III, Johan de Kleer</p></summary>
<p>

**Abstract:** Artificial intelligence (AI) has been increasingly applied in scientific activities for decades; however, it is still far from an insightful and trustworthy collaborator in the scientific process. Most existing AI methods are either too simplistic to be useful in real problems faced by scientists or too domain-specialized (even dogmatized), stifling transformative discoveries or paradigm shifts. We present an AI research associate for early-stage scientific discovery based on (a) a novel minimally-biased ontology for physics-based modeling that is context-aware, interpretable, and generalizable across classical and relativistic physics; (b) automatic search for viable and parsimonious hypotheses, represented at a high-level (via domain-agnostic constructs) with built-in invariants, e.g., postulated forms of conservation principles implied by a presupposed spacetime topology; and (c) automatic compilation of the enumerated hypotheses to domain-specific, interpretable, and trainable/testable tensor-based computation graphs to learn phenomenological relations, e.g., constitutive or material laws, from sparse (and possibly noisy) data sets.

</p>
</details>

<details><summary><b>Heterogeneous manifolds for curvature-aware graph embedding</b>
<a href="https://arxiv.org/abs/2202.01185">arxiv:2202.01185</a>
&#x1F4C8; 21 <br>
<p>Francesco Di Giovanni, Giulia Luise, Michael Bronstein</p></summary>
<p>

**Abstract:** Graph embeddings, wherein the nodes of the graph are represented by points in a continuous space, are used in a broad range of Graph ML applications. The quality of such embeddings crucially depends on whether the geometry of the space matches that of the graph. Euclidean spaces are often a poor choice for many types of real-world graphs, where hierarchical structure and a power-law degree distribution are linked to negative curvature. In this regard, it has recently been shown that hyperbolic spaces and more general manifolds, such as products of constant-curvature spaces and matrix manifolds, are advantageous to approximately match nodes pairwise distances. However, all these classes of manifolds are homogeneous, implying that the curvature distribution is the same at each point, making them unsuited to match the local curvature (and related structural properties) of the graph. In this paper, we study graph embeddings in a broader class of heterogeneous rotationally-symmetric manifolds. By adding a single extra radial dimension to any given existing homogeneous model, we can both account for heterogeneous curvature distributions on graphs and pairwise distances. We evaluate our approach on reconstruction tasks on synthetic and real datasets and show its potential in better preservation of high-order structures and heterogeneous random graphs generation.

</p>
</details>

<details><summary><b>GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information</b>
<a href="https://arxiv.org/abs/2202.00965">arxiv:2202.00965</a>
&#x1F4C8; 18 <br>
<p>Hai Dang, Lukas Mecke, Daniel Buschek</p></summary>
<p>

**Abstract:** We investigate how multiple sliders with and without feedforward visualizations influence users' control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.

</p>
</details>

<details><summary><b>Generative Flow Networks for Discrete Probabilistic Modeling</b>
<a href="https://arxiv.org/abs/2202.01361">arxiv:2202.01361</a>
&#x1F4C8; 10 <br>
<p>Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, Yoshua Bengio</p></summary>
<p>

**Abstract:** We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN's effectiveness on various probabilistic modeling tasks.

</p>
</details>

<details><summary><b>Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization</b>
<a href="https://arxiv.org/abs/2202.01334">arxiv:2202.01334</a>
&#x1F4C8; 7 <br>
<p>Dianbo Liu, Alex Lamb, Xu Ji, Pascal Notsawo, Mike Mozer, Yoshua Bengio, Kenji Kawaguchi</p></summary>
<p>

**Abstract:** Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. It has been theoretically and empirically shown that discretization of representations leads to improved generalization, including in reinforcement learning where discretization can be used to bottleneck multi-agent communication to promote agent specialization and robustness. The discretization tightness of most VQ-based methods is defined by the number of discrete codes in the representation vector and the codebook size, which are fixed as hyperparameters. In this work, we propose learning to dynamically select discretization tightness conditioned on inputs, based on the hypothesis that data naturally contains variations in complexity that call for different levels of representational coarseness. We show that dynamically varying tightness in communication bottlenecks can improve model performance on visual reasoning and reinforcement learning tasks.

</p>
</details>

<details><summary><b>Causal Inference Through the Structural Causal Marginal Problem</b>
<a href="https://arxiv.org/abs/2202.01300">arxiv:2202.01300</a>
&#x1F4C8; 7 <br>
<p>Luigi Gresele, Julius von Kügelgen, Jonas M. Kübler, Elke Kirschbaum, Bernhard Schölkopf, Dominik Janzing</p></summary>
<p>

**Abstract:** We introduce an approach to counterfactual inference based on merging information from multiple datasets. We consider a causal reformulation of the statistical marginal problem: given a collection of marginal structural causal models (SCMs) over distinct but overlapping sets of variables, determine the set of joint SCMs that are counterfactually consistent with the marginal ones. We formalise this approach for categorical SCMs using the response function formulation and show that it reduces the space of allowed marginal and joint SCMs. Our work thus highlights a new mode of falsifiability through additional variables, in contrast to the statistical one via additional data.

</p>
</details>

<details><summary><b>Auto-Transfer: Learning to Route Transferrable Representations</b>
<a href="https://arxiv.org/abs/2202.01011">arxiv:2202.01011</a>
&#x1F4C8; 6 <br>
<p>Keerthiram Murugesan, Vijay Sadashivaiah, Ronny Luss, Karthikeyan Shanmugam, Pin-Yu Chen, Amit Dhurandhar</p></summary>
<p>

**Abstract:** Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labelled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach which automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67, and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features our target network focuses on in different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning.

</p>
</details>

<details><summary><b>Understanding Knowledge Integration in Language Models with Graph Convolutions</b>
<a href="https://arxiv.org/abs/2202.00964">arxiv:2202.00964</a>
&#x1F4C8; 6 <br>
<p>Yifan Hou, Guoji Fu, Mrinmaya Sachan</p></summary>
<p>

**Abstract:** Pretrained language models (LMs) do not capture factual knowledge very well. This has led to the development of a number of knowledge integration (KI) methods which aim to incorporate external knowledge into pretrained LMs. Even though KI methods show some performance gains over vanilla LMs, the inner-workings of these methods are not well-understood. For instance, it is unclear how and what kind of knowledge is effectively integrated into these models and if such integration may lead to catastrophic forgetting of already learned knowledge. This paper revisits the KI process in these models with an information-theoretic view and shows that KI can be interpreted using a graph convolution operation. We propose a probe model called \textit{Graph Convolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and exposing what kind of knowledge is integrated into these models. We conduct experiments to verify that our GCS can indeed be used to correctly interpret the KI process, and we use it to analyze two well-known knowledge-enhanced LMs: ERNIE and K-Adapter, and find that only a small amount of factual knowledge is integrated in them. We stratify knowledge in terms of various relation types and find that ERNIE and K-Adapter integrate different kinds of knowledge to different extent. Our analysis also shows that simply increasing the size of the KI corpus may not lead to better KI; fundamental advances may be needed.

</p>
</details>

<details><summary><b>Exploring Sub-skeleton Trajectories for Interpretable Recognition of Sign Language</b>
<a href="https://arxiv.org/abs/2202.01390">arxiv:2202.01390</a>
&#x1F4C8; 5 <br>
<p>Joachim Gudmundsson, Martin P. Seybold, John Pfeifer</p></summary>
<p>

**Abstract:** Recent advances in tracking sensors and pose estimation software enable smart systems to use trajectories of skeleton joint locations for supervised learning. We study the problem of accurately recognizing sign language words, which is key to narrowing the communication gap between hard and non-hard of hearing people.
  Our method explores a geometric feature space that we call `sub-skeleton' aspects of movement. We assess similarity of feature space trajectories using natural, speed invariant distance measures, which enables clear and insightful nearest neighbor classification. The simplicity and scalability of our basic method allows for immediate application in different data domains with little to no parameter tuning.
  We demonstrate the effectiveness of our basic method, and a boosted variation, with experiments on data from different application domains and tracking technologies. Surprisingly, our simple methods improve sign recognition over recent, state-of-the-art approaches.

</p>
</details>

<details><summary><b>Parameters or Privacy: A Provable Tradeoff Between Overparameterization and Membership Inference</b>
<a href="https://arxiv.org/abs/2202.01243">arxiv:2202.01243</a>
&#x1F4C8; 5 <br>
<p>Jasper Tan, Blake Mason, Hamid Javadi, Richard G. Baraniuk</p></summary>
<p>

**Abstract:** A surprising phenomenon in modern machine learning is the ability of a highly overparameterized model to generalize well (small error on the test data) even when it is trained to memorize the training data (zero error on the training data). This has led to an arms race towards increasingly overparameterized models (c.f., deep learning). In this paper, we study an underexplored hidden cost of overparameterization: the fact that overparameterized models are more vulnerable to privacy attacks, in particular the membership inference attack that predicts the (potentially sensitive) examples used to train a model. We significantly extend the relatively few empirical results on this problem by theoretically proving for an overparameterized linear regression model with Gaussian data that the membership inference vulnerability increases with the number of parameters. Moreover, a range of empirical studies indicates that more complex, nonlinear models exhibit the same behavior. Finally, we study different methods for mitigating such attacks in the overparameterized regime, such as noise addition and regularization, and conclude that simply reducing the parameters of an overparameterized model is an effective strategy to protect it from membership inference without greatly decreasing its generalization error.

</p>
</details>

<details><summary><b>RescoreBERT: Discriminative Speech Recognition Rescoring with BERT</b>
<a href="https://arxiv.org/abs/2202.01094">arxiv:2202.01094</a>
&#x1F4C8; 5 <br>
<p>Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan, Ankur Gandhe, Ariya Rastrow, Andreas Stolcke, Ivan Bulyko</p></summary>
<p>

**Abstract:** Second-pass rescoring is an important component in automatic speech recognition (ASR) systems that is used to improve the outputs from a first-pass decoder by implementing a lattice rescoring or $n$-best re-ranking. While pretraining with a masked language model (MLM) objective has received great success in various natural language understanding (NLU) tasks, it has not gained traction as a rescoring model for ASR. Specifically, training a bidirectional model like BERT on a discriminative objective such as minimum WER (MWER) has not been explored. Here we show how to train a BERT-based rescoring model with MWER loss, to incorporate the improvements of a discriminative loss into fine-tuning of deep bidirectional pretrained models for ASR. Specifically, we propose a fusion strategy that incorporates the MLM into the discriminative training process to effectively distill knowledge from a pretrained model. We further propose an alternative discriminative loss. We name this approach RescoreBERT. On the LibriSpeech corpus, it reduces WER by 6.6%/3.4% relative on clean/other test sets over a BERT baseline without discriminative objective. We also evaluate our method on an internal dataset from a conversational agent and find that it reduces both latency and WER (by 3 to 8% relative) over an LSTM rescoring model.

</p>
</details>

<details><summary><b>Automated Detection of Doxing on Twitter</b>
<a href="https://arxiv.org/abs/2202.00879">arxiv:2202.00879</a>
&#x1F4C8; 5 <br>
<p>Younes Karimi, Anna Squicciarini, Shomir Wilson</p></summary>
<p>

**Abstract:** Doxing refers to the practice of disclosing sensitive personal information about a person without their consent. This form of cyberbullying is an unpleasant and sometimes dangerous phenomenon for online social networks. Although prior work exists on automated identification of other types of cyberbullying, a need exists for methods capable of detecting doxing on Twitter specifically. We propose and evaluate a set of approaches for automatically detecting second- and third-party disclosures on Twitter of sensitive private information, a subset of which constitutes doxing. We summarize our findings of common intentions behind doxing episodes and compare nine different approaches for automated detection based on string-matching and one-hot encoded heuristics, as well as word and contextualized string embedding representations of tweets. We identify an approach providing 96.86% accuracy and 97.37% recall using contextualized string embeddings and conclude by discussing the practicality of our proposed methods.

</p>
</details>

<details><summary><b>Dynamic Virtual Network Embedding Algorithm based on Graph Convolution Neural Network and Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2202.02140">arxiv:2202.02140</a>
&#x1F4C8; 4 <br>
<p>Peiying Zhang, Chao Wang, Neeraj Kumar, Weishan Zhang, Lei Liu</p></summary>
<p>

**Abstract:** Network virtualization (NV) is a technology with broad application prospects. Virtual network embedding (VNE) is the core orientation of VN, which aims to provide more flexible underlying physical resource allocation for user function requests. The classical VNE problem is usually solved by heuristic method, but this method often limits the flexibility of the algorithm and ignores the time limit. In addition, the partition autonomy of physical domain and the dynamic characteristics of virtual network request (VNR) also increase the difficulty of VNE. This paper proposed a new type of VNE algorithm, which applied reinforcement learning (RL) and graph neural network (GNN) theory to the algorithm, especially the combination of graph convolutional neural network (GCNN) and RL algorithm. Based on a self-defined fitness matrix and fitness value, we set up the objective function of the algorithm implementation, realized an efficient dynamic VNE algorithm, and effectively reduced the degree of resource fragmentation. Finally, we used comparison algorithms to evaluate the proposed method. Simulation experiments verified that the dynamic VNE algorithm based on RL and GCNN has good basic VNE characteristics. By changing the resource attributes of physical network and virtual network, it can be proved that the algorithm has good flexibility.

</p>
</details>

<details><summary><b>DeepQMLP: A Scalable Quantum-Classical Hybrid DeepNeural Network Architecture for Classification</b>
<a href="https://arxiv.org/abs/2202.01899">arxiv:2202.01899</a>
&#x1F4C8; 4 <br>
<p>Mahabubul Alam, Swaroop Ghosh</p></summary>
<p>

**Abstract:** Quantum machine learning (QML) is promising for potential speedups and improvements in conventional machine learning (ML) tasks (e.g., classification/regression). The search for ideal QML models is an active research field. This includes identification of efficient classical-to-quantum data encoding scheme, construction of parametric quantum circuits (PQC) with optimal expressivity and entanglement capability, and efficient output decoding scheme to minimize the required number of measurements, to name a few. However, most of the empirical/numerical studies lack a clear path towards scalability. Any potential benefit observed in a simulated environment may diminish in practical applications due to the limitations of noisy quantum hardware (e.g., under decoherence, gate-errors, and crosstalk). We present a scalable quantum-classical hybrid deep neural network (DeepQMLP) architecture inspired by classical deep neural network architectures. In DeepQMLP, stacked shallow Quantum Neural Network (QNN) models mimic the hidden layers of a classical feed-forward multi-layer perceptron network. Each QNN layer produces a new and potentially rich representation of the input data for the next layer. This new representation can be tuned by the parameters of the circuit. Shallow QNN models experience less decoherence, gate errors, etc. which make them (and the network) more resilient to quantum noise. We present numerical studies on a variety of classification problems to show the trainability of DeepQMLP. We also show that DeepQMLP performs reasonably well on unseen data and exhibits greater resilience to noise over QNN models that use a deep quantum circuit. DeepQMLP provided up to 25.3% lower loss and 7.92% higher accuracy during inference under noise than QMLP.

</p>
</details>

<details><summary><b>Yordle: An Efficient Imitation Learning for Branch and Bound</b>
<a href="https://arxiv.org/abs/2202.01896">arxiv:2202.01896</a>
&#x1F4C8; 4 <br>
<p>Qingyu Qu, Xijun Li, Yunfan Zhou</p></summary>
<p>

**Abstract:** Combinatorial optimization problems have aroused extensive research interests due to its huge application potential. In practice, there are highly redundant patterns and characteristics during solving the combinatorial optimization problem, which can be captured by machine learning models. Thus, the 2021 NeurIPS Machine Learning for Combinatorial Optimization (ML4CO) competition is proposed with the goal of improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning techniques. This work presents our solution and insights gained by team qqy in the dual task of the competition. Our solution is a highly efficient imitation learning framework for performance improvement of Branch and Bound (B&B), named Yordle. It employs a hybrid sampling method and an efficient data selection method, which not only accelerates the model training but also improves the decision quality during branching variable selection. In our experiments, Yordle greatly outperforms the baseline algorithm adopted by the competition while requiring significantly less time and amounts of data to train the decision model. Specifically, we use only 1/4 of the amount of data compared to that required for the baseline algorithm, to achieve around 50% higher score than baseline algorithm. The proposed framework Yordle won the championship of the student leaderboard.

</p>
</details>

<details><summary><b>A Comparison of Online Hate on Reddit and 4chan: A Case Study of the 2020 US Election</b>
<a href="https://arxiv.org/abs/2202.01302">arxiv:2202.01302</a>
&#x1F4C8; 4 <br>
<p>Fatima Zahrah, Jason R. C. Nurse, Michael Goldsmith</p></summary>
<p>

**Abstract:** The rapid integration of the Internet into our daily lives has led to many benefits but also to a number of new, wide-spread threats such as online hate, trolling, bullying, and generally aggressive behaviours. While research has traditionally explored online hate, in particular, on one platform, the reality is that such hate is a phenomenon that often makes use of multiple online networks. In this article, we seek to advance the discussion into online hate by harnessing a comparative approach, where we make use of various Natural Language Processing (NLP) techniques to computationally analyse hateful content from Reddit and 4chan relating to the 2020 US Presidential Elections. Our findings show how content and posting activity can differ depending on the platform being used. Through this, we provide initial comparison into the platform-specific behaviours of online hate, and how different platforms can serve specific purposes. We further provide several avenues for future research utilising a cross-platform approach so as to gain a more comprehensive understanding of the global hate ecosystem.

</p>
</details>

<details><summary><b>An Experience Report of Executive-Level Artificial Intelligence Education in the United Arab Emirates</b>
<a href="https://arxiv.org/abs/2202.01281">arxiv:2202.01281</a>
&#x1F4C8; 4 <br>
<p>David Johnson, Mohammad Alsharid, Rasheed El-Bouri, Nigel Mehdi, Farah Shamout, Alexandre Szenicer, David Toman, Saqr Binghalib</p></summary>
<p>

**Abstract:** Teaching artificial intelligence (AI) is challenging. It is a fast moving field and therefore difficult to keep people updated with the state-of-the-art. Educational offerings for students are ever increasing, beyond university degree programs where AI education traditionally lay. In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE). Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes. We present an overview of our course, curriculum and teaching methods, and we discuss our reflections on teaching adult learners, and to students in the UAE.

</p>
</details>

<details><summary><b>Global Optimization Networks</b>
<a href="https://arxiv.org/abs/2202.01277">arxiv:2202.01277</a>
&#x1F4C8; 4 <br>
<p>Sen Zhao, Erez Louidor, Olexander Mangylov, Maya Gupta</p></summary>
<p>

**Abstract:** We consider the problem of estimating a good maximizer of a black-box function given noisy examples. To solve such problems, we propose to fit a new type of function which we call a global optimization network (GON), defined as any composition of an invertible function and a unimodal function, whose unique global maximizer can be inferred in $\mathcal{O}(D)$ time. In this paper, we show how to construct invertible and unimodal functions by using linear inequality constraints on lattice models. We also extend to \emph{conditional} GONs that find a global maximizer conditioned on specified inputs of other dimensions. Experiments show the GON maximizers are statistically significantly better predictions than those produced by convex fits, GPR, or DNNs, and are more reasonable predictions for real-world problems.

</p>
</details>

<details><summary><b>Make Some Noise: Reliable and Efficient Single-Step Adversarial Training</b>
<a href="https://arxiv.org/abs/2202.01181">arxiv:2202.01181</a>
&#x1F4C8; 4 <br>
<p>Pau de Jorge, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip H. S. Torr, Grégory Rogez, Puneet K. Dokania</p></summary>
<p>

**Abstract:** Recently, Wong et al. showed that adversarial training with single-step FGSM leads to a characteristic failure mode named catastrophic overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. They showed that adding a random perturbation prior to FGSM (RS-FGSM) seemed to be sufficient to prevent CO. However, Andriushchenko and Flammarion observed that RS-FGSM still leads to CO for larger perturbations, and proposed an expensive regularizer (GradAlign) to avoid CO. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with not clipping is highly effective in avoiding CO for large perturbation radii. Based on these observations, we then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous single-step methods while achieving a 3$\times$ speed-up.

</p>
</details>

<details><summary><b>An Eye for an Eye: Defending against Gradient-based Attacks with Gradients</b>
<a href="https://arxiv.org/abs/2202.01117">arxiv:2202.01117</a>
&#x1F4C8; 4 <br>
<p>Hanbin Hong, Yuan Hong, Yu Kong</p></summary>
<p>

**Abstract:** Deep learning models have been shown to be vulnerable to adversarial attacks. In particular, gradient-based attacks have demonstrated high success rates recently. The gradient measures how each image pixel affects the model output, which contains critical information for generating malicious perturbations. In this paper, we show that the gradients can also be exploited as a powerful weapon to defend against adversarial attacks. By using both gradient maps and adversarial images as inputs, we propose a Two-stream Restoration Network (TRN) to restore the adversarial images. To optimally restore the perturbed images with two streams of inputs, a Gradient Map Estimation Mechanism is proposed to estimate the gradients of adversarial images, and a Fusion Block is designed in TRN to explore and fuse the information in two streams. Once trained, our TRN can defend against a wide range of attack methods without significantly degrading the performance of benign inputs. Also, our method is generalizable, scalable, and hard to bypass. Experimental results on CIFAR10, SVHN, and Fashion MNIST demonstrate that our method outperforms state-of-the-art defense methods.

</p>
</details>

<details><summary><b>Financial Vision Based Reinforcement Learning Trading Strategy</b>
<a href="https://arxiv.org/abs/2202.04115">arxiv:2202.04115</a>
&#x1F4C8; 3 <br>
<p>Yun-Cheng Tsai, Fu-Min Szu, Jun-Hao Chen, Samuel Yen-Chi Chen</p></summary>
<p>

**Abstract:** Recent advances in artificial intelligence (AI) for quantitative trading have led to its general superhuman performance in significant trading performance. However, the potential risk of AI trading is a "black box" decision. Some AI computing mechanisms are complex and challenging to understand. If we use AI without proper supervision, AI may lead to wrong choices and make huge losses. Hence, we need to ask about the AI "black box", including why did AI decide to do this or not? Why can people trust AI or not? How can people fix their mistakes? These problems also highlight the challenges that AI technology can explain in the trading field.

</p>
</details>

<details><summary><b>Proceedings 10th International Workshop on Theorem Proving Components for Educational Software</b>
<a href="https://arxiv.org/abs/2202.02144">arxiv:2202.02144</a>
&#x1F4C8; 3 <br>
<p>João Marcos, Walther Neuper, Pedro Quaresma</p></summary>
<p>

**Abstract:** This EPTCS volume contains the proceedings of the ThEdu'21 workshop, promoted on 11 July 2021, as a satellite event of CADE-28. Due to the COVID-19 pandemic, CADE-28 and all its co-located events happened as virtual events. ThEdu'21 was a vibrant workshop, with an invited talk by Gilles Dowek (ENS Paris-Saclay), eleven contributions, and one demonstration.  After the workshop an open call for papers was issued and attracted 10 submissions, 7 of which have been accepted by the reviewers, and collected in the present post-proceedings volume.
  The ThEdu series pursues the smooth transition from an intuitive way of doing mathematics at secondary school to a more formal approach to the subject in STEM education, while favouring software support for this transition by exploiting the power of theorem-proving technologies.
  The volume editors hope that this collection of papers will further promote the development of theorem-proving based software, and that it will collaborate on improving mutual understanding between computer scientists, mathematicians and stakeholders in education.

</p>
</details>

<details><summary><b>AtmoDist: Self-supervised Representation Learning for Atmospheric Dynamics</b>
<a href="https://arxiv.org/abs/2202.01897">arxiv:2202.01897</a>
&#x1F4C8; 3 <br>
<p>Sebastian Hoffmann, Christian Lessig</p></summary>
<p>

**Abstract:** Representation learning has proven to be a powerful methodology in a wide variety of machine learning applications. For atmospheric dynamics, however, it has so far not been considered, arguably due to the lack of large-scale, labeled datasets that could be used for training. In this work, we show that the difficulty is benign and introduce a self-supervised learning task that defines a categorical loss for a wide variety of unlabeled atmospheric datasets. Specifically, we train a neural network on the simple yet intricate task of predicting the temporal distance between atmospheric fields, e.g. the components of the wind field, from distinct but nearby times. Despite this simplicity, a neural network will provide good predictions only when it develops an internal representation that captures intrinsic aspects of atmospheric dynamics. We demonstrate this by introducing a data-driven distance metric for atmospheric states based on representations learned from ERA5 reanalysis. When employ as a loss function for downscaling, this Atmodist distance leads to downscaled fields that match the true statistics more closely than the previous state-of-the-art based on an l2-loss and whose local behavior is more realistic. Since it is derived from observational data, AtmoDist also provides a novel perspective on atmospheric predictability.

</p>
</details>

<details><summary><b>Fenrir: Physics-Enhanced Regression for Initial Value Problems</b>
<a href="https://arxiv.org/abs/2202.01287">arxiv:2202.01287</a>
&#x1F4C8; 3 <br>
<p>Filip Tronarp, Nathanael Bosch, Philipp Hennig</p></summary>
<p>

**Abstract:** We show how probabilistic numerics can be used to convert an initial value problem into a Gauss--Markov process parametrised by the dynamics of the initial value problem. Consequently, the often difficult problem of parameter estimation in ordinary differential equations is reduced to hyperparameter estimation in Gauss--Markov regression, which tends to be considerably easier. The method's relation and benefits in comparison to classical numerical integration and gradient matching approaches is elucidated. In particular, the method can, in contrast to gradient matching, handle partial observations, and has certain routes for escaping local optima not available to classical numerical integration. Experimental results demonstrate that the method is on par or moderately better than competing approaches.

</p>
</details>

<details><summary><b>Robust Estimation for Nonparametric Families via Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2202.01269">arxiv:2202.01269</a>
&#x1F4C8; 3 <br>
<p>Banghua Zhu, Jiantao Jiao, Michael I. Jordan</p></summary>
<p>

**Abstract:** We provide a general framework for designing Generative Adversarial Networks (GANs) to solve high dimensional robust statistics problems, which aim at estimating unknown parameter of the true distribution given adversarially corrupted samples. Prior work focus on the problem of robust mean and covariance estimation when the true distribution lies in the family of Gaussian distributions or elliptical distributions, and analyze depth or scoring rule based GAN losses for the problem. Our work extend these to robust mean estimation, second moment estimation, and robust linear regression when the true distribution only has bounded Orlicz norms, which includes the broad family of sub-Gaussian, sub-Exponential and bounded moment distributions. We also provide a different set of sufficient conditions for the GAN loss to work: we only require its induced distance function to be a cumulative density function of some light-tailed distribution, which is easily satisfied by neural networks with sigmoid activation. In terms of techniques, our proposed GAN losses can be viewed as a smoothed and generalized Kolmogorov-Smirnov distance, which overcomes the computational intractability of the original Kolmogorov-Smirnov distance used in the prior work.

</p>
</details>

<details><summary><b>L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources</b>
<a href="https://arxiv.org/abs/2202.01159">arxiv:2202.01159</a>
&#x1F4C8; 3 <br>
<p>Raviraj Joshi</p></summary>
<p>

**Abstract:** We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from different internet sources. We expand the existing Marathi monolingual corpus with 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT, and MahaRoBerta all BERT-based masked language models, and MahaFT, the fast text word embeddings both trained on full Marathi corpus with 752M tokens. We show the effectiveness of these resources on downstream classification and NER tasks. Marathi is a popular language in India but still lacks these resources. This work is a step forward in building open resources for the Marathi language. The data and models are available at https://github.com/l3cube-pune/MarathiNLP .

</p>
</details>

<details><summary><b>Improving Screening Processes via Calibrated Subset Selection</b>
<a href="https://arxiv.org/abs/2202.01147">arxiv:2202.01147</a>
&#x1F4C8; 3 <br>
<p>Lequn Wang, Thorsten Joachims, Manuel Gomez Rodriguez</p></summary>
<p>

**Abstract:** Many selection processes such as finding patients qualifying for a medical trial or retrieval pipelines in search engines consist of multiple stages, where an initial screening stage focuses the resources on shortlisting the most promising candidates. In this paper, we investigate what guarantees a screening classifier can provide, independently of whether it is constructed manually or trained. We find that current solutions do not enjoy distribution-free theoretical guarantees -- we show that, in general, even for a perfectly calibrated classifier, there always exist specific pools of candidates for which its shortlist is suboptimal. Then, we develop a distribution-free screening algorithm -- called Calibrated Subset Selection (CSS) -- that, given any classifier and some amount of calibration data, finds near-optimal shortlists of candidates that contain a desired number of qualified candidates in expectation. Moreover, we show that a variant of our algorithm that calibrates a given classifier multiple times across specific groups can create shortlists with provable diversity guarantees. Experiments on US Census survey data validate our theoretical results and show that the shortlists provided by our algorithm are superior to those provided by several competitive baselines.

</p>
</details>

<details><summary><b>Language Models Explain Word Reading Times Better Than Empirical Predictability</b>
<a href="https://arxiv.org/abs/2202.01128">arxiv:2202.01128</a>
&#x1F4C8; 3 <br>
<p>Markus J. Hofmann, Steffen Remus, Chris Biemann, Ralph Radach, Lars Kuchinke</p></summary>
<p>

**Abstract:** Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP.

</p>
</details>

<details><summary><b>Lipschitz-constrained Unsupervised Skill Discovery</b>
<a href="https://arxiv.org/abs/2202.00914">arxiv:2202.00914</a>
&#x1F4C8; 3 <br>
<p>Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, Gunhee Kim</p></summary>
<p>

**Abstract:** We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner - i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/.

</p>
</details>

<details><summary><b>Active Multi-Task Representation Learning</b>
<a href="https://arxiv.org/abs/2202.00911">arxiv:2202.00911</a>
&#x1F4C8; 3 <br>
<p>Yifang Chen, Simon S. Du, Kevin Jamieson</p></summary>
<p>

**Abstract:** To leverage the power of big data from source tasks and overcome the scarcity of the target task samples, representation learning based on multi-task pretraining has become a standard approach in many applications. However, up until now, choosing which source tasks to include in the multi-task learning has been more art than science. In this paper, we give the first formal study on resource task sampling by leveraging the techniques from active learning. We propose an algorithm that iteratively estimates the relevance of each source task to the target task and samples from each source task based on the estimated relevance. Theoretically, we show that for the linear representation class, to achieve the same error rate, our algorithm can save up to a \textit{number of source tasks} factor in the source task sample complexity, compared with the naive uniform sampling from all source tasks. We also provide experiments on real-world computer vision datasets to illustrate the effectiveness of our proposed method on both linear and convolutional neural network representation classes. We believe our paper serves as an important initial step to bring techniques from active learning to representation learning.

</p>
</details>

<details><summary><b>A Longitudinal Dataset of Twitter ISIS Users</b>
<a href="https://arxiv.org/abs/2202.00878">arxiv:2202.00878</a>
&#x1F4C8; 3 <br>
<p>Younes Karimi, Anna Squicciarini, Peter K. Forster, Kira M. Leavitt</p></summary>
<p>

**Abstract:** We present a large longitudinal dataset of tweets from two sets of users that are suspected to be affiliated with ISIS. These sets of users are identified based on a prior study and a campaign aimed at shutting down ISIS Twitter accounts. These users have engaged with known ISIS accounts at least once during 2014-2015 and are still active as of 2021. Some of them have directly supported the ISIS users and their tweets by retweeting them, and some of the users that have quoted tweets of ISIS, have uncertain connections to ISIS seed accounts. This study and the dataset represent a unique approach to analyzing ISIS data. Although much research exists on ISIS online activities, few studies have focused on individual accounts. Our approach to validating accounts as well as developing a framework for differentiating accounts' functionality (e.g., propaganda versus operational planning) offers a foundation for future research. We perform some descriptive statistics and preliminary analyses on our collected data to provide deeper insight and highlight the significance and practicality of such analyses. We further discuss several cross-disciplinary potential use cases and research directions.

</p>
</details>

<details><summary><b>Accelerating Part-Scale Simulation in Liquid Metal Jet Additive Manufacturing via Operator Learning</b>
<a href="https://arxiv.org/abs/2202.03665">arxiv:2202.03665</a>
&#x1F4C8; 2 <br>
<p>Søren Taverniers, Svyatoslav Korneev, Kyle M. Pietrzyk, Morad Behandish</p></summary>
<p>

**Abstract:** Predicting part quality for additive manufacturing (AM) processes requires high-fidelity numerical simulation of partial differential equations (PDEs) governing process multiphysics on a scale of minimum manufacturable features. This makes part-scale predictions computationally demanding, especially when they require many small-scale simulations. We consider drop-on-demand liquid metal jetting (LMJ) as an illustrative example of such computational complexity. A model describing droplet coalescence for LMJ may include coupled incompressible fluid flow, heat transfer, and phase change equations. Numerically solving these equations becomes prohibitively expensive when simulating the build process for a full part consisting of thousands to millions of droplets. Reduced-order models (ROMs) based on neural networks (NN) or k-nearest neighbor (kNN) algorithms have been built to replace the original physics-based solver and are computationally tractable for part-level simulations. However, their quick inference capabilities often come at the expense of accuracy, robustness, and generalizability. We apply an operator learning (OL) approach to learn a mapping between initial and final states of the droplet coalescence process for enabling rapid and accurate part-scale build simulation. Preliminary results suggest that OL requires order-of-magnitude fewer data points than a kNN approach and is generalizable beyond the training set while achieving similar prediction error.

</p>
</details>

<details><summary><b>IoV Scenario: Implementation of a Bandwidth Aware Algorithm in Wireless Network Communication Mode</b>
<a href="https://arxiv.org/abs/2202.03488">arxiv:2202.03488</a>
&#x1F4C8; 2 <br>
<p>Peiying Zhang, Chao Wang, Gagangeet Singh Aujla, Neeraj Kumar, Mohsen Guizani</p></summary>
<p>

**Abstract:** The wireless network communication mode represented by the Internet of vehicles (IoV) has been widely used. However, due to the limitations of traditional network architecture, resource scheduling in wireless network environment is still facing great challenges. This paper focuses on the allocation of bandwidth resources in the virtual network environment. This paper proposes a bandwidth aware multi domain virtual network embedding algorithm (BA-VNE). The algorithm is mainly aimed at the problem that users need a lot of bandwidth in wireless communication mode, and solves the problem of bandwidth resource allocation from the perspective of virtual network embedding (VNE). In order to improve the performance of the algorithm, we introduce particle swarm optimization (PSO) algorithm to optimize the performance of the algorithm. In order to verify the effectiveness of the algorithm, we have carried out simulation experiments from link bandwidth, mapping cost and virtual network request (VNR) acceptance rate. The final results show that the proposed algorithm is better than other representative algorithms in the above indicators.

</p>
</details>

<details><summary><b>Human Activity Recognition Using Tools of Convolutional Neural Networks: A State of the Art Review, Data Sets, Challenges and Future Prospects</b>
<a href="https://arxiv.org/abs/2202.03274">arxiv:2202.03274</a>
&#x1F4C8; 2 <br>
<p>Md. Milon Islam, Sheikh Nooruddin, Fakhri Karray, Ghulam Muhammad</p></summary>
<p>

**Abstract:** Human Activity Recognition (HAR) plays a significant role in the everyday life of people because of its ability to learn extensive high-level information about human activity from wearable or stationary devices. A substantial amount of research has been conducted on HAR and numerous approaches based on deep learning and machine learning have been exploited by the research community to classify human activities. The main goal of this review is to summarize recent works based on a wide range of deep neural networks architecture, namely convolutional neural networks (CNNs) for human activity recognition. The reviewed systems are clustered into four categories depending on the use of input devices like multimodal sensing devices, smartphones, radar, and vision devices. This review describes the performances, strengths, weaknesses, and the used hyperparameters of CNN architectures for each reviewed system with an overview of available public data sources. In addition, a discussion with the current challenges to CNN-based HAR systems is presented. Finally, this review is concluded with some potential future directions that would be of great assistance for the researchers who would like to contribute to this field.

</p>
</details>

<details><summary><b>Security-Aware Virtual Network Embedding Algorithm based on Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2202.02452">arxiv:2202.02452</a>
&#x1F4C8; 2 <br>
<p>Peiying Zhang, Chao Wang, Chunxiao Jiang, Abderrahim Benslimane</p></summary>
<p>

**Abstract:** Virtual network embedding (VNE) algorithm is always the key problem in network virtualization (NV) technology. At present, the research in this field still has the following problems. The traditional way to solve VNE problem is to use heuristic algorithm. However, this method relies on manual embedding rules, which does not accord with the actual situation of VNE. In addition, as the use of intelligent learning algorithm to solve the problem of VNE has become a trend, this method is gradually outdated. At the same time, there are some security problems in VNE. However, there is no intelligent algorithm to solve the security problem of VNE. For this reason, this paper proposes a security-aware VNE algorithm based on reinforcement learning (RL). In the training phase, we use a policy network as a learning agent and take the extracted attributes of the substrate nodes to form a feature matrix as input. The learning agent is trained in this environment to get the mapping probability of each substrate node. In the test phase, we map nodes according to the mapping probability and use the breadth-first strategy (BFS) to map links. For the security problem, we add security requirements level constraint for each virtual node and security level constraint for each substrate node. Virtual nodes can only be embedded on substrate nodes that are not lower than the level of security requirements. Experimental results show that the proposed algorithm is superior to other typical algorithms in terms of long-term average return, long-term revenue consumption ratio and virtual network request (VNR) acceptance rate.

</p>
</details>

<details><summary><b>Even Simpler Deterministic Matrix Sketching</b>
<a href="https://arxiv.org/abs/2202.01780">arxiv:2202.01780</a>
&#x1F4C8; 2 <br>
<p>Edo Liberty</p></summary>
<p>

**Abstract:** This paper provides a one-line proof of Frequent Directions (FD) for sketching streams of matrices. The simpler proof arises from sketching the covariance of the stream of matrices rather than the stream itself.

</p>
</details>

<details><summary><b>An Empirical Review of Optimization Techniques for Quantum Variational Circuits</b>
<a href="https://arxiv.org/abs/2202.01389">arxiv:2202.01389</a>
&#x1F4C8; 2 <br>
<p>Owen Lockwood</p></summary>
<p>

**Abstract:** Quantum Variational Circuits (QVCs) are often claimed as one of the most potent uses of both near term and long term quantum hardware. The standard approaches to optimizing these circuits rely on a classical system to compute the new parameters at every optimization step. However, this process can be extremely challenging, due to the nature of navigating the exponentially scaling complex Hilbert space, barren plateaus, and the noise present in all foreseeable quantum hardware. Although a variety of optimization algorithms are employed in practice, there is often a lack of theoretical or empirical motivations for this choice. To this end we empirically evaluate the potential of many common gradient and gradient free optimizers on a variety of optimization tasks. These tasks include both classical and quantum data based optimization routines. Our evaluations were conducted in both noise free and noisy simulations. The large number of problems and optimizers evaluated yields strong empirical guidance for choosing optimizers for QVCs that is currently lacking.

</p>
</details>

<details><summary><b>Cyclical Pruning for Sparse Neural Networks</b>
<a href="https://arxiv.org/abs/2202.01290">arxiv:2202.01290</a>
&#x1F4C8; 2 <br>
<p>Suraj Srinivas, Andrey Kuzmin, Markus Nagel, Mart van Baalen, Andrii Skliar, Tijmen Blankevoort</p></summary>
<p>

**Abstract:** Current methods for pruning neural network weights iteratively apply magnitude-based pruning on the model weights and re-train the resulting model to recover lost accuracy. In this work, we show that such strategies do not allow for the recovery of erroneously pruned weights. To enable weight recovery, we propose a simple strategy called \textit{cyclical pruning} which requires the pruning schedule to be periodic and allows for weights pruned erroneously in one cycle to recover in subsequent ones. Experimental results on both linear models and large-scale deep neural networks show that cyclical pruning outperforms existing pruning algorithms, especially at high sparsity ratios. Our approach is easy to tune and can be readily incorporated into existing pruning pipelines to boost performance.

</p>
</details>

<details><summary><b>Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features</b>
<a href="https://arxiv.org/abs/2202.01273">arxiv:2202.01273</a>
&#x1F4C8; 2 <br>
<p>Zhaowei Zhu, Jialu Wang, Yang Liu</p></summary>
<p>

**Abstract:** The label noise transition matrix, denoting the transition probabilities from clean labels to noisy labels, is crucial knowledge for designing statistically robust solutions. Existing estimators for noise transition matrices, e.g., using either anchor points or clusterability, focus on computer vision tasks that are relatively easier to obtain high-quality representations. However, for other tasks with lower-quality features, the uninformative variables may obscure the useful counterpart and make anchor-point or clusterability conditions hard to satisfy. We empirically observe the failures of these approaches on a number of commonly used datasets. In this paper, to handle this issue, we propose a generally practical information-theoretic approach to down-weight the less informative parts of the lower-quality features. The salient technical challenge is to compute the relevant information-theoretical metrics using only noisy labels instead of clean ones. We prove that the celebrated $f$-mutual information measure can often preserve the order when calculated using noisy labels. The necessity and effectiveness of the proposed method is also demonstrated by evaluating the estimation error on a varied set of tabular data and text classification tasks with lower-quality features. Code is available at github.com/UCSC-REAL/Est-T-MI.

</p>
</details>

<details><summary><b>NoisyMix: Boosting Robustness by Combining Data Augmentations, Stability Training, and Noise Injections</b>
<a href="https://arxiv.org/abs/2202.01263">arxiv:2202.01263</a>
&#x1F4C8; 2 <br>
<p>N. Benjamin Erichson, Soon Hoe Lim, Francisco Utrera, Winnie Xu, Ziang Cao, Michael W. Mahoney</p></summary>
<p>

**Abstract:** For many real-world applications, obtaining stable and robust statistical performance is more important than simply achieving state-of-the-art predictive test accuracy, and thus robustness of neural networks is an increasingly important topic. Relatedly, data augmentation schemes have been shown to improve robustness with respect to input perturbations and domain shifts. Motivated by this, we introduce NoisyMix, a training scheme that combines data augmentations with stability training and noise injections to improve both model robustness and in-domain accuracy. This combination promotes models that are consistently more robust and that provide well-calibrated estimates of class membership probabilities. We demonstrate the benefits of NoisyMix on a range of benchmark datasets, including ImageNet-C, ImageNet-R, and ImageNet-P. Moreover, we provide theory to understand implicit regularization and robustness of NoisyMix.

</p>
</details>

<details><summary><b>PolarDenseNet: A Deep Learning Model for CSI Feedback in MIMO Systems</b>
<a href="https://arxiv.org/abs/2202.01246">arxiv:2202.01246</a>
&#x1F4C8; 2 <br>
<p>Pranav Madadi, Jeongho Jeon, Joonyoung Cho, Caleb Lo, Juho Lee, Jianzhong Zhang</p></summary>
<p>

**Abstract:** In multiple-input multiple-output (MIMO) systems, the high-resolution channel information (CSI) is required at the base station (BS) to ensure optimal performance, especially in the case of multi-user MIMO (MU-MIMO) systems. In the absence of channel reciprocity in frequency division duplex (FDD) systems, the user needs to send the CSI to the BS. Often the large overhead associated with this CSI feedback in FDD systems becomes the bottleneck in improving the system performance. In this paper, we propose an AI-based CSI feedback based on an auto-encoder architecture that encodes the CSI at UE into a low-dimensional latent space and decodes it back at the BS by effectively reducing the feedback overhead while minimizing the loss during recovery. Our simulation results show that the AI-based proposed architecture outperforms the state-of-the-art high-resolution linear combination codebook using the DFT basis adopted in the 5G New Radio (NR) system.

</p>
</details>

<details><summary><b>Approximate Bisimulation Relations for Neural Networks and Application to Assured Neural Network Compression</b>
<a href="https://arxiv.org/abs/2202.01214">arxiv:2202.01214</a>
&#x1F4C8; 2 <br>
<p>Weiming Xiang, Zhongzhu Shao</p></summary>
<p>

**Abstract:** In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.

</p>
</details>

<details><summary><b>Error Correction in ASR using Sequence-to-Sequence Models</b>
<a href="https://arxiv.org/abs/2202.01157">arxiv:2202.01157</a>
&#x1F4C8; 2 <br>
<p>Samrat Dutta, Shreyansh Jain, Ayush Maheshwari, Ganesh Ramakrishnan, Preethi Jyothi</p></summary>
<p>

**Abstract:** Post-editing in Automatic Speech Recognition (ASR) entails automatically correcting common and systematic errors produced by the ASR system. The outputs of an ASR system are largely prone to phonetic and spelling errors. In this paper, we propose to use a powerful pre-trained sequence-to-sequence model, BART, further adaptively trained to serve as a denoising model, to correct errors of such types. The adaptive training is performed on an augmented dataset obtained by synthetically inducing errors as well as by incorporating actual errors from an existing ASR system. We also propose a simple approach to rescore the outputs using word level alignments. Experimental results on accented speech data demonstrate that our strategy effectively rectifies a significant number of ASR errors and produces improved WER results when compared against a competitive baseline.

</p>
</details>

<details><summary><b>Unpaired Image Super-Resolution with Optimal Transport Maps</b>
<a href="https://arxiv.org/abs/2202.01116">arxiv:2202.01116</a>
&#x1F4C8; 2 <br>
<p>Milena Gazdieva, Litu Rout, Alexander Korotin, Alexander Filippov, Evgeny Burnaev</p></summary>
<p>

**Abstract:** Real-world image super-resolution (SR) tasks often do not have paired datasets limiting the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs) which yield complex training losses with several regularization terms such as content and identity losses. We theoretically investigate the optimization problems which arise in such models and find two surprising observations. First, the learned SR map is always an optimal transport (OT) map. Second, we empirically show that the learned map is biased, i.e., it may not actually transform the distribution of low-resolution images to high-resolution images. Inspired by these findings, we propose an algorithm for unpaired SR which learns an unbiased OT map for the perceptual transport cost. Unlike existing GAN-based alternatives, our algorithm has a simple optimization objective reducing the neccesity to perform complex hyperparameter selection and use additional regularizations. At the same time, it provides nearly state-of-the-art performance on the large-scale unpaired AIM-19 dataset.

</p>
</details>

<details><summary><b>TONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic Music</b>
<a href="https://arxiv.org/abs/2202.00951">arxiv:2202.00951</a>
&#x1F4C8; 2 <br>
<p>Ke Chen, Shuai Yu, Cheng-i Wang, Wei Li, Taylor Berg-Kirkpatrick, Shlomo Dubnov</p></summary>
<p>

**Abstract:** Singing melody extraction is an important problem in the field of music information retrieval. Existing methods typically rely on frequency-domain representations to estimate the sung frequencies. However, this design does not lead to human-level performance in the perception of melody information for both tone (pitch-class) and octave. In this paper, we propose TONet, a plug-and-play model that improves both tone and octave perceptions by leveraging a novel input representation and a novel network architecture. First, we present an improved input representation, the Tone-CFP, that explicitly groups harmonics via a rearrangement of frequency-bins. Second, we introduce an encoder-decoder architecture that is designed to obtain a salience feature map, a tone feature map, and an octave feature map. Third, we propose a tone-octave fusion mechanism to improve the final salience feature map. Experiments are done to verify the capability of TONet with various baseline backbone models. Our results show that tone-octave fusion with Tone-CFP can significantly improve the singing voice extraction performance across various datasets -- with substantial gains in octave and tone accuracy.

</p>
</details>

<details><summary><b>Eikonal Fields for Refractive Novel-View Synthesis</b>
<a href="https://arxiv.org/abs/2202.00948">arxiv:2202.00948</a>
&#x1F4C8; 2 <br>
<p>Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, Tobias Ritschel</p></summary>
<p>

**Abstract:** We tackle the problem of generating novel-view images from collections of 2D images showing refractive and reflective objects. Current solutions assume opaque or transparent light transport along straight paths following the emission-absorption model. Instead, we optimize for a field of 3D-varying Index of Refraction (IoR) and trace light through it that bends toward the spatial gradients of said IoR according to the laws of eikonal light transport.

</p>
</details>

<details><summary><b>Non-Stationary Dueling Bandits</b>
<a href="https://arxiv.org/abs/2202.00935">arxiv:2202.00935</a>
&#x1F4C8; 2 <br>
<p>Patrick Kolpaczki, Viktor Bengs, Eyke Hüllermeier</p></summary>
<p>

**Abstract:** We study the non-stationary dueling bandits problem with $K$ arms, where the time horizon $T$ consists of $M$ stationary segments, each of which is associated with its own preference matrix. The learner repeatedly selects a pair of arms and observes a binary preference between them as feedback. To minimize the accumulated regret, the learner needs to pick the Condorcet winner of each stationary segment as often as possible, despite preference matrices and segment lengths being unknown. We propose the $\mathrm{Beat\, the\, Winner\, Reset}$ algorithm and prove a bound on its expected binary weak regret in the stationary case, which tightens the bound of current state-of-art algorithms. We also show a regret bound for the non-stationary case, without requiring knowledge of $M$ or $T$. We further propose and analyze two meta-algorithms, $\mathrm{DETECT}$ for weak regret and $\mathrm{Monitored\, Dueling\, Bandits}$ for strong regret, both based on a detection-window approach that can incorporate any dueling bandit algorithm as a black-box algorithm. Finally, we prove a worst-case lower bound for expected weak regret in the non-stationary case.

</p>
</details>

<details><summary><b>Spectro Temporal EEG Biomarkers For Binary Emotion Classification</b>
<a href="https://arxiv.org/abs/2202.03271">arxiv:2202.03271</a>
&#x1F4C8; 1 <br>
<p>Upasana Tiwari, Rupayan Chakraborty, Sunil Kumar Kopparapu</p></summary>
<p>

**Abstract:** Electroencephalogram (EEG) is one of the most reliable physiological signal for emotion detection. Being non-stationary in nature, EEGs are better analysed by spectro temporal representations. Standard features like Discrete Wavelet Transformation (DWT) can represent temporal changes in spectral dynamics of an EEG, but is insufficient to extract information other way around, i.e. spectral changes in temporal dynamics. On the other hand, Empirical mode decomposition (EMD) based features can be useful to bridge the above mentioned gap. Towards this direction, we extract two novel features on top of EMD, namely, (a) marginal hilbert spectrum (MHS) and (b) Holo-Hilbert spectral analysis (HHSA) based on EMD, to better represent emotions in 2D arousal-valence (A-V) space. The usefulness of these features for EEG emotion classification is investigated through extensive experiments using state-of-the-art classifiers. In addition, experiments conducted on DEAP dataset for binary emotion classification in both A-V space, reveal the efficacy of the proposed features over the standard set of temporal and spectral features.

</p>
</details>

<details><summary><b>Space-Air-Ground Integrated Multi-domain Network Resource Orchestration based on Virtual Network Architecture: a DRL Method</b>
<a href="https://arxiv.org/abs/2202.02459">arxiv:2202.02459</a>
&#x1F4C8; 1 <br>
<p>Peiying Zhang, Chao Wang, Neeraj Kumar, Lei Liu</p></summary>
<p>

**Abstract:** Traditional ground wireless communication networks cannot provide high-quality services for artificial intelligence (AI) applications such as intelligent transportation systems (ITS) due to deployment, coverage and capacity issues. The space-air-ground integrated network (SAGIN) has become a research focus in the industry. Compared with traditional wireless communication networks, SAGIN is more flexible and reliable, and it has wider coverage and higher quality of seamless connection. However, due to its inherent heterogeneity, time-varying and self-organizing characteristics, the deployment and use of SAGIN still faces huge challenges, among which the orchestration of heterogeneous resources is a key issue. Based on virtual network architecture and deep reinforcement learning (DRL), we model SAGIN's heterogeneous resource orchestration as a multi-domain virtual network embedding (VNE) problem, and propose a SAGIN cross-domain VNE algorithm. We model the different network segments of SAGIN, and set the network attributes according to the actual situation of SAGIN and user needs. In DRL, the agent is acted by a five-layer policy network. We build a feature matrix based on network attributes extracted from SAGIN and use it as the agent training environment. Through training, the probability of each underlying node being embedded can be derived. In test phase, we complete the embedding process of virtual nodes and links in turn based on this probability. Finally, we verify the effectiveness of the algorithm from both training and testing.

</p>
</details>

<details><summary><b>Performance of multilabel machine learning models and risk stratification schemas for predicting stroke and bleeding risk in patients with non-valvular atrial fibrillation</b>
<a href="https://arxiv.org/abs/2202.01975">arxiv:2202.01975</a>
&#x1F4C8; 1 <br>
<p>Juan Lu, Rebecca Hutchens, Joseph Hung, Mohammed Bennamoun, Brendan McQuillan, Tom Briffa, Ferdous Sohel, Kevin Murray, Jonathon Stewart, Benjamin Chow, Frank Sanfilippo, Girish Dwivedi</p></summary>
<p>

**Abstract:** Appropriate antithrombotic therapy for patients with atrial fibrillation (AF) requires assessment of ischemic stroke and bleeding risks. However, risk stratification schemas such as CHA2DS2-VASc and HAS-BLED have modest predictive capacity for patients with AF. Machine learning (ML) techniques may improve predictive performance and support decision-making for appropriate antithrombotic therapy. We compared the performance of multilabel ML models with the currently used risk scores for predicting outcomes in AF patients. Materials and Methods This was a retrospective cohort study of 9670 patients, mean age 76.9 years, 46% women, who were hospitalized with non-valvular AF, and had 1-year follow-up. The primary outcome was ischemic stroke and major bleeding admission. The secondary outcomes were all-cause death and event-free survival. The discriminant power of ML models was compared with clinical risk scores by the area under the curve (AUC). Risk stratification was assessed using the net reclassification index. Results Multilabel gradient boosting machine provided the best discriminant power for stroke, major bleeding, and death (AUC = 0.685, 0.709, and 0.765 respectively) compared to other ML models. It provided modest performance improvement for stroke compared to CHA2DS2-VASc (AUC = 0.652), but significantly improved major bleeding prediction compared to HAS-BLED (AUC = 0.522). It also had a much greater discriminant power for death compared with CHA2DS2-VASc (AUC = 0.606). Also, models identified additional risk features (such as hemoglobin level, renal function, etc.) for each outcome. Conclusions Multilabel ML models can outperform clinical risk stratification scores for predicting the risk of major bleeding and death in non-valvular AF patients.

</p>
</details>

<details><summary><b>Fair Representation Clustering with Several Protected Classes</b>
<a href="https://arxiv.org/abs/2202.01391">arxiv:2202.01391</a>
&#x1F4C8; 1 <br>
<p>Zhen Dai, Yury Makarychev, Ali Vakilian</p></summary>
<p>

**Abstract:** We study the problem of fair $k$-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation $k$-median problem, we are given a set of points $X$ in a metric space. Each point $x\in X$ belongs to one of $\ell$ groups. Further, we are given fair representation parameters $α_j$ and $β_j$ for each group $j\in [\ell]$. We say that a $k$-clustering $C_1, \cdots, C_k$ fairly represents all groups if the number of points from group $j$ in cluster $C_i$ is between $α_j |C_i|$ and $β_j |C_i|$ for every $j\in[\ell]$ and $i\in [k]$. The goal is to find a set $\mathcal{C}$ of $k$ centers and an assignment $φ: X\rightarrow \mathcal{C}$ such that the clustering defined by $(\mathcal{C}, φ)$ fairly represents all groups and minimizes the $\ell_1$-objective $\sum_{x\in X} d(x, φ(x))$.
  We present an $O(\log k)$-approximation algorithm that runs in time $n^{O(\ell)}$. Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both $k$ and $\ell$. We also consider an important special case of the problem where $α_j = β_j = \frac{f_j}{f}$ and $f_j, f \in \mathbb{N}$ for all $j\in [\ell]$. For this special case, we present an $O(\log k)$-approximation algorithm that runs in $(kf)^{O(\ell)}\log n + poly(n)$ time.

</p>
</details>

<details><summary><b>Machine Learning Solar Wind Driving Magnetospheric Convection in Tail Lobes</b>
<a href="https://arxiv.org/abs/2202.01383">arxiv:2202.01383</a>
&#x1F4C8; 1 <br>
<p>Xin Cao, Jasper S. Halekas, Stein Haaland, Suranga Ruhunusiri, Karl-Heinz Glassmeier</p></summary>
<p>

**Abstract:** To quantitatively study the driving mechanisms of magnetospheric convection in the magnetotail lobes on a global scale, we utilize data from the ARTEMIS spacecraft in the deep tail and the Cluster spacecraft in the near tail. Previous work demonstrated that, in the lobes near the Moon, we can estimate the convection by utilizing ARTEMIS measurements of lunar ions velocity. In this paper, we analyze these datasets with machine learning models to determine what upstream factors drive the lobe convection in different magnetotail regions and thereby understand the mechanisms that control the dynamics of the tail lobes. Our results show that the correlations between the predicted and test convection velocities for the machine learning models (>0.75) are much better than those of the multiple linear regression model (~ 0.23 - 0.43). The systematic analysis reveals that the IMF and magnetospheric activity play an important role in influencing plasma convection in the global magnetotail lobes.

</p>
</details>

<details><summary><b>Gradient estimators for normalising flows</b>
<a href="https://arxiv.org/abs/2202.01314">arxiv:2202.01314</a>
&#x1F4C8; 1 <br>
<p>Piotr Bialas, Piotr Korcyl, Tomasz Stebel</p></summary>
<p>

**Abstract:** Recently a machine learning approach to Monte-Carlo simulations called Neural Markov Chain Monte-Carlo (NMCMC) is gaining traction. In its most popular form it uses the neural networks to construct normalizing flows which are then trained to approximate the desired target distribution. As this distribution is usually defined via a Hamiltonian or action, the standard learning algorithm requires estimation of the action gradient with respect to the fields. In this contribution we present another gradient estimator (and the corresponding [PyTorch implementation) that avoids this calculation, thus potentially speeding up training for models with more complicated actions. We also study the statistical properties of several gradient estimators and show that our formulation leads to better training results.

</p>
</details>

<details><summary><b>Multi-Resolution Factor Graph Based Stereo Correspondence Algorithm</b>
<a href="https://arxiv.org/abs/2202.01309">arxiv:2202.01309</a>
&#x1F4C8; 1 <br>
<p>Hanieh Shabanian, Madhusudhanan Balasubramanian</p></summary>
<p>

**Abstract:** A dense depth-map of a scene at an arbitrary view orientation can be estimated from dense view correspondences among multiple lower-dimensional views of the scene. These low-dimensional view correspondences are dependent on the geometrical relationship among the views and the scene. Determining dense view correspondences is difficult in part due to presence of homogeneous regions in the scene and due to presence of occluded regions and illumination differences among the views. We present a new multi-resolution factor graph-based stereo matching algorithm (MR-FGS) that utilizes both intra- and inter-resolution dependencies among the views as well as among the disparity estimates. The proposed framework allows exchange of information among multiple resolutions of the correspondence problem and is useful for handling larger homogeneous regions in a scene. The MR-FGS algorithm was evaluated qualitatively and quantitatively using stereo pairs in the Middlebury stereo benchmark dataset based on commonly used performance measures. When compared to a recently developed factor graph model (FGS), the MR-FGS algorithm provided more accurate disparity estimates without requiring the commonly used post-processing procedure known as the left-right consistency check. The multi-resolution dependency constraint within the factor-graph model significantly improved contrast along depth boundaries in the MR-FGS generated disparity maps.

</p>
</details>

<details><summary><b>Harmony: Overcoming the hurdles of GPU memory capacity to train massive DNN models on commodity servers</b>
<a href="https://arxiv.org/abs/2202.01306">arxiv:2202.01306</a>
&#x1F4C8; 1 <br>
<p>Youjie Li, Amar Phanishayee, Derek Murray, Jakub Tarnawski, Nam Sung Kim</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) have grown exponentially in complexity and size over the past decade, leaving only those who have access to massive datacenter-based resources with the ability to develop and train such models. One of the main challenges for the long tail of researchers who might have access to only limited resources (e.g., a single multi-GPU server) is limited GPU memory capacity compared to model size. The problem is so acute that the memory requirement of training large DNN models can often exceed the aggregate capacity of all available GPUs on commodity servers; this problem only gets worse with the trend of ever-growing model sizes. Current solutions that rely on virtualizing GPU memory (by swapping to/from CPU memory) incur excessive swapping overhead. In this paper, we present a new training framework, Harmony, and advocate rethinking how DNN frameworks schedule computation and move data to push the boundaries of training large models efficiently on modest multi-GPU deployments. Across many large DNN models, Harmony is able to reduce swap load by up to two orders of magnitude and obtain a training throughput speedup of up to 7.6x over highly optimized baselines with virtualized memory.

</p>
</details>

<details><summary><b>FedSpace: An Efficient Federated Learning Framework at Satellites and Ground Stations</b>
<a href="https://arxiv.org/abs/2202.01267">arxiv:2202.01267</a>
&#x1F4C8; 1 <br>
<p>Jinhyun So, Kevin Hsieh, Behnaz Arzani, Shadi Noghabi, Salman Avestimehr, Ranveer Chandra</p></summary>
<p>

**Abstract:** Large-scale deployments of low Earth orbit (LEO) satellites collect massive amount of Earth imageries and sensor data, which can empower machine learning (ML) to address global challenges such as real-time disaster navigation and mitigation. However, it is often infeasible to download all the high-resolution images and train these ML models on the ground because of limited downlink bandwidth, sparse connectivity, and regularization constraints on the imagery resolution. To address these challenges, we leverage Federated Learning (FL), where ground stations and satellites collaboratively train a global ML model without sharing the captured images on the satellites. We show fundamental challenges in applying existing FL algorithms among satellites and ground stations, and we formulate an optimization problem which captures a unique trade-off between staleness and idleness. We propose a novel FL framework, named FedSpace, which dynamically schedules model aggregation based on the deterministic and time-varying connectivity according to satellite orbits. Extensive numerical evaluations based on real-world satellite images and satellite networks show that FedSpace reduces the training time by 1.7 days (38.6%) over the state-of-the-art FL algorithms.

</p>
</details>

<details><summary><b>Efficient Memory Partitioning in Software Defined Hardware</b>
<a href="https://arxiv.org/abs/2202.01261">arxiv:2202.01261</a>
&#x1F4C8; 1 <br>
<p>Matthew Feldman, Tian Zhao, Kunle Olukotun</p></summary>
<p>

**Abstract:** As programmers turn to software-defined hardware (SDH) to maintain a high level of productivity while programming hardware to run complex algorithms, heavy-lifting must be done by the compiler to automatically partition on-chip arrays. In this paper, we introduce an automatic memory partitioning system that can quickly compute more efficient partitioning schemes than prior systems. Our system employs a variety of resource-saving optimizations and an ML cost model to select the best partitioning scheme from an array of candidates. We compared our system against various state-of-the-art SDH compilers and FPGAs on a variety of benchmarks and found that our system generates solutions that, on average, consume 40.3% fewer logic resources, 78.3% fewer FFs, 54.9% fewer Block RAMs (BRAMs), and 100% fewer DSPs.

</p>
</details>

<details><summary><b>Accelerated Quality-Diversity for Robotics through Massive Parallelism</b>
<a href="https://arxiv.org/abs/2202.01258">arxiv:2202.01258</a>
&#x1F4C8; 1 <br>
<p>Bryan Lim, Maxime Allard, Luca Grillotti, Antoine Cully</p></summary>
<p>

**Abstract:** Quality-Diversity (QD) algorithms are a well-known approach to generate large collections of diverse and high-quality policies. However, QD algorithms are also known to be data-inefficient, requiring large amounts of computational resources and are slow when used in practice for robotics tasks. Policy evaluations are already commonly performed in parallel to speed up QD algorithms but have limited capabilities on a single machine as most physics simulators run on CPUs. With recent advances in simulators that run on accelerators, thousands of evaluations can performed in parallel on single GPU/TPU. In this paper, we present QDax, an implementation of MAP-Elites which leverages massive parallelism on accelerators to make QD algorithms more accessible. We first demonstrate the improvements on the number of evaluations per second that parallelism using accelerated simulators can offer. More importantly, we show that QD algorithms are ideal candidates and can scale with massive parallelism to be run at interactive timescales. The increase in parallelism does not significantly affect the performance of QD algorithms, while reducing experiment runtimes by two factors of magnitudes, turning days of computation into minutes. These results show that QD can now benefit from hardware acceleration, which contributed significantly to the bloom of deep learning.

</p>
</details>

<details><summary><b>Training Semantic Descriptors for Image-Based Localization</b>
<a href="https://arxiv.org/abs/2202.01212">arxiv:2202.01212</a>
&#x1F4C8; 1 <br>
<p>Ibrahim Cinaroglu, Yalin Bastanlar</p></summary>
<p>

**Abstract:** Vision based solutions for the localization of vehicles have become popular recently. We employ an image retrieval based visual localization approach. The database images are kept with GPS coordinates and the location of the retrieved database image serves as an approximate position of the query image. We show that localization can be performed via descriptors solely extracted from semantically segmented images. It is reliable especially when the environment is subjected to severe illumination and seasonal changes. Our experiments reveal that the localization performance of a semantic descriptor can increase up to the level of state-of-the-art RGB image based methods.

</p>
</details>

<details><summary><b>Probabilistically Robust Learning: Balancing Average- and Worst-case Performance</b>
<a href="https://arxiv.org/abs/2202.01136">arxiv:2202.01136</a>
&#x1F4C8; 1 <br>
<p>Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani</p></summary>
<p>

**Abstract:** Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness.

</p>
</details>

<details><summary><b>Image-based Navigation in Real-World Environments via Multiple Mid-level Representations: Fusion Models, Benchmark and Efficient Evaluation</b>
<a href="https://arxiv.org/abs/2202.01069">arxiv:2202.01069</a>
&#x1F4C8; 1 <br>
<p>Marco Rosano, Antonino Furnari, Luigi Gulino, Corrado Santoro, Giovanni Maria Farinella</p></summary>
<p>

**Abstract:** Navigating complex indoor environments requires a deep understanding of the space the robotic agent is acting into to correctly inform the navigation process of the agent towards the goal location. In recent learning-based navigation approaches, the scene understanding and navigation abilities of the agent are achieved simultaneously by collecting the required experience in simulation. Unfortunately, even if simulators represent an efficient tool to train navigation policies, the resulting models often fail when transferred into the real world. One possible solution is to provide the navigation model with mid-level visual representations containing important domain-invariant properties of the scene. But, what are the best representations that facilitate the transfer of a model to the real-world? How can they be combined? In this work we address these issues by proposing a benchmark of Deep Learning architectures to combine a range of mid-level visual representations, to perform a PointGoal navigation task following a Reinforcement Learning setup. All the proposed navigation models have been trained with the Habitat simulator on a synthetic office environment and have been tested on the same real-world environment using a real robotic platform. To efficiently assess their performance in a real context, a validation tool has been proposed to generate realistic navigation episodes inside the simulator. Our experiments showed that navigation models can benefit from the multi-modal input and that our validation tool can provide good estimation of the expected navigation performance in the real world, while saving time and resources. The acquired synthetic and real 3D models of the environment, together with the code of our validation tool built on top of Habitat, are publicly available at the following link: https://iplab.dmi.unict.it/EmbodiedVN/

</p>
</details>

<details><summary><b>Element selection for functional materials discovery by integrated machine learning of atomic contributions to properties</b>
<a href="https://arxiv.org/abs/2202.01051">arxiv:2202.01051</a>
&#x1F4C8; 1 <br>
<p>Andrij Vasylenko, Dmytro Antypov, Vladimir Gusev, Michael W. Gaultois, Matthew S. Dyer, Matthew J. Rosseinsky</p></summary>
<p>

**Abstract:** At the high level, the fundamental differences between materials originate from the unique nature of the constituent chemical elements. Before specific differences emerge according to the precise ratios of elements (composition) in a given crystal structure (phase), the material can be represented by its phase field defined simply as the set of the constituent chemical elements. Classification of the materials at the level of their phase fields can accelerate materials discovery by selecting the elemental combinations that are likely to produce desirable functional properties in synthetically accessible materials. Here, we demonstrate that classification of the materials phase field with respect to the maximum expected value of a target functional property can be combined with the ranking of the materials synthetic accessibility. This end-to-end machine learning approach (PhaseSelect) first derives the atomic characteristics from the compositional environments in all computationally and experimentally explored materials and then employs these characteristics to classify the phase field by their merit. PhaseSelect can quantify the materials potential at the level of the periodic table, which we demonstrate with significant accuracy for three avenues of materials applications: high-temperature superconducting, high-temperature magnetic and targetted energy band gap materials.

</p>
</details>

<details><summary><b>MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray</b>
<a href="https://arxiv.org/abs/2202.01020">arxiv:2202.01020</a>
&#x1F4C8; 1 <br>
<p>Abril Corona-Figueroa, Jonathan Frawley, Sam Bond-Taylor, Sarath Bethapudi, Hubert P. H. Shum, Chris G. Willcocks</p></summary>
<p>

**Abstract:** Computed tomography (CT) is an effective medical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multiplanar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qualitative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets will be available at our GitHub.

</p>
</details>

<details><summary><b>Fairness of Machine Learning Algorithms in Demography</b>
<a href="https://arxiv.org/abs/2202.01013">arxiv:2202.01013</a>
&#x1F4C8; 1 <br>
<p>Ibe Chukwuemeka Emmanuel, Ekaterina Mitrofanova</p></summary>
<p>

**Abstract:** The paper is devoted to the study of the model fairness and process fairness of the Russian demographic dataset by making predictions of divorce of the 1st marriage, religiosity, 1st employment and completion of education. Our goal was to make classifiers more equitable by reducing their reliance on sensitive features while increasing or at least maintaining their accuracy. We took inspiration from "dropout" techniques in neural-based approaches and suggested a model that uses "feature drop-out" to address process fairness. To evaluate a classifier's fairness and decide the sensitive features to eliminate, we used "LIME Explanations". This results in a pool of classifiers due to feature dropout whose ensemble has been shown to be less reliant on sensitive features and to have improved or no effect on accuracy. Our empirical study was performed on four families of classifiers (Logistic Regression, Random Forest, Bagging, and Adaboost) and carried out on real-life dataset (Russian demographic data derived from Generations and Gender Survey), and it showed that all of the models became less dependent on sensitive features (such as gender, breakup of the 1st partnership, 1st partnership, etc.) and showed improvements or no impact on accuracy

</p>
</details>

<details><summary><b>Gradient Variance Loss for Structure-Enhanced Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2202.00997">arxiv:2202.00997</a>
&#x1F4C8; 1 <br>
<p>Lusine Abrahamyan, Anh Minh Truong, Wilfried Philips, Nikos Deligiannis</p></summary>
<p>

**Abstract:** Recent success in the field of single image super-resolution (SISR) is achieved by optimizing deep convolutional neural networks (CNNs) in the image space with the L1 or L2 loss. However, when trained with these loss functions, models usually fail to recover sharp edges present in the high-resolution (HR) images for the reason that the model tends to give a statistical average of potential HR solutions. During our research, we observe that gradient maps of images generated by the models trained with the L1 or L2 loss have significantly lower variance than the gradient maps of the original high-resolution images. In this work, we propose to alleviate the above issue by introducing a structure-enhancing loss function, coined Gradient Variance (GV) loss, and generate textures with perceptual-pleasant details. Specifically, during the training of the model, we extract patches from the gradient maps of the target and generated output, calculate the variance of each patch and form variance maps for these two images. Further, we minimize the distance between the computed variance maps to enforce the model to produce high variance gradient maps that will lead to the generation of high-resolution images with sharper edges. Experimental results show that the GV loss can significantly improve both Structure Similarity (SSIM) and peak signal-to-noise ratio (PSNR) performance of existing image super-resolution (SR) deep learning models.

</p>
</details>

<details><summary><b>Dictionary learning for clustering on hyperspectral images</b>
<a href="https://arxiv.org/abs/2202.00990">arxiv:2202.00990</a>
&#x1F4C8; 1 <br>
<p>Joshua Bruton, Hairong Wang</p></summary>
<p>

**Abstract:** Dictionary learning and sparse coding have been widely studied as mechanisms for unsupervised feature learning. Unsupervised learning could bring enormous benefit to the processing of hyperspectral images and to other remote sensing data analysis because labelled data are often scarce in this field. We propose a method for clustering the pixels of hyperspectral images using sparse coefficients computed from a representative dictionary as features. We show empirically that the proposed method works more effectively than clustering on the original pixels. We also demonstrate that our approach, in certain circumstances, outperforms the clustering results of features extracted using principal component analysis and non-negative matrix factorisation. Furthermore, our method is suitable for applications in repetitively clustering an ever-growing amount of high-dimensional data, which is the case when working with hyperspectral satellite imagery.

</p>
</details>

<details><summary><b>DCSAU-Net: A Deeper and More Compact Split-Attention U-Net for Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2202.00972">arxiv:2202.00972</a>
&#x1F4C8; 1 <br>
<p>Qing Xu, Wenting Duan, Na He</p></summary>
<p>

**Abstract:** Image segmentation is a key step for medical image analysis. Approaches based on deep neural networks have been introduced and performed more reliable results than traditional image processing methods. However, many models focus on one medical image application and still show limited abilities to work with complex images. In this paper, we propose a novel deeper and more compact split-attention u-shape network (DCSAU-Net) that extracts useful features using multi-scale combined split-attention and deeper depthwise convolution. We evaluate the proposed model on CVC-ClinicDB, 2018 Data Science Bowl, ISIC-2018 and SegPC-2021 datasets. As a result, DCSAU-Net displays better performance than other state-of-the-art (SOTA) methods in terms of the mean Intersection over Union (mIoU) and F1-socre. More significantly, the proposed model demonstrate better segmentation performance on challenging images.

</p>
</details>

<details><summary><b>Modularity-Aware Graph Autoencoders for Joint Community Detection and Link Prediction</b>
<a href="https://arxiv.org/abs/2202.00961">arxiv:2202.00961</a>
&#x1F4C8; 1 <br>
<p>Guillaume Salha-Galvan, Johannes F. Lutzeyer, George Dasoulas, Romain Hennequin, Michalis Vazirgiannis</p></summary>
<p>

**Abstract:** Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as powerful methods for link prediction. Their performances are less impressive on community detection problems where, according to recent and concurring experimental evaluations, they are often outperformed by simpler alternatives such as the Louvain method. It is currently still unclear to which extent one can improve community detection with GAE and VGAE, especially in the absence of node features. It is moreover uncertain whether one could do so while simultaneously preserving good performances on link prediction. In this paper, we show that jointly addressing these two tasks with high accuracy is possible. For this purpose, we introduce and theoretically study a community-preserving message passing scheme, doping our GAE and VGAE encoders by considering both the initial graph structure and modularity-based prior communities when computing embedding spaces. We also propose novel training and optimization strategies, including the introduction of a modularity-inspired regularizer complementing the existing reconstruction losses for joint link prediction and community detection. We demonstrate the empirical effectiveness of our approach, referred to as Modularity-Aware GAE and VGAE, through in-depth experimental validation on various real-world graphs.

</p>
</details>

<details><summary><b>CTMSTOU driven markets: simulated environment for regime-awareness in trading policies</b>
<a href="https://arxiv.org/abs/2202.00941">arxiv:2202.00941</a>
&#x1F4C8; 1 <br>
<p>Selim Amrouni, Aymeric Moulin, Tucker Balch</p></summary>
<p>

**Abstract:** Market regimes is a popular topic in quantitative finance even though there is little consensus on the details of how they should be defined. They arise as a feature both in financial market prediction problems and financial market task performing problems.
  In this work we use discrete event time multi-agent market simulation to freely experiment in a reproducible and understandable environment where regimes can be explicitly switched and enforced.
  We introduce a novel stochastic process to model the fundamental value perceived by market participants: Continuous-Time Markov Switching Trending Ornstein-Uhlenbeck (CTMSTOU), which facilitates the study of trading policies in regime switching markets.
  We define the notion of regime-awareness for a trading agent as well and illustrate its importance through the study of different order placement strategies in the context of order execution problems.

</p>
</details>

<details><summary><b>Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Care Domain</b>
<a href="https://arxiv.org/abs/2202.00916">arxiv:2202.00916</a>
&#x1F4C8; 1 <br>
<p>Kai Wang, Shresth Verma, Aditya Mate, Sanket Shah, Aparna Taneja, Neha Madhiwalla, Aparna Hegde, Milind Tambe</p></summary>
<p>

**Abstract:** This paper studies restless multi-armed bandit (RMAB) problems with unknown arm transition dynamics but with known correlated arm features. The goal is to learn a model to predict transition dynamics given features, where the Whittle index policy solves the RMAB problems using predicted transitions. However, prior works often learn the model by maximizing the predictive accuracy instead of final RMAB solution quality, causing a mismatch between training and evaluation objectives. To address this shortcoming we propose a novel approach for decision-focused learning in RMAB that directly trains the predictive model to maximize the Whittle index solution quality. We present three key contributions: (i) we establish the differentiability of the Whittle index policy to support decision-focused learning; (ii) we significantly improve the scalability of previous decision-focused learning approaches in sequential problems; (iii) we apply our algorithm to the service call scheduling problem on a real-world maternal and child health domain. Our algorithm is the first for decision-focused learning in RMAB that scales to large-scale real-world problems. \end{abstract}

</p>
</details>

<details><summary><b>Quantification and aggregation over concepts of the ontology</b>
<a href="https://arxiv.org/abs/2202.00898">arxiv:2202.00898</a>
&#x1F4C8; 1 <br>
<p>Pierre Carbonnelle, Matthias Van der Hallen, Marc Denecker</p></summary>
<p>

**Abstract:** This paper focuses on quantifications whose nature, we believe, is generally undervalued within the Knowledge Representation community: they range over a set of concepts, i.e., of intensional objects identified in the ontology. Hence, we extend first order logic to allow referring to the intension of a symbol, i.e., to the concept it represents. Our formalism is more elaboration tolerant than simpler formalisms that require reification, but also introduces the possibility of syntactically incorrect formula.We introduce a guarding mechanism to make formula syntactically correct, and present a method to verify correctness. The complexity of the method is linear with the length of the formula. We also extend FO($\cdot$) (aka FO-dot), a logic-based knowledge representation language, in a similar way, and show how it helped solve practical problems. The value of expressing intensional statements has been well-established in modal logic. We show how our approach expands on the understanding of intensions as studied in modal settings by, e.g., Fitting, in a way that is of value in non-modal settings as well.

</p>
</details>

<details><summary><b>Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint</b>
<a href="https://arxiv.org/abs/2202.00886">arxiv:2202.00886</a>
&#x1F4C8; 1 <br>
<p>Yifu Wang, Wenqing Jiang, Kun Huang, Soren Schwertfeger, Laurent Kneip</p></summary>
<p>

**Abstract:** Multi-perspective cameras are quickly gaining importance in many applications such as smart vehicles and virtual or augmented reality. However, a large system size or absence of overlap in neighbouring fields-of-view often complicate their calibration. We present a novel solution which relies on the availability of an external motion capture system. Our core contribution consists of an extension to the hand-eye calibration problem which jointly solves multi-eye-to-base problems in closed form. We furthermore demonstrate its equivalence to the multi-eye-in-hand problem. The practical validity of our approach is supported by our experiments, indicating that the method is highly efficient and accurate, and outperforms existing closed-form alternatives.

</p>
</details>

<details><summary><b>Automotive Parts Assessment: Applying Real-time Instance-Segmentation Models to Identify Vehicle Parts</b>
<a href="https://arxiv.org/abs/2202.00884">arxiv:2202.00884</a>
&#x1F4C8; 1 <br>
<p>Syed Adnan Yusuf, Abdulmalik Ali Aldawsari, Riad Souissi</p></summary>
<p>

**Abstract:** The problem of automated car damage assessment presents a major challenge in the auto repair and damage assessment industry. The domain has several application areas ranging from car assessment companies such as car rentals and body shops to accidental damage assessment for car insurance companies. In vehicle assessment, the damage can take any form including scratches, minor and major dents to missing parts. More often, the assessment area has a significant level of noise such as dirt, grease, oil or rush that makes an accurate identification challenging. Moreover, the identification of a particular part is the first step in the repair industry to have an accurate labour and part assessment where the presence of different car models, shapes and sizes makes the task even more challenging for a machine-learning model to perform well. To address these challenges, this research explores and applies various instance segmentation methodologies to evaluate the best performing models.
  The scope of this work focusses on two genres of real-time instance segmentation models due to their industrial significance, namely SipMask and Yolact. These methodologies are evaluated against a previously reported car parts dataset (DSMLR) and an internally curated dataset extracted from local car repair workshops. The Yolact-based part localization and segmentation method performed well when compared to other real-time instance mechanisms with a mAP of 66.5. For the workshop repair dataset, SipMask++ reported better accuracies for object detection with a mAP of 57.0 with outcomes for AP_IoU=.50and AP_IoU=.75 reporting 72.0 and 67.0 respectively while Yolact was found to be a better performer for AP_s with 44.0 and 2.6 for object detection and segmentation categories respectively.

</p>
</details>

<details><summary><b>Self-consistent Gradient-like Eigen Decomposition in Solving Schrödinger Equations</b>
<a href="https://arxiv.org/abs/2202.01388">arxiv:2202.01388</a>
&#x1F4C8; 0 <br>
<p>Xihan Li, Xiang Chen, Rasul Tutunov, Haitham Bou-Ammar, Lei Wang, Jun Wang</p></summary>
<p>

**Abstract:** The Schrödinger equation is at the heart of modern quantum mechanics. Since exact solutions of the ground state are typically intractable, standard approaches approximate Schrödinger equation as forms of nonlinear generalized eigenvalue problems $F(V)V = SVΛ$ in which $F(V)$, the matrix to be decomposed, is a function of its own top-$k$ smallest eigenvectors $V$, leading to a "self-consistency problem". Traditional iterative methods heavily rely on high-quality initial guesses of $V$ generated via domain-specific heuristics methods based on quantum mechanics. In this work, we eliminate such a need for domain-specific heuristics by presenting a novel framework, Self-consistent Gradient-like Eigen Decomposition (SCGLED) that regards $F(V)$ as a special "online data generator", thus allows gradient-like eigendecomposition methods in streaming $k$-PCA to approach the self-consistency of the equation from scratch in an iterative way similar to online learning. With several critical numerical improvements, SCGLED is robust to initial guesses, free of quantum-mechanism-based heuristics designs, and neat in implementation. Our experiments show that it not only can simply replace traditional heuristics-based initial guess methods with large performance advantage (achieved averagely 25x more precise than the best baseline in similar wall time), but also is capable of finding highly precise solutions independently without any traditional iterative methods.

</p>
</details>

<details><summary><b>Resource Management and Security Scheme of ICPSs and IoT Based on VNE Algorithm</b>
<a href="https://arxiv.org/abs/2202.01375">arxiv:2202.01375</a>
&#x1F4C8; 0 <br>
<p>Peiying Zhang, Chao Wang, Chunxiao Jiang, Neeraj Kumar, Qinghua Lu</p></summary>
<p>

**Abstract:** The development of Intelligent Cyber-Physical Systems (ICPSs) in virtual network environment is facing severe challenges. On the one hand, the Internet of things (IoT) based on ICPSs construction needs a large amount of reasonable network resources support. On the other hand, ICPSs are facing severe network security problems. The integration of ICPSs and network virtualization (NV) can provide more efficient network resource support and security guarantees for IoT users. Based on the above two problems faced by ICPSs, we propose a virtual network embedded (VNE) algorithm with computing, storage resources and security constraints to ensure the rationality and security of resource allocation in ICPSs. In particular, we use reinforcement learning (RL) method as a means to improve algorithm performance. We extract the important attribute characteristics of underlying network as the training environment of RL agent. Agent can derive the optimal node embedding strategy through training, so as to meet the requirements of ICPSs for resource management and security. The embedding of virtual links is based on the breadth first search (BFS) strategy. Therefore, this is a comprehensive two-stage RL-VNE algorithm considering the constraints of computing, storage and security three-dimensional resources. Finally, we design a large number of simulation experiments from the perspective of typical indicators of VNE algorithms. The experimental results effectively illustrate the effectiveness of the algorithm in the application of ICPSs.

</p>
</details>

<details><summary><b>Smoothed Embeddings for Certified Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2202.01186">arxiv:2202.01186</a>
&#x1F4C8; 0 <br>
<p>Mikhail Pautov, Olesya Kuznetsova, Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets</p></summary>
<p>

**Abstract:** Randomized smoothing is considered to be the state-of-the-art provable defense against adversarial perturbations. However, it heavily exploits the fact that classifiers map input objects to class probabilities and do not focus on the ones that learn a metric space in which classification is performed by computing distances to embeddings of classes prototypes. In this work, we extend randomized smoothing to few-shot learning models that map inputs to normalized embeddings. We provide analysis of Lipschitz continuity of such models and derive robustness certificate against $\ell_2$-bounded perturbations that may be useful in few-shot learning scenarios. Our theoretical results are confirmed by experiments on different datasets.

</p>
</details>

<details><summary><b>Pop Quiz! Can a Large Language Model Help With Reverse Engineering?</b>
<a href="https://arxiv.org/abs/2202.01142">arxiv:2202.01142</a>
&#x1F4C8; 0 <br>
<p>Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt</p></summary>
<p>

**Abstract:** Large language models (such as OpenAI's Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, LLMs are not yet ready for zero-shot reverse engineering.

</p>
</details>

<details><summary><b>Federated Reinforcement Learning for Collective Navigation of Robotic Swarms</b>
<a href="https://arxiv.org/abs/2202.01141">arxiv:2202.01141</a>
&#x1F4C8; 0 <br>
<p>Seongin Na, Tomáš Krajník, Barry Lennox, Farshad Arvin</p></summary>
<p>

**Abstract:** The recent advancement of Deep Reinforcement Learning (DRL) contributed to robotics by allowing automatic controller design. Automatic controller design is a crucial approach for designing swarm robotic systems, which require more complex controller than a single robot system to lead a desired collective behaviour. Although DRL-based controller design method showed its effectiveness, the reliance on the central training server is a critical problem in the real-world environments where the robot-server communication is unstable or limited. We propose a novel Federated Learning (FL) based DRL training strategy for use in swarm robotic applications. As FL reduces the number of robot-server communication by only sharing neural network model weights, not local data samples, the proposed strategy reduces the reliance on the central server during controller training with DRL. The experimental results from the collective learning scenario showed that the proposed FL-based strategy dramatically reduced the number of communication by minimum 1600 times and even increased the success rate of navigation with the trained controller by 2.8 times compared to the baseline strategies that share a central server. The results suggest that our proposed strategy can efficiently train swarm robotic systems in the real-world environments with the limited robot-server communication, e.g. agri-robotics, underwater and damaged nuclear facilities.

</p>
</details>

<details><summary><b>Surrogate Modeling for Physical Systems with Preserved Properties and Adjustable Tradeoffs</b>
<a href="https://arxiv.org/abs/2202.01139">arxiv:2202.01139</a>
&#x1F4C8; 0 <br>
<p>Randi Wang, Morad Behandish</p></summary>
<p>

**Abstract:** Determining the proper level of details to develop and solve physical models is usually difficult when one encounters new engineering problems. Such difficulty comes from how to balance the time (simulation cost) and accuracy for the physical model simulation afterwards. We propose a framework for automatic development of a family of surrogate models of physical systems that provide flexible cost-accuracy tradeoffs to assist making such determinations. We present both a model-based and a data-driven strategy to generate surrogate models. The former starts from a high-fidelity model generated from first principles and applies a bottom-up model order reduction (MOR) that preserves stability and convergence while providing a priori error bounds, although the resulting reduced-order model may lose its interpretability. The latter generates interpretable surrogate models by fitting artificial constitutive relations to a presupposed topological structure using experimental or simulation data. For the latter, we use Tonti diagrams to systematically produce differential equations from the assumed topological structure using algebraic topological semantics that are common to various lumped-parameter models (LPM). The parameter for the constitutive relations are estimated using standard system identification algorithms. Our framework is compatible with various spatial discretization schemes for distributed parameter models (DPM), and can supports solving engineering problems in different domains of physics.

</p>
</details>

<details><summary><b>Structure-preserving GANs</b>
<a href="https://arxiv.org/abs/2202.01129">arxiv:2202.01129</a>
&#x1F4C8; 0 <br>
<p>Jeremiah Birrell, Markos A. Katsoulakis, Luc Rey-Bellet, Wei Zhu</p></summary>
<p>

**Abstract:** Generative adversarial networks (GANs), a class of distribution-learning methods based on a two-player game between a generator and a discriminator, can generally be formulated as a minmax problem based on the variational representation of a divergence between the unknown and the generated distributions. We introduce structure-preserving GANs as a data-efficient framework for learning distributions with additional structure such as group symmetry, by developing new variational representations for divergences. Our theory shows that we can reduce the discriminator space to its projection on the invariant discriminator space, using the conditional expectation with respect to the $σ$-algebra associated to the underlying structure. In addition, we prove that the discriminator space reduction must be accompanied by a careful design of structured generators, as flawed designs may easily lead to a catastrophic "mode collapse" of the learned distribution. We contextualize our framework by building symmetry-preserving GANs for distributions with intrinsic group symmetry, and demonstrate that both players, namely the equivariant generator and invariant discriminator, play important but distinct roles in the learning process. Empirical experiments and ablation studies across a broad range of data sets, including real-world medical imaging, validate our theory, and show our proposed methods achieve significantly improved sample fidelity and diversity -- almost an order of magnitude measured in Fréchet Inception Distance -- especially in the small data regime.

</p>
</details>

<details><summary><b>GLISp-r: A preference-based optimization algorithm with convergence guarantees</b>
<a href="https://arxiv.org/abs/2202.01125">arxiv:2202.01125</a>
&#x1F4C8; 0 <br>
<p>Davide Previtali, Mirko Mazzoleni, Antonio Ferramosca, Fabio Previdi</p></summary>
<p>

**Abstract:** Preference-based optimization algorithms are iterative procedures that seek the optimal value for a decision variable based only on comparisons between couples of different samples. At each iteration, a human decision-maker is asked to express a preference between two samples, highlighting which one, if any, is better than the other. The optimization procedure must use the observed preferences to find the value of the decision variable that is most preferred by the human decision-maker, while also minimizing the number of comparisons. In this work, we propose GLISp-r, an extension of a recent preference-based optimization procedure called GLISp. The latter uses a Radial Basis Function surrogate to describe the tastes of the individual. Iteratively, GLISp proposes new samples to compare with the current best candidate by trading off exploitation of the surrogate model and exploration of the decision space. In GLISp-r, we propose a different criterion to use when looking for a new candidate sample that is inspired by MSRS, a popular procedure in the black-box optimization framework (which is closely related to the preference-based one). Compared to GLISp, GLISp-r is less likely to get stuck on local optimizers of the preference-based optimization problem. We motivate this claim theoretically, with a proof of convergence, and empirically, by comparing the performances of GLISp and GLISp-r on different benchmark optimization problems.

</p>
</details>

<details><summary><b>Communication Efficient Federated Learning for Generalized Linear Bandits</b>
<a href="https://arxiv.org/abs/2202.01087">arxiv:2202.01087</a>
&#x1F4C8; 0 <br>
<p>Chuanhao Li, Hongning Wang</p></summary>
<p>

**Abstract:** Contextual bandit algorithms have been recently studied under the federated learning setting to satisfy the demand of keeping data decentralized and pushing the learning of bandit models to the client side. But limited by the required communication efficiency, existing solutions are restricted to linear models to exploit their closed-form solutions for parameter estimation. Such a restricted model choice greatly hampers these algorithms' practical utility. In this paper, we take the first step to addressing this challenge by studying generalized linear bandit models under a federated learning setting. We propose a communication-efficient solution framework that employs online regression for local update and offline regression for global update. We rigorously proved that, though the setting is more general and challenging, our algorithm can attain sub-linear rate in both regret and communication cost, which is also validated by our extensive empirical evaluations.

</p>
</details>

<details><summary><b>Giga-scale Kernel Matrix Vector Multiplication on GPU</b>
<a href="https://arxiv.org/abs/2202.01085">arxiv:2202.01085</a>
&#x1F4C8; 0 <br>
<p>Robert Hu, Dino Sejdinovic, Joan Alexis Glaunès</p></summary>
<p>

**Abstract:** Kernel matrix vector multiplication (KMVM) is a ubiquitous operation in machine learning and scientific computing, spanning from the kernel literature to signal processing. As kernel matrix vector multiplication tends to scale quadratically in both memory and time, applications are often limited by these computational scaling constraints. We propose a novel approximation procedure coined Faster-Fast and Free Memory Method ($\text{F}^3$M) to address these scaling issues for KMVM. Extensive experiments demonstrate that $\text{F}^3$M has empirical \emph{linear time and memory} complexity with a relative error of order $10^{-3}$ and can compute a full KMVM for a billion points \emph{in under one minute} on a high-end GPU, leading to a significant speed-up in comparison to existing CPU methods. We further demonstrate the utility of our procedure by applying it as a drop-in for the state-of-the-art GPU-based linear solver FALKON, \emph{improving speed 3-5 times} at the cost of $<$1\% drop in accuracy.

</p>
</details>

<details><summary><b>Using Ballistocardiography for Sleep Stage Classification</b>
<a href="https://arxiv.org/abs/2202.01038">arxiv:2202.01038</a>
&#x1F4C8; 0 <br>
<p>iebei Liu, Peter Morris, Krista Nelson, Mehdi Boukhechba</p></summary>
<p>

**Abstract:** A practical way of detecting sleep stages has become more necessary as we begin to learn about the vast effects that sleep has on people's lives. The current methods of sleep stage detection are expensive, invasive to a person's sleep, and not practical in a modern home setting. While the method of detecting sleep stages via the monitoring of brain activity, muscle activity, and eye movement, through electroencephalogram in a lab setting, provide the gold standard for detection, this paper aims to investigate a new method that will allow a person to gain similar insight and results with no obtrusion to their normal sleeping habits. Ballistocardiography (BCG) is a non-invasive sensing technology that collects information by measuring the ballistic forces generated by the heart. Using features extracted from BCG such as time of usage, heart rate, respiration rate, relative stroke volume, and heart rate variability, we propose to implement a sleep stage detection algorithm and compare it against sleep stages extracted from a Fitbit Sense Smart Watch. The accessibility, ease of use, and relatively-low cost of the BCG offers many applications and advantages for using this device. By standardizing this device, people will be able to benefit from the BCG in analyzing their own sleep patterns and draw conclusions on their sleep efficiency. This work demonstrates the feasibility of using BCG for an accurate and non-invasive sleep monitoring method that can be set up in the comfort of a one's personal sleep environment.

</p>
</details>

<details><summary><b>Detecting Privacy Requirements from User Stories with NLP Transfer Learning Models</b>
<a href="https://arxiv.org/abs/2202.01035">arxiv:2202.01035</a>
&#x1F4C8; 0 <br>
<p>Francesco Casillo, Vincenzo Deufemia, Carmine Gravino</p></summary>
<p>

**Abstract:** To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems. Objective: We present an approach to decrease privacy risks during agile software development by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile Requirement Engineering (RE). Methods: The proposed approach combines Natural Language Processing (NLP) and linguistic resources with deep learning algorithms to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and syntactic structure of the text. This information is then processed by a pre-trained convolutional neural network, which paved the way for the implementation of a Transfer Learning technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories. Results: The experimental results show that deep learning algorithms allow to obtain better predictions than those achieved with conventional (shallow) machine learning methods. Moreover, the application of Transfer Learning allows to considerably improve the accuracy of the predictions, ca. 10%. Conclusions: Our study contributes to encourage software engineering researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models.

</p>
</details>

<details><summary><b>MD-GAN with multi-particle input: the machine learning of long-time molecular behavior from short-time MD data</b>
<a href="https://arxiv.org/abs/2202.00995">arxiv:2202.00995</a>
&#x1F4C8; 0 <br>
<p>Ryo Kawada, Katsuhiro Endo, Daisuke Yuhara, Kenji Yasuoka</p></summary>
<p>

**Abstract:** MD-GAN is a machine learning-based method that can evolve part of the system at any time step, accelerating the generation of molecular dynamics data. For the accurate prediction of MD-GAN, sufficient information on the dynamics of a part of the system should be included with the training data. Therefore, the selection of the part of the system is important for efficient learning. In a previous study, only one particle (or vector) of each molecule was extracted as part of the system. Therefore, we investigated the effectiveness of adding information from other particles to the learning process. In the experiment of the polyethylene system, when the dynamics of three particles of each molecule were used, the diffusion was successfully predicted using one-third of the time length of the training data, compared to the single-particle input. Surprisingly, the unobserved transition of diffusion in the training data was also predicted using this method.

</p>
</details>

<details><summary><b>Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions</b>
<a href="https://arxiv.org/abs/2202.00992">arxiv:2202.00992</a>
&#x1F4C8; 0 <br>
<p>Maksim Velikanov, Dmitry Yarotsky</p></summary>
<p>

**Abstract:** Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions. In this paper we perform a systematic study of a range of classical single-step and multi-step first order optimization algorithms, with adaptive and non-adaptive, constant and non-constant learning rates: vanilla Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients. For each of these, we prove that a power law spectral assumption entails a power law for convergence rate of the algorithm, with the convergence rate exponent given by a specific multiple of the spectral exponent. We establish both upper and lower bounds, showing that the results are tight. Finally, we demonstrate applications of these results to kernel learning and training of neural networks in the NTK regime.

</p>
</details>

<details><summary><b>Posterior temperature optimized Bayesian models for inverse problems in medical imaging</b>
<a href="https://arxiv.org/abs/2202.00986">arxiv:2202.00986</a>
&#x1F4C8; 0 <br>
<p>Max-Heinrich Laves, Malte Tölle, Alexander Schlaefer, Sandy Engelhardt</p></summary>
<p>

**Abstract:** We present Posterior Temperature Optimized Bayesian Inverse Models (POTOBIM), an unsupervised Bayesian approach to inverse problems in medical imaging using mean-field variational inference with a fully tempered posterior. Bayesian methods exhibit useful properties for approaching inverse tasks, such as tomographic reconstruction or image denoising. A suitable prior distribution introduces regularization, which is needed to solve the ill-posed problem and reduces overfitting the data. In practice, however, this often results in a suboptimal posterior temperature, and the full potential of the Bayesian approach is not being exploited. In POTOBIM, we optimize both the parameters of the prior distribution and the posterior temperature with respect to reconstruction accuracy using Bayesian optimization with Gaussian process regression. Our method is extensively evaluated on four different inverse tasks on a variety of modalities with images from public data sets and we demonstrate that an optimized posterior temperature outperforms both non-Bayesian and Bayesian approaches without temperature optimization. The use of an optimized prior distribution and posterior temperature leads to improved accuracy and uncertainty estimation and we show that it is sufficient to find these hyperparameters per task domain. Well-tempered posteriors yield calibrated uncertainty, which increases the reliability in the predictions. Our source code is publicly available at github.com/Cardio-AI/mfvi-dip-mia.

</p>
</details>

<details><summary><b>VC-PCR: A Prediction Method based on Supervised Variable Selection and Clustering</b>
<a href="https://arxiv.org/abs/2202.00975">arxiv:2202.00975</a>
&#x1F4C8; 0 <br>
<p>Rebecca Marion, Johannes Lederer, Bernadette Govaerts, Rainer von Sachs</p></summary>
<p>

**Abstract:** Sparse linear prediction methods suffer from decreased prediction accuracy when the predictor variables have cluster structure (e.g. there are highly correlated groups of variables). To improve prediction accuracy, various methods have been proposed to identify variable clusters from the data and integrate cluster information into a sparse modeling process. But none of these methods achieve satisfactory performance for prediction, variable selection and variable clustering simultaneously. This paper presents Variable Cluster Principal Component Regression (VC-PCR), a prediction method that supervises variable selection and variable clustering in order to solve this problem. Experiments with real and simulated data demonstrate that, compared to competitor methods, VC-PCR achieves better prediction, variable selection and clustering performance when cluster structure is present.

</p>
</details>

<details><summary><b>Asynchronous Decentralized Learning over Unreliable Wireless Networks</b>
<a href="https://arxiv.org/abs/2202.00955">arxiv:2202.00955</a>
&#x1F4C8; 0 <br>
<p>Eunjeong Jeong, Matteo Zecchin, Marios Kountouris</p></summary>
<p>

**Abstract:** Decentralized learning enables edge users to collaboratively train models by exchanging information via device-to-device communication, yet prior works have been limited to wireless networks with fixed topologies and reliable workers. In this work, we propose an asynchronous decentralized stochastic gradient descent (DSGD) algorithm, which is robust to the inherent computation and communication failures occurring at the wireless network edge. We theoretically analyze its performance and establish a non-asymptotic convergence guarantee. Experimental results corroborate our analysis, demonstrating the benefits of asynchronicity and outdated gradient information reuse in decentralized learning over unreliable wireless networks.

</p>
</details>

<details><summary><b>Approximative Algorithms for Multi-Marginal Optimal Transport and Free-Support Wasserstein Barycenters</b>
<a href="https://arxiv.org/abs/2202.00954">arxiv:2202.00954</a>
&#x1F4C8; 0 <br>
<p>Johannes von Lindheim</p></summary>
<p>

**Abstract:** Computationally solving multi-marginal optimal transport (MOT) with squared Euclidean costs for $N$ discrete probability measures has recently attracted considerable attention, in part because of the correspondence of its solutions with Wasserstein-$2$ barycenters, which have many applications in data science. In general, this problem is NP-hard, calling for practical approximative algorithms. While entropic regularization has been successfully applied to approximate Wasserstein barycenters, this loses the sparsity of the optimal solution, making it difficult to solve the MOT problem directly in practice because of the curse of dimensionality. Thus, for obtaining barycenters, one usually resorts to fixed-support restrictions to a grid, which is, however, prohibitive in higher ambient dimensions $d$. In this paper, after analyzing the relationship between MOT and barycenters, we present two algorithms to approximate the solution of MOT directly, requiring mainly just $N-1$ standard two-marginal OT computations. Thus, they are fast, memory-efficient and easy to implement and can be used with any sparse OT solver as a black box. Moreover, they produce sparse solutions and show promising numerical results. We analyze these algorithms theoretically, proving upper and lower bounds for the relative approximation error.

</p>
</details>

<details><summary><b>Invariant Ancestry Search</b>
<a href="https://arxiv.org/abs/2202.00913">arxiv:2202.00913</a>
&#x1F4C8; 0 <br>
<p>Phillip B. Mogensen, Nikolaj Thams, Jonas Peters</p></summary>
<p>

**Abstract:** Recently, methods have been proposed that exploit the invariance of prediction models with respect to changing environments to infer subsets of the causal parents of a response variable. If the environments influence only few of the underlying mechanisms, the subset identified by invariant causal prediction, for example, may be small, or even empty. We introduce the concept of minimal invariance and propose invariant ancestry search (IAS). In its population version, IAS outputs a set which contains only ancestors of the response and is a superset of the output of ICP. When applied to data, corresponding guarantees hold asymptotically if the underlying test for invariance has asymptotic level and power. We develop scalable algorithms and perform experiments on simulated and real data.

</p>
</details>

<details><summary><b>Using Deep Learning to Bootstrap Abstractions for Hierarchical Robot Planning</b>
<a href="https://arxiv.org/abs/2202.00907">arxiv:2202.00907</a>
&#x1F4C8; 0 <br>
<p>Naman Shah, Siddharth Srivastava</p></summary>
<p>

**Abstract:** This paper addresses the problem of learning abstractions that boost robot planning performance while providing strong guarantees of reliability. Although state-of-the-art hierarchical robot planning algorithms allow robots to efficiently compute long-horizon motion plans for achieving user desired tasks, these methods typically rely upon environment-dependent state and action abstractions that need to be hand-designed by experts.
  We present a new approach for bootstrapping the entire hierarchical planning process. It shows how abstract states and actions for new environments can be computed automatically using the critical regions predicted by a deep neural-network with an auto-generated robot specific architecture. It uses the learned abstractions in a novel multi-source bi-directional hierarchical robot planning algorithm that is sound and probabilistically complete. An extensive empirical evaluation on twenty different settings using holonomic and non-holonomic robots shows that (a) the learned abstractions provide the information necessary for efficient multi-source hierarchical planning; and that (b) this approach of learning abstraction and planning outperforms state-of-the-art baselines by nearly a factor of ten in terms of planning time on test environments not seen during training.

</p>
</details>

<details><summary><b>Methodology for forecasting and optimization in IEEE-CIS 3rd Technical Challenge</b>
<a href="https://arxiv.org/abs/2202.00894">arxiv:2202.00894</a>
&#x1F4C8; 0 <br>
<p>Richard Bean</p></summary>
<p>

**Abstract:** This report provides a description of the methodology I used in the IEEE-CIS 3rd Technical Challenge.
  For the forecast, I used a quantile regression forest approach using the solar variables provided by the Bureau of Meterology of Australia (BOM) and many of the weather variables from the European Centre for Medium-Range Weather Forecasting (ECMWF).
  Groups of buildings and all of the solar instances were trained together as they were observed to be closely correlated over time. Other variables used included Fourier values based on hour of day and day of year, and binary variables for combinations of days of the week.
  The start dates for the time series were carefully tuned based on phase 1 and cleaning and thresholding was used to reduce the observed error rate for each time series.
  For the optimization, a four-step approach was used using the forecast developed. First, a mixed-integer program (MIP) was solved for the recurring and recurring plus once-off activities, then each of these was extended using a mixed-integer quadratic program (MIQP).
  The general strategy was chosen from one of two ("array" from the "array" and "tuples" approaches) while the specific step improvement strategy was chosen from one of five ("no forced discharge").

</p>
</details>

<details><summary><b>MPVNN: Mutated Pathway Visible Neural Network Architecture for Interpretable Prediction of Cancer-specific Survival Risk</b>
<a href="https://arxiv.org/abs/2202.00882">arxiv:2202.00882</a>
&#x1F4C8; 0 <br>
<p>Gourab Ghosh Roy, Nicholas Geard, Karin Verspoor, Shan He</p></summary>
<p>

**Abstract:** Survival risk prediction using gene expression data is important in making treatment decisions in cancer. Standard neural network (NN) survival analysis models are black boxes with lack of interpretability. More interpretable visible neural network (VNN) architectures are designed using biological pathway knowledge. But they do not model how pathway structures can change for particular cancer types. We propose a novel Mutated Pathway VNN or MPVNN architecture, designed using prior signaling pathway knowledge and gene mutation data-based edge randomization simulating signal flow disruption. As a case study, we use the PI3K-Akt pathway and demonstrate overall improved cancer-specific survival risk prediction results of MPVNN over standard non-NN and other similar sized NN survival analysis methods. We show that trained MPVNN architecture interpretation, which points to smaller sets of genes connected by signal flow within the PI3K-Akt pathway that are important in risk prediction for particular cancer types, is reliable.

</p>
</details>


{% endraw %}
Prev: [2022.02.01]({{ '/2022/02/01/2022.02.01.html' | relative_url }})  Next: [2022.02.03]({{ '/2022/02/03/2022.02.03.html' | relative_url }})