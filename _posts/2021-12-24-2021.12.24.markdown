Prev: [2021.12.23]({{ '/2021/12/23/2021.12.23.html' | relative_url }})  Next: [2021.12.25]({{ '/2021/12/25/2021.12.25.html' | relative_url }})
{% raw %}
## Summary for 2021-12-24, created on 2022-01-03


<details><summary><b>The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence</b>
<a href="https://arxiv.org/abs/2112.13121">arxiv:2112.13121</a>
&#x1F4C8; 2130 <br>
<p>Brando Miranda, Yu-Xiong Wang, Sanmi Koyejo</p></summary>
<p>

**Abstract:** It has been recently observed that a transfer learning solution might be all we needed to solve many few-shot learning benchmarks. This raises important questions about when and how meta-learning algorithms should be deployed. In this paper, we make a first step in clarifying these questions by first formulating a computable metric for a few-shot learning benchmark that we hypothesize is predictive of whether meta-learning solutions will succeed or not. We name this metric the diversity coefficient of a few-shot learning benchmark. Using the diversity coefficient, we show that the MiniImagenet benchmark has zero diversity - according to twenty-four different ways to compute the diversity. We proceed to show that when making a fair comparison between MAML learned solutions to transfer learning, both have identical meta-test accuracy. This suggests that transfer learning fails to outperform MAML - contrary to what previous work suggests. Together, these two facts provide the first test of whether diversity correlates with meta-learning success and therefore show that a diversity coefficient of zero correlates with a high similarity between transfer learning and MAML learned solutions - especially at meta-test time. We therefore conjecture meta-learned solutions have the same meta-test performance as transfer learning when the diversity coefficient is zero.

</p>
</details>

<details><summary><b>Profile Guided Optimization without Profiles: A Machine Learning Approach</b>
<a href="https://arxiv.org/abs/2112.14679">arxiv:2112.14679</a>
&#x1F4C8; 46 <br>
<p>Nadav Rotem, Chris Cummins</p></summary>
<p>

**Abstract:** Profile guided optimization is an effective technique for improving the optimization ability of compilers based on dynamic behavior, but collecting profile data is expensive, cumbersome, and requires regular updating to remain fresh. We present a novel statistical approach to inferring branch probabilities that improves the performance of programs that are compiled without profile guided optimizations. We perform offline training using information that is collected from a large corpus of binaries that have branch probabilities information. The learned model is used by the compiler to predict the branch probabilities of regular uninstrumented programs, which the compiler can then use to inform optimization decisions. We integrate our technique directly in LLVM, supplementing the existing human-engineered compiler heuristics. We evaluate our technique on a suite of benchmarks, demonstrating some gains over compiling without profile information. In deployment, our technique requires no profiling runs and has negligible effect on compilation time.

</p>
</details>

<details><summary><b>Gaussian Process Bandits with Aggregated Feedback</b>
<a href="https://arxiv.org/abs/2112.13029">arxiv:2112.13029</a>
&#x1F4C8; 25 <br>
<p>Mengyan Zhang, Russell Tsuchida, Cheng Soon Ong</p></summary>
<p>

**Abstract:** We consider the continuum-armed bandits problem, under a novel setting of recommending the best arms within a fixed budget under aggregated feedback. This is motivated by applications where the precise rewards are impossible or expensive to obtain, while an aggregated reward or feedback, such as the average over a subset, is available. We constrain the set of reward functions by assuming that they are from a Gaussian Process and propose the Gaussian Process Optimistic Optimisation (GPOO) algorithm. We adaptively construct a tree with nodes as subsets of the arm space, where the feedback is the aggregated reward of representatives of a node. We propose a new simple regret notion with respect to aggregated feedback on the recommended arms. We provide theoretical analysis for the proposed algorithm, and recover single point feedback as a special case. We illustrate GPOO and compare it with related algorithms on simulated data.

</p>
</details>

<details><summary><b>Is Importance Weighting Incompatible with Interpolating Classifiers?</b>
<a href="https://arxiv.org/abs/2112.12986">arxiv:2112.12986</a>
&#x1F4C8; 9 <br>
<p>Ke Alexander Wang, Niladri S. Chatterji, Saminul Haque, Tatsunori Hashimoto</p></summary>
<p>

**Abstract:** Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized neural networks. Is importance weighting truly incompatible with the training of overparameterized neural networks? Our paper answers this in the negative. We show that importance weighting fails not because of the overparameterization, but instead, as a result of using exponentially-tailed losses like the logistic or cross-entropy loss. As a remedy, we show that polynomially-tailed losses restore the effects of importance reweighting in correcting distribution shift in overparameterized models. We characterize the behavior of gradient descent on importance weighted polynomially-tailed losses with overparameterized linear models, and theoretically demonstrate the advantage of using polynomially-tailed losses in a label shift setting. Surprisingly, our theory shows that using weights that are obtained by exponentiating the classical unbiased importance weights can improve performance. Finally, we demonstrate the practical value of our analysis with neural network experiments on a subpopulation shift and a label shift dataset. When reweighted, our loss function can outperform reweighted cross-entropy by as much as 9% in test accuracy. Our loss function also gives test accuracies comparable to, or even exceeding, well-tuned state-of-the-art methods for correcting distribution shifts.

</p>
</details>

<details><summary><b>Application of Markov Structure of Genomes to Outlier Identification and Read Classification</b>
<a href="https://arxiv.org/abs/2112.13117">arxiv:2112.13117</a>
&#x1F4C8; 7 <br>
<p>Alan F. Karr, Jason Hauzel, Adam A. Porter, Marcel Schaefer</p></summary>
<p>

**Abstract:** In this paper we apply the structure of genomes as second-order Markov processes specified by the distributions of successive triplets of bases to two bioinformatics problems: identification of outliers in genome databases and read classification in metagenomics, using real coronavirus and adenovirus data.

</p>
</details>

<details><summary><b>Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs</b>
<a href="https://arxiv.org/abs/2112.13175">arxiv:2112.13175</a>
&#x1F4C8; 6 <br>
<p>Mingyu Guo, Jialiang Li, Aneta Neumann, Frank Neumann, Hung Nguyen</p></summary>
<p>

**Abstract:** Active Directory is the default security management system for Windows domain networks. We study the shortest path edge interdiction problem for defending Active Directory style attack graphs. The problem is formulated as a Stackelberg game between one defender and one attacker. The attack graph contains one destination node and multiple entry nodes. The attacker's entry node is chosen by nature. The defender chooses to block a set of edges limited by his budget. The attacker then picks the shortest unblocked attack path. The defender aims to maximize the expected shortest path length for the attacker, where the expectation is taken over entry nodes.
  We observe that practical Active Directory attack graphs have small maximum attack path lengths and are structurally close to trees. We first show that even if the maximum attack path length is a constant, the problem is still $W[1]$-hard with respect to the defender's budget. Having a small maximum attack path length and a small budget is not enough to design fixed-parameter algorithms. If we further assume that the number of entry nodes is small, then we derive a fixed-parameter tractable algorithm.
  We then propose two other fixed-parameter algorithms by exploiting the tree-like features. One is based on tree decomposition and requires a small tree width. The other assumes a small number of splitting nodes (nodes with multiple out-going edges). Finally, the last algorithm is converted into a graph convolutional neural network based heuristic, which scales to larger graphs with more splitting nodes.

</p>
</details>

<details><summary><b>Virtuoso: Video-based Intelligence for real-time tuning on SOCs</b>
<a href="https://arxiv.org/abs/2112.13076">arxiv:2112.13076</a>
&#x1F4C8; 6 <br>
<p>Jayoung Lee, PengCheng Wang, Ran Xu, Venkat Dasari, Noah Weston, Yin Li, Saurabh Bagchi, Somali Chaterji</p></summary>
<p>

**Abstract:** Efficient and adaptive computer vision systems have been proposed to make computer vision tasks, such as image classification and object detection, optimized for embedded or mobile devices. These solutions, quite recent in their origin, focus on optimizing the model (a deep neural network, DNN) or the system by designing an adaptive system with approximation knobs. In spite of several recent efforts, we show that existing solutions suffer from two major drawbacks. First, the system does not consider energy consumption of the models while making a decision on which model to run. Second, the evaluation does not consider the practical scenario of contention on the device, due to other co-resident workloads. In this work, we propose an efficient and adaptive video object detection system, Virtuoso, which is jointly optimized for accuracy, energy efficiency, and latency. Underlying Virtuoso is a multi-branch execution kernel that is capable of running at different operating points in the accuracy-energy-latency axes, and a lightweight runtime scheduler to select the best fit execution branch to satisfy the user requirement. To fairly compare with Virtuoso, we benchmark 15 state-of-the-art or widely used protocols, including Faster R-CNN (FRCNN), YOLO v3, SSD, EfficientDet, SELSA, MEGA, REPP, FastAdapt, and our in-house adaptive variants of FRCNN+, YOLO+, SSD+, and EfficientDet+ (our variants have enhanced efficiency for mobiles). With this comprehensive benchmark, Virtuoso has shown superiority to all the above protocols, leading the accuracy frontier at every efficiency level on NVIDIA Jetson mobile GPUs. Specifically, Virtuoso has achieved an accuracy of 63.9%, which is more than 10% higher than some of the popular object detection models, FRCNN at 51.1%, and YOLO at 49.5%.

</p>
</details>

<details><summary><b>Raw Produce Quality Detection with Shifted Window Self-Attention</b>
<a href="https://arxiv.org/abs/2112.13845">arxiv:2112.13845</a>
&#x1F4C8; 4 <br>
<p>Oh Joon Kwon, Byungsoo Kim, Youngduck Choi</p></summary>
<p>

**Abstract:** Global food insecurity is expected to worsen in the coming decades with the accelerated rate of climate change and the rapidly increasing population. In this vein, it is important to remove inefficiencies at every level of food production. The recent advances in deep learning can help reduce such inefficiencies, yet their application has not yet become mainstream throughout the industry, inducing economic costs at a massive scale. To this point, modern techniques such as CNNs (Convolutional Neural Networks) have been applied to RPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's successful debut in the vision among other modalities led us to expect a better performance with these Transformer-based models in RPQD. In this work, we exclusively investigate the recent state-of-the-art Swin (Shifted Windows) Transformer which computes self-attention in both intra- and inter-window fashion. We compare Swin Transformer against CNN models on four RPQD image datasets, each containing different kinds of raw produce: fruits and vegetables, fish, pork, and beef. We observe that Swin Transformer not only achieves better or competitive performance but also is data- and compute-efficient, making it ideal for actual deployment in real-world setting. To the best of our knowledge, this is the first large-scale empirical study on RPQD task, which we hope will gain more attention in future works.

</p>
</details>

<details><summary><b>Measuring Quality of DNA Sequence Data via Degradation</b>
<a href="https://arxiv.org/abs/2112.13111">arxiv:2112.13111</a>
&#x1F4C8; 4 <br>
<p>Alan F. Karr, Jason Hauzel, Adam A. Porter, Marcel Schaefer</p></summary>
<p>

**Abstract:** We propose and apply a novel paradigm for characterization of genome data quality, which quantifies the effects of intentional degradation of quality. The rationale is that the higher the initial quality, the more fragile the genome and the greater the effects of degradation. We demonstrate that this phenomenon is ubiquitous, and that quantified measures of degradation can be used for multiple purposes. We focus on identifying outliers that may be problematic with respect to data quality, but might also be true anomalies or even attempts to subvert the database.

</p>
</details>

<details><summary><b>Multi-Scale Feature Fusion: Learning Better Semantic Segmentation for Road Pothole Detection</b>
<a href="https://arxiv.org/abs/2112.13082">arxiv:2112.13082</a>
&#x1F4C8; 4 <br>
<p>Jiahe Fan, Mohammud J. Bocus, Brett Hosking, Rigen Wu, Yanan Liu, Sergey Vityazev, Rui Fan</p></summary>
<p>

**Abstract:** This paper presents a novel pothole detection approach based on single-modal semantic segmentation. It first extracts visual features from input images using a convolutional neural network. A channel attention module then reweighs the channel features to enhance the consistency of different feature maps. Subsequently, we employ an atrous spatial pyramid pooling module (comprising of atrous convolutions in series, with progressive rates of dilation) to integrate the spatial context information. This helps better distinguish between potholes and undamaged road areas. Finally, the feature maps in the adjacent layers are fused using our proposed multi-scale feature fusion module. This further reduces the semantic gap between different feature channel layers. Extensive experiments were carried out on the Pothole-600 dataset to demonstrate the effectiveness of our proposed method. The quantitative comparisons suggest that our method achieves the state-of-the-art (SoTA) performance on both RGB images and transformed disparity images, outperforming three SoTA single-modal semantic segmentation networks.

</p>
</details>

<details><summary><b>CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing</b>
<a href="https://arxiv.org/abs/2112.13064">arxiv:2112.13064</a>
&#x1F4C8; 4 <br>
<p>Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming</p></summary>
<p>

**Abstract:** The success of deep neural networks (DNNs) in real-world applications has benefited from abundant pre-trained models. However, the backdoored pre-trained models can pose a significant trojan threat to the deployment of downstream DNNs. Existing DNN testing methods are mainly designed to find incorrect corner case behaviors in adversarial settings but fail to discover the backdoors crafted by strong trojan attacks. Observing the trojan network behaviors shows that they are not just reflected by a single compromised neuron as proposed by previous work but attributed to the critical neural paths in the activation intensity and frequency of multiple neurons. This work formulates the DNN backdoor testing and proposes the CatchBackdoor framework. Via differential fuzzing of critical neurons from a small number of benign examples, we identify the trojan paths and particularly the critical ones, and generate backdoor testing examples by simulating the critical neurons in the identified paths. Extensive experiments demonstrate the superiority of CatchBackdoor, with higher detection performance than existing methods. CatchBackdoor works better on detecting backdoors by stealthy blending and adaptive attacks, which existing methods fail to detect. Moreover, our experiments show that CatchBackdoor may reveal the potential backdoors of models in Model Zoo.

</p>
</details>

<details><summary><b>NIP: Neuron-level Inverse Perturbation Against Adversarial Attacks</b>
<a href="https://arxiv.org/abs/2112.13060">arxiv:2112.13060</a>
&#x1F4C8; 4 <br>
<p>Ruoxi Chen, Haibo Jin, Jinyin Chen, Haibin Zheng, Yue Yu, Shouling Ji</p></summary>
<p>

**Abstract:** Although deep learning models have achieved unprecedented success, their vulnerabilities towards adversarial attacks have attracted increasing attention, especially when deployed in security-critical domains. To address the challenge, numerous defense strategies, including reactive and proactive ones, have been proposed for robustness improvement. From the perspective of image feature space, some of them cannot reach satisfying results due to the shift of features. Besides, features learned by models are not directly related to classification results. Different from them, We consider defense method essentially from model inside and investigated the neuron behaviors before and after attacks. We observed that attacks mislead the model by dramatically changing the neurons that contribute most and least to the correct label. Motivated by it, we introduce the concept of neuron influence and further divide neurons into front, middle and tail part. Based on it, we propose neuron-level inverse perturbation(NIP), the first neuron-level reactive defense method against adversarial attacks. By strengthening front neurons and weakening those in the tail part, NIP can eliminate nearly all adversarial perturbations while still maintaining high benign accuracy. Besides, it can cope with different sizes of perturbations via adaptivity, especially larger ones. Comprehensive experiments conducted on three datasets and six models show that NIP outperforms the state-of-the-art baselines against eleven adversarial attacks. We further provide interpretable proofs via neuron activation and visualization for better understanding.

</p>
</details>

<details><summary><b>Generalized Wasserstein Dice Loss, Test-time Augmentation, and Transformers for the BraTS 2021 challenge</b>
<a href="https://arxiv.org/abs/2112.13054">arxiv:2112.13054</a>
&#x1F4C8; 4 <br>
<p>Lucas Fidon, Suprosanna Shit, Ivan Ezhov, Johannes C. Paetzold, SÃ©bastien Ourselin, Tom Vercauteren</p></summary>
<p>

**Abstract:** Brain tumor segmentation from multiple Magnetic Resonance Imaging (MRI) modalities is a challenging task in medical image computation. The main challenges lie in the generalizability to a variety of scanners and imaging protocols. In this paper, we explore strategies to increase model robustness without increasing inference time. Towards this aim, we explore finding a robust ensemble from models trained using different losses, optimizers, and train-validation data split. Importantly, we explore the inclusion of a transformer in the bottleneck of the U-Net architecture. While we find transformer in the bottleneck performs slightly worse than the baseline U-Net in average, the generalized Wasserstein Dice loss consistently produces superior results. Further, we adopt an efficient test time augmentation strategy for faster and robust inference. Our final ensemble of seven 3D U-Nets with test-time augmentation produces an average dice score of 89.4% and an average Hausdorff 95% distance of 10.0 mm when evaluated on the BraTS 2021 testing dataset. Our code and trained models are publicly available at https://github.com/LucasFidon/TRABIT_BraTS2021.

</p>
</details>

<details><summary><b>Lyapunov Exponents for Diversity in Differentiable Games</b>
<a href="https://arxiv.org/abs/2112.14570">arxiv:2112.14570</a>
&#x1F4C8; 3 <br>
<p>Jonathan Lorraine, Paul Vicol, Jack Parker-Holder, Tal Kachman, Luke Metz, Jakob Foerster</p></summary>
<p>

**Abstract:** Ridge Rider (RR) is an algorithm for finding diverse solutions to optimization problems by following eigenvectors of the Hessian ("ridges"). RR is designed for conservative gradient systems (i.e., settings involving a single loss function), where it branches at saddles - easy-to-find bifurcation points. We generalize this idea to non-conservative, multi-agent gradient systems by proposing a method - denoted Generalized Ridge Rider (GRR) - for finding arbitrary bifurcation points. We give theoretical motivation for our method by leveraging machinery from the field of dynamical systems. We construct novel toy problems where we can visualize new phenomena while giving insight into high-dimensional problems of interest. Finally, we empirically evaluate our method by finding diverse solutions in the iterated prisoners' dilemma and relevant machine learning problems including generative adversarial networks.

</p>
</details>

<details><summary><b>Cyberattack Detection in Large-Scale Smart Grids using Chebyshev Graph Convolutional Networks</b>
<a href="https://arxiv.org/abs/2112.13166">arxiv:2112.13166</a>
&#x1F4C8; 3 <br>
<p>Osman Boyaci, Mohammad Rasoul Narimani, Katherine Davis, Erchin Serpedin</p></summary>
<p>

**Abstract:** As a highly complex and integrated cyber-physical system, modern power grids are exposed to cyberattacks. False data injection attacks (FDIAs), specifically, represent a major class of cyber threats to smart grids by targeting the measurement data's integrity. Although various solutions have been proposed to detect those cyberattacks, the vast majority of the works have ignored the inherent graph structure of the power grid measurements and validated their detectors only for small test systems with less than a few hundred buses. To better exploit the spatial correlations of smart grid measurements, this paper proposes a deep learning model for cyberattack detection in large-scale AC power grids using Chebyshev Graph Convolutional Networks (CGCN). By reducing the complexity of spectral graph filters and making them localized, CGCN provides a fast and efficient convolution operation to model the graph structural smart grid data. We numerically verify that the proposed CGCN based detector surpasses the state-of-the-art model by 7.86 in detection rate and 9.67 in false alarm rate for a large-scale power grid with 2848 buses. It is notable that the proposed approach detects cyberattacks under 4 milliseconds for a 2848-bus system, which makes it a good candidate for real-time detection of cyberattacks in large systems.

</p>
</details>

<details><summary><b>Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform</b>
<a href="https://arxiv.org/abs/2112.13149">arxiv:2112.13149</a>
&#x1F4C8; 3 <br>
<p>Cesar Carranza, Daniel Llamocca, Marios Pattichis</p></summary>
<p>

**Abstract:** The Discrete Periodic Radon Transform (DPRT) has been extensively used in applications that involve image reconstructions from projections. This manuscript introduces a fast and scalable approach for computing the forward and inverse DPRT that is based on the use of: (i) a parallel array of fixed-point adder trees, (ii) circular shift registers to remove the need for accessing external memory components when selecting the input data for the adder trees, (iii) an image block-based approach to DPRT computation that can fit the proposed architecture to available resources, and (iv) fast transpositions that are computed in one or a few clock cycles that do not depend on the size of the input image. As a result, for an $N\times N$ image ($N$ prime), the proposed approach can compute up to $N^{2}$ additions per clock cycle. Compared to previous approaches, the scalable approach provides the fastest known implementations for different amounts of computational resources. For example, for a $251\times 251$ image, for approximately $25\%$ fewer flip-flops than required for a systolic implementation, we have that the scalable DPRT is computed 36 times faster. For the fastest case, we introduce optimized architectures that can compute the DPRT and its inverse in just $2N+\left\lceil \log_{2}N\right\rceil+1$ and $2N+3\left\lceil \log_{2}N\right\rceil+B+2$ cycles respectively, where $B$ is the number of bits used to represent each input pixel. On the other hand, the scalable DPRT approach requires more 1-bit additions than for the systolic implementation and provides a trade-off between speed and additional 1-bit additions. All of the proposed DPRT architectures were implemented in VHDL and validated using an FPGA implementation.

</p>
</details>

<details><summary><b>Does MAML Only Work via Feature Re-use? A Data Centric Perspective</b>
<a href="https://arxiv.org/abs/2112.13137">arxiv:2112.13137</a>
&#x1F4C8; 3 <br>
<p>Brando Miranda, Yu-Xiong Wang, Sanmi Koyejo</p></summary>
<p>

**Abstract:** Recent work has suggested that a good embedding is all we need to solve many few-shot learning benchmarks. Furthermore, other work has strongly suggested that Model Agnostic Meta-Learning (MAML) also works via this same method - by learning a good embedding. These observations highlight our lack of understanding of what meta-learning algorithms are doing and when they work. In this work, we provide empirical results that shed some light on how meta-learned MAML representations function. In particular, we identify three interesting properties: 1) In contrast to previous work, we show that it is possible to define a family of synthetic benchmarks that result in a low degree of feature re-use - suggesting that current few-shot learning benchmarks might not have the properties needed for the success of meta-learning algorithms; 2) meta-overfitting occurs when the number of classes (or concepts) are finite, and this issue disappears once the task has an unbounded number of concepts (e.g., online learning); 3) more adaptation at meta-test time with MAML does not necessarily result in a significant representation change or even an improvement in meta-test performance - even when training on our proposed synthetic benchmarks. Finally, we suggest that to understand meta-learning algorithms better, we must go beyond tracking only absolute performance and, in addition, formally quantify the degree of meta-learning and track both metrics together. Reporting results in future work this way will help us identify the sources of meta-overfitting more accurately and help us design more flexible meta-learning algorithms that learn beyond fixed feature re-use. Finally, we conjecture the core challenge of re-thinking meta-learning is in the design of few-shot learning data sets and benchmarks - rather than in the algorithms, as suggested by previous work.

</p>
</details>

<details><summary><b>Ultrasound Speckle Suppression and Denoising using MRI-derived Normalizing Flow Priors</b>
<a href="https://arxiv.org/abs/2112.13110">arxiv:2112.13110</a>
&#x1F4C8; 3 <br>
<p>Vincent van de Schaft, Ruud J. G. van Sloun</p></summary>
<p>

**Abstract:** Ultrasonography offers an inexpensive, widely-accessible and compact medical imaging solution. However, compared to other imaging modalities such as CT and MRI, ultrasound images notoriously suffer from strong speckle noise, which originates from the random interference of sub-wavelength scattering. This deteriorates ultrasound image quality and makes interpretation challenging. We here propose a new unsupervised ultrasound speckle reduction and image denoising method based on maximum-a-posteriori estimation with deep generative priors that are learned from high-quality MRI images. To model the generative tissue reflectivity prior, we exploit normalizing flows, which in recent years have shown to be very powerful in modeling signal priors across a variety of applications. To facilitate generaliation, we factorize the prior and train our flow model on patches from the NYU fastMRI (fully-sampled) dataset. This prior is then used for inference in an iterative denoising scheme. We first validate the utility of our learned priors on noisy MRI data (no prior domain shift), and then turn to evaluating performance on both simulated and in-vivo ultrasound images from the PICMUS and CUBDL datasets. The results show that the method outperforms other (unsupervised) ultrasound denoising methods (NLM and OBNLM) both quantitatively and qualitatively.

</p>
</details>

<details><summary><b>Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation</b>
<a href="https://arxiv.org/abs/2112.13047">arxiv:2112.13047</a>
&#x1F4C8; 3 <br>
<p>Jiaxing Yan, Hong Zhao, Penghui Bu, YuSheng Jin</p></summary>
<p>

**Abstract:** Self-supervised learning has shown very promising results for monocular depth estimation. Scene structure and local details both are significant clues for high-quality depth estimation. Recent works suffer from the lack of explicit modeling of scene structure and proper handling of details information, which leads to a performance bottleneck and blurry artefacts in predicted results. In this paper, we propose the Channel-wise Attention-based Depth Estimation Network (CADepth-Net) with two effective contributions: 1) The structure perception module employs the self-attention mechanism to capture long-range dependencies and aggregates discriminative features in channel dimensions, explicitly enhances the perception of scene structure, obtains the better scene understanding and rich feature representation. 2) The detail emphasis module re-calibrates channel-wise feature maps and selectively emphasizes the informative features, aiming to highlight crucial local details information and fuse different level features more efficiently, resulting in more precise and sharper depth prediction. Furthermore, the extensive experiments validate the effectiveness of our method and show that our model achieves the state-of-the-art results on the KITTI benchmark and Make3D datasets.

</p>
</details>

<details><summary><b>Doppler velocity-based algorithm for Clustering and Velocity Estimation of moving objects</b>
<a href="https://arxiv.org/abs/2112.12984">arxiv:2112.12984</a>
&#x1F4C8; 3 <br>
<p>Mian Guo, Kai Zhong, Xiaozhi Wang</p></summary>
<p>

**Abstract:** We propose a Doppler velocity-based cluster and velocity estimation algorithm based on the characteristics of FMCW LiDAR which achieves highly accurate, single-scan, and real-time motion state detection and velocity estimation. We prove the continuity of the Doppler velocity on the same object. Based on this principle, we achieve the distinction between moving objects and stationary background via region growing clustering algorithm. The obtained stationary background will be used to estimate the velocity of the FMCW LiDAR by the least-squares method. Then we estimate the velocity of the moving objects using the estimated LiDAR velocity and the Doppler velocity of moving objects obtained by clustering. To ensure real-time processing, we set the appropriate least-squares parameters. Meanwhile, to verify the effectiveness of the algorithm, we create the FMCW LiDAR model on the autonomous driving simulation platform CARLA for spawning data. The results show that our algorithm can process at least a 4.5million points and estimate the velocity of 150 moving objects per second under the arithmetic power of the Ryzen 3600x CPU, with a motion state detection accuracy of over 99% and estimated velocity accuracy of 0.1 m/s.

</p>
</details>

<details><summary><b>Deep ensembles in bioimage segmentation</b>
<a href="https://arxiv.org/abs/2112.12955">arxiv:2112.12955</a>
&#x1F4C8; 3 <br>
<p>Loris Nanni, Daniela Cuza, Alessandra Lumini, Andrea Loreggia, Sheryl Brahnam</p></summary>
<p>

**Abstract:** Semantic segmentation consists in classifying each pixel of an image by assigning it to a specific label chosen from a set of all the available ones. During the last few years, a lot of attention shifted to this kind of task. Many computer vision researchers tried to apply autoencoder structures to develop models that can learn the semantics of the image as well as a low-level representation of it. In an autoencoder architecture, given an input, an encoder computes a low dimensional representation of the input that is then used by a decoder to reconstruct the original data. In this work, we propose an ensemble of convolutional neural networks (CNNs). In ensemble methods, many different models are trained and then used for classification, the ensemble aggregates the outputs of the single classifiers. The approach leverages on differences of various classifiers to improve the performance of the whole system. Diversity among the single classifiers is enforced by using different loss functions. In particular, we present a new loss function that results from the combination of Dice and Structural Similarity Index. The proposed ensemble is implemented by combining different backbone networks using the DeepLabV3+ and HarDNet environment. The proposal is evaluated through an extensive empirical evaluation on two real-world scenarios: polyp and skin segmentation. All the code is available online at https://github.com/LorisNanni.

</p>
</details>

<details><summary><b>Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures</b>
<a href="https://arxiv.org/abs/2112.13150">arxiv:2112.13150</a>
&#x1F4C8; 2 <br>
<p>Cesar Carranza, Daniel Llamocca, Marios Pattichis</p></summary>
<p>

**Abstract:** The manuscript describes fast and scalable architectures and associated algorithms for computing convolutions and cross-correlations. The basic idea is to map 2D convolutions and cross-correlations to a collection of 1D convolutions and cross-correlations in the transform domain. This is accomplished through the use of the Discrete Periodic Radon Transform (DPRT) for general kernels and the use of SVD-LU decompositions for low-rank kernels. The approach uses scalable architectures that can be fitted into modern FPGA and Zynq-SOC devices. Based on different types of available resources, for $P\times P$ blocks, 2D convolutions and cross-correlations can be computed in just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a trade-off between performance and required numbers and types of resources. We provide implementations of the proposed architectures using modern programmable devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required resources, we show that the proposed approaches significantly outperform current methods.

</p>
</details>

<details><summary><b>SoK: A Study of the Security on Voice Processing Systems</b>
<a href="https://arxiv.org/abs/2112.13144">arxiv:2112.13144</a>
&#x1F4C8; 2 <br>
<p>Robert Chang, Logan Kuo, Arthur Liu, Nader Sehatbakhsh</p></summary>
<p>

**Abstract:** As the use of Voice Processing Systems (VPS) continues to become more prevalent in our daily lives through the increased reliance on applications such as commercial voice recognition devices as well as major text-to-speech software, the attacks on these systems are increasingly complex, varied, and constantly evolving. With the use cases for VPS rapidly growing into new spaces and purposes, the potential consequences regarding privacy are increasingly more dangerous. In addition, the growing number and increased practicality of over-the-air attacks have made system failures much more probable. In this paper, we will identify and classify an arrangement of unique attacks on voice processing systems. Over the years research has been moving from specialized, untargeted attacks that result in the malfunction of systems and the denial of services to more general, targeted attacks that can force an outcome controlled by an adversary. The current and most frequently used machine learning systems and deep neural networks, which are at the core of modern voice processing systems, were built with a focus on performance and scalability rather than security. Therefore, it is critical for us to reassess the developing voice processing landscape and to identify the state of current attacks and defenses so that we may suggest future developments and theoretical improvements.

</p>
</details>

<details><summary><b>On the Unreasonable Efficiency of State Space Clustering in Personalization Tasks</b>
<a href="https://arxiv.org/abs/2112.13141">arxiv:2112.13141</a>
&#x1F4C8; 2 <br>
<p>Anton Dereventsov, Ranga Raju Vatsavai, Clayton Webster</p></summary>
<p>

**Abstract:** In this effort we consider a reinforcement learning (RL) technique for solving personalization tasks with complex reward signals. In particular, our approach is based on state space clustering with the use of a simplistic $k$-means algorithm as well as conventional choices of the network architectures and optimization algorithms. Numerical examples demonstrate the efficiency of different RL procedures and are used to illustrate that this technique accelerates the agent's ability to learn and does not restrict the agent's performance.

</p>
</details>

<details><summary><b>Accelerated and instance-optimal policy evaluation with linear function approximation</b>
<a href="https://arxiv.org/abs/2112.13109">arxiv:2112.13109</a>
&#x1F4C8; 2 <br>
<p>Tianjiao Li, Guanghui Lan, Ashwin Pananjady</p></summary>
<p>

**Abstract:** We study the problem of policy evaluation with linear function approximation and present efficient and practical algorithms that come with strong optimality guarantees. We begin by proving lower bounds that establish baselines on both the deterministic error and stochastic error in this problem. In particular, we prove an oracle complexity lower bound on the deterministic error in an instance-dependent norm associated with the stationary distribution of the transition kernel, and use the local asymptotic minimax machinery to prove an instance-dependent lower bound on the stochastic error in the i.i.d. observation model. Existing algorithms fail to match at least one of these lower bounds: To illustrate, we analyze a variance-reduced variant of temporal difference learning, showing in particular that it fails to achieve the oracle complexity lower bound. To remedy this issue, we develop an accelerated, variance-reduced fast temporal difference algorithm (VRFTD) that simultaneously matches both lower bounds and attains a strong notion of instance-optimality. Finally, we extend the VRFTD algorithm to the setting with Markovian observations, and provide instance-dependent convergence results that match those in the i.i.d. setting up to a multiplicative factor that is proportional to the mixing time of the chain. Our theoretical guarantees of optimality are corroborated by numerical experiments.

</p>
</details>

<details><summary><b>Dual Hierarchical Attention Networks for Bi-typed Heterogeneous Graph Learning</b>
<a href="https://arxiv.org/abs/2112.13078">arxiv:2112.13078</a>
&#x1F4C8; 2 <br>
<p>Yu Zhao, Shaopeng Wei, Huaming Du, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, Gang Kou</p></summary>
<p>

**Abstract:** Bi-typed heterogeneous graphs are applied in many real-world scenarios. However, previous heterogeneous graph learning studies usually ignore the complex interactions among the bi-typed entities in such heterogeneous graphs. To address this issue, in this paper we propose a novel Dual Hierarchical Attention Networks (DHAN) to learn comprehensive node representations on the bi-typed heterogeneous graphs with intra-class and inter-class hierarchical attention networks. Specifically, the intra-class attention aims to learn the node representations from its same type of neighbors, while inter-class attention is able to aggregate node representations from its different types of neighbors. Hence, the dual attention operations enable DHAN to sufficiently leverage not only the node intra-class neighboring information but also the inter-class neighboring information in the bi-typed heterogeneous graph. Experimental results on various tasks against the state-of-the-arts sufficiently confirm the capability of DHAN in learning node comprehensive representations on the bi-typed heterogeneous

</p>
</details>

<details><summary><b>DP-UTIL: Comprehensive Utility Analysis of Differential Privacy in Machine Learning</b>
<a href="https://arxiv.org/abs/2112.12998">arxiv:2112.12998</a>
&#x1F4C8; 2 <br>
<p>Ismat Jarin, Birhanu Eshete</p></summary>
<p>

**Abstract:** Differential Privacy (DP) has emerged as a rigorous formalism to reason about quantifiable privacy leakage. In machine learning (ML), DP has been employed to limit inference/disclosure of training examples. Prior work leveraged DP across the ML pipeline, albeit in isolation, often focusing on mechanisms such as gradient perturbation. In this paper, we present, DP-UTIL, a holistic utility analysis framework of DP across the ML pipeline with focus on input perturbation, objective perturbation, gradient perturbation, output perturbation, and prediction perturbation. Given an ML task on privacy-sensitive data, DP-UTIL enables a ML privacy practitioner perform holistic comparative analysis on the impact of DP in these five perturbation spots, measured in terms of model utility loss, privacy leakage, and the number of truly revealed training samples. We evaluate DP-UTIL over classification tasks on vision, medical, and financial datasets, using two representative learning algorithms (logistic regression and deep neural network) against membership inference attack as a case study attack. One of the highlights of our results is that prediction perturbation consistently achieves the lowest utility loss on all models across all datasets. In logistic regression models, objective perturbation results in lowest privacy leakage compared to other perturbation techniques. For deep neural networks, gradient perturbation results in lowest privacy leakage. Moreover, our results on true revealed records suggest that as privacy leakage increases a differentially private model reveals more number of member samples. Overall, our findings suggest that to make informed decisions as to which perturbation mechanism to use, a ML privacy practitioner needs to examine the dynamics between optimization techniques (convex vs. non-convex), perturbation mechanisms, number of classes, and privacy budget.

</p>
</details>

<details><summary><b>Toeplitz Least Squares Problems, Fast Algorithms and Big Data</b>
<a href="https://arxiv.org/abs/2112.12994">arxiv:2112.12994</a>
&#x1F4C8; 2 <br>
<p>Ali Eshragh, Oliver Di Pietro, Michael A. Saunders</p></summary>
<p>

**Abstract:** In time series analysis, when fitting an autoregressive model, one must solve a Toeplitz ordinary least squares problem numerous times to find an appropriate model, which can severely affect computational times with large data sets. Two recent algorithms (LSAR and Repeated Halving) have applied randomized numerical linear algebra (RandNLA) techniques to fitting an autoregressive model to big time-series data. We investigate and compare the quality of these two approximation algorithms on large-scale synthetic and real-world data. While both algorithms display comparable results for synthetic datasets, the LSAR algorithm appears to be more robust when applied to real-world time series data. We conclude that RandNLA is effective in the context of big-data time series.

</p>
</details>

<details><summary><b>Deep Neuroevolution Squeezes More out of Small Neural Networks and Small Training Sets: Sample Application to MRI Brain Sequence Classification</b>
<a href="https://arxiv.org/abs/2112.12990">arxiv:2112.12990</a>
&#x1F4C8; 2 <br>
<p>Joseph N Stember, Hrithwik Shalu</p></summary>
<p>

**Abstract:** Purpose: Deep Neuroevolution (DNE) holds the promise of providing radiology artificial intelligence (AI) that performs well with small neural networks and small training sets. We seek to realize this potential via a proof-of-principle application to MRI brain sequence classification.
  Methods: We analyzed a training set of 20 patients, each with four sequences/weightings: T1, T1 post-contrast, T2, and T2-FLAIR. We trained the parameters of a relatively small convolutional neural network (CNN) as follows: First, we randomly mutated the CNN weights. We then measured the CNN training set accuracy, using the latter as the fitness evaluation metric. The fittest child CNNs were identified. We incorporated their mutations into the parent CNN. This selectively mutated parent became the next generation's parent CNN. We repeated this process for approximately 50,000 generations.
  Results: DNE achieved monotonic convergence to 100% training set accuracy. DNE also converged monotonically to 100% testing set accuracy.
  Conclusions: DNE can achieve perfect accuracy with small training sets and small CNNs. Particularly when combined with Deep Reinforcement Learning, DNE may provide a path forward in the quest to make radiology AI more human-like in its ability to learn. DNE may very well turn out to be a key component of the much-anticipated meta-learning regime of radiology AI algorithms that can adapt to new tasks and new image types, similar to human radiologists.

</p>
</details>

<details><summary><b>Domain-Aware Continual Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2112.12989">arxiv:2112.12989</a>
&#x1F4C8; 2 <br>
<p>Kai Yi, Mohamed Elhoseiny</p></summary>
<p>

**Abstract:** We introduce Domain Aware Continual Zero-Shot Learning (DACZSL), the task of visually recognizing images of unseen categories in unseen domains sequentially. We created DACZSL on top of the DomainNet dataset by dividing it into a sequence of tasks, where classes are incrementally provided on seen domains during training and evaluation is conducted on unseen domains for both seen and unseen classes. We also proposed a novel Domain-Invariant CZSL Network (DIN), which outperforms state-of-the-art baseline models that we adapted to DACZSL setting. We adopt a structure-based approach to alleviate forgetting knowledge from previous tasks with a small per-task private network in addition to a global shared network. To encourage the private network to capture the domain and task-specific representation, we train our model with a novel adversarial knowledge disentanglement setting to make our global network task-invariant and domain-invariant over all the tasks. Our method also learns a class-wise learnable prompt to obtain better class-level text representation, which is used to represent side information to enable zero-shot prediction of future unseen classes. Our code and benchmarks will be made publicly available.

</p>
</details>

<details><summary><b>SGTR: End-to-end Scene Graph Generation with Transformer</b>
<a href="https://arxiv.org/abs/2112.12970">arxiv:2112.12970</a>
&#x1F4C8; 2 <br>
<p>Rongjie Li, Songyang Zhang, Xuming He</p></summary>
<p>

**Abstract:** Scene Graph Generation (SGG) remains a challenging visual understanding task due to its complex compositional property. Most previous works adopt a bottom-up two-stage or a point-based one-stage approach, which often suffers from overhead time complexity or sub-optimal design assumption. In this work, we propose a novel SGG method to address the aforementioned issues, which formulates the task as a bipartite graph construction problem. To solve the problem, we develop a transformer-based end-to-end framework that first generates the entity and predicate proposal set, followed by inferring directed edges to form the relation triplets. In particular, we develop a new entity-aware predicate representation based on a structural predicate generator to leverage the compositional property of relationships. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on two challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. We hope our model can serve as a strong baseline for the Transformer-based scene graph generation.

</p>
</details>

<details><summary><b>Supraventricular Tachycardia Detection and Classification Model of ECG signal Using Machine Learning</b>
<a href="https://arxiv.org/abs/2112.12953">arxiv:2112.12953</a>
&#x1F4C8; 2 <br>
<p>Pampa Howladar, Manodipan Sahoo</p></summary>
<p>

**Abstract:** Investigation on the electrocardiogram (ECG) signals is an essential way to diagnose heart disease since the ECG process is noninvasive and easy to use. This work presents a supraventricular arrhythmia prediction model consisting of a few stages, including filtering of noise, a unique collection of ECG characteristics, and automated learning classifying model to classify distinct types, depending on their severity. We de-trend and de-noise a signal to reduce noise to better determine functionality before extractions are performed. After that, we present one R-peak detection method and Q-S detection method as a part of necessary feature extraction. Next parameters are computed that correspond to these features. Using these characteristics, we have developed a classification model based on machine learning that can successfully categorize different types of supraventricular tachycardia. Our findings suggest that decision-tree-based models are the most efficient machine learning models for supraventricular tachycardia arrhythmia. Among all the machine learning models, this model most efficiently lowers the crucial signal misclassification of supraventricular tachycardia. Experimental results indicate satisfactory improvements and demonstrate a superior efficiency of the proposed approach with 97% accuracy.

</p>
</details>

<details><summary><b>AI-Bind: Improving Binding Predictions for Novel Protein Targets and Ligands</b>
<a href="https://arxiv.org/abs/2112.13168">arxiv:2112.13168</a>
&#x1F4C8; 1 <br>
<p>Ayan Chatterjee, Omair Shafi Ahmed, Robin Walters, Zohair Shafi, Deisy Gysi, Rose Yu, Tina Eliassi-Rad, Albert-LÃ¡szlÃ³ BarabÃ¡si, Giulia Menichetti</p></summary>
<p>

**Abstract:** Identifying novel drug-target interactions (DTI) is a critical and rate limiting step in drug discovery. While deep learning models have been proposed to accelerate the identification process, we show that state-of-the-art models fail to generalize to novel (i.e., never-before-seen) structures. We first unveil the mechanisms responsible for this shortcoming, demonstrating how models rely on shortcuts that leverage the topology of the protein-ligand bipartite network, rather than learning the node features. Then, we introduce AI-Bind, a pipeline that combines network-based sampling strategies with unsupervised pre-training, allowing us to limit the annotation imbalance and improve binding predictions for novel proteins and ligands. We illustrate the value of AI-Bind by predicting drugs and natural compounds with binding affinity to SARS-CoV-2 viral proteins and the associated human proteins. We also validate these predictions via auto-docking simulations and comparison with recent experimental evidence. Overall, AI-Bind offers a powerful high-throughput approach to identify drug-target combinations, with the potential of becoming a powerful tool in drug discovery.

</p>
</details>

<details><summary><b>A Survey on Interpretable Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2112.13112">arxiv:2112.13112</a>
&#x1F4C8; 1 <br>
<p>Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, Wulong Liu</p></summary>
<p>

**Abstract:** Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.

</p>
</details>

<details><summary><b>Fine-Tuning Data Structures for Analytical Query Processing</b>
<a href="https://arxiv.org/abs/2112.13099">arxiv:2112.13099</a>
&#x1F4C8; 1 <br>
<p>Amir Shaikhha, Marios Kelepeshis, Mahdi Ghorbani</p></summary>
<p>

**Abstract:** We introduce a framework for automatically choosing data structures to support efficient computation of analytical workloads. Our contributions are twofold. First, we introduce a novel low-level intermediate language that can express the algorithms behind various query processing paradigms such as classical joins, groupjoin, and in-database machine learning engines. This language is designed around the notion of dictionaries, and allows for a more fine-grained choice of its low-level implementation. Second, the cost model for alternative implementations is automatically inferred by combining machine learning and program reasoning. The dictionary cost model is learned using a regression model trained over the profiling dataset of dictionary operations on a given hardware architecture. The program cost model is inferred using static program analysis.
  Our experimental results show the effectiveness of the trained cost model on micro benchmarks. Furthermore, we show that the performance of the code generated by our framework either outperforms or is on par with the state-of-the-art analytical query engines and a recent in-database machine learning framework.

</p>
</details>

<details><summary><b>DARTS without a Validation Set: Optimizing the Marginal Likelihood</b>
<a href="https://arxiv.org/abs/2112.13023">arxiv:2112.13023</a>
&#x1F4C8; 1 <br>
<p>Miroslav Fil, Binxin Ru, Clare Lyle, Yarin Gal</p></summary>
<p>

**Abstract:** The success of neural architecture search (NAS) has historically been limited by excessive compute requirements. While modern weight-sharing NAS methods such as DARTS are able to finish the search in single-digit GPU days, extracting the final best architecture from the shared weights is notoriously unreliable. Training-Speed-Estimate (TSE), a recently developed generalization estimator with a Bayesian marginal likelihood interpretation, has previously been used in place of the validation loss for gradient-based optimization in DARTS. This prevents the DARTS skip connection collapse, which significantly improves performance on NASBench-201 and the original DARTS search space. We extend those results by applying various DARTS diagnostics and show several unusual behaviors arising from not using a validation set. Furthermore, our experiments yield concrete examples of the depth gap and topology selection in DARTS having a strongly negative impact on the search performance despite generally receiving limited attention in the literature compared to the operations selection.

</p>
</details>

<details><summary><b>Stochastic Learning Equation using Monotone Increasing Resolution of Quantization</b>
<a href="https://arxiv.org/abs/2112.13006">arxiv:2112.13006</a>
&#x1F4C8; 1 <br>
<p>Jinwuk Seok, Jeong-Si Kim</p></summary>
<p>

**Abstract:** In this paper, we propose a quantized learning equation with a monotone increasing resolution of quantization and stochastic analysis for the proposed algorithm. According to the white noise hypothesis for the quantization error with dense and uniform distribution, we can regard the quantization error as i.i.d.\ white noise. Based on this, we show that the learning equation with monotonically increasing quantization resolution converges weakly as the distribution viewpoint. The analysis of this paper shows that global optimization is possible for a domain that satisfies the Lipschitz condition instead of local convergence properties such as the Hessian constraint of the objective function.

</p>
</details>

<details><summary><b>DeepGANTT: A Scalable Deep Learning Scheduler for Backscatter Networks</b>
<a href="https://arxiv.org/abs/2112.12985">arxiv:2112.12985</a>
&#x1F4C8; 1 <br>
<p>Daniel F. Perez-Ramirez, Carlos Perez-Penichet, Nicolas Tsiftes, Thiemo Voigt, Dejan Kostic, Magnus Boman</p></summary>
<p>

**Abstract:** Recent backscatter communication techniques enable ultra low power wireless devices that operate without batteries while interoperating directly with unmodified commodity wireless devices. Commodity devices cooperate in providing the unmodulated carrier that the battery-free nodes need to communicate while collecting energy from their environment to perform sensing, computation, and communication tasks. The optimal provision of the unmodulated carrier limits the size of the network because it is an NP-hard combinatorial optimization problem. Consequently, previous works either ignore carrier optimization altogether or resort to suboptimal heuristics, wasting valuable energy and spectral resources. We present DeepGANTT, a deep learning scheduler for battery-free devices interoperating with wireless commodity ones. DeepGANTT leverages graph neural networks to overcome variable input and output size challenges inherent to this problem. We train our deep learning scheduler with optimal schedules of relatively small size obtained from a constraint optimization solver. DeepGANTT not only outperforms a carefully crafted heuristic solution but also performs within ~3% of the optimal scheduler on trained problem sizes. Finally, DeepGANTT generalizes to problems more than four times larger than the maximum used for training, therefore breaking the scalability limitations of the optimal scheduler and paving the way for more efficient backscatter networks.

</p>
</details>

<details><summary><b>Error-bounded Approximate Time Series Joins using Compact Dictionary Representations of Time Series</b>
<a href="https://arxiv.org/abs/2112.12965">arxiv:2112.12965</a>
&#x1F4C8; 1 <br>
<p>Chin-Chia Michael Yeh, Yan Zheng, Junpeng Wang, Huiyuan Chen, Zhongfang Zhuang, Wei Zhang, Eamonn Keogh</p></summary>
<p>

**Abstract:** The matrix profile is an effective data mining tool that provides similarity join functionality for time series data. Users of the matrix profile can either join a time series with itself using intra-similarity join (i.e., self-join) or join a time series with another time series using inter-similarity join. By invoking either or both types of joins, the matrix profile can help users discover both conserved and anomalous structures in the data. Since the introduction of the matrix profile five years ago, multiple efforts have been made to speed up the computation with approximate joins; however, the majority of these efforts only focus on self-joins. In this work, we show that it is possible to efficiently perform approximate inter-time series similarity joins with error bounded guarantees by creating a compact "dictionary" representation of time series. Using the dictionary representation instead of the original time series, we are able to improve the throughput of an anomaly mining system by at least 20X, with essentially no decrease in accuracy. As a side effect, the dictionaries also summarize the time series in a semantically meaningful way and can provide intuitive and actionable insights. We demonstrate the utility of our dictionary-based inter-time series similarity joins on domains as diverse as medicine and transportation.

</p>
</details>

<details><summary><b>Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces</b>
<a href="https://arxiv.org/abs/2112.12961">arxiv:2112.12961</a>
&#x1F4C8; 1 <br>
<p>Chaoxia Yuan, Chao Ying, Zhou Yu, Fang Fang</p></summary>
<p>

**Abstract:** Support vector machine (SVM) is a powerful classification method that has achieved great success in many fields. Since its performance can be seriously impaired by redundant covariates, model selection techniques are widely used for SVM with high dimensional covariates. As an alternative to model selection, significant progress has been made in the area of model averaging in the past decades. Yet no frequentist model averaging method was considered for SVM. This work aims to fill the gap and to propose a frequentist model averaging procedure for SVM which selects the optimal weight by cross validation. Even when the number of covariates diverges at an exponential rate of the sample size, we show asymptotic optimality of the proposed method in the sense that the ratio of its hinge loss to the lowest possible loss converges to one. We also derive the convergence rate which provides more insights to model averaging. Compared to model selection methods of SVM which require a tedious but critical task of tuning parameter selection, the model averaging method avoids the task and shows promising performances in the empirical studies.

</p>
</details>

<details><summary><b>Noninvasive Fetal Electrocardiography: Models, Technologies and Algorithms</b>
<a href="https://arxiv.org/abs/2112.13021">arxiv:2112.13021</a>
&#x1F4C8; 0 <br>
<p>Reza Sameni</p></summary>
<p>

**Abstract:** The fetal electrocardiogram (fECG) was first recorded from the maternal abdominal surface in the early 1900s. During the past fifty years, the most advanced electronics technologies and signal processing algorithms have been used to convert noninvasive fetal electrocardiography into a reliable technology for fetal cardiac monitoring. In this chapter, the major signal processing techniques, which have been developed for the modeling, extraction and analysis of the fECG from noninvasive maternal abdominal recordings are reviewed and compared with one another in detail. The major topics of the chapter include: 1) the electrophysiology of the fECG from the signal processing viewpoint, 2) the mathematical model of the maternal volume conduction media and the waveform models of the fECG acquired from body surface leads, 3) the signal acquisition requirements, 4) model-based techniques for fECG noise and interference cancellation, including adaptive filters and semi-blind source separation techniques, and 5) recent algorithmic advances for fetal motion tracking and online fECG extraction from few number of channels.

</p>
</details>

<details><summary><b>A machine learning pipeline for autonomous numerical analytic continuation of Dyson-Schwinger equations</b>
<a href="https://arxiv.org/abs/2112.13011">arxiv:2112.13011</a>
&#x1F4C8; 0 <br>
<p>Andreas Windisch, Thomas Gallien, Christopher Schwarzlmueller</p></summary>
<p>

**Abstract:** Dyson-Schwinger equations (DSEs) are a non-perturbative way to express n-point functions in quantum field theory. Working in Euclidean space and in Landau gauge, for example, one can study the quark propagator Dyson-Schwinger equation in the real and complex domain, given that a suitable and tractable truncation has been found. When aiming for solving these equations in the complex domain, that is, for complex external momenta, one has to deform the integration contour of the radial component in the complex plane of the loop momentum expressed in hyper-spherical coordinates. This has to be done in order to avoid poles and branch cuts in the integrand of the self-energy loop. Since the nature of Dyson-Schwinger equations is such, that they have to be solved in a self-consistent way, one cannot analyze the analytic properties of the integrand after every iteration step, as this would not be feasible. In these proceedings, we suggest a machine learning pipeline based on deep learning (DL) approaches to computer vision (CV), as well as deep reinforcement learning (DRL), that could solve this problem autonomously by detecting poles and branch cuts in the numerical integrand after every iteration step and by suggesting suitable integration contour deformations that avoid these obstructions. We sketch out a proof of principle for both of these tasks, that is, the pole and branch cut detection, as well as the contour deformation.

</p>
</details>

<details><summary><b>Total Energy Shaping with Neural Interconnection and Damping Assignment -- Passivity Based Control</b>
<a href="https://arxiv.org/abs/2112.12999">arxiv:2112.12999</a>
&#x1F4C8; 0 <br>
<p>Santiago Sanchez-Escalonilla, Rodolfo Reyes-Baez, Bayu Jayawardhana</p></summary>
<p>

**Abstract:** In this work we exploit the universal approximation property of Neural Networks (NNs) to design interconnection and damping assignment (IDA) passivity-based control (PBC) schemes for fully-actuated mechanical systems in the port-Hamiltonian (pH) framework. To that end, we transform the IDA-PBC method into a supervised learning problem that solves the partial differential matching equations, and fulfills equilibrium assignment and Lyapunov stability conditions. A main consequence of this, is that the output of the learning algorithm has a clear control-theoretic interpretation in terms of passivity and Lyapunov stability. The proposed control design methodology is validated for mechanical systems of one and two degrees-of-freedom via numerical simulations.

</p>
</details>

<details><summary><b>Integrating Physics-Based Modeling with Machine Learning for Lithium-Ion Batteries</b>
<a href="https://arxiv.org/abs/2112.12979">arxiv:2112.12979</a>
&#x1F4C8; 0 <br>
<p>Hao Tu, Scott Moura, Yebin Wang, Huazhen Fang</p></summary>
<p>

**Abstract:** Mathematical modeling of lithium-ion batteries (LiBs) is a primary challenge in advanced battery management. This paper proposes two new frameworks to integrate a physics-based model with machine learning to achieve high-precision modeling for LiBs. The frameworks are characterized by informing the machine learning model of the state information of the physical model, enabling a deep integration between physics and machine learning. Based on the frameworks, a series of hybrid models are constructed, through combining an electrochemical model and an equivalent circuit model, respectively, with a feedforward neural network. The hybrid models are relatively parsimonious in structure and can provide considerable predictive accuracy under a broad range of C-rates, as shown by extensive simulations and experiments. The study further expands to conduct aging-aware hybrid modeling, leading to the design of a hybrid model conscious of the state-of-health to make prediction. Experiments show that the model has high predictive accuracy throughout a LiB's cycle life.

</p>
</details>

<details><summary><b>Machine Learning-based Efficient Ventricular Tachycardia Detection Model of ECG Signal</b>
<a href="https://arxiv.org/abs/2112.12956">arxiv:2112.12956</a>
&#x1F4C8; 0 <br>
<p>Pampa Howladar, Manodipan Sahoo</p></summary>
<p>

**Abstract:** In primary diagnosis and analysis of heart defects, an ECG signal plays a significant role. This paper presents a model for the prediction of ventricular tachycardia arrhythmia using noise filtering, a unique set of ECG features, and a machine learning-based classifier model. Before signal feature extraction, we detrend and denoise the signal to eliminate the noise for detecting features properly. After that necessary features have been extracted and necessary parameters related to these features are measured. Using these parameters, we prepared one efficient multiclass classifier model using a machine learning approach that can classify different types of ventricular tachycardia arrhythmias efficiently. Our results indicate that Logistic regression and Decision tree-based models are the most efficient machine learning models for detecting ventricular tachycardia arrhythmia. In order to diagnose heart diseases and find care for a patient, an early, reliable diagnosis of different types of arrhythmia is necessary. By implementing our proposed method, this work deals with the problem of reducing the misclassification of the critical signal related to ventricular tachycardia very efficiently. Experimental findings demonstrate satisfactory enhancements and demonstrate high resilience to the algorithm that we have proposed. With this assistance, doctors can assess this type of arrhythmia of a patient early and take the right decision at the proper time.

</p>
</details>


{% endraw %}
Prev: [2021.12.23]({{ '/2021/12/23/2021.12.23.html' | relative_url }})  Next: [2021.12.25]({{ '/2021/12/25/2021.12.25.html' | relative_url }})