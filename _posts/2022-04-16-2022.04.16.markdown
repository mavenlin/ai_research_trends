Prev: [2022.04.15]({{ '/2022/04/15/2022.04.15.html' | relative_url }})  Next: [2022.04.17]({{ '/2022/04/17/2022.04.17.html' | relative_url }})
{% raw %}
## Summary for 2022-04-16, created on 2022-04-20


<details><summary><b>TVShowGuess: Character Comprehension in Stories as Speaker Guessing</b>
<a href="https://arxiv.org/abs/2204.07721">arxiv:2204.07721</a>
&#x1F4C8; 4 <br>
<p>Yisi Sang, Xiangyang Mou, Mo Yu, Shunyu Yao, Jing Li, Jeffrey Stanton</p></summary>
<p>

**Abstract:** We propose a new task for assessing machines' skills of understanding fictional characters in narrative stories. The task, TVShowGuess, builds on the scripts of TV series and takes the form of guessing the anonymous main characters based on the backgrounds of the scenes and the dialogues. Our human study supports that this form of task covers comprehension of multiple types of character persona, including understanding characters' personalities, facts and memories of personal experience, which are well aligned with the psychological and literary theories about the theory of mind (ToM) of human beings on understanding fictional characters during reading. We further propose new model architectures to support the contextualized encoding of long scene texts. Experiments show that our proposed approaches significantly outperform baselines, yet still largely lag behind the (nearly perfect) human performance. Our work serves as a first step toward the goal of narrative character comprehension.

</p>
</details>

<details><summary><b>Visual Attention Methods in Deep Learning: An In-Depth Survey</b>
<a href="https://arxiv.org/abs/2204.07756">arxiv:2204.07756</a>
&#x1F4C8; 3 <br>
<p>Mohammed Hassanin, Saeed Anwar, Ibrahim Radwan, Fahad S Khan, Ajmal Mian</p></summary>
<p>

**Abstract:** Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated in one network. Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey specific to attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and open questions related to attention mechanism in general. Finally, we recommend possible future research directions for deep attention.

</p>
</details>


{% endraw %}
Prev: [2022.04.15]({{ '/2022/04/15/2022.04.15.html' | relative_url }})  Next: [2022.04.17]({{ '/2022/04/17/2022.04.17.html' | relative_url }})