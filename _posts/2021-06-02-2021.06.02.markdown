{% raw %}
## Summary for 2021-06-02, created on 2021-12-20


<details><summary><b>Decision Transformer: Reinforcement Learning via Sequence Modeling</b>
<a href="https://arxiv.org/abs/2106.01345">arxiv:2106.01345</a>
&#x1F4C8; 375 <br>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch</p></summary>
<p>

**Abstract:** We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.

</p>
</details>

<details><summary><b>SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training</b>
<a href="https://arxiv.org/abs/2106.01342">arxiv:2106.01342</a>
&#x1F4C8; 123 <br>
<p>Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, Tom Goldstein</p></summary>
<p>

**Abstract:** Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.

</p>
</details>

<details><summary><b>A Generalizable Approach to Learning Optimizers</b>
<a href="https://arxiv.org/abs/2106.00958">arxiv:2106.00958</a>
&#x1F4C8; 88 <br>
<p>Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba</p></summary>
<p>

**Abstract:** A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks.

</p>
</details>

<details><summary><b>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</b>
<a href="https://arxiv.org/abs/2106.01317">arxiv:2106.01317</a>
&#x1F4C8; 69 <br>
<p>Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng Gao</p></summary>
<p>

**Abstract:** Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-TRANSFORMER outperforms the Transformer and the original TP-TRANSFORMER significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and improved syntactic interpretability in the TPR layer outputs. Code and models are available at https://github.com/jiangycTarheel/TPT-Summ.

</p>
</details>

<details><summary><b>Towards Robust Classification Model by Counterfactual and Invariant Data Generation</b>
<a href="https://arxiv.org/abs/2106.01127">arxiv:2106.01127</a>
&#x1F4C8; 66 <br>
<p>Chun-Hao Chang, George Alexandru Adam, Anna Goldenberg</p></summary>
<p>

**Abstract:** Despite the success of machine learning applications in science, industry, and society in general, many approaches are known to be non-robust, often relying on spurious correlations to make predictions. Spuriousness occurs when some features correlate with labels but are not causal; relying on such features prevents models from generalizing to unseen environments where such correlations break. In this work, we focus on image classification and propose two data generation processes to reduce spuriousness. Given human annotations of the subset of the features responsible (causal) for the labels (e.g. bounding boxes), we modify this causal set to generate a surrogate image that no longer has the same label (i.e. a counterfactual image). We also alter non-causal features to generate images still recognized as the original labels, which helps to learn a model invariant to these features. In several challenging datasets, our data generations outperform state-of-the-art methods in accuracy when spurious correlations break, and increase the saliency focus on causal features providing better explanations.

</p>
</details>

<details><summary><b>Not All Knowledge Is Created Equal</b>
<a href="https://arxiv.org/abs/2106.01489">arxiv:2106.01489</a>
&#x1F4C8; 60 <br>
<p>Ziyun Li, Xinshao Wang, Haojin Yang, Di Hu, Neil M. Robertson, David A. Clifton, Christoph Meinel</p></summary>
<p>

**Abstract:** Mutual knowledge distillation (MKD) improves a model by distilling knowledge from another model. However, not all knowledge is certain and correct, especially under adverse conditions. For example, label noise usually leads to less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge misleads the learning rather than helps. This problem can be handled by two aspects: (i) improving the reliability of a model where the knowledge is from (i.e., knowledge source's reliability); (ii) selecting reliable knowledge for distillation. In the literature, making a model more reliable is widely studied while selective MKD receives little attention. Therefore, we focus on studying selective MKD and highlight its importance in this work.
  Concretely, a generic MKD framework, Confident knowledge selection followed by Mutual Distillation (CMD), is designed. The key component of CMD is a generic knowledge selection formulation, making the selection threshold either static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special cases: zero knowledge and all knowledge, leading to a unified MKD framework. We empirically find CMD-P performs better than CMD-S. The main reason is that a model's knowledge upgrades and becomes confident as the training progresses.
  Extensive experiments are present to demonstrate the effectiveness of CMD and thoroughly justify the design of CMD. For example, CMD-P obtains new state-of-the-art results in robustness against label noise.

</p>
</details>

<details><summary><b>Database Reasoning Over Text</b>
<a href="https://arxiv.org/abs/2106.01074">arxiv:2106.01074</a>
&#x1F4C8; 31 <br>
<p>James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Halevy</p></summary>
<p>

**Abstract:** Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as "List/Count all female athletes who were born in 20th century", which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.

</p>
</details>

<details><summary><b>MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network</b>
<a href="https://arxiv.org/abs/2106.07352">arxiv:2106.07352</a>
&#x1F4C8; 28 <br>
<p>Nicholas FitzGerald, Jan A. Botha, Daniel Gillick, Daniel M. Bikel, Tom Kwiatkowski, Andrew McCallum</p></summary>
<p>

**Abstract:** We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as "class prototypes" as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor's entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks.

</p>
</details>

<details><summary><b>LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</b>
<a href="https://arxiv.org/abs/2106.01487">arxiv:2106.01487</a>
&#x1F4C8; 24 <br>
<p>Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi</p></summary>
<p>

**Abstract:** Learning binary representations of instances and classes is a classical problem with several high potential applications. In modern settings, the compression of high-dimensional neural representations to low-dimensional binary codes is a challenging task and often require large bit-codes to be accurate. In this work, we propose a novel method for Learning Low-dimensional binary Codes (LLC) for instances as well as classes. Our method does not require any side-information, like annotated attributes or label meta-data, and learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The learnt codes are super-efficient while still ensuring nearly optimal classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the learnt codes capture intrinsically important features in the data, by discovering an intuitive taxonomy over classes. We further quantitatively measure the quality of our codes by applying it to the efficient image retrieval as well as out-of-distribution (OOD) detection problems. For ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit HashNet using only 10 bits and also are as accurate as 10 dimensional real representations. Finally, our learnt binary codes can perform OOD detection, out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune its threshold, while we require none. Code is open-sourced at https://github.com/RAIVNLab/LLC.

</p>
</details>

<details><summary><b>multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning</b>
<a href="https://arxiv.org/abs/2106.01354">arxiv:2106.01354</a>
&#x1F4C8; 22 <br>
<p>Swarnadeep Saha, Prateek Yadav, Mohit Bansal</p></summary>
<p>

**Abstract:** We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent. Our code and models are publicly available at https://github.com/swarnaHub/multiPRover

</p>
</details>

<details><summary><b>Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?</b>
<a href="https://arxiv.org/abs/2106.01465">arxiv:2106.01465</a>
&#x1F4C8; 20 <br>
<p>Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Kai-Wei Chang</p></summary>
<p>

**Abstract:** Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.

</p>
</details>

<details><summary><b>The Limitations of Limited Context for Constituency Parsing</b>
<a href="https://arxiv.org/abs/2106.01580">arxiv:2106.01580</a>
&#x1F4C8; 19 <br>
<p>Yuchen Li, Andrej Risteski</p></summary>
<p>

**Abstract:** Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits. For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing. Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019). Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score).
  However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG.

</p>
</details>

<details><summary><b>Undecidability of Learnability</b>
<a href="https://arxiv.org/abs/2106.01382">arxiv:2106.01382</a>
&#x1F4C8; 13 <br>
<p>Matthias C. Caro</p></summary>
<p>

**Abstract:** Machine learning researchers and practitioners steadily enlarge the multitude of successful learning models. They achieve this through in-depth theoretical analyses and experiential heuristics. However, there is no known general-purpose procedure for rigorously evaluating whether newly proposed models indeed successfully learn from data. We show that such a procedure cannot exist. For PAC binary classification, uniform and universal online learning, and exact learning through teacher-learner interactions, learnability is in general undecidable, both in the sense of independence of the axioms in a formal system and in the sense of uncomputability. Our proofs proceed via computable constructions of function classes that encode the consistency problem for formal systems and the halting problem for Turing machines into complexity measures that characterize learnability. Our work shows that undecidability appears in the theoretical foundations of machine learning: There is no one-size-fits-all algorithm for deciding whether a machine learning model can be successful. We cannot in general automatize the process of assessing new learning models.

</p>
</details>

<details><summary><b>Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions</b>
<a href="https://arxiv.org/abs/2106.01098">arxiv:2106.01098</a>
&#x1F4C8; 13 <br>
<p>Leslie O'Bray, Max Horn, Bastian Rieck, Karsten Borgwardt</p></summary>
<p>

**Abstract:** Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models.

</p>
</details>

<details><summary><b>NVC-Net: End-to-End Adversarial Voice Conversion</b>
<a href="https://arxiv.org/abs/2106.00992">arxiv:2106.00992</a>
&#x1F4C8; 12 <br>
<p>Bac Nguyen, Fabien Cardinaux</p></summary>
<p>

**Abstract:** Voice conversion has gained increasing popularity in many applications of speech synthesis. The idea is to change the voice identity from one speaker into another while keeping the linguistic content unchanged. Many voice conversion approaches rely on the use of a vocoder to reconstruct the speech from acoustic features, and as a consequence, the speech quality heavily depends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end adversarial network, which performs voice conversion directly on the raw audio waveform of arbitrary length. By disentangling the speaker identity from the speech content, NVC-Net is able to perform non-parallel traditional many-to-many voice conversion as well as zero-shot voice conversion from a short utterance of an unseen target speaker. Importantly, NVC-Net is non-autoregressive and fully convolutional, achieving fast inference. Our model is capable of producing samples at a rate of more than 3600 kHz on an NVIDIA V100 GPU, being orders of magnitude faster than state-of-the-art methods under the same hardware configurations. Objective and subjective evaluations on non-parallel many-to-many voice conversion tasks show that NVC-Net obtains competitive results with significantly fewer parameters.

</p>
</details>

<details><summary><b>Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour</b>
<a href="https://arxiv.org/abs/2106.01434">arxiv:2106.01434</a>
&#x1F4C8; 10 <br>
<p>Xihan Bian, Oscar Mendez, Simon Hadfield</p></summary>
<p>

**Abstract:** Robots need to be able to work in multiple different environments. Even when performing similar tasks, different behaviour should be deployed to best fit the current environment. In this paper, We propose a new approach to navigation, where it is treated as a multi-task learning problem. This enables the robot to learn to behave differently in visual navigation tasks for different environments while also learning shared expertise across environments. We evaluated our approach in both simulated environments as well as real-world data. Our method allows our system to converge with a 26% reduction in training time, while also increasing accuracy.

</p>
</details>

<details><summary><b>Evidence-based Factual Error Correction</b>
<a href="https://arxiv.org/abs/2106.01072">arxiv:2106.01072</a>
&#x1F4C8; 10 <br>
<p>James Thorne, Andreas Vlachos</p></summary>
<p>

**Abstract:** This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.

</p>
</details>

<details><summary><b>SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues</b>
<a href="https://arxiv.org/abs/2106.01006">arxiv:2106.01006</a>
&#x1F4C8; 10 <br>
<p>Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying Nian Wu, Song-Chun Zhu</p></summary>
<p>

**Abstract:** Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly. We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues. Moreover, we formulate a sequential structure prediction task, and propose an $α$-$β$-$γ$ strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an $α$ process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a $β$ process updating the social relations based on related attributes, and (iii) a $γ$ process updating individual's attributes based on interpersonal social relations. Empirical results on DialogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods. Moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference.

</p>
</details>

<details><summary><b>Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights</b>
<a href="https://arxiv.org/abs/2106.05852">arxiv:2106.05852</a>
&#x1F4C8; 9 <br>
<p>Devaraja Adiga, Rishabh Kumar, Amrith Krishna, Preethi Jyothi, Ganesh Ramakrishnan, Pawan Goyal</p></summary>
<p>

**Abstract:** Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a new modelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts.

</p>
</details>

<details><summary><b>The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop</b>
<a href="https://arxiv.org/abs/2106.01364">arxiv:2106.01364</a>
&#x1F4C8; 9 <br>
<p>Jong-Chyi Su, Subhransu Maji</p></summary>
<p>

**Abstract:** Semi-iNat is a challenging dataset for semi-supervised classification with a long-tailed distribution of classes, fine-grained categories, and domain shifts between labeled and unlabeled data. This dataset is behind the second iteration of the semi-supervised recognition challenge to be held at the FGVC8 workshop at CVPR 2021. Different from the previous one, this dataset (i) includes images of species from different kingdoms in the natural taxonomy, (ii) is at a larger scale -- with 810 in-class and 1629 out-of-class species for a total of 330k images, and (iii) does not provide in/out-of-class labels, but provides coarse taxonomic labels (kingdom and phylum) for the unlabeled images. This document describes baseline results and the details of the dataset which is available here: \url{https://github.com/cvl-umass/semi-inat-2021}.

</p>
</details>

<details><summary><b>NeRP: Neural Rearrangement Planning for Unknown Objects</b>
<a href="https://arxiv.org/abs/2106.01352">arxiv:2106.01352</a>
&#x1F4C8; 9 <br>
<p>Ahmed H. Qureshi, Arsalan Mousavian, Chris Paxton, Michael C. Yip, Dieter Fox</p></summary>
<p>

**Abstract:** Robots will be expected to manipulate a wide variety of objects in complex and arbitrary ways as they become more widely used in human environments. As such, the rearrangement of objects has been noted to be an important benchmark for AI capabilities in recent years. We propose NeRP (Neural Rearrangement Planning), a deep learning based approach for multi-step neural object rearrangement planning which works with never-before-seen objects, that is trained on simulation data, and generalizes to the real world. We compare NeRP to several naive and model-based baselines, demonstrating that our approach is measurably better and can efficiently arrange unseen objects in fewer steps and with less planning time. Finally, we demonstrate it on several challenging rearrangement problems in the real world.

</p>
</details>

<details><summary><b>Framing RNN as a kernel method: A neural ODE approach</b>
<a href="https://arxiv.org/abs/2106.01202">arxiv:2106.01202</a>
&#x1F4C8; 9 <br>
<p>Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, Gérard Biau</p></summary>
<p>

**Abstract:** Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets.

</p>
</details>

<details><summary><b>SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis</b>
<a href="https://arxiv.org/abs/2106.01444">arxiv:2106.01444</a>
&#x1F4C8; 8 <br>
<p>Joshua Feinglass, Yezhou Yang</p></summary>
<p>

**Abstract:** The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce "typicality", a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics. Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties. Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences. Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics.

</p>
</details>

<details><summary><b>Optimization of Heterogeneous Systems with AI Planning Heuristics and Machine Learning: A Performance and Energy Aware Approach</b>
<a href="https://arxiv.org/abs/2106.01441">arxiv:2106.01441</a>
&#x1F4C8; 8 <br>
<p>Suejb Memeti, Sabri Pllana</p></summary>
<p>

**Abstract:** Heterogeneous computing systems provide high performance and energy efficiency. However, to optimally utilize such systems, solutions that distribute the work across host CPUs and accelerating devices are needed. In this paper, we present a performance and energy aware approach that combines AI planning heuristics for parameter space exploration with a machine learning model for performance and energy evaluation to determine a near-optimal system configuration. For data-parallel applications our approach determines a near-optimal host-device distribution of work, number of processing units required and the corresponding scheduling strategy. We evaluate our approach for various heterogeneous systems accelerated with GPU or the Intel Xeon Phi. The experimental results demonstrate that our approach finds a near-optimal system configuration by evaluating only about 7% of reasonable configurations. Furthermore, the performance per Joule estimation of system configurations using our machine learning model is more than 1000x faster compared to the system evaluation by program execution.

</p>
</details>

<details><summary><b>Data augmentation and pre-trained networks for extremely low data regimes unsupervised visual inspection</b>
<a href="https://arxiv.org/abs/2106.01277">arxiv:2106.01277</a>
&#x1F4C8; 8 <br>
<p>Pierre Gutierrez, Antoine Cordier, Thaïs Caldeira, Théophile Sautory</p></summary>
<p>

**Abstract:** The use of deep features coming from pre-trained neural networks for unsupervised anomaly detection purposes has recently gathered momentum in the computer vision field. In particular, industrial inspection applications can take advantage of such features, as demonstrated by the multiple successes of related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These methods make use of neural networks pre-trained on auxiliary classification tasks such as ImageNet. However, to our knowledge, no comparative study of robustness to the low data regimes between these approaches has been conducted yet. For quality inspection applications, the handling of limited sample sizes may be crucial as large quantities of images are not available for small series. In this work, we aim to compare three approaches based on deep pre-trained features when varying the quantity of available data in MVTec AD: KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly robust to small sample sizes, they still can benefit greatly from using data augmentation in the original image space, which allows to deal with very small production runs.

</p>
</details>

<details><summary><b>GemNet: Universal Directional Graph Neural Networks for Molecules</b>
<a href="https://arxiv.org/abs/2106.08903">arxiv:2106.08903</a>
&#x1F4C8; 7 <br>
<p>Johannes Klicpera, Florian Becker, Stephan Günnemann</p></summary>
<p>

**Abstract:** Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with directed edge embeddings and two-hop message passing are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then leverage these insights and multiple structural improvements to propose the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online.

</p>
</details>

<details><summary><b>When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations</b>
<a href="https://arxiv.org/abs/2106.01548">arxiv:2106.01548</a>
&#x1F4C8; 7 <br>
<p>Xiangning Chen, Cho-Jui Hsieh, Boqing Gong</p></summary>
<p>

**Abstract:** Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and +11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. They also possess more perceptive attention maps. Our model checkpoints are released at \url{https://github.com/google-research/vision_transformer}.

</p>
</details>

<details><summary><b>Online and Real-Time Tracking in a Surveillance Scenario</b>
<a href="https://arxiv.org/abs/2106.01153">arxiv:2106.01153</a>
&#x1F4C8; 7 <br>
<p>Oliver Urbann, Oliver Bredtmann, Maximilian Otten, Jan-Philip Richter, Thilo Bauer, David Zibriczky</p></summary>
<p>

**Abstract:** This paper presents an approach for tracking in a surveillance scenario. Typical aspects for this scenario are a 24/7 operation with a static camera mounted above the height of a human with many objects or people. The Multiple Object Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show that our approach is real-time capable on this benchmark and outperforms all other real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by contributing a fast Siamese network reformulated for linear runtime (instead of quadratic) to generate fingerprints from detections. Thus, it is possible to associate the detections to Kalman filters based on multiple tracking specific ratings: Cosine similarity of fingerprints, Intersection over Union, and pixel distance ratio in the image.

</p>
</details>

<details><summary><b>Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning</b>
<a href="https://arxiv.org/abs/2106.01132">arxiv:2106.01132</a>
&#x1F4C8; 7 <br>
<p>Benoit Dufumier, Pietro Gori, Ilaria Battaglia, Julie Victor, Antoine Grigis, Edouard Duchesnay</p></summary>
<p>

**Abstract:** Deep Learning (DL) and specifically CNN models have become a de facto method for a wide range of vision tasks, outperforming traditional machine learning (ML) methods. Consequently, they drew a lot of attention in the neuroimaging field in particular for phenotype prediction or computer-aided diagnosis. However, most of the current studies often deal with small single-site cohorts, along with a specific pre-processing pipeline and custom CNN architectures, which make them difficult to compare to. We propose an extensive benchmark of recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM) pre-processing and quasi-raw images. Experiments were conducted on a large multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3 challenging tasks: age prediction, sex classification, and schizophrenia diagnosis. We found that all models provide significantly better predictions with VBM images than quasi-raw data. This finding evolved as the training set approaches 10k samples where quasi-raw data almost reach the performance of VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter version that we proposed, provide a good compromise in terms of performance in all data regime. Therefore, we suggest to employ them as the architectures by default. Critically, we also showed that current CNN are still very biased towards the acquisition site, even when trained with N=10k multi-site images. In this context, VBM pre-processing provides an efficient way to limit this site effect. Surprisingly, we did not find any clear benefit from data augmentation techniques. Finally, we proved that deep ensemble learning is well suited to re-calibrate big CNN models without sacrificing performance.

</p>
</details>

<details><summary><b>Online Coreset Selection for Rehearsal-based Continual Learning</b>
<a href="https://arxiv.org/abs/2106.01085">arxiv:2106.01085</a>
&#x1F4C8; 7 <br>
<p>Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang</p></summary>
<p>

**Abstract:** A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a target dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner.

</p>
</details>

<details><summary><b>Expected Scalarised Returns Dominance: A New Solution Concept for Multi-Objective Decision Making</b>
<a href="https://arxiv.org/abs/2106.01048">arxiv:2106.01048</a>
&#x1F4C8; 7 <br>
<p>Conor F. Hayes, Timothy Verstraeten, Diederik M. Roijers, Enda Howley, Patrick Mannion</p></summary>
<p>

**Abstract:** In many real-world scenarios, the utility of a user is derived from the single execution of a policy. In this case, to apply multi-objective reinforcement learning, the expected utility of the returns must be optimised. Various scenarios exist where a user's preferences over objectives (also known as the utility function) are unknown or difficult to specify. In such scenarios, a set of optimal policies must be learned. However, settings where the expected utility must be maximised have been largely overlooked by the multi-objective reinforcement learning community and, as a consequence, a set of optimal solutions has yet to be defined. In this paper we address this challenge by proposing first-order stochastic dominance as a criterion to build solution sets to maximise expected utility. We also propose a new dominance criterion, known as expected scalarised returns (ESR) dominance, that extends first-order stochastic dominance to allow a set of optimal policies to be learned in practice. We then define a new solution concept called the ESR set, which is a set of policies that are ESR dominant. Finally, we define a new multi-objective distributional tabular reinforcement learning (MOT-DRL) algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.

</p>
</details>

<details><summary><b>A Systematic Investigation of KB-Text Embedding Alignment at Scale</b>
<a href="https://arxiv.org/abs/2106.01586">arxiv:2106.01586</a>
&#x1F4C8; 6 <br>
<p>Vardaan Pahuja, Yu Gu, Wenhu Chen, Mehdi Bahrami, Lei Liu, Wei-Peng Chen, Yu Su</p></summary>
<p>

**Abstract:** Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods. We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study.

</p>
</details>

<details><summary><b>Figurative Language in Recognizing Textual Entailment</b>
<a href="https://arxiv.org/abs/2106.01195">arxiv:2106.01195</a>
&#x1F4C8; 6 <br>
<p>Tuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, Smaranda Muresan</p></summary>
<p>

**Abstract:** We introduce a collection of recognizing textual entailment (RTE) datasets focused on figurative language. We leverage five existing datasets annotated for a variety of figurative language -- simile, metaphor, and irony -- and frame them into over 12,500 RTE examples.We evaluate how well state-of-the-art models trained on popular RTE datasets capture different aspects of figurative language. Our results and analyses indicate that these models might not sufficiently capture figurative language, struggling to perform pragmatic inference and reasoning about world knowledge. Ultimately, our datasets provide a challenging testbed for evaluating RTE models.

</p>
</details>

<details><summary><b>Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification</b>
<a href="https://arxiv.org/abs/2106.01191">arxiv:2106.01191</a>
&#x1F4C8; 6 <br>
<p>Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He</p></summary>
<p>

**Abstract:** Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https://github.com/jasenchn/TARSA.

</p>
</details>

<details><summary><b>Digital homotopy relations and digital homology theories</b>
<a href="https://arxiv.org/abs/2106.01171">arxiv:2106.01171</a>
&#x1F4C8; 6 <br>
<p>P. Christopher Staecker</p></summary>
<p>

**Abstract:** In this paper we prove results relating to two homotopy relations and four homology theories developed in the topology of digital images.
  We introduce a new type of homotopy relation for digitally continuous functions which we call "strong homotopy." Both digital homotopy and strong homotopy are natural digitizations of classical topological homotopy: the difference between them is analogous to the difference between digital 4-adjacency and 8-adjacency in the plane.
  We also consider four different digital homology theories: a simplicial homology theory by Arslan et al which is the homology of the clique complex, a singular simplicial homology theory by D. W. Lee, a cubical homology theory by Jamil and Ali, and a new kind of cubical homology for digital images with $c_1$-adjacency which is easily computed, and generalizes a construction by Karaca \& Ege. We show that the two simplicial homology theories are isomorphic to each other, but distinct from the two cubical theories.
  We also show that homotopic maps have the same induced homomorphisms in the cubical homology theory, and strong homotopic maps additionally have the same induced homomorphisms in the simplicial theory.

</p>
</details>

<details><summary><b>Deep Learning based Full-reference and No-reference Quality Assessment Models for Compressed UGC Videos</b>
<a href="https://arxiv.org/abs/2106.01111">arxiv:2106.01111</a>
&#x1F4C8; 6 <br>
<p>Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, Guangtao Zhai</p></summary>
<p>

**Abstract:** In this paper, we propose a deep learning based video quality assessment (VQA) framework to evaluate the quality of the compressed user's generated content (UGC) videos. The proposed VQA framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (CNN) network into final quality-aware feature representation, which enables the model to make full use of visual information from low-level to high-level. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (FR) VQA model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (NR) VQA model. For the quality regression module, we use the fully connected (FC) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the state-of-the-art FR and NR VQA models on the Compressed UGC VQA database and also achieves pretty good performance on the in-the-wild UGC VQA databases.

</p>
</details>

<details><summary><b>Unsupervised Out-of-Domain Detection via Pre-trained Transformers</b>
<a href="https://arxiv.org/abs/2106.00948">arxiv:2106.00948</a>
&#x1F4C8; 6 <br>
<p>Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, Caiming Xiong</p></summary>
<p>

**Abstract:** Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.

</p>
</details>

<details><summary><b>Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines</b>
<a href="https://arxiv.org/abs/2106.01506">arxiv:2106.01506</a>
&#x1F4C8; 5 <br>
<p>Matthew A. Wright, Joseph E. Gonzalez</p></summary>
<p>

**Abstract:** Despite their ubiquity in core AI fields like natural language processing, the mechanics of deep attention-based neural networks like the Transformer model are not fully understood. In this article, we present a new perspective towards understanding how Transformers work. In particular, we show that the "dot-product attention" that is the core of the Transformer's operation can be characterized as a kernel learning method on a pair of Banach spaces. In particular, the Transformer's kernel is characterized as having an infinite feature dimension. Along the way we consider an extension of the standard kernel learning problem to a binary setting, where data come from two input domains and a response is defined for every cross-domain pair. We prove a new representer theorem for these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels (implying that the functions learned are elements of reproducing kernel Banach spaces rather than Hilbert spaces), and also prove a new universal approximation theorem showing that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. We experiment with new kernels in Transformers, and obtain results that suggest the infinite dimensionality of the standard Transformer kernel is partially responsible for its performance. This paper's results provide a new theoretical understanding of a very important but poorly understood model in modern machine~learning.

</p>
</details>

<details><summary><b>Smooth Bilevel Programming for Sparse Regularization</b>
<a href="https://arxiv.org/abs/2106.01429">arxiv:2106.01429</a>
&#x1F4C8; 5 <br>
<p>Clarice Poon, Gabriel Peyré</p></summary>
<p>

**Abstract:** Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efficient but typically rely on specific coordinate pruning schemes. In this work, we show how a surprisingly simple reparametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators). Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there is no spurious minima and that saddle points are "ridable", so that there always exists a descent direction. We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach simple, robust and efficient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the specificity of the ML problem under study.

</p>
</details>

<details><summary><b>Single-component gradient rules for variational quantum algorithms</b>
<a href="https://arxiv.org/abs/2106.01388">arxiv:2106.01388</a>
&#x1F4C8; 5 <br>
<p>Thomas Hubregtsen, Frederik Wilde, Shozab Qasim, Jens Eisert</p></summary>
<p>

**Abstract:** Many near-term quantum computing algorithms are conceived as variational quantum algorithms, in which parameterized quantum circuits are optimized in a hybrid quantum-classical setup. Examples are variational quantum eigensolvers, quantum approximate optimization algorithms as well as various algorithms in the context of quantum-assisted machine learning. A common bottleneck of any such algorithm is constituted by the optimization of the variational parameters. A popular set of optimization methods work on the estimate of the gradient, obtained by means of circuit evaluations. We will refer to the way in which one can combine these circuit evaluations as gradient rules. This work provides a comprehensive picture of the family of gradient rules that vary parameters of quantum gates individually. The most prominent known members of this family are the parameter shift rule and the finite differences method. To unite this family, we propose a generalized parameter shift rule that expresses all members of the aforementioned family as special cases, and discuss how all of these can be seen as providing access to a linear combination of exact first- and second-order derivatives. We further prove that a parameter shift rule with one non-shifted evaluation and only one shifted circuit evaluation can not exist does not exist, and introduce a novel perspective for approaching new gradient rules.

</p>
</details>

<details><summary><b>On Efficiently Explaining Graph-Based Classifiers</b>
<a href="https://arxiv.org/abs/2106.01350">arxiv:2106.01350</a>
&#x1F4C8; 5 <br>
<p>Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, Joao Marques-Silva</p></summary>
<p>

**Abstract:** Recent work has shown that not only decision trees (DTs) may not be interpretable but also proposed a polynomial-time algorithm for computing one PI-explanation of a DT. This paper shows that for a wide range of classifiers, globally referred to as decision graphs, and which include decision trees and binary decision diagrams, but also their multi-valued variants, there exist polynomial-time algorithms for computing one PI-explanation. In addition, the paper also proposes a polynomial-time algorithm for computing one contrastive explanation. These novel algorithms build on explanation graphs (XpG's). XpG's denote a graph representation that enables both theoretical and practically efficient computation of explanations for decision graphs. Furthermore, the paper proposes a practically efficient solution for the enumeration of explanations, and studies the complexity of deciding whether a given feature is included in some explanation. For the concrete case of decision trees, the paper shows that the set of all contrastive explanations can be enumerated in polynomial time. Finally, the experimental results validate the practical applicability of the algorithms proposed in the paper on a wide range of publicly available benchmarks.

</p>
</details>

<details><summary><b>Addressing the Long-term Impact of ML Decisions via Policy Regret</b>
<a href="https://arxiv.org/abs/2106.01325">arxiv:2106.01325</a>
&#x1F4C8; 5 <br>
<p>David Lindner, Hoda Heidari, Andreas Krause</p></summary>
<p>

**Abstract:** Machine Learning (ML) increasingly informs the allocation of opportunities to individuals and communities in areas such as lending, education, employment, and beyond. Such decisions often impact their subjects' future characteristics and capabilities in an a priori unknown fashion. The decision-maker, therefore, faces exploration-exploitation dilemmas akin to those in multi-armed bandits. Following prior work, we model communities as arms. To capture the long-term effects of ML-based allocation decisions, we study a setting in which the reward from each arm evolves every time the decision-maker pulls that arm. We focus on reward functions that are initially increasing in the number of pulls but may become (and remain) decreasing after a certain point. We argue that an acceptable sequential allocation of opportunities must take an arm's potential for growth into account. We capture these considerations through the notion of policy regret, a much stronger notion than the often-studied external regret, and present an algorithm with provably sub-linear policy regret for sufficiently long time horizons. We empirically compare our algorithm with several baselines and find that it consistently outperforms them, in particular for long time horizons.

</p>
</details>

<details><summary><b>Bottom-Up and Top-Down Neural Processing Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2106.01288">arxiv:2106.01288</a>
&#x1F4C8; 5 <br>
<p>Charlotte Frenkel, David Bol, Giacomo Indiveri</p></summary>
<p>

**Abstract:** While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of new alternative brain-inspired computing architectures that promise to achieve the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic intelligence represents a paradigm shift in computing based on the implementation of spiking neural network architectures tightly co-locating processing and memory. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity present in existing silicon implementations, comparing approaches that aim at replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down), and assessing the benefits of the different circuit design styles used to achieve these goals. First, we present the analog, mixed-signal and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation and novel devices. Next, we highlight the key tradeoffs for each of the bottom-up and top-down approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify both necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic edge computing over conventional machine-learning accelerators, and outline the key elements for a framework toward neuromorphic intelligence.

</p>
</details>

<details><summary><b>Matrix factorisation and the interpretation of geodesic distance</b>
<a href="https://arxiv.org/abs/2106.01260">arxiv:2106.01260</a>
&#x1F4C8; 5 <br>
<p>Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy</p></summary>
<p>

**Abstract:** Given a graph or similarity matrix, we consider the problem of recovering a notion of true distance between the nodes, and so their true positions. Through new insights into the manifold geometry underlying a generic latent position model, we show that this can be accomplished in two steps: matrix factorisation, followed by nonlinear dimension reduction. This combination is effective because the point cloud obtained in the first step lives close to a manifold in which latent distance is encoded as geodesic distance. Hence, a nonlinear dimension reduction tool, approximating geodesic distance, can recover the latent positions, up to a simple transformation. We give a detailed account of the case where spectral embedding is used, followed by Isomap, and provide encouraging experimental evidence for other combinations of techniques.

</p>
</details>

<details><summary><b>Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy</b>
<a href="https://arxiv.org/abs/2106.01100">arxiv:2106.01100</a>
&#x1F4C8; 5 <br>
<p>Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli</p></summary>
<p>

**Abstract:** During lung cancer radiotherapy, the position of infrared reflective objects on the chest can be recorded to estimate the tumor location. However, radiotherapy systems usually have a latency inherent to robot control limitations that impedes the radiation delivery precision. Not taking this phenomenon into account may cause unwanted damage to healthy tissues and lead to side effects such as radiation pneumonitis. In this research, we use nine observation records of the three-dimensional position of three external markers on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the recorded trajectories range from 6mm to 40mm in the superior-inferior direction. We forecast the location of each marker simultaneously with a horizon value (the time interval in advance for which the prediction is made) between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with unbiased online recurrent optimization (UORO). We compare its performance with an RNN trained with real-time recurrent learning, least mean squares (LMS), and offline linear regression. Training and cross-validation are performed during the first minute of each sequence. On average, UORO achieves the lowest root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and 8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s, and UORO for horizon values greater than 0.6s.

</p>
</details>

<details><summary><b>End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net</b>
<a href="https://arxiv.org/abs/2106.00952">arxiv:2106.00952</a>
&#x1F4C8; 5 <br>
<p>Tuan-Anh Nguyen Dang, Dat-Thanh Nguyen</p></summary>
<p>

**Abstract:** Information extraction from document images has received a lot of attention recently, due to the need for digitizing a large volume of unstructured documents such as invoices, receipts, bank transfers, etc. In this paper, we propose a novel deep learning architecture for end-to-end information extraction on the 2D character-grid embedding of the document, namely the \textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and spatial relations between 2D elements, our model leverages a specialized multi-stage encoder-decoders design, in conjunction with efficient uses of the self-attention mechanism and the box convolution. Experimental results on different datasets show that our model outperforms the baseline U-Net architecture by a large margin while using 40\% fewer parameters. Moreover, it also significantly improved the baseline in erroneous OCR and limited training data scenario, thus becomes practical for real-world applications.

</p>
</details>

<details><summary><b>Knowing More About Questions Can Help: Improving Calibration in Question Answering</b>
<a href="https://arxiv.org/abs/2106.01494">arxiv:2106.01494</a>
&#x1F4C8; 4 <br>
<p>Shujian Zhang, Chengyue Gong, Eunsol Choi</p></summary>
<p>

**Abstract:** We study calibration in question answering, estimating whether model correctly predicts answer for each question. Unlike prior work which mainly rely on the model's confidence score, our calibrator incorporates information about the input example (e.g., question and the evidence context). Together with data augmentation via back translation, our simple approach achieves 5-10% gains in calibration accuracy on reading comprehension benchmarks. Furthermore, we present the first calibration study in the open retrieval setting, comparing the calibration accuracy of retrieval-based span prediction models and answer generation models. Here again, our approach shows consistent gains over calibrators relying on the model confidence. Our simple and efficient calibrator can be easily adapted to many tasks and model architectures, showing robust gains in all settings.

</p>
</details>

<details><summary><b>MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain</b>
<a href="https://arxiv.org/abs/2106.01491">arxiv:2106.01491</a>
&#x1F4C8; 4 <br>
<p>Christine Herlihy, Rachel Rudinger</p></summary>
<p>

**Abstract:** Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (Romanov and Shivade, 2018). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains.

</p>
</details>

<details><summary><b>Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal</b>
<a href="https://arxiv.org/abs/2106.01467">arxiv:2106.01467</a>
&#x1F4C8; 4 <br>
<p>Kamil Akhmetov</p></summary>
<p>

**Abstract:** Bringing empathy to a computerized system could significantly improve the quality of human-computer communications, as soon as machines would be able to understand customer intentions and better serve their needs. According to different studies (Literature Review), visual information is one of the most important channels of human interaction and contains significant behavioral signals, that may be captured from facial expressions. Therefore, it is consistent and natural that the research in the field of Facial Expression Recognition (FER) has acquired increased interest over the past decade due to having diverse application area including health-care, sociology, psychology, driver-safety, virtual reality, cognitive sciences, security, entertainment, marketing, etc. We propose a new architecture for the task of FER and examine the impact of domain discrimination loss regularization on the learning process. With regard to observations, including both classical training conditions and unsupervised domain adaptation scenarios, important aspects of the considered domain adaptation approach integration are traced. The results may serve as a foundation for further research in the field.

</p>
</details>

<details><summary><b>BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks</b>
<a href="https://arxiv.org/abs/2106.01452">arxiv:2106.01452</a>
&#x1F4C8; 4 <br>
<p>Yannik Keller, Jan Mackensen, Steffen Eger</p></summary>
<p>

**Abstract:** Adversarial attacks expose important blind spots of deep learning systems. While word- and sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT's masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning.

</p>
</details>

<details><summary><b>One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations</b>
<a href="https://arxiv.org/abs/2106.01423">arxiv:2106.01423</a>
&#x1F4C8; 4 <br>
<p>Henry Kvinge, Scott Howland, Nico Courts, Lauren A. Phillips, John Buckheit, Zachary New, Elliott Skomski, Jung H. Lee, Sandeep Tiwari, Jessica Hibler, Courtney D. Corley, Nathan O. Hodas</p></summary>
<p>

**Abstract:** The field of few-shot learning has made remarkable strides in developing powerful models that can operate in the small data regime. Nearly all of these methods assume every unlabeled instance encountered will belong to a handful of known classes for which one has examples. This can be problematic for real-world use cases where one routinely finds 'none-of-the-above' examples. In this paper we describe this challenge of identifying what we term 'out-of-support' (OOS) examples. We describe how this problem is subtly different from out-of-distribution detection and describe a new method of identifying OOS examples within the Prototypical Networks framework using a fixed point which we call the generic representation. We show that our method outperforms other existing approaches in the literature as well as other approaches that we propose in this paper. Finally, we investigate how the use of such a generic point affects the geometry of a model's feature space.

</p>
</details>

<details><summary><b>Dual Script E2E framework for Multilingual and Code-Switching ASR</b>
<a href="https://arxiv.org/abs/2106.01400">arxiv:2106.01400</a>
&#x1F4C8; 4 <br>
<p>Mari Ganesh Kumar, Jom Kuriakose, Anand Thyagachandran, Arun Kumar A, Ashish Seth, Lodagala Durga Prasad, Saish Jaiswal, Anusha Prakash, Hema Murthy</p></summary>
<p>

**Abstract:** India is home to multiple languages, and training automatic speech recognition (ASR) systems for languages is challenging. Over time, each language has adopted words from other languages, such as English, leading to code-mixing. Most Indian languages also have their own unique scripts, which poses a major limitation in training multilingual and code-switching ASR systems.
  Inspired by results in text-to-speech synthesis, in this work, we use an in-house rule-based phoneme-level common label set (CLS) representation to train multilingual and code-switching ASR for Indian languages. We propose two end-to-end (E2E) ASR systems. In the first system, the E2E model is trained on the CLS representation, and we use a novel data-driven back-end to recover the native language script. In the second system, we propose a modification to the E2E model, wherein the CLS representation and the native language characters are used simultaneously for training. We show our results on the multilingual and code-switching tasks of the Indic ASR Challenge 2021. Our best results achieve 6% and 5% improvement (approx) in word error rate over the baseline system for the multilingual and code-switching tasks, respectively, on the challenge development data.

</p>
</details>

<details><summary><b>JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data</b>
<a href="https://arxiv.org/abs/2106.00942">arxiv:2106.00942</a>
&#x1F4C8; 4 <br>
<p>Kourosh Hakhamaneshi, Pieter Abbeel, Vladimir Stojanovic, Aditya Grover</p></summary>
<p>

**Abstract:** The goal of Multi-task Bayesian Optimization (MBO) is to minimize the number of queries required to accurately optimize a target black-box function, given access to offline evaluations of other auxiliary functions. When offline datasets are large, the scalability of prior approaches comes at the expense of expressivity and inference quality. We propose JUMBO, an MBO algorithm that sidesteps these limitations by querying additional data based on a combination of acquisition signals derived from training two Gaussian Processes (GP): a cold-GP operating directly in the input domain and a warm-GP that operates in the feature space of a deep neural network pretrained using the offline data. Such a decomposition can dynamically control the reliability of information derived from the online and offline data and the use of pretrained neural networks permits scalability to large offline datasets. Theoretically, we derive regret bounds for JUMBO and show that it achieves no-regret under conditions analogous to GP-UCB (Srinivas et. al. 2010). Empirically, we demonstrate significant performance improvements over existing approaches on two real-world optimization problems: hyper-parameter optimization and automated circuit design.

</p>
</details>

<details><summary><b>Fully Steerable 3D Spherical Neurons</b>
<a href="https://arxiv.org/abs/2106.13863">arxiv:2106.13863</a>
&#x1F4C8; 3 <br>
<p>Pavlo Melnyk, Michael Felsberg, Mårten Wadenbäck</p></summary>
<p>

**Abstract:** Emerging from low-level vision theory, steerable filters found their counterpart in prior work on steerable convolutional neural networks equivariant to rigid transformations. In our work, we propose a steerable feed-forward learning-based approach that consists of spherical decision surfaces and operates on point clouds. Focusing on 3D geometry, we derive a 3D steerability constraint for hypersphere neurons, which are obtained by conformal embedding of Euclidean space and have recently been revisited in the context of learning representations of point sets. Exploiting the rotational equivariance, we show how our model parameters are fully steerable at inference time. We use a synthetic point set and real-world 3D skeleton data to show how the proposed spherical filter banks enable making equivariant and, after online optimization, invariant class predictions for known point sets in unknown orientations.

</p>
</details>

<details><summary><b>Long Term Object Detection and Tracking in Collaborative Learning Environments</b>
<a href="https://arxiv.org/abs/2106.07556">arxiv:2106.07556</a>
&#x1F4C8; 3 <br>
<p>Sravani Teeparthi</p></summary>
<p>

**Abstract:** Human activity recognition in videos is a challenging problem that has drawn a lot of interest, particularly when the goal requires the analysis of a large video database. AOLME project provides a collaborative learning environment for middle school students to explore mathematics, computer science, and engineering by processing digital images and videos. As part of this project, around 2200 hours of video data was collected for analysis. Because of the size of the dataset, it is hard to analyze all the videos of the dataset manually. Thus, there is a huge need for reliable computer-based methods that can detect activities of interest. My thesis is focused on the development of accurate methods for detecting and tracking objects in long videos. All the models are validated on videos from 7 different sessions, ranging from 45 minutes to 90 minutes. The keyboard detector achieved a very high average precision (AP) of 92% at 0.5 intersection over union (IoU). Furthermore, a combined system of the detector with a fast tracker KCF (159fps) was developed so that the algorithm runs significantly faster without sacrificing accuracy. For a video of 23 minutes having resolution 858X480 @ 30 fps, the detection alone runs at 4.7Xthe real-time, and the combined algorithm runs at 21Xthe real-time for an average IoU of 0.84 and 0.82, respectively. The hand detector achieved average precision (AP) of 72% at 0.5 IoU. The detection results were improved to 81% using optimal data augmentation parameters. The hand detector runs at 4.7Xthe real-time with AP of 81% at 0.5 IoU. The hand detection method was integrated with projections and clustering for accurate proposal generation. This approach reduced the number of false-positive hand detections by 80%. The overall hand detection system runs at 4Xthe real-time, capturing all the activity regions of the current collaborative group.

</p>
</details>

<details><summary><b>Posthoc Verification and the Fallibility of the Ground Truth</b>
<a href="https://arxiv.org/abs/2106.07353">arxiv:2106.07353</a>
&#x1F4C8; 3 <br>
<p>Yifan Ding, Nicholas Botzer, Tim Weninger</p></summary>
<p>

**Abstract:** Classifiers commonly make use of pre-annotated datasets, wherein a model is evaluated by pre-defined metrics on a held-out test set typically made of human-annotated labels. Metrics used in these evaluations are tied to the availability of well-defined ground truth labels, and these metrics typically do not allow for inexact matches. These noisy ground truth labels and strict evaluation metrics may compromise the validity and realism of evaluation results. In the present work, we discuss these concerns and conduct a systematic posthoc verification experiment on the entity linking (EL) task. Unlike traditional methodologies, which asks annotators to provide free-form annotations, we ask annotators to verify the correctness of annotations after the fact (i.e., posthoc). Compared to pre-annotation evaluation, state-of-the-art EL models performed extremely well according to the posthoc evaluation methodology. Posthoc validation also permits the validation of the ground truth dataset. Surprisingly, we find predictions from EL models had a similar or higher verification rate than the ground truth. We conclude with a discussion on these findings and recommendations for future evaluations.

</p>
</details>

<details><summary><b>PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack</b>
<a href="https://arxiv.org/abs/2106.01538">arxiv:2106.01538</a>
&#x1F4C8; 3 <br>
<p>Alexander Matyasko, Lap-Pui Chau</p></summary>
<p>

**Abstract:** State-of-the-art deep neural networks are sensitive to small input perturbations. Since the discovery of this intriguing vulnerability, many defence methods have been proposed that attempt to improve robustness to adversarial noise. Fast and accurate attacks are required to compare various defence methods. However, evaluating adversarial robustness has proven to be extremely challenging. Existing norm minimisation adversarial attacks require thousands of iterations (e.g. Carlini & Wagner attack), are limited to the specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results (e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast, general and accurate, ignores the norm minimisation penalty and solves a simpler perturbation-constrained problem. In this work, we introduce a fast, general and accurate adversarial attack that optimises the original non-convex constrained minimisation problem. We interpret optimising the Lagrangian of the adversarial attack optimisation problem as a two-player game: the first player minimises the Lagrangian wrt the adversarial noise; the second player maximises the Lagrangian wrt the regularisation penalty. Our attack algorithm simultaneously optimises primal and dual variables to find the minimal adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation, such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual proximal gradient descent attack. We show in the experiments that our attack outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and $l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against unregularised and adversarially trained models.

</p>
</details>

<details><summary><b>DeepCompress: Efficient Point Cloud Geometry Compression</b>
<a href="https://arxiv.org/abs/2106.01504">arxiv:2106.01504</a>
&#x1F4C8; 3 <br>
<p>Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan</p></summary>
<p>

**Abstract:** Point clouds are a basic data type that is increasingly of interest as 3D content becomes more ubiquitous. Applications using point clouds include virtual, augmented, and mixed reality and autonomous driving. We propose a more efficient deep learning-based encoder architecture for point clouds compression that incorporates principles from established 3D object detection and image compression architectures. Through an ablation study, we show that incorporating the learned activation function from Computational Efficient Neural Image Compression (CENIC) and designing more parameter-efficient convolutional blocks yields dramatic gains in efficiency and performance. Our proposed architecture incorporates Generalized Divisive Normalization activations and propose a spatially separable InceptionV4-inspired block. We then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized Full Bodies dataset to evaluate our model's performance. Our proposed modifications outperform the baseline approaches by a small margin in terms of Bjontegard delta rate and PSNR values, yet reduces necessary encoder convolution operations by 8 percent and reduces total encoder parameters by 20 percent. Our proposed architecture, when considered on its own, has a small penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit rate in Point to Plane Distance for the same peak signal-to-noise ratio.

</p>
</details>

<details><summary><b>Attention-based Contextual Language Model Adaptation for Speech Recognition</b>
<a href="https://arxiv.org/abs/2106.01451">arxiv:2106.01451</a>
&#x1F4C8; 3 <br>
<p>Richard Diehl Martinez, Scott Novotney, Ivan Bulyko, Ariya Rastrow, Andreas Stolcke, Ankur Gandhe</p></summary>
<p>

**Abstract:** Language modeling (LM) for automatic speech recognition (ASR) does not usually incorporate utterance level contextual information. For some domains like voice assistants, however, additional context, such as the time at which an utterance was spoken, provides a rich input signal. We introduce an attention mechanism for training neural speech recognition language models on both text and non-linguistic contextual data. When applied to a large de-identified dataset of utterances collected by a popular voice assistant platform, our method reduces perplexity by 7.0% relative over a standard LM that does not incorporate contextual information. When evaluated on utterances extracted from the long tail of the dataset, our method improves perplexity by 9.0% relative over a standard LM and by over 2.8% relative when compared to a state-of-the-art model for contextual LM.

</p>
</details>

<details><summary><b>Parallelizing Thompson Sampling</b>
<a href="https://arxiv.org/abs/2106.01420">arxiv:2106.01420</a>
&#x1F4C8; 3 <br>
<p>Amin Karbasi, Vahab Mirrokni, Mohammad Shadravan</p></summary>
<p>

**Abstract:** How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon $T$, our \textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only $O(\log T)$ batch queries. To achieve this exponential reduction, i.e., reducing the number of interactions from $T$ to $O(\log T)$, our batch policy dynamically determines the duration of each batch in order to balance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines such as static batch allocations.

</p>
</details>

<details><summary><b>Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2106.01404">arxiv:2106.01404</a>
&#x1F4C8; 3 <br>
<p>Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, Shixiang Shane Gu</p></summary>
<p>

**Abstract:** Learning to reach goal states and learning diverse skills through mutual information (MI) maximization have been proposed as principled frameworks for self-supervised reinforcement learning, allowing agents to acquire broadly applicable multitask policies with minimal reward engineering. Starting from a simple observation that the standard goal-conditioned RL (GCRL) is encapsulated by the optimization objective of variational empowerment, we discuss how GCRL and MI-based RL can be generalized into a single family of methods, which we name variational GCRL (VGCRL), interpreting variational MI maximization, or variational empowerment, as representation learning methods that acquire functionally-aware state representations for goal reaching. This novel perspective allows us to: (1) derive simple but unexplored variants of GCRL to study how adding small representation capacity can already expand its capabilities; (2) investigate how discriminator function capacity and smoothness determine the quality of discovered skills, or latent goals, through modifying latent dimensionality and applying spectral normalization; (3) adapt techniques such as hindsight experience replay (HER) from GCRL to MI-based RL; and lastly, (4) propose a novel evaluation metric, named latent goal reaching (LGR), for comparing empowerment algorithms with different choices of latent dimensionality and discriminator parameterization. Through principled mathematical derivations and careful experimental studies, our work lays a novel foundation from which to evaluate, analyze, and develop representation learning techniques in goal-based RL.

</p>
</details>

<details><summary><b>Improved Rates for Differentially Private Stochastic Convex Optimization with Heavy-Tailed Data</b>
<a href="https://arxiv.org/abs/2106.01336">arxiv:2106.01336</a>
&#x1F4C8; 3 <br>
<p>Gautam Kamath, Xingtu Liu, Huanyu Zhang</p></summary>
<p>

**Abstract:** We study stochastic convex optimization with heavy-tailed data under the constraint of differential privacy (DP). Most prior work on this problem is restricted to the case where the loss function is Lipschitz. Instead, as introduced by Wang, Xiao, Devadas, and Xu \cite{WangXDX20}, we study general convex loss functions with the assumption that the distribution of gradients has bounded $k$-th moments. We provide improved upper bounds on the excess population risk under concentrated DP for convex and strongly convex loss functions. Along the way, we derive new algorithms for private mean estimation of heavy-tailed distributions, under both pure and concentrated DP. Finally, we prove nearly-matching lower bounds for private stochastic convex optimization with strongly convex losses and mean estimation, showing new separations between pure and concentrated DP.

</p>
</details>

<details><summary><b>Spectral embedding for dynamic networks with stability guarantees</b>
<a href="https://arxiv.org/abs/2106.01282">arxiv:2106.01282</a>
&#x1F4C8; 3 <br>
<p>Ian Gallagher, Andrew Jones, Patrick Rubin-Delanchy</p></summary>
<p>

**Abstract:** We consider the problem of embedding a dynamic network, to obtain time-evolving vector representations of each node, which can then be used to describe the changes in behaviour of a single node, one or more communities, or the entire graph. Given this open-ended remit, we wish to guarantee stability in the spatio-temporal positioning of the nodes: assigning the same position, up to noise, to nodes behaving similarly at a given time (cross-sectional stability) and a constant position, up to noise, to a single node behaving similarly across different times (longitudinal stability). These properties are defined formally within a generic dynamic latent position model. By showing how this model can be recast as a multilayer random dot product graph, we demonstrate that unfolded adjacency spectral embedding satisfies both stability conditions, allowing, for example, spatio-temporal clustering under the dynamic stochastic block model. We also show how alternative methods, such as omnibus, independent or time-averaged spectral embedding, lack one or the other form of stability.

</p>
</details>

<details><summary><b>Deep learning-based multi-output quantile forecasting of PV generation</b>
<a href="https://arxiv.org/abs/2106.01271">arxiv:2106.01271</a>
&#x1F4C8; 3 <br>
<p>Jonathan Dumas, Colin Cointe, Xavier Fettweis, Bertrand Cornélusse</p></summary>
<p>

**Abstract:** This paper develops probabilistic PV forecasters by taking advantage of recent breakthroughs in deep learning. It tailored forecasting tool, named encoder-decoder, is implemented to compute intraday multi-output PV quantiles forecasts to efficiently capture the time correlation. The models are trained using quantile regression, a non-parametric approach that assumes no prior knowledge of the probabilistic forecasting distribution. The case study is composed of PV production monitored on-site at the University of Liège (ULiège), Belgium. The weather forecasts from the regional climate model provided by the Laboratory of Climatology are used as inputs of the deep learning models. The forecast quality is quantitatively assessed by the continuous ranked probability and interval scores. The results indicate this architecture improves the forecast quality and is computationally efficient to be incorporated in an intraday decision-making tool for robust optimization.

</p>
</details>

<details><summary><b>Learning neural network potentials from experimental data via Differentiable Trajectory Reweighting</b>
<a href="https://arxiv.org/abs/2106.01138">arxiv:2106.01138</a>
&#x1F4C8; 3 <br>
<p>Stephan Thaler, Julija Zavadlav</p></summary>
<p>

**Abstract:** In molecular dynamics (MD), neural network (NN) potentials trained bottom-up on quantum mechanical data have seen tremendous success recently. Top-down approaches that learn NN potentials directly from experimental data have received less attention, typically facing numerical and computational challenges when backpropagating through MD simulations. We present the Differentiable Trajectory Reweighting (DiffTRe) method, which bypasses differentiation through the MD simulation for time-independent observables. Leveraging thermodynamic perturbation theory, we avoid exploding gradients and achieve around 2 orders of magnitude speed-up in gradient computation for top-down learning. We show effectiveness of DiffTRe in learning NN potentials for an atomistic model of diamond and a coarse-grained model of water based on diverse experimental observables including thermodynamic, structural and mechanical properties. Importantly, DiffTRe also generalizes bottom-up structural coarse-graining methods such as iterative Boltzmann inversion to arbitrary potentials. The presented method constitutes an important milestone towards enriching NN potentials with experimental data, particularly when accurate bottom-up data is unavailable.

</p>
</details>

<details><summary><b>Connections and Equivalences between the Nyström Method and Sparse Variational Gaussian Processes</b>
<a href="https://arxiv.org/abs/2106.01121">arxiv:2106.01121</a>
&#x1F4C8; 3 <br>
<p>Veit Wild, Motonobu Kanagawa, Dino Sejdinovic</p></summary>
<p>

**Abstract:** We investigate the connections between sparse approximation methods for making kernel methods and Gaussian processes (GPs) scalable to massive data, focusing on the Nyström method and the Sparse Variational Gaussian Processes (SVGP). While sparse approximation methods for GPs and kernel methods share some algebraic similarities, the literature lacks a deep understanding of how and why they are related. This is a possible obstacle for the communications between the GP and kernel communities, making it difficult to transfer results from one side to the other. Our motivation is to remove this possible obstacle, by clarifying the connections between the sparse approximations for GPs and kernel methods. In this work, we study the two popular approaches, the Nyström and SVGP approximations, in the context of a regression problem, and establish various connections and equivalences between them. In particular, we provide an RKHS interpretation of the SVGP approximation, and show that the Evidence Lower Bound of the SVGP contains the objective function of the Nyström approximation, revealing the origin of the algebraic equivalence between the two approaches. We also study recently established convergence results for the SVGP and how they are related to the approximation quality of the Nyström method.

</p>
</details>

<details><summary><b>Learning a Single Neuron with Bias Using Gradient Descent</b>
<a href="https://arxiv.org/abs/2106.01101">arxiv:2106.01101</a>
&#x1F4C8; 3 <br>
<p>Gal Vardi, Gilad Yehudai, Ohad Shamir</p></summary>
<p>

**Abstract:** We theoretically study the fundamental problem of learning a single neuron with a bias term ($\mathbf{x} \mapsto σ(<\mathbf{w},\mathbf{x}> + b)$) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a significantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons.

</p>
</details>

<details><summary><b>Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey</b>
<a href="https://arxiv.org/abs/2106.00997">arxiv:2106.00997</a>
&#x1F4C8; 3 <br>
<p>Changhee Han, Takayuki Okamoto, Koichi Takeuchi, Dimitris Katsios, Andrey Grushnikov, Masaaki Kobayashi, Antoine Choppin, Yutaka Kurashina, Yuki Shimahara</p></summary>
<p>

**Abstract:** Convolutional Neural Networks (CNNs) intrinsically requires large-scale data whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to over-fitting. Therefore, based on our development experience and related work, this paper thoroughly introduces tricks to improve generalization in the CXR diagnosis: how to (i) leverage additional data, (ii) augment/distillate data, (iii) regularize training, and (iv) conduct efficient segmentation. As a development example based on such optimization techniques, we also feature LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131, respectively, while maintaining specificity.

</p>
</details>

<details><summary><b>Few-Shot Partial-Label Learning</b>
<a href="https://arxiv.org/abs/2106.00984">arxiv:2106.00984</a>
&#x1F4C8; 3 <br>
<p>Yunfeng Zhao, Guoxian Yu, Lei Liu, Zhongmin Yan, Lizhen Cui, Carlotta Domeniconi</p></summary>
<p>

**Abstract:** Partial-label learning (PLL) generally focuses on inducing a noise-tolerant multi-class classifier by training on overly-annotated samples, each of which is annotated with a set of labels, but only one is the valid label. A basic promise of existing PLL solutions is that there are sufficient partial-label (PL) samples for training. However, it is more common than not to have just few PL samples at hand when dealing with new tasks. Furthermore, existing few-shot learning algorithms assume precise labels of the support set; as such, irrelevant labels may seriously mislead the meta-learner and thus lead to a compromised performance. How to enable PLL under a few-shot learning setting is an important problem, but not yet well studied. In this paper, we introduce an approach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance metric learning by an embedding network and rectifying prototypes on the tasks previously encountered. Next, it calculates the prototype of each class of a new task in the embedding network. An unseen example can then be classified via its distance to each prototype. Experimental results on widely-used few-shot datasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a superior performance than the state-of-the-art methods across different settings, and it needs fewer samples for quickly adapting to new tasks.

</p>
</details>

<details><summary><b>COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences</b>
<a href="https://arxiv.org/abs/2106.00969">arxiv:2106.00969</a>
&#x1F4C8; 3 <br>
<p>Shikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-Lin Wu, Xuezhe Ma, Nanyun Peng</p></summary>
<p>

**Abstract:** Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and ~51% pairwise accuracy, well below human performance (~95% for both metrics). The dataset is available at https://github.com/PlusLabNLP/Com2Sense.

</p>
</details>

<details><summary><b>Artificial Perceptual Learning: Image Categorization with Weak Supervision</b>
<a href="https://arxiv.org/abs/2106.07559">arxiv:2106.07559</a>
&#x1F4C8; 2 <br>
<p>Chengliang Tang, María Uriarte, Helen Jin, Douglas C. Morton, Tian Zheng</p></summary>
<p>

**Abstract:** Machine learning has achieved much success on supervised learning tasks with large sets of well-annotated training samples. However, in many practical situations, such strong and high-quality supervision provided by training data is unavailable due to the expensive and labor-intensive labeling process. Automatically identifying and recognizing object categories in a large volume of unlabeled images with weak supervision remains an important, yet unsolved challenge in computer vision. In this paper, we propose a novel machine learning framework, artificial perceptual learning (APL), to tackle the problem of weakly supervised image categorization. The proposed APL framework is constructed using state-of-the-art machine learning algorithms as building blocks to mimic the cognitive development process known as infant categorization. We develop and illustrate the proposed framework by implementing a wide-field fine-grain ecological survey of tree species over an 8,000-hectare area of the El Yunque rainforest in Puerto Rico. It is based on unlabeled high-resolution aerial images of the tree canopy. Misplaced ground-based labels were available for less than 1% of these images, which serve as the only weak supervision for this learning framework. We validate the proposed framework using a small set of images with high quality human annotations and show that the proposed framework attains human-level cognitive economy.

</p>
</details>

<details><summary><b>General Rough Modeling of Cluster Analysis</b>
<a href="https://arxiv.org/abs/2106.04683">arxiv:2106.04683</a>
&#x1F4C8; 2 <br>
<p>A. Mani</p></summary>
<p>

**Abstract:** In this research, a general theoretical framework for clustering is proposed over specific partial algebraic systems by the present author. Her theory helps in isolating minimal assumptions necessary for different concepts of clustering information in any form to be realized in a situation (and therefore in a semantics). \emph{It is well-known that of the limited number of proofs in the theory of hard and soft clustering that are known to exist, most involve statistical assumptions}. Many methods seem to work because they seem to work in specific empirical practice. A new general rough method of analyzing clusterings is invented, and this opens the subject to clearer conceptions and contamination-free theoretical proofs. Numeric ideas of validation are also proposed to be replaced by those based on general rough approximation. The essence of the approach is explained in brief and supported by an example.

</p>
</details>

<details><summary><b>Random Forest classifier for EEG-based seizure prediction</b>
<a href="https://arxiv.org/abs/2106.04510">arxiv:2106.04510</a>
&#x1F4C8; 2 <br>
<p>Remy Ben Messaoud, Mario Chavez</p></summary>
<p>

**Abstract:** Epileptic seizure prediction has gained considerable interest in the computational Epilepsy research community. This paper presents a Machine Learning based method for epileptic seizure prediction which outperforms state-of-the art methods. We compute a probability for a given epoch, of being pre-ictal against interictal using the Random Forest classifier and introduce new concepts to enhance the robustness of the algorithm to false alarms. We assessed our method on 20 patients of the benchmark scalp EEG CHB-MIT dataset for a seizure prediction horizon (SPH) of 5 minutes and a seizure occurrence period (SOP) of 30 minutes. Our approach achieves a sensitivity of 82.07 % and a low false positive rate (FPR) of 0.0799 /h. We also tested our approach on intracranial EEG recordings.

</p>
</details>

<details><summary><b>Federated Neural Collaborative Filtering</b>
<a href="https://arxiv.org/abs/2106.04405">arxiv:2106.04405</a>
&#x1F4C8; 2 <br>
<p>Vasileios Perifanis, Pavlos S. Efraimidis</p></summary>
<p>

**Abstract:** In this work, we present a federated version of the state-of-the-art Neural Collaborative Filtering (NCF) approach for item recommendations. The system, named FedNCF, allows learning without requiring users to expose or transmit their raw data. Experimental validation shows that FedNCF achieves comparable recommendation quality to the original NCF system. Although federated learning (FL) enables learning without raw data transmission, recent attacks showed that FL alone does not eliminate privacy concerns. To overcome this challenge, we integrate a privacy-preserving enhancement with a secure aggregation scheme that satisfies the security requirements against an honest-but-curious (HBC) entity, without affecting the quality of the original model. Finally, we discuss the peculiarities observed in the application of FL in a collaborative filtering (CF) task as well as we evaluate the privacy-preserving mechanism in terms of computational cost.

</p>
</details>

<details><summary><b>A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2106.01577">arxiv:2106.01577</a>
&#x1F4C8; 2 <br>
<p>Honghao Wei, Xin Liu, Lei Ying</p></summary>
<p>

**Abstract:** This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three "Q" values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }δH^4 S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $δ$ is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs and is computationally efficient.

</p>
</details>

<details><summary><b>Normalizing Flows for Knockoff-free Controlled Feature Selection</b>
<a href="https://arxiv.org/abs/2106.01528">arxiv:2106.01528</a>
&#x1F4C8; 2 <br>
<p>Derek Hansen, Brian Manzo, Jeffrey Regier</p></summary>
<p>

**Abstract:** Controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (FDR) to a predefined level. Recently, multiple deep-learning-based methods have been proposed to perform controlled feature selection through the Model-X knockoff framework. We demonstrate, however, that these methods often fail to control the FDR for two reasons. First, these methods often learn inaccurate models of features. Second, the "swap" property, which is required for knockoffs to be valid, is often not well enforced. We propose a new procedure called FlowSelect that remedies both of these problems. To more accurately model the features, FlowSelect uses normalizing flows, the state-of-the-art method for density estimation. To circumvent the need to enforce the swap property, FlowSelect uses a novel MCMC-based procedure to calculate p-values for each feature directly. Asymptotically, FlowSelect computes valid p-values. Empirically, FlowSelect consistently controls the FDR on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. FlowSelect also demonstrates greater power on these benchmarks. Additionally, FlowSelect correctly infers the genetic variants associated with specific soybean traits from GWAS data.

</p>
</details>

<details><summary><b>Weakly Supervised Learning Creates a Fusion of Modeling Cultures</b>
<a href="https://arxiv.org/abs/2106.01485">arxiv:2106.01485</a>
&#x1F4C8; 2 <br>
<p>Chengliang Tang, Gan Yuan, Tian Zheng</p></summary>
<p>

**Abstract:** The past two decades have witnessed the great success of the algorithmic modeling framework advocated by Breiman et al. (2001). Nevertheless, the excellent prediction performance of these black-box models rely heavily on the availability of strong supervision, i.e. a large set of accurate and exact ground-truth labels. In practice, strong supervision can be unavailable or expensive, which calls for modeling techniques under weak supervision. In this comment, we summarize the key concepts in weakly supervised learning and discuss some recent developments in the field. Using algorithmic modeling alone under a weak supervision might lead to unstable and misleading results. A promising direction would be integrating the data modeling culture into such a framework.

</p>
</details>

<details><summary><b>Testing Directed Acyclic Graph via Structural, Supervised and Generative Adversarial Learning</b>
<a href="https://arxiv.org/abs/2106.01474">arxiv:2106.01474</a>
&#x1F4C8; 2 <br>
<p>Chengchun Shi, Yunzhe Zhou, Lexin Li</p></summary>
<p>

**Abstract:** In this article, we propose a new hypothesis testing method for directed acyclic graph (DAG). While there is a rich class of DAG estimation methods, there is a relative paucity of DAG inference solutions. Moreover, the existing methods often impose some specific model structures such as linear models or additive models, and assume independent data observations. Our proposed test instead allows the associations among the random variables to be nonlinear and the data to be time-dependent. We build the test based on some highly flexible neural networks learners. We establish the asymptotic guarantees of the test, while allowing either the number of subjects or the number of time points for each subject to diverge to infinity. We demonstrate the efficacy of the test through simulations and a brain connectivity network analysis.

</p>
</details>

<details><summary><b>Inferring Black Hole Properties from Astronomical Multivariate Time Series with Bayesian Attentive Neural Processes</b>
<a href="https://arxiv.org/abs/2106.01450">arxiv:2106.01450</a>
&#x1F4C8; 2 <br>
<p>Ji Won Park, Ashley Villar, Yin Li, Yan-Fei Jiang, Shirley Ho, Joshua Yao-Yu Lin, Philip J. Marshall, Aaron Roodman</p></summary>
<p>

**Abstract:** Among the most extreme objects in the Universe, active galactic nuclei (AGN) are luminous centers of galaxies where a black hole feeds on surrounding matter. The variability patterns of the light emitted by an AGN contain information about the physical properties of the underlying black hole. Upcoming telescopes will observe over 100 million AGN in multiple broadband wavelengths, yielding a large sample of multivariate time series with long gaps and irregular sampling. We present a method that reconstructs the AGN time series and simultaneously infers the posterior probability density distribution (PDF) over the physical quantities of the black hole, including its mass and luminosity. We apply this method to a simulated dataset of 11,000 AGN and report precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole mass. This work is the first to address probabilistic time series reconstruction and parameter inference for AGN in an end-to-end fashion.

</p>
</details>

<details><summary><b>Rectangular Flows for Manifold Learning</b>
<a href="https://arxiv.org/abs/2106.01413">arxiv:2106.01413</a>
&#x1F4C8; 2 <br>
<p>Anthony L. Caterini, Gabriel Loaiza-Ganem, Geoff Pleiss, John P. Cunningham</p></summary>
<p>

**Abstract:** Normalizing flows are invertible neural networks with tractable change-of-volume terms, which allow optimization of their parameters to be efficiently performed via maximum likelihood. However, data of interest are typically assumed to live in some (often unknown) low-dimensional manifold embedded in a high-dimensional ambient space. The result is a modelling mismatch since -- by construction -- the invertibility requirement implies high-dimensional support of the learned distribution. Injective flows, mappings from low- to high-dimensional spaces, aim to fix this discrepancy by learning distributions on manifolds, but the resulting volume-change term becomes more challenging to evaluate. Current approaches either avoid computing this term entirely using various heuristics, or assume the manifold is known beforehand and therefore are not widely applicable. Instead, we propose two methods to tractably calculate the gradient of this term with respect to the parameters of the model, relying on careful use of automatic differentiation and techniques from numerical linear algebra. Both approaches perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold. We study the trade-offs between our proposed methods, empirically verify that we outperform approaches ignoring the volume-change term by more accurately learning manifolds and the corresponding distributions on them, and show promising results on out-of-distribution detection. Our code is available at https://github.com/layer6ai-labs/rectangular-flows.

</p>
</details>

<details><summary><b>Automatic Assessment of the Design Quality of Python Programs with Personalized Feedback</b>
<a href="https://arxiv.org/abs/2106.01399">arxiv:2106.01399</a>
&#x1F4C8; 2 <br>
<p>J. Walker Orr, Nathaniel Russell</p></summary>
<p>

**Abstract:** The assessment of program functionality can generally be accomplished with straight-forward unit tests. However, assessing the design quality of a program is a much more difficult and nuanced problem. Design quality is an important consideration since it affects the readability and maintainability of programs. Assessing design quality and giving personalized feedback is very time consuming task for instructors and teaching assistants. This limits the scale of giving personalized feedback to small class settings. Further, design quality is nuanced and is difficult to concisely express as a set of rules. For these reasons, we propose a neural network model to both automatically assess the design of a program and provide personalized feedback to guide students on how to make corrections. The model's effectiveness is evaluated on a corpus of student programs written in Python. The model has an accuracy rate from 83.67% to 94.27%, depending on the dataset, when predicting design scores as compared to historical instructor assessment. Finally, we present a study where students tried to improve the design of their programs based on the personalized feedback produced by the model. Students who participated in the study improved their program design scores by 19.58%.

</p>
</details>

<details><summary><b>Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize</b>
<a href="https://arxiv.org/abs/2106.01257">arxiv:2106.01257</a>
&#x1F4C8; 2 <br>
<p>Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, Hoi-To Wai</p></summary>
<p>

**Abstract:** This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system $\bar{A}θ= \bar{b}$ for which $\bar{A}$ and $\bar{b}$ can only be accessed through random estimates $\{({\bf A}_n, {\bf b}_n): n \in \mathbb{N}^*\}$. Our analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. We derive high probability bounds on the performance of LSA under weaker conditions on the sequence $\{({\bf A}_n, {\bf b}_n): n \in \mathbb{N}^*\}$ than previous works. However, in contrast, we establish polynomial concentration bounds with order depending on the stepsize. We show that our conclusions cannot be improved without additional assumptions on the sequence of random matrices $\{{\bf A}_n: n \in \mathbb{N}^*\}$, and in particular that no Gaussian or exponential high probability bounds can hold. Finally, we pay a particular attention to establishing bounds with sharp order with respect to the number of iterations and the stepsize and whose leading terms contain the covariance matrices appearing in the central limit theorems.

</p>
</details>

<details><summary><b>A Privacy-Preserving and Trustable Multi-agent Learning Framework</b>
<a href="https://arxiv.org/abs/2106.01242">arxiv:2106.01242</a>
&#x1F4C8; 2 <br>
<p>Anudit Nagar, Cuong Tran, Ferdinando Fioretto</p></summary>
<p>

**Abstract:** Distributed multi-agent learning enables agents to cooperatively train a model without requiring to share their datasets. While this setting ensures some level of privacy, it has been shown that, even when data is not directly shared, the training process is vulnerable to privacy attacks including data reconstruction and model inversion attacks. Additionally, malicious agents that train on inverted labels or random data, may arbitrarily weaken the accuracy of the global model. This paper addresses these challenges and presents Privacy-preserving and trustable Distributed Learning (PT-DL), a fully decentralized framework that relies on Differential Privacy to guarantee strong privacy protections of the agents' data, and Ethereum smart contracts to ensure trustability. The paper shows that PT-DL is resilient up to a 50% collusion attack, with high probability, in a malicious trust model and the experimental evaluation illustrates the benefits of the proposed model as a privacy-preserving and trustable distributed multi-agent learning system on several classification tasks.

</p>
</details>

<details><summary><b>Accurate and Robust Deep Learning Framework for Solving Wave-Based Inverse Problems in the Super-Resolution Regime</b>
<a href="https://arxiv.org/abs/2106.01143">arxiv:2106.01143</a>
&#x1F4C8; 2 <br>
<p>Matthew Li, Laurent Demanet, Leonardo Zepeda-Núñez</p></summary>
<p>

**Abstract:** We propose an end-to-end deep learning framework that comprehensively solves the inverse wave scattering problem across all length scales. Our framework consists of the newly introduced wide-band butterfly network coupled with a simple training procedure that dynamically injects noise during training. While our trained network provides competitive results in classical imaging regimes, most notably it also succeeds in the super-resolution regime where other comparable methods fail. This encompasses both (i) reconstruction of scatterers with sub-wavelength geometric features, and (ii) accurate imaging when two or more scatterers are separated by less than the classical diffraction limit. We demonstrate these properties are retained even in the presence of strong noise and extend to scatterers not previously seen in the training set. In addition, our network is straightforward to train requiring no restarts and has an online runtime that is an order of magnitude faster than optimization-based algorithms. We perform experiments with a variety of wave scattering mediums and we demonstrate that our proposed framework outperforms both classical inversion and competing network architectures that specialize in oscillatory wave scattering data.

</p>
</details>

<details><summary><b>Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs</b>
<a href="https://arxiv.org/abs/2106.01128">arxiv:2106.01128</a>
&#x1F4C8; 2 <br>
<p>Meyer Scetbon, Gabriel Peyré, Marco Cuturi</p></summary>
<p>

**Abstract:** The ability to compare and align related datasets living in heterogeneous spaces plays an increasingly important role in machine learning. The Gromov-Wasserstein (GW) formalism can help tackle this problem. Its main goal is to seek an assignment (more generally a coupling matrix) that can register points across otherwise incomparable datasets. As a non-convex and quadratic generalization of optimal transport (OT), GW is NP-hard. Yet, heuristics are known to work reasonably well in practice, the state of the art approach being to solve a sequence of nested regularized OT problems. While popular, that heuristic remains too costly to scale, with cubic complexity in the number of samples $n$. We show in this paper how a recent variant of the Sinkhorn algorithm can substantially speed up the resolution of GW. That variant restricts the set of admissible couplings to those admitting a low rank factorization as the product of two sub-couplings. By updating alternatively each sub-coupling, our algorithm computes a stationary point of the problem in quadratic time with respect to the number of samples. When cost matrices have themselves low rank, our algorithm has time complexity $\mathcal{O}(n)$. We demonstrate the efficiency of our method on simulated and real data.

</p>
</details>

<details><summary><b>Improvement over Pinball Loss Support Vector Machine</b>
<a href="https://arxiv.org/abs/2106.01109">arxiv:2106.01109</a>
&#x1F4C8; 2 <br>
<p>Pritam Anand, Reshma Rastogi, Suresh Chandra</p></summary>
<p>

**Abstract:** Recently, there have been several papers that discuss the extension of the Pinball loss Support Vector Machine (Pin-SVM) model, originally proposed by Huang et al.,[1][2]. Pin-SVM classifier deals with the pinball loss function, which has been defined in terms of the parameter $τ$. The parameter $τ$ can take values in $[ -1,1]$. The existing Pin-SVM model requires to solve the same optimization problem for all values of $τ$ in $[ -1,1]$. In this paper, we improve the existing Pin-SVM model for the binary classification task. At first, we note that there is major difficulty in Pin-SVM model (Huang et al. [1]) for $ -1 \leq τ< 0$. Specifically, we show that the Pin-SVM model requires the solution of different optimization problem for $ -1 \leq τ< 0$. We further propose a unified model termed as Unified Pin-SVM which results in a QPP valid for all $-1\leq τ\leq 1$ and hence more convenient to use. The proposed Unified Pin-SVM model can obtain a significant improvement in accuracy over the existing Pin-SVM model which has also been empirically justified by extensive numerical experiments with real-world datasets.

</p>
</details>

<details><summary><b>Testing Group Fairness via Optimal Transport Projections</b>
<a href="https://arxiv.org/abs/2106.01070">arxiv:2106.01070</a>
&#x1F4C8; 2 <br>
<p>Nian Si, Karthyek Murthy, Jose Blanchet, Viet Anh Nguyen</p></summary>
<p>

**Abstract:** We present a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. The proposed test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure onto the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for testing composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.

</p>
</details>

<details><summary><b>Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties of Non-Gaussian Distributions</b>
<a href="https://arxiv.org/abs/2106.01043">arxiv:2106.01043</a>
&#x1F4C8; 2 <br>
<p>Rohan Giriraj, Sinnu Susan Thomas</p></summary>
<p>

**Abstract:** In recent years, causal modelling has been used widely to improve generalization and to provide interpretability in machine learning models. To determine cause-effect relationships in the absence of a randomized trial, we can model causal systems with counterfactuals and interventions given enough domain knowledge. However, there are several cases where domain knowledge is almost absent and the only recourse is using a statistical method to estimate causal relationships. While there have been several works done in estimating causal relationships in unstructured data, we are yet to find a well-defined framework for estimating causal relationships in Knowledge Graphs (KG). It is commonly used to provide a semantic framework for data with complex inter-domain relationships. In this work, we define a hybrid approach that allows us to discover cause-effect relationships in KG. The proposed approach is based around the finding of the instantaneous causal structure of a non-experimental matrix using a non-Gaussian model, i.e; finding the causal ordering of the variables in a non-Gaussian setting. The non-experimental matrix is a low-dimensional tensor projection obtained by decomposing the adjacency tensor of a KG. We use two different pre-existing algorithms, one for the causal discovery and the other for decomposing the KG and combining them to get the causal structure in a KG.

</p>
</details>

<details><summary><b>Deep Reinforcement Learning-based UAV Navigation and Control: A Soft Actor-Critic with Hindsight Experience Replay Approach</b>
<a href="https://arxiv.org/abs/2106.01016">arxiv:2106.01016</a>
&#x1F4C8; 2 <br>
<p>Myoung Hoon Lee, Jun Moon</p></summary>
<p>

**Abstract:** In this paper, we propose SACHER (soft actor-critic (SAC) with hindsight experience replay (HER)), which constitutes a class of deep reinforcement learning (DRL) algorithms. SAC is known as an off-policy model-free DRL algorithm based on the maximum entropy framework, which outperforms earlier DRL algorithms in terms of exploration, robustness and learning performance. However, in SAC, maximizing the entropy-augmented objective may degrade the optimality of learning outcomes. HER is known as a sample-efficient replay method that enhances the performance of off-policy DRL algorithms by allowing the agent to learn from both failures and successes. We apply HER to SAC and propose SACHER to improve the learning performance of SAC. More precisely, SACHER achieves the desired optimal outcomes faster and more accurately than SAC, since HER improves the sample efficiency of SAC. We apply SACHER to the navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER generates the optimal navigation path of the UAV under various obstacles in operation. Specifically, we show the effectiveness of SACHER in terms of the tracking error and cumulative reward in UAV operation by comparing them with those of state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV navigation and control problems can be applied to arbitrary models of UAVs.

</p>
</details>

<details><summary><b>FedHealth 2: Weighted Federated Transfer Learning via Batch Normalization for Personalized Healthcare</b>
<a href="https://arxiv.org/abs/2106.01009">arxiv:2106.01009</a>
&#x1F4C8; 2 <br>
<p>Yiqiang Chen, Wang Lu, Jindong Wang, Xin Qin</p></summary>
<p>

**Abstract:** The success of machine learning applications often needs a large quantity of data. Recently, federated learning (FL) is attracting increasing attention due to the demand for data privacy and security, especially in the medical field. However, the performance of existing FL approaches often deteriorates when there exist domain shifts among clients, and few previous works focus on personalization in healthcare. In this article, we propose FedHealth 2, an extension of FedHealth \cite{chen2020fedhealth} to tackle domain shifts and get personalized models for local clients. FedHealth 2 obtains the client similarities via a pretrained model, and then it averages all weighted models with preserving local batch normalization. Wearable activity recognition and COVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can achieve better accuracy (10%+ improvement for activity recognition) and personalized healthcare without compromising privacy and security.

</p>
</details>

<details><summary><b>On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio Correction</b>
<a href="https://arxiv.org/abs/2106.00993">arxiv:2106.00993</a>
&#x1F4C8; 2 <br>
<p>Jiawei Huang, Nan Jiang</p></summary>
<p>

**Abstract:** In this paper, we study the convergence properties of off-policy policy improvement algorithms with state-action density ratio correction under function approximation setting, where the objective function is formulated as a max-max-min optimization problem. We characterize the bias of the learning objective and present two strategies with finite-time convergence guarantees. In our first strategy, we present algorithm P-SREDA with convergence rate $O(ε^{-3})$, whose dependency on $ε$ is optimal. In our second strategy, we propose a new off-policy actor-critic style algorithm named O-SPIM. We prove that O-SPIM converges to a stationary point with total complexity $O(ε^{-4})$, which matches the convergence rate of some recent actor-critic algorithms in the on-policy setting.

</p>
</details>

<details><summary><b>OctoPath: An OcTree Based Self-Supervised Learning Approach to Local Trajectory Planning for Mobile Robots</b>
<a href="https://arxiv.org/abs/2106.00988">arxiv:2106.00988</a>
&#x1F4C8; 2 <br>
<p>Bogdan Trasnea, Cosmin Ginerica, Mihai Zaha, Gigel Macesanu, Claudiu Pozna, Sorin Grigorescu</p></summary>
<p>

**Abstract:** Autonomous mobile robots are usually faced with challenging situations when driving in complex environments. Namely, they have to recognize the static and dynamic obstacles, plan the driving path and execute their motion. For addressing the issue of perception and path planning, in this paper, we introduce OctoPath , which is an encoder-decoder deep neural network, trained in a self-supervised manner to predict the local optimal trajectory for the ego-vehicle. Using the discretization provided by a 3D octree environment model, our approach reformulates trajectory prediction as a classification problem with a configurable resolution. During training, OctoPath minimizes the error between the predicted and the manually driven trajectories in a given training dataset. This allows us to avoid the pitfall of regression-based trajectory estimation, in which there is an infinite state space for the output trajectory points. Environment sensing is performed using a 40-channel mechanical LiDAR sensor, fused with an inertial measurement unit and wheels odometry for state estimation. The experiments are performed both in simulation and real-life, using our own developed GridSim simulator and RovisLab's Autonomous Mobile Test Unit platform. We evaluate the predictions of OctoPath in different driving scenarios, both indoor and outdoor, while benchmarking our system against a baseline hybrid A-Star algorithm and a regression-based supervised learning method, as well as against a CNN learning-based optimal path planning method.

</p>
</details>

<details><summary><b>Teaching Machine Learning in K-12 Computing Education: Potential and Pitfalls</b>
<a href="https://arxiv.org/abs/2106.11034">arxiv:2106.11034</a>
&#x1F4C8; 1 <br>
<p>Matti Tedre, Tapani Toivonen, Juho Kaihila, Henriikka Vartiainen, Teemu Valtonen, Ilkka Jormanainen, Arnold Pears</p></summary>
<p>

**Abstract:** Over the past decades, numerous practical applications of machine learning techniques have shown the potential of data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based "traditional" programming is a central aspect and building block in developing next generation computational thinking.

</p>
</details>

<details><summary><b>The Struggle with Academic Plagiarism: Approaches based on Semantic Similarity</b>
<a href="https://arxiv.org/abs/2106.04404">arxiv:2106.04404</a>
&#x1F4C8; 1 <br>
<p>Tedo Vrbanec, Ana Mestrovic</p></summary>
<p>

**Abstract:** Academic plagiarism is a serious problem nowadays. Due to the existence of inexhaustible sources of digital information, today it is easier to plagiarize more than ever before. The good thing is that plagiarism detection techniques have improved and are powerful enough to detect attempts of plagiarism in education. We are now witnessing efficient plagiarism detection software in action, such as Turnitin, iThenticate or SafeAssign. In the introduction we explore software that is used within the Croatian academic community for plagiarism detection in universities and/or in scientific journals. The question is: is this enough? Current software has proven to be successful, however the problem of identifying paraphrasing or obfuscation plagiarism remains unresolved. In this paper we present a report of how semantic similarity measures can be used in the plagiarism detection task.

</p>
</details>

<details><summary><b>DNA-GCN: Graph convolutional networks for predicting DNA-protein binding</b>
<a href="https://arxiv.org/abs/2106.01836">arxiv:2106.01836</a>
&#x1F4C8; 1 <br>
<p>Yuhang Guo, Xiao Luo, Liang Chen, Minghua Deng</p></summary>
<p>

**Abstract:** Predicting DNA-protein binding is an important and classic problem in bioinformatics. Convolutional neural networks have outperformed conventional methods in modeling the sequence specificity of DNA-protein binding. However, none of the studies has utilized graph convolutional networks for motif inference. In this work, we propose to use graph convolutional networks for motif inference. We build a sequence k-mer graph for the whole dataset based on k-mer co-occurrence and k-mer sequence relationship and then learn DNA Graph Convolutional Network (DNA-GCN) for the whole dataset. Our DNA-GCN is initialized with a one-hot representation for all nodes, and it then jointly learns the embeddings for both k-mers and sequences, as supervised by the known labels of sequences. We evaluate our model on 50 datasets from ENCODE. DNA-GCN shows its competitive performance compared with the baseline model. Besides, we analyze our model and design several different architectures to help fit different datasets.

</p>
</details>

<details><summary><b>Deep Learning Based Analysis of Prostate Cancer from MP-MRI</b>
<a href="https://arxiv.org/abs/2106.01835">arxiv:2106.01835</a>
&#x1F4C8; 1 <br>
<p>Pedro C. Neto</p></summary>
<p>

**Abstract:** The diagnosis of prostate cancer faces a problem with overdiagnosis that leads to damaging side effects due to unnecessary treatment. Research has shown that the use of multi-parametric magnetic resonance images to conduct biopsies can drastically help to mitigate the overdiagnosis, thus reducing the side effects on healthy patients. This study aims to investigate the use of deep learning techniques to explore computer-aid diagnosis based on MRI as input. Several diagnosis problems ranging from classification of lesions as being clinically significant or not to the detection and segmentation of lesions are addressed with deep learning based approaches.
  This thesis tackled two main problems regarding the diagnosis of prostate cancer. Firstly, XmasNet was used to conduct two large experiments on the classification of lesions. Secondly, detection and segmentation experiments were conducted, first on the prostate and afterward on the prostate cancer lesions. The former experiments explored the lesions through a two-dimensional space, while the latter explored models to work with three-dimensional inputs. For this task, the 3D models explored were the 3D U-Net and a pretrained 3D ResNet-18. A rigorous analysis of all these problems was conducted with a total of two networks, two cropping techniques, two resampling techniques, two crop sizes, five input sizes and data augmentations experimented for lesion classification. While for segmentation two models, two input sizes and data augmentations were experimented. However, while the binary classification of the clinical significance of lesions and the detection and segmentation of the prostate already achieve the desired results (0.870 AUC and 0.915 dice score respectively), the classification of the PIRADS score and the segmentation of lesions still have a large margin to improve (0.664 accuracy and 0.690 dice score respectively).

</p>
</details>

<details><summary><b>Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins</b>
<a href="https://arxiv.org/abs/2106.01501">arxiv:2106.01501</a>
&#x1F4C8; 1 <br>
<p>Sahaana Suri, Ihab F. Ilyas, Christopher Ré, Theodoros Rekatsinas</p></summary>
<p>

**Abstract:** Structured data, or data that adheres to a pre-defined schema, can suffer from fragmented context: information describing a single entity can be scattered across multiple datasets or tables tailored for specific business needs, with no explicit linking keys (e.g., primary key-foreign key relationships or heuristic functions). Context enrichment, or rebuilding fragmented context, using keyless joins is an implicit or explicit step in machine learning (ML) pipelines over structured data sources. This process is tedious, domain-specific, and lacks support in now-prevalent no-code ML systems that let users create ML pipelines using just input data and high-level configuration files. In response, we propose Ember, a system that abstracts and automates keyless joins to generalize context enrichment. Our key insight is that Ember can enable a general keyless join operator by constructing an index populated with task-specific embeddings. Ember learns these embeddings by leveraging Transformer-based representation learning techniques. We describe our core architectural principles and operators when developing Ember, and empirically demonstrate that Ember allows users to develop no-code pipelines for five domains, including search, recommendation and question answering, and can exceed alternatives by up to 39% recall, with as little as a single line configuration change.

</p>
</details>

<details><summary><b>IoT Solutions with Multi-Sensor Fusion and Signal-Image Encoding for Secure Data Transfer and Decision Making</b>
<a href="https://arxiv.org/abs/2106.01497">arxiv:2106.01497</a>
&#x1F4C8; 1 <br>
<p>Piyush K. Sharma, Mark Dennison, Adrienne Raglin</p></summary>
<p>

**Abstract:** Deployment of Internet of Things (IoT) devices and Data Fusion techniques have gained popularity in public and government domains. This usually requires capturing and consolidating data from multiple sources. As datasets do not necessarily originate from identical sensors, fused data typically results in a complex data problem. Because military is investigating how heterogeneous IoT devices can aid processes and tasks, we investigate a multi-sensor approach. Moreover, we propose a signal to image encoding approach to transform information (signal) to integrate (fuse) data from IoT wearable devices to an image which is invertible and easier to visualize supporting decision making. Furthermore, we investigate the challenge of enabling an intelligent identification and detection operation and demonstrate the feasibility of the proposed Deep Learning and Anomaly Detection models that can support future application that utilizes hand gesture data from wearable devices.

</p>
</details>

<details><summary><b>Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck</b>
<a href="https://arxiv.org/abs/2106.01177">arxiv:2106.01177</a>
&#x1F4C8; 1 <br>
<p>Nicolas Skatchkovsky, Osvaldo Simeone, Hyeryung Jang</p></summary>
<p>

**Abstract:** One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets.

</p>
</details>

<details><summary><b>Hybrid Ensemble optimized algorithm based on Genetic Programming for imbalanced data classification</b>
<a href="https://arxiv.org/abs/2106.01176">arxiv:2106.01176</a>
&#x1F4C8; 1 <br>
<p>Maliheh Roknizadeh, Hossein Monshizadeh Naeen</p></summary>
<p>

**Abstract:** One of the most significant current discussions in the field of data mining is classifying imbalanced data. In recent years, several ways are proposed such as algorithm level (internal) approaches, data level (external) techniques, and cost-sensitive methods. Although extensive research has been carried out on imbalanced data classification, however, several unsolved challenges remain such as no attention to the importance of samples to balance, determine the appropriate number of classifiers, and no optimization of classifiers in the combination of classifiers. The purpose of this paper is to improve the efficiency of the ensemble method in the sampling of training data sets, especially in the minority class, and to determine better basic classifiers for combining classifiers than existing methods. We proposed a hybrid ensemble algorithm based on Genetic Programming (GP) for two classes of imbalanced data classification. In this study uses historical data from UCI Machine Learning Repository to assess minority classes in imbalanced datasets. The performance of our proposed algorithm is evaluated by Rapid-miner studio v.7.5. Experimental results show the performance of the proposed method on the specified data sets in the size of the training set shows 40% and 50% better accuracy than other dimensions of the minority class prediction.

</p>
</details>

<details><summary><b>Opening the Black Box of Deep Neural Networks in Physical Layer Communication</b>
<a href="https://arxiv.org/abs/2106.01124">arxiv:2106.01124</a>
&#x1F4C8; 1 <br>
<p>Jun Liu, Kai Mei, Dongtang Ma, Jibo Wei</p></summary>
<p>

**Abstract:** Deep Neural Network (DNN)-based physical layer techniques are attracting considerable interest due to their potential to enhance communication systems. However, most studies in the physical layer have tended to focus on the application of DNN models to wireless communication problems but not to theoretically understand how does a DNN work in a communication system. In this letter, we aim to quantitatively analyse why DNNs can achieve comparable performance in the physical layer comparing with traditional techniques and their cost in terms of computational complexity. We further investigate and also experimentally validate how information is flown in a DNN-based communication system under the information theoretic concepts.

</p>
</details>

<details><summary><b>Design and Comparison of Reward Functions in Reinforcement Learning for Energy Management of Sensor Nodes</b>
<a href="https://arxiv.org/abs/2106.01114">arxiv:2106.01114</a>
&#x1F4C8; 1 <br>
<p>Yohann Rioual, Yannick Le Moullec, Johann Laurent, Muhidul Islam Khan, Jean-Philippe Diguet</p></summary>
<p>

**Abstract:** Interest in remote monitoring has grown thanks to recent advancements in Internet-of-Things (IoT) paradigms. New applications have emerged, using small devices called sensor nodes capable of collecting data from the environment and processing it. However, more and more data are processed and transmitted with longer operational periods. At the same, the battery technologies have not improved fast enough to cope with these increasing needs. This makes the energy consumption issue increasingly challenging and thus, miniaturized energy harvesting devices have emerged to complement traditional energy sources. Nevertheless, the harvested energy fluctuates significantly during the node operation, increasing uncertainty in actually available energy resources. Recently, approaches in energy management have been developed, in particular using reinforcement learning approaches. However, in reinforcement learning, the algorithm's performance relies greatly on the reward function. In this paper, we present two contributions. First, we explore five different reward functions to identify the most suitable variables to use in such functions to obtain the desired behaviour. Experiments were conducted using the Q-learning algorithm to adjust the energy consumption depending on the energy harvested. Results with the five reward functions illustrate how the choice thereof impacts the energy consumption of the node. Secondly, we propose two additional reward functions able to find the compromise between energy consumption and a node performance using a non-fixed balancing parameter. Our simulation results show that the proposed reward functions adjust the node's performance depending on the battery level and reduce the learning time.

</p>
</details>

<details><summary><b>Selecting the optimal dialogue response once for all from a panoramic view</b>
<a href="https://arxiv.org/abs/2106.01263">arxiv:2106.01263</a>
&#x1F4C8; 0 <br>
<p>Chiyu Song, Hongliang He, Haofei Yu, Huachuan Qiu, Zhenzhong Lan</p></summary>
<p>

**Abstract:** As an essential component of dialogue systems, response selection aims to pick out the optimal response among candidates to continue the dialogue. In existing studies, this task is usually regarded as a binary classification problem, where every candidate is ranked respectively for appropriateness. To improve its performance, we reformulate this task as a multiple-choice problem that allows the best selection to be made in one-shot inference. This new view inspires us to propose an architecture called Panoramic-encoder (Our work will be open-source for reproducibility and future research.) with a novel Candidates Attention Mechanism (CAM), which allows context-wise attention between responses and leads to fine-grained comparisons. Furthermore, we investigate and incorporate several techniques that have been proven effective for improving response selection. Experiments on three benchmarks show that our method pushes the state-of-the-art while achieving approximately 3X faster inference speed.

</p>
</details>

<details><summary><b>T-BERT -- Model for Sentiment Analysis of Micro-blogs Integrating Topic Model and BERT</b>
<a href="https://arxiv.org/abs/2106.01097">arxiv:2106.01097</a>
&#x1F4C8; 0 <br>
<p>Sarojadevi Palani, Prabhu Rajagopal, Sidharth Pancholi</p></summary>
<p>

**Abstract:** Sentiment analysis (SA) has become an extensive research area in recent years impacting diverse fields including ecommerce, consumer business, and politics, driven by increasing adoption and usage of social media platforms. It is challenging to extract topics and sentiments from unsupervised short texts emerging in such contexts, as they may contain figurative words, strident data, and co-existence of many possible meanings for a single word or phrase, all contributing to obtaining incorrect topics. Most prior research is based on a specific theme/rhetoric/focused-content on a clean dataset. In the work reported here, the effectiveness of BERT(Bidirectional Encoder Representations from Transformers) in sentiment classification tasks from a raw live dataset taken from a popular microblogging platform is demonstrated. A novel T-BERT framework is proposed to show the enhanced performance obtainable by combining latent topics with contextual BERT embeddings. Numerical experiments were conducted on an ensemble with about 42000 datasets using NimbleBox.ai platform with a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU, 15GB RAM running on an isolated Google Cloud Platform instance. The empirical results show that the model improves in performance while adding topics to BERT and an accuracy rate of 90.81% on sentiment classification using BERT with the proposed approach.

</p>
</details>


[Next Page]({{ '/2021/06/01/2021.06.01.html' | relative_url }})
{% endraw %}
