## Summary for 2021-05-25, created on 2021-12-21


<details><summary><b>From Motor Control to Team Play in Simulated Humanoid Football</b>
<a href="https://arxiv.org/abs/2105.12196">arxiv:2105.12196</a>
&#x1F4C8; 227 <br>
<p>Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S. M. Ali Eslami, Daniel Hennes, Wojciech M. Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y. Siegel, Leonard Hasenclever, Luke Marris, Saran Tunyasuvunakool, H. Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D. Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess</p></summary>
<p>

**Abstract:** Intelligent behaviour in the physical world exhibits structure at multiple spatial and temporal scales. Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals defined on much longer timescales, and in terms of relations that extend far beyond the body itself, ultimately involving coordination with other agents. Recent research in artificial intelligence has shown the promise of learning-based approaches to the respective problems of complex movement, longer-term planning and multi-agent coordination. However, there is limited research aimed at their integration. We study this problem by training teams of physically simulated humanoid avatars to play football in a realistic virtual environment. We develop a method that combines imitation learning, single- and multi-agent reinforcement learning and population-based training, and makes use of transferable representations of behaviour for decision making at different levels of abstraction. In a sequence of stages, players first learn to control a fully articulated body to perform realistic, human-like movements such as running and turning; they then acquire mid-level football skills such as dribbling and shooting; finally, they develop awareness of others and play as a team, bridging the gap between low-level motor control at a timescale of milliseconds, and coordinated goal-directed behaviour as a team at the timescale of tens of seconds. We investigate the emergence of behaviours at different levels of abstraction, as well as the representations that underlie these behaviours using several analysis techniques, including statistics from real-world sports analytics. Our work constitutes a complete demonstration of integrated decision-making at multiple scales in a physically embodied multi-agent setting. See project video at https://youtu.be/KHMwq9pv7mg.

</p>
</details>

<details><summary><b>Dense Regression Activation Maps For Lesion Segmentation in CT scans of COVID-19 patients</b>
<a href="https://arxiv.org/abs/2105.11748">arxiv:2105.11748</a>
&#x1F4C8; 34 <br>
<p>Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Bram van Ginneken</p></summary>
<p>

**Abstract:** Automatic lesion segmentation on thoracic CT enables rapid quantitative analysis of lung involvement in COVID-19 infections. However, obtaining a large amount of voxel-level annotations for training segmentation networks is prohibitively expensive. Therefore, we propose a weakly-supervised segmentation method based on dense regression activation maps (dRAMs). Most weakly-supervised segmentation approaches exploit class activation maps (CAMs) to localize objects. However, because CAMs were trained for classification, they do not align precisely with the object segmentations. Instead, we produce high-resolution activation maps using dense features from a segmentation network that was trained to estimate a per-lobe lesion percentage. In this way, the network can exploit knowledge regarding the required lesion volume. In addition, we propose an attention neural network module to refine dRAMs, optimized together with the main regression task. We evaluated our algorithm on 90 subjects. Results show our method achieved 70.2% Dice coefficient, substantially outperforming the CAM-based baseline at 48.6%.

</p>
</details>

<details><summary><b>CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning</b>
<a href="https://arxiv.org/abs/2105.11863">arxiv:2105.11863</a>
&#x1F4C8; 33 <br>
<p>Manvel Avetisian, Ilya Burenko, Konstantin Egorov, Vladimir Kokh, Aleksandr Nesterov, Aleksandr Nikolaev, Alexander Ponomarchuk, Elena Sokolova, Alex Tuzhilin, Dmitry Umerenkov</p></summary>
<p>

**Abstract:** Analysis of chest CT scans can be used in detecting parts of lungs that are affected by infectious diseases such as COVID-19.Determining the volume of lungs affected by lesions is essential for formulating treatment recommendations and prioritizingpatients by severity of the disease. In this paper we adopted an approach based on using an ensemble of deep convolutionalneural networks for segmentation of slices of lung CT scans. Using our models we are able to segment the lesions, evaluatepatients dynamics, estimate relative volume of lungs affected by lesions and evaluate the lung damage stage. Our modelswere trained on data from different medical centers. We compared predictions of our models with those of six experiencedradiologists and our segmentation model outperformed most of them. On the task of classification of disease severity, ourmodel outperformed all the radiologists.

</p>
</details>

<details><summary><b>Scaling Properties of Deep Residual Networks</b>
<a href="https://arxiv.org/abs/2105.12245">arxiv:2105.12245</a>
&#x1F4C8; 31 <br>
<p>Alain-Sam Cohen, Rama Cont, Alain Rossier, Renyuan Xu</p></summary>
<p>

**Abstract:** Residual networks (ResNets) have displayed impressive results in pattern recognition and, recently, have garnered considerable theoretical interest due to a perceived link with neural ordinary differential equations (neural ODEs). This link relies on the convergence of network weights to a smooth function as the number of layers increases. We investigate the properties of weights trained by stochastic gradient descent and their scaling with network depth through detailed numerical experiments. We observe the existence of scaling regimes markedly different from those assumed in neural ODE literature. Depending on certain features of the network architecture, such as the smoothness of the activation function, one may obtain an alternative ODE limit, a stochastic differential equation or neither of these. These findings cast doubts on the validity of the neural ODE model as an adequate asymptotic description of deep ResNets and point to an alternative class of differential equations as a better description of the deep network limit.

</p>
</details>

<details><summary><b>Learning Generative Prior with Latent Space Sparsity Constraints</b>
<a href="https://arxiv.org/abs/2105.11956">arxiv:2105.11956</a>
&#x1F4C8; 30 <br>
<p>Vinayak Killedar, Praveen Kumar Pokala, Chandra Sekhar Seelamantula</p></summary>
<p>

**Abstract:** We address the problem of compressed sensing using a deep generative prior model and consider both linear and learned nonlinear sensing mechanisms, where the nonlinear one involves either a fully connected neural network or a convolutional neural network. Recently, it has been argued that the distribution of natural images do not lie in a single manifold but rather lie in a union of several submanifolds. We propose a sparsity-driven latent space sampling (SDLSS) framework and develop a proximal meta-learning (PML) algorithm to enforce sparsity in the latent space. SDLSS allows the range-space of the generator to be considered as a union-of-submanifolds. We also derive the sample complexity bounds within the SDLSS framework for the linear measurement model. The results demonstrate that for a higher degree of compression, the SDLSS method is more efficient than the state-of-the-art method. We first consider a comparison between linear and nonlinear sensing mechanisms on Fashion-MNIST dataset and show that the learned nonlinear version is superior to the linear one. Subsequent comparisons with the deep compressive sensing (DCS) framework proposed in the literature are reported. We also consider the effect of the dimension of the latent space and the sparsity factor in validating the SDLSS framework. Performance quantification is carried out by employing three objective metrics: peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and reconstruction error (RE).

</p>
</details>

<details><summary><b>ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</b>
<a href="https://arxiv.org/abs/2105.11741">arxiv:2105.11741</a>
&#x1F4C8; 24 <br>
<p>Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu</p></summary>
<p>

**Abstract:** Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.

</p>
</details>

<details><summary><b>Learning Bipedal Robot Locomotion from Human Movement</b>
<a href="https://arxiv.org/abs/2105.12277">arxiv:2105.12277</a>
&#x1F4C8; 23 <br>
<p>Michael Taylor, Sergey Bashkirov, Javier Fernandez Rico, Ike Toriyama, Naoyuki Miyada, Hideki Yanagisawa, Kensaku Ishizuka</p></summary>
<p>

**Abstract:** Teaching an anthropomorphic robot from human example offers the opportunity to impart humanlike qualities on its movement. In this work we present a reinforcement learning based method for teaching a real world bipedal robot to perform movements directly from human motion capture data. Our method seamlessly transitions from training in a simulation environment to executing on a physical robot without requiring any real world training iterations or offline steps. To overcome the disparity in joint configurations between the robot and the motion capture actor, our method incorporates motion re-targeting into the training process. Domain randomization techniques are used to compensate for the differences between the simulated and physical systems. We demonstrate our method on an internally developed humanoid robot with movements ranging from a dynamic walk cycle to complex balancing and waving. Our controller preserves the style imparted by the motion capture data and exhibits graceful failure modes resulting in safe operation for the robot. This work was performed for research purposes only.

</p>
</details>

<details><summary><b>Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting</b>
<a href="https://arxiv.org/abs/2105.11698">arxiv:2105.11698</a>
&#x1F4C8; 22 <br>
<p>Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li, Chenghua Lin, Yefeng Zheng</p></summary>
<p>

**Abstract:** This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method.

</p>
</details>

<details><summary><b>ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents</b>
<a href="https://arxiv.org/abs/2105.11672">arxiv:2105.11672</a>
&#x1F4C8; 21 <br>
<p>Weihong Lin, Qifang Gao, Lei Sun, Zhuoyao Zhong, Kai Hu, Qin Ren, Qiang Huo</p></summary>
<p>

**Abstract:** Recent grid-based document representations like BERTgrid allow the simultaneous encoding of the textual and layout information of a document in a 2D feature map so that state-of-the-art image segmentation and/or object detection models can be straightforwardly leveraged to extract key information from documents. However, such methods have not achieved comparable performance to state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK yet. In this paper, we propose a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, where the input of CNN is a document image and the BERTgrid is a grid of word embeddings, to generate a more powerful grid-based document representation, named ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal backbone network are trained jointly. Our experimental results demonstrate that this joint training strategy improves significantly the representation ability of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction approach has achieved state-of-the-art performance on real-world datasets.

</p>
</details>

<details><summary><b>Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation</b>
<a href="https://arxiv.org/abs/2106.06471">arxiv:2106.06471</a>
&#x1F4C8; 15 <br>
<p>Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma</p></summary>
<p>

**Abstract:** Medical report generation is one of the most challenging tasks in medical image analysis. Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation. MedWriter first employs the Visual-Language Retrieval~(VLR) module to retrieve the most relevant reports for the given images. To guarantee the logical coherence between sentences, the Language-Language Retrieval~(LLR) module is introduced to retrieve relevant sentences based on the previous generated description. At last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports. We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., Open-I and MIMIC-CXR.

</p>
</details>

<details><summary><b>FNAS: Uncertainty-Aware Fast Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2105.11694">arxiv:2105.11694</a>
&#x1F4C8; 15 <br>
<p>Jihao Liu, Ming Zhang, Yangting Sun, Boxiao Liu, Guanglu Song, Yu Liu, Hongsheng Li</p></summary>
<p>

**Abstract:** Reinforcement learning (RL)-based neural architecture search (NAS) generally guarantees better convergence yet suffers from the requirement of huge computational resources compared with gradient-based approaches, due to the rollout bottleneck -- exhaustive training for each sampled generation on proxy tasks. In this paper, we propose a general pipeline to accelerate the convergence of the rollout process as well as the RL process in NAS. It is motivated by the interesting observation that both the architecture and the parameter knowledge can be transferred between different experiments and even different tasks. We first introduce an uncertainty-aware critic (value function) in Proximal Policy Optimization (PPO) to utilize the architecture knowledge in previous experiments, which stabilizes the training process and reduces the searching time by 4 times. Further, an architecture knowledge pool together with a block similarity function is proposed to utilize parameter knowledge and reduces the searching time by 2 times. It is the first to introduce block-level weight sharing in RLbased NAS. The block similarity function guarantees a 100% hitting ratio with strict fairness. Besides, we show that a simply designed off-policy correction factor used in "replay buffer" in RL optimization can further reduce half of the searching time. Experiments on the Mobile Neural Architecture Search (MNAS) search space show the proposed Fast Neural Architecture Search (FNAS) accelerates standard RL-based NAS process by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x hour for MNAS), and guarantees better performance on various vision tasks.

</p>
</details>

<details><summary><b>TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2105.11871">arxiv:2105.11871</a>
&#x1F4C8; 14 <br>
<p>Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang, Zhenguo Li</p></summary>
<p>

**Abstract:** Recent breakthroughs of Neural Architecture Search (NAS) extend the field's research scope towards a broader range of vision tasks and more diversified search spaces. While existing NAS methods mostly design architectures on a single task, algorithms that look beyond single-task search are surging to pursue a more efficient and universal solution across various tasks. Many of them leverage transfer learning and seek to preserve, reuse, and refine network design knowledge to achieve higher efficiency in future tasks. However, the enormous computational cost and experiment complexity of cross-task NAS are imposing barriers for valuable research in this direction. Existing NAS benchmarks all focus on one type of vision task, i.e., classification. In this work, we propose TransNAS-Bench-101, a benchmark dataset containing network performance across seven tasks, covering classification, regression, pixel-level prediction, and self-supervised tasks. This diversity provides opportunities to transfer NAS methods among tasks and allows for more complex transfer schemes to evolve. We explore two fundamentally different types of search space: cell-level search space and macro-level search space. With 7,352 backbones evaluated on seven tasks, 51,464 trained models with detailed training information are provided. With TransNAS-Bench-101, we hope to encourage the advent of exceptional NAS algorithms that raise cross-task search efficiency and generalizability to the next level. Our dataset file will be available at Mindspore, VEGA.

</p>
</details>

<details><summary><b>GCNBoost: Artwork Classification by Label Propagation through a Knowledge Graph</b>
<a href="https://arxiv.org/abs/2105.11852">arxiv:2105.11852</a>
&#x1F4C8; 14 <br>
<p>Cheikh Brahim El Vaigh, Noa Garcia, Benjamin Renoust, Chenhui Chu, Yuta Nakashima, Hajime Nagahara</p></summary>
<p>

**Abstract:** The rise of digitization of cultural documents offers large-scale contents, opening the road for development of AI systems in order to preserve, search, and deliver cultural heritage. To organize such cultural content also means to classify them, a task that is very familiar to modern computer science. Contextual information is often the key to structure such real world data, and we propose to use it in form of a knowledge graph. Such a knowledge graph, combined with content analysis, enhances the notion of proximity between artworks so it improves the performances in classification tasks. In this paper, we propose a novel use of a knowledge graph, that is constructed on annotated data and pseudo-labeled data. With label propagation, we boost artwork classification by training a model using a graph convolutional network, relying on the relationships between entities of the knowledge graph. Following a transductive learning framework, our experiments show that relying on a knowledge graph modeling the relations between labeled data and unlabeled data allows to achieve state-of-the-art results on multiple classification tasks on a dataset of paintings, and on a dataset of Buddha statues. Additionally, we show state-of-the-art results for the difficult case of dealing with unbalanced data, with the limitation of disregarding classes with extremely low degrees in the knowledge graph.

</p>
</details>

<details><summary><b>Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift</b>
<a href="https://arxiv.org/abs/2105.11804">arxiv:2105.11804</a>
&#x1F4C8; 14 <br>
<p>Etienne Bennequin, Victor Bouvier, Myriam Tami, Antoine Toubhans, Céline Hudelot</p></summary>
<p>

**Abstract:** Few-Shot Learning (FSL) algorithms have made substantial progress in learning novel concepts with just a handful of labelled data. To classify query instances from novel classes encountered at test-time, they only require a support set composed of a few labelled samples. FSL benchmarks commonly assume that those queries come from the same distribution as instances in the support set. However, in a realistic set-ting, data distribution is plausibly subject to change, a situation referred to as Distribution Shift (DS). The present work addresses the new and challenging problem of Few-Shot Learning under Support/Query Shift (FSQS) i.e., when support and query instances are sampled from related but different distributions. Our contributions are the following. First, we release a testbed for FSQS, including datasets, relevant baselines and a protocol for a rigorous and reproducible evaluation. Second, we observe that well-established FSL algorithms unsurprisingly suffer from a considerable drop in accuracy when facing FSQS, stressing the significance of our study. Finally, we show that transductive algorithms can limit the inopportune effect of DS. In particular, we study both the role of Batch-Normalization and Optimal Transport (OT) in aligning distributions, bridging Unsupervised Domain Adaptation with FSL. This results in a new method that efficiently combines OT with the celebrated Prototypical Networks. We bring compelling experiments demonstrating the advantage of our method. Our work opens an exciting line of research by providing a testbed and strong baselines. Our code is available at https://github.com/ebennequin/meta-domain-shift.

</p>
</details>

<details><summary><b>Deep learning-based bias transfer for overcoming laboratory differences of microscopic images</b>
<a href="https://arxiv.org/abs/2105.11765">arxiv:2105.11765</a>
&#x1F4C8; 14 <br>
<p>Ann-Katrin Thebille, Esther Dietrich, Martin Klaus, Lukas Gernhold, Maximilian Lennartz, Christoph Kuppe, Rafael Kramann, Tobias B. Huber, Guido Sauter, Victor G. Puelles, Marina Zimmermann, Stefan Bonn</p></summary>
<p>

**Abstract:** The automated analysis of medical images is currently limited by technical and biological noise and bias. The same source tissue can be represented by vastly different images if the image acquisition or processing protocols vary. For an image analysis pipeline, it is crucial to compensate such biases to avoid misinterpretations. Here, we evaluate, compare, and improve existing generative model architectures to overcome domain shifts for immunofluorescence (IF) and Hematoxylin and Eosin (H&E) stained microscopy images. To determine the performance of the generative models, the original and transformed images were segmented or classified by deep neural networks that were trained only on images of the target bias. In the scope of our analysis, U-Net cycleGANs trained with an additional identity and an MS-SSIM-based loss and Fixed-Point GANs trained with an additional structure loss led to the best results for the IF and H&E stained samples, respectively. Adapting the bias of the samples significantly improved the pixel-level segmentation for human kidney glomeruli and podocytes and improved the classification accuracy for human prostate biopsies by up to 14%.

</p>
</details>

<details><summary><b>Emotion Recognition in Horses with Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2105.11953">arxiv:2105.11953</a>
&#x1F4C8; 13 <br>
<p>Luis A. Corujo, Peter A. Gloor, Emily Kieson</p></summary>
<p>

**Abstract:** Creating intelligent systems capable of recognizing emotions is a difficult task, especially when looking at emotions in animals. This paper describes the process of designing a "proof of concept" system to recognize emotions in horses. This system is formed by two elements, a detector and a model. The detector is a faster region-based convolutional neural network that detects horses in an image. The second one, the model, is a convolutional neural network that predicts the emotion of those horses. These two models were trained with multiple images of horses until they achieved high accuracy in their tasks, creating therefore the desired system. 400 images of horses were used to train both the detector and the model while 80 were used to validate the system. Once the two components were validated they were combined into a testable system that would detect equine emotions based on established behavioral ethograms indicating emotional affect through head, neck, ear, muzzle, and eye position. The system showed an accuracy of between 69% and 74% on the validation set, demonstrating that it is possible to predict emotions in animals using autonomous intelligent systems. It is a first "proof of concept" approach that can be enhanced in many ways. Such a system has multiple applications including further studies in the growing field of animal emotions as well as in the veterinary field to determine the physical welfare of horses or other livestock.

</p>
</details>

<details><summary><b>A Geometry-Informed Deep Learning Framework for Ultra-Sparse 3D Tomographic Image Reconstruction</b>
<a href="https://arxiv.org/abs/2105.11692">arxiv:2105.11692</a>
&#x1F4C8; 13 <br>
<p>Liyue Shen, Wei Zhao, Dante Capaldi, John Pauly, Lei Xing</p></summary>
<p>

**Abstract:** Deep learning affords enormous opportunities to augment the armamentarium of biomedical imaging, albeit its design and implementation have potential flaws. Fundamentally, most deep learning models are driven entirely by data without consideration of any prior knowledge, which dramatically increases the complexity of neural networks and limits the application scope and model generalizability. Here we establish a geometry-informed deep learning framework for ultra-sparse 3D tomographic image reconstruction. We introduce a novel mechanism for integrating geometric priors of the imaging system. We demonstrate that the seamless inclusion of known priors is essential to enhance the performance of 3D volumetric computed tomography imaging with ultra-sparse sampling. The study opens new avenues for data-driven biomedical imaging and promises to provide substantially improved imaging tools for various clinical imaging and image-guided interventions.

</p>
</details>

<details><summary><b>Quantifying Uncertainty in Deep Spatiotemporal Forecasting</b>
<a href="https://arxiv.org/abs/2105.11982">arxiv:2105.11982</a>
&#x1F4C8; 12 <br>
<p>Dongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro Vespignani, Yi-An Ma, Rose Yu</p></summary>
<p>

**Abstract:** Deep learning is gaining increasing popularity for spatiotemporal forecasting. However, prior works have mostly focused on point estimates without quantifying the uncertainty of the predictions. In high stakes domains, being able to generate probabilistic forecasts with confidence intervals is critical to risk assessment and decision making. Hence, a systematic study of uncertainty quantification (UQ) methods for spatiotemporal forecasting is missing in the community. In this paper, we describe two types of spatiotemporal forecasting problems: regular grid-based and graph-based. Then we analyze UQ methods from both the Bayesian and the frequentist point of view, casting in a unified framework via statistical decision theory. Through extensive experiments on real-world road network traffic, epidemics, and air quality forecasting tasks, we reveal the statistical and computational trade-offs for different UQ methods: Bayesian methods are typically more robust in mean prediction, while confidence levels obtained from frequentist methods provide more extensive coverage over data variations. Computationally, quantile regression type methods are cheaper for a single confidence interval but require re-training for different intervals. Sampling based methods generate samples that can form multiple confidence intervals, albeit at a higher computational cost.

</p>
</details>

<details><summary><b>A Modulation Front-End for Music Audio Tagging</b>
<a href="https://arxiv.org/abs/2105.11836">arxiv:2105.11836</a>
&#x1F4C8; 12 <br>
<p>Cyrus Vahidi, Charalampos Saitis, György Fazekas</p></summary>
<p>

**Abstract:** Convolutional Neural Networks have been extensively explored in the task of automatic music tagging. The problem can be approached by using either engineered time-frequency features or raw audio as input. Modulation filter bank representations that have been actively researched as a basis for timbre perception have the potential to facilitate the extraction of perceptually salient features. We explore end-to-end learned front-ends for audio representation learning, ModNet and SincModNet, that incorporate a temporal modulation processing block. The structure is effectively analogous to a modulation filter bank, where the FIR filter center frequencies are learned in a data-driven manner. The expectation is that a perceptually motivated filter bank can provide a useful representation for identifying music features. Our experimental results provide a fully visualisable and interpretable front-end temporal modulation decomposition of raw audio. We evaluate the performance of our model against the state-of-the-art of music tagging on the MagnaTagATune dataset. We analyse the impact on performance for particular tags when time-frequency bands are subsampled by the modulation filters at a progressively reduced rate. We demonstrate that modulation filtering provides promising results for music tagging and feature representation, without using extensive musical domain knowledge in the design of this front-end.

</p>
</details>

<details><summary><b>Matching Targets Across Domains with RADON, the Re-Identification Across Domain Network</b>
<a href="https://arxiv.org/abs/2105.12056">arxiv:2105.12056</a>
&#x1F4C8; 10 <br>
<p>Cassandra Burgess, Cordelia Neisinger, Rafael Dinner</p></summary>
<p>

**Abstract:** We present a novel convolutional neural network that learns to match images of an object taken from different viewpoints or by different optical sensors. Our Re-Identification Across Domain Network (RADON) scores pairs of input images from different domains on similarity. Our approach extends previous work on Siamese networks and modifies them to more challenging use cases, including low- and no-shot learning, in which few images of a specific target are available for training. RADON shows strong performance on cross-view vehicle matching and cross-domain person identification in a no-shot learning environment.

</p>
</details>

<details><summary><b>Model-Constrained Deep Learning Approaches for Inverse Problems</b>
<a href="https://arxiv.org/abs/2105.12033">arxiv:2105.12033</a>
&#x1F4C8; 10 <br>
<p>Hai V. Nguyen, Tan Bui-Thanh</p></summary>
<p>

**Abstract:** Deep Learning (DL), in particular deep neural networks (DNN), by design is purely data-driven and in general does not require physics. This is the strength of DL but also one of its key limitations when applied to science and engineering problems in which underlying physical properties (such as stability, conservation, and positivity) and desired accuracy need to be achieved. DL methods in their original forms are not capable of respecting the underlying mathematical models or achieving desired accuracy even in big-data regimes. On the other hand, many data-driven science and engineering problems, such as inverse problems, typically have limited experimental or observational data, and DL would overfit the data in this case. Leveraging information encoded in the underlying mathematical models, we argue, not only compensates missing information in low data regimes but also provides opportunities to equip DL methods with the underlying physics and hence obtaining higher accuracy. This short communication introduces several model-constrained DL approaches (including both feed-forward DNN and autoencoders) that are capable of learning not only information hidden in the training data but also in the underlying mathematical models to solve inverse problems. We present and provide intuitions for our formulations for general nonlinear problems. For linear inverse problems and linear networks, the first order optimality conditions show that our model-constrained DL approaches can learn information encoded in the underlying mathematical models, and thus can produce consistent or equivalent inverse solutions, while naive purely data-based counterparts cannot.

</p>
</details>

<details><summary><b>Principal Component Hierarchy for Sparse Quadratic Programs</b>
<a href="https://arxiv.org/abs/2105.12022">arxiv:2105.12022</a>
&#x1F4C8; 10 <br>
<p>Robbie Vreugdenhil, Viet Anh Nguyen, Armin Eftekhari, Peyman Mohajerin Esfahani</p></summary>
<p>

**Abstract:** We propose a novel approximation hierarchy for cardinality-constrained, convex quadratic programs that exploits the rank-dominating eigenvectors of the quadratic matrix. Each level of approximation admits a min-max characterization whose objective function can be optimized over the binary variables analytically, while preserving convexity in the continuous variables. Exploiting this property, we propose two scalable optimization algorithms, coined as the "best response" and the "dual program", that can efficiently screen the potential indices of the nonzero elements of the original program. We show that the proposed methods are competitive with the existing screening methods in the current sparse regression literature, and it is particularly fast on instances with high number of measurements in experiments with both synthetic and real datasets.

</p>
</details>

<details><summary><b>Towards a method to anticipate dark matter signals with deep learning at the LHC</b>
<a href="https://arxiv.org/abs/2105.12018">arxiv:2105.12018</a>
&#x1F4C8; 10 <br>
<p>Ernesto Arganda, Anibal D. Medina, Andres D. Perez, Alejandro Szynkman</p></summary>
<p>

**Abstract:** We study several simplified dark matter (DM) models and their signatures at the LHC using neural networks. We focus on the usual monojet plus missing transverse energy channel, but to train the algorithms we organize the data in 2D histograms instead of event-by-event arrays. This results in a large performance boost to distinguish between standard model (SM) only and SM plus new physics signals. We use the kinematic monojet features as input data which allow us to describe families of models with a single data sample. We found that the neural network performance does not depend on the simulated number of background events if they are presented as a function of $S/\sqrt{B}$, where $S$ and $B$ are the number of signal and background events per histogram, respectively. This provides flexibility to the method, since testing a particular model in that case only requires knowing the new physics monojet cross section. Furthermore, we also discuss the network performance under incorrect assumptions about the true DM nature. Finally, we propose multimodel classifiers to search and identify new signals in a more general way, for the next LHC run.

</p>
</details>

<details><summary><b>Model Mismatch Trade-offs in LMMSE Estimation</b>
<a href="https://arxiv.org/abs/2105.11964">arxiv:2105.11964</a>
&#x1F4C8; 10 <br>
<p>Martin Hellkvist, Ayça Özçelikkale</p></summary>
<p>

**Abstract:** We consider a linear minimum mean squared error (LMMSE) estimation framework with model mismatch where the assumed model order is smaller than that of the underlying linear system which generates the data used in the estimation process. By modelling the regressors of the underlying system as random variables, we analyze the average behaviour of the mean squared error (MSE). Our results quantify how the MSE depends on the interplay between the number of samples and the number of parameters in the underlying system and in the assumed model. In particular, if the number of samples is not sufficiently large, neither increasing the number of samples nor the assumed model complexity is sufficient to guarantee a performance improvement.

</p>
</details>

<details><summary><b>Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing</b>
<a href="https://arxiv.org/abs/2105.11798">arxiv:2105.11798</a>
&#x1F4C8; 10 <br>
<p>Carlos Basto</p></summary>
<p>

**Abstract:** A data-centric approach with Natural Language Processing (NLP) to predict personality types based on the MBTI (an introspective self-assessment questionnaire that indicates different psychological preferences about how people perceive the world and make decisions) through systematic enrichment of text representation, based on the domain of the area, under the generation of features based on three types of analysis: sentimental, grammatical and aspects. The experimentation had a robust baseline of stacked models, with premature optimization of hyperparameters through grid search, with gradual feedback, for each of the four classifiers (dichotomies) of MBTI. The results showed that attention to the data iteration loop focused on quality, explanatory power and representativeness for the abstraction of more relevant/important resources for the studied phenomenon made it possible to improve the evaluation metrics results more quickly and less costly than complex models such as the LSTM or state of the art ones as BERT, as well as the importance of these results by comparisons made from various perspectives. In addition, the study demonstrated a broad spectrum for the evolution and deepening of the task and possible approaches for a greater extension of the abstraction of personality types.

</p>
</details>

<details><summary><b>Calibration and Uncertainty Quantification of Bayesian Convolutional Neural Networks for Geophysical Applications</b>
<a href="https://arxiv.org/abs/2105.12115">arxiv:2105.12115</a>
&#x1F4C8; 9 <br>
<p>Lukas Mosser, Ehsan Zabihi Naeini</p></summary>
<p>

**Abstract:** Deep neural networks offer numerous potential applications across geoscience, for example, one could argue that they are the state-of-the-art method for predicting faults in seismic datasets. In quantitative reservoir characterization workflows, it is common to incorporate the uncertainty of predictions thus such subsurface models should provide calibrated probabilities and the associated uncertainties in their predictions. It has been shown that popular Deep Learning-based models are often miscalibrated, and due to their deterministic nature, provide no means to interpret the uncertainty of their predictions. We compare three different approaches to obtaining probabilistic models based on convolutional neural networks in a Bayesian formalism, namely Deep Ensembles, Concrete Dropout, and Stochastic Weight Averaging-Gaussian (SWAG). These methods are consistently applied to fault detection case studies where Deep Ensembles use independently trained models to provide fault probabilities, Concrete Dropout represents an extension to the popular Dropout technique to approximate Bayesian neural networks, and finally, we apply SWAG, a recent method that is based on the Bayesian inference equivalence of mini-batch Stochastic Gradient Descent. We provide quantitative results in terms of model calibration and uncertainty representation, as well as qualitative results on synthetic and real seismic datasets. Our results show that the approximate Bayesian methods, Concrete Dropout and SWAG, both provide well-calibrated predictions and uncertainty attributes at a lower computational cost when compared to the baseline Deep Ensemble approach. The resulting uncertainties also offer a possibility to further improve the model performance as well as enhancing the interpretability of the models.

</p>
</details>

<details><summary><b>Trajectory Modeling via Random Utility Inverse Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.12092">arxiv:2105.12092</a>
&#x1F4C8; 9 <br>
<p>Anselmo R. Pitombeira-Neto, Helano P. Santos, Ticiana L. Coelho da Silva, José Antonio F. de Macedo</p></summary>
<p>

**Abstract:** We consider the problem of modeling trajectories of drivers in a road network from the perspective of inverse reinforcement learning. As rational agents, drivers are trying to maximize some reward function unknown to an external observer as they make up their trajectories. We apply the concept of random utility from microeconomic theory to model the unknown reward function as a function of observable features plus an error term which represents features known only to the driver. We develop a parameterized generative model for the trajectories based on a random utility Markov decision process formulation of drivers decisions. We show that maximum entropy inverse reinforcement learning is a particular case of our proposed formulation when we assume a Gumbel density function for the unobserved reward error terms. We illustrate Bayesian inference on model parameters through a case study with real trajectory data from a large city obtained from sensors placed on sparsely distributed points on the street network.

</p>
</details>

<details><summary><b>Quantum Embedding Search for Quantum Machine Learning</b>
<a href="https://arxiv.org/abs/2105.11853">arxiv:2105.11853</a>
&#x1F4C8; 9 <br>
<p>Nam Nguyen, Kwang-Chen Chen</p></summary>
<p>

**Abstract:** This paper introduces a novel quantum embedding search algorithm (QES, pronounced as "quest"), enabling search for optimal quantum embedding design for a specific dataset of interest. First, we establish the connection between the structures of quantum embedding and the representations of directed multi-graphs, enabling a well-defined search space. Second, we instigate the entanglement level to reduce the cardinality of the search space to a feasible size for practical implementations. Finally, we mitigate the cost of evaluating the true loss function by using surrogate models via sequential model-based optimization. We demonstrate the feasibility of our proposed approach on synthesis and Iris datasets, which empirically shows that found quantum embedding architecture by QES outperforms manual designs whereas achieving comparable performance to classical machine learning models.

</p>
</details>

<details><summary><b>DiBS: Differentiable Bayesian Structure Learning</b>
<a href="https://arxiv.org/abs/2105.11839">arxiv:2105.11839</a>
&#x1F4C8; 9 <br>
<p>Lars Lorch, Jonas Rothfuss, Bernhard Schölkopf, Andreas Krause</p></summary>
<p>

**Abstract:** Bayesian structure learning allows inferring Bayesian network structure from data while reasoning about the epistemic uncertainty -- a key element towards enabling active causal discovery and designing interventions in real world systems. In this work, we propose a general, fully differentiable framework for Bayesian structure learning (DiBS) that operates in the continuous space of a latent probabilistic graph representation. Contrary to existing work, DiBS is agnostic to the form of the local conditional distributions and allows for joint posterior inference of both the graph structure and the conditional distribution parameters. This makes our formulation directly applicable to posterior inference of complex Bayesian network models, e.g., with nonlinear dependencies encoded by neural networks. Using DiBS, we devise an efficient, general purpose variational inference method for approximating distributions over structural models. In evaluations on simulated and real-world data, our method significantly outperforms related approaches to joint posterior inference.

</p>
</details>

<details><summary><b>SGD with Coordinate Sampling: Theory and Practice</b>
<a href="https://arxiv.org/abs/2105.11818">arxiv:2105.11818</a>
&#x1F4C8; 9 <br>
<p>Rémi Leluc, François Portier</p></summary>
<p>

**Abstract:** While classical forms of stochastic gradient descent algorithm treat the different coordinates in the same way, a framework allowing for adaptive (non uniform) coordinate sampling is developed to leverage structure in data. In a non-convex setting and including zeroth order gradient estimate, almost sure convergence as well as non-asymptotic bounds are established. Within the proposed framework, we develop an algorithm, MUSKETEER, based on a reinforcement strategy: after collecting information on the noisy gradients, it samples the most promising coordinate (all for one); then it moves along the one direction yielding an important decrease of the objective (one for all). Numerical experiments on both synthetic and real data examples confirm the effectiveness of MUSKETEER in large scale problems.

</p>
</details>

<details><summary><b>We Know What You Want: An Advertising Strategy Recommender System for Online Advertising</b>
<a href="https://arxiv.org/abs/2105.14188">arxiv:2105.14188</a>
&#x1F4C8; 8 <br>
<p>Liyi Guo, Junqi Jin, Haoqi Zhang, Zhenzhe Zheng, Zhiye Yang, Zhizhuang Xing, Fei Pan, Lvyin Niu, Fan Wu, Haiyang Xu, Chuan Yu, Yuning Jiang, Xiaoqiang Zhu</p></summary>
<p>

**Abstract:** Advertising expenditures have become the major source of revenue for e-commerce platforms. Providing good advertising experiences for advertisers by reducing their costs of trial and error in discovering the optimal advertising strategies is crucial for the long-term prosperity of online advertising. To achieve this goal, the advertising platform needs to identify the advertiser's optimization objectives, and then recommend the corresponding strategies to fulfill the objectives. In this work, we first deploy a prototype of strategy recommender system on Taobao display advertising platform, which indeed increases the advertisers' performance and the platform's revenue, indicating the effectiveness of strategy recommendation for online advertising. We further augment this prototype system by explicitly learning the advertisers' preferences over various advertising performance indicators and then optimization objectives through their adoptions of different recommending advertising strategies. We use contextual bandit algorithms to efficiently learn the advertisers' preferences and maximize the recommendation adoption, simultaneously. Simulation experiments based on Taobao online bidding data show that the designed algorithms can effectively optimize the strategy adoption rate of advertisers.

</p>
</details>

<details><summary><b>Practical Schemes for Finding Near-Stationary Points of Convex Finite-Sums</b>
<a href="https://arxiv.org/abs/2105.12062">arxiv:2105.12062</a>
&#x1F4C8; 8 <br>
<p>Kaiwen Zhou, Lai Tian, Anthony Man-Cho So, James Cheng</p></summary>
<p>

**Abstract:** The problem of finding near-stationary points in convex optimization has not been adequately studied yet, unlike other optimality measures such as minimizing function value. Even in the deterministic case, the optimal method (OGM-G, due to Kim and Fessler (2021)) has just been discovered recently. In this work, we conduct a systematic study of the algorithmic techniques in finding near-stationary points of convex finite-sums. Our main contributions are several algorithmic discoveries: (1) we discover a memory-saving variant of OGM-G based on the performance estimation problem approach (Drori and Teboulle, 2014); (2) we design a new accelerated SVRG variant that can simultaneously achieve fast rates for both minimizing gradient norm and function value; (3) we propose an adaptively regularized accelerated SVRG variant, which does not require the knowledge of some unknown initial constants and achieves near-optimal complexities. We put an emphasis on the simplicity and practicality of the new schemes, which could facilitate future developments.

</p>
</details>

<details><summary><b>On learning parametric distributions from quantized samples</b>
<a href="https://arxiv.org/abs/2105.12019">arxiv:2105.12019</a>
&#x1F4C8; 8 <br>
<p>Septimia Sarbu, Abdellatif Zaidi</p></summary>
<p>

**Abstract:** We consider the problem of learning parametric distributions from their quantized samples in a network. Specifically, $n$ agents or sensors observe independent samples of an unknown parametric distribution; and each of them uses $k$ bits to describe its observed sample to a central processor whose goal is to estimate the unknown distribution. First, we establish a generalization of the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in terms of Generalized Fisher information. Then, we develop minimax lower bounds on the estimation error for two losses: general $L_p$-norms and the related Wasserstein loss from optimal transport.

</p>
</details>

<details><summary><b>Hierarchical Subspace Learning for Dimensionality Reduction to Improve Classification Accuracy in Large Data Sets</b>
<a href="https://arxiv.org/abs/2105.12005">arxiv:2105.12005</a>
&#x1F4C8; 8 <br>
<p>Parisa Abdolrahim Poorheravi, Vincent Gaudet</p></summary>
<p>

**Abstract:** Manifold learning is used for dimensionality reduction, with the goal of finding a projection subspace to increase and decrease the inter- and intraclass variances, respectively. However, a bottleneck for subspace learning methods often arises from the high dimensionality of datasets. In this paper, a hierarchical approach is proposed to scale subspace learning methods, with the goal of improving classification in large datasets by a range of 3% to 10%. Different combinations of methods are studied. We assess the proposed method on five publicly available large datasets, for different eigen-value based subspace learning methods such as linear discriminant analysis, principal component analysis, generalized discriminant analysis, and reconstruction independent component analysis. To further examine the effect of the proposed method on various classification methods, we fed the generated result to linear discriminant analysis, quadratic linear analysis, k-nearest neighbor, and random forest classifiers. The resulting classification accuracies are compared to show the effectiveness of the hierarchical approach, reporting results of an average of 5% increase in classification accuracy.

</p>
</details>

<details><summary><b>Spectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices</b>
<a href="https://arxiv.org/abs/2105.11856">arxiv:2105.11856</a>
&#x1F4C8; 8 <br>
<p>Michał Kośmider</p></summary>
<p>

**Abstract:** Machine learning algorithms, when trained on audio recordings from a limited set of devices, may not generalize well to samples recorded using other devices with different frequency responses. In this work, a relatively straightforward method is introduced to address this problem. Two variants of the approach are presented. First requires aligned examples from multiple devices, the second approach alleviates this requirement. This method works for both time and frequency domain representations of audio recordings. Further, a relation to standardization and Cepstral Mean Subtraction is analysed. The proposed approach becomes effective even when very few examples are provided. This method was developed during the Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 challenge and won the 1st place in the scenario with mis-matched recording devices with the accuracy of 75%. Source code for the experiments can be found online.

</p>
</details>

<details><summary><b>RNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral Features</b>
<a href="https://arxiv.org/abs/2105.11813">arxiv:2105.11813</a>
&#x1F4C8; 8 <br>
<p>Constantine C. Doumanidis, Christina Anagnostou, Evangelia-Sofia Arvaniti, Anthi Papadopoulou</p></summary>
<p>

**Abstract:** Recent interest in exploiting Deep Learning techniques for Noise Suppression, has led to the creation of Hybrid Denoising Systems that combine classic Signal Processing with Deep Learning. In this paper, we concentrated our efforts on extending the RNNoise denoising system (arXiv:1709.08243) with the inclusion of complementary features during the training phase. We present a comprehensive explanation of the set-up process of a modified system and present the comparative results derived from a performance evaluation analysis, using a reference version of RNNoise as control.

</p>
</details>

<details><summary><b>Affine Transport for Sim-to-Real Domain Adaptation</b>
<a href="https://arxiv.org/abs/2105.11739">arxiv:2105.11739</a>
&#x1F4C8; 8 <br>
<p>Anton Mallasto, Karol Arndt, Markus Heinonen, Samuel Kaski, Ville Kyrki</p></summary>
<p>

**Abstract:** Sample-efficient domain adaptation is an open problem in robotics. In this paper, we present affine transport -- a variant of optimal transport, which models the mapping between state transition distributions between the source and target domains with an affine transformation. First, we derive the affine transport framework; then, we extend the basic framework with Procrustes alignment to model arbitrary affine transformations. We evaluate the method in a number of OpenAI Gym sim-to-sim experiments with simulation environments, as well as on a sim-to-real domain adaptation task of a robot hitting a hockeypuck such that it slides and stops at a target position. In each experiment, we evaluate the results when transferring between each pair of dynamics domains. The results show that affine transport can significantly reduce the model adaptation error in comparison to using the original, non-adapted dynamics model.

</p>
</details>

<details><summary><b>Exploring Autoencoder-based Error-bounded Compression for Scientific Data</b>
<a href="https://arxiv.org/abs/2105.11730">arxiv:2105.11730</a>
&#x1F4C8; 8 <br>
<p>Jinyang Liu, Sheng Di, Kai Zhao, Sian Jin, Dingwen Tao, Xin Liang, Zizhong Chen, Franck Cappello</p></summary>
<p>

**Abstract:** Error-bounded lossy compression is becoming an indispensable technique for the success of today's scientific projects with vast volumes of data produced during the simulations or instrument data acquisitions. Not only can it significantly reduce data size, but it also can control the compression errors based on user-specified error bounds. Autoencoder (AE) models have been widely used in image compression, but few AE-based compression approaches support error-bounding features, which are highly required by scientific applications. To address this issue, we explore using convolutional autoencoders to improve error-bounded lossy compression for scientific data, with the following three key contributions. (1) We provide an in-depth investigation of the characteristics of various autoencoder models and develop an error-bounded autoencoder-based framework in terms of the SZ model. (2) We optimize the compression quality for main stages in our designed AE-based error-bounded compression framework, fine-tuning the block sizes and latent sizes and also optimizing the compression efficiency of latent vectors. (3) We evaluate our proposed solution using five real-world scientific datasets and comparing them with six other related works. Experiments show that our solution exhibits a very competitive compression quality from among all the compressors in our tests. In absolute terms, it can obtain a much better compression quality (100% ~ 800% improvement in compression ratio with the same data distortion) compared with SZ2.1 and ZFP in cases with a high compression ratio.

</p>
</details>

<details><summary><b>SHAFF: Fast and consistent SHApley eFfect estimates via random Forests</b>
<a href="https://arxiv.org/abs/2105.11724">arxiv:2105.11724</a>
&#x1F4C8; 8 <br>
<p>Clément Bénard, Gérard Biau, Sébastien da Veiga, Erwan Scornet</p></summary>
<p>

**Abstract:** Interpretability of learning algorithms is crucial for applications involving critical decisions, and variable importance is one of the main interpretation tools. Shapley effects are now widely used to interpret both tree ensembles and neural networks, as they can efficiently handle dependence and interactions in the data, as opposed to most other variable importance measures. However, estimating Shapley effects is a challenging task, because of the computational complexity and the conditional expectation estimates. Accordingly, existing Shapley algorithms have flaws: a costly running time, or a bias when input variables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via random Forests, a fast and accurate Shapley effect estimate, even when input variables are dependent. We show SHAFF efficiency through both a theoretical analysis of its consistency, and the practical performance improvements over competitors with extensive experiments. An implementation of SHAFF in C++ and R is available online.

</p>
</details>

<details><summary><b>Provable Representation Learning for Imitation with Contrastive Fourier Features</b>
<a href="https://arxiv.org/abs/2105.12272">arxiv:2105.12272</a>
&#x1F4C8; 7 <br>
<p>Ofir Nachum, Mengjiao Yang</p></summary>
<p>

**Abstract:** In imitation learning, it is common to learn a behavior policy to match an unknown target policy via max-likelihood training on a collected set of target demonstrations. In this work, we consider using offline experience datasets - potentially far from the target distribution - to learn low-dimensional state representations that provably accelerate the sample-efficiency of downstream imitation learning. A central challenge in this setting is that the unknown target policy itself may not exhibit low-dimensional behavior, and so there is a potential for the representation learning objective to alias states in which the target policy acts differently. Circumventing this challenge, we derive a representation learning objective that provides an upper bound on the performance difference between the target policy and a lowdimensional policy trained with max-likelihood, and this bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. Moving to the practicality of our method, we show that our objective can be implemented as contrastive learning, in which the transition dynamics are approximated by either an implicit energy-based model or, in some special cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical benefits of our proposed objective.

</p>
</details>

<details><summary><b>Investigating Manifold Neighborhood size for Nonlinear Analysis of LIBS Amino Acid Spectra</b>
<a href="https://arxiv.org/abs/2105.12089">arxiv:2105.12089</a>
&#x1F4C8; 7 <br>
<p>Piyush K. Sharma, Gary Holness, Poopalasingam Sivakumar, Yuri Markushin, Noureddine Melikechi</p></summary>
<p>

**Abstract:** Classification and identification of amino acids in aqueous solutions is important in the study of biomacromolecules. Laser Induced Breakdown Spectroscopy (LIBS) uses high energy laser-pulses for ablation of chemical compounds whose radiated spectra are captured and recorded to reveal molecular structure. Spectral peaks and noise from LIBS are impacted by experimental protocols. Current methods for LIBS spectral analysis achieves promising results using PCA, a linear method. It is well-known that the underlying physical processes behind LIBS are highly nonlinear. Our work set out to understand the impact of LIBS spectra on suitable neighborhood size over which to consider pattern phenomena, if nonlinear methods capture pattern phenomena with increased efficacy, and how they improve classification and identification of compounds. We analyzed four amino acids, polysaccharide, and a control group, water. We developed an information theoretic method for measurement of LIBS energy spectra, implemented manifold methods for nonlinear dimensionality reduction, and found while clustering results were not statistically significantly different, nonlinear methods lead to increased classification accuracy. Moreover, our approach uncovered the contribution of micro-wells (experimental protocol) in LIBS spectra. To the best of our knowledge, ours is the first application of Manifold methods to LIBS amino-acid analysis in the research literature.

</p>
</details>

<details><summary><b>Graph Based Link Prediction between Human Phenotypes and Genes</b>
<a href="https://arxiv.org/abs/2105.11989">arxiv:2105.11989</a>
&#x1F4C8; 7 <br>
<p>Rushabh Patel, Yanhui Guo</p></summary>
<p>

**Abstract:** Background: The learning of genotype-phenotype associations and history of human disease by doing detailed and precise analysis of phenotypic abnormalities can be defined as deep phenotyping. To understand and detect this interaction between phenotype and genotype is a fundamental step when translating precision medicine to clinical practice. The recent advances in the field of machine learning is efficient to predict these interactions between abnormal human phenotypes and genes.
  Methods: In this study, we developed a framework to predict links between human phenotype ontology (HPO) and genes. The annotation data from the heterogeneous knowledge resources i.e., orphanet, is used to parse human phenotype-gene associations. To generate the embeddings for the nodes (HPO & genes), an algorithm called node2vec was used. It performs node sampling on this graph based on random walks, then learns features over these sampled nodes to generate embeddings. These embeddings were used to perform the downstream task to predict the presence of the link between these nodes using 5 different supervised machine learning algorithms.
  Results: The downstream link prediction task shows that the Gradient Boosting Decision Tree based model (LightGBM) achieved an optimal AUROC 0.904 and AUCPR 0.784. In addition, LightGBM achieved an optimal weighted F1 score of 0.87. Compared to the other 4 methods LightGBM is able to find more accurate interaction/link between human phenotype & gene pairs.

</p>
</details>

<details><summary><b>GraphFM: Graph Factorization Machines for Feature Interaction Modeling</b>
<a href="https://arxiv.org/abs/2105.11866">arxiv:2105.11866</a>
&#x1F4C8; 7 <br>
<p>Zekun Li, Shu Wu, Zeyu Cui, Xiaoyu Zhang</p></summary>
<p>

**Abstract:** Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion, on the other hand, taking into account interaction between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach Graph Factorization Machine (GraphFM) by naturally representing features in the graph structure. In particular, a novel mechanism is designed to select the beneficial feature interactions and formulate them as edges between features. Then our proposed model which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experimental results on several real-world datasets has demonstrated the rationality and effectiveness of our proposed approach.

</p>
</details>

<details><summary><b>Self-Adaptive Swarm System (SASS)</b>
<a href="https://arxiv.org/abs/2106.04679">arxiv:2106.04679</a>
&#x1F4C8; 6 <br>
<p>Qin Yang</p></summary>
<p>

**Abstract:** Distributed artificial intelligence (DAI) studies artificial intelligence entities working together to reason, plan, solve problems, organize behaviors and strategies, make collective decisions and learn. This Ph.D. research proposes a principled Multi-Agent Systems (MAS) cooperation framework -- Self-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap between perception, communication, planning, execution, decision-making, and learning.

</p>
</details>

<details><summary><b>Neural Radiosity</b>
<a href="https://arxiv.org/abs/2105.12319">arxiv:2105.12319</a>
&#x1F4C8; 6 <br>
<p>Saeed Hadadan, Shuhong Chen, Matthias Zwicker</p></summary>
<p>

**Abstract:** We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.

</p>
</details>

<details><summary><b>Context-Sensitive Visualization of Deep Learning Natural Language Processing Models</b>
<a href="https://arxiv.org/abs/2105.12202">arxiv:2105.12202</a>
&#x1F4C8; 6 <br>
<p>Andrew Dunn, Diana Inkpen, Răzvan Andonie</p></summary>
<p>

**Abstract:** The introduction of Transformer neural networks has changed the landscape of Natural Language Processing (NLP) during the last years. So far, none of the visualization systems has yet managed to examine all the facets of the Transformers. This gave us the motivation of the current work. We propose a new NLP Transformer context-sensitive visualization method that leverages existing NLP tools to find the most significant groups of tokens (words) that have the greatest effect on the output, thus preserving some context from the original text. First, we use a sentence-level dependency parser to highlight promising word groups. The dependency parser creates a tree of relationships between the words in the sentence. Next, we systematically remove adjacent and non-adjacent tuples of \emph{n} tokens from the input text, producing several new texts with those tokens missing. The resulting texts are then passed to a pre-trained BERT model. The classification output is compared with that of the full text, and the difference in the activation strength is recorded. The modified texts that produce the largest difference in the target classification output neuron are selected, and the combination of removed words are then considered to be the most influential on the model's output. Finally, the most influential word combinations are visualized in a heatmap.

</p>
</details>

<details><summary><b>Towards Teachable Autonomous Agents</b>
<a href="https://arxiv.org/abs/2105.11977">arxiv:2105.11977</a>
&#x1F4C8; 6 <br>
<p>Olivier Sigaud, Hugo Caselles-Dupré, Cédric Colas, Ahmed Akakzia, Pierre-Yves Oudeyer, Mohamed Chetouani</p></summary>
<p>

**Abstract:** Autonomous discovery and direct instruction are two extreme sources of learning in children, but educational sciences have shown that intermediate approaches such as assisted discovery or guided play resulted in better acquisition of skills. When turning to Artificial Intelligence, the above dichotomy is translated into the distinction between autonomous agents which learn in isolation and interactive learning agents which can be taught by social partners but generally lack autonomy. In between should stand teachable autonomous agents: agents learning from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Such agents could learn on their own in the real world, but non-expert users could drive their learning behavior towards their expectations. More fundamentally, combining both capabilities might also be a key step towards general intelligence. In this paper we elucidate obstacles along this research line. First, we build on a seminal work of Bruner to extract relevant features of the assisted discovery processes. Second, we describe current research on autotelic agents, i.e. agents equipped with forms of intrinsic motivations that enable them to represent, self-generate and pursue their own goals. We argue that autotelic capabilities are paving the way towards teachable and autonomous agents. Finally, we adopt a social learning perspective on tutoring interactions and we highlight some components that are currently missing to autotelic agents before they can be taught by ordinary people using natural pedagogy, and we provide a list of specific research questions that emerge from this perspective.

</p>
</details>

<details><summary><b>Bias-Robust Bayesian Optimization via Dueling Bandits</b>
<a href="https://arxiv.org/abs/2105.11802">arxiv:2105.11802</a>
&#x1F4C8; 6 <br>
<p>Johannes Kirschner, Andreas Krause</p></summary>
<p>

**Abstract:** We consider Bayesian optimization in settings where observations can be adversarially biased, for example by an uncontrolled hidden confounder. Our first contribution is a reduction of the confounded setting to the dueling bandit model. Then we propose a novel approach for dueling bandits based on information-directed sampling (IDS). Thereby, we obtain the first efficient kernelized algorithm for dueling bandits that comes with cumulative regret guarantees. Our analysis further generalizes a previously proposed semi-parametric linear bandit model to non-linear reward functions, and uncovers interesting links to doubly-robust estimation.

</p>
</details>

<details><summary><b>Transfer Learning and Curriculum Learning in Sokoban</b>
<a href="https://arxiv.org/abs/2105.11702">arxiv:2105.11702</a>
&#x1F4C8; 6 <br>
<p>Zhao Yang, Mike Preuss, Aske Plaat</p></summary>
<p>

**Abstract:** Transfer learning can speed up training in machine learning and is regularly used in classification tasks. It reuses prior knowledge from other tasks to pre-train networks for new tasks. In reinforcement learning, learning actions for a behavior policy that can be applied to new environments is still a challenge, especially for tasks that involve much planning. Sokoban is a challenging puzzle game. It has been used widely as a benchmark in planning-based reinforcement learning. In this paper, we show how prior knowledge improves learning in Sokoban tasks. We find that reusing feature representations learned previously can accelerate learning new, more complex, instances. In effect, we show how curriculum learning, from simple to complex tasks, works in Sokoban. Furthermore, feature representations learned in simpler instances are more general, and thus lead to positive transfers towards more complex tasks, but not vice versa. We have also studied which part of the knowledge is most important for transfer to succeed, and identify which layers should be used for pre-training.

</p>
</details>

<details><summary><b>Unbiased Asymmetric Actor-Critic for Partially Observable Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.11674">arxiv:2105.11674</a>
&#x1F4C8; 6 <br>
<p>Andrea Baisero, Christopher Amato</p></summary>
<p>

**Abstract:** In partially observable reinforcement learning, offline training gives access to latent information which is not available during online training and/or execution, such as the system state. Asymmetric actor-critic methods exploit such information by training a history-based policy via a state-based critic. However, many asymmetric methods lack theoretical foundation, and are only evaluated on limited domains. We examine the theory of asymmetric actor-critic methods which use state-based critics, and expose fundamental issues which undermine the validity of a common variant, and its ability to address high partial observability. We propose an unbiased asymmetric actor-critic variant which is able to exploit state information while remaining theoretically sound, maintaining the validity of the policy gradient theorem, and introducing no bias and relatively low variance into the training process. An empirical evaluation performed on domains which exhibit significant partial observability confirms our analysis, and shows the unbiased asymmetric actor-critic converges to better policies and/or faster than symmetric actor-critic and standard asymmetric actor-critic baselines.

</p>
</details>

<details><summary><b>Graph Neural Network Based VC Investment Success Prediction</b>
<a href="https://arxiv.org/abs/2105.11537">arxiv:2105.11537</a>
&#x1F4C8; 6 <br>
<p>Shiwei Lyu, Shuai Ling, Kaihao Guo, Haipeng Zhang, Kunpeng Zhang, Suting Hong, Qing Ke, Jinjie Gu</p></summary>
<p>

**Abstract:** Predicting the start-ups that will eventually succeed is essentially important for the venture capital business and worldwide policy makers, especially at an early stage such that rewards can possibly be exponential.
  Though various empirical studies and data-driven modeling work have been done, the predictive power of the complex networks of stakeholders including venture capital investors, start-ups, and start-ups' managing members has not been thoroughly explored. We design an incremental representation learning mechanism and a sequential learning model, utilizing the network structure together with the rich attributes of the nodes. In general, our method achieves the state-of-the-art prediction performance on a comprehensive dataset of global venture capital investments and surpasses human investors by large margins. Specifically, it excels at predicting the outcomes for start-ups in industries such as healthcare and IT. Meanwhile, we shed light on impacts on start-up success from observable factors including gender, education, and networking, which can be of value for practitioners as well as policy makers when they screen ventures of high growth potentials.

</p>
</details>

<details><summary><b>Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling</b>
<a href="https://arxiv.org/abs/2106.07343">arxiv:2106.07343</a>
&#x1F4C8; 5 <br>
<p>Yutai Hou, Yongkui Lai, Cheng Chen, Wanxiang Che, Ting Liu</p></summary>
<p>

**Abstract:** In this paper, we investigate few-shot joint learning for dialogue language understanding. Most existing few-shot models learn a single task each time with only a few examples. However, dialogue language understanding contains two closely related tasks, i.e., intent detection and slot filling, and often benefits from jointly learning the two tasks. This calls for new few-shot learning techniques that are able to capture task relations from only a few examples and jointly learn multiple tasks. To achieve this, we propose a similarity-based few-shot learning scheme, named Contrastive Prototype Merging network (ConProm), that learns to bridge metric spaces of intent and slot on data-rich domains, and then adapt the bridged metric space to the specific few-shot domain. Experiments on two public datasets, Snips and FewJoint, show that our model significantly outperforms the strong baselines in one and five shots settings.

</p>
</details>

<details><summary><b>Block Dense Weighted Networks with Augmented Degree Correction</b>
<a href="https://arxiv.org/abs/2105.12290">arxiv:2105.12290</a>
&#x1F4C8; 5 <br>
<p>Benjamin Leinwand, Vladas Pipiras</p></summary>
<p>

**Abstract:** Dense networks with weighted connections often exhibit a community like structure, where although most nodes are connected to each other, different patterns of edge weights may emerge depending on each node's community membership. We propose a new framework for generating and estimating dense weighted networks with potentially different connectivity patterns across different communities. The proposed model relies on a particular class of functions which map individual node characteristics to the edges connecting those nodes, allowing for flexibility while requiring a small number of parameters relative to the number of edges. By leveraging the estimation techniques, we also develop a bootstrap methodology for generating new networks on the same set of vertices, which may be useful in circumstances where multiple data sets cannot be collected. Performance of these methods are analyzed in theory, simulations, and real data.

</p>
</details>

<details><summary><b>Bridging the Gap Between Explainable AI and Uncertainty Quantification to Enhance Trustability</b>
<a href="https://arxiv.org/abs/2105.11828">arxiv:2105.11828</a>
&#x1F4C8; 5 <br>
<p>Dominik Seuß</p></summary>
<p>

**Abstract:** After the tremendous advances of deep learning and other AI methods, more attention is flowing into other properties of modern approaches, such as interpretability, fairness, etc. combined in frameworks like Responsible AI. Two research directions, namely Explainable AI and Uncertainty Quantification are becoming more and more important, but have been so far never combined and jointly explored. In this paper, I show how both research areas provide potential for combination, why more research should be done in this direction and how this would lead to an increase in trustability in AI systems.

</p>
</details>

<details><summary><b>Speed Benchmarking of Genetic Programming Frameworks</b>
<a href="https://arxiv.org/abs/2106.11919">arxiv:2106.11919</a>
&#x1F4C8; 4 <br>
<p>Francisco Baeta, João Correia, Tiago Martins, Penousal Machado</p></summary>
<p>

**Abstract:** Genetic Programming (GP) is known to suffer from the burden of being computationally expensive by design. While, over the years, many techniques have been developed to mitigate this issue, data vectorization, in particular, is arguably still the most attractive strategy due to the parallel nature of GP. In this work, we employ a series of benchmarks meant to compare both the performance and evolution capabilities of different vectorized and iterative implementation approaches across several existing frameworks. Namely, TensorGP, a novel open-source engine written in Python, is shown to greatly benefit from the TensorFlow library to accelerate the domain evaluation phase in GP. The presented performance benchmarks demonstrate that the TensorGP engine manages to pull ahead, with relative speedups above two orders of magnitude for problems with a higher number of fitness cases. Additionally, as a consequence of being able to compute larger domains, we argue that TensorGP performance gains aid the discovery of more accurate candidate solutions.

</p>
</details>

<details><summary><b>Occlusion Aware Kernel Correlation Filter Tracker using RGB-D</b>
<a href="https://arxiv.org/abs/2105.12161">arxiv:2105.12161</a>
&#x1F4C8; 4 <br>
<p>Srishti Yadav</p></summary>
<p>

**Abstract:** Unlike deep learning which requires large training datasets, correlation filter-based trackers like Kernelized Correlation Filter (KCF) uses implicit properties of tracked images (circulant matrices) for training in real-time. Despite their practical application in tracking, a need for a better understanding of the fundamentals associated with KCF in terms of theoretically, mathematically, and experimentally exists. This thesis first details the workings prototype of the tracker and investigates its effectiveness in real-time applications and supporting visualizations. We further address some of the drawbacks of the tracker in cases of occlusions, scale changes, object rotation, out-of-view and model drift with our novel RGB-D Kernel Correlation tracker. We also study the use of particle filters to improve trackers' accuracy. Our results are experimentally evaluated using a) standard dataset and b) real-time using the Microsoft Kinect V2 sensor. We believe this work will set the basis for a better understanding of the effectiveness of kernel-based correlation filter trackers and to further define some of its possible advantages in tracking.

</p>
</details>

<details><summary><b>Security in Next Generation Mobile Payment Systems: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2105.12097">arxiv:2105.12097</a>
&#x1F4C8; 4 <br>
<p>Waqas Ahmed, Amir Rasool, Neeraj Kumar, Abdul RehmanJaved, Thippa Reddy Gadekallu, Zunera Jalil, Natalia Kryvinska</p></summary>
<p>

**Abstract:** Cash payment is still king in several markets, accounting for more than 90\ of the payments in almost all the developing countries. The usage of mobile phones is pretty ordinary in this present era. Mobile phones have become an inseparable friend for many users, serving much more than just communication tools. Every subsequent person is heavily relying on them due to multifaceted usage and affordability. Every person wants to manage his/her daily transactions and related issues by using his/her mobile phone. With the rise and advancements of mobile-specific security, threats are evolving as well. In this paper, we provide a survey of various security models for mobile phones. We explore multiple proposed models of the mobile payment system (MPS), their technologies and comparisons, payment methods, different security mechanisms involved in MPS, and provide analysis of the encryption technologies, authentication methods, and firewall in MPS. We also present current challenges and future directions of mobile phone security.

</p>
</details>

<details><summary><b>DTNN: Energy-efficient Inference with Dendrite Tree Inspired Neural Networks for Edge Vision Applications</b>
<a href="https://arxiv.org/abs/2105.11848">arxiv:2105.11848</a>
&#x1F4C8; 4 <br>
<p>Tao Luo, Wai Teng Tang, Matthew Kay Fei Lee, Chuping Qu, Weng-Fai Wong, Rick Goh</p></summary>
<p>

**Abstract:** Deep neural networks (DNN) have achieved remarkable success in computer vision (CV). However, training and inference of DNN models are both memory and computation intensive, incurring significant overhead in terms of energy consumption and silicon area. In particular, inference is much more cost-sensitive than training because training can be done offline with powerful platforms, while inference may have to be done on battery powered devices with constrained form factors, especially for mobile or edge vision applications. In order to accelerate DNN inference, model quantization was proposed. However previous works only focus on the quantization rate without considering the efficiency of operations. In this paper, we propose Dendrite-Tree based Neural Network (DTNN) for energy-efficient inference with table lookup operations enabled by activation quantization. In DTNN both costly weight access and arithmetic computations are eliminated for inference. We conducted experiments on various kinds of DNN models such as LeNet-5, MobileNet, VGG, and ResNet with different datasets, including MNIST, Cifar10/Cifar100, SVHN, and ImageNet. DTNN achieved significant energy saving (19.4X and 64.9X improvement on ResNet-18 and VGG-11 with ImageNet, respectively) with negligible loss of accuracy. To further validate the effectiveness of DTNN and compare with state-of-the-art low energy implementation for edge vision, we design and implement DTNN based MLP image classifiers using off-the-shelf FPGAs. The results show that DTNN on the FPGA, with higher accuracy, could achieve orders of magnitude better energy consumption and latency compared with the state-of-the-art low energy approaches reported that use ASIC chips.

</p>
</details>

<details><summary><b>Training Speech Enhancement Systems with Noisy Speech Datasets</b>
<a href="https://arxiv.org/abs/2105.12315">arxiv:2105.12315</a>
&#x1F4C8; 3 <br>
<p>Koichi Saito, Stefan Uhlich, Giorgio Fabbro, Yuki Mitsufuji</p></summary>
<p>

**Abstract:** Recently, deep neural network (DNN)-based speech enhancement (SE) systems have been used with great success. During training, such systems require clean speech data - ideally, in large quantity with a variety of acoustic conditions, many different speaker characteristics and for a given sampling rate (e.g., 48kHz for fullband SE). However, obtaining such clean speech data is not straightforward - especially, if only considering publicly available datasets. At the same time, a lot of material for automatic speech recognition (ASR) with the desired acoustic/speaker/sampling rate characteristics is publicly available except being clean, i.e., it also contains background noise as this is even often desired in order to have ASR systems that are noise-robust. Hence, using such data to train SE systems is not straightforward. In this paper, we propose two improvements to train SE systems on noisy speech data. First, we propose several modifications of the loss functions, which make them robust against noisy speech targets. In particular, computing the median over the sample axis before averaging over time-frequency bins allows to use such data. Furthermore, we propose a noise augmentation scheme for mixture-invariant training (MixIT), which allows using it also in such scenarios. For our experiments, we use the Mozilla Common Voice dataset and we show that using our robust loss function improves PESQ by up to 0.19 compared to a system trained in the traditional way. Similarly, for MixIT we can see an improvement of up to 0.27 in PESQ when using our proposed noise augmentation.

</p>
</details>

<details><summary><b>Optimal Transport Based Refinement of Physics-Informed Neural Networks</b>
<a href="https://arxiv.org/abs/2105.12307">arxiv:2105.12307</a>
&#x1F4C8; 3 <br>
<p>Vaishnav Tadiparthi, Raktim Bhattacharya</p></summary>
<p>

**Abstract:** In this paper, we propose a refinement strategy to the well-known Physics-Informed Neural Networks (PINNs) for solving partial differential equations (PDEs) based on the concept of Optimal Transport (OT).
  Conventional black-box PINNs solvers have been found to suffer from a host of issues: spectral bias in fully-connected architectures, unstable gradient pathologies, as well as difficulties with convergence and accuracy.
  Current network training strategies are agnostic to dimension sizes and rely on the availability of powerful computing resources to optimize through a large number of collocation points.
  This is particularly challenging when studying stochastic dynamical systems with the Fokker-Planck-Kolmogorov Equation (FPKE), a second-order PDE which is typically solved in high-dimensional state space.
  While we focus exclusively on the stationary form of the FPKE, positivity and normalization constraints on its solution make it all the more unfavorable to solve directly using standard PINNs approaches.
  To mitigate the above challenges, we present a novel training strategy for solving the FPKE using OT-based sampling to supplement the existing PINNs framework.
  It is an iterative approach that induces a network trained on a small dataset to add samples to its training dataset from regions where it nominally makes the most error.
  The new samples are found by solving a linear programming problem at every iteration.
  The paper is complemented by an experimental evaluation of the proposed method showing its applicability on a variety of stochastic systems with nonlinear dynamics.

</p>
</details>

<details><summary><b>SG-PALM: a Fast Physically Interpretable Tensor Graphical Model</b>
<a href="https://arxiv.org/abs/2105.12271">arxiv:2105.12271</a>
&#x1F4C8; 3 <br>
<p>Yu Wang, Alfred Hero</p></summary>
<p>

**Abstract:** We propose a new graphical model inference procedure, called SG-PALM, for learning conditional dependency structure of high-dimensional tensor-variate data. Unlike most other tensor graphical models the proposed model is interpretable and computationally scalable to high dimension. Physical interpretability follows from the Sylvester generative (SG) model on which SG-PALM is based: the model is exact for any observation process that is a solution of a partial differential equation of Poisson type. Scalability follows from the fast proximal alternating linearized minimization (PALM) procedure that SG-PALM uses during training. We establish that SG-PALM converges linearly (i.e., geometric convergence rate) to a global optimum of its objective function. We demonstrate the scalability and accuracy of SG-PALM for an important but challenging climate prediction problem: spatio-temporal forecasting of solar flares from multimodal imaging data.

</p>
</details>

<details><summary><b>Style Similarity as Feedback for Product Design</b>
<a href="https://arxiv.org/abs/2105.12256">arxiv:2105.12256</a>
&#x1F4C8; 3 <br>
<p>Mathew Schwartz, Tomer Weiss, Esra Ataer-Cansizoglu, Jae-Woo Choi</p></summary>
<p>

**Abstract:** Matching and recommending products is beneficial for both customers and companies. With the rapid increase in home goods e-commerce, there is an increasing demand for quantitative methods for providing such recommendations for millions of products. This approach is facilitated largely by online stores such as Amazon and Wayfair, in which the goal is to maximize overall sales. Instead of focusing on overall sales, we take a product design perspective, by employing big-data analysis for determining the design qualities of a highly recommended product. Specifically, we focus on the visual style compatibility of such products. We build off previous work which implemented a style-based similarity metric for thousands of furniture products. Using analysis and visualization, we extract attributes of furniture products that are highly compatible style-wise. We propose a designer in-the-loop workflow that mirrors methods of displaying similar products to consumers browsing e-commerce websites. Our findings are useful when designing new products, since they provide insight regarding what furniture will be strongly compatible across multiple styles, and hence, more likely to be recommended.

</p>
</details>

<details><summary><b>Interpretable UAV Collision Avoidance using Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.12254">arxiv:2105.12254</a>
&#x1F4C8; 3 <br>
<p>Deepak-George Thomas, Daniil Olshanskyi, Karter Krueger, Tichakorn Wongpiromsarn, Ali Jannesari</p></summary>
<p>

**Abstract:** The significant components of any successful autonomous flight system are task completion and collision avoidance. Most deep learning algorithms successfully execute these aspects under the environment and conditions they are trained. However, they fail when subjected to novel environments. This paper presents an autonomous multi-rotor flight algorithm, using Deep Reinforcement Learning augmented with Self-Attention Models, that can effectively reason when subjected to varying inputs. In addition to their reasoning ability, they are also interpretable, enabling it to be used under real-world conditions. We have tested our algorithm under different weather conditions and environments and found it robust compared to conventional Deep Reinforcement Learning algorithms.

</p>
</details>

<details><summary><b>Practical Convex Formulation of Robust One-hidden-layer Neural Network Training</b>
<a href="https://arxiv.org/abs/2105.12237">arxiv:2105.12237</a>
&#x1F4C8; 3 <br>
<p>Yatong Bai, Tanmay Gautam, Yu Gai, Somayeh Sojoudi</p></summary>
<p>

**Abstract:** Recent work has shown that the training of a one-hidden-layer, scalar-output fully-connected ReLU neural network can be reformulated as a finite-dimensional convex program. Unfortunately, the scale of such a convex program grows exponentially in data size. In this work, we prove that a stochastic procedure with a linear complexity well approximates the exact formulation. Moreover, we derive a convex optimization approach to efficiently solve the "adversarial training" problem, which trains neural networks that are robust to adversarial input perturbations. Our method can be applied to binary classification and regression, and provides an alternative to the current adversarial training methods, such as Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). We demonstrate in experiments that the proposed method achieves a noticeably better adversarial robustness and performance than the existing methods.

</p>
</details>

<details><summary><b>Rank-one matrix estimation: analytic time evolution of gradient descent dynamics</b>
<a href="https://arxiv.org/abs/2105.12257">arxiv:2105.12257</a>
&#x1F4C8; 2 <br>
<p>Antoine Bodin, Nicolas Macris</p></summary>
<p>

**Abstract:** We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an $n$-component unknown vector on the sphere of radius $\sqrt{n}$, and we consider the problem of estimating this vector from the corrupted matrix in the high dimensional limit of $n$ large, by gradient descent for a quadratic cost function on the sphere. Explicit formulas for the whole time evolution of the overlap between the estimator and unknown vector, as well as the cost, are rigorously derived. In the long time limit we recover the well known spectral phase transition, as a function of the signal-to-noise ratio. The explicit formulas also allow to point out interesting transient features of the time evolution. Our analysis technique is based on recent progress in random matrix theory and uses local versions of the semi-circle law.

</p>
</details>

<details><summary><b>Safe Value Functions</b>
<a href="https://arxiv.org/abs/2105.12204">arxiv:2105.12204</a>
&#x1F4C8; 2 <br>
<p>Pierre-François Massiani, Steve Heim, Friedrich Solowjow, Sebastian Trimpe</p></summary>
<p>

**Abstract:** Safety constraints and optimality are important, but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions: value functions that are both optimal for a given task, and enforce safety constraints. We reveal the structure of this relationship through a proof of strong duality, showing that there always exists a finite penalty that induces a safe value function. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.

</p>
</details>

<details><summary><b>Self-Organized Variational Autoencoders (Self-VAE) for Learned Image Compression</b>
<a href="https://arxiv.org/abs/2105.12107">arxiv:2105.12107</a>
&#x1F4C8; 2 <br>
<p>M. Akın Yılmaz, Onur Keleş, Hilal Güven, A. Murat Tekalp, Junaid Malik, Serkan Kıranyaz</p></summary>
<p>

**Abstract:** In end-to-end optimized learned image compression, it is standard practice to use a convolutional variational autoencoder with generalized divisive normalization (GDN) to transform images into a latent space. Recently, Operational Neural Networks (ONNs) that learn the best non-linearity from a set of alternatives, and their self-organized variants, Self-ONNs, that approximate any non-linearity via Taylor series have been proposed to address the limitations of convolutional layers and a fixed nonlinear activation. In this paper, we propose to replace the convolutional and GDN layers in the variational autoencoder with self-organized operational layers, and propose a novel self-organized variational autoencoder (Self-VAE) architecture that benefits from stronger non-linearity. The experimental results demonstrate that the proposed Self-VAE yields improvements in both rate-distortion performance and perceptual image quality.

</p>
</details>

<details><summary><b>Adversarial Attack Driven Data Augmentation for Accurate And Robust Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2105.12106">arxiv:2105.12106</a>
&#x1F4C8; 2 <br>
<p>Mst. Tasnim Pervin, Linmi Tao, Aminul Huq, Zuoxiang He, Li Huo</p></summary>
<p>

**Abstract:** Segmentation is considered to be a very crucial task in medical image analysis. This task has been easier since deep learning models have taken over with its high performing behavior. However, deep learning models dependency on large data proves it to be an obstacle in medical image analysis because of insufficient data samples. Several data augmentation techniques have been used to mitigate this problem. We propose a new augmentation method by introducing adversarial learning attack techniques, specifically Fast Gradient Sign Method (FGSM). Furthermore, We have also introduced the concept of Inverse FGSM (InvFGSM), which works in the opposite manner of FGSM for the data augmentation. This two approaches worked together to improve the segmentation accuracy, as well as helped the model to gain robustness against adversarial attacks. The overall analysis of experiments indicates a novel use of adversarial machine learning along with robustness enhancement.

</p>
</details>

<details><summary><b>NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling Answers into Question for Matching Reading Comprehension</b>
<a href="https://arxiv.org/abs/2105.12051">arxiv:2105.12051</a>
&#x1F4C8; 2 <br>
<p>Zhixiang Chen, Yikun Lei, Pai Liu, Guibing Guo</p></summary>
<p>

**Abstract:** SemEval task 4 aims to find a proper option from multiple candidates to resolve the task of machine reading comprehension. Most existing approaches propose to concat question and option together to form a context-aware model. However, we argue that straightforward concatenation can only provide a coarse-grained context for the MRC task, ignoring the specific positions of the option relative to the question. In this paper, we propose a novel MRC model by filling options into the question to produce a fine-grained context (defined as summary) which can better reveal the relationship between option and question. We conduct a series of experiments on the given dataset, and the results show that our approach outperforms other counterparts to a large extent.

</p>
</details>

<details><summary><b>An Integrated Dynamic Method for Allocating Roles and Planning Tasks for Mixed Human-Robot Teams</b>
<a href="https://arxiv.org/abs/2105.12031">arxiv:2105.12031</a>
&#x1F4C8; 2 <br>
<p>Fabio Fusaro, Edoardo Lamon, Elena De Momi, Arash Ajoudani</p></summary>
<p>

**Abstract:** This paper proposes a novel integrated dynamic method based on Behavior Trees for planning and allocating tasks in mixed human robot teams, suitable for manufacturing environments. The Behavior Tree formulation allows encoding a single job as a compound of different tasks with temporal and logic constraints. In this way, instead of the well-studied offline centralized optimization problem, the role allocation problem is solved with multiple simplified online optimization sub-problem, without complex and cross-schedule task dependencies. These sub-problems are defined as Mixed-Integer Linear Programs, that, according to the worker-actions related costs and the workers' availability, allocate the yet-to-execute tasks among the available workers. To characterize the behavior of the developed method, we opted to perform different simulation experiments in which the results of the action-worker allocation and computational complexity are evaluated. The obtained results, due to the nature of the algorithm and to the possibility of simulating the agents' behavior, should describe well also how the algorithm performs in real experiments.

</p>
</details>

<details><summary><b>Analogical discovery of disordered perovskite oxides by crystal structure information hidden in unsupervised material fingerprints</b>
<a href="https://arxiv.org/abs/2105.11877">arxiv:2105.11877</a>
&#x1F4C8; 2 <br>
<p>Achintha Ihalage, Yang Hao</p></summary>
<p>

**Abstract:** Compositional disorder induces myriad captivating phenomena in perovskites. Target-driven discovery of perovskite solid solutions has been a great challenge due to the analytical complexity introduced by disorder. Here, we demonstrate that an unsupervised deep learning strategy can find fingerprints of disordered materials that embed perovskite formability and underlying crystal structure information by learning only from the chemical composition, manifested in (A1-xA'x)BO3 and A(B1-xB'x)O3 formulae. This phenomenon can be capitalized to predict the crystal symmetry of experimental compositions, outperforming several supervised machine learning (ML) algorithms. The educated nature of material fingerprints has led to the conception of analogical materials discovery that facilitates inverse exploration of promising perovskites based on similarity investigation with known materials. The search space of unstudied perovskites is screened from ~600,000 feasible compounds using experimental data powered ML models and automated web mining tools at a 94% success rate. This concept further provides insights on possible phase transitions and computational modelling of complex compositions. The proposed quantitative analysis of materials analogies is expected to bridge the gap between the existing materials literature and the undiscovered terrain.

</p>
</details>

<details><summary><b>FENXI: Deep-learning Traffic Analytics at the Edge</b>
<a href="https://arxiv.org/abs/2105.11738">arxiv:2105.11738</a>
&#x1F4C8; 2 <br>
<p>Massimo Gallo, Alessandro Finamore, Gwendal Simon, Dario Rossi</p></summary>
<p>

**Abstract:** Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance the processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations.
  In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both adversarial and realistic network conditions. Our analysis shows that FENXI can sustain 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions.

</p>
</details>

<details><summary><b>Predicting Links on Wikipedia with Anchor Text Information</b>
<a href="https://arxiv.org/abs/2105.11734">arxiv:2105.11734</a>
&#x1F4C8; 2 <br>
<p>Robin Brochier, Frédéric Béchet</p></summary>
<p>

**Abstract:** Wikipedia, the largest open-collaborative online encyclopedia, is a corpus of documents bound together by internal hyperlinks. These links form the building blocks of a large network whose structure contains important information on the concepts covered in this encyclopedia. The presence of a link between two articles, materialised by an anchor text in the source page pointing to the target page, can increase readers' understanding of a topic. However, the process of linking follows specific editorial rules to avoid both under-linking and over-linking. In this paper, we study the transductive and the inductive tasks of link prediction on several subsets of the English Wikipedia and identify some key challenges behind automatic linking based on anchor text information. We propose an appropriate evaluation sampling methodology and compare several algorithms. Moreover, we propose baseline models that provide a good estimation of the overall difficulty of the tasks.

</p>
</details>

<details><summary><b>PyTorch, Explain! A Python library for Logic Explained Networks</b>
<a href="https://arxiv.org/abs/2105.11697">arxiv:2105.11697</a>
&#x1F4C8; 2 <br>
<p>Pietro Barbiero, Gabriele Ciravegna, Dobrik Georgiev, Franscesco Giannini</p></summary>
<p>

**Abstract:** "PyTorch, Explain!" is a Python module integrating a variety of state-of-the-art approaches to provide logic explanations from neural networks. This package focuses on bringing these methods to non-specialists. It has minimal dependencies and it is distributed under the Apache 2.0 licence allowing both academic and commercial use. Source code and documentation can be downloaded from the github repository: https://github.com/pietrobarbiero/pytorch_explain.

</p>
</details>

<details><summary><b>Gamers Private Network Performance Forecasting. From Raw Data to the Data Warehouse with Machine Learning and Neural Nets</b>
<a href="https://arxiv.org/abs/2107.00998">arxiv:2107.00998</a>
&#x1F4C8; 1 <br>
<p>Albert Wong, Chun Yin Chiu, Gaétan Hains, Jack Humphrey, Hans Fuhrmann, Youry Khmelevsky, Chris Mazur</p></summary>
<p>

**Abstract:** Gamers Private Network (GPN) is a client/server technology that guarantees a connection for online video games that is more reliable and lower latency than a standard internet connection. Users of the GPN technology benefit from a stable and high-quality gaming experience for online games, which are hosted and played across the world. After transforming a massive volume of raw networking data collected by WTFast, we have structured the cleaned data into a special-purpose data warehouse and completed the extensive analysis using machine learning and neural nets technologies, and business intelligence tools. These analyses demonstrate the ability to predict and quantify changes in the network and demonstrate the benefits gained from the use of a GPN for users when connected to an online game session.

</p>
</details>

<details><summary><b>Database Workload Characterization with Query Plan Encoders</b>
<a href="https://arxiv.org/abs/2105.12287">arxiv:2105.12287</a>
&#x1F4C8; 1 <br>
<p>Debjyoti Paul, Jie Cao, Feifei Li, Vivek Srikumar</p></summary>
<p>

**Abstract:** Smart databases are adopting artificial intelligence (AI) technologies to achieve {\em instance optimality}, and in the future, databases will come with prepackaged AI models within their core components. The reason is that every database runs on different workloads, demands specific resources, and settings to achieve optimal performance. It prompts the necessity to understand workloads running in the system along with their features comprehensively, which we dub as workload characterization.
  To address this workload characterization problem, we propose our query plan encoders that learn essential features and their correlations from query plans. Our pretrained encoders capture the {\em structural} and the {\em computational performance} of queries independently. We show that our pretrained encoders are adaptable to workloads that expedite the transfer learning process. We performed independent assessments of structural encoder and performance encoders with multiple downstream tasks. For the overall evaluation of our query plan encoders, we architect two downstream tasks (i) query latency prediction and (ii) query classification. These tasks show the importance of feature-based workload characterization. We also performed extensive experiments on individual encoders to verify the effectiveness of representation learning and domain adaptability.

</p>
</details>

<details><summary><b>A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering</b>
<a href="https://arxiv.org/abs/2105.12208">arxiv:2105.12208</a>
&#x1F4C8; 1 <br>
<p>Yu Qin, Brittany Terese Fasy, Carola Wenk, Brian Summa</p></summary>
<p>

**Abstract:** Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.

</p>
</details>

<details><summary><b>Neural Network Based Sleep Phases Classification for Resource Constraint Environments</b>
<a href="https://arxiv.org/abs/2105.11452">arxiv:2105.11452</a>
&#x1F4C8; 1 <br>
<p>Berkay Köprü, Murat Aslan, Alisher Kholmatov</p></summary>
<p>

**Abstract:** Sleep is restoration process of the body. The efficiency of this restoration process is directly correlated to the amount of time spent at each sleep phase. Hence, automatic tracking of sleep via wearable devices has attracted both the researchers and industry. Current state-of-the-art sleep tracking solutions are memory and processing greedy and they require cloud or mobile phone connectivity. We propose a memory efficient sleep tracking architecture which can work in the embedded environment without needing any cloud or mobile phone connection. In this study, a novel architecture is proposed that consists of a feature extraction and Artificial Neural Networks based stacking classifier. Besides, we discussed how to tackle with sequential nature of the sleep staging for the memory constraint environments through the proposed framework. To verify the system, a dataset is collected from 24 different subjects for 31 nights with a wrist worn device having 3-axis accelerometer (ACC) and photoplethysmogram (PPG) sensors. Over the collected dataset, the proposed classification architecture achieves 20\% and 14\% better F1 scores than its competitors. Apart from the superior performance, proposed architecture is a promising solution for resource constraint embedded systems by allocating only 4.2 kilobytes of memory (RAM).

</p>
</details>

<details><summary><b>GraphVICRegHSIC: Towards improved self-supervised representation learning for graphs with a hyrbid loss function</b>
<a href="https://arxiv.org/abs/2105.12247">arxiv:2105.12247</a>
&#x1F4C8; 0 <br>
<p>Sayan Nag</p></summary>
<p>

**Abstract:** Self-supervised learning and pre-training strategieshave developed over the last few years especiallyfor Convolutional Neural Networks (CNNs). Re-cently application of such methods can also be no-ticed for Graph Neural Networks (GNNs) . In thispaper, we have used a graph based self-supervisedlearning strategy with different loss functions (Bar-low Twins[Zbontaret al., 2021], HSIC[Tsaiet al.,2021], VICReg[Bardeset al., 2021]) which haveshown promising results when applied with CNNspreviously. We have also proposed a hybrid lossfunction combining the advantages of VICReg andHSIC and called it as VICRegHSIC. The perfor-mance of these aforementioned methods have beencompared when applied to 7 different datasets suchas MUTAG, PROTEINS, IMDB-Binary, etc. Ex-periments showed that our hybrid loss function per-formed better than the remaining ones in 4 out of7 cases. Moreover, the impact of different batchsizes, projector dimensions and data augmentationstrategies have also been explored.

</p>
</details>

<details><summary><b>AutoMate: A Dataset and Learning Approach for Automatic Mating of CAD Assemblies</b>
<a href="https://arxiv.org/abs/2105.12238">arxiv:2105.12238</a>
&#x1F4C8; 0 <br>
<p>Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vladimir G. Kim, Adriana Schulz</p></summary>
<p>

**Abstract:** Assembly modeling is a core task of computer aided design (CAD), comprising around one third of the work in a CAD workflow. Optimizing this process therefore represents a huge opportunity in the design of a CAD system, but current research of assembly based modeling is not directly applicable to modern CAD systems because it eschews the dominant data structure of modern CAD: parametric boundary representations (BREPs). CAD assembly modeling defines assemblies as a system of pairwise constraints, called mates, between parts, which are defined relative to BREP topology rather than in world coordinates common to existing work. We propose SB-GCN, a representation learning scheme on BREPs that retains the topological structure of parts, and use these learned representations to predict CAD type mates. To train our system, we compiled the first large scale dataset of BREP CAD assemblies, which we are releasing along with benchmark mate prediction tasks. Finally, we demonstrate the compatibility of our model with an existing commercial CAD system by building a tool that assists users in mate creation by suggesting mate completions, with 72.2% accuracy.

</p>
</details>

<details><summary><b>The Nonlinearity Coefficient - A Practical Guide to Neural Architecture Design</b>
<a href="https://arxiv.org/abs/2105.12210">arxiv:2105.12210</a>
&#x1F4C8; 0 <br>
<p>George Philipp</p></summary>
<p>

**Abstract:** In essence, a neural network is an arbitrary differentiable, parametrized function. Choosing a neural network architecture for any task is as complex as searching the space of those functions. For the last few years, 'neural architecture design' has been largely synonymous with 'neural architecture search' (NAS), i.e. brute-force, large-scale search. NAS has yielded significant gains on practical tasks. However, NAS methods end up searching for a local optimum in architecture space in a small neighborhood around architectures that often go back decades, based on CNN or LSTM.
  In this work, we present a different and complementary approach to architecture design, which we term 'zero-shot architecture design' (ZSAD). We develop methods that can predict, without any training, whether an architecture will achieve a relatively high test or training error on a task after training. We then go on to explain the error in terms of the architecture definition itself and develop tools for modifying the architecture based on this explanation. This confers an unprecedented level of control on the deep learning practitioner. They can make informed design decisions before the first line of code is written, even for tasks for which no prior art exists.
  Our first major contribution is to show that the 'degree of nonlinearity' of a neural architecture is a key causal driver behind its performance, and a primary aspect of the architecture's model complexity. We introduce the 'nonlinearity coefficient' (NLC), a scalar metric for measuring nonlinearity. Via extensive empirical study, we show that the value of the NLC in the architecture's randomly initialized state before training is a powerful predictor of test error after training and that attaining a right-sized NLC is essential for attaining an optimal test error. The NLC is also conceptually simple, well-defined for any feedforward network, easy and cheap to compute, has extensive theoretical, empirical and conceptual grounding, follows instructively from the architecture definition, and can be easily controlled via our 'nonlinearity normalization' algorithm. We argue that the NLC is the most powerful scalar statistic for architecture design specifically and neural network analysis in general. Our analysis is fueled by mean field theory, which we use to uncover the 'meta-distribution' of layers.
  Beyond the NLC, we uncover and flesh out a range of metrics and properties that have a significant explanatory influence on test and training error. We go on to explain the majority of the error variation across a wide range of randomly generated architectures with these metrics and properties. We compile our insights into a practical guide for architecture designers, which we argue can significantly shorten the trial-and-error phase of deep learning deployment.
  Our results are grounded in an experimental protocol that exceeds that of the vast majority of other deep learning studies in terms of carefulness and rigor. We study the impact of e.g. dataset, learning rate, floating-point precision, loss function, statistical estimation error and batch inter-dependency on performance and other key properties. We promote research practices that we believe can significantly accelerate progress in architecture design research.

</p>
</details>

<details><summary><b>Density estimation on low-dimensional manifolds: an inflation-deflation approach</b>
<a href="https://arxiv.org/abs/2105.12152">arxiv:2105.12152</a>
&#x1F4C8; 0 <br>
<p>Christian Horvat, Jean-Pascal Pfister</p></summary>
<p>

**Abstract:** Normalizing Flows (NFs) are universal density estimators based on Neuronal Networks. However, this universality is limited: the density's support needs to be diffeomorphic to a Euclidean space. In this paper, we propose a novel method to overcome this limitation without sacrificing universality. The proposed method inflates the data manifold by adding noise in the normal space, trains an NF on this inflated manifold, and, finally, deflates the learned density. Our main result provides sufficient conditions on the manifold and the specific choice of noise under which the corresponding estimator is exact. Our method has the same computational complexity as NFs and does not require computing an inverse flow. We also show that, if the embedding dimension is much larger than the manifold dimension, noise in the normal space can be well approximated by Gaussian noise. This allows to use our method for approximating arbitrary densities on non-flat manifolds provided that the manifold dimension is known.

</p>
</details>

<details><summary><b>Providing Meaningful Data Summarizations Using Exemplar-based Clustering in Industry 4.0</b>
<a href="https://arxiv.org/abs/2105.12026">arxiv:2105.12026</a>
&#x1F4C8; 0 <br>
<p>Philipp-Jan Honysz, Alexander Schulze-Struchtrup, Sebastian Buschjäger, Katharina Morik</p></summary>
<p>

**Abstract:** Data summarizations are a valuable tool to derive knowledge from large data streams and have proven their usefulness in a great number of applications. Summaries can be found by optimizing submodular functions. These functions map subsets of data to real values, which indicate their "representativeness" and which should be maximized to find a diverse summary of the underlying data. In this paper, we studied Exemplar-based clustering as a submodular function and provide a GPU algorithm to cope with its high computational complexity. We show, that our GPU implementation provides speedups of up to 72x using single-precision and up to 452x using half-precision computation compared to conventional CPU algorithms. We also show, that the GPU algorithm not only provides remarkable runtime benefits with workstation-grade GPUs but also with low-power embedded computation units for which speedups of up to 35x are possible. Furthermore, we apply our algorithm to real-world data from injection molding manufacturing processes and discuss how found summaries help with steering this specific process to cut costs and reduce the manufacturing of bad parts. Beyond pure speedup considerations, we show, that our approach can provide summaries within reasonable time frames for this kind of industrial, real-world data.

</p>
</details>

<details><summary><b>CI-dataset and DetDSCI methodology for detecting too small and too large critical infrastructures in satellite images: Airports and electrical substations as case study</b>
<a href="https://arxiv.org/abs/2105.11844">arxiv:2105.11844</a>
&#x1F4C8; 0 <br>
<p>Francisco Pérez-Hernández, José Rodríguez-Ortega, Yassir Benhammou, Francisco Herrera, Siham Tabik</p></summary>
<p>

**Abstract:** The detection of critical infrastructures in large territories represented by aerial and satellite images is of high importance in several fields such as in security, anomaly detection, land use planning and land use change detection. However, the detection of such infrastructures is complex as they have highly variable shapes and sizes, i.e., some infrastructures, such as electrical substations, are too small while others, such as airports, are too large. Besides, airports can have a surface area either small or too large with completely different shapes, which makes its correct detection challenging. As far as we know, these limitations have not been tackled yet in previous works. This paper presents (1) a smart Critical Infrastructure dataset, named CI-dataset, organised into two scales, small and large scales critical infrastructures and (2) a two-level resolution-independent critical infrastructure detection (DetDSCI) methodology that first determines the spatial resolution of the input image using a classification model, then analyses the image using the appropriate detector for that spatial resolution. The present study targets two representative classes, airports and electrical substations. Our experiments show that DetDSCI methodology achieves up to 37,53% F1 improvement with respect to Faster R-CNN, one of the most influential detection models.

</p>
</details>

<details><summary><b>Towards Understanding the Condensation of Neural Networks at Initial Training</b>
<a href="https://arxiv.org/abs/2105.11686">arxiv:2105.11686</a>
&#x1F4C8; 0 <br>
<p>Zhi-Qin John Xu, Hanxu Zhou, Tao Luo, Yaoyu Zhang</p></summary>
<p>

**Abstract:** Implicit regularization is important for understanding the learning of neural networks (NNs). Empirical works show that input weights of hidden neurons (the input weight of a hidden neuron consists of the weight from its input layer to the hidden neuron and its bias term) condense on isolated orientations with a small initialization. The condensation dynamics implies that the training implicitly regularizes a NN towards one with much smaller effective size. In this work, we utilize multilayer networks to show that the maximal number of condensed orientations in the initial training stage is twice the multiplicity of the activation function, where "multiplicity" is multiple roots of activation function at origin. Our theoretical analysis confirms experiments for two cases, one is for the activation function of multiplicity one with arbitrary dimension input, which contains many common activation functions, and the other is for the layer with one-dimensional input and arbitrary multiplicity. This work makes a step towards understanding how small initialization implicitly leads NNs to condensation at initial training stage, which lays a foundation for the future study of the nonlinear dynamics of NNs and its implicit regularization effect at a later stage of training.

</p>
</details>


[Next Page]({{ '/2021/05/24/2021.05.24.html' | relative_url }})
