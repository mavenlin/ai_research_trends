Prev: [2021.01.18]({{ '/2021/01/18/2021.01.18.html' | relative_url }})  Next: [2021.01.20]({{ '/2021/01/20/2021.01.20.html' | relative_url }})
{% raw %}
## Summary for 2021-01-19, created on 2021-12-24


<details><summary><b>Deep Learning Models for Calculation of Cardiothoracic Ratio from Chest Radiographs for Assisted Diagnosis of Cardiomegaly</b>
<a href="https://arxiv.org/abs/2101.07606">arxiv:2101.07606</a>
&#x1F4C8; 46 <br>
<p>Tanveer Gupte, Mrunmai Niljikar, Manish Gawali, Viraj Kulkarni, Amit Kharat, Aniruddha Pant</p></summary>
<p>

**Abstract:** We propose an automated method based on deep learning to compute the cardiothoracic ratio and detect the presence of cardiomegaly from chest radiographs. We develop two separate models to demarcate the heart and chest regions in an X-ray image using bounding boxes and use their outputs to calculate the cardiothoracic ratio. We obtain a sensitivity of 0.96 at a specificity of 0.81 with a mean absolute error of 0.0209 on a held-out test dataset and a sensitivity of 0.84 at a specificity of 0.97 with a mean absolute error of 0.018 on an independent dataset from a different hospital. We also compare three different segmentation model architectures for the proposed method and observe that Attention U-Net yields better results than SE-Resnext U-Net and EfficientNet U-Net. By providing a numeric measurement of the cardiothoracic ratio, we hope to mitigate human subjectivity arising out of visual assessment in the detection of cardiomegaly.

</p>
</details>

<details><summary><b>Electrocardiogram Classification and Visual Diagnosis of Atrial Fibrillation with DenseECG</b>
<a href="https://arxiv.org/abs/2101.07535">arxiv:2101.07535</a>
&#x1F4C8; 43 <br>
<p>Dacheng Chen, Dan Li, Xiuqin Xu, Ruizhi Yang, See-Kiong Ng</p></summary>
<p>

**Abstract:** Atrial Fibrillation (AF) is a common cardiac arrhythmia affecting a large number of people around the world. If left undetected, it will develop into chronic disability or even early mortality. However, patients who have this problem can barely feel its presence, especially in its early stage. A non-invasive, automatic, and effective detection method is therefore needed to help early detection so that medical intervention can be implemented in time to prevent its progression.
  Electrocardiogram (ECG), which records the electrical activities of the heart, has been widely used for detecting the presence of AF. However, due to the subtle patterns of AF, the performance of detection models have largely depended on complicated data pre-processing and expertly engineered features. In our work, we developed DenseECG, an end-to-end model based on 5 layers 1D densely connected convolutional neural network. We trained our model using the publicly available dataset from 2017 PhysioNet Computing in Cardiology(CinC) Challenge containing 8528 single-lead ECG recordings of short-term heart rhythms (9-61s). Our trained model was able to outperform the other state-of-the-art AF detection models on this dataset without complicated data pre-processing and expert-supervised feature engineering.

</p>
</details>

<details><summary><b>Quantum Permutation Synchronization</b>
<a href="https://arxiv.org/abs/2101.07755">arxiv:2101.07755</a>
&#x1F4C8; 42 <br>
<p>Tolga Birdal, Vladislav Golyanik, Christian Theobalt, Leonidas Guibas</p></summary>
<p>

**Abstract:** We present QuantumSync, the first quantum algorithm for solving a synchronization problem in the context of computer vision. In particular, we focus on permutation synchronization which involves solving a non-convex optimization problem in discrete variables. We start by formulating synchronization into a quadratic unconstrained binary optimization problem (QUBO). While such formulation respects the binary nature of the problem, ensuring that the result is a set of permutations requires extra care. Hence, we: (I) show how to insert permutation constraints into a QUBO problem and (ii) solve the constrained QUBO problem on the current generation of the adiabatic quantum computers D-Wave. Thanks to the quantum annealing, we guarantee global optimality with high probability while sampling the energy landscape to yield confidence estimates. Our proof-of-concepts realization on the adiabatic D-Wave computer demonstrates that quantum machines offer a promising way to solve the prevalent yet difficult synchronization problems.

</p>
</details>

<details><summary><b>Situation and Behavior Understanding by Trope Detection on Films</b>
<a href="https://arxiv.org/abs/2101.07632">arxiv:2101.07632</a>
&#x1F4C8; 38 <br>
<p>Chen-Hsi Chang, Hung-Ting Su, Jui-heng Hsu, Yu-Siang Wang, Yu-Cheng Chang, Zhe Yu Liu, Ya-Liang Chang, Wen-Feng Cheng, Ke-Jyun Wang, Winston H. Hsu</p></summary>
<p>

**Abstract:** The human ability of deep cognitive skills are crucial for the development of various real-world applications that process diverse and abundant user generated input. While recent progress of deep learning and natural language processing have enabled learning system to reach human performance on some benchmarks requiring shallow semantics, such human ability still remains challenging for even modern contextual embedding models, as pointed out by many recent studies. Existing machine comprehension datasets assume sentence-level input, lack of casual or motivational inferences, or could be answered with question-answer bias. Here, we present a challenging novel task, trope detection on films, in an effort to create a situation and behavior understanding for machines. Tropes are storytelling devices that are frequently used as ingredients in recipes for creative works. Comparing to existing movie tag prediction tasks, tropes are more sophisticated as they can vary widely, from a moral concept to a series of circumstances, and embedded with motivations and cause-and-effects. We introduce a new dataset, Tropes in Movie Synopses (TiMoS), with 5623 movie synopses and 95 different tropes collecting from a Wikipedia-style database, TVTropes. We present a multi-stream comprehension network (MulCom) leveraging multi-level attention of words, sentences, and role relations. Experimental result demonstrates that modern models including BERT contextual embedding, movie tag prediction systems, and relational networks, perform at most 37% of human performance (23.97/64.87) in terms of F1 score. Our MulCom outperforms all modern baselines, by 1.5 to 5.0 F1 score and 1.5 to 3.0 mean of average precision (mAP) score. We also provide a detailed analysis and human evaluation to pave ways for future research.

</p>
</details>

<details><summary><b>Spatial Assembly: Generative Architecture With Reinforcement Learning, Self Play and Tree Search</b>
<a href="https://arxiv.org/abs/2101.07579">arxiv:2101.07579</a>
&#x1F4C8; 38 <br>
<p>Panagiotis Tigas, Tyson Hosmer</p></summary>
<p>

**Abstract:** With this work, we investigate the use of Reinforcement Learning (RL) for the generation of spatial assemblies, by combining ideas from Procedural Generation algorithms (Wave Function Collapse algorithm (WFC)) and RL for Game Solving. WFC is a Generative Design algorithm, inspired by Constraint Solving. In WFC, one defines a set of tiles/blocks and constraints and the algorithm generates an assembly that satisfies these constraints. Casting the problem of generation of spatial assemblies as a Markov Decision Process whose states transitions are defined by WFC, we propose an algorithm that uses Reinforcement Learning and Self-Play to learn a policy that generates assemblies that maximize objectives set by the designer. Finally, we demonstrate the use of our Spatial Assembly algorithm in Architecture Design.

</p>
</details>

<details><summary><b>Interpretable Models for Granger Causality Using Self-explaining Neural Networks</b>
<a href="https://arxiv.org/abs/2101.07600">arxiv:2101.07600</a>
&#x1F4C8; 37 <br>
<p>Ričards Marcinkevičs, Julia E. Vogt</p></summary>
<p>

**Abstract:** Exploratory analysis of time series data can yield a better understanding of complex dynamical systems. Granger causality is a practical framework for analysing interactions in sequential data, applied in a wide range of domains. In this paper, we propose a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks. This framework is more interpretable than other neural-network-based techniques for inferring Granger causality, since in addition to relational inference, it also allows detecting signs of Granger-causal effects and inspecting their variability over time. In comprehensive experiments on simulated data, we show that our framework performs on par with several powerful baseline methods at inferring Granger causality and that it achieves better performance at inferring interaction signs. The results suggest that our framework is a viable and more interpretable alternative to sparse-input neural networks for inferring Granger causality.

</p>
</details>

<details><summary><b>Illuminating the Space of Beatable Lode Runner Levels Produced By Various Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2101.07868">arxiv:2101.07868</a>
&#x1F4C8; 33 <br>
<p>Kirby Steckel, Jacob Schrum</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) are capable of generating convincing imitations of elements from a training set, but the distribution of elements in the training set affects to difficulty of properly training the GAN and the quality of the outputs it produces. This paper looks at six different GANs trained on different subsets of data from the game Lode Runner. The quality diversity algorithm MAP-Elites was used to explore the set of quality levels that could be produced by each GAN, where quality was defined as being beatable and having the longest solution path possible. Interestingly, a GAN trained on only 20 levels generated the largest set of diverse beatable levels while a GAN trained on 150 levels generated the smallest set of diverse beatable levels, thus challenging the notion that more is always better when training GANs.

</p>
</details>

<details><summary><b>Comparative Evaluation of 3D and 2D Deep Learning Techniques for Semantic Segmentation in CT Scans</b>
<a href="https://arxiv.org/abs/2101.07612">arxiv:2101.07612</a>
&#x1F4C8; 31 <br>
<p>Abhishek Shivdeo, Rohit Lokwani, Viraj Kulkarni, Amit Kharat, Aniruddha Pant</p></summary>
<p>

**Abstract:** Image segmentation plays a pivotal role in several medical-imaging applications by assisting the segmentation of the regions of interest. Deep learning-based approaches have been widely adopted for semantic segmentation of medical data. In recent years, in addition to 2D deep learning architectures, 3D architectures have been employed as the predictive algorithms for 3D medical image data. In this paper, we propose a 3D stack-based deep learning technique for segmenting manifestations of consolidation and ground-glass opacities in 3D Computed Tomography (CT) scans. We also present a comparison based on the segmentation results, the contextual information retained, and the inference time between this 3D technique and a traditional 2D deep learning technique. We also define the area-plot, which represents the peculiar pattern observed in the slice-wise areas of the pathology regions predicted by these deep learning models. In our exhaustive evaluation, 3D technique performs better than the 2D technique for the segmentation of CT scans. We get dice scores of 79% and 73% for the 3D and the 2D techniques respectively. The 3D technique results in a 5X reduction in the inference time compared to the 2D technique. Results also show that the area-plots predicted by the 3D model are more similar to the ground truth than those predicted by the 2D model. We also show how increasing the amount of contextual information retained during the training can improve the 3D model's performance.

</p>
</details>

<details><summary><b>The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods</b>
<a href="https://arxiv.org/abs/2101.07528">arxiv:2101.07528</a>
&#x1F4C8; 30 <br>
<p>Louis Thiry, Michael Arbel, Eugene Belilovsky, Edouard Oyallon</p></summary>
<p>

**Abstract:** A recent line of work showed that various forms of convolutional kernel methods can be competitive with standard supervised deep convolutional networks on datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while being more amenable to theoretical analysis. In this work, we highlight the importance of a data-dependent feature extraction step that is key to the obtain good performance in convolutional kernel methods. This step typically corresponds to a whitened dictionary of patches, and gives rise to a data-driven convolutional kernel methods. We extensively study its effect, demonstrating it is the key ingredient for high performance of these methods. Specifically, we show that one of the simplest instances of such kernel methods, based on a single layer of image patches followed by a linear classifier is already obtaining classification accuracies on CIFAR-10 in the same range as previous more sophisticated convolutional kernel methods. We scale this method to the challenging ImageNet dataset, showing such a simple approach can exceed all existing non-learned representation methods. This is a new baseline for object recognition without representation learning methods, that initiates the investigation of convolutional kernel models on ImageNet. We conduct experiments to analyze the dictionary that we used, our ablations showing they exhibit low-dimensional properties.

</p>
</details>

<details><summary><b>Implicit Bias of Linear RNNs</b>
<a href="https://arxiv.org/abs/2101.07833">arxiv:2101.07833</a>
&#x1F4C8; 26 <br>
<p>Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, Alyson K. Fletcher</p></summary>
<p>

**Abstract:** Contemporary wisdom based on empirical studies suggests that standard recurrent neural networks (RNNs) do not perform well on tasks requiring long-term memory. However, precise reasoning for this behavior is still unknown. This paper provides a rigorous explanation of this property in the special case of linear RNNs. Although this work is limited to linear RNNs, even these systems have traditionally been difficult to analyze due to their non-linear parameterization. Using recently-developed kernel regime analysis, our main result shows that linear RNNs learned from random initializations are functionally equivalent to a certain weighted 1D-convolutional network. Importantly, the weightings in the equivalent model cause an implicit bias to elements with smaller time lags in the convolution and hence, shorter memory. The degree of this bias depends on the variance of the transition kernel matrix at initialization and is related to the classic exploding and vanishing gradients problem. The theory is validated in both synthetic and real data experiments.

</p>
</details>

<details><summary><b>Collaboration among Image and Object Level Features for Image Colourisation</b>
<a href="https://arxiv.org/abs/2101.07576">arxiv:2101.07576</a>
&#x1F4C8; 26 <br>
<p>Rita Pucci, Christian Micheloni, Niki Martinel</p></summary>
<p>

**Abstract:** Image colourisation is an ill-posed problem, with multiple correct solutions which depend on the context and object instances present in the input datum. Previous approaches attacked the problem either by requiring intense user interactions or by exploiting the ability of convolutional neural networks (CNNs) in learning image level (context) features. However, obtaining human hints is not always feasible and CNNs alone are not able to learn object-level semantics unless multiple models pretrained with supervision are considered. In this work, we propose a single network, named UCapsNet, that separate image-level features obtained through convolutions and object-level features captured by means of capsules. Then, by skip connections over different layers, we enforce collaboration between such disentangling factors to produce high quality and plausible image colourisation. We pose the problem as a classification task that can be addressed by a fully self-supervised approach, thus requires no human effort. Experimental results on three benchmark datasets show that our approach outperforms existing methods on standard quality metrics and achieves a state of the art performances on image colourisation. A large scale user study shows that our method is preferred over existing solutions.

</p>
</details>

<details><summary><b>LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition</b>
<a href="https://arxiv.org/abs/2101.07922">arxiv:2101.07922</a>
&#x1F4C8; 23 <br>
<p>Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John Dickerson, Gavin Taylor, Tom Goldstein</p></summary>
<p>

**Abstract:** Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.

</p>
</details>

<details><summary><b>A System for Automated Open-Source Threat Intelligence Gathering and Management</b>
<a href="https://arxiv.org/abs/2101.07769">arxiv:2101.07769</a>
&#x1F4C8; 19 <br>
<p>Peng Gao, Xiaoyuan Liu, Edward Choi, Bhavna Soman, Chinmaya Mishra, Kate Farris, Dawn Song</p></summary>
<p>

**Abstract:** To remain aware of the fast-evolving cyber threat landscape, open-source Cyber Threat Intelligence (OSCTI) has received growing attention from the community. Commonly, knowledge about threats is presented in a vast number of OSCTI reports. Despite the pressing need for high-quality OSCTI, existing OSCTI gathering and management platforms, however, have primarily focused on isolated, low-level Indicators of Compromise. On the other hand, higher-level concepts (e.g., adversary tactics, techniques, and procedures) and their relationships have been overlooked, which contain essential knowledge about threat behaviors that is critical to uncovering the complete threat scenario. To bridge the gap, we propose SecurityKG, a system for automated OSCTI gathering and management. SecurityKG collects OSCTI reports from various sources, uses a combination of AI and NLP techniques to extract high-fidelity knowledge about threat behaviors, and constructs a security knowledge graph. SecurityKG also provides a UI that supports various types of interactivity to facilitate knowledge graph exploration.

</p>
</details>

<details><summary><b>Meta-Reinforcement Learning for Adaptive Motor Control in Changing Robot Dynamics and Environments</b>
<a href="https://arxiv.org/abs/2101.07599">arxiv:2101.07599</a>
&#x1F4C8; 19 <br>
<p>Timothée Anne, Jack Wilkinson, Zhibin Li</p></summary>
<p>

**Abstract:** This work developed a meta-learning approach that adapts the control policy on the fly to different changing conditions for robust locomotion. The proposed method constantly updates the interaction model, samples feasible sequences of actions of estimated the state-action trajectories, and then applies the optimal actions to maximize the reward. To achieve online model adaptation, our proposed method learns different latent vectors of each training condition, which are selected online given the newly collected data. Our work designs appropriate state space and reward functions, and optimizes feasible actions in an MPC fashion which are then sampled directly in the joint space considering constraints, hence requiring no prior design of specific walking gaits. We further demonstrate the robot's capability of detecting unexpected changes during interaction and adapting control policies quickly. The extensive validation on the SpotMicro robot in a physics simulation shows adaptive and robust locomotion skills under varying ground friction, external pushes, and different robot models including hardware faults and changes.

</p>
</details>

<details><summary><b>Utilizing Import Vector Machines to Identify Dangerous Pro-active Traffic Conditions</b>
<a href="https://arxiv.org/abs/2101.07683">arxiv:2101.07683</a>
&#x1F4C8; 17 <br>
<p>Kui Yang, Wenjing Zhao, Constantinos Antoniou</p></summary>
<p>

**Abstract:** Traffic accidents have been a severe issue in metropolises with the development of traffic flow. This paper explores the theory and application of a recently developed machine learning technique, namely Import Vector Machines (IVMs), in real-time crash risk analysis, which is a hot topic to reduce traffic accidents. Historical crash data and corresponding traffic data from Shanghai Urban Expressway System were employed and matched. Traffic conditions are labelled as dangerous (i.e. probably leading to a crash) and safe (i.e. a normal traffic condition) based on 5-minute measurements of average speed, volume and occupancy. The IVM algorithm is trained to build the classifier and its performance is compared to the popular and successfully applied technique of Support Vector Machines (SVMs). The main findings indicate that IVMs could successfully be employed in real-time identification of dangerous pro-active traffic conditions. Furthermore, similar to the "support points" of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM, and its classification rates are similar to those of SVMs. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.

</p>
</details>

<details><summary><b>IntentNet: Learning to Predict Intention from Raw Sensor Data</b>
<a href="https://arxiv.org/abs/2101.07907">arxiv:2101.07907</a>
&#x1F4C8; 16 <br>
<p>Sergio Casas, Wenjie Luo, Raquel Urtasun</p></summary>
<p>

**Abstract:** In order to plan a safe maneuver, self-driving vehicles need to understand the intent of other traffic participants. We define intent as a combination of discrete high-level behaviors as well as continuous trajectories describing future motion. In this paper, we develop a one-stage detector and forecaster that exploits both 3D point clouds produced by a LiDAR sensor as well as dynamic maps of the environment. Our multi-task model achieves better accuracy than the respective separate modules while saving computation, which is critical to reducing reaction time in self-driving applications.

</p>
</details>

<details><summary><b>Minimax Off-Policy Evaluation for Multi-Armed Bandits</b>
<a href="https://arxiv.org/abs/2101.07781">arxiv:2101.07781</a>
&#x1F4C8; 16 <br>
<p>Cong Ma, Banghua Zhu, Jiantao Jiao, Martin J. Wainwright</p></summary>
<p>

**Abstract:** We study the problem of off-policy evaluation in the multi-armed bandit model with bounded rewards, and develop minimax rate-optimal procedures under three settings. First, when the behavior policy is known, we show that the Switch estimator, a method that alternates between the plug-in and importance sampling estimators, is minimax rate-optimal for all sample sizes. Second, when the behavior policy is unknown, we analyze performance in terms of the competitive ratio, thereby revealing a fundamental gap between the settings of known and unknown behavior policies. When the behavior policy is unknown, any estimator must have mean-squared error larger -- relative to the oracle estimator equipped with the knowledge of the behavior policy -- by a multiplicative factor proportional to the support size of the target policy. Moreover, we demonstrate that the plug-in approach achieves this worst-case competitive ratio up to a logarithmic factor. Third, we initiate the study of the partial knowledge setting in which it is assumed that the minimum probability taken by the behavior policy is known. We show that the plug-in estimator is optimal for relatively large values of the minimum probability, but is sub-optimal when the minimum probability is low. In order to remedy this gap, we propose a new estimator based on approximation by Chebyshev polynomials that provably achieves the optimal estimation error. Numerical experiments on both simulated and real data corroborate our theoretical findings.

</p>
</details>

<details><summary><b>Information Theoretic Secure Aggregation with User Dropouts</b>
<a href="https://arxiv.org/abs/2101.07750">arxiv:2101.07750</a>
&#x1F4C8; 10 <br>
<p>Yizhou Zhao, Hua Sun</p></summary>
<p>

**Abstract:** In the robust secure aggregation problem, a server wishes to learn and only learn the sum of the inputs of a number of users while some users may drop out (i.e., may not respond). The identity of the dropped users is not known a priori and the server needs to securely recover the sum of the remaining surviving users. We consider the following minimal two-round model of secure aggregation. Over the first round, any set of no fewer than $U$ users out of $K$ users respond to the server and the server wants to learn the sum of the inputs of all responding users. The remaining users are viewed as dropped. Over the second round, any set of no fewer than $U$ users of the surviving users respond (i.e., dropouts are still possible over the second round) and from the information obtained from the surviving users over the two rounds, the server can decode the desired sum. The security constraint is that even if the server colludes with any $T$ users and the messages from the dropped users are received by the server (e.g., delayed packets), the server is not able to infer any additional information beyond the sum in the information theoretic sense. For this information theoretic secure aggregation problem, we characterize the optimal communication cost. When $U \leq T$, secure aggregation is not feasible, and when $U > T$, to securely compute one symbol of the sum, the minimum number of symbols sent from each user to the server is $1$ over the first round, and $1/(U-T)$ over the second round.

</p>
</details>

<details><summary><b>An Artificial Intelligence based approach to estimating time of arrival and bus occupancy for public transport systems in Africa</b>
<a href="https://arxiv.org/abs/2101.07674">arxiv:2101.07674</a>
&#x1F4C8; 9 <br>
<p>Appau Ernest</p></summary>
<p>

**Abstract:** This document entails a progressive report on the design and implementation of a bus tracking and monitoring system . This report has its contents within the limits of five chapters with each concisely exploring their various objectives. Chapter one is the introductory chapter. It entails a brief description of a bus tracking and monitoring system ,the need and the aims and objectives of this project. Chapter two consists the literature review of this project. This entails the critical analysis of previous related research and projects undertaken by other people. The merits and demerits of the various implementations.Chapter three consists of theory and design considerations of the proposed system for Kwame Nkrumah University campus. Chapter four talks about the methods used to collect data and the approach and technology stack adopted to build the proposed system.Chapter five concludes the thesis and discusses the results of test and deployment of the proposed system on Kwame Nkrumah University of Science and Technology campus

</p>
</details>

<details><summary><b>Edge-Featured Graph Attention Network</b>
<a href="https://arxiv.org/abs/2101.07671">arxiv:2101.07671</a>
&#x1F4C8; 9 <br>
<p>Jun Chen, Haopeng Chen</p></summary>
<p>

**Abstract:** Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.

</p>
</details>

<details><summary><b>A framework to compare music generative models using automatic evaluation metrics extended to rhythm</b>
<a href="https://arxiv.org/abs/2101.07669">arxiv:2101.07669</a>
&#x1F4C8; 9 <br>
<p>Sebastian Garcia-Valencia, Alejandro Betancourt, Juan G. Lalinde-Pulido</p></summary>
<p>

**Abstract:** To train a machine learning model is necessary to take numerous decisions about many options for each process involved, in the field of sequence generation and more specifically of music composition, the nature of the problem helps to narrow the options but at the same time, some other options appear for specific challenges. This paper takes the framework proposed in a previous research that did not consider rhythm to make a series of design decisions, then, rhythm support is added to evaluate the performance of two RNN memory cells in the creation of monophonic music. The model considers the handling of music transposition and the framework evaluates the quality of the generated pieces using automatic quantitative metrics based on geometry which have rhythm support added as well.

</p>
</details>

<details><summary><b>Analysis and tuning of hierarchical topic models based on Renyi entropy approach</b>
<a href="https://arxiv.org/abs/2101.07598">arxiv:2101.07598</a>
&#x1F4C8; 9 <br>
<p>Sergei Koltcov, Vera Ignatenko, Maxim Terpilovskii, Paolo Rosso</p></summary>
<p>

**Abstract:** Hierarchical topic modeling is a potentially powerful instrument for determining the topical structure of text collections that allows constructing a topical hierarchy representing levels of topical abstraction. However, tuning of parameters of hierarchical models, including the number of topics on each hierarchical level, remains a challenging task and an open issue. In this paper, we propose a Renyi entropy-based approach for a partial solution to the above problem. First, we propose a Renyi entropy-based metric of quality for hierarchical models. Second, we propose a practical concept of hierarchical topic model tuning tested on datasets with human mark-up. In the numerical experiments, we consider three different hierarchical models, namely, hierarchical latent Dirichlet allocation (hLDA) model, hierarchical Pachinko allocation model (hPAM), and hierarchical additive regularization of topic models (hARTM). We demonstrate that hLDA model possesses a significant level of instability and, moreover, the derived numbers of topics are far away from the true numbers for labeled datasets. For hPAM model, the Renyi entropy approach allows us to determine only one level of the data structure. For hARTM model, the proposed approach allows us to estimate the number of topics for two hierarchical levels.

</p>
</details>

<details><summary><b>UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</b>
<a href="https://arxiv.org/abs/2101.07597">arxiv:2101.07597</a>
&#x1F4C8; 9 <br>
<p>Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang</p></summary>
<p>

**Abstract:** In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6% against the previous approach.

</p>
</details>

<details><summary><b>Using StyleGAN for Visual Interpretability of Deep Learning Models on Medical Images</b>
<a href="https://arxiv.org/abs/2101.07563">arxiv:2101.07563</a>
&#x1F4C8; 9 <br>
<p>Kathryn Schutte, Olivier Moindrot, Paul Hérent, Jean-Baptiste Schiratti, Simon Jégou</p></summary>
<p>

**Abstract:** As AI-based medical devices are becoming more common in imaging fields like radiology and histology, interpretability of the underlying predictive models is crucial to expand their use in clinical practice. Existing heatmap-based interpretability methods such as GradCAM only highlight the location of predictive features but do not explain how they contribute to the prediction. In this paper, we propose a new interpretability method that can be used to understand the predictions of any black-box model on images, by showing how the input image would be modified in order to produce different predictions. A StyleGAN is trained on medical images to provide a mapping between latent vectors and images. Our method identifies the optimal direction in the latent space to create a change in the model prediction. By shifting the latent representation of an input image along this direction, we can produce a series of new synthetic images with changed predictions. We validate our approach on histology and radiology images, and demonstrate its ability to provide meaningful explanations that are more informative than GradCAM heatmaps. Our method reveals the patterns learned by the model, which allows clinicians to build trust in the model's predictions, discover new biomarkers and eventually reveal potential biases.

</p>
</details>

<details><summary><b>Disentangled Recurrent Wasserstein Autoencoder</b>
<a href="https://arxiv.org/abs/2101.07496">arxiv:2101.07496</a>
&#x1F4C8; 9 <br>
<p>Jun Han, Martin Renqiang Min, Ligong Han, Li Erran Li, Xuan Zhang</p></summary>
<p>

**Abstract:** Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.

</p>
</details>

<details><summary><b>Directed Acyclic Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2101.07965">arxiv:2101.07965</a>
&#x1F4C8; 8 <br>
<p>Veronika Thost, Jie Chen</p></summary>
<p>

**Abstract:** Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.

</p>
</details>

<details><summary><b>Noise Learning Based Denoising Autoencoder</b>
<a href="https://arxiv.org/abs/2101.07937">arxiv:2101.07937</a>
&#x1F4C8; 8 <br>
<p>Woong-Hee Lee, Mustafa Ozger, Ursula Challita, Ki Won Sung</p></summary>
<p>

**Abstract:** This letter introduces a new denoiser that modifies the structure of denoising autoencoder (DAE), namely noise learning based DAE (nlDAE). The proposed nlDAE learns the noise of the input data. Then, the denoising is performed by subtracting the regenerated noise from the noisy input. Hence, nlDAE is more effective than DAE when the noise is simpler to regenerate than the original data. To validate the performance of nlDAE, we provide three case studies: signal restoration, symbol demodulation, and precise localization. Numerical results suggest that nlDAE requires smaller latent space dimension and smaller training dataset compared to DAE.

</p>
</details>

<details><summary><b>Cross-domain few-shot learning with unlabelled data</b>
<a href="https://arxiv.org/abs/2101.07899">arxiv:2101.07899</a>
&#x1F4C8; 8 <br>
<p>Fupin Yao</p></summary>
<p>

**Abstract:** Few shot learning aims to solve the data scarcity problem. If there is a domain shift between the test set and the training set, their performance will decrease a lot. This setting is called Cross-domain few-shot learning. However, this is very challenging because the target domain is unseen during training. Thus we propose a new setting some unlabelled data from the target domain is provided, which can bridge the gap between the source domain and the target domain. A benchmark for this setting is constructed using DomainNet \cite{peng2018oment}. We come up with a self-supervised learning method to fully utilize the knowledge in the labeled training set and the unlabelled set. Extensive experiments show that our methods outperforms several baseline methods by a large margin. We also carefully design an episodic training pipeline which yields a significant performance boost.

</p>
</details>

<details><summary><b>Classification of COVID-19 X-ray Images Using a Combination of Deep and Handcrafted Features</b>
<a href="https://arxiv.org/abs/2101.07866">arxiv:2101.07866</a>
&#x1F4C8; 8 <br>
<p>Weihan Zhang, Bryan Pogorelsky, Mark Loveland, Trevor Wolf</p></summary>
<p>

**Abstract:** Coronavirus Disease 2019 (COVID-19) demonstrated the need for accurate and fast diagnosis methods for emergent viral diseases. Soon after the emergence of COVID-19, medical practitioners used X-ray and computed tomography (CT) images of patients' lungs to detect COVID-19. Machine learning methods are capable of improving the identification accuracy of COVID-19 in X-ray and CT images, delivering near real-time results, while alleviating the burden on medical practitioners. In this work, we demonstrate the efficacy of a support vector machine (SVM) classifier, trained with a combination of deep convolutional and handcrafted features extracted from X-ray chest scans. We use this combination of features to discriminate between healthy, common pneumonia, and COVID-19 patients. The performance of the combined feature approach is compared with a standard convolutional neural network (CNN) and the SVM trained with handcrafted features. We find that combining the features in our novel framework improves the performance of the classification task compared to the independent application of convolutional and handcrafted features. Specifically, we achieve an accuracy of 0.988 in the classification task with our combined approach compared to 0.963 and 0.983 accuracy for the handcrafted features with SVM and CNN respectively.

</p>
</details>

<details><summary><b>The Devils in the Point Clouds: Studying the Robustness of Point Cloud Convolutions</b>
<a href="https://arxiv.org/abs/2101.07832">arxiv:2101.07832</a>
&#x1F4C8; 8 <br>
<p>Xingyi Li, Wenxuan Wu, Xiaoli Z. Fern, Li Fuxin</p></summary>
<p>

**Abstract:** Recently, there has been a significant interest in performing convolution over irregularly sampled point clouds. Since point clouds are very different from regular raster images, it is imperative to study the generalization of the convolution networks more closely, especially their robustness under variations in scale and rotations of the input data. This paper investigates different variants of PointConv, a convolution network on point clouds, to examine their robustness to input scale and rotation changes. Of the variants we explored, two are novel and generated significant improvements. The first is replacing the multilayer perceptron based weight function with much simpler third degree polynomials, together with a Sobolev norm regularization. Secondly, for 3D datasets, we derive a novel viewpoint-invariant descriptor by utilizing 3D geometric properties as the input to PointConv, in addition to the regular 3D coordinates. We have also explored choices of activation functions, neighborhood, and subsampling methods. Experiments are conducted on the 2D MNIST & CIFAR-10 datasets as well as the 3D SemanticKITTI & ScanNet datasets. Results reveal that on 2D, using third degree polynomials greatly improves PointConv's robustness to scale changes and rotations, even surpassing traditional 2D CNNs for the MNIST dataset. On 3D datasets, the novel viewpoint-invariant descriptor significantly improves the performance as well as robustness of PointConv. We achieve the state-of-the-art semantic segmentation performance on the SemanticKITTI dataset, as well as comparable performance with the current highest framework on the ScanNet dataset among point-based approaches.

</p>
</details>

<details><summary><b>A survey on shape-constraint deep learning for medical image segmentation</b>
<a href="https://arxiv.org/abs/2101.07721">arxiv:2101.07721</a>
&#x1F4C8; 8 <br>
<p>Simon Bohlender, Ilkay Oksuz, Anirban Mukhopadhyay</p></summary>
<p>

**Abstract:** Since the advent of U-Net, fully convolutional deep neural networks and its many variants have completely changed the modern landscape of deep learning based medical image segmentation. However, the over dependence of these methods on pixel level classification and regression has been identified early on as a problem. Especially when trained on medical databases with sparse available annotation, these methods are prone to generate segmentation artifacts such as fragmented structures, topological inconsistencies and islands of pixel. These artefacts are especially problematic in medical imaging since segmentation is almost always a pre-processing step for some downstream evaluation. The range of possible downstream evaluations is rather big, for example surgical planning, visualization, shape analysis, prognosis, treatment planning etc. However, one common thread across all these downstream tasks is the demand of anatomical consistency. To ensure the segmentation result is anatomically consistent, approaches based on Markov/ Conditional Random Fields, Statistical Shape Models are becoming increasingly popular over the past 5 years. In this review paper, a broad overview of recent literature on bringing anatomical constraints for medical image segmentation is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, important details such as the underlying method, datasets and performance are tabulated.

</p>
</details>

<details><summary><b>Meningioma segmentation in T1-weighted MRI leveraging global context and attention mechanisms</b>
<a href="https://arxiv.org/abs/2101.07715">arxiv:2101.07715</a>
&#x1F4C8; 8 <br>
<p>David Bouget, André Pedersen, Sayied Abdol Mohieb Hosainey, Ole Solheim, Ingerid Reinertsen</p></summary>
<p>

**Abstract:** Meningiomas are the most common type of primary brain tumor, accounting for approximately 30% of all brain tumors. A substantial number of these tumors are never surgically removed but rather monitored over time. Automatic and precise meningioma segmentation is therefore beneficial to enable reliable growth estimation and patient-specific treatment planning. In this study, we propose the inclusion of attention mechanisms over a U-Net architecture: (i) Attention-gated U-Net (AGUNet) and (ii) Dual Attention U-Net (DAUNet), using a 3D MRI volume as input. Attention has the potential to leverage the global context and identify features' relationships across the entire volume. To limit spatial resolution degradation and loss of detail inherent to encoder-decoder architectures, we studied the impact of multi-scale input and deep supervision components. The proposed architectures are trainable end-to-end and each concept can be seamlessly disabled for ablation studies. The validation studies were performed using a 5-fold cross validation over 600 T1-weighted MRI volumes from St. Olavs University Hospital, Trondheim, Norway. For the best performing architecture, an average Dice score of 81.6% was reached for an F1-score of 95.6%. With an almost perfect precision of 98%, meningiomas smaller than 3ml were occasionally missed hence reaching an overall recall of 93%. Leveraging global context from a 3D MRI volume provided the best performances, even if the native volume resolution could not be processed directly. Overall, near-perfect detection was achieved for meningiomas larger than 3ml which is relevant for clinical use. In the future, the use of multi-scale designs and refinement networks should be further investigated to improve the performance. A larger number of cases with meningiomas below 3ml might also be needed to improve the performance for the smallest tumors.

</p>
</details>

<details><summary><b>Predicting Pneumonia and Region Detection from X-Ray Images using Deep Neural Network</b>
<a href="https://arxiv.org/abs/2101.07717">arxiv:2101.07717</a>
&#x1F4C8; 7 <br>
<p>Sheikh Md Hanif Hossain, S M Raju, Amelia Ritahani Ismail</p></summary>
<p>

**Abstract:** Biomedical images are increasing drastically. Along the way, many machine learning algorithms have been proposed to predict and identify various kinds of diseases. One such disease is Pneumonia which is an infection caused by both bacteria and viruses through the inflammation of a person's lung air sacs. In this paper, an algorithm was proposed that receives x-ray images as input and verifies whether this patient is infected by Pneumonia as well as specific region of the lungs that the inflammation has occurred at. The algorithm is based on the transfer learning mechanism where pre-trained ResNet-50 (Convolutional Neural Network) was used followed by some custom layer for making the prediction. The model has achieved an accuracy of 90.6 percent which confirms that the model is effective and can be implemented for the detection of Pneumonia in patients. Furthermore, a class activation map is used for the detection of the infected region in the lungs. Also, PneuNet was developed so that users can access more easily and use the services.

</p>
</details>

<details><summary><b>Machine learning for rapid discovery of laminar flow channel wall modifications that enhance heat transfer</b>
<a href="https://arxiv.org/abs/2101.08130">arxiv:2101.08130</a>
&#x1F4C8; 6 <br>
<p>Matthias Schniewind, Alexander Stroh, Bradley P. Ladewig, Pascal Friederich</p></summary>
<p>

**Abstract:** The calculation of heat transfer in fluid flow in simple flat channels is a relatively easy task for various simulations methods. However, once the channel geometry becomes more complex, numerical simulations become a bottleneck in optimizing wall geometries. We present a combination of accurate numerical simulations of arbitrary, non-flat channels and machine learning models predicting drag coefficient and Stanton number. We show that convolutional neural networks can accurately predict the target properties at a fraction of the time of numerical simulations. We use the CNN models in a virtual high-throughput screening approach to explore a large number of possible, randomly generated wall architectures. We find that S-shaped channel geometries are Pareto-optimal, a result which seems intuitive, but was not obvious before analysing the data. The general approach is not only applicable to simple flow setups as presented here, but can be extended to more complex tasks, such as multiphase or even reactive unit operations in chemical engineering.

</p>
</details>

<details><summary><b>Safe and Efficient Model-free Adaptive Control via Bayesian Optimization</b>
<a href="https://arxiv.org/abs/2101.07825">arxiv:2101.07825</a>
&#x1F4C8; 6 <br>
<p>Christopher König, Matteo Turchetta, John Lygeros, Alisa Rupenyan, Andreas Krause</p></summary>
<p>

**Abstract:** Adaptive control approaches yield high-performance controllers when a precise system model or suitable parametrizations of the controller are available. Existing data-driven approaches for adaptive control mostly augment standard model-based methods with additional information about uncertainties in the dynamics or about disturbances. In this work, we propose a purely data-driven, model-free approach for adaptive control. Tuning low-level controllers based solely on system data raises concerns on the underlying algorithm safety and computational performance. Thus, our approach builds on GoOSE, an algorithm for safe and sample-efficient Bayesian optimization. We introduce several computational and algorithmic modifications in GoOSE that enable its practical use on a rotational motion system. We numerically demonstrate for several types of disturbances that our approach is sample efficient, outperforms constrained Bayesian optimization in terms of safety, and achieves the performance optima computed by grid evaluation. We further demonstrate the proposed adaptive control approach experimentally on a rotational motion system.

</p>
</details>

<details><summary><b>Explainable Patterns: Going from Findings to Insights to Support Data Analytics Democratization</b>
<a href="https://arxiv.org/abs/2101.08655">arxiv:2101.08655</a>
&#x1F4C8; 5 <br>
<p>Leonardo Christino, Martha D. Ferreira, Asal Jalilvand, Fernando V. Paulovich</p></summary>
<p>

**Abstract:** In the past decades, massive efforts involving companies, non-profit organizations, governments, and others have been put into supporting the concept of data democratization, promoting initiatives to educate people to confront information with data. Although this represents one of the most critical advances in our free world, access to data without concrete facts to check or the lack of an expert to help on understanding the existing patterns hampers its intrinsic value and lessens its democratization. So the benefits of giving full access to data will only be impactful if we go a step further and support the Data Analytics Democratization, assisting users in transforming findings into insights without the need of domain experts to promote unconstrained access to data interpretation and verification. In this paper, we present Explainable Patterns (ExPatt), a new framework to support lay users in exploring and creating data storytellings, automatically generating plausible explanations for observed or selected findings using an external (textual) source of information, avoiding or reducing the need for domain experts. ExPatt applicability is confirmed via different use-cases involving world demographics indicators and Wikipedia as an external source of explanations, showing how it can be used in practice towards the data analytics democratization.

</p>
</details>

<details><summary><b>Multi-Task Network Pruning and Embedded Optimization for Real-time Deployment in ADAS</b>
<a href="https://arxiv.org/abs/2101.07831">arxiv:2101.07831</a>
&#x1F4C8; 4 <br>
<p>Flora Dellinger, Thomas Boulay, Diego Mendoza Barrenechea, Said El-Hachimi, Isabelle Leang, Fabian Bürger</p></summary>
<p>

**Abstract:** Camera-based Deep Learning algorithms are increasingly needed for perception in Automated Driving systems. However, constraints from the automotive industry challenge the deployment of CNNs by imposing embedded systems with limited computational resources. In this paper, we propose an approach to embed a multi-task CNN network under such conditions on a commercial prototype platform, i.e. a low power System on Chip (SoC) processing four surround-view fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network architecture. Secondly, a pruning method is applied to compress the CNN, helping to reduce the runtime and memory usage by a factor of 2 without lowering the performances significantly. Finally, several embedded optimization techniques such as mixed-quantization format usage and efficient data transfers between different memory areas are proposed to ensure real-time execution and avoid bandwidth bottlenecks. The approach is evaluated on the hardware platform, considering embedded detection performances, runtime and memory bandwidth. Unlike most works from the literature that focus on classification task, we aim here to study the effect of pruning and quantization on a compact multi-task network with object detection, semantic segmentation and soiling detection tasks.

</p>
</details>

<details><summary><b>Learning over Families of Sets -- Hypergraph Representation Learning for Higher Order Tasks</b>
<a href="https://arxiv.org/abs/2101.07773">arxiv:2101.07773</a>
&#x1F4C8; 4 <br>
<p>Balasubramaniam Srinivasan, Da Zheng, George Karypis</p></summary>
<p>

**Abstract:** Graph representation learning has made major strides over the past decade. However, in many relational domains, the input data are not suited for simple graph representations as the relationships between entities go beyond pairwise interactions. In such cases, the relationships in the data are better represented as hyperedges (set of entities) of a non-uniform hypergraph. While there have been works on principled methods for learning representations of nodes of a hypergraph, these approaches are limited in their applicability to tasks on non-uniform hypergraphs (hyperedges with different cardinalities). In this work, we exploit the incidence structure to develop a hypergraph neural network to learn provably expressive representations of variable sized hyperedges which preserve local-isomorphism in the line graph of the hypergraph, while also being invariant to permutations of its constituent vertices. Specifically, for a given vertex set, we propose frameworks for (1) hyperedge classification and (2) variable sized expansion of partially observed hyperedges which captures the higher order interactions among vertices and hyperedges. We evaluate performance on multiple real-world hypergraph datasets and demonstrate consistent, significant improvement in accuracy, over state-of-the-art models.

</p>
</details>

<details><summary><b>GLocalX -- From Local to Global Explanations of Black Box AI Models</b>
<a href="https://arxiv.org/abs/2101.07685">arxiv:2101.07685</a>
&#x1F4C8; 4 <br>
<p>Mattia Setzu, Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, Fosca Giannotti</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are "black boxes" which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating "local" explanations. We present GLocalX, a "local-first" model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.

</p>
</details>

<details><summary><b>Performance analysis of greedy algorithms for minimising a Maximum Mean Discrepancy</b>
<a href="https://arxiv.org/abs/2101.07564">arxiv:2101.07564</a>
&#x1F4C8; 4 <br>
<p>Luc Pronzato</p></summary>
<p>

**Abstract:** We analyse the performance of several iterative algorithms for the quantisation of a probability measure $μ$, based on the minimisation of a Maximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy MMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the finite-sample-size approximation error, measured by the MMD, decreases as $1/n$ for SBQ and also for kernel herding and greedy MMD minimisation when using a suitable step-size sequence. The upper bound on the approximation error is slightly better for SBQ, but the other methods are significantly faster, with a computational cost that increases only linearly with the number of points selected. This is illustrated by two numerical examples, with the target measure $μ$ being uniform (a space-filling design application) and with $μ$ a Gaussian mixture.

</p>
</details>

<details><summary><b>Variance Based Samples Weighting for Supervised Deep Learning</b>
<a href="https://arxiv.org/abs/2101.07561">arxiv:2101.07561</a>
&#x1F4C8; 4 <br>
<p>Paul Novello, Gaël Poëtte, David Lugato, Pietro Congedo</p></summary>
<p>

**Abstract:** In the context of supervised learning of a function by a Neural Network (NN), we claim and empirically justify that a NN yields better results when the distribution of the data set focuses on regions where the function to learn is steeper. We first traduce this assumption in a mathematically workable way using Taylor expansion. Then, theoretical derivations allow to construct a methodology that we call Variance Based Samples Weighting (VBSW). VBSW uses local variance of the labels to weight the training points. This methodology is general, scalable, cost effective, and significantly increases the performances of a large class of NNs for various classification and regression tasks on image, text and multivariate data.  We highlight its benefits with experiments involving NNs from shallow linear NN to Resnet or Bert.

</p>
</details>

<details><summary><b>The Next Decade of Telecommunications Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2101.09163">arxiv:2101.09163</a>
&#x1F4C8; 3 <br>
<p>Ye Ouyang, Lilei Wang, Aidong Yang, Maulik Shah, David Belanger, Tongqing Gao, Leping Wei, Yaqin Zhang</p></summary>
<p>

**Abstract:** It has been an exciting journey since the mobile communications and artificial intelligence were conceived 37 years and 64 years ago. While both fields evolved independently and profoundly changed communications and computing industries, the rapid convergence of 5G and deep learning is beginning to significantly transform the core communication infrastructure, network management and vertical applications. The paper first outlines the individual roadmaps of mobile communications and artificial intelligence in the early stage, with a concentration to review the era from 3G to 5G when AI and mobile communications started to converge. With regard to telecommunications artificial intelligence, the paper further introduces in detail the progress of artificial intelligence in the ecosystem of mobile communications. The paper then summarizes the classifications of AI in telecom ecosystems along with its evolution paths specified by various international telecommunications standardization bodies. Towards the next decade, the paper forecasts the prospective roadmap of telecommunications artificial intelligence. In line with 3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network intelligence following 3GPP and ORAN routes respectively, experience and intention driven network management and operation, network AI signalling system, intelligent middle-office based BSS, intelligent customer experience management and policy control driven by BSS and OSS convergence, evolution from SLA to ELA, and intelligent private network for verticals. The paper is concluded with the vision that AI will reshape the future B5G or 6G landscape and we need pivot our R&D, standardizations, and ecosystem to fully take the unprecedented opportunities.

</p>
</details>

<details><summary><b>Source-free Domain Adaptation via Distributional Alignment by Matching Batch Normalization Statistics</b>
<a href="https://arxiv.org/abs/2101.10842">arxiv:2101.10842</a>
&#x1F4C8; 2 <br>
<p>Masato Ishii, Masashi Sugiyama</p></summary>
<p>

**Abstract:** In this paper, we propose a novel domain adaptation method for the source-free setting. In this setting, we cannot access source data during adaptation, while unlabeled target data and a model pretrained with source data are given. Due to lack of source data, we cannot directly match the data distributions between domains unlike typical domain adaptation algorithms. To cope with this problem, we propose utilizing batch normalization statistics stored in the pretrained model to approximate the distribution of unobserved source data. Specifically, we fix the classifier part of the model during adaptation and only fine-tune the remaining feature encoder part so that batch normalization statistics of the features extracted by the encoder match those stored in the fixed classifier. Additionally, we also maximize the mutual information between the features and the classifier's outputs to further boost the classification performance. Experimental results with several benchmark datasets show that our method achieves competitive performance with state-of-the-art domain adaptation methods even though it does not require access to source data.

</p>
</details>

<details><summary><b>Personalized Education in the AI Era: What to Expect Next?</b>
<a href="https://arxiv.org/abs/2101.10074">arxiv:2101.10074</a>
&#x1F4C8; 2 <br>
<p>Setareh Maghsudi, Andrew Lan, Jie Xu, Mihaela van der Schaar</p></summary>
<p>

**Abstract:** The objective of personalized learning is to design an effective knowledge acquisition track that matches the learner's strengths and bypasses her weaknesses to ultimately meet her desired goal. This concept emerged several years ago and is being adopted by a rapidly-growing number of educational institutions around the globe. In recent years, the boost of artificial intelligence (AI) and machine learning (ML), together with the advances in big data analysis, has unfolded novel perspectives to enhance personalized education in numerous dimensions. By taking advantage of AI/ML methods, the educational platform precisely acquires the student's characteristics. This is done, in part, by observing the past experiences as well as analyzing the available big data through exploring the learners' features and similarities. It can, for example, recommend the most appropriate content among numerous accessible ones, advise a well-designed long-term curriculum, connect appropriate learners by suggestion, accurate performance evaluation, and the like. Still, several aspects of AI-based personalized education remain unexplored. These include, among others, compensating for the adverse effects of the absence of peers, creating and maintaining motivations for learning, increasing diversity, removing the biases induced by the data and algorithms, and the like. In this paper, while providing a brief review of state-of-the-art research, we investigate the challenges of AI/ML-based personalized education and discuss potential solutions.

</p>
</details>

<details><summary><b>AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID</b>
<a href="https://arxiv.org/abs/2101.08238">arxiv:2101.08238</a>
&#x1F4C8; 2 <br>
<p>Ammarah Farooq, Muhammad Awais, Josef Kittler, Syed Safwan Khalid</p></summary>
<p>

**Abstract:** Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align inter-modality representations according to semantic information present for a person and ignore background information. In this work, we present AXM-Net, a novel CNN based architecture designed for learning semantically aligned visual and textual representations. The underlying building block consists of multiple streams of feature maps coming from visual and textual modalities and a novel learnable context sharing semantic alignment network. We also propose complementary intra modal attention learning mechanisms to focus on more fine-grained local details in the features along with a cross-modal affinity loss for robust feature matching. Our design is unique in its ability to implicitly learn feature alignments from data. The entire AXM-Net can be trained in an end-to-end manner. We report results on both person search and cross-modal Re-ID tasks. Extensive experimentation validates the proposed framework and demonstrates its superiority by outperforming the current state-of-the-art methods by a significant margin.

</p>
</details>

<details><summary><b>Near-Optimal Regret Bounds for Contextual Combinatorial Semi-Bandits with Linear Payoff Functions</b>
<a href="https://arxiv.org/abs/2101.07957">arxiv:2101.07957</a>
&#x1F4C8; 2 <br>
<p>Kei Takemura, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro Fukunaga, Naonori Kakimura, Ken-ichi Kawarabayashi</p></summary>
<p>

**Abstract:** The contextual combinatorial semi-bandit problem with linear payoff functions is a decision-making problem in which a learner chooses a set of arms with the feature vectors in each round under given constraints so as to maximize the sum of rewards of arms. Several existing algorithms have regret bounds that are optimal with respect to the number of rounds $T$. However, there is a gap of $\tilde{O}(\max(\sqrt{d}, \sqrt{k}))$ between the current best upper and lower bounds, where $d$ is the dimension of the feature vectors, $k$ is the number of the chosen arms in a round, and $\tilde{O}(\cdot)$ ignores the logarithmic factors. The dependence of $k$ and $d$ is of practical importance because $k$ may be larger than $T$ in real-world applications such as recommender systems. In this paper, we fill the gap by improving the upper and lower bounds. More precisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu (2014) has the optimal regret bound $\tilde{O}(d\sqrt{kT} + dk)$ for the partition matroid constraints. For general constraints, we propose an algorithm that modifies the reward estimates of arms in the C${}^2$UCB algorithm and demonstrate that it enjoys the optimal regret bound for a more general problem that can take into account other objectives simultaneously. We also show that our technique would be applicable to related problems. Numerical experiments support our theoretical results and considerations.

</p>
</details>

<details><summary><b>WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation Track</b>
<a href="https://arxiv.org/abs/2101.07947">arxiv:2101.07947</a>
&#x1F4C8; 2 <br>
<p>Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng, Jie Zhou</p></summary>
<p>

**Abstract:** We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara et al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2 (Interactive Dialogue). In sub-task 1, we employ a pre-trained language model to generate topic-related responses and propose a response ensemble method for response selection. In sub-task2, we propose a novel Dialogue Planning Model (DPM) to capture conversation flow in the interaction with humans. We also design an integrated open-domain dialogue system containing pre-process, dialogue model, scoring model, and post-process, which can generate fluent, coherent, consistent, and humanlike responses. We tie 1st on human ratings and also get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on interactive human evaluation in sub-task 2.

</p>
</details>

<details><summary><b>PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer</b>
<a href="https://arxiv.org/abs/2101.07918">arxiv:2101.07918</a>
&#x1F4C8; 2 <br>
<p>HongChien Yu, Zhuyun Dai, Jamie Callan</p></summary>
<p>

**Abstract:** Most research on pseudo relevance feedback (PRF) has been done in vector space and probabilistic retrieval models. This paper shows that Transformer-based rerankers can also benefit from the extra context that PRF provides. It presents PGT, a graph-based Transformer that sparsifies attention between graph nodes to enable PRF while avoiding the high computational complexity of most Transformer architectures. Experiments show that PGT improves upon non-PRF Transformer reranker, and it is at least as accurate as Transformer PRF models that use full attention, but with lower computational costs.

</p>
</details>

<details><summary><b>Scalable Optimization for Wind Farm Control using Coordination Graphs</b>
<a href="https://arxiv.org/abs/2101.07844">arxiv:2101.07844</a>
&#x1F4C8; 2 <br>
<p>Timothy Verstraeten, Pieter-Jan Daems, Eugenio Bargiacchi, Diederik M. Roijers, Pieter J. K. Libin, Jan Helsen</p></summary>
<p>

**Abstract:** Wind farms are a crucial driver toward the generation of ecological and renewable energy. Due to their rapid increase in capacity, contemporary wind farms need to adhere to strict constraints on power output to ensure stability of the electricity grid. Specifically, a wind farm controller is required to match the farm's power production with a power demand imposed by the grid operator. This is a non-trivial optimization problem, as complex dependencies exist between the wind turbines. State-of-the-art wind farm control typically relies on physics-based heuristics that fail to capture the full load spectrum that defines a turbine's health status. When this is not taken into account, the long-term viability of the farm's turbines is put at risk. Given the complex dependencies that determine a turbine's lifetime, learning a flexible and optimal control strategy requires a data-driven approach. However, as wind farms are large-scale multi-agent systems, optimizing control strategies over the full joint action space is intractable. We propose a new learning method for wind farm control that leverages the sparse wind farm structure to factorize the optimization problem. Using a Bayesian approach, based on multi-agent Thompson sampling, we explore the factored joint action space for configurations that match the demand, while considering the lifetime of turbines. We apply our method to a grid-like wind farm layout, and evaluate configurations using a state-of-the-art wind flow simulator. Our results are competitive with a physics-based heuristic approach in terms of demand error, while, contrary to the heuristic, our method prolongs the lifetime of high-risk turbines.

</p>
</details>

<details><summary><b>Internet of Predictable Things (IoPT) Framework to Increase Cyber-Physical System Resiliency</b>
<a href="https://arxiv.org/abs/2101.07816">arxiv:2101.07816</a>
&#x1F4C8; 2 <br>
<p>Umit Cali, Murat Kuzlu, Vinayak Sharma, Manisa Pipattanasomporn, Ferhat Ozgur Catak</p></summary>
<p>

**Abstract:** During the last two decades, distributed energy systems, especially renewable energy sources (RES), have become more economically viable with increasing market share and penetration levels on power systems. In addition to decarbonization and decentralization of energy systems, digitalization has also become very important. The use of artificial intelligence (AI), advanced optimization algorithms, Industrial Internet of Things (IIoT), and other digitalization frameworks makes modern power system assets more intelligent, while vulnerable to cybersecurity risks. This paper proposes the concept of the Internet of Predictable Things (IoPT) that incorporates advanced data analytics and machine learning methods to increase the resiliency of cyber-physical systems against cybersecurity risks. The proposed concept is demonstrated using a cyber-physical system testbed under a variety of cyber attack scenarios as a proof of concept (PoC).

</p>
</details>

<details><summary><b>Multi-target detection with rotations</b>
<a href="https://arxiv.org/abs/2101.07709">arxiv:2101.07709</a>
&#x1F4C8; 2 <br>
<p>Tamir Bendory, Ti-Yen Lan, Nicholas F. Marshall, Iris Rukshin, Amit Singer</p></summary>
<p>

**Abstract:** We consider the multi-target detection problem of estimating a two-dimensional target image from a large noisy measurement image that contains many randomly rotated and translated copies of the target image. Motivated by single-particle cryo-electron microscopy, we focus on the low signal-to-noise regime, where it is difficult to estimate the locations and orientations of the target images in the measurement. Our approach uses autocorrelation analysis to estimate rotationally and translationally invariant features of the target image. We demonstrate that, regardless of the level of noise, our technique can be used to recover the target image when the measurement is sufficiently large.

</p>
</details>

<details><summary><b>Obsolete Personal Information Update System for the Prevention of Falls among Elderly Patients</b>
<a href="https://arxiv.org/abs/2101.10132">arxiv:2101.10132</a>
&#x1F4C8; 1 <br>
<p>Salma Chaieb, Brahim Hnich, Ali Ben Mrad</p></summary>
<p>

**Abstract:** Falls are a common problem affecting the older adults and a major public health issue. Centers for Disease Control and Prevention, and World Health Organization report that one in three adults over the age of 65 and half of the adults over 80 fall each year. In recent years, an ever-increasing range of applications have been developed to help deliver more effective falls prevention interventions. All these applications rely on a huge elderly personal database collected from hospitals, mutual health, and other organizations in caring for elderly. The information describing an elderly is continually evolving and may become obsolete at a given moment and contradict what we already know on the same person. So, it needs to be continuously checked and updated in order to restore the database consistency and then provide better service. This paper provides an outline of an Obsolete personal Information Update System (OIUS) designed in the context of the elderly-fall prevention project. Our OIUS aims to control and update in real-time the information acquired about each older adult, provide on-demand consistent information and supply tailored interventions to caregivers and fall-risk patients. The approach outlined for this purpose is based on a polynomial-time algorithm build on top of a causal Bayesian network representing the elderly data. The result is given as a recommendation tree with some accuracy level. We conduct a thorough empirical study for such a model on an elderly personal information base. Experiments confirm the viability and effectiveness of our OIUS.

</p>
</details>

<details><summary><b>On Provable Backdoor Defense in Collaborative Learning</b>
<a href="https://arxiv.org/abs/2101.08177">arxiv:2101.08177</a>
&#x1F4C8; 1 <br>
<p>Ximing Qiao, Yuhua Bai, Siping Hu, Ang Li, Yiran Chen, Hai Li</p></summary>
<p>

**Abstract:** As collaborative learning allows joint training of a model using multiple sources of data, the security problem has been a central concern. Malicious users can upload poisoned data to prevent the model's convergence or inject hidden backdoors. The so-called backdoor attacks are especially difficult to detect since the model behaves normally on standard test data but gives wrong outputs when triggered by certain backdoor keys. Although Byzantine-tolerant training algorithms provide convergence guarantee, provable defense against backdoor attacks remains largely unsolved. Methods based on randomized smoothing can only correct a small number of corrupted pixels or labels; methods based on subset aggregation cause a severe drop in classification accuracy due to low data utilization. We propose a novel framework that generalizes existing subset aggregation methods. The framework shows that the subset selection process, a deciding factor for subset aggregation methods, can be viewed as a code design problem. We derive the theoretical bound of data utilization ratio and provide optimal code construction. Experiments on non-IID versions of MNIST and CIFAR-10 show that our method with optimal codes significantly outperforms baselines using non-overlapping partition and random selection. Additionally, integration with existing coding theory results shows that special codes can track the location of the attackers. Such capability provides new countermeasures to backdoor attacks.

</p>
</details>

<details><summary><b>Improving type information inferred by decompilers with supervised machine learning</b>
<a href="https://arxiv.org/abs/2101.08116">arxiv:2101.08116</a>
&#x1F4C8; 1 <br>
<p>Javier Escalada, Ted Scully, Francisco Ortin</p></summary>
<p>

**Abstract:** In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.

</p>
</details>

<details><summary><b>Quarter Laplacian Filter for Edge Aware Image Processing</b>
<a href="https://arxiv.org/abs/2101.07933">arxiv:2101.07933</a>
&#x1F4C8; 1 <br>
<p>Yuanhao Gong, Wenming Tang, Lebin Zhou, Lantao Yu, Guoping Qiu</p></summary>
<p>

**Abstract:** This paper presents a quarter Laplacian filter that can preserve corners and edges during image smoothing. Its support region is $2\times2$, which is smaller than the $3\times3$ support region of Laplacian filter. Thus, it is more local. Moreover, this filter can be implemented via the classical box filter, leading to high performance for real time applications. Finally, we show its edge preserving property in several image processing tasks, including image smoothing, texture enhancement, and low-light image enhancement. The proposed filter can be adopted in a wide range of image processing applications.

</p>
</details>

<details><summary><b>SEMULATOR: Emulating the Dynamics of Crossbar Array-based Analog Neural System with Regression Neural Networks</b>
<a href="https://arxiv.org/abs/2101.07864">arxiv:2101.07864</a>
&#x1F4C8; 1 <br>
<p>Chaeun Lee, Seyoung Kim</p></summary>
<p>

**Abstract:** As deep neural networks require tremendous amount of computation and memory, analog computing with emerging memory devices is a promising alternative to digital computing for edge devices. However, because of the increasing simulation time for analog computing system, it has not been explored. To overcome this issue, analytically approximated simulators are developed, but these models are inaccurate and narrow down the options for peripheral circuits for multiply-accumulate operation (MAC). In this sense, we propose a methodology, SEMULATOR (SiMULATOR by Emulating the analog computing block) which uses a deep neural network to emulate the behavior of crossbar-based analog computing system. With the proposed neural architecture, we experimentally and theoretically shows that it emulates a MAC unit for neural computation. In addition, the simulation time is incomparably reduced when it compared to the circuit simulators such as SPICE.

</p>
</details>

<details><summary><b>Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice Cardiac MR Images by Incorporating Pretrained Task Networks</b>
<a href="https://arxiv.org/abs/2101.07653">arxiv:2101.07653</a>
&#x1F4C8; 1 <br>
<p>Sven Koehler, Tarique Hussain, Zach Blair, Tyler Huffaker, Florian Ritzmann, Animesh Tandon, Thomas Pickardt, Samir Sarikouch, Heiner Latus, Gerald Greil, Ivo Wolf, Sandy Engelhardt</p></summary>
<p>

**Abstract:** Anisotropic multi-slice Cardiac Magnetic Resonance (CMR) Images are conventionally acquired in patient-specific short-axis (SAX) orientation. In specific cardiovascular diseases that affect right ventricular (RV) morphology, acquisitions in standard axial (AX) orientation are preferred by some investigators, due to potential superiority in RV volume measurement for treatment planning. Unfortunately, due to the rare occurrence of these diseases, data in this domain is scarce. Recent research in deep learning-based methods mainly focused on SAX CMR images and they had proven to be very successful. In this work, we show that there is a considerable domain shift between AX and SAX images, and therefore, direct application of existing models yield sub-optimal results on AX samples. We propose a novel unsupervised domain adaptation approach, which uses task-related probabilities in an attention mechanism. Beyond that, cycle consistency is imposed on the learned patient-individual 3D rigid transformation to improve stability when automatically re-sampling the AX images to SAX orientations. The network was trained on 122 registered 3D AX-SAX CMR volume pairs from a multi-centric patient cohort. A mean 3D Dice of $0.86\pm{0.06}$ for the left ventricle, $0.65\pm{0.08}$ for the myocardium, and $0.77\pm{0.10}$ for the right ventricle could be achieved. This is an improvement of $25\%$ in Dice for RV in comparison to direct application on axial slices. To conclude, our pre-trained task module has neither seen CMR images nor labels from the target domain, but is able to segment them after the domain gap is reduced. Code: https://github.com/Cardio-AI/3d-mri-domain-adaptation

</p>
</details>

<details><summary><b>Trading Transforms of Non-weighted Simple Games and Integer Weights of Weighted Simple Games</b>
<a href="https://arxiv.org/abs/2101.07621">arxiv:2101.07621</a>
&#x1F4C8; 1 <br>
<p>Akihiro Kawana, Tomomi Matsui</p></summary>
<p>

**Abstract:** This study investigates simple games. A fundamental research question in this field is to determine necessary and sufficient conditions for a simple game to be a weighted majority game. Taylor and Zwicker (1992) showed that a simple game is non-weighted if and only if there exists a trading transform of finite size. They also provided an upper bound on the size of such a trading transform, if it exists. Gvozdeva and Slinko (2011) improved that upper bound; their proof employed a property of linear inequalities demonstrated by Muroga (1971).In this study, we provide a new proof of the existence of a trading transform when a given simple game is non-weighted. Our proof employs Farkas' lemma (1894), and yields an improved upper bound on the size of a trading transform.
  We also discuss an integer-weight representation of a weighted simple game, improving the bounds obtained by Muroga (1971). We show that our bound on the quota is tight when the number of players is less than or equal to five, based on the computational results obtained by Kurz (2012).
  Furthermore, we discuss the problem of finding an integer-weight representation under the assumption that we have minimal winning coalitions and maximal losing coalitions.In particular, we show a performance of a rounding method.
  Lastly, we address roughly weighted simple games. Gvozdeva and Slinko (2011) showed that a given simple game is not roughly weighted if and only if there exists a potent certificate of non-weightedness. We give an upper bound on the length of a potent certificate of non-weightedness. We also discuss an integer-weight representation of a roughly weighted simple game.

</p>
</details>

<details><summary><b>A Lightweight Structure Aimed to Utilize Spatial Correlation for Sparse-View CT Reconstruction</b>
<a href="https://arxiv.org/abs/2101.07613">arxiv:2101.07613</a>
&#x1F4C8; 1 <br>
<p>Yitong Liu, Ken Deng, Chang Sun, Hongwen Yang</p></summary>
<p>

**Abstract:** Sparse-view computed tomography (CT) is known as a widely used approach to reduce radiation dose while accelerating imaging through lowered projection views and correlated calculations. However, its severe imaging noise and streaking artifacts turn out to be a major issue in the low dose protocol. In this paper, we propose a dual-domain deep learning-based method that breaks through the limitations of currently prevailing algorithms that merely process single image slices. Since the scanned object usually contains a high degree of spatial continuity, the obtained consecutive imaging slices embody rich information that is largely unexplored. Therefore, we establish a cascade model named LS-AAE which aims to tackle the above problem. In addition, in order to adapt to the social trend of lightweight medical care, our model adopts the inverted residual with linear bottleneck in the module design to make it mobile and lightweight (reduce model parameters to one-eighth of its original) without sacrificing its performance. In our experiments, sparse sampling is conducted at intervals of 4°, 8° and 16°, which appears to be a challenging sparsity that few scholars have attempted before. Nevertheless, our method still exhibits its robustness and achieves the state-of-the-art performance by reaching the PSNR of 40.305 and the SSIM of 0.948, while ensuring high model mobility. Particularly, it still exceeds other current methods when the sampling rate is one-fourth of them, thereby demonstrating its remarkable superiority.

</p>
</details>

<details><summary><b>Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain Based on Spatial Information</b>
<a href="https://arxiv.org/abs/2101.07594">arxiv:2101.07594</a>
&#x1F4C8; 1 <br>
<p>Ken Deng, Chang Sun, Yitong Liu, Hongwen Yang</p></summary>
<p>

**Abstract:** Low-dose Computed Tomography is a common issue in reality. Current reduction, sparse sampling and limited-view scanning can all cause it. Between them, limited-view CT is general in the industry due to inevitable mechanical and physical limitation. However, limited-view CT can cause serious imaging problem on account of its massive information loss. Thus, we should effectively utilize the scant prior information to perform completion. It is an undeniable fact that CT imaging slices are extremely dense, which leads to high continuity between successive images. We realized that fully exploit the spatial correlation between consecutive frames can significantly improve restoration results in video inpainting. Inspired by this, we propose a deep learning-based three-stage algorithm that hoist limited-view CT imaging quality based on spatial information. In stage one, to better utilize prior information in the Radon domain, we design an adversarial autoencoder to complement the Radon data. In the second stage, a model is built to perform inpainting based on spatial continuity in the image domain. At this point, we have roughly restored the imaging, while its texture still needs to be finely repaired. Hence, we propose a model to accurately restore the image in stage three, and finally achieve an ideal inpainting result. In addition, we adopt FBP instead of SART-TV to make our algorithm more suitable for real-time use. In the experiment, we restore and reconstruct the Radon data that has been cut the rear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while precisely present the texture.

</p>
</details>

<details><summary><b>Intelligent Frame Selection as a Privacy-Friendlier Alternative to Face Recognition</b>
<a href="https://arxiv.org/abs/2101.07529">arxiv:2101.07529</a>
&#x1F4C8; 1 <br>
<p>Mattijs Baert, Sam Leroux, Pieter Simoens</p></summary>
<p>

**Abstract:** The widespread deployment of surveillance cameras for facial recognition gives rise to many privacy concerns. This study proposes a privacy-friendly alternative to large scale facial recognition. While there are multiple techniques to preserve privacy, our work is based on the minimization principle which implies minimizing the amount of collected personal data. Instead of running facial recognition software on all video data, we propose to automatically extract a high quality snapshot of each detected person without revealing his or her identity. This snapshot is then encrypted and access is only granted after legal authorization. We introduce a novel unsupervised face image quality assessment method which is used to select the high quality snapshots. For this, we train a variational autoencoder on high quality face images from a publicly available dataset and use the reconstruction probability as a metric to estimate the quality of each face crop. We experimentally confirm that the reconstruction probability can be used as biometric quality predictor. Unlike most previous studies, we do not rely on a manually defined face quality metric as everything is learned from data. Our face quality assessment method outperforms supervised, unsupervised and general image quality assessment methods on the task of improving face verification performance by rejecting low quality images. The effectiveness of the whole system is validated qualitatively on still images and videos.

</p>
</details>

<details><summary><b>PConv: Simple yet Effective Convolutional Layer for Generative Adversarial Network</b>
<a href="https://arxiv.org/abs/2101.10841">arxiv:2101.10841</a>
&#x1F4C8; 0 <br>
<p>Seung Park, Yoon-Jae Yeo, Yong-Goo Shin</p></summary>
<p>

**Abstract:** This paper presents a novel convolutional layer, called perturbed convolution (PConv), which focuses on achieving two goals simultaneously: improving the generative adversarial network (GAN) performance and alleviating the memorization problem in which the discriminator memorizes all images from a given dataset as training progresses. In PConv, perturbed features are generated by randomly disturbing an input tensor before performing the convolution operation. This approach is simple but surprisingly effective. First, to produce a similar output even with the perturbed tensor, each layer in the discriminator should learn robust features having a small local Lipschitz value. Second, since the input tensor is randomly perturbed during the training procedure like the dropout in neural networks, the memorization problem could be alleviated. To show the generalization ability of the proposed method, we conducted extensive experiments with various loss functions and datasets including CIFAR-10, CelebA, CelebA-HQ, LSUN, and tiny-ImageNet. The quantitative evaluations demonstrate that PConv effectively boosts the performance of GAN and conditional GAN in terms of Frechet inception distance (FID).

</p>
</details>

<details><summary><b>Autocart -- spatially-aware regression trees for ecological and spatial modeling</b>
<a href="https://arxiv.org/abs/2101.08258">arxiv:2101.08258</a>
&#x1F4C8; 0 <br>
<p>Ethan Ancell, Brennan Bean</p></summary>
<p>

**Abstract:** Many ecological and spatial processes are complex in nature and are not accurately modeled by linear models. Regression trees promise to handle the high-order interactions that are present in ecological and spatial datasets, but fail to produce physically realistic characterizations of the underlying landscape. The "autocart" (autocorrelated regression trees) R package extends the functionality of previously proposed spatial regression tree methods through a spatially aware splitting function and novel adaptive inverse distance weighting method in each terminal node. The efficacy of these autocart models, including an autocart extension of random forest, is demonstrated on multiple datasets. This highlights the ability of autocart to model complex interactions between spatial variables while still providing physically realistic representations of the landscape.

</p>
</details>

<details><summary><b>Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2101.07525">arxiv:2101.07525</a>
&#x1F4C8; 0 <br>
<p>Zeming Li, Songtao Liu, Jian Sun</p></summary>
<p>

**Abstract:** In this paper, we present a novel approach, Momentum$^2$ Teacher, for student-teacher based self-supervised learning. The approach performs momentum update on both network weights and batch normalization (BN) statistics. The teacher's weight is a momentum update of the student, and the teacher's BN statistics is a momentum update of those in history. The Momentum$^2$ Teacher is simple and efficient. It can achieve the state of the art results (74.5\%) under ImageNet linear evaluation protocol using small-batch size(\eg, 128), without requiring large-batch training on special hardware like TPU or inefficient across GPU operation (\eg, shuffling BN, synced BN). Our implementation and pre-trained models will be given on GitHub\footnote{https://github.com/zengarden/momentum2-teacher}.

</p>
</details>


{% endraw %}
Prev: [2021.01.18]({{ '/2021/01/18/2021.01.18.html' | relative_url }})  Next: [2021.01.20]({{ '/2021/01/20/2021.01.20.html' | relative_url }})