Prev: [2022.11.24]({{ '/2022/11/24/2022.11.24.html' | relative_url }})  Next: [2022.11.26]({{ '/2022/11/26/2022.11.26.html' | relative_url }})
{% raw %}
## Summary for 2022-11-25, created on 2022-11-29


<details><summary><b>SpaText: Spatio-Textual Representation for Controllable Image Generation</b>
<a href="https://arxiv.org/abs/2211.14305">arxiv:2211.14305</a>
&#x1F4C8; 39 <br>
<p>Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin</p></summary>
<p>

**Abstract:** Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.

</p>
</details>

<details><summary><b>A Note on Model-Free Reinforcement Learning with the Decision-Estimation Coefficient</b>
<a href="https://arxiv.org/abs/2211.14250">arxiv:2211.14250</a>
&#x1F4C8; 23 <br>
<p>Dylan J. Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, Ayush Sekhari</p></summary>
<p>

**Abstract:** We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this note, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation.

</p>
</details>

<details><summary><b>Fault-Tolerant Offline Multi-Agent Path Planning</b>
<a href="https://arxiv.org/abs/2211.13908">arxiv:2211.13908</a>
&#x1F4C8; 15 <br>
<p>Keisuke Okumura, Sébastien Tixeuil</p></summary>
<p>

**Abstract:** We study a novel graph path planning problem for multiple agents that may crash at runtime, and block part of the workspace. In our setting, agents can detect neighboring crashed agents, and change followed paths at runtime. The objective is then to prepare a set of paths and switching rules for each agent, ensuring that all correct agents reach their destinations without collisions or deadlocks, despite unforeseen crashes of other agents. Such planning is attractive to build reliable multi-robot systems. We present problem formalization, theoretical analysis such as computational complexities, and how to solve this offline planning problem.

</p>
</details>

<details><summary><b>Operator Splitting Value Iteration</b>
<a href="https://arxiv.org/abs/2211.13937">arxiv:2211.13937</a>
&#x1F4C8; 7 <br>
<p>Amin Rakhsha, Andrew Wang, Mohammad Ghavamzadeh, Amir-massoud Farahmand</p></summary>
<p>

**Abstract:** We introduce new planning and reinforcement learning algorithms for discounted MDPs that utilize an approximate model of the environment to accelerate the convergence of the value function. Inspired by the splitting approach in numerical linear algebra, we introduce Operator Splitting Value Iteration (OS-VI) for both Policy Evaluation and Control problems. OS-VI achieves a much faster convergence rate when the model is accurate enough. We also introduce a sample-based version of the algorithm called OS-Dyna. Unlike the traditional Dyna architecture, OS-Dyna still converges to the correct value function in presence of model approximation error.

</p>
</details>

<details><summary><b>A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation</b>
<a href="https://arxiv.org/abs/2211.14296">arxiv:2211.14296</a>
&#x1F4C8; 6 <br>
<p>Hiroki Furuta, Yusuke Iwasawa, Yutaka Matsuo, Shixiang Shane Gu</p></summary>
<p>

**Abstract:** The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.

</p>
</details>

<details><summary><b>Solving math word problems with process- and outcome-based feedback</b>
<a href="https://arxiv.org/abs/2211.14275">arxiv:2211.14275</a>
&#x1F4C8; 5 <br>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins</p></summary>
<p>

**Abstract:** Recent work has shown that asking language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education. We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use process-based supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from 16.8% $\to$ 12.7% final-answer error and 14.0% $\to$ 3.4% reasoning error among final-answer-correct solutions.

</p>
</details>

<details><summary><b>Cross-Domain Ensemble Distillation for Domain Generalization</b>
<a href="https://arxiv.org/abs/2211.14058">arxiv:2211.14058</a>
&#x1F4C8; 5 <br>
<p>Kyungmoon Lee, Sungyeon Kim, Suha Kwak</p></summary>
<p>

**Abstract:** Domain generalization is the task of learning models that generalize to unseen target domains. We propose a simple yet effective method for domain generalization, named cross-domain ensemble distillation (XDED), that learns domain-invariant features while encouraging the model to converge to flat minima, which recently turned out to be a sufficient condition for domain generalization. To this end, our method generates an ensemble of the output logits from training data with the same label but from different domains and then penalizes each output for the mismatch with the ensemble. Also, we present a de-stylization technique that standardizes features to encourage the model to produce style-consistent predictions even in an arbitrary target domain. Our method greatly improves generalization capability in public benchmarks for cross-domain image classification, cross-dataset person re-ID, and cross-dataset semantic segmentation. Moreover, we show that models learned by our method are robust against adversarial attacks and image corruptions.

</p>
</details>

<details><summary><b>Collection and Evaluation of a Long-Term 4D Agri-Robotic Dataset</b>
<a href="https://arxiv.org/abs/2211.14013">arxiv:2211.14013</a>
&#x1F4C8; 5 <br>
<p>Riccardo Polvara, Sergi Molina Mellado, Ibrahim Hroob, Grzegorz Cielniak, Marc Hanheide</p></summary>
<p>

**Abstract:** Long-term autonomy is one of the most demanded capabilities looked into a robot. The possibility to perform the same task over and over on a long temporal horizon, offering a high standard of reproducibility and robustness, is appealing. Long-term autonomy can play a crucial role in the adoption of robotics systems for precision agriculture, for example in assisting humans in monitoring and harvesting crops in a large orchard. With this scope in mind, we report an ongoing effort in the long-term deployment of an autonomous mobile robot in a vineyard for data collection across multiple months. The main aim is to collect data from the same area at different points in time so to be able to analyse the impact of the environmental changes in the mapping and localisation tasks. In this work, we present a map-based localisation study taking 4 data sessions. We identify expected failures when the pre-built map visually differs from the environment's current appearance and we anticipate LTS-Net, a solution pointed at extracting stable temporal features for improving long-term 4D localisation results.

</p>
</details>

<details><summary><b>Generating 2D and 3D Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution</b>
<a href="https://arxiv.org/abs/2211.13964">arxiv:2211.13964</a>
&#x1F4C8; 5 <br>
<p>Tomer Friedlander, Ron Shmelkin, Lior Wolf</p></summary>
<p>

**Abstract:** A master face is a face image that passes face-based identity authentication for a high percentage of the population. These faces can be used to impersonate, with a high probability of success, any user, without having access to any user information. We optimize these faces for 2D and 3D face verification models, by using an evolutionary algorithm in the latent embedding space of the StyleGAN face generator. For 2D face verification, multiple evolutionary strategies are compared, and we propose a novel approach that employs a neural network to direct the search toward promising samples, without adding fitness evaluations. The results we present demonstrate that it is possible to obtain a considerable coverage of the identities in the LFW or RFW datasets with less than 10 master faces, for six leading deep face recognition systems. In 3D, we generate faces using the 2D StyleGAN2 generator and predict a 3D structure using a deep 3D face reconstruction network. When employing two different 3D face recognition systems, we are able to obtain a coverage of 40%-50%. Additionally, we present the generation of paired 2D RGB and 3D master faces, which simultaneously match 2D and 3D models with high impersonation rates.

</p>
</details>

<details><summary><b>Analysis of Error Feedback in Federated Non-Convex Optimization with Biased Compression</b>
<a href="https://arxiv.org/abs/2211.14292">arxiv:2211.14292</a>
&#x1F4C8; 4 <br>
<p>Xiaoyun Li, Ping Li</p></summary>
<p>

**Abstract:** In federated learning (FL) systems, e.g., wireless networks, the communication cost between the clients and the central server can often be a bottleneck. To reduce the communication cost, the paradigm of communication compression has become a popular strategy in the literature. In this paper, we focus on biased gradient compression techniques in non-convex FL problems. In the classical setting of distributed learning, the method of error feedback (EF) is a common technique to remedy the downsides of biased gradient compression. In this work, we study a compressed FL scheme equipped with error feedback, named Fed-EF. We further propose two variants: Fed-EF-SGD and Fed-EF-AMS, depending on the choice of the global model optimizer. We provide a generic theoretical analysis, which shows that directly applying biased compression in FL leads to a non-vanishing bias in the convergence rate. The proposed Fed-EF is able to match the convergence rate of the full-precision FL counterparts under data heterogeneity with a linear speedup.
  Moreover, we develop a new analysis of the EF under partial client participation, which is an important scenario in FL. We prove that under partial participation, the convergence rate of Fed-EF exhibits an extra slow-down factor due to a so-called ``stale error compensation'' effect. A numerical study is conducted to justify the intuitive impact of stale error accumulation on the norm convergence of Fed-EF under partial participation. Finally, we also demonstrate that incorporating the two-way compression in Fed-EF does not change the convergence results. In summary, our work conducts a thorough analysis of the error feedback in federated non-convex optimization. Our analysis with partial client participation also provides insights on a theoretical limitation of the error feedback mechanism, and possible directions for improvements.

</p>
</details>

<details><summary><b>Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing</b>
<a href="https://arxiv.org/abs/2211.14227">arxiv:2211.14227</a>
&#x1F4C8; 4 <br>
<p>Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, Danyang Zhuo</p></summary>
<p>

**Abstract:** Over the last decade, deep neural networks have transformed our society, and they are already widely applied in various machine learning applications. State-of-art deep neural networks are becoming larger in size every year to deliver increasing model accuracy, and as a result, model training consumes substantial computing resources and will only consume more in the future. Using current training methods, in each iteration, to process a data point $x \in \mathbb{R}^d$ in a layer, we need to spend $Θ(md)$ time to evaluate all the $m$ neurons in the layer. This means processing the entire layer takes $Θ(nmd)$ time for $n$ data points. Recent work [Song, Yang and Zhang, NeurIPS 2021] reduces this time per iteration to $o(nmd)$, but requires exponential time to preprocess either the data or the neural network weights, making it unlikely to have practical usage.
  In this work, we present a new preprocessing method that simply stores the weight-data correlation in a tree data structure in order to quickly, dynamically detect which neurons fire at each iteration. Our method requires only $O(nmd)$ time in preprocessing and still achieves $o(nmd)$ time per iteration. We complement our new algorithm with a lower bound, proving that assuming a popular conjecture from complexity theory, one could not substantially speed up our algorithm for dynamic detection of firing neurons.

</p>
</details>

<details><summary><b>Inverse Solvability and Security with Applications to Federated Learning</b>
<a href="https://arxiv.org/abs/2211.14115">arxiv:2211.14115</a>
&#x1F4C8; 4 <br>
<p>Tomasz Piotrowski, Matthias Frey, Renato L. G. Cavalcante, Rafail Ismailov</p></summary>
<p>

**Abstract:** We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.

</p>
</details>

<details><summary><b>An Ensemble-Based Deep Framework for Estimating Thermo-Chemical State Variables from Flamelet Generated Manifolds</b>
<a href="https://arxiv.org/abs/2211.14098">arxiv:2211.14098</a>
&#x1F4C8; 4 <br>
<p>Amol Salunkhe, Georgios Georgalis, Abani Patra, Varun Chandola</p></summary>
<p>

**Abstract:** Complete computation of turbulent combustion flow involves two separate steps: mapping reaction kinetics to low-dimensional manifolds and looking-up this approximate manifold during CFD run-time to estimate the thermo-chemical state variables. In our previous work, we showed that using a deep architecture to learn the two steps jointly, instead of separately, is 73% more accurate at estimating the source energy, a key state variable, compared to benchmarks and can be integrated within a DNS turbulent combustion framework. In their natural form, such deep architectures do not allow for uncertainty quantification of the quantities of interest: the source energy and key species source terms. In this paper, we expand on such architectures, specifically ChemTab, by introducing deep ensembles to approximate the posterior distribution of the quantities of interest. We investigate two strategies of creating these ensemble models: one that keeps the flamelet origin information (Flamelets strategy) and one that ignores the origin and considers all the data independently (Points strategy). To train these models we used flamelet data generated by the GRI--Mech 3.0 methane mechanism, which consists of 53 chemical species and 325 reactions. Our results demonstrate that the Flamelets strategy is superior in terms of the absolute prediction error for the quantities of interest, but is reliant on the types of flamelets used to train the ensemble. The Points strategy is best at capturing the variability of the quantities of interest, independent of the flamelet types. We conclude that, overall, ChemTab Deep Ensembles allows for a more accurate representation of the source energy and key species source terms, compared to the model without these modifications.

</p>
</details>

<details><summary><b>Detecting broken Absorber Tubes in CSP plants using intelligent sampling and dual loss</b>
<a href="https://arxiv.org/abs/2211.14077">arxiv:2211.14077</a>
&#x1F4C8; 4 <br>
<p>Miguel Angel Pérez-Cutiño, Juan Sebastián Valverde, José Miguel Díaz-Báñez</p></summary>
<p>

**Abstract:** Concentrated solar power (CSP) is one of the growing technologies that is leading the process of changing from fossil fuels to renewable energies. The sophistication and size of the systems require an increase in maintenance tasks to ensure reliability, availability, maintainability and safety. Currently, automatic fault detection in CSP plants using Parabolic Trough Collector systems evidences two main drawbacks: 1) the devices in use needs to be manually placed near the receiver tube, 2) the Machine Learning-based solutions are not tested in real plants. We address both gaps by combining the data extracted with the use of an Unmaned Aerial Vehicle, and the data provided by sensors placed within 7 real plants. The resulting dataset is the first one of this type and can help to standardize research activities for the problem of fault detection in this type of plants. Our work proposes supervised machine-learning algorithms for detecting broken envelopes of the absorber tubes in CSP plants. The proposed solution takes the class imbalance problem into account, boosting the accuracy of the algorithms for the minority class without harming the overall performance of the models. For a Deep Residual Network, we solve an imbalance and a balance problem at the same time, which increases by 5% the Recall of the minority class with no harm to the F1-score. Additionally, the Random Under Sampling technique boost the performance of traditional Machine Learning models, being the Histogram Gradient Boost Classifier the algorithm with the highest increase (3%) in the F1-Score. To the best of our knowledge, this paper is the first providing an automated solution to this problem using data from operating plants.

</p>
</details>

<details><summary><b>MorphPool: Efficient Non-linear Pooling & Unpooling in CNNs</b>
<a href="https://arxiv.org/abs/2211.14037">arxiv:2211.14037</a>
&#x1F4C8; 4 <br>
<p>Rick Groenendijk, Leo Dorst, Theo Gevers</p></summary>
<p>

**Abstract:** Pooling is essentially an operation from the field of Mathematical Morphology, with max pooling as a limited special case. The more general setting of MorphPooling greatly extends the tool set for building neural networks. In addition to pooling operations, encoder-decoder networks used for pixel-level predictions also require unpooling. It is common to combine unpooling with convolution or deconvolution for up-sampling. However, using its morphological properties, unpooling can be generalised and improved. Extensive experimentation on two tasks and three large-scale datasets shows that morphological pooling and unpooling lead to improved predictive performance at much reduced parameter counts.

</p>
</details>

<details><summary><b>Doubly robust nearest neighbors in factor models</b>
<a href="https://arxiv.org/abs/2211.14297">arxiv:2211.14297</a>
&#x1F4C8; 3 <br>
<p>Raaz Dwivedi, Katherine Tian, Sabina Tomkins, Predrag Klasnja, Susan Murphy, Devavrat Shah</p></summary>
<p>

**Abstract:** In this technical note, we introduce an improved variant of nearest neighbors for counterfactual inference in panel data settings where multiple units are assigned multiple treatments over multiple time points, each sampled with constant probabilities. We call this estimator a doubly robust nearest neighbor estimator and provide a high probability non-asymptotic error bound for the mean parameter corresponding to each unit at each time. Our guarantee shows that the doubly robust estimator provides a (near-)quadratic improvement in the error compared to nearest neighbor estimators analyzed in prior work for these settings.

</p>
</details>

<details><summary><b>CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis</b>
<a href="https://arxiv.org/abs/2211.14286">arxiv:2211.14286</a>
&#x1F4C8; 3 <br>
<p>Shichong Peng, Alireza Moazeni, Ke Li</p></summary>
<p>

**Abstract:** A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.

</p>
</details>

<details><summary><b>Training Data Improvement for Image Forgery Detection using Comprint</b>
<a href="https://arxiv.org/abs/2211.14079">arxiv:2211.14079</a>
&#x1F4C8; 3 <br>
<p>Hannes Mareen, Dante Vanden Bussche, Glenn Van Wallendael, Luisa Verdoliva, Peter Lambert</p></summary>
<p>

**Abstract:** Manipulated images are a threat to consumers worldwide, when they are used to spread disinformation. Therefore, Comprint enables forgery detection by utilizing JPEG-compression fingerprints. This paper evaluates the impact of the training set on Comprint's performance. Most interestingly, we found that including images compressed with low quality factors during training does not have a significant effect on the accuracy, whereas incorporating recompression boosts the robustness. As such, consumers can use Comprint on their smartphones to verify the authenticity of images.

</p>
</details>

<details><summary><b>A Survey of Learning Curves with Bad Behavior: or How More Data Need Not Lead to Better Performance</b>
<a href="https://arxiv.org/abs/2211.14061">arxiv:2211.14061</a>
&#x1F4C8; 3 <br>
<p>Marco Loog, Tom Viering</p></summary>
<p>

**Abstract:** Plotting a learner's generalization performance against the training set size results in a so-called learning curve. This tool, providing insight in the behavior of the learner, is also practically valuable for model selection, predicting the effect of more training data, and reducing the computational complexity of training. We set out to make the (ideal) learning curve concept precise and briefly discuss the aforementioned usages of such curves. The larger part of this survey's focus, however, is on learning curves that show that more data does not necessarily leads to better generalization performance. A result that seems surprising to many researchers in the field of artificial intelligence. We point out the significance of these findings and conclude our survey with an overview and discussion of open problems in this area that warrant further theoretical and empirical investigation.

</p>
</details>

<details><summary><b>CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry</b>
<a href="https://arxiv.org/abs/2211.14054">arxiv:2211.14054</a>
&#x1F4C8; 3 <br>
<p>Steven Moonen, Bram Vanherle, Joris de Hoog, Taoufik Bourgana, Abdellatif Bey-Temsamani, Nick Michiels</p></summary>
<p>

**Abstract:** The use of computer vision for product and assembly quality control is becoming ubiquitous in the manufacturing industry. Lately, it is apparent that machine learning based solutions are outperforming classical computer vision algorithms in terms of performance and robustness. However, a main drawback is that they require sufficiently large and labeled training datasets, which are often not available or too tedious and too time consuming to acquire. This is especially true for low-volume and high-variance manufacturing. Fortunately, in this industry, CAD models of the manufactured or assembled products are available. This paper introduces CAD2Render, a GPU-accelerated synthetic data generator based on the Unity High Definition Render Pipeline (HDRP). CAD2Render is designed to add variations in a modular fashion, making it possible for high customizable data generation, tailored to the needs of the industrial use case at hand. Although CAD2Render is specifically designed for manufacturing use cases, it can be used for other domains as well. We validate CAD2Render by demonstrating state of the art performance in two industrial relevant setups. We demonstrate that the data generated by our approach can be used to train object detection and pose estimation models with a high enough accuracy to direct a robot. The code for CAD2Render is available at https://github.com/EDM-Research/CAD2Render.

</p>
</details>

<details><summary><b>On the Universal Approximation Property of Deep Fully Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2211.14047">arxiv:2211.14047</a>
&#x1F4C8; 3 <br>
<p>Ting Lin, Zuowei Shen, Qianxiao Li</p></summary>
<p>

**Abstract:** We study the approximation of shift-invariant or equivariant functions by deep fully convolutional networks from the dynamical systems perspective. We prove that deep residual fully convolutional networks and their continuous-layer counterpart can achieve universal approximation of these symmetric functions at constant channel width. Moreover, we show that the same can be achieved by non-residual variants with at least 2 channels in each layer and convolutional kernel size of at least 2. In addition, we show that these requirements are necessary, in the sense that networks with fewer channels or smaller kernels fail to be universal approximators.

</p>
</details>

<details><summary><b>Molecular Joint Representation Learning via Multi-modal Information</b>
<a href="https://arxiv.org/abs/2211.14042">arxiv:2211.14042</a>
&#x1F4C8; 3 <br>
<p>Tianyu Wu, Yang Tang, Qiyu Sun, Luolin Xiong</p></summary>
<p>

**Abstract:** In recent years, artificial intelligence has played an important role on accelerating the whole process of drug discovery. Various of molecular representation schemes of different modals (e.g. textual sequence or graph) are developed. By digitally encoding them, different chemical information can be learned through corresponding network structures. Molecular graphs and Simplified Molecular Input Line Entry System (SMILES) are popular means for molecular representation learning in current. Previous works have done attempts by combining both of them to solve the problem of specific information loss in single-modal representation on various tasks. To further fusing such multi-modal imformation, the correspondence between learned chemical feature from different representation should be considered. To realize this, we propose a novel framework of molecular joint representation learning via Multi-Modal information of SMILES and molecular Graphs, called MMSG. We improve the self-attention mechanism by introducing bond level graph representation as attention bias in Transformer to reinforce feature correspondence between multi-modal information. We further propose a Bidirectional Message Communication Graph Neural Network (BMC GNN) to strengthen the information flow aggregated from graphs for further combination. Numerous experiments on public property prediction datasets have demonstrated the effectiveness of our model.

</p>
</details>

<details><summary><b>Toward Unlimited Self-Learning Monte Carlo with Annealing Process Using VAE's Implicit Isometricity</b>
<a href="https://arxiv.org/abs/2211.14024">arxiv:2211.14024</a>
&#x1F4C8; 3 <br>
<p>Yuma Ichikawa, Akira Nakagawa, Hiromoto Masayuki, Yuhei Umeda</p></summary>
<p>

**Abstract:** Self-learning Monte Carlo (SLMC) methods are recently proposed to accelerate Markov chain Monte Carlo (MCMC) methods by using a machine learning model.With generative models having latent variables, SLMC methods realize efficient Monte Carlo updates with less autocorrelation. However, SLMC methods are difficult to directly apply to multimodal distributions for which training data are difficult to obtain. In this paper, we propose a novel SLMC method called the ``annealing VAE-SLMC" to drastically expand the range of applications. Our VAE-SLMC utilizes a variational autoencoder (VAE) as a generative model to make efficient parallel proposals independent of any previous state by applying the theoretically derived implicit isometricity of the VAE. We combine an adaptive annealing process to the VAE-SLMC, making our method applicable to the cases where obtaining unbiased training data is difficult in practical sense due to slow mixing. We also propose a parallel annealing process and an exchange process between chains to make the annealing operation more precise and efficient. Experiments validate that our method can proficiently obtain unbiased samples from multiple multimodal toy distributions and practical multimodal posterior distributions, which is difficult to achieve with the existing SLMC methods.

</p>
</details>

<details><summary><b>Combating noisy labels in object detection datasets</b>
<a href="https://arxiv.org/abs/2211.13993">arxiv:2211.13993</a>
&#x1F4C8; 3 <br>
<p>Krystian Chachuła, Adam Popowicz, Jakub Łyskawa, Bartłomiej Olber, Piotr Frątczak, Krystian Radlak</p></summary>
<p>

**Abstract:** The quality of training datasets for deep neural networks is a key factor contributing to the accuracy of resulting models. This is even more important in difficult tasks such as object detection. Dealing with errors in these datasets was in the past limited to accepting that some fraction of examples is incorrect or predicting their confidence and assigning appropriate weights during training. In this work, we propose a different approach. For the first time, we extended the confident learning algorithm to the object detection task. By focusing on finding incorrect labels in the original training datasets, we can eliminate erroneous examples in their root. Suspicious bounding boxes can be re-annotated in order to improve the quality of the dataset itself, thus leading to better models without complicating their already complex architectures. We can effectively point out 99\% of artificially disturbed bounding boxes with FPR below 0.3. We see this method as a promising path to correcting well-known object detection datasets.

</p>
</details>

<details><summary><b>TrustGAN: Training safe and trustworthy deep learning models through generative adversarial networks</b>
<a href="https://arxiv.org/abs/2211.13991">arxiv:2211.13991</a>
&#x1F4C8; 3 <br>
<p>Hélion du Mas des Bourboux</p></summary>
<p>

**Abstract:** Deep learning models have been developed for a variety of tasks and are deployed every day to work in real conditions. Some of these tasks are critical and models need to be trusted and safe, e.g. military communications or cancer diagnosis. These models are given real data, simulated data or combination of both and are trained to be highly predictive on them. However, gathering enough real data or simulating them to be representative of all the real conditions is: costly, sometimes impossible due to confidentiality and most of the time impossible. Indeed, real conditions are constantly changing and sometimes are intractable. A solution is to deploy machine learning models that are able to give predictions when they are confident enough otherwise raise a flag or abstain. One issue is that standard models easily fail at detecting out-of-distribution samples where their predictions are unreliable.
  We present here TrustGAN, a generative adversarial network pipeline targeting trustness. It is a deep learning pipeline which improves a target model estimation of the confidence without impacting its predictive power. The pipeline can accept any given deep learning model which outputs a prediction and a confidence on this prediction. Moreover, the pipeline does not need to modify this target model. It can thus be easily deployed in a MLOps (Machine Learning Operations) setting.
  The pipeline is applied here to a target classification model trained on MNIST data to recognise numbers based on images. We compare such a model when trained in the standard way and with TrustGAN. We show that on out-of-distribution samples, here FashionMNIST and CIFAR10, the estimated confidence is largely reduced. We observe similar conclusions for a classification model trained on 1D radio signals from AugMod, tested on RML2016.04C. We also publicly release the code.

</p>
</details>

<details><summary><b>Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?</b>
<a href="https://arxiv.org/abs/2211.13972">arxiv:2211.13972</a>
&#x1F4C8; 3 <br>
<p>Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, Percy Liang</p></summary>
<p>

**Abstract:** As the scope of machine learning broadens, we observe a recurring theme of algorithmic monoculture: the same systems, or systems that share components (e.g. training data), are deployed by multiple decision-makers. While sharing offers clear advantages (e.g. amortizing costs), does it bear risks? We introduce and formalize one such risk, outcome homogenization: the extent to which particular individuals or groups experience negative outcomes from all decision-makers. If the same individuals or groups exclusively experience undesirable outcomes, this may institutionalize systemic exclusion and reinscribe social hierarchy. To relate algorithmic monoculture and outcome homogenization, we propose the component-sharing hypothesis: if decision-makers share components like training data or specific models, then they will produce more homogeneous outcomes. We test this hypothesis on algorithmic fairness benchmarks, demonstrating that sharing training data reliably exacerbates homogenization, with individual-level effects generally exceeding group-level effects. Further, given the dominant paradigm in AI of foundation models, i.e. models that can be adapted for myriad downstream tasks, we test whether model sharing homogenizes outcomes across tasks. We observe mixed results: we find that for both vision and language settings, the specific methods for adapting a foundation model significantly influence the degree of outcome homogenization. We conclude with philosophical analyses of and societal challenges for outcome homogenization, with an eye towards implications for deployed machine learning systems.

</p>
</details>

<details><summary><b>Learning General Audio Representations with Large-Scale Training of Patchout Audio Transformers</b>
<a href="https://arxiv.org/abs/2211.13956">arxiv:2211.13956</a>
&#x1F4C8; 3 <br>
<p>Khaled Koutini, Shahed Masoudian, Florian Schmid, Hamid Eghbal-zadeh, Jan Schlüter, Gerhard Widmer</p></summary>
<p>

**Abstract:** The success of supervised deep learning methods is largely due to their ability to learn relevant features from raw data. Deep Neural Networks (DNNs) trained on large-scale datasets are capable of capturing a diverse set of features, and learning a representation that can generalize onto unseen tasks and datasets that are from the same domain. Hence, these models can be used as powerful feature extractors, in combination with shallower models as classifiers, for smaller tasks and datasets where the amount of training data is insufficient for learning an end-to-end model from scratch. During the past years, Convolutional Neural Networks (CNNs) have largely been the method of choice for audio processing. However, recently attention-based transformer models have demonstrated great potential in supervised settings, outperforming CNNs. In this work, we investigate the use of audio transformers trained on large-scale datasets to learn general-purpose representations. We study how the different setups in these audio transformers affect the quality of their embeddings. We experiment with the models' time resolution, extracted embedding level, and receptive fields in order to see how they affect performance on a variety of tasks and datasets, following the HEAR 2021 NeurIPS challenge evaluation setup. Our results show that representations extracted by audio transformers outperform CNN representations. Furthermore, we will show that transformers trained on Audioset can be extremely effective representation extractors for a wide range of downstream tasks.

</p>
</details>

<details><summary><b>Particle-based Variational Inference with Preconditioned Functional Gradient Flow</b>
<a href="https://arxiv.org/abs/2211.13954">arxiv:2211.13954</a>
&#x1F4C8; 3 <br>
<p>Hanze Dong, Xi Wang, Yong Lin, Tong Zhang</p></summary>
<p>

**Abstract:** Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper remedies the problem by proposing a general framework to obtain tractable functional gradient flow estimates. The functional gradient flow in our framework can be defined by a general functional regularization term that includes the RKHS norm as a special case. We use our framework to propose a new particle-based VI algorithm: preconditioned functional gradient flow (PFG). Compared with SVGD, the proposed method has several advantages: larger function class; greater scalability in large particle-size scenarios; better adaptation to ill-conditioned distributions; provable continuous-time convergence in KL divergence. Non-linear function classes such as neural networks can be incorporated to estimate the gradient flow. Both theory and experiments have shown the effectiveness of our framework.

</p>
</details>

<details><summary><b>Towards Good Practices for Missing Modality Robust Action Recognition</b>
<a href="https://arxiv.org/abs/2211.13916">arxiv:2211.13916</a>
&#x1F4C8; 3 <br>
<p>Sangmin Woo, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, Changick Kim</p></summary>
<p>

**Abstract:** Standard multi-modal models assume the use of the same modalities in training and inference stages. However, in practice, the environment in which multi-modal models operate may not satisfy such assumption. As such, their performances degrade drastically if any modality is missing in the inference stage. We ask: how can we train a model that is robust to missing modalities? This paper seeks a set of good practices for multi-modal action recognition, with a particular interest in circumstances where some modalities are not available at an inference time. First, we study how to effectively regularize the model during training (e.g., data augmentation). Second, we investigate on fusion methods for robustness to missing modalities: we find that transformer-based fusion shows better robustness for missing modality than summation or concatenation. Third, we propose a simple modular network, ActionMAE, which learns missing modality predictive coding by randomly dropping modality features and tries to reconstruct them with the remaining modality features. Coupling these good practices, we build a model that is not only effective in multi-modal action recognition but also robust to modality missing. Our model achieves the state-of-the-arts on multiple benchmarks and maintains competitive performances even in missing modality scenarios. Codes are available at https://github.com/sangminwoo/ActionMAE.

</p>
</details>

<details><summary><b>Forecasting Actions and Characteristic 3D Poses</b>
<a href="https://arxiv.org/abs/2211.14309">arxiv:2211.14309</a>
&#x1F4C8; 2 <br>
<p>Christian Diller, Thomas Funkhouser, Angela Dai</p></summary>
<p>

**Abstract:** We propose to model longer-term future human behavior by jointly predicting action labels and 3D characteristic poses (3D poses representative of the associated actions). While previous work has considered action and 3D pose forecasting separately, we observe that the nature of the two tasks is coupled, and thus we predict them together. Starting from an input 2D video observation, we jointly predict a future sequence of actions along with 3D poses characterizing these actions. Since coupled action labels and 3D pose annotations are difficult and expensive to acquire for videos of complex action sequences, we train our approach with action labels and 2D pose supervision from two existing action video datasets, in tandem with an adversarial loss that encourages likely 3D predicted poses. Our experiments demonstrate the complementary nature of joint action and characteristic 3D pose prediction: our joint approach outperforms each task treated individually, enables robust longer-term sequence prediction, and outperforms alternative approaches to forecast actions and characteristic 3D poses.

</p>
</details>

<details><summary><b>RUST: Latent Neural Scene Representations from Unposed Imagery</b>
<a href="https://arxiv.org/abs/2211.14306">arxiv:2211.14306</a>
&#x1F4C8; 2 <br>
<p>Mehdi S. M. Sajjadi, Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel Duckworth, Mario Lucic, Klaus Greff</p></summary>
<p>

**Abstract:** Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation into the learned latent pose structure and show that it allows meaningful test-time camera transformations and accurate explicit pose readouts. Perhaps surprisingly, RUST achieves similar quality as methods which have access to perfect camera pose, thereby unlocking the potential for large-scale training of amortized neural scene representations.

</p>
</details>

<details><summary><b>PIP: Positional-encoding Image Prior</b>
<a href="https://arxiv.org/abs/2211.14298">arxiv:2211.14298</a>
&#x1F4C8; 2 <br>
<p>Nimrod Shabtay, Eli Schwartz, Raja Giryes</p></summary>
<p>

**Abstract:** In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted to map a latent space to a degraded (e.g. noisy) image but in the process learns to reconstruct the clean image. This phenomenon is attributed to CNN's internal image-prior. We revisit the DIP framework, examining it from the perspective of a neural implicit representation. Motivated by this perspective, we replace the random or learned latent with Fourier-Features (Positional Encoding). We show that thanks to the Fourier features properties, we can replace the convolution layers with simple pixel-level MLPs. We name this scheme ``Positional Encoding Image Prior" (PIP) and exhibit that it performs very similarly to DIP on various image-reconstruction tasks with much less parameters required. Additionally, we demonstrate that PIP can be easily extended to videos, where 3D-DIP struggles and suffers from instability. Code and additional examples for all tasks, including videos, are available on the project page https://nimrodshabtay.github.io/PIP/

</p>
</details>

<details><summary><b>Strategyproof Decision-Making in Panel Data Settings and Beyond</b>
<a href="https://arxiv.org/abs/2211.14236">arxiv:2211.14236</a>
&#x1F4C8; 2 <br>
<p>Keegan Harris, Anish Agarwal, Chara Podimata, Zhiwei Steven Wu</p></summary>
<p>

**Abstract:** We propose a framework for decision-making in the presence of strategic agents with panel data, a standard setting in econometrics and statistics where one gets noisy, repeated measurements of multiple units. We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign treatment to each unit. Our model can be thought of as a generalization of the synthetic controls and synthetic interventions frameworks, where units (or agents) may strategically manipulate pre-intervention outcomes to receive a more desirable intervention. We identify necessary and sufficient conditions under which a strategyproof mechanism that assigns interventions in the post-intervention period exists. Under a latent factor model assumption, we show that whenever a strategyproof mechanism exists, there is one with a simple closed form. In the setting where there is a single treatment and control (i.e., no other interventions), we establish that there is always a strategyproof mechanism, and provide an algorithm for learning such a mechanism. For the setting of multiple interventions, we provide an algorithm for learning a strategyproof mechanism, if there exists a sufficiently large gap in rewards between the different interventions. Along the way, we prove impossibility results for multi-class strategic classification, which may be of independent interest.

</p>
</details>

<details><summary><b>Data-driven identification and analysis of the glass transition in polymer melts</b>
<a href="https://arxiv.org/abs/2211.14220">arxiv:2211.14220</a>
&#x1F4C8; 2 <br>
<p>Atreyee Banerjee, Hsiao-Ping Hsu, Kurt Kremer, Oleksandra Kukharenko</p></summary>
<p>

**Abstract:** We propose a data-driven approach based on information about structural fluctuations of polymer chains, which clearly identifies the glass transition temperature $T_g$ of polymer melts of weakly semiflexible chains. We use principal component analysis (PCA) with clustering to distinguish between liquid and glassy states and predict $T_g$ in the asymptotic limit. Our method indicates that for temperatures approaching $T_g$ from above it is sufficient to consider short molecular dynamics simulation trajectories, which just reach into the Rouse-like monomer displacement regime. The first eigenvalue of PCA and participation ratio show sharp changes around $T_g$. Our approach requires minimum user inputs and is robust and transferable.

</p>
</details>

<details><summary><b>Isolation Scheme for Virtual Network Embedding Based on Reinforcement Learning for Smart City Vertical Industries</b>
<a href="https://arxiv.org/abs/2211.14158">arxiv:2211.14158</a>
&#x1F4C8; 2 <br>
<p>Ali Gohar</p></summary>
<p>

**Abstract:** Modern ICT infrastructure is built on virtualization technologies, which connect a diverse set of dedicated networks to support a variety of smart city vertical industries (SCVI), such as energy, healthcare, manufacturing, entertainment, and intelligent transportation. The wide range of SCVI use cases require services to operate continuously and reliably. The violation of isolation by a specific SCVI, that is, a SCVI network must operate independently of other SCVI networks, complicates service assurance for infrastructure providers (InPs) significantly. As a result, a solution must be considered from the standpoint of isolation, which raises two issues: first, these SCVI networks have diverse resource requirements, and second, they necessitate additional functionality requirements such as isolation. Based on the above two problems faced by SCVI use cases, we propose a virtual network embedding (VNE) algorithm with resource and isolation constraints based on deep reinforcement learning (DRL). The proposed DRL_VNE algorithm can automatically adapt to changing dynamics and outperforms existing three state-of-the-art solutions by 12.9%, 19.0% and 4% in terms of the acceptance rate, the long-term average revenue, and long-term average revenue to cost ratio.

</p>
</details>

<details><summary><b>Automating Cobb Angle Measurement for Adolescent Idiopathic Scoliosis using Instance Segmentation</b>
<a href="https://arxiv.org/abs/2211.14122">arxiv:2211.14122</a>
&#x1F4C8; 2 <br>
<p>Chaojun Chen, Khashayar Namdar, Yujie Wu, Shahob Hosseinpour, Manohar Shroff, Andrea S. Doria, Farzad Khalvati</p></summary>
<p>

**Abstract:** Scoliosis is a three-dimensional deformity of the spine, most often diagnosed in childhood. It affects 2-3% of the population, which is approximately seven million people in North America. Currently, the reference standard for assessing scoliosis is based on the manual assignment of Cobb angles at the site of the curvature center. This manual process is time consuming and unreliable as it is affected by inter- and intra-observer variance. To overcome these inaccuracies, machine learning (ML) methods can be used to automate the Cobb angle measurement process. This paper proposes to address the Cobb angle measurement task using YOLACT, an instance segmentation model. The proposed method first segments the vertebrae in an X-Ray image using YOLACT, then it tracks the important landmarks using the minimum bounding box approach. Lastly, the extracted landmarks are used to calculate the corresponding Cobb angles. The model achieved a Symmetric Mean Absolute Percentage Error (SMAPE) score of 10.76%, demonstrating the reliability of this process in both vertebra localization and Cobb angle measurement.

</p>
</details>

<details><summary><b>Deep grading for MRI-based differential diagnosis of Alzheimer's disease and Frontotemporal dementia</b>
<a href="https://arxiv.org/abs/2211.14096">arxiv:2211.14096</a>
&#x1F4C8; 2 <br>
<p>Huy-Dung Nguyen, Michaël Clément, Vincent Planche, Boris Mansencal, Pierrick Coupé</p></summary>
<p>

**Abstract:** Alzheimer's disease and Frontotemporal dementia are common forms of neurodegenerative dementia. Behavioral alterations and cognitive impairments are found in the clinical courses of both diseases and their differential diagnosis is sometimes difficult for physicians. Therefore, an accurate tool dedicated to this diagnostic challenge can be valuable in clinical practice. However, current structural imaging methods mainly focus on the detection of each disease but rarely on their differential diagnosis. In this paper, we propose a deep learning based approach for both problems of disease detection and differential diagnosis. We suggest utilizing two types of biomarkers for this application: structure grading and structure atrophy. First, we propose to train a large ensemble of 3D U-Nets to locally determine the anatomical patterns of healthy people, patients with Alzheimer's disease and patients with Frontotemporal dementia using structural MRI as input. The output of the ensemble is a 2-channel disease's coordinate map able to be transformed into a 3D grading map which is easy to interpret for clinicians. This 2-channel map is coupled with a multi-layer perceptron classifier for different classification tasks. Second, we propose to combine our deep learning framework with a traditional machine learning strategy based on volume to improve the model discriminative capacity and robustness. After both cross-validation and external validation, our experiments based on 3319 MRI demonstrated competitive results of our method compared to the state-of-the-art methods for both disease detection and differential diagnosis.

</p>
</details>

<details><summary><b>A Hierarchical Variable Autonomy Mixed-Initiative Framework for Human-Robot Teaming in Mobile Robotics</b>
<a href="https://arxiv.org/abs/2211.14095">arxiv:2211.14095</a>
&#x1F4C8; 2 <br>
<p>Dimitris Panagopoulos, Giannis Petousakis, Aniketh Ramesh, Tianshu Ruan, Grigoris Nikolaou, Rustam Stolkin, Manolis Chiou</p></summary>
<p>

**Abstract:** This paper presents a Mixed-Initiative (MI) framework for addressing the problem of control authority transfer between a remote human operator and an AI agent when cooperatively controlling a mobile robot. Our Hierarchical Expert-guided Mixed-Initiative Control Switcher (HierEMICS) leverages information on the human operator's state and intent. The control switching policies are based on a criticality hierarchy. An experimental evaluation was conducted in a high-fidelity simulated disaster response and remote inspection scenario, comparing HierEMICS with a state-of-the-art Expert-guided Mixed-Initiative Control Switcher (EMICS) in the context of mobile robot navigation. Results suggest that HierEMICS reduces conflicts for control between the human and the AI agent, which is a fundamental challenge in both the MI control paradigm and also in the related shared control paradigm. Additionally, we provide statistically significant evidence of improved, navigational safety (i.e., fewer collisions), LOA switching efficiency, and conflict for control reduction.

</p>
</details>

<details><summary><b>Positive unlabeled learning with tensor networks</b>
<a href="https://arxiv.org/abs/2211.14085">arxiv:2211.14085</a>
&#x1F4C8; 2 <br>
<p>Bojan Žunkovič</p></summary>
<p>

**Abstract:** Positive unlabeled learning is a binary classification problem with positive and unlabeled data. It is common in domains where negative labels are costly or impossible to obtain, e.g., medicine and personalized advertising. We apply the locally purified state tensor network to the positive unlabeled learning problem and test our model on the MNIST image and 15 categorical/mixed datasets. On the MNIST dataset, we achieve state-of-the-art results even with very few labeled positive samples. Similarly, we significantly improve the state-of-the-art on categorical datasets. Further, we show that the agreement fraction between outputs of different models on unlabeled samples is a good indicator of the model's performance. Finally, our method can generate new positive and negative instances, which we demonstrate on simple synthetic datasets.

</p>
</details>

<details><summary><b>Open-Source Skull Reconstruction with MONAI</b>
<a href="https://arxiv.org/abs/2211.14051">arxiv:2211.14051</a>
&#x1F4C8; 2 <br>
<p>Jianning Li, André Ferreira, Behrus Puladi, Victor Alves, Michael Kamp, Moon-Sung Kim, Felix Nensa, Jens Kleesiek, Seyed-Ahmad Ahmadi, Jan Egger</p></summary>
<p>

**Abstract:** We present a deep learning-based approach for skull reconstruction for MONAI, which has been pre-trained on the MUG500+ skull dataset. The implementation follows the MONAI contribution guidelines, hence, it can be easily tried out and used, and extended by MONAI users. The primary goal of this paper lies in the investigation of open-sourcing codes and pre-trained deep learning models under the MONAI framework. Nowadays, open-sourcing software, especially (pre-trained) deep learning models, has become increasingly important. Over the years, medical image analysis experienced a tremendous transformation. Over a decade ago, algorithms had to be implemented and optimized with low-level programming languages, like C or C++, to run in a reasonable time on a desktop PC, which was not as powerful as today's computers. Nowadays, users have high-level scripting languages like Python, and frameworks like PyTorch and TensorFlow, along with a sea of public code repositories at hand. As a result, implementations that had thousands of lines of C or C++ code in the past, can now be scripted with a few lines and in addition executed in a fraction of the time. To put this even on a higher level, the Medical Open Network for Artificial Intelligence (MONAI) framework tailors medical imaging research to an even more convenient process, which can boost and push the whole field. The MONAI framework is a freely available, community-supported, open-source and PyTorch-based framework, that also enables to provide research contributions with pre-trained models to others. Codes and pre-trained weights for skull reconstruction are publicly available at: https://github.com/Project-MONAI/research-contributions/tree/master/SkullRec

</p>
</details>

<details><summary><b>Real-Time Under-Display Cameras Image Restoration and HDR on Mobile Devices</b>
<a href="https://arxiv.org/abs/2211.14040">arxiv:2211.14040</a>
&#x1F4C8; 2 <br>
<p>Marcos V. Conde, Florin Vasluianu, Sabari Nathan, Radu Timofte</p></summary>
<p>

**Abstract:** The new trend of full-screen devices implies positioning the camera behind the screen to bring a larger display-to-body ratio, enhance eye contact, and provide a notch-free viewing experience on smartphones, TV or tablets. On the other hand, the images captured by under-display cameras (UDCs) are degraded by the screen in front of them. Deep learning methods for image restoration can significantly reduce the degradation of captured images, providing satisfying results for the human eyes. However, most proposed solutions are unreliable or efficient enough to be used in real-time on mobile devices.
  In this paper, we aim to solve this image restoration problem using efficient deep learning methods capable of processing FHD images in real-time on commercial smartphones while providing high-quality results. We propose a lightweight model for blind UDC Image Restoration and HDR, and we also provide a benchmark comparing the performance and runtime of different methods on smartphones. Our models are competitive on UDC benchmarks while using x4 less operations than others. To the best of our knowledge, we are the first work to approach and analyze this real-world single image restoration problem from the efficiency and production point of view.

</p>
</details>

<details><summary><b>Automata Cascades: Expressivity and Sample Complexity</b>
<a href="https://arxiv.org/abs/2211.14028">arxiv:2211.14028</a>
&#x1F4C8; 2 <br>
<p>Alessandro Ronca, Nadezda A. Knorozova, Giuseppe De Giacomo</p></summary>
<p>

**Abstract:** Every automaton can be decomposed into a cascade of basic automata. This is the Prime Decomposition Theorem by Krohn and Rhodes. We show that cascades allow for describing the sample complexity of automata in terms of their components. In particular, we show that the sample complexity is linear in the number of components and the maximum complexity of a single component. This opens to the possibility for learning automata representing large dynamic systems consisting of many parts interacting with each other. It is in sharp contrast with the established understanding of the sample complexity of automata, described in terms of the overall number of states and input letters, which in turn implies that it is only possible to learn automata where the number of states and letters is linear in the amount of data available. Instead our results show that one can in principle learn automata with infinite input alphabets and a number of states that is exponential in the amount of data available.

</p>
</details>

<details><summary><b>Cross-network transferable neural models for WLAN interference estimation</b>
<a href="https://arxiv.org/abs/2211.14026">arxiv:2211.14026</a>
&#x1F4C8; 2 <br>
<p>Danilo Marinho Fernandes, Jonatan Krolikowski, Zied Ben Houidi, Fuxing Chen, Dario Rossi</p></summary>
<p>

**Abstract:** Airtime interference is a key performance indicator for WLANs, measuring, for a given time period, the percentage of time during which a node is forced to wait for other transmissions before to transmitting or receiving. Being able to accurately estimate interference resulting from a given state change (e.g., channel, bandwidth, power) would allow a better control of WLAN resources, assessing the impact of a given configuration before actually implementing it.
  In this paper, we adopt a principled approach to interference estimation in WLANs. We first use real data to characterize the factors that impact it, and derive a set of relevant synthetic workloads for a controlled comparison of various deep learning architectures in terms of accuracy, generalization and robustness to outlier data. We find, unsurprisingly, that Graph Convolutional Networks (GCNs) yield the best performance overall, leveraging the graph structure inherent to campus WLANs. We notice that, unlike e.g. LSTMs, they struggle to learn the behavior of specific nodes, unless given the node indexes in addition. We finally verify GCN model generalization capabilities, by applying trained models on operational deployments unseen at training time.

</p>
</details>

<details><summary><b>Expanding Small-Scale Datasets with Guided Imagination</b>
<a href="https://arxiv.org/abs/2211.13976">arxiv:2211.13976</a>
&#x1F4C8; 2 <br>
<p>Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng</p></summary>
<p>

**Abstract:** The power of Deep Neural Networks (DNNs) depends heavily on the training data quantity, quality and diversity. However, in many real scenarios, it is costly and time-consuming to collect and annotate large-scale data. This has severely hindered the application of DNNs. To address this challenge, we explore a new task of dataset expansion, which seeks to automatically create new labeled samples to expand a small dataset. To this end, we present a Guided Imagination Framework (GIF) that leverages the recently developed big generative models (e.g., DALL-E2) and reconstruction models (e.g., MAE) to "imagine" and create informative new data from seed data to expand small datasets. Specifically, GIF conducts imagination by optimizing the latent features of seed data in a semantically meaningful space, which are fed into the generative models to generate photo-realistic images with new contents. For guiding the imagination towards creating samples useful for model training, we exploit the zero-shot recognition ability of CLIP and introduce three criteria to encourage informative sample generation, i.e., prediction consistency, entropy maximization and diversity promotion. With these essential criteria as guidance, GIF works well for expanding datasets in different domains, leading to 29.9% accuracy gain on average over six natural image datasets, and 12.3% accuracy gain on average over three medical image datasets.

</p>
</details>

<details><summary><b>MPCViT: Searching for MPC-friendly Vision Transformer with Heterogeneous Attention</b>
<a href="https://arxiv.org/abs/2211.13955">arxiv:2211.13955</a>
&#x1F4C8; 2 <br>
<p>Wenxuan Zeng, Meng Li, Wenjie Xiong, Wenjie Lu, Jin Tan, Runsheng Wang, Ru Huang</p></summary>
<p>

**Abstract:** Secure multi-party computation (MPC) enables computation directly on encrypted data on non-colluding untrusted servers and protects both data and model privacy in deep learning inference. However, existing neural network (NN) architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC protocols and incur significant latency overhead due to the Softmax function in the multi-head attention (MHA). In this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. We systematically compare different attention variants in MPC and propose a heterogeneous attention search space, which combines the high-accuracy and MPC-efficient attentions with diverse structure granularities. We further propose a simple yet effective differentiable neural architecture search (NAS) algorithm for fast ViT optimization. MPCViT significantly outperforms prior-art ViT variants in MPC. With the proposed NAS algorithm, our extensive experiments demonstrate that MPCViT achieves 7.9x and 2.8x latency reduction with better accuracy compared to Linformer and MPCFormer on the Tiny-ImageNet dataset, respectively. Further, with proper knowledge distillation (KD), MPCViT even achieves 1.9% better accuracy compared to the baseline ViT with 9.9x latency reduction on the Tiny-ImageNet dataset.

</p>
</details>

<details><summary><b>TRAC: A Textual Benchmark for Reasoning about Actions and Change</b>
<a href="https://arxiv.org/abs/2211.13930">arxiv:2211.13930</a>
&#x1F4C8; 2 <br>
<p>Weinan He, Canming Huang, Zhanhao Xiao, Yongmei Liu</p></summary>
<p>

**Abstract:** Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicates that additional efforts are needed to tackle challenges raised by TRAC.

</p>
</details>

<details><summary><b>Confidence Interval Construction for Multivariate time series using Long Short Term Memory Network</b>
<a href="https://arxiv.org/abs/2211.13915">arxiv:2211.13915</a>
&#x1F4C8; 2 <br>
<p>Aryan Bhambu, Arabin Kumar Dey</p></summary>
<p>

**Abstract:** In this paper we propose a novel procedure to construct a confidence interval for multivariate time series predictions using long short term memory network. The construction uses a few novel block bootstrap techniques. We also propose an innovative block length selection procedure for each of these schemes. Two novel benchmarks help us to compare the construction of this confidence intervals by different bootstrap techniques. We illustrate the whole construction through S\&P $500$ and Dow Jones Index datasets.

</p>
</details>

<details><summary><b>TAOTF: A Two-stage Approximately Orthogonal Training Framework in Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2211.13902">arxiv:2211.13902</a>
&#x1F4C8; 2 <br>
<p>Taoyong Cui, Jianze Li, Yuhan Dong, Li Liu</p></summary>
<p>

**Abstract:** The orthogonality constraints, including the hard and soft ones, have been used to normalize the weight matrices of Deep Neural Network (DNN) models, especially the Convolutional Neural Network (CNN) and Vision Transformer (ViT), to reduce model parameter redundancy and improve training stability. However, the robustness to noisy data of these models with constraints is not always satisfactory. In this work, we propose a novel two-stage approximately orthogonal training framework (TAOTF) to find a trade-off between the orthogonal solution space and the main task solution space to solve this problem in noisy data scenarios. In the first stage, we propose a novel algorithm called polar decomposition-based orthogonal initialization (PDOI) to find a good initialization for the orthogonal optimization. In the second stage, unlike other existing methods, we apply soft orthogonal constraints for all layers of DNN model. We evaluate the proposed model-agnostic framework both on the natural image and medical image datasets, which show that our method achieves stable and superior performances to existing methods.

</p>
</details>

<details><summary><b>DoubleU-NetPlus: A Novel Attention and Context Guided Dual U-Net with Multi-Scale Residual Feature Fusion Network for Semantic Segmentation of Medical Images</b>
<a href="https://arxiv.org/abs/2211.14235">arxiv:2211.14235</a>
&#x1F4C8; 1 <br>
<p>Md. Rayhan Ahmed, Adnan Ferdous Ashrafi, Raihan Uddin Ahmed, Swakkhar Shatabda, A. K. M. Muzahidul Islam, Salekul Islam</p></summary>
<p>

**Abstract:** Accurate segmentation of the region of interest in medical images can provide an essential pathway for devising effective treatment plans for life-threatening diseases. It is still challenging for U-Net, and its state-of-the-art variants, such as CE-Net and DoubleU-Net, to effectively model the higher-level output feature maps of the convolutional units of the network mostly due to the presence of various scales of the region of interest, intricacy of context environments, ambiguous boundaries, and multiformity of textures in medical images. In this paper, we exploit multi-contextual features and several attention strategies to increase networks' ability to model discriminative feature representation for more accurate medical image segmentation, and we present a novel dual U-Net-based architecture named DoubleU-NetPlus. The DoubleU-NetPlus incorporates several architectural modifications. In particular, we integrate EfficientNetB7 as the feature encoder module, a newly designed multi-kernel residual convolution module, and an adaptive feature re-calibrating attention-based atrous spatial pyramid pooling module to progressively and precisely accumulate discriminative multi-scale high-level contextual feature maps and emphasize the salient regions. In addition, we introduce a novel triple attention gate module and a hybrid triple attention module to encourage selective modeling of relevant medical image features. Moreover, to mitigate the gradient vanishing issue and incorporate high-resolution features with deeper spatial details, the standard convolution operation is replaced with the attention-guided residual convolution operations, ...

</p>
</details>

<details><summary><b>The Effect of Epigenetic Blocking on Dynamic Multi-Objective Optimisation Problems</b>
<a href="https://arxiv.org/abs/2211.14222">arxiv:2211.14222</a>
&#x1F4C8; 1 <br>
<p>Sizhe Yuen, Thomas H. G. Ezard, Adam J. Sobey</p></summary>
<p>

**Abstract:** Hundreds of Evolutionary Computation approaches have been reported. From an evolutionary perspective they focus on two fundamental mechanisms: cultural inheritance in Swarm Intelligence and genetic inheritance in Evolutionary Algorithms. Contemporary evolutionary biology looks beyond genetic inheritance, proposing a so-called ``Extended Evolutionary Synthesis''. Many concepts from the Extended Evolutionary Synthesis have been left out of Evolutionary Computation as interest has moved toward specific implementations of the same general mechanisms. One such concept is epigenetic inheritance, which is increasingly considered central to evolutionary thinking. Epigenetic mechanisms allow quick non- or partially-genetic adaptations to environmental changes. Dynamic multi-objective optimisation problems represent similar circumstances to the natural world where fitness can be determined by multiple objectives (traits), and the environment is constantly changing.
  This paper asks if the advantages that epigenetic inheritance provide in the natural world are replicated in dynamic multi-objective optimisation problems. Specifically, an epigenetic blocking mechanism is applied to a state-of-the-art multi-objective genetic algorithm, MOEA/D-DE, and its performance is compared on three sets of dynamic test functions, FDA, JY, and UDF. The mechanism shows improved performance on 12 of the 16 test problems, providing initial evidence that more algorithms should explore the wealth of epigenetic mechanisms seen in the natural world.

</p>
</details>

<details><summary><b>Task-Oriented Communication for Edge Video Analytics</b>
<a href="https://arxiv.org/abs/2211.14049">arxiv:2211.14049</a>
&#x1F4C8; 1 <br>
<p>Jiawei Shao, Xinjie Zhang, Jun Zhang</p></summary>
<p>

**Abstract:** With the development of artificial intelligence (AI) techniques and the increasing popularity of camera-equipped devices, many edge video analytics applications are emerging, calling for the deployment of computation-intensive AI models at the network edge. Edge inference is a promising solution to move the computation-intensive workloads from low-end devices to a powerful edge server for video analytics, but the device-server communications will remain a bottleneck due to the limited bandwidth. This paper proposes a task-oriented communication framework for edge video analytics, where multiple devices collect the visual sensory data and transmit the informative features to an edge server for processing. To enable low-latency inference, this framework removes video redundancy in spatial and temporal domains and transmits minimal information that is essential for the downstream task, rather than reconstructing the videos at the edge server. Specifically, it extracts compact task-relevant features based on the deterministic information bottleneck (IB) principle, which characterizes a tradeoff between the informativeness of the features and the communication cost. As the features of consecutive frames are temporally correlated, we propose a temporal entropy model (TEM) to reduce the bitrate by taking the previous features as side information in feature encoding. To further improve the inference performance, we build a spatial-temporal fusion module at the server to integrate features of the current and previous frames for joint inference. Extensive experiments on video analytics tasks evidence that the proposed framework effectively encodes task-relevant information of video data and achieves a better rate-performance tradeoff than existing methods.

</p>
</details>

<details><summary><b>Strategic Facility Location with Clients that Minimize Total Waiting Time</b>
<a href="https://arxiv.org/abs/2211.14016">arxiv:2211.14016</a>
&#x1F4C8; 1 <br>
<p>Simon Krogmann, Pascal Lenzner, Alexander Skopalik</p></summary>
<p>

**Abstract:** We study a non-cooperative two-sided facility location game in which facilities and clients behave strategically. This is in contrast to many other facility location games in which clients simply visit their closest facility. Facility agents select a location on a graph to open a facility to attract as much purchasing power as possible, while client agents choose which facilities to patronize by strategically distributing their purchasing power in order to minimize their total waiting time. Here, the waiting time of a facility depends on its received total purchasing power. We show that our client stage is an atomic splittable congestion game, which implies existence, uniqueness and efficient computation of a client equilibrium. Therefore, facility agents can efficiently predict client behavior and make strategic decisions accordingly. Despite that, we prove that subgame perfect equilibria do not exist in all instances of this game and that their existence is NP-hard to decide. On the positive side, we provide a simple and efficient algorithm to compute 3-approximate subgame perfect equilibria.

</p>
</details>

<details><summary><b>Affine Transformation Edited and Refined Deep Neural Network for Quantitative Susceptibility Mapping</b>
<a href="https://arxiv.org/abs/2211.13942">arxiv:2211.13942</a>
&#x1F4C8; 1 <br>
<p>Zhuang Xiong, Yang Gao, Feng Liu, Hongfu Sun</p></summary>
<p>

**Abstract:** Deep neural networks have demonstrated great potential in solving dipole inversion for Quantitative Susceptibility Mapping (QSM). However, the performances of most existing deep learning methods drastically degrade with mismatched sequence parameters such as acquisition orientation and spatial resolution. We propose an end-to-end AFfine Transformation Edited and Refined (AFTER) deep neural network for QSM, which is robust against arbitrary acquisition orientation and spatial resolution up to 0.6 mm isotropic at the finest. The AFTER-QSM neural network starts with a forward affine transformation layer, followed by an Unet for dipole inversion, then an inverse affine transformation layer, followed by a Residual Dense Network (RDN) for QSM refinement. Simulation and in-vivo experiments demonstrated that the proposed AFTER-QSM network architecture had excellent generalizability. It can successfully reconstruct susceptibility maps from highly oblique and anisotropic scans, leading to the best image quality assessments in simulation tests and suppressed streaking artifacts and noise levels for in-vivo experiments compared with other methods. Furthermore, ablation studies showed that the RDN refinement network significantly reduced image blurring and susceptibility underestimation due to affine transformations. In addition, the AFTER-QSM network substantially shortened the reconstruction time from minutes using conventional methods to only a few seconds.

</p>
</details>

<details><summary><b>Combinatorial Civic Crowdfunding with Budgeted Agents: Welfare Optimality at Equilibrium and Optimal Deviation</b>
<a href="https://arxiv.org/abs/2211.13941">arxiv:2211.13941</a>
&#x1F4C8; 1 <br>
<p>Sankarshan Damle, Manisha Padala, Sujit Gujar</p></summary>
<p>

**Abstract:** Civic Crowdfunding (CC) uses the ``power of the crowd'' to garner contributions towards public projects. As these projects are non-excludable, agents may prefer to ``free-ride,'' resulting in the project not being funded. For single project CC, researchers propose to provide refunds to incentivize agents to contribute, thereby guaranteeing the project's funding. These funding guarantees are applicable only when agents have an unlimited budget. This work focuses on a combinatorial setting, where multiple projects are available for CC and agents have a limited budget. We study certain specific conditions where funding can be guaranteed. Further, funding the optimal social welfare subset of projects is desirable when every available project cannot be funded due to budget restrictions. We prove the impossibility of achieving optimal welfare at equilibrium for any monotone refund scheme. We then study different heuristics that the agents can use to contribute to the projects in practice. Through simulations, we demonstrate the heuristics' performance as the average-case trade-off between welfare obtained and agent utility.

</p>
</details>

<details><summary><b>Efficient Incremental Text-to-Speech on GPUs</b>
<a href="https://arxiv.org/abs/2211.13939">arxiv:2211.13939</a>
&#x1F4C8; 1 <br>
<p>Muyang Du, Chuan Liu, Jiaxing Qi, Junjie Lai</p></summary>
<p>

**Abstract:** Incremental text-to-speech, also known as streaming TTS, has been increasingly applied to online speech applications that require ultra-low response latency to provide an optimal user experience. However, most of the existing speech synthesis pipelines deployed on GPU are still non-incremental, which uncovers limitations in high-concurrency scenarios, especially when the pipeline is built with end-to-end neural network models. To address this issue, we present a highly efficient approach to perform real-time incremental TTS on GPUs with Instant Request Pooling and Module-wise Dynamic Batching. Experimental results demonstrate that the proposed method is capable of producing high-quality speech with a first-chunk latency lower than 80ms under 100 QPS on a single NVIDIA A10 GPU and significantly outperforms the non-incremental twin in both concurrency and latency. Our work reveals the effectiveness of high-performance incremental TTS on GPUs.

</p>
</details>

<details><summary><b>Generative Modeling in Sinogram Domain for Sparse-view CT Reconstruction</b>
<a href="https://arxiv.org/abs/2211.13926">arxiv:2211.13926</a>
&#x1F4C8; 1 <br>
<p>Bing Guan, Cailian Yang, Liu Zhang, Shanzhou Niu, Minghui Zhang, Yuhao Wang, Weiwen Wu, Qiegen Liu</p></summary>
<p>

**Abstract:** The radiation dose in computed tomography (CT) examinations is harmful for patients but can be significantly reduced by intuitively decreasing the number of projection views. Reducing projection views usually leads to severe aliasing artifacts in reconstructed images. Previous deep learning (DL) techniques with sparse-view data require sparse-view/full-view CT image pairs to train the network with supervised manners. When the number of projection view changes, the DL network should be retrained with updated sparse-view/full-view CT image pairs. To relieve this limitation, we present a fully unsupervised score-based generative model in sinogram domain for sparse-view CT reconstruction. Specifically, we first train a score-based generative model on full-view sinogram data and use multi-channel strategy to form highdimensional tensor as the network input to capture their prior distribution. Then, at the inference stage, the stochastic differential equation (SDE) solver and data-consistency step were performed iteratively to achieve fullview projection. Filtered back-projection (FBP) algorithm was used to achieve the final image reconstruction. Qualitative and quantitative studies were implemented to evaluate the presented method with several CT data. Experimental results demonstrated that our method achieved comparable or better performance than the supervised learning counterparts.

</p>
</details>

<details><summary><b>Domain generalization in fetal brain MRI segmentation \\with multi-reconstruction augmentation</b>
<a href="https://arxiv.org/abs/2211.14282">arxiv:2211.14282</a>
&#x1F4C8; 0 <br>
<p>Priscille de Dumast, Meritxell Bach Cuadra</p></summary>
<p>

**Abstract:** Quantitative analysis of in utero human brain development is crucial for abnormal characterization. Magnetic resonance image (MRI) segmentation is therefore an asset for quantitative analysis. However, the development of automated segmentation methods is hampered by the scarce availability of fetal brain MRI annotated datasets and the limited variability within these cohorts. In this context, we propose to leverage the power of fetal brain MRI super-resolution (SR) reconstruction methods to generate multiple reconstructions of a single subject with different parameters, thus as an efficient tuning-free data augmentation strategy. Overall, the latter significantly improves the generalization of segmentation methods over SR pipelines.

</p>
</details>

<details><summary><b>M$^2$M: A general method to perform various data analysis tasks from a differentially private sketch</b>
<a href="https://arxiv.org/abs/2211.14062">arxiv:2211.14062</a>
&#x1F4C8; 0 <br>
<p>Florimond Houssiau, Vincent Schellekens, Antoine Chatalic, Shreyas Kumar Annamraju, Yves-Alexandre de Montjoye</p></summary>
<p>

**Abstract:** Differential privacy is the standard privacy definition for performing analyses over sensitive data. Yet, its privacy budget bounds the number of tasks an analyst can perform with reasonable accuracy, which makes it challenging to deploy in practice. This can be alleviated by private sketching, where the dataset is compressed into a single noisy sketch vector which can be shared with the analysts and used to perform arbitrarily many analyses. However, the algorithms to perform specific tasks from sketches must be developed on a case-by-case basis, which is a major impediment to their use. In this paper, we introduce the generic moment-to-moment (M$^2$M) method to perform a wide range of data exploration tasks from a single private sketch. Among other things, this method can be used to estimate empirical moments of attributes, the covariance matrix, counting queries (including histograms), and regression models. Our method treats the sketching mechanism as a black-box operation, and can thus be applied to a wide variety of sketches from the literature, widening their ranges of applications without further engineering or privacy loss, and removing some of the technical barriers to the wider adoption of sketches for data exploration under differential privacy. We validate our method with data exploration tasks on artificial and real-world data, and show that it can be used to reliably estimate statistics and train classification models from private sketches.

</p>
</details>

<details><summary><b>Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization</b>
<a href="https://arxiv.org/abs/2211.14053">arxiv:2211.14053</a>
&#x1F4C8; 0 <br>
<p>Chen Zhao, Shuming Liu, Karttikeya Mangalam, Bernard Ghanem</p></summary>
<p>

**Abstract:** Temporal action localization (TAL) requires long-form reasoning to predict actions of various lengths and complex content. Given limited GPU memory, training TAL end-to-end on such long-form videos (i.e., from videos to predictions) is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a large variety of reversible networks are easily obtained from existing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible versions. Re2TAL reaches 37.01% average mAP, a new state-of-the-art record on ActivityNet-v1.3, and mAP 64.9% at tIoU=0.5 on THUMOS-14 without using optimal flow.

</p>
</details>

<details><summary><b>The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future</b>
<a href="https://arxiv.org/abs/2211.13960">arxiv:2211.13960</a>
&#x1F4C8; 0 <br>
<p>Philipp Hacker</p></summary>
<p>

**Abstract:** The optimal liability framework for AI systems remains an unsolved problem across the globe. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive and a revision of the Product Liability Directive. They constitute the final, and much-anticipated, cornerstone of AI regulation in the EU. Crucially, the liability proposals and the EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a Brussels effect in AI regulation, with significant consequences for the US and other countries.
  This paper makes three novel contributions. First, it examines in detail the Commission proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted as foreseen, AI liability in the EU will primarily rest on disclosure of evidence mechanisms and a set of narrowly defined presumptions concerning fault, defectiveness and causality. Hence, second, the article suggests amendments, which are collected in an Annex at the end of the paper. Third, based on an analysis of the key risks AI poses, the final part of the paper maps out a road for the future of AI liability and regulation, in the EU and beyond. This includes: a comprehensive framework for AI liability; provisions to support innovation; an extension to non-discrimination/algorithmic fairness, as well as explainable AI; and sustainability. I propose to jump-start sustainable AI regulation via sustainability impact assessments in the AI Act and sustainable design defects in the liability regime. In this way, the law may help spur not only fair AI and XAI, but potentially also sustainable AI (SAI).

</p>
</details>


{% endraw %}
Prev: [2022.11.24]({{ '/2022/11/24/2022.11.24.html' | relative_url }})  Next: [2022.11.26]({{ '/2022/11/26/2022.11.26.html' | relative_url }})