Prev: [2022.05.16]({{ '/2022/05/16/2022.05.16.html' | relative_url }})  Next: [2022.05.18]({{ '/2022/05/18/2022.05.18.html' | relative_url }})
{% raw %}
## Summary for 2022-05-17, created on 2022-05-21


<details><summary><b>An Evaluation Framework for Legal Document Summarization</b>
<a href="https://arxiv.org/abs/2205.08478">arxiv:2205.08478</a>
&#x1F4C8; 4590 <br>
<p>Ankan Mullick, Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, R Raghav, Roshni Kar</p></summary>
<p>

**Abstract:** A law practitioner has to go through numerous lengthy legal case proceedings for their practices of various categories, such as land dispute, corruption, etc. Hence, it is important to summarize these documents, and ensure that summaries contain phrases with intent matching the category of the case. To the best of our knowledge, there is no evaluation metric that evaluates a summary based on its intent. We propose an automated intent-based summarization metric, which shows a better agreement with human evaluation as compared to other automated metrics like BLEU, ROUGE-L etc. in terms of human satisfaction. We also curate a dataset by annotating intent phrases in legal documents, and show a proof of concept as to how this system can be automated. Additionally, all the code and data to generate reproducible results is available on Github.

</p>
</details>

<details><summary><b>Experiments on Generalizability of User-Oriented Fairness in Recommender Systems</b>
<a href="https://arxiv.org/abs/2205.08289">arxiv:2205.08289</a>
&#x1F4C8; 15 <br>
<p>Hossein A. Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, Mohammad Aliannejadi</p></summary>
<p>

**Abstract:** Recent work in recommender systems mainly focuses on fairness in recommendations as an important aspect of measuring recommendations quality. A fairness-aware recommender system aims to treat different user groups similarly. Relevant work on user-oriented fairness highlights the discriminative behavior of fairness-unaware recommendation algorithms towards a certain user group, defined based on users' activity level. Typical solutions include proposing a user-centered fairness re-ranking framework applied on top of a base ranking model to mitigate its unfair behavior towards a certain user group i.e., disadvantaged group. In this paper, we re-produce a user-oriented fairness study and provide extensive experiments to analyze the dependency of their proposed method on various fairness and recommendation aspects, including the recommendation domain, nature of the base ranking model, and user grouping method. Moreover, we evaluate the final recommendations provided by the re-ranking framework from both user- (e.g., NDCG, user-fairness) and item-side (e.g., novelty, item-fairness) metrics. We discover interesting trends and trade-offs between the model's performance in terms of different evaluation metrics. For instance, we see that the definition of the advantaged/disadvantaged user groups plays a crucial role in the effectiveness of the fairness algorithm and how it improves the performance of specific base ranking models. Finally, we highlight some important open challenges and future directions in this field. We release the data, evaluation pipeline, and the trained models publicly on https://github.com/rahmanidashti/FairRecSys.

</p>
</details>

<details><summary><b>OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval</b>
<a href="https://arxiv.org/abs/2205.08605">arxiv:2205.08605</a>
&#x1F4C8; 9 <br>
<p>Tong Niu, Kazuma Hashimoto, Yingbo Zhou, Caiming Xiong</p></summary>
<p>

**Abstract:** Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation. In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. When trained with all language pairs of a large-scale parallel multilingual corpus (OPUS-100), this model achieves the state-of-the-art result on the Tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. When finetuned on a single rich-resource language pair, be it English-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.

</p>
</details>

<details><summary><b>SKILL: Structured Knowledge Infusion for Large Language Models</b>
<a href="https://arxiv.org/abs/2205.08184">arxiv:2205.08184</a>
&#x1F4C8; 8 <br>
<p>Fedor Moiseev, Zhe Dong, Enrique Alfonseca, Martin Jaggi</p></summary>
<p>

**Abstract:** Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task compared to T5 baseline. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.

</p>
</details>

<details><summary><b>High-dimensional additive Gaussian processes under monotonicity constraints</b>
<a href="https://arxiv.org/abs/2205.08528">arxiv:2205.08528</a>
&#x1F4C8; 7 <br>
<p>Andrés F. López-Lopera, François Bachoc, Olivier Roustant</p></summary>
<p>

**Abstract:** We introduce an additive Gaussian process framework accounting for monotonicity constraints and scalable to high dimensions. Our contributions are threefold. First, we show that our framework enables to satisfy the constraints everywhere in the input space. We also show that more general componentwise linear inequality constraints can be handled similarly, such as componentwise convexity. Second, we propose the additive MaxMod algorithm for sequential dimension reduction. By sequentially maximizing a squared-norm criterion, MaxMod identifies the active input dimensions and refines the most important ones. This criterion can be computed explicitly at a linear cost. Finally, we provide open-source codes for our full framework. We demonstrate the performance and scalability of the methodology in several synthetic examples with hundreds of dimensions under monotonicity constraints as well as on a real-world flood application.

</p>
</details>

<details><summary><b>Conditional Visual Servoing for Multi-Step Tasks</b>
<a href="https://arxiv.org/abs/2205.08441">arxiv:2205.08441</a>
&#x1F4C8; 7 <br>
<p>Sergio Izquierdo, Max Argus, Thomas Brox</p></summary>
<p>

**Abstract:** Visual Servoing has been effectively used to move a robot into specific target locations or to track a recorded demonstration. It does not require manual programming, but it is typically limited to settings where one demonstration maps to one environment state. We propose a modular approach to extend visual servoing to scenarios with multiple demonstration sequences. We call this conditional servoing, as we choose the next demonstration conditioned on the observation of the robot. This method presents an appealing strategy to tackle multi-step problems, as individual demonstrations can be combined flexibly into a control policy. We propose different selection functions and compare them on a shape-sorting task in simulation. With the reprojection error yielding the best overall results, we implement this selection function on a real robot and show the efficacy of the proposed conditional servoing. For videos of our experiments, please check out our project page: https://lmb.informatik.uni-freiburg.de/projects/conditional_servoing/

</p>
</details>

<details><summary><b>blob loss: instance imbalance aware loss functions for semantic segmentation</b>
<a href="https://arxiv.org/abs/2205.08209">arxiv:2205.08209</a>
&#x1F4C8; 7 <br>
<p>Florian Kofler, Suprosanna Shit, Ivan Ezhov, Lucas Fidon, Izabela Horvath, Rami Al-Maskari, Hongwei Li, Harsharan Bhatia, Timo Loehr, Marie Piraud, Ali Erturk, Jan Kirschke, Jan Peeken, Tom Vercauteren, Claus Zimmer, Benedikt Wiestler, Bjoern Menze</p></summary>
<p>

**Abstract:** Deep convolutional neural networks have proven to be remarkably effective in semantic segmentation tasks. Most popular loss functions were introduced targeting improved volumetric scores, such as the Sorensen Dice coefficient. By design, DSC can tackle class imbalance; however, it does not recognize instance imbalance within a class. As a result, a large foreground instance can dominate minor instances and still produce a satisfactory Sorensen Dice coefficient. Nevertheless, missing out on instances will lead to poor detection performance. This represents a critical issue in applications such as disease progression monitoring. For example, it is imperative to locate and surveil small-scale lesions in the follow-up of multiple sclerosis patients. We propose a novel family of loss functions, nicknamed blob loss, primarily aimed at maximizing instance-level detection metrics, such as F1 score and sensitivity. Blob loss is designed for semantic segmentation problems in which the instances are the connected components within a class. We extensively evaluate a DSC-based blob loss in five complex 3D semantic segmentation tasks featuring pronounced instance heterogeneity in terms of texture and morphology. Compared to soft Dice loss, we achieve 5 percent improvement for MS lesions, 3 percent improvement for liver tumor, and an average 2 percent improvement for Microscopy segmentation tasks considering F1 score.

</p>
</details>

<details><summary><b>A psychological theory of explainability</b>
<a href="https://arxiv.org/abs/2205.08452">arxiv:2205.08452</a>
&#x1F4C8; 6 <br>
<p>Scott Cheng-Hsin Yang, Tomas Folke, Patrick Shafto</p></summary>
<p>

**Abstract:** The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explanations. The lack of theory means that validation of XAI must be done empirically, on a case-by-case basis, which prevents systematic theory-building in XAI. We propose a psychological theory of how humans draw conclusions from saliency maps, the most common form of XAI explanation, which for the first time allows for precise prediction of explainee inference conditioned on explanation. Our theory posits that absent explanation humans expect the AI to make similar decisions to themselves, and that they interpret an explanation by comparison to the explanations they themselves would give. Comparison is formalized via Shepard's universal law of generalization in a similarity space, a classic theory from cognitive science. A pre-registered user study on AI image classifications with saliency map explanations demonstrate that our theory quantitatively matches participants' predictions of the AI.

</p>
</details>

<details><summary><b>Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space</b>
<a href="https://arxiv.org/abs/2205.08129">arxiv:2205.08129</a>
&#x1F4C8; 6 <br>
<p>Kuan Fang, Patrick Yin, Ashvin Nair, Sergey Levine</p></summary>
<p>

**Abstract:** General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected data through offline reinforcement learning, and then fine-tunes the policy via online exploration. This fine-tuning process is itself facilitated by the planned subgoals, which breaks down the original target task into short-horizon goal-reaching tasks that are significantly easier to learn. We conduct experiments in both the simulation and real world, in which the policy is pre-trained on demonstrations of short primitive behaviors and fine-tuned for temporally extended tasks that are unseen in the offline data. Our experimental results show that PTP can generate feasible sequences of subgoals that enable the policy to efficiently solve the target tasks.

</p>
</details>

<details><summary><b>Practical Skills Demand Forecasting via Representation Learning of Temporal Dynamics</b>
<a href="https://arxiv.org/abs/2205.09508">arxiv:2205.09508</a>
&#x1F4C8; 5 <br>
<p>Maysa M. Garcia de Macedo, Wyatt Clarke, Eli Lucherini, Tyler Baldwin, Dilermando Queiroz Neto, Rogerio de Paula, Subhro Das</p></summary>
<p>

**Abstract:** Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.

</p>
</details>

<details><summary><b>Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.08698">arxiv:2205.08698</a>
&#x1F4C8; 5 <br>
<p>Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai</p></summary>
<p>

**Abstract:** Prediction intervals offer an effective tool for quantifying the uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to unforeseen changes in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles. It relies on the online learning ability of reinforcement learning to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve the quality of PIs. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.

</p>
</details>

<details><summary><b>Need is All You Need: Homeostatic Neural Networks Adapt to Concept Shift</b>
<a href="https://arxiv.org/abs/2205.08645">arxiv:2205.08645</a>
&#x1F4C8; 5 <br>
<p>Kingson Man, Antonio Damasio, Hartmut Neven</p></summary>
<p>

**Abstract:** In living organisms, homeostasis is the natural regulation of internal states aimed at maintaining conditions compatible with life. Typical artificial systems are not equipped with comparable regulatory features. Here, we introduce an artificial neural network that incorporates homeostatic features. Its own computing substrate is placed in a needful and vulnerable relation to the very objects over which it computes. For example, artificial neurons performing classification of MNIST digits or Fashion-MNIST articles of clothing may receive excitatory or inhibitory effects, which alter their own learning rate as a direct result of perceiving and classifying the digits. In this scenario, accurate recognition is desirable to the agent itself because it guides decisions to regulate its vulnerable internal states and functionality. Counterintuitively, the addition of vulnerability to a learner does not necessarily impair its performance. On the contrary, self-regulation in response to vulnerability confers benefits under certain conditions. We show that homeostatic design confers increased adaptability under concept shift, in which the relationships between labels and data change over time, and that the greatest advantages are obtained under the highest rates of shift. This necessitates the rapid un-learning of past associations and the re-learning of new ones. We also demonstrate the superior abilities of homeostatic learners in environments with dynamically changing rates of concept shift. Our homeostatic design exposes the artificial neural network's thinking machinery to the consequences of its own "thoughts", illustrating the advantage of putting one's own "skin in the game" to improve fluid intelligence.

</p>
</details>

<details><summary><b>Label-Efficient Self-Supervised Federated Learning for Tackling Data Heterogeneity in Medical Imaging</b>
<a href="https://arxiv.org/abs/2205.08576">arxiv:2205.08576</a>
&#x1F4C8; 5 <br>
<p>Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Lei Xing, Yuyin Zhou</p></summary>
<p>

**Abstract:** The curation of large-scale medical datasets from multiple institutions necessary for training deep learning models is challenged by the difficulty in sharing patient data with privacy-preserving. Federated learning (FL), a paradigm that enables privacy-protected collaborative learning among different institutions, is a promising solution to this challenge. However, FL generally suffers from performance deterioration due to heterogeneous data distributions across institutions and the lack of quality labeled data. In this paper, we present a robust and label-efficient self-supervised FL framework for medical image analysis. Specifically, we introduce a novel distributed self-supervised pre-training paradigm into the existing FL pipeline (i.e., pre-training the models directly on the decentralized target task datasets). Built upon the recent success of Vision Transformers, we employ masked image encoding tasks for self-supervised pre-training, to facilitate more effective knowledge transfer to downstream federated models. Extensive empirical results on simulated and real-world medical imaging federated datasets show that self-supervised pre-training largely benefits the robustness of federated models against various degrees of data heterogeneity. Notably, under severe data heterogeneity, our method, without relying on any additional pre-training data, achieves an improvement of 5.06%, 1.53% and 4.58% in test accuracy on retinal, dermatology and chest X-ray classification compared with the supervised baseline with ImageNet pre-training. Moreover, we show that our self-supervised FL algorithm generalizes well to out-of-distribution data and learns federated models more effectively in limited label scenarios, surpassing the supervised baseline by 10.36% and the semi-supervised FL method by 8.3% in test accuracy.

</p>
</details>

<details><summary><b>Disentangling Visual Embeddings for Attributes and Objects</b>
<a href="https://arxiv.org/abs/2205.08536">arxiv:2205.08536</a>
&#x1F4C8; 5 <br>
<p>Nirat Saini, Khoi Pham, Abhinav Shrivastava</p></summary>
<p>

**Abstract:** We study the problem of compositional zero-shot learning for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly distinct features associated with attributes. To overcome this challenge, these studies employ supervision from the linguistic space, and use pre-trained word embeddings to better separate and compose attribute-object pairs for recognition. Analogous to linguistic embedding space, which already has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and propose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decomposed features to hallucinate embeddings that are representative for the seen and novel compositions to better regularize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, models, and dataset splits are publicly available at https://github.com/nirat1606/OADis.

</p>
</details>

<details><summary><b>Pairwise Comparison Network for Remote Sensing Scene Classification</b>
<a href="https://arxiv.org/abs/2205.08147">arxiv:2205.08147</a>
&#x1F4C8; 5 <br>
<p>Zhang Yue, Zheng Xiangtao, Lu Xiaoqiang</p></summary>
<p>

**Abstract:** Remote sensing scene classification aims to assign a specific semantic label to a remote sensing image. Recently, convolutional neural networks have greatly improved the performance of remote sensing scene classification. However, some confused images may be easily recognized as the incorrect category, which generally degrade the performance. The differences between image pairs can be used to distinguish image categories. This paper proposed a pairwise comparison network, which contains two main steps: pairwise selection and pairwise representation. The proposed network first selects similar image pairs, and then represents the image pairs with pairwise representations. The self-representation is introduced to highlight the informative parts of each image itself, while the mutual-representation is proposed to capture the subtle differences between image pairs. Comprehensive experimental results on two challenging datasets (AID, NWPU-RESISC45) demonstrate the effectiveness of the proposed network. The code are provided in https://github.com/spectralpublic/PCNet.git.

</p>
</details>

<details><summary><b>Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey</b>
<a href="https://arxiv.org/abs/2205.08099">arxiv:2205.08099</a>
&#x1F4C8; 5 <br>
<p>Paul Wimmer, Jens Mehnert, Alexandru Paul Condurache</p></summary>
<p>

**Abstract:** State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of trainable parameters is shrunken which reduces gradient computations and the dimensionality of the model's optimization space. In this survey we first propose dimensionality reduced training as an underlying mathematical model that covers pruning and freezing during training. Afterwards, we present and discuss different dimensionality reduced training methods.

</p>
</details>

<details><summary><b>It Isn't Sh!tposting, It's My CAT Posting</b>
<a href="https://arxiv.org/abs/2205.08710">arxiv:2205.08710</a>
&#x1F4C8; 4 <br>
<p>Parthsarthi Rawat, Sayan Das, Jorge Aguirre, Akhil Daphara</p></summary>
<p>

**Abstract:** In this paper, we describe a novel architecture which can generate hilarious captions for a given input image. The architecture is split into two halves, i.e. image captioning and hilarious text conversion. The architecture starts with a pre-trained CNN model, VGG16 in this implementation, and applies attention LSTM on it to generate normal caption. These normal captions then are fed forward to our hilarious text conversion transformer which converts this text into something hilarious while maintaining the context of the input image. The architecture can also be split into two halves and only the seq2seq transformer can be used to generate hilarious caption by inputting a sentence.This paper aims to help everyday user to be more lazy and hilarious at the same time by generating captions using CATNet.

</p>
</details>

<details><summary><b>The Solvability of Interpretability Evaluation Metrics</b>
<a href="https://arxiv.org/abs/2205.08696">arxiv:2205.08696</a>
&#x1F4C8; 4 <br>
<p>Yilun Zhou, Julie Shah</p></summary>
<p>

**Abstract:** Feature attribution methods are popular for explaining neural network predictions, and they are often evaluated on metrics such as comprehensiveness and sufficiency, which are motivated by the principle that more important features -- as judged by the explanation -- should have larger impacts on model prediction. In this paper, we highlight an intriguing property of these metrics: their solvability. Concretely, we can define the problem of optimizing an explanation for a metric and solve it using beam search. This brings up the obvious question: given such solvability, why do we still develop other explainers and then evaluate them on the metric? We present a series of investigations showing that this beam search explainer is generally comparable or favorable to current choices such as LIME and SHAP, suggest rethinking the goals of model interpretability, and identify several directions towards better evaluations of new method proposals.

</p>
</details>

<details><summary><b>Hyperparameter Optimization with Neural Network Pruning</b>
<a href="https://arxiv.org/abs/2205.08695">arxiv:2205.08695</a>
&#x1F4C8; 4 <br>
<p>Kangil Lee, Junho Yim</p></summary>
<p>

**Abstract:** Since the deep learning model is highly dependent on hyperparameters, hyperparameter optimization is essential in developing deep learning model-based applications, even if it takes a long time. As service development using deep learning models has gradually become competitive, many developers highly demand rapid hyperparameter optimization algorithms. In order to keep pace with the needs of faster hyperparameter optimization algorithms, researchers are focusing on improving the speed of hyperparameter optimization algorithm. However, the huge time consumption of hyperparameter optimization due to the high computational cost of the deep learning model itself has not been dealt with in-depth. Like using surrogate model in Bayesian optimization, to solve this problem, it is necessary to consider proxy model for a neural network (N_B) to be used for hyperparameter optimization. Inspired by the main goal of neural network pruning, i.e., high computational cost reduction and performance preservation, we presumed that the neural network (N_P) obtained through neural network pruning would be a good proxy model of N_B. In order to verify our idea, we performed extensive experiments by using CIFAR10, CFIAR100, and TinyImageNet datasets and three generally-used neural networks and three representative hyperparameter optmization methods. Through these experiments, we verified that N_P can be a good proxy model of N_B for rapid hyperparameter optimization. The proposed hyperparameter optimization framework can reduce the amount of time up to 37%.

</p>
</details>

<details><summary><b>Policy Distillation with Selective Input Gradient Regularization for Efficient Interpretability</b>
<a href="https://arxiv.org/abs/2205.08685">arxiv:2205.08685</a>
&#x1F4C8; 4 <br>
<p>Jinwei Xing, Takashi Nagata, Xinyun Zou, Emre Neftci, Jeffrey L. Krichmar</p></summary>
<p>

**Abstract:** Although deep Reinforcement Learning (RL) has proven successful in a wide range of tasks, one challenge it faces is interpretability when applied to real-world problems. Saliency maps are frequently used to provide interpretability for deep neural networks. However, in the RL domain, existing saliency map approaches are either computationally expensive and thus cannot satisfy the real-time requirement of real-world scenarios or cannot produce interpretable saliency maps for RL policies. In this work, we propose an approach of Distillation with selective Input Gradient Regularization (DIGR) which uses policy distillation and input gradient regularization to produce new policies that achieve both high interpretability and computation efficiency in generating saliency maps. Our approach is also found to improve the robustness of RL policies to multiple adversarial attacks. We conduct experiments on three tasks, MiniGrid (Fetch Object), Atari (Breakout) and CARLA Autonomous Driving, to demonstrate the importance and effectiveness of our approach.

</p>
</details>

<details><summary><b>QAPPA: Quantization-Aware Power, Performance, and Area Modeling of DNN Accelerators</b>
<a href="https://arxiv.org/abs/2205.08648">arxiv:2205.08648</a>
&#x1F4C8; 4 <br>
<p>Ahmet Inci, Siri Garudanagiri Virupaksha, Aman Jain, Venkata Vivek Thallam, Ruizhou Ding, Diana Marculescu</p></summary>
<p>

**Abstract:** As the machine learning and systems community strives to achieve higher energy-efficiency through custom DNN accelerators and model compression techniques, there is a need for a design space exploration framework that incorporates quantization-aware processing elements into the accelerator design space while having accurate and fast power, performance, and area models. In this work, we present QAPPA, a highly parameterized quantization-aware power, performance, and area modeling framework for DNN accelerators. Our framework can facilitate the future research on design space exploration of DNN accelerators for various design choices such as bit precision, processing element type, scratchpad sizes of processing elements, global buffer size, device bandwidth, number of total processing elements in the the design, and DNN workloads. Our results show that different bit precisions and processing element types lead to significant differences in terms of performance per area and energy. Specifically, our proposed lightweight processing elements achieve up to 4.9x more performance per area and energy improvement when compared to INT16 based implementation.

</p>
</details>

<details><summary><b>Hierarchical Distribution-Aware Testing of Deep Learning</b>
<a href="https://arxiv.org/abs/2205.08589">arxiv:2205.08589</a>
&#x1F4C8; 4 <br>
<p>Wei Huang, Xingyu Zhao, Alec Banks, Victoria Cox, Xiaowei Huang</p></summary>
<p>

**Abstract:** With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Despite recent efforts made in detecting Adversarial Examples (AEs) via state-of-the-art attacking and testing methods, they are normally input distribution agnostic and/or disregard the perception quality of AEs. Consequently, the detected AEs are irrelevant inputs in the application context or unnatural/unrealistic that can be easily noticed by humans. This may lead to a limited effect on improving the DL model's dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this paper, we propose a new robustness testing approach for detecting AEs that considers both the input distribution and the perceptual quality of inputs. The two considerations are encoded by a novel hierarchical mechanism. First, at the feature level, the input data distribution is extracted and approximated by data compression techniques and probability density estimators. Such quantified feature level distribution, together with indicators that are highly correlated with local robustness, are considered in selecting test seeds. Given a test seed, we then develop a two-step genetic algorithm for local test case generation at the pixel level, in which two fitness functions work alternatively to control the quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions at feature and pixel levels is superior to state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only the quality of detected AEs but also improving the overall robustness of the DL model under testing.

</p>
</details>

<details><summary><b>The Power of Reuse: A Multi-Scale Transformer Model for Structural Dynamic Segmentation in Symbolic Music Generation</b>
<a href="https://arxiv.org/abs/2205.08579">arxiv:2205.08579</a>
&#x1F4C8; 4 <br>
<p>Guowei Wu, Shipei Liu, Xiaoya Fan</p></summary>
<p>

**Abstract:** Symbolic Music Generation relies on the contextual representation capabilities of the generative model, where the most prevalent approach is the Transformer-based model. Not only that, the learning of long-term context is also related to the dynamic segmentation of musical structures, i.e. intro, verse and chorus, which is currently overlooked by the research community. In this paper, we propose a multi-scale Transformer, which uses coarse-decoder and fine-decoders to model the contexts at the global and section-level, respectively. Concretely, we designed a Fragment Scope Localization layer to syncopate the music into sections, which were later used to pre-train fine-decoders. After that, we designed a Music Style Normalization layer to transfer the style information from the original sections to the generated sections to achieve consistency in music style. The generated sections are combined in the aggregation layer and fine-tuned by the coarse decoder. Our model is evaluated on two open MIDI datasets, and experiments show that our model outperforms the best contemporary symbolic music generative models. More excitingly, visual evaluation shows that our model is superior in melody reuse, resulting in more realistic music.

</p>
</details>

<details><summary><b>Unsupervised Segmentation in Real-World Images via Spelke Object Inference</b>
<a href="https://arxiv.org/abs/2205.08515">arxiv:2205.08515</a>
&#x1F4C8; 4 <br>
<p>Honglin Chen, Rahul Venkatesh, Yoni Friedman, Jiajun Wu, Joshua B. Tenenbaum, Daniel L. K. Yamins, Daniel M. Bear</p></summary>
<p>

**Abstract:** Self-supervised category-agnostic segmentation of real-world images into objects is a challenging open problem in computer vision. Here, we show how to learn static grouping priors from motion self-supervision, building on the cognitive science notion of Spelke Objects: groupings of stuff that move together. We introduce Excitatory-Inhibitory Segment Extraction Network (EISEN), which learns from optical flow estimates to extract pairwise affinity graphs for static scenes. EISEN then produces segments from affinities using a novel graph propagation and competition mechanism. Correlations between independent sources of motion (e.g. robot arms) and objects they move are resolved into separate segments through a bootstrapping training process. We show that EISEN achieves a substantial improvement in the state of the art for self-supervised segmentation on challenging synthetic and real-world robotic image datasets. We also present an ablation analysis illustrating the importance of each element of the EISEN architecture.

</p>
</details>

<details><summary><b>Recovering Private Text in Federated Learning of Language Models</b>
<a href="https://arxiv.org/abs/2205.08514">arxiv:2205.08514</a>
&#x1F4C8; 4 <br>
<p>Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, Danqi Chen</p></summary>
<p>

**Abstract:** Federated learning allows distributed users to collaboratively train a model while keeping each user's data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models -- for the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Different from image-recovery methods which are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. The key insight of our attack is to leverage either prior knowledge in pre-trained language models or memorization during training. Despite its simplicity, we demonstrate that FILM can work well with several large-scale datasets -- it can extract single sentences with high fidelity even for large batch sizes and recover multiple sentences from the batch successfully if the attack is applied iteratively. We hope our results can motivate future work in developing stronger attacks as well as new defense methods for training language models in federated learning. Our code is publicly available at https://github.com/Princeton-SysML/FILM.

</p>
</details>

<details><summary><b>Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer</b>
<a href="https://arxiv.org/abs/2205.08467">arxiv:2205.08467</a>
&#x1F4C8; 4 <br>
<p>Haiqing Zhang, Chen Li, Shiliang Ai, Haoyuan Chen, Yuchao Zheng, Yixin Li, Xiaoyan Li, Hongzan Sun, Xinyu Huang, Marcin Grzegorzek</p></summary>
<p>

**Abstract:** The gold standard for gastric cancer detection is gastric histopathological image analysis, but there are certain drawbacks in the existing histopathological detection and diagnosis. In this paper, based on the study of computer aided diagnosis system, graph based features are applied to gastric cancer histopathology microscopic image analysis, and a classifier is used to classify gastric cancer cells from benign cells. Firstly, image segmentation is performed, and after finding the region, cell nuclei are extracted using the k-means method, the minimum spanning tree (MST) is drawn, and graph based features of the MST are extracted. The graph based features are then put into the classifier for classification. In this study, different segmentation methods are compared in the tissue segmentation stage, among which are Level-Set, Otsu thresholding, watershed, SegNet, U-Net and Trans-U-Net segmentation; Graph based features, Red, Green, Blue features, Grey-Level Co-occurrence Matrix features, Histograms of Oriented Gradient features and Local Binary Patterns features are compared in the feature extraction stage; Radial Basis Function (RBF) Support Vector Machine (SVM), Linear SVM, Artificial Neural Network, Random Forests, k-NearestNeighbor, VGG16, and Inception-V3 are compared in the classifier stage. It is found that using U-Net to segment tissue areas, then extracting graph based features, and finally using RBF SVM classifier gives the optimal results with 94.29%.

</p>
</details>

<details><summary><b>CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI</b>
<a href="https://arxiv.org/abs/2205.08239">arxiv:2205.08239</a>
&#x1F4C8; 4 <br>
<p>Liu Li, Qiang Ma, Matthew Sinclair, Antonios Makropoulos, Joseph Hajnal, A. David Edwards, Bernhard Kainz, Daniel Rueckert, Amir Alansary</p></summary>
<p>

**Abstract:** Fetal Magnetic Resonance Imaging (MRI) is used in prenatal diagnosis and to assess early brain development. Accurate segmentation of the different brain tissues is a vital step in several brain analysis tasks, such as cortical surface reconstruction and tissue thickness measurements. Fetal MRI scans, however, are prone to motion artifacts that can affect the correctness of both manual and automatic segmentation techniques. In this paper, we propose a novel network structure that can simultaneously generate conditional atlases and predict brain tissue segmentation, called CAS-Net. The conditional atlases provide anatomical priors that can constrain the segmentation connectivity, despite the heterogeneity of intensity values caused by motion or partial volume effects. The proposed method is trained and evaluated on 253 subjects from the developing Human Connectome Project (dHCP). The results demonstrate that the proposed method can generate conditional age-specific atlas with sharp boundary and shape variance. It also segment multi-category brain tissues for fetal MRI with a high overall Dice similarity coefficient (DSC) of $85.2\%$ for the selected 9 tissue labels.

</p>
</details>

<details><summary><b>SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation</b>
<a href="https://arxiv.org/abs/2205.08180">arxiv:2205.08180</a>
&#x1F4C8; 4 <br>
<p>Sameer Khurana, Antoine Laurent, James Glass</p></summary>
<p>

**Abstract:** We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation learning framework. Unlike previous works on speech representation learning, which learns multilingual contextual speech embedding at the resolution of an acoustic frame (10-20ms), this work focuses on learning multimodal (speech-text) multilingual speech embedding at the resolution of a sentence (5-10s) such that the embedding vector space is semantically aligned across different languages. We combine state-of-the-art multilingual acoustic frame-level speech representation learning model XLS-R with the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an utterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we train SAMU-XLSR with only multilingual transcribed speech data, cross-lingual speech-text and speech-speech associations emerge in its learned representation space. To substantiate our claims, we use SAMU-XLSR speech encoder in combination with a pre-trained LaBSE text sentence encoder for cross-lingual speech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual speech-to-speech translation retrieval. We highlight these applications by performing several cross-lingual text and speech translation retrieval tasks across several datasets.

</p>
</details>

<details><summary><b>Spatial-Temporal Interactive Dynamic Graph Convolution Network for Traffic Forecasting</b>
<a href="https://arxiv.org/abs/2205.08689">arxiv:2205.08689</a>
&#x1F4C8; 3 <br>
<p>Aoyu Liu, Yaying Zhang</p></summary>
<p>

**Abstract:** Accurate traffic forecasting is essential for smart cities to achieve traffic flow control, route planning, and detection. Although many spatial-temporal methods are currently proposed, these methods are deficient in capturing the spatial-temporal dependence of traffic data synchronously. In addition, most of the methods ignore the dynamically changing correlations between road network nodes that arise as traffic data changes. To address the above challenges, we propose a neural network-based Spatial-Temporal Interactive Dynamic Graph Convolutional Network (STIDGCN) for traffic forecasting in this paper. In STIDGCN, we propose an interactive dynamic graph convolution structure, which first divides the sequences at intervals and captures the spatial-temporal dependence of the traffic data simultaneously through an interactive learning strategy for effective long-term prediction. We propose a novel dynamic graph convolution module consisting of a graph generator, fusion graph convolution. The dynamic graph convolution module can use the input traffic data, pre-defined graph structure to generate a graph structure and fuse it with the defined adaptive adjacency matrix, which is used to achieve the filling of the pre-defined graph structure and simulate the generation of dynamic associations between nodes in the road network. Extensive experiments on four real-world traffic flow datasets demonstrate that STIDGCN outperforms the state-of-the-art baseline.

</p>
</details>

<details><summary><b>DPO: Dynamic-Programming Optimization on Hybrid Constraints</b>
<a href="https://arxiv.org/abs/2205.08632">arxiv:2205.08632</a>
&#x1F4C8; 3 <br>
<p>Vu H. N. Phan, Moshe Y. Vardi</p></summary>
<p>

**Abstract:** In Bayesian inference, the most probable explanation (MPE) problem requests a variable instantiation with the highest probability given some evidence. Since a Bayesian network can be encoded as a literal-weighted CNF formula $\varphi$, we study Boolean MPE, a more general problem that requests a model $τ$ of $\varphi$ with the highest weight, where the weight of $τ$ is the product of weights of literals satisfied by $τ$. It is known that Boolean MPE can be solved via reduction to (weighted partial) MaxSAT. Recent work proposed DPMC, a dynamic-programming model counter that leverages graph-decomposition techniques to construct project-join trees. A project-join tree is an execution plan that specifies how to conjoin clauses and project out variables. We build on DPMC and introduce DPO, a dynamic-programming optimizer that exactly solves Boolean MPE. By using algebraic decision diagrams (ADDs) to represent pseudo-Boolean (PB) functions, DPO is able to handle disjunctive clauses as well as XOR clauses. (Cardinality constraints and PB constraints may also be compactly represented by ADDs, so one can further extend DPO's support for hybrid inputs.) To test the competitiveness of DPO, we generate random XOR-CNF formulas. On these hybrid benchmarks, DPO significantly outperforms MaxHS, UWrMaxSat, and GaussMaxHS, which are state-of-the-art exact solvers for MaxSAT.

</p>
</details>

<details><summary><b>Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2205.08625">arxiv:2205.08625</a>
&#x1F4C8; 3 <br>
<p>Nidhi Vakil, Hadi Amiri</p></summary>
<p>

**Abstract:** We present a generic and trend-aware curriculum learning approach for graph neural networks. It extends existing approaches by incorporating sample-level loss trends to better discriminate easier from harder samples and schedule them for training. The model effectively integrates textual and structural information for relation extraction in text graphs. Experimental results show that the model provides robust estimations of sample difficulty and shows sizable improvement over the state-of-the-art approaches across several datasets.

</p>
</details>

<details><summary><b>Multibit Tries Packet Classification with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.08606">arxiv:2205.08606</a>
&#x1F4C8; 3 <br>
<p>Hasibul Jamil, Ning Weng</p></summary>
<p>

**Abstract:** High performance packet classification is a key component to support scalable network applications like firewalls, intrusion detection, and differentiated services. With ever increasing in the line-rate in core networks, it becomes a great challenge to design a scalable and high performance packet classification solution using hand-tuned heuristics approaches. In this paper, we present a scalable learning-based packet classification engine and its performance evaluation. By exploiting the sparsity of ruleset, our algorithm uses a few effective bits (EBs) to extract a large number of candidate rules with just a few of memory access. These effective bits are learned with deep reinforcement learning and they are used to create a bitmap to filter out the majority of rules which do not need to be full-matched to improve the online system performance. Moreover, our EBs learning-based selection method is independent of the ruleset, which can be applied to varying rulesets. Our multibit tries classification engine outperforms lookup time both in worst and average case by 55% and reduce memory footprint, compared to traditional decision tree without EBs.

</p>
</details>

<details><summary><b>Learning Quantum Entanglement Distillation with Noisy Classical Communications</b>
<a href="https://arxiv.org/abs/2205.08561">arxiv:2205.08561</a>
&#x1F4C8; 3 <br>
<p>Hari Hara Suthan Chittoor, Osvaldo Simeone</p></summary>
<p>

**Abstract:** Quantum networking relies on the management and exploitation of entanglement. Practical sources of entangled qubits are imperfect, producing mixed quantum state with reduced fidelity with respect to ideal Bell pairs. Therefore, an important primitive for quantum networking is entanglement distillation, whose goal is to enhance the fidelity of entangled qubits through local operations and classical communication (LOCC). Existing distillation protocols assume the availability of ideal, noiseless, communication channels. In this paper, we study the case in which communication takes place over noisy binary symmetric channels. We propose to implement local processing through parameterized quantum circuits (PQCs) that are optimized to maximize the average fidelity, while accounting for communication errors. The introduced approach, Noise Aware-LOCCNet (NA-LOCCNet), is shown to have significant advantages over existing protocols designed for noiseless communications.

</p>
</details>

<details><summary><b>Do Neural Networks Compress Manifolds Optimally?</b>
<a href="https://arxiv.org/abs/2205.08518">arxiv:2205.08518</a>
&#x1F4C8; 3 <br>
<p>Sourbh Bhadane, Aaron B. Wagner, Johannes Ballé</p></summary>
<p>

**Abstract:** Artificial Neural-Network-based (ANN-based) lossy compressors have recently obtained striking results on several sources. Their success may be ascribed to an ability to identify the structure of low-dimensional manifolds in high-dimensional ambient spaces. Indeed, prior work has shown that ANN-based compressors can achieve the optimal entropy-distortion curve for some such sources. In contrast, we determine the optimal entropy-distortion tradeoffs for two low-dimensional manifolds with circular structure and show that state-of-the-art ANN-based compressors fail to optimally compress the sources, especially at high rates.

</p>
</details>

<details><summary><b>Utterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation</b>
<a href="https://arxiv.org/abs/2205.08455">arxiv:2205.08455</a>
&#x1F4C8; 3 <br>
<p>William Ravenscroft, Stefan Goetze, Thomas Hain</p></summary>
<p>

**Abstract:** Speech dereverberation is an important stage in many speech technology applications. Recent work in this area has been dominated by deep neural network models. Temporal convolutional networks (TCNs) are deep learning models that have been proposed for sequence modelling in the task of dereverberating speech. In this work a weighted multi-dilation depthwise-separable convolution is proposed to replace standard depthwise-separable convolutions in TCN models. This proposed convolution enables the TCN to dynamically focus on more or less local information in its receptive field at each convolutional block in the network. It is shown that this weighted multi-dilation temporal convolutional network (WD-TCN) consistently outperforms the TCN across various model configurations and using the WD-TCN model is a more parameter efficient method to improve the performance of the model than increasing the number of convolutional blocks. The best performance improvement over the baseline TCN is 0.55 dB scale-invariant signal-to-distortion ratio (SISDR) and the best performing WD-TCN model attains 12.26 dB SISDR on the WHAMR dataset.

</p>
</details>

<details><summary><b>HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images</b>
<a href="https://arxiv.org/abs/2205.08390">arxiv:2205.08390</a>
&#x1F4C8; 3 <br>
<p>Yuhao Mo, Chu Han, Yu Liu, Min Liu, Zhenwei Shi, Jiatai Lin, Bingchao Zhao, Chunwang Huang, Bingjiang Qiu, Yanfen Cui, Lei Wu, Xipeng Pan, Zeyan Xu, Xiaomei Huang, Zaiyi Liu, Ying Wang, Changhong Liang</p></summary>
<p>

**Abstract:** Ultrasonography is an important routine examination for breast cancer diagnosis, due to its non-invasive, radiation-free and low-cost properties. However, it is still not the first-line screening test for breast cancer due to its inherent limitations. It would be a tremendous success if we can precisely diagnose breast cancer by breast ultrasound images (BUS). Many learning-based computer-aided diagnostic methods have been proposed to achieve breast cancer diagnosis/lesion classification. However, most of them require a pre-define ROI and then classify the lesion inside the ROI. Conventional classification backbones, such as VGG16 and ResNet50, can achieve promising classification results with no ROI requirement. But these models lack interpretability, thus restricting their use in clinical practice. In this study, we propose a novel ROI-free model for breast cancer diagnosis in ultrasound images with interpretable feature representations. We leverage the anatomical prior knowledge that malignant and benign tumors have different spatial relationships between different tissue layers, and propose a HoVer-Transformer to formulate this prior knowledge. The proposed HoVer-Trans block extracts the inter- and intra-layer spatial information horizontally and vertically. We conduct and release an open dataset GDPH&GYFYY for breast cancer diagnosis in BUS. The proposed model is evaluated in three datasets by comparing with four CNN-based models and two vision transformer models via a five-fold cross validation. It achieves state-of-the-art classification performance with the best model interpretability.

</p>
</details>

<details><summary><b>A unified framework for dataset shift diagnostics</b>
<a href="https://arxiv.org/abs/2205.08340">arxiv:2205.08340</a>
&#x1F4C8; 3 <br>
<p>Felipe Maia Polo, Rafael Izbicki, Evanildo Gomes Lacerda Jr, Juan Pablo Ibieta-Jimenez, Renato Vicente</p></summary>
<p>

**Abstract:** Most machine learning (ML) methods assume that the data used in the training phase comes from the distribution of the target population. However, in practice one often faces dataset shift, which, if not properly taken into account, may decrease the predictive performance of the ML models. In general, if the practitioner knows which type of shift is taking place - e.g., covariate shift or label shift - they may apply transfer learning methods to obtain better predictions. Unfortunately, current methods for detecting shift are only designed to detect specific types of shift or cannot formally test their presence. We introduce a general framework that gives insights on how to improve prediction methods by detecting the presence of different types of shift and quantifying how strong they are. Our approach can be used for any data type (tabular/image/text) and both for classification and regression tasks. Moreover, it uses formal hypotheses tests that controls false alarms. We illustrate how our framework is useful in practice using both artificial and real datasets. Our package for dataset shift detection can be found in https://github.com/felipemaiapolo/detectshift.

</p>
</details>

<details><summary><b>Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation</b>
<a href="https://arxiv.org/abs/2205.08316">arxiv:2205.08316</a>
&#x1F4C8; 3 <br>
<p>Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Abhinav Valada</p></summary>
<p>

**Abstract:** In recent years, policy learning methods using either reinforcement or imitation have made significant progress. However, both techniques still suffer from being computationally expensive and requiring large amounts of training data. This problem is especially prevalent in real-world robotic manipulation tasks, where access to ground truth scene features is not available and policies are instead learned from raw camera observations. In this paper, we demonstrate the efficacy of learning image keypoints via the Dense Correspondence pretext task for downstream policy learning. Extending prior work to challenging multi-object scenes, we show that our model can be trained to deal with important problems in representation learning, primarily scale-invariance and occlusion. We evaluate our approach on diverse robot manipulation tasks, compare it to other visual representation learning approaches, and demonstrate its flexibility and effectiveness for sample-efficient policy learning.

</p>
</details>

<details><summary><b>A two-steps approach to improve the performance of Android malware detectors</b>
<a href="https://arxiv.org/abs/2205.08265">arxiv:2205.08265</a>
&#x1F4C8; 3 <br>
<p>Nadia Daoudi, Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein</p></summary>
<p>

**Abstract:** The popularity of Android OS has made it an appealing target to malware developers. To evade detection, including by ML-based techniques, attackers invest in creating malware that closely resemble legitimate apps. In this paper, we propose GUIDED RETRAINING, a supervised representation learning-based method that boosts the performance of a malware detector. First, the dataset is split into "easy" and "difficult" samples, where difficulty is associated to the prediction probabilities yielded by a malware detector: for difficult samples, the probabilities are such that the classifier is not confident on the predictions, which have high error rates. Then, we apply our GUIDED RETRAINING method on the difficult samples to improve their classification. For the subset of "easy" samples, the base malware detector is used to make the final predictions since the error rate on that subset is low by construction. For the subset of "difficult" samples, we rely on GUIDED RETRAINING, which leverages the correct predictions and the errors made by the base malware detector to guide the retraining process. GUIDED RETRAINING focuses on the difficult samples: it learns new embeddings of these samples using Supervised Contrastive Learning and trains an auxiliary classifier for the final predictions. We validate our method on four state-of-the-art Android malware detection approaches using over 265k malware and benign apps, and we demonstrate that GUIDED RETRAINING can reduce up to 40.41% prediction errors made by the malware detectors. Our method is generic and designed to enhance the classification performance on a binary classification task. Consequently, it can be applied to other classification problems beyond Android malware detection.

</p>
</details>

<details><summary><b>Delaytron: Efficient Learning of Multiclass Classifiers with Delayed Bandit Feedbacks</b>
<a href="https://arxiv.org/abs/2205.08234">arxiv:2205.08234</a>
&#x1F4C8; 3 <br>
<p>Naresh Manwani, Mudit Agarwal</p></summary>
<p>

**Abstract:** In this paper, we present online algorithm called {\it Delaytron} for learning multi class classifiers using delayed bandit feedbacks. The sequence of feedback delays $\{d_t\}_{t=1}^T$ is unknown to the algorithm. At the $t$-th round, the algorithm observes an example $\mathbf{x}_t$ and predicts a label $\tilde{y}_t$ and receives the bandit feedback $\mathbb{I}[\tilde{y}_t=y_t]$ only $d_t$ rounds later. When $t+d_t>T$, we consider that the feedback for the $t$-th round is missing. We show that the proposed algorithm achieves regret of $\mathcal{O}\left(\sqrt{\frac{2 K}γ\left[\frac{T}{2}+\left(2+\frac{L^2}{R^2\Vert \W\Vert_F^2}\right)\sum_{t=1}^Td_t\right]}\right)$ when the loss for each missing sample is upper bounded by $L$. In the case when the loss for missing samples is not upper bounded, the regret achieved by Delaytron is $\mathcal{O}\left(\sqrt{\frac{2 K}γ\left[\frac{T}{2}+2\sum_{t=1}^Td_t+\vert \mathcal{M}\vert T\right]}\right)$ where $\mathcal{M}$ is the set of missing samples in $T$ rounds. These bounds were achieved with a constant step size which requires the knowledge of $T$ and $\sum_{t=1}^Td_t$. For the case when $T$ and $\sum_{t=1}^Td_t$ are unknown, we use a doubling trick for online learning and proposed Adaptive Delaytron. We show that Adaptive Delaytron achieves a regret bound of $\mathcal{O}\left(\sqrt{T+\sum_{t=1}^Td_t}\right)$. We show the effectiveness of our approach by experimenting on various datasets and comparing with state-of-the-art approaches.

</p>
</details>

<details><summary><b>Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility</b>
<a href="https://arxiv.org/abs/2205.08187">arxiv:2205.08187</a>
&#x1F4C8; 3 <br>
<p>Hoil Lee, Fadhel Ayed, Paul Jung, Juho Lee, Hongseok Yang, François Caron</p></summary>
<p>

**Abstract:** This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a Lévy measure on the positive reals. If the scalar parameters are strictly positive and the Lévy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the Lévy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian processes (MoGP) in the large-width limit. The behaviour of the neural network in this regime is very different from the GP regime. One obtains correlated outputs, with non-Gaussian distributions, possibly with heavy tails. Additionally, we show that, in this regime, the weights are compressible, and feature learning is possible. Many sparsity-promoting neural network models can be recast as special cases of our approach, and we discuss their infinite-width limits; we also present an asymptotic analysis of the pruning error. We illustrate some of the benefits of the MoGP regime over the GP regime in terms of representation learning and compressibility on simulated, MNIST and Fashion MNIST datasets.

</p>
</details>

<details><summary><b>CellTypeGraph: A New Geometric Computer Vision Benchmark</b>
<a href="https://arxiv.org/abs/2205.08166">arxiv:2205.08166</a>
&#x1F4C8; 3 <br>
<p>Lorenzo Cerrone, Athul Vijayan, Tejasvinee Mody, Kay Schneitz, Fred A. Hamprecht</p></summary>
<p>

**Abstract:** Classifying all cells in an organ is a relevant and difficult problem from plant developmental biology. We here abstract the problem into a new benchmark for node classification in a geo-referenced graph. Solving it requires learning the spatial layout of the organ including symmetries. To allow the convenient testing of new geometrical learning methods, the benchmark of Arabidopsis thaliana ovules is made available as a PyTorch data loader, along with a large number of precomputed features. Finally, we benchmark eight recent graph neural network architectures, finding that DeeperGCN currently works best on this problem.

</p>
</details>

<details><summary><b>Uncertainty-based Network for Few-shot Image Classification</b>
<a href="https://arxiv.org/abs/2205.08157">arxiv:2205.08157</a>
&#x1F4C8; 3 <br>
<p>Minglei Yuan, Qian Xu, Chunhao Cai, Yin-Dong Zheng, Tao Wang, Tong Lu</p></summary>
<p>

**Abstract:** The transductive inference is an effective technique in the few-shot learning task, where query sets update prototypes to improve themselves. However, these methods optimize the model by considering only the classification scores of the query instances as confidence while ignoring the uncertainty of these classification scores. In this paper, we propose a novel method called Uncertainty-Based Network, which models the uncertainty of classification results with the help of mutual information. Specifically, we first data augment and classify the query instance and calculate the mutual information of these classification scores. Then, mutual information is used as uncertainty to assign weights to classification scores, and the iterative update strategy based on classification scores and uncertainties assigns the optimal weights to query instances in prototype optimization. Extensive results on four benchmarks show that Uncertainty-Based Network achieves comparable performance in classification accuracy compared to state-of-the-art method.

</p>
</details>

<details><summary><b>Dark Solitons in Bose-Einstein Condensates: A Dataset for Many-body Physics Research</b>
<a href="https://arxiv.org/abs/2205.09114">arxiv:2205.09114</a>
&#x1F4C8; 2 <br>
<p>Amilson R. Fritsch, Shangjie Guo, Sophia M. Koh, I. B. Spielman, Justyna P. Zwolak</p></summary>
<p>

**Abstract:** We establish a dataset of over $1.6\times10^4$ experimental images of Bose-Einstein condensates containing solitonic excitations to enable machine learning (ML) for many-body physics research. About 33 % of this dataset has manually assigned and carefully curated labels. The remainder is automatically labeled using SolDet -- an implementation of a physics-informed ML data analysis framework -- consisting of a convolutional-neural-network-based classifier and object detector as well as a statistically motivated physics-informed classifier and a quality metric. This technical note constitutes the definitive reference of the dataset, providing an opportunity for the data science community to develop more sophisticated analysis tools, to further understand nonlinear many-body physics, and even advance cold atom experiments.

</p>
</details>

<details><summary><b>Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation</b>
<a href="https://arxiv.org/abs/2205.08675">arxiv:2205.08675</a>
&#x1F4C8; 2 <br>
<p>Kevin Yang, Olivia Deng, Charles Chen, Richard Shin, Subhro Roy, Benjamin Van Durme</p></summary>
<p>

**Abstract:** We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a low-resource setting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we observe 33% relative improvement over a non-data-augmented baseline in top-1 match.

</p>
</details>

<details><summary><b>Intuitive and Efficient Human-robot Collaboration via Real-time Approximate Bayesian Inference</b>
<a href="https://arxiv.org/abs/2205.08657">arxiv:2205.08657</a>
&#x1F4C8; 2 <br>
<p>Javier Felip Leon, David Gonzalez-Aguirre, Lama Nachman</p></summary>
<p>

**Abstract:** The combination of collaborative robots and end-to-end AI, promises flexible automation of human tasks in factories and warehouses. However, such promise seems a few breakthroughs away. In the meantime, humans and cobots will collaborate helping each other. For these collaborations to be effective and safe, robots need to model, predict and exploit human's intents for responsive decision making processes.
  Approximate Bayesian Computation (ABC) is an analysis-by-synthesis approach to perform probabilistic predictions upon uncertain quantities. ABC includes priors conveniently, leverages sampling algorithms for inference and is flexible to benefit from complex models, e.g. via simulators. However, ABC is known to be computationally too intensive to run at interactive frame rates required for effective human-robot collaboration tasks.
  In this paper, we formulate human reaching intent prediction as an ABC problem and describe two key performance innovations which allow computations at interactive rates. Our real-world experiments with a collaborative robot set-up, demonstrate the viability of our proposed approach. Experimental evaluations convey the advantages and value of human intent prediction for packing cooperative tasks. Qualitative results show how anticipating human's reaching intent improves human-robot collaboration without compromising safety. Quantitative task fluency metrics confirm the qualitative claims.

</p>
</details>

<details><summary><b>Frank Wolfe Meets Metric Entropy</b>
<a href="https://arxiv.org/abs/2205.08634">arxiv:2205.08634</a>
&#x1F4C8; 2 <br>
<p>Suhas Vijaykumar</p></summary>
<p>

**Abstract:** The Frank-Wolfe algorithm has seen a resurgence in popularity due to its ability to efficiently solve constrained optimization problems in machine learning and high-dimensional statistics. As such, there is much interest in establishing when the algorithm may possess a "linear" $O(\log(1/ε))$ dimension-free iteration complexity comparable to projected gradient descent.
  In this paper, we provide a general technique for establishing domain specific and easy-to-estimate lower bounds for Frank-Wolfe and its variants using the metric entropy of the domain. Most notably, we show that a dimension-free linear upper bound must fail not only in the worst case, but in the \emph{average case}: for a Gaussian or spherical random polytope in $\mathbb{R}^d$ with $\mathrm{poly}(d)$ vertices, Frank-Wolfe requires up to $\tildeΩ(d)$ iterations to achieve a $O(1/d)$ error bound, with high probability. We also establish this phenomenon for the nuclear norm ball.
  The link with metric entropy also has interesting positive implications for conditional gradient algorithms in statistics, such as gradient boosting and matching pursuit. In particular, we show that it is possible to extract fast-decaying upper bounds on the excess risk directly from an analysis of the underlying optimization procedure.

</p>
</details>

<details><summary><b>Classification as Direction Recovery: Improved Guarantees via Scale Invariance</b>
<a href="https://arxiv.org/abs/2205.08633">arxiv:2205.08633</a>
&#x1F4C8; 2 <br>
<p>Suhas Vijaykumar, Claire Lazar Reich</p></summary>
<p>

**Abstract:** Modern algorithms for binary classification rely on an intermediate regression problem for computational tractability. In this paper, we establish a geometric distinction between classification and regression that allows risk in these two settings to be more precisely related. In particular, we note that classification risk depends only on the direction of the regressor, and we take advantage of this scale invariance to improve existing guarantees for how classification risk is bounded by the risk in the intermediate regression problem. Building on these guarantees, our analysis makes it possible to compare algorithms more accurately against each other and suggests viewing classification as unique from regression rather than a byproduct of it. While regression aims to converge toward the conditional expectation function in location, we propose that classification should instead aim to recover its direction.

</p>
</details>

<details><summary><b>Geographical Distance Is The New Hyperparameter: A Case Study Of Finding The Optimal Pre-trained Language For English-isiZulu Machine Translation</b>
<a href="https://arxiv.org/abs/2205.08621">arxiv:2205.08621</a>
&#x1F4C8; 2 <br>
<p>Muhammad Umair Nasir, Innocent Amos Mchechesi</p></summary>
<p>

**Abstract:** Stemming from the limited availability of datasets and textual resources for low-resource languages such as isiZulu, there is a significant need to be able to harness knowledge from pre-trained models to improve low resource machine translation. Moreover, a lack of techniques to handle the complexities of morphologically rich languages has compounded the unequal development of translation models, with many widely spoken African languages being left behind. This study explores the potential benefits of transfer learning in an English-isiZulu translation framework. The results indicate the value of transfer learning from closely related languages to enhance the performance of low-resource translation models, thus providing a key strategy for low-resource translation going forward. We gathered results from 8 different language corpora, including one multi-lingual corpus, and saw that isiXhosa-isiZulu outperformed all languages, with a BLEU score of 8.56 on the test set which was better from the multi-lingual corpora pre-trained model by 2.73. We also derived a new coefficient, Nasir's Geographical Distance Coefficient (NGDC) which provides an easy selection of languages for the pre-trained models. NGDC also indicated that isiXhosa should be selected as the language for the pre-trained model.

</p>
</details>

<details><summary><b>Bagged Polynomial Regression and Neural Networks</b>
<a href="https://arxiv.org/abs/2205.08609">arxiv:2205.08609</a>
&#x1F4C8; 2 <br>
<p>Sylvia Klosin, Jaume Vives-i-Bastida</p></summary>
<p>

**Abstract:** Series and polynomial regression are able to approximate the same function classes as neural networks. However, these methods are rarely used in practice, although they offer more interpretability than neural networks. In this paper, we show that a potential reason for this is the slow convergence rate of polynomial regression estimators and propose the use of bagged polynomial regression (BPR) as an attractive alternative to neural networks. Theoretically, we derive new finite sample and asymptotic $L^2$ convergence rates for series estimators. We show that the rates can be improved in smooth settings by splitting the feature space and generating polynomial features separately for each partition. Empirically, we show that our proposed estimator, the BPR, can perform as well as more complex models with more parameters. Our estimator also performs close to state-of-the-art prediction methods in the benchmark MNIST handwritten digit dataset.

</p>
</details>

<details><summary><b>Deep Neural Network Classifier for Multi-dimensional Functional Data</b>
<a href="https://arxiv.org/abs/2205.08592">arxiv:2205.08592</a>
&#x1F4C8; 2 <br>
<p>Shuoyang Wang, Guanqun Cao, Zuofeng Shang</p></summary>
<p>

**Abstract:** We propose a new approach, called as functional deep neural network (FDNN), for classifying multi-dimensional functional data. Specifically, a deep neural network is trained based on the principle components of the training data which shall be used to predict the class label of a future data function. Unlike the popular functional discriminant analysis approaches which rely on Gaussian assumption, the proposed FDNN approach applies to general non-Gaussian multi-dimensional functional data. Moreover, when the log density ratio possesses a locally connected functional modular structure, we show that FDNN achieves minimax optimality. The superiority of our approach is demonstrated through both simulated and real-world datasets.

</p>
</details>

<details><summary><b>Experimentally realized in situ backpropagation for deep learning in nanophotonic neural networks</b>
<a href="https://arxiv.org/abs/2205.08501">arxiv:2205.08501</a>
&#x1F4C8; 2 <br>
<p>Sunil Pai, Zhanghao Sun, Tyler W. Hughes, Taewon Park, Ben Bartlett, Ian A. D. Williamson, Momchil Minkov, Maziyar Milanizadeh, Nathnael Abebe, Francesco Morichetti, Andrea Melloni, Shanhui Fan, Olav Solgaard, David A. B. Miller</p></summary>
<p>

**Abstract:** Neural networks are widely deployed models across many scientific disciplines and commercial endeavors ranging from edge computing and sensing to large-scale signal processing in data centers. The most efficient and well-entrenched method to train such networks is backpropagation, or reverse-mode automatic differentiation. To counter an exponentially increasing energy budget in the artificial intelligence sector, there has been recent interest in analog implementations of neural networks, specifically nanophotonic neural networks for which no analog backpropagation demonstration exists. We design mass-manufacturable silicon photonic neural networks that alternately cascade our custom designed "photonic mesh" accelerator with digitally implemented nonlinearities. These reconfigurable photonic meshes program computationally intensive arbitrary matrix multiplication by setting physical voltages that tune the interference of optically encoded input data propagating through integrated Mach-Zehnder interferometer networks. Here, using our packaged photonic chip, we demonstrate in situ backpropagation for the first time to solve classification tasks and evaluate a new protocol to keep the entire gradient measurement and update of physical device voltages in the analog domain, improving on past theoretical proposals. Our method is made possible by introducing three changes to typical photonic meshes: (1) measurements at optical "grating tap" monitors, (2) bidirectional optical signal propagation automated by fiber switch, and (3) universal generation and readout of optical amplitude and phase. After training, our classification achieves accuracies similar to digital equivalents even in presence of systematic error. Our findings suggest a new training paradigm for photonics-accelerated artificial intelligence based entirely on a physical analog of the popular backpropagation technique.

</p>
</details>

<details><summary><b>Dynamic Recognition of Speakers for Consent Management by Contrastive Embedding Replay</b>
<a href="https://arxiv.org/abs/2205.08459">arxiv:2205.08459</a>
&#x1F4C8; 2 <br>
<p>Arash Shahmansoori, Utz Roedig</p></summary>
<p>

**Abstract:** Voice assistants record sound and can overhear conversations. Thus, a consent management mechanism is desirable such that users can express their wish to be recorded or not. Consent management can be implemented using speaker recognition; users that do not give consent enrol their voice and all further recordings of these users is subsequently not processed. Building speaker recognition based consent management is challenging due to the dynamic nature of the problem, required scalability for large number of speakers, and need for fast speaker recognition with high accuracy. This paper describes a speaker recognition based consent management system addressing the aforementioned challenges. A fully supervised batch contrastive learning is applied to learn the underlying speaker equivariance inductive bias during the training on the set of speakers noting recording dissent. Speakers that do not provide consent are grouped in buckets which are trained continuously. The embeddings are contrastively learned for speakers in their buckets during training and act later as a replay buffer for classification. The buckets are progressively registered during training and a novel multi-strided random sampling of the contrastive embedding replay buffer is proposed. Buckets are contrastively trained for a few steps only in each iteration and replayed for classification progressively leading to fast convergence. An algorithm for fast and dynamic registration and removal of speakers in buckets is described. The evaluation results show that the proposed approach provides the desired fast and dynamic solution for consent management and outperforms existing approaches in terms of convergence speed and adaptive capabilities as well as verification performance during inference.

</p>
</details>

<details><summary><b>On the Privacy of Decentralized Machine Learning</b>
<a href="https://arxiv.org/abs/2205.08443">arxiv:2205.08443</a>
&#x1F4C8; 2 <br>
<p>Dario Pasquini, Mathilde Raynal, Carmela Troncoso</p></summary>
<p>

**Abstract:** In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at circumventing the main limitations of federated learning. We identify the decentralized learning properties that affect users' privacy and we introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantages over more practical approaches such as federated learning. Rather, it tends to degrade users' privacy by increasing the attack surface and enabling any user in the system to perform powerful privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also reveal that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require abandoning any possible advantage over the federated setup, completely defeating the objective of the decentralized approach.

</p>
</details>

<details><summary><b>Perturbation of Deep Autoencoder Weights for Model Compression and Classification of Tabular Data</b>
<a href="https://arxiv.org/abs/2205.08358">arxiv:2205.08358</a>
&#x1F4C8; 2 <br>
<p>Manar Samad, Sakib Abrar</p></summary>
<p>

**Abstract:** Fully connected deep neural networks (DNN) often include redundant weights leading to overfitting and high memory requirements. Additionally, the performance of DNN is often challenged by traditional machine learning models in tabular data classification. In this paper, we propose periodical perturbations (prune and regrow) of DNN weights, especially at the self-supervised pre-training stage of deep autoencoders. The proposed weight perturbation strategy outperforms dropout learning in four out of six tabular data sets in downstream classification tasks. The L1 or L2 regularization of weights at the same pretraining stage results in inferior classification performance compared to dropout or our weight perturbation routine. Unlike dropout learning, the proposed weight perturbation routine additionally achieves 15% to 40% sparsity across six tabular data sets for the compression of deep pretrained models. Our experiments reveal that a pretrained deep autoencoder with weight perturbation or dropout can outperform traditional machine learning in tabular data classification when fully connected DNN fails miserably. However, traditional machine learning models appear superior to any deep models when a tabular data set contains uncorrelated variables. Therefore, the success of deep models can be attributed to the inevitable presence of correlated variables in real-world data sets.

</p>
</details>

<details><summary><b>Accurate Machine Learned Quantum-Mechanical Force Fields for Biomolecular Simulations</b>
<a href="https://arxiv.org/abs/2205.08306">arxiv:2205.08306</a>
&#x1F4C8; 2 <br>
<p>Oliver T. Unke, Martin Stöhr, Stefan Ganscha, Thomas Unterthiner, Hartmut Maennel, Sergii Kashubin, Daniel Ahlin, Michael Gastegger, Leonardo Medrano Sandonas, Alexandre Tkatchenko, Klaus-Robert Müller</p></summary>
<p>

**Abstract:** Molecular dynamics (MD) simulations allow atomistic insights into chemical and biological processes. Accurate MD simulations require computationally demanding quantum-mechanical calculations, being practically limited to short timescales and few atoms. For larger systems, efficient, but much less reliable empirical force fields are used. Recently, machine learned force fields (MLFFs) emerged as an alternative means to execute MD simulations, offering similar accuracy as ab initio methods at orders-of-magnitude speedup. Until now, MLFFs mainly capture short-range interactions in small molecules or periodic materials, due to the increased complexity of constructing models and obtaining reliable reference data for large molecules, where long-ranged many-body effects become important. This work proposes a general approach to constructing accurate MLFFs for large-scale molecular simulations (GEMS) by training on "bottom-up" and "top-down" molecular fragments of varying size, from which the relevant physicochemical interactions can be learned. GEMS is applied to study the dynamics of alanine-based peptides and the 46-residue protein crambin in aqueous solution, allowing nanosecond-scale MD simulations of >25k atoms at essentially ab initio quality. Our findings suggest that structural motifs in peptides and proteins are more flexible than previously thought, indicating that simulations at ab initio accuracy might be necessary to understand dynamic biomolecular processes such as protein (mis)folding, drug-protein binding, or allosteric regulation.

</p>
</details>

<details><summary><b>Semi-Parametric Contextual Bandits with Graph-Laplacian Regularization</b>
<a href="https://arxiv.org/abs/2205.08295">arxiv:2205.08295</a>
&#x1F4C8; 2 <br>
<p>Young-Geun Choi, Gi-Soo Kim, Seunghoon Paik, Myunghee Cho Paik</p></summary>
<p>

**Abstract:** Non-stationarity is ubiquitous in human behavior and addressing it in the contextual bandits is challenging. Several works have addressed the problem by investigating semi-parametric contextual bandits and warned that ignoring non-stationarity could harm performances. Another prevalent human behavior is social interaction which has become available in a form of a social network or graph structure. As a result, graph-based contextual bandits have received much attention. In this paper, we propose "SemiGraphTS," a novel contextual Thompson-sampling algorithm for a graph-based semi-parametric reward model. Our algorithm is the first to be proposed in this setting. We derive an upper bound of the cumulative regret that can be expressed as a multiple of a factor depending on the graph structure and the order for the semi-parametric model without a graph. We evaluate the proposed and existing algorithms via simulation and real data example.

</p>
</details>

<details><summary><b>Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers</b>
<a href="https://arxiv.org/abs/2205.08288">arxiv:2205.08288</a>
&#x1F4C8; 2 <br>
<p>Davide Locatelli, Ariadna Quattoni</p></summary>
<p>

**Abstract:** Prior to deep learning the semantic parsing community has been interested in understanding and modeling the range of possible word alignments between natural language sentences and their corresponding meaning representations. Sequence-to-sequence models changed the research landscape suggesting that we no longer need to worry about alignments since they can be learned automatically by means of an attention mechanism. More recently, researchers have started to question such premise. In this work we investigate whether seq2seq models can handle both simple and complex alignments. To answer this question we augment the popular Geo semantic parsing dataset with alignment annotations and create Geo-Aligned. We then study the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments. Our empirical study shows that performance is significantly better over monotonic alignments.

</p>
</details>

<details><summary><b>KGNN: Distributed Framework for Graph Neural Knowledge Representation</b>
<a href="https://arxiv.org/abs/2205.08285">arxiv:2205.08285</a>
&#x1F4C8; 2 <br>
<p>Binbin Hu, Zhiyang Hu, Zhiqiang Zhang, Jun Zhou, Chuan Shi</p></summary>
<p>

**Abstract:** Knowledge representation learning has been commonly adopted to incorporate knowledge graph (KG) into various online services. Although existing knowledge representation learning methods have achieved considerable performance improvement, they ignore high-order structure and abundant attribute information, resulting unsatisfactory performance on semantics-rich KGs. Moreover, they fail to make prediction in an inductive manner and cannot scale to large industrial graphs. To address these issues, we develop a novel framework called KGNN to take full advantage of knowledge data for representation learning in the distributed learning system. KGNN is equipped with GNN based encoder and knowledge aware decoder, which aim to jointly explore high-order structure and attribute information together in a fine-grained fashion and preserve the relation patterns in KGs, respectively. Extensive experiments on three datasets for link prediction and triplet classification task demonstrate the effectiveness and scalability of KGNN framework.

</p>
</details>

<details><summary><b>Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning</b>
<a href="https://arxiv.org/abs/2205.08274">arxiv:2205.08274</a>
&#x1F4C8; 2 <br>
<p>Ailisi Li, Xueyao Jiang, Bang Liu, Jiaqing Liang, Yanghua Xiao</p></summary>
<p>

**Abstract:** Math Word Problems (MWP) is an important task that requires the ability of understanding and reasoning over mathematical text. Existing approaches mostly formalize it as a generation task by adopting Seq2Seq or Seq2Tree models to encode an input math problem in natural language as a global representation and generate the output mathematical expression. Such approaches only learn shallow heuristics and fail to capture fine-grained variations in inputs. In this paper, we propose to model a math word problem in a fine-to-coarse manner to capture both the local fine-grained information and the global logical structure of it. Instead of generating a complete equation sequence or expression tree from the global features, we iteratively combine low-level operands to predict a higher-level operator, abstracting the problem and reasoning about the solving operators from bottom to up. Our model is naturally more sensitive to local variations and can better generalize to unseen problem types. Extensive evaluations on Math23k and SVAMP datasets demonstrate the accuracy and robustness of our method.

</p>
</details>

<details><summary><b>Sharp asymptotics on the compression of two-layer neural networks</b>
<a href="https://arxiv.org/abs/2205.08199">arxiv:2205.08199</a>
&#x1F4C8; 2 <br>
<p>Mohammad Hossein Amani, Simone Bombari, Marco Mondelli, Rattana Pukdee, Stefano Rini</p></summary>
<p>

**Abstract:** In this paper, we study the compression of a target two-layer neural network with N nodes into a compressed network with M < N nodes. More precisely, we consider the setting in which the weights of the target network are i.i.d. sub-Gaussian, and we minimize the population L2 loss between the outputs of the target and of the compressed network, under the assumption of Gaussian inputs. By using tools from high-dimensional probability, we show that this non-convex problem can be simplified when the target network is sufficiently over-parameterized, and provide the error rate of this approximation as a function of the input dimension and N . For a ReLU activation function, we conjecture that the optimum of the simplified optimization problem is achieved by taking weights on the Equiangular Tight Frame (ETF), while the scaling of the weights and the orientation of the ETF depend on the parameters of the target network. Numerical evidence is provided to support this conjecture.

</p>
</details>

<details><summary><b>SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection</b>
<a href="https://arxiv.org/abs/2205.08159">arxiv:2205.08159</a>
&#x1F4C8; 2 <br>
<p>Prabhav Singh, Ridam Srivastava, K. P. S. Rana, Vineet Kumar</p></summary>
<p>

**Abstract:** Fake News Detection (FND) is an essential field in natural language processing that aims to identify and check the truthfulness of major claims in a news article to decide the news veracity. FND finds its uses in preventing social, political and national damage caused due to misrepresentation of facts which may harm a certain section of society. Further, with the explosive rise in fake news dissemination over social media, including images and text, it has become imperative to identify fake news faster and more accurately. To solve this problem, this work investigates a novel multimodal stacked ensemble-based approach (SEMIFND) to fake news detection. Focus is also kept on ensuring faster performance with fewer parameters. Moreover, to improve multimodal performance, a deep unimodal analysis is done on the image modality to identify NasNet Mobile as the most appropriate model for the task. For text, an ensemble of BERT and ELECTRA is used. The approach was evaluated on two datasets: Twitter MediaEval and Weibo Corpus. The suggested framework offered accuracies of 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These reported metrics are found to be superior when compared to similar recent works. Further, we also report a reduction in the number of parameters used in training when compared to recent relevant works. SEMI-FND offers an overall parameter reduction of at least 20% with unimodal parametric reduction on text being 60%. Therefore, based on the investigations presented, it is concluded that the application of a stacked ensembling significantly improves FND over other approaches while also improving speed.

</p>
</details>

<details><summary><b>Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation</b>
<a href="https://arxiv.org/abs/2205.08143">arxiv:2205.08143</a>
&#x1F4C8; 2 <br>
<p>Yu Wang, Binbin Zhu, Lingsi Kong, Jianlin Wang, Bin Gao, Jianhua Wang, Dingcheng Tian, Yudong Yao</p></summary>
<p>

**Abstract:** Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve block anesthesia method that can observe the target nerve and its surrounding structures, the puncture needle's advancement, and local anesthetics spread in real-time. The key in UGNB is nerve identification. With the help of deep learning methods, the automatic identification or segmentation of nerves can be realized, assisting doctors in completing nerve block anesthesia accurately and efficiently. Here, we establish a public dataset containing 320 ultrasound images of brachial plexus (BP). Three experienced doctors jointly produce the BP segmentation ground truth and label brachial plexus trunks. We design a brachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys achieves experienced-doctor-level nerve identification performance in various experiments. We evaluate BPSegSys' performance in terms of intersection-over-union (IoU), a commonly used performance measure for segmentation experiments. Considering three dataset groups in our established public dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029, respectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced doctors. In addition, we show that BPSegSys can help doctors identify brachial plexus trunks more accurately, with IoU improvement up to 27%, which has significant clinical application value.

</p>
</details>

<details><summary><b>Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland</b>
<a href="https://arxiv.org/abs/2205.08123">arxiv:2205.08123</a>
&#x1F4C8; 2 <br>
<p>Tommi Keski-Filppula, Marko Nikki, Marianne Haapea, Naglis Ramanauskas, Osmo Tervonen</p></summary>
<p>

**Abstract:** Objectives: To assess the use of artificial intelligence-based software in ruling out chest X-ray cases, with no significant findings in a primary health care setting.
  Methods: In this retrospective study, a commercially available artificial intelligence (AI) software was used to analyse 10 000 chest X-rays of Finnish primary health care patients. In studies with a mismatch between an AI normal report and the original radiologist report, a consensus read by two board-certified radiologists was conducted to make the final diagnosis.
  Results: After the exclusion of cases not meeting the study criteria, 9579 cases were analysed by AI. Of these cases, 4451 were considered normal in the original radiologist report and 4644 after the consensus reading. The number of cases correctly found nonsignificant by AI was 1692 (17.7% of all studies and 36.4% of studies with no significant findings). After the consensus read, there were nine confirmed false-negative studies. These studies included four cases of slightly enlarged heart size, four cases of slightly increased pulmonary opacification and one case with a small unilateral pleural effusion. This gives the AI a sensitivity of 99.8% (95% CI= 99.65-99.92) and specificity of 36.4 % (95% CI= 35.05-37.84) for recognising significant pathology on a chest X-ray.
  Conclusions: AI was able to correctly rule out 36.4% of chest X-rays with no significant findings of primary health care patients, with a minimal number of false negatives that would lead to effectively no compromise on patient safety. No critical findings were missed by the software.

</p>
</details>

<details><summary><b>ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks</b>
<a href="https://arxiv.org/abs/2205.08119">arxiv:2205.08119</a>
&#x1F4C8; 2 <br>
<p>Haoran You, Baopu Li, Huihong Shi, Yonggan Fu, Yingyan Lin</p></summary>
<p>

**Abstract:** Neural networks (NNs) with intensive multiplications (e.g., convolutions and transformers) are capable yet power hungry, impeding their more extensive deployment into resource-constrained devices. As such, multiplication-free networks, which follow a common practice in energy-efficient hardware implementation to parameterize NNs with more efficient operators (e.g., bitwise shifts and additions), have gained growing attention. However, multiplication-free networks usually under-perform their vanilla counterparts in terms of the achieved accuracy. To this end, this work advocates hybrid NNs that consist of both powerful yet costly multiplications and efficient yet less powerful operators for marrying the best of both worlds, and proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs. Our ShiftAddNAS highlights two enablers. Specifically, it integrates (1) the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs; and (2) a novel weight sharing strategy that enables effective weight sharing among different operators that follow heterogeneous distributions (e.g., Gaussian for convolutions vs. Laplacian for add operators) and simultaneously leads to a largely reduced supernet size and much better searched networks. Extensive experiments and ablation studies on various models, datasets, and tasks consistently validate the efficacy of ShiftAddNAS, e.g., achieving up to a +7.7% higher accuracy or a +4.9 better BLEU score compared to state-of-the-art NN, while leading to up to 93% or 69% energy and latency savings, respectively. Codes and pretrained models are available at https://github.com/RICE-EIC/ShiftAddNAS.

</p>
</details>

<details><summary><b>Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients</b>
<a href="https://arxiv.org/abs/2205.08106">arxiv:2205.08106</a>
&#x1F4C8; 2 <br>
<p>Chia-Hung Yang, Yun-Chien Cheng, Chin Kuo</p></summary>
<p>

**Abstract:** The purpose of this research is to develop a system that generates simulated computed tomography pulmonary angiography (CTPA) images clinically for pulmonary embolism diagnoses. Nowadays, CTPA images are the gold standard computerized detection method to determine and identify the symptoms of pulmonary embolism (PE), although performing CTPA is harmful for patients and also expensive. Therefore, we aim to detect possible PE patients through CT images. The system will simulate CTPA images with deep learning models for the identification of PE patients' symptoms, providing physicians with another reference for determining PE patients. In this study, the simulated CTPA image generation system uses a generative antagonistic network to enhance the features of pulmonary vessels in the CT images to strengthen the reference value of the images and provide a basis for hospitals to judge PE patients. We used the CT images of 22 patients from National Cheng Kung University Hospital and the corresponding CTPA images as the training data for the task of simulating CTPA images and generated them using two sets of generative countermeasure networks. This study is expected to propose a new approach to the clinical diagnosis of pulmonary embolism, in which a deep learning network is used to assist in the complex screening process and to review the generated simulated CTPA images, allowing physicians to assess whether a patient needs to undergo detailed testing for CTPA, improving the speed of detection of pulmonary embolism and significantly reducing the number of undetected patients.

</p>
</details>

<details><summary><b>Can We Do Better Than Random Start? The Power of Data Outsourcing</b>
<a href="https://arxiv.org/abs/2205.08098">arxiv:2205.08098</a>
&#x1F4C8; 2 <br>
<p>Yi Chen, Jing Dong, Xin T. Tong</p></summary>
<p>

**Abstract:** Many organizations have access to abundant data but lack the computational power to process the data. While they can outsource the computational task to other facilities, there are various constraints on the amount of data that can be shared. It is natural to ask what can data outsourcing accomplish under such constraints. We address this question from a machine learning perspective. When training a model with optimization algorithms, the quality of the results often relies heavily on the points where the algorithms are initialized. Random start is one of the most popular methods to tackle this issue, but it can be computationally expensive and not feasible for organizations lacking computing resources. Based on three different scenarios, we propose simulation-based algorithms that can utilize a small amount of outsourced data to find good initial points accordingly. Under suitable regularity conditions, we provide theoretical guarantees showing the algorithms can find good initial points with high probability. We also conduct numerical experiments to demonstrate that our algorithms perform significantly better than the random start approach.

</p>
</details>

<details><summary><b>Predicting failure characteristics of structural materials via deep learning based on nondestructive void topology</b>
<a href="https://arxiv.org/abs/2205.09075">arxiv:2205.09075</a>
&#x1F4C8; 1 <br>
<p>Leslie Ching Ow Tiong, Gunjick Lee, Seok Su Sohn, Donghun Kim</p></summary>
<p>

**Abstract:** Accurate predictions of the failure progression of structural materials is critical for preventing failure-induced accidents. Despite considerable mechanics modeling-based efforts, accurate prediction remains a challenging task in real-world environments due to unexpected damage factors and defect evolutions. Here, we report a novel method for predicting material failure characteristics that uniquely combines nondestructive X-ray computed tomography (X-CT), persistent homology (PH), and deep multimodal learning (DML). The combined method exploits the microstructural defect state at the time of material examination as an input, and outputs the failure-related properties. Our method is demonstrated to be effective using two types of fracture datasets (tensile and fatigue datasets) with ferritic low alloy steel as a representative structural material. The method achieves a mean absolute error (MAE) of 0.09 in predicting the local strain with the tensile dataset and an MAE of 0.14 in predicting the fracture progress with the fatigue dataset. These high accuracies are mainly due to PH processing of the X-CT images, which transforms complex and noisy three-dimensional X-CT images into compact two-dimensional persistence diagrams that preserve key topological features such as the internal void size, density, and distribution. The combined PH and DML processing of 3D X-CT data is our unique approach enabling reliable failure predictions at the time of material examination based on void topology progressions, and the method can be extended to various nondestructive failure tests for practical use.

</p>
</details>

<details><summary><b>A Pulse-and-Glide-driven Adaptive Cruise Control System for Electric Vehicle</b>
<a href="https://arxiv.org/abs/2205.08682">arxiv:2205.08682</a>
&#x1F4C8; 1 <br>
<p>Zhaofeng Tian, Liangkai Liu, Weisong Shi</p></summary>
<p>

**Abstract:** As the adaptive cruise control system (ACCS) on vehicles is well-developed today, vehicle manufacturers have increasingly employed this technology in new-generation intelligent vehicles. Pulse-and-glide (PnG) strategy is an efficacious driving strategy to diminish fuel consumption in traditional oil-fueled vehicles. However, current studies rarely focus on the verification of the energy-saving effect of PnG on an electric vehicle (EV) and embedding PnG in ACCS. This paper proposes a pulse-and-glide-driven adaptive cruise control system (PGACCS) model which leverages PnG strategy as a parallel function with cruise control (CC) and verifies that PnG is an efficacious energy-saving strategy on EV by optimizing the energy cost of the PnG operation using Intelligent Genetic Algorithm and Particle Swarm Optimization (IGPSO). This paper builds up a simulation model of an EV with regenerative braking and ACCS based on which the performance of PGACCS and regenerative braking is evaluated; the PnG energy performance is optimized and the effect of regenerative braking on PnG energy performance is evaluated. As a result of PnG optimization, the PnG operation in the PGACCS could cut down 28.3% energy cost of the EV compared to the CC operation in the traditional ACCS which verifies that PnG is an effective energy-saving strategy for EV and PGACCS is a promising option for EV.

</p>
</details>

<details><summary><b>Learning to Learn Quantum Turbo Detection</b>
<a href="https://arxiv.org/abs/2205.08611">arxiv:2205.08611</a>
&#x1F4C8; 1 <br>
<p>Bryan Liu, Toshiaki Koike-Akino, Ye Wang, Kieran Parsons</p></summary>
<p>

**Abstract:** This paper investigates a turbo receiver employing a variational quantum circuit (VQC). The VQC is configured with an ansatz of the quantum approximate optimization algorithm (QAOA). We propose a 'learning to learn' (L2L) framework to optimize the turbo VQC decoder such that high fidelity soft-decision output is generated. Besides demonstrating the proposed algorithm's computational complexity, we show that the L2L VQC turbo decoder can achieve an excellent performance close to the optimal maximum-likelihood performance in a multiple-input multiple-output system.

</p>
</details>

<details><summary><b>All-Photonic Artificial Neural Network Processor Via Non-linear Optics</b>
<a href="https://arxiv.org/abs/2205.08608">arxiv:2205.08608</a>
&#x1F4C8; 1 <br>
<p>Jasvith Raj Basani, Mikkel Heuck, Dirk R. Englund, Stefan Krastanov</p></summary>
<p>

**Abstract:** Optics and photonics has recently captured interest as a platform to accelerate linear matrix processing, that has been deemed as a bottleneck in traditional digital electronic architectures. In this paper, we propose an all-photonic artificial neural network processor wherein information is encoded in the amplitudes of frequency modes that act as neurons. The weights among connected layers are encoded in the amplitude of controlled frequency modes that act as pumps. Interaction among these modes for information processing is enabled by non-linear optical processes. Both the matrix multiplication and element-wise activation functions are performed through coherent processes, enabling the direct representation of negative and complex numbers without the use of detectors or digital electronics. Via numerical simulations, we show that our design achieves a performance commensurate with present-day state-of-the-art computational networks on image-classification benchmarks. Our architecture is unique in providing a completely unitary, reversible mode of computation. Additionally, the computational speed increases with the power of the pumps to arbitrarily high rates, as long as the circuitry can sustain the higher optical power.

</p>
</details>

<details><summary><b>Variational Quantum Compressed Sensing for Joint User and Channel State Acquisition in Grant-Free Device Access Systems</b>
<a href="https://arxiv.org/abs/2205.08603">arxiv:2205.08603</a>
&#x1F4C8; 1 <br>
<p>Bryan Liu, Toshiaki Koike-Akino, Ye Wang, Kieran Parsons</p></summary>
<p>

**Abstract:** This paper introduces a new quantum computing framework integrated with a two-step compressed sensing technique, applied to a joint channel estimation and user identification problem. We propose a variational quantum circuit (VQC) design as a new denoising solution. For a practical grant-free communications system having correlated device activities, variational quantum parameters for Pauli rotation gates in the proposed VQC system are optimized to facilitate to the non-linear estimation. Numerical results show that the VQC method can outperform modern compressed sensing techniques using an element-wise denoiser.

</p>
</details>

<details><summary><b>Universal characteristics of deep neural network loss surfaces from random matrix theory</b>
<a href="https://arxiv.org/abs/2205.08601">arxiv:2205.08601</a>
&#x1F4C8; 1 <br>
<p>Nicholas P Baskerville, Jonathan P Keating, Francesco Mezzadri, Joseph Najnudel, Diego Granziol</p></summary>
<p>

**Abstract:** This paper considers several aspects of random matrix universality in deep neural networks. Motivated by recent experimental work, we use universal properties of random matrices related to local statistics to derive practical implications for deep neural networks based on a realistic model of their Hessians. In particular we derive universal aspects of outliers in the spectra of deep neural networks and demonstrate the important role of random matrix local laws in popular pre-conditioning gradient descent algorithms. We also present insights into deep neural network loss surfaces from quite general arguments based on tools from statistical physics and random matrix theory.

</p>
</details>

<details><summary><b>Supervised Learning for Coverage-Directed Test Selection in Simulation-Based Verification</b>
<a href="https://arxiv.org/abs/2205.08524">arxiv:2205.08524</a>
&#x1F4C8; 1 <br>
<p>Nyasha Masamba, Kerstin Eder, Tim Blackmore</p></summary>
<p>

**Abstract:** Constrained random test generation is one the most widely adopted methods for generating stimuli for simulation-based verification. Randomness leads to test diversity, but tests tend to repeatedly exercise the same design logic. Constraints are written (typically manually) to bias random tests towards interesting, hard-to-reach, and yet-untested logic. However, as verification progresses, most constrained random tests yield little to no effect on functional coverage. If stimuli generation consumes significantly less resources than simulation, then a better approach involves randomly generating a large number tests, selecting the most effective subset, and only simulating that subset. In this paper, we introduce a novel method for automatic constraint extraction and test selection. This method, which we call coverage-directed test selection, is based on supervised learning from coverage feedback. Our method biases selection towards tests that have a high probability of increasing functional coverage, and prioritises them for simulation. We show how coverage-directed test selection can reduce manual constraint writing, prioritise effective tests, reduce verification resource consumption, and accelerate coverage closure on a large, real-life industrial hardware design.

</p>
</details>

<details><summary><b>Towards the optimization of ballistics in proton therapy using genetic algorithms: implementation issues</b>
<a href="https://arxiv.org/abs/2205.08283">arxiv:2205.08283</a>
&#x1F4C8; 1 <br>
<p>François Smekens, Nicolas Freud, Bruno Sixou, Guillaume Beslon, Jean M Létang</p></summary>
<p>

**Abstract:** The dose delivered to the planning target volume by proton beams is highly conformal, sparing organs at risk and normal tissues. New treatment planning systems adapted to spot scanning techniques have been recently proposed to simultaneously optimize several fields and thus improve dose delivery. In this paper, we investigate a new optimization framework based on a genetic algorithm approach. This tool is intended to make it possible to explore new schemes of treatment delivery, possibly with future enhanced technologies. The optimization framework is designed to be versatile and to account for many degrees of freedom, without any {\it a priori} technological constraint. To test the behavior of our algorithm, we propose in this paper, as an example, to optimize beam fluences, target points and irradiation directions at the same time.
  The proposed optimization routine takes typically into account several thousands of spots of fixed size. The evolution is carried out by the three standard genetic operators: mutation, crossover and selection. The figure-of-merit (or fitness) is based on an objective function relative to the dose prescription to the tumor and to the limits set for organs at risk and normal tissues. Fluence optimization is carried out via a specific scheme based on a plain gradient with analytical solution. Several specific genetic algorithm issues are addressed: (i) the mutation rate is tuned to balance the search and selection forces, (ii) the initial population is selected using a bootstrap technique and (iii) to scale down the computation time, dose calculations are carried out with a fast analytical ray tracing method and are multi-threaded.
  In this paper implementation issues of the optimization framework are thoroughly described. The behavior of the proposed genetic algorithm is illustrated in both elementary and clinically-realistic test cases.

</p>
</details>

<details><summary><b>Adaptive Momentum-Based Policy Gradient with Second-Order Information</b>
<a href="https://arxiv.org/abs/2205.08253">arxiv:2205.08253</a>
&#x1F4C8; 1 <br>
<p>Saber Salehkaleybar, Sadegh Khorasani, Negar Kiyavash, Niao He, Patrick Thiran</p></summary>
<p>

**Abstract:** The variance reduced gradient estimators for policy gradient methods has been one of the main focus of research in the reinforcement learning in recent years as they allow acceleration of the estimation process. We propose a variance reduced policy gradient method, called SGDHess-PG, which incorporates second-order information into stochastic gradient descent (SGD) using momentum with an adaptive learning rate. SGDHess-PG algorithm can achieve $ε$-approximate first-order stationary point with $\tilde{O}(ε^{-3})$ number of trajectories, while using a batch size of $O(1)$ at each iteration. Unlike most previous work, our proposed algorithm does not require importance sampling techniques which can compromise the advantage of variance reduction process. Our extensive experimental results show the effectiveness of the proposed algorithm on various control tasks and its advantage over the state of the art in practice.

</p>
</details>

<details><summary><b>Monotonicity Regularization: Improved Penalties and Novel Applications to Disentangled Representation Learning and Robust Classification</b>
<a href="https://arxiv.org/abs/2205.08247">arxiv:2205.08247</a>
&#x1F4C8; 1 <br>
<p>Joao Monteiro, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, Greg Mori</p></summary>
<p>

**Abstract:** We study settings where gradient penalties are used alongside risk minimization with the goal of obtaining predictors satisfying different notions of monotonicity. Specifically, we present two sets of contributions. In the first part of the paper, we show that different choices of penalties define the regions of the input space where the property is observed. As such, previous methods result in models that are monotonic only in a small volume of the input space. We thus propose an approach that uses mixtures of training instances and random points to populate the space and enforce the penalty in a much larger region. As a second set of contributions, we introduce regularization strategies that enforce other notions of monotonicity in different settings. In this case, we consider applications, such as image classification and generative modeling, where monotonicity is not a hard constraint but can help improve some aspects of the model. Namely, we show that inducing monotonicity can be beneficial in applications such as: (1) allowing for controllable data generation, (2) defining strategies to detect anomalous data, and (3) generating explanations for predictions. Our proposed approaches do not introduce relevant computational overhead while leading to efficient procedures that provide extra benefits over baseline models.

</p>
</details>

<details><summary><b>Moral reinforcement learning using actual causation</b>
<a href="https://arxiv.org/abs/2205.08192">arxiv:2205.08192</a>
&#x1F4C8; 1 <br>
<p>Tue Herlau</p></summary>
<p>

**Abstract:** Reinforcement learning systems will to a greater and greater extent make decisions that significantly impact the well-being of humans, and it is therefore essential that these systems make decisions that conform to our expectations of morally good behavior. The morally good is often defined in causal terms, as in whether one's actions have in fact caused a particular outcome, and whether the outcome could have been anticipated. We propose an online reinforcement learning method that learns a policy under the constraint that the agent should not be the cause of harm. This is accomplished by defining cause using the theory of actual causation and assigning blame to the agent when its actions are the actual cause of an undesirable outcome. We conduct experiments on a toy ethical dilemma in which a natural choice of reward function leads to clearly undesirable behavior, but our method learns a policy that avoids being the cause of harmful behavior, demonstrating the soundness of our approach. Allowing an agent to learn while observing causal moral distinctions such as blame, opens the possibility to learning policies that better conform to our moral judgments.

</p>
</details>

<details><summary><b>Forecasting Solar Power Generation on the basis of Predictive and Corrective Maintenance Activities</b>
<a href="https://arxiv.org/abs/2205.08109">arxiv:2205.08109</a>
&#x1F4C8; 1 <br>
<p>Soham Vyas, Yuvraj Goyal, Neel Bhatt, Sanskar Bhuwania, Hardik Patel, Shakti Mishra, Brijesh Tripathi</p></summary>
<p>

**Abstract:** Solar energy forecasting has seen tremendous growth in the last decade using historical time series collected from a weather station, such as weather variables wind speed and direction, solar radiance, and temperature. It helps in the overall management of solar power plants. However, the solar power plant regularly requires preventive and corrective maintenance activities that further impact energy production. This paper presents a novel work for forecasting solar power energy production based on maintenance activities, problems observed at a power plant, and weather data. The results accomplished on the datasets obtained from the 1MW solar power plant of PDEU (our university) that has generated data set with 13 columns as daily entries from 2012 to 2020. There are 12 structured columns and one unstructured column with manual text entries about different maintenance activities, problems observed, and weather conditions daily. The unstructured column is used to create a new feature column vector using Hash Map, flag words, and stop words. The final dataset comprises five important feature vector columns based on correlation and causality analysis.

</p>
</details>

<details><summary><b>Exploring the Adjugate Matrix Approach to Quaternion Pose Extraction</b>
<a href="https://arxiv.org/abs/2205.09116">arxiv:2205.09116</a>
&#x1F4C8; 0 <br>
<p>Andrew J. Hanson, Sonya M. Hanson</p></summary>
<p>

**Abstract:** Quaternions are important for a wide variety of rotation-related problems in computer graphics, machine vision, and robotics. We study the nontrivial geometry of the relationship between quaternions and rotation matrices by exploiting the adjugate matrix of the characteristic equation of a related eigenvalue problem to obtain the manifold of the space of a quaternion eigenvector. We argue that quaternions parameterized by their corresponding rotation matrices cannot be expressed, for example, in machine learning tasks, as single-valued functions: the quaternion solution must instead be treated as a manifold, with different algebraic solutions for each of several single-valued sectors represented by the adjugate matrix. We conclude with novel constructions exploiting the quaternion adjugate variables to revisit several classic pose estimation applications: 2D point-cloud matching, 2D point-cloud-to-projection matching, 3D point-cloud matching, 3D orthographic point-cloud-to-projection matching, and 3D perspective point-cloud-to-projection matching. We find an exact solution to the 3D orthographic least squares pose extraction problem, and apply it successfully also to the perspective pose extraction problem with results that improve on existing methods.

</p>
</details>

<details><summary><b>New Lower Bounds for Private Estimation and a Generalized Fingerprinting Lemma</b>
<a href="https://arxiv.org/abs/2205.08532">arxiv:2205.08532</a>
&#x1F4C8; 0 <br>
<p>Gautam Kamath, Argyris Mouzakis, Vikrant Singhal</p></summary>
<p>

**Abstract:** We prove new lower bounds for statistical estimation tasks under the constraint of $(\varepsilon, δ)$-differential privacy. First, we provide tight lower bounds for private covariance estimation of Gaussian distributions. We show that estimating the covariance matrix in Frobenius norm requires $Ω(d^2)$ samples, and in spectral norm requires $Ω(d^{3/2})$ samples, both matching upper bounds up to logarithmic factors. We prove these bounds via our main technical contribution, a broad generalization of the fingerprinting method to exponential families. Additionally, using the private Assouad method of Acharya, Sun, and Zhang, we show a tight $Ω(d/(α^2 \varepsilon))$ lower bound for estimating the mean of a distribution with bounded covariance to $α$-error in $\ell_2$-distance. Prior known lower bounds for all these problems were either polynomially weaker or held under the stricter condition of $(\varepsilon,0)$-differential privacy.

</p>
</details>

<details><summary><b>High-resolution landscape-scale biomass mapping using a spatiotemporal patchwork of LiDAR coverages</b>
<a href="https://arxiv.org/abs/2205.08530">arxiv:2205.08530</a>
&#x1F4C8; 0 <br>
<p>Lucas K. Johnson, Michael J. Mahoney, Eddie Bevilacqua, Stephen V. Stehman, Grant Domke, Colin M. Beier</p></summary>
<p>

**Abstract:** Estimating forest aboveground biomass at fine spatial scales has become increasingly important for greenhouse gas estimation, monitoring, and verification efforts to mitigate climate change. Airborne LiDAR continues to be a valuable source of remote sensing data for estimating aboveground biomass. However airborne LiDAR collections may take place at local or regional scales covering irregular, non-contiguous footprints, resulting in a 'patchwork' of different landscape segments at different points in time. Here we addressed common obstacles including selection of training data, the investigation of regional or coverage specific patterns in bias and error, and map agreement, and model-based precision assessments at multiple scales.
  Three machine learning algorithms and an ensemble model were trained using field inventory data (FIA), airborne LiDAR, and topographic, climatic and cadastral geodata. Using strict selection criteria, 801 FIA plots were selected with co-located point clouds drawn from a patchwork of 17 leaf-off LiDAR coverages 2014-2019). Our ensemble model created 30m AGB prediction surfaces within a predictor-defined area of applicability (98% of LiDAR coverage) and resulting AGB predictions were compared with FIA plot-level and areal estimates at multiple scales of aggregation. Our model was overall accurate (% RMSE 13-33%), had very low bias (MBE $\leq$ $\pm$5 Mg ha$^{-1}$), explained most field-observed variation (R$^2$ 0.74-0.93), produced estimates that were both largely consistent with FIA's aggregate summaries (86% of estimates within 95% CI), as well as precise when aggregated to arbitrary small-areas (mean bootstrap standard error 0.37 Mg ha$^{-1}$). We share practical solutions to challenges faced when using spatiotemporal patchworks of LiDAR to meet growing needs for biomass prediction and mapping, and applications in carbon accounting and ecosystem stewardship.

</p>
</details>

<details><summary><b>Can You Still See Me?: Reconstructing Robot Operations Over End-to-End Encrypted Channels</b>
<a href="https://arxiv.org/abs/2205.08426">arxiv:2205.08426</a>
&#x1F4C8; 0 <br>
<p>Ryan Shah, Chuadhry Mujeeb Ahmed, Shishir Nagaraja</p></summary>
<p>

**Abstract:** Connected robots play a key role in Industry 4.0, providing automation and higher efficiency for many industrial workflows. Unfortunately, these robots can leak sensitive information regarding these operational workflows to remote adversaries. While there exists mandates for the use of end-to-end encryption for data transmission in such settings, it is entirely possible for passive adversaries to fingerprint and reconstruct entire workflows being carried out -- establishing an understanding of how facilities operate. In this paper, we investigate whether a remote attacker can accurately fingerprint robot movements and ultimately reconstruct operational workflows. Using a neural network approach to traffic analysis, we find that one can predict TLS-encrypted movements with around \textasciitilde60\% accuracy, increasing to near-perfect accuracy under realistic network conditions. Further, we also find that attackers can reconstruct warehousing workflows with similar success. Ultimately, simply adopting best cybersecurity practices is clearly not enough to stop even weak (passive) adversaries.

</p>
</details>

<details><summary><b>Variable length genetic algorithm with continuous parameters optimization of beam layout in proton therapy</b>
<a href="https://arxiv.org/abs/2205.08398">arxiv:2205.08398</a>
&#x1F4C8; 0 <br>
<p>François Smekens, Nicolas Freud, Bruno Sixou, Guillaume Beslon, Jean M Létang</p></summary>
<p>

**Abstract:** Proton therapy is a modality in fast development. Characterized by a maximum dose deposition at the end of the proton trajectory followed by a sharp fall-off, proton beams can deliver a highly conformal dose to the tumor while sparing organs at risk and surrounding healthy tissues. New treatment planning systems based on spot scanning techniques can now propose multi-field optimization. However, in most cases, this optimization only processes the field fluences whereas the choice of ballistics (field geometry) is left to the oncologist and medical physicist.
  In this work, we investigate a new optimization framework based on a genetic approach. This tool is intended to explore new irradiation schemes and to evaluate the potential of actual or future irradiation systems. We propose to optimize simultaneously the target points and beam incidence angles in a continuous manner and with a variable number of beams. No \textit{a priori} technological constraints are taken into account, \textit{i.e.}~the beam energy values, incidence directions and target points are free parameters.
  The proposed algorithm is based on a modified version of classical genetic operators: mutation, crossover and selection. We use the real coding associated with random perturbations of the parameters to obtain a continuous variation of the potential solutions. We also introduce a perturbation in the exchange points of the crossover to allow variations of the number of beams. These variations are controlled by introducing a beam fluence lower limit.
  In this paper, we present a complete description of the algorithm and of its behaviour in an elementary test case. The proposed method is finally assessed in a clinically-realistic test case.

</p>
</details>

<details><summary><b>Finite Element Method-enhanced Neural Network for Forward and Inverse Problems</b>
<a href="https://arxiv.org/abs/2205.08321">arxiv:2205.08321</a>
&#x1F4C8; 0 <br>
<p>Rishith Ellath Meethal, Birgit Obst, Mohamed Khalil, Aditya Ghantasala, Anoop Kodakkal, Kai-Uwe Bletzinger, Roland Wüchner</p></summary>
<p>

**Abstract:** We introduce a novel hybrid methodology combining classical finite element methods (FEM) with neural networks to create a well-performing and generalizable surrogate model for forward and inverse problems. The residual from finite element methods and custom loss functions from neural networks are merged to form the algorithm. The Finite Element Method-enhanced Neural Network hybrid model (FEM-NN hybrid) is data-efficient and physics conforming. The proposed methodology can be used for surrogate models in real-time simulation, uncertainty quantification, and optimization in the case of forward problems. It can be used for updating the models in the case of inverse problems. The method is demonstrated with examples, and the accuracy of the results and performance is compared against the conventional way of network training and the classical finite element method. An application of the forward-solving algorithm is demonstrated for the uncertainty quantification of wind effects on a high-rise buildings. The inverse algorithm is demonstrated in the speed-dependent bearing coefficient identification of fluid bearings. The hybrid methodology of this kind will serve as a paradigm shift in the simulation methods currently used.

</p>
</details>

<details><summary><b>IIsy: Practical In-Network Classification</b>
<a href="https://arxiv.org/abs/2205.08243">arxiv:2205.08243</a>
&#x1F4C8; 0 <br>
<p>Changgang Zheng, Zhaoqi Xiong, Thanh T Bui, Siim Kaupmees, Riyad Bensoussane, Antoine Bernabeu, Shay Vargaftik, Yaniv Ben-Itzhak, Noa Zilberman</p></summary>
<p>

**Abstract:** The rat race between user-generated data and data-processing systems is currently won by data. The increased use of machine learning leads to further increase in processing requirements, while data volume keeps growing. To win the race, machine learning needs to be applied to the data as it goes through the network. In-network classification of data can reduce the load on servers, reduce response time and increase scalability. In this paper, we introduce IIsy, implementing machine learning classification models in a hybrid fashion using off-the-shelf network devices. IIsy targets three main challenges of in-network classification: (i) mapping classification models to network devices (ii) extracting the required features and (iii) addressing resource and functionality constraints. IIsy supports a range of traditional and ensemble machine learning models, scaling independently of the number of stages in a switch pipeline. Moreover, we demonstrate the use of IIsy for hybrid classification, where a small model is implemented on a switch and a large model at the backend, achieving near optimal classification results, while significantly reducing latency and load on the servers.

</p>
</details>

<details><summary><b>ROP inception: signal estimation with quadratic random sketching</b>
<a href="https://arxiv.org/abs/2205.08225">arxiv:2205.08225</a>
&#x1F4C8; 0 <br>
<p>Rémi Delogne, Vincent Schellekens, Laurent Jacques</p></summary>
<p>

**Abstract:** Rank-one projections (ROP) of matrices and quadratic random sketching of signals support several data processing and machine learning methods, as well as recent imaging applications, such as phase retrieval or optical processing units. In this paper, we demonstrate how signal estimation can be operated directly through such quadratic sketches--equivalent to the ROPs of the "lifted signal" obtained as its outer product with itself--without explicitly reconstructing that signal. Our analysis relies on showing that, up to a minor debiasing trick, the ROP measurement operator satisfies a generalised sign product embedding (SPE) property. In a nutshell, the SPE shows that the scalar product of a signal sketch with the "sign" of the sketch of a given pattern approximates the square of the projection of that signal on this pattern. This thus amounts to an insertion (an "inception") of a ROP model inside a ROP sketch. The effectiveness of our approach is evaluated in several synthetic experiments.

</p>
</details>

<details><summary><b>An Application of Scenario Exploration to Find New Scenarios for the Development and Testing of Automated Driving Systems in Urban Scenarios</b>
<a href="https://arxiv.org/abs/2205.08202">arxiv:2205.08202</a>
&#x1F4C8; 0 <br>
<p>Barbara Schütt, Marc Heinrich, Sonja Marahrens, J. Marius Zöllner, Eric Sax</p></summary>
<p>

**Abstract:** Verification and validation are major challenges for developing automated driving systems. A concept that gets more and more recognized for testing in automated driving is scenario-based testing. However, it introduces the problem of what scenarios are relevant for testing and which are not. This work aims to find relevant, interesting, or critical parameter sets within logical scenarios by utilizing Bayes optimization and Gaussian processes. The parameter optimization is done by comparing and evaluating six different metrics in two urban intersection scenarios. Finally, a list of ideas this work leads to and should be investigated further is presented.

</p>
</details>

<details><summary><b>Automatic Acquisition of a Repertoire of Diverse Grasping Trajectories through Behavior Shaping and Novelty Search</b>
<a href="https://arxiv.org/abs/2205.08189">arxiv:2205.08189</a>
&#x1F4C8; 0 <br>
<p>Aurélien Morel, Yakumo Kunimoto, Alex Coninx, Stéphane Doncieux</p></summary>
<p>

**Abstract:** Grasping a particular object may require a dedicated grasping movement that may also be specific to the robot end-effector. No generic and autonomous method does exist to generate these movements without making hypotheses on the robot or on the object. Learning methods could help to autonomously discover relevant grasping movements, but they face an important issue: grasping movements are so rare that a learning method based on exploration has little chance to ever observe an interesting movement, thus creating a bootstrap issue. We introduce an approach to generate diverse grasping movements in order to solve this problem. The movements are generated in simulation, for particular object positions. We test it on several simulated robots: Baxter, Pepper and a Kuka Iiwa arm. Although we show that generated movements actually work on a real Baxter robot, the aim is to use this method to create a large dataset to bootstrap deep learning methods.

</p>
</details>

<details><summary><b>On the Convergence of Policy in Unregularized Policy Mirror Descent</b>
<a href="https://arxiv.org/abs/2205.08176">arxiv:2205.08176</a>
&#x1F4C8; 0 <br>
<p>Dachao Lin, Zhihua Zhang</p></summary>
<p>

**Abstract:** In this short note, we give the convergence analysis of the policy in the recent famous policy mirror descent (PMD). We mainly consider the unregularized setting following [11] with generalized Bregman divergence. The difference is that we directly give the convergence rates of policy under generalized Bregman divergence. Our results are inspired by the convergence of value function in previous works and are an extension study of policy mirror descent. Though some results have already appeared in previous work, we further discover a large body of Bregman divergences could give finite-step convergence to an optimal policy, such as the classical Euclidean distance.

</p>
</details>

<details><summary><b>Latent Variable Method Demonstrator -- Software for Understanding Multivariate Data Analytics Algorithms</b>
<a href="https://arxiv.org/abs/2205.08132">arxiv:2205.08132</a>
&#x1F4C8; 0 <br>
<p>Joachim Schaeffer, Richard Braatz</p></summary>
<p>

**Abstract:** The ever-increasing quantity of multivariate process data is driving a need for skilled engineers to analyze, interpret, and build models from such data. Multivariate data analytics relies heavily on linear algebra, optimization, and statistics and can be challenging for students to understand given that most curricula do not have strong coverage in the latter three topics. This article describes interactive software -- the Latent Variable Demonstrator (LAVADE) -- for teaching, learning, and understanding latent variable methods. In this software, users can interactively compare latent variable methods such as Partial Least Squares (PLS), and Principal Component Regression (PCR) with other regression methods such as Least Absolute Shrinkage and Selection Operator (lasso), Ridge Regression (RR), and Elastic Net (EN). LAVADE helps to build intuition on choosing appropriate methods, hyperparameter tuning, and model coefficient interpretation, fostering a conceptual understanding of the algorithms' differences. The software contains a data generation method and three chemical process datasets, allowing for comparing results of datasets with different levels of complexity. LAVADE is released as open-source software so that others can apply and advance the tool for use in teaching or research.

</p>
</details>


{% endraw %}
Prev: [2022.05.16]({{ '/2022/05/16/2022.05.16.html' | relative_url }})  Next: [2022.05.18]({{ '/2022/05/18/2022.05.18.html' | relative_url }})