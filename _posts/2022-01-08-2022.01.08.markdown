Prev: [2022.01.07]({{ '/2022/01/07/2022.01.07.html' | relative_url }})  Next: [2022.01.09]({{ '/2022/01/09/2022.01.09.html' | relative_url }})
{% raw %}
## Summary for 2022-01-08, created on 2022-01-18


<details><summary><b>LoMar: A Local Defense Against Poisoning Attack on Federated Learning</b>
<a href="https://arxiv.org/abs/2201.02873">arxiv:2201.02873</a>
&#x1F4C8; 5 <br>
<p>Xingyu Li, Zhe Qu, Shangqing Zhao, Bo Tang, Zhuo Lu, Yao Liu</p></summary>
<p>

**Abstract:** Federated learning (FL) provides a high efficient decentralized machine learning framework, where the training data remains distributed at remote clients in a network. Though FL enables a privacy-preserving mobile edge computing framework using IoT devices, recent studies have shown that this approach is susceptible to poisoning attacks from the side of remote clients. To address the poisoning attacks on FL, we provide a \textit{two-phase} defense algorithm called {Lo}cal {Ma}licious Facto{r} (LoMar). In phase I, LoMar scores model updates from each remote client by measuring the relative distribution over their neighbors using a kernel density estimation method. In phase II, an optimal threshold is approximated to distinguish malicious and clean updates from a statistical perspective. Comprehensive experiments on four real-world datasets have been conducted, and the experimental results show that our defense strategy can effectively protect the FL system. {Specifically, the defense performance on Amazon dataset under a label-flipping attack indicates that, compared with FG+Krum, LoMar increases the target label testing accuracy from $96.0\%$ to $98.8\%$, and the overall averaged testing accuracy from $90.1\%$ to $97.0\%$.

</p>
</details>

<details><summary><b>Optimal 1-Wasserstein Distance for WGANs</b>
<a href="https://arxiv.org/abs/2201.02824">arxiv:2201.02824</a>
&#x1F4C8; 5 <br>
<p>Arthur Stéphanovitch, Ugo Tanielian, Benoît Cadre, Nicolas Klutchnikoff, Gérard Biau</p></summary>
<p>

**Abstract:** The mathematical forces at work behind Generative Adversarial Networks raise challenging theoretical issues. Motivated by the important question of characterizing the geometrical properties of the generated distributions, we provide a thorough analysis of Wasserstein GANs (WGANs) in both the finite sample and asymptotic regimes. We study the specific case where the latent space is univariate and derive results valid regardless of the dimension of the output space. We show in particular that for a fixed sample size, the optimal WGANs are closely linked with connected paths minimizing the sum of the squared Euclidean distances between the sample points. We also highlight the fact that WGANs are able to approach (for the 1-Wasserstein distance) the target distribution as the sample size tends to infinity, at a given convergence rate and provided the family of generative Lipschitz functions grows appropriately. We derive in passing new results on optimal transport theory in the semi-discrete setting.

</p>
</details>

<details><summary><b>AnomMAN: Detect Anomaly on Multi-view Attributed Networks</b>
<a href="https://arxiv.org/abs/2201.02822">arxiv:2201.02822</a>
&#x1F4C8; 5 <br>
<p>Ling-Hao Chen, He Li, Wenhao Yang</p></summary>
<p>

**Abstract:** Anomaly detection on attributed networks is widely used in web shopping, financial transactions, communication networks, and so on. However, most work tries to detect anomalies on attributed networks only considering a single interaction action, which cannot consider rich kinds of interaction actions in multi-view attributed networks. In fact, it remains a challenging task to consider all different kinds of interaction actions uniformly and detect anomalous instances in multi-view attributed networks. In this paper, we propose a Graph Convolution based framework, AnomMAN, to detect \textbf{Anom}aly on \textbf{M}ulti-view \textbf{A}ttributed \textbf{N}etworks. To consider the attributes and all interaction actions jointly, we use the attention mechanism to define the importance of all views in networks. Besides, the Graph Convolution operation cannot be simply applied in anomaly detection tasks on account of its low-pass characteristic. Therefore, AnomMAN uses a graph auto-encoder module to overcome the shortcoming and transform it to our strength. According to experiments on real-world datasets, AnomMAN outperforms state-of-the-art models and two variants of our proposed model. Besides, the Accuracy@50 indicator of AnomMAN reaches 1.000 on the dataset, which shows that the top 50 anomalous instances detected by AnomMAN are all anomalous ones.

</p>
</details>

<details><summary><b>RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues</b>
<a href="https://arxiv.org/abs/2201.02798">arxiv:2201.02798</a>
&#x1F4C8; 5 <br>
<p>Klaas Kelchtermans, Tinne Tuytelaars</p></summary>
<p>

**Abstract:** The gap between simulation and the real-world restrains many machine learning breakthroughs in computer vision and reinforcement learning from being applicable in the real world. In this work, we tackle this gap for the specific case of camera-based navigation, formulating it as following a visual cue in the foreground with arbitrary backgrounds. The visual cue in the foreground can often be simulated realistically, such as a line, gate or cone. The challenge then lies in coping with the unknown backgrounds and integrating both. As such, the goal is to train a visual agent on data captured in an empty simulated environment except for this foreground cue and test this model directly in a visually diverse real world. In order to bridge this big gap, we show it's crucial to combine following techniques namely: Randomized augmentation of the fore- and background, regularization with both deep supervision and triplet loss and finally abstraction of the dynamics by using waypoints rather than direct velocity commands. The various techniques are ablated in our experimental results both qualitatively and quantitatively finally demonstrating a successful transfer from simulation to the real world.

</p>
</details>

<details><summary><b>A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers</b>
<a href="https://arxiv.org/abs/2201.02771">arxiv:2201.02771</a>
&#x1F4C8; 5 <br>
<p>Shuyue Guan, Murray Loew</p></summary>
<p>

**Abstract:** Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and dark regions in Grad-CAM's heatmaps also contribute to classification.

</p>
</details>

<details><summary><b>Impact of Stop Sets on Stopping Active Learning for Text Classification</b>
<a href="https://arxiv.org/abs/2201.05460">arxiv:2201.05460</a>
&#x1F4C8; 4 <br>
<p>Luke Kurlandski, Michael Bloodgood</p></summary>
<p>

**Abstract:** Active learning is an increasingly important branch of machine learning and a powerful technique for natural language processing. The main advantage of active learning is its potential to reduce the amount of labeled data needed to learn high-performing models. A vital aspect of an effective active learning algorithm is the determination of when to stop obtaining additional labeled data. Several leading state-of-the-art stopping methods use a stop set to help make this decision. However, there has been relatively less attention given to the choice of stop set than to the stopping algorithms that are applied on the stop set. Different choices of stop sets can lead to significant differences in stopping method performance. We investigate the impact of different stop set choices on different stopping methods. This paper shows the choice of the stop set can have a significant impact on the performance of stopping methods and the impact is different for stability-based methods from that on confidence-based methods. Furthermore, the unbiased representative stop sets suggested by original authors of methods work better than the systematically biased stop sets used in recently published work, and stopping methods based on stabilizing predictions have stronger performance than confidence-based stopping methods when unbiased representative stop sets are used. We provide the largest quantity of experimental results on the impact of stop sets to date. The findings are important for helping to illuminate the impact of this important aspect of stopping methods that has been under-considered in recently published work and that can have a large practical impact on the performance of stopping methods for important semantic computing applications such as technology assisted review and text classification more broadly.

</p>
</details>

<details><summary><b>Attention-based Random Forest and Contamination Model</b>
<a href="https://arxiv.org/abs/2201.02880">arxiv:2201.02880</a>
&#x1F4C8; 4 <br>
<p>Lev V. Utkin, Andrei V. Konstantinov</p></summary>
<p>

**Abstract:** A new approach called ABRF (the attention-based random forest) and its modifications for applying the attention mechanism to the random forest (RF) for regression and classification are proposed. The main idea behind the proposed ABRF models is to assign attention weights with trainable parameters to decision trees in a specific way. The weights depend on the distance between an instance, which falls into a corresponding leaf of a tree, and instances, which fall in the same leaf. This idea stems from representation of the Nadaraya-Watson kernel regression in the form of a RF. Three modifications of the general approach are proposed. The first one is based on applying the Huber's contamination model and on computing the attention weights by solving quadratic or linear optimization problems. The second and the third modifications use the gradient-based algorithms for computing trainable parameters. Numerical experiments with various regression and classification datasets illustrate the proposed method.

</p>
</details>

<details><summary><b>Scaling Knowledge Graph Embedding Models</b>
<a href="https://arxiv.org/abs/2201.02791">arxiv:2201.02791</a>
&#x1F4C8; 4 <br>
<p>Nasrullah Sheikh, Xiao Qin, Berthold Reinwald, Chuan Lei</p></summary>
<p>

**Abstract:** Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the high data dependencies which entail high computational cost and huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. Both, partitioning strategy and constraint-based negative sampling, avoid cross partition data transfer during training. In our experimental evaluation, we show that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance as non-distributed methods on standard metrics.

</p>
</details>

<details><summary><b>Lazy Lagrangians with Predictions for Online Learning</b>
<a href="https://arxiv.org/abs/2201.02890">arxiv:2201.02890</a>
&#x1F4C8; 3 <br>
<p>Daron Anderson, George Iosifidis, Douglas J. Leith</p></summary>
<p>

**Abstract:** We consider the general problem of online convex optimization with time-varying additive constraints in the presence of predictions for the next cost and constraint functions. A novel primal-dual algorithm is designed by combining a Follow-The-Regularized-Leader iteration with prediction-adaptive dynamic steps. The algorithm achieves $\mathcal O(T^{\frac{3-β}{4}})$ regret and $\mathcal O(T^{\frac{1+β}{2}})$ constraint violation bounds that are tunable via parameter $β\!\in\![1/2,1)$ and have constant factors that shrink with the predictions quality, achieving eventually $\mathcal O(1)$ regret for perfect predictions. Our work extends the FTRL framework for this constrained OCO setting and outperforms the respective state-of-the-art greedy-based solutions, without imposing conditions on the quality of predictions, the cost functions or the geometry of constraints, beyond convexity.

</p>
</details>

<details><summary><b>Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision</b>
<a href="https://arxiv.org/abs/2201.02885">arxiv:2201.02885</a>
&#x1F4C8; 3 <br>
<p>Maurice Günder, Facundo R. Ispizua Yamati, Jana Kierdorf, Ribana Roscher, Anne-Katrin Mahlein, Christian Bauckhage</p></summary>
<p>

**Abstract:** UAV-based image retrieval in modern agriculture enables gathering large amounts of spatially referenced crop image data. In large-scale experiments, however, UAV images suffer from containing a multitudinous amount of crops in a complex canopy architecture. Especially for the observation of temporal effects, this complicates the recognition of individual plants over several images and the extraction of relevant information tremendously. In this work, we present a hands-on workflow for the automatized temporal and spatial identification and individualization of crop images from UAVs abbreviated as "cataloging" based on comprehensible computer vision methods. We evaluate the workflow on two real-world datasets. One dataset is recorded for observation of Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing cycle. The other one deals with harvest prediction of cauliflower plants. The plant catalog is utilized for the extraction of single plant images seen over multiple time points. This gathers large-scale spatio-temporal image dataset that in turn can be applied to train further machine learning models including various data layers. The presented approach improves analysis and interpretation of UAV data in agriculture significantly. By validation with some reference data, our method shows an accuracy that is similar to more complex deep learning-based recognition techniques. Our workflow is able to automatize plant cataloging and training image extraction, especially for large datasets.

</p>
</details>

<details><summary><b>Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture</b>
<a href="https://arxiv.org/abs/2201.02874">arxiv:2201.02874</a>
&#x1F4C8; 3 <br>
<p>Tiago Gaspar Oliveira, Arlindo L. Oliveira</p></summary>
<p>

**Abstract:** The model-based reinforcement learning paradigm, which uses planning algorithms and neural network models, has recently achieved unprecedented results in diverse applications, leading to what is now known as deep reinforcement learning. These agents are quite complex and involve multiple components, factors that can create challenges for research. In this work, we propose a new modular software architecture suited for these types of agents, and a set of building blocks that can be easily reused and assembled to construct new model-based reinforcement learning agents. These building blocks include planning algorithms, policies, and loss functions.
  We illustrate the use of this architecture by combining several of these building blocks to implement and test agents that are optimized to three different test environments: Cartpole, Minigrid, and Tictactoe. One particular planning algorithm, made available in our implementation and not previously used in reinforcement learning, which we called averaged minimax, achieved good results in the three tested environments.
  Experiments performed with this architecture have shown that the best combination of planning algorithm, policy, and loss function is heavily problem dependent. This result provides evidence that the proposed architecture, which is modular and reusable, is useful for reinforcement learning researchers who want to study new environments and techniques.

</p>
</details>

<details><summary><b>Self-aligned Spatial Feature Extraction Network for UAV Vehicle Re-identification</b>
<a href="https://arxiv.org/abs/2201.02836">arxiv:2201.02836</a>
&#x1F4C8; 3 <br>
<p>Aihuan Yao, Jiahao Qi, Ping Zhong</p></summary>
<p>

**Abstract:** Compared with existing vehicle re-identification (ReID) tasks conducted with datasets collected by fixed surveillance cameras, vehicle ReID for unmanned aerial vehicle (UAV) is still under-explored and could be more challenging. Vehicles with the same color and type show extremely similar appearance from the UAV's perspective so that mining fine-grained characteristics becomes necessary. Recent works tend to extract distinguishing information by regional features and component features. The former requires input images to be aligned and the latter entails detailed annotations, both of which are difficult to meet in UAV application. In order to extract efficient fine-grained features and avoid tedious annotating work, this letter develops an unsupervised self-aligned network consisting of three branches. The network introduced a self-alignment module to convert the input images with variable orientations to a uniform orientation, which is implemented under the constraint of triple loss function designed with spatial features. On this basis, spatial features, obtained by vertical and horizontal segmentation methods, and global features are integrated to improve the representation ability in embedded space. Extensive experiments are conducted on UAV-VeID dataset, and our method achieves the best performance compared with recent ReID works.

</p>
</details>

<details><summary><b>Clustering Text Using Attention</b>
<a href="https://arxiv.org/abs/2201.02816">arxiv:2201.02816</a>
&#x1F4C8; 3 <br>
<p>Lovedeep Singh</p></summary>
<p>

**Abstract:** Clustering Text has been an important problem in the domain of Natural Language Processing. While there are techniques to cluster text based on using conventional clustering techniques on top of contextual or non-contextual vector space representations, it still remains a prevalent area of research possible to various improvements in performance and implementation of these techniques. This paper discusses a novel technique to cluster text using attention mechanisms. Attention Mechanisms have proven to be highly effective in various NLP tasks in recent times. This paper extends the idea of attention mechanism in clustering space and sheds some light on a whole new area of research

</p>
</details>

<details><summary><b>Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization</b>
<a href="https://arxiv.org/abs/2201.02812">arxiv:2201.02812</a>
&#x1F4C8; 3 <br>
<p>Chong Peng, Yang Liu, Yongyong Chen, Xinxin Wu, Andrew Cheng, Zhao Kang, Chenglizhao Chen, Qiang Cheng</p></summary>
<p>

**Abstract:** In this paper, we propose a novel nonconvex approach to robust principal component analysis for HSI denoising, which focuses on simultaneously developing more accurate approximations to both rank and column-wise sparsity for the low-rank and sparse components, respectively. In particular, the new method adopts the log-determinant rank approximation and a novel $\ell_{2,\log}$ norm, to restrict the local low-rank or column-wisely sparse properties for the component matrices, respectively. For the $\ell_{2,\log}$-regularized shrinkage problem, we develop an efficient, closed-form solution, which is named $\ell_{2,\log}$-shrinkage operator. The new regularization and the corresponding operator can be generally used in other problems that require column-wise sparsity. Moreover, we impose the spatial-spectral total variation regularization in the log-based nonconvex RPCA model, which enhances the global piece-wise smoothness and spectral consistency from the spatial and spectral views in the recovered HSI. Extensive experiments on both simulated and real HSIs demonstrate the effectiveness of the proposed method in denoising HSIs.

</p>
</details>

<details><summary><b>A Fair and Efficient Hybrid Federated Learning Framework based on XGBoost for Distributed Power Prediction</b>
<a href="https://arxiv.org/abs/2201.02783">arxiv:2201.02783</a>
&#x1F4C8; 3 <br>
<p>Haizhou Liu, Xuan Zhang, Xinwei Shen, Hongbin Sun</p></summary>
<p>

**Abstract:** In a modern power system, real-time data on power generation/consumption and its relevant features are stored in various distributed parties, including household meters, transformer stations and external organizations. To fully exploit the underlying patterns of these distributed data for accurate power prediction, federated learning is needed as a collaborative but privacy-preserving training scheme. However, current federated learning frameworks are polarized towards addressing either the horizontal or vertical separation of data, and tend to overlook the case where both are present. Furthermore, in mainstream horizontal federated learning frameworks, only artificial neural networks are employed to learn the data patterns, which are considered less accurate and interpretable compared to tree-based models on tabular datasets. To this end, we propose a hybrid federated learning framework based on XGBoost, for distributed power prediction from real-time external features. In addition to introducing boosted trees to improve accuracy and interpretability, we combine horizontal and vertical federated learning, to address the scenario where features are scattered in local heterogeneous parties and samples are scattered in various local districts. Moreover, we design a dynamic task allocation scheme such that each party gets a fair share of information, and the computing power of each party can be fully leveraged to boost training efficiency. A follow-up case study is presented to justify the necessity of adopting the proposed framework. The advantages of the proposed framework in fairness, efficiency and accuracy performance are also confirmed.

</p>
</details>

<details><summary><b>PocketNN: Integer-only Training and Inference of Neural Networks via Direct Feedback Alignment and Pocket Activations in Pure C++</b>
<a href="https://arxiv.org/abs/2201.02863">arxiv:2201.02863</a>
&#x1F4C8; 2 <br>
<p>Jaewoo Song, Fangzhen Lin</p></summary>
<p>

**Abstract:** Standard deep learning algorithms are implemented using floating-point real numbers. This presents an obstacle for implementing them on low-end devices which may not have dedicated floating-point units (FPUs). As a result, researchers in TinyML have considered machine learning algorithms that can train and run a deep neural network (DNN) on a low-end device using integer operations only. In this paper we propose PocketNN, a light and self-contained proof-of-concept framework in pure C++ for the training and inference of DNNs using only integers. Unlike other approaches, PocketNN directly operates on integers without requiring any explicit quantization algorithms or customized fixed-point formats. This was made possible by pocket activations, which are a family of activation functions devised for integer-only DNNs, and an emerging DNN training algorithm called direct feedback alignment (DFA). Unlike the standard backpropagation (BP), DFA trains each layer independently, thus avoiding integer overflow which is a key problem when using BP with integer-only operations. We used PocketNN to train some DNNs on two well-known datasets, MNIST and Fashion-MNIST. Our experiments show that the DNNs trained with our PocketNN achieved 96.98% and 87.7% accuracies on MNIST and Fashion-MNIST datasets, respectively. The accuracies are very close to the equivalent DNNs trained using BP with floating-point real number operations, such that accuracy degradations were just 1.02%p and 2.09%p, respectively. Finally, our PocketNN has high compatibility and portability for low-end devices as it is open source and implemented in pure C++ without any dependencies.

</p>
</details>

<details><summary><b>Fake Hilsa Fish Detection Using Machine Vision</b>
<a href="https://arxiv.org/abs/2201.02853">arxiv:2201.02853</a>
&#x1F4C8; 2 <br>
<p>Mirajul Islam, Jannatul Ferdous Ani, Abdur Rahman, Zakia Zaman</p></summary>
<p>

**Abstract:** Hilsa is the national fish of Bangladesh. Bangladesh is earning a lot of foreign currency by exporting this fish. Unfortunately, in recent days, some unscrupulous businessmen are selling fake Hilsa fishes to gain profit. The Sardines and Sardinella are the most sold in the market as Hilsa. The government agency of Bangladesh, namely Bangladesh Food Safety Authority said that these fake Hilsa fish contain high levels of cadmium and lead which are detrimental for humans. In this research, we have proposed a method that can readily identify original Hilsa fish and fake Hilsa fish. Based on the research available on online literature, we are the first to do research on identifying original Hilsa fish. We have collected more than 16,000 images of original and counterfeit Hilsa fish. To classify these images, we have used several deep learning-based models. Then, the performance has been compared between them. Among those models, DenseNet201 achieved the highest accuracy of 97.02%.

</p>
</details>

<details><summary><b>Weighted Encoding Optimization for Dynamic Single-pixel Imaging and Sensing</b>
<a href="https://arxiv.org/abs/2201.02833">arxiv:2201.02833</a>
&#x1F4C8; 2 <br>
<p>Xinrui Zhan, Liheng Bian, Chunli Zhu, Jun Zhang</p></summary>
<p>

**Abstract:** Using single-pixel detection, the end-to-end neural network that jointly optimizes both encoding and decoding enables high-precision imaging and high-level semantic sensing. However, for varied sampling rates, the large-scale network requires retraining that is laboursome and computation-consuming. In this letter, we report a weighted optimization technique for dynamic rate-adaptive single-pixel imaging and sensing, which only needs to train the network for one time that is available for any sampling rates. Specifically, we introduce a novel weighting scheme in the encoding process to characterize different patterns' modulation efficiency. While the network is training at a high sampling rate, the modulation patterns and corresponding weights are updated iteratively, which produces optimal ranked encoding series when converged. In the experimental implementation, the optimal pattern series with the highest weights are employed for light modulation, thus achieving highly-efficient imaging and sensing. The reported strategy saves the additional training of another low-rate network required by the existing dynamic single-pixel networks, which further doubles training efficiency. Experiments on the MNIST dataset validated that once the network is trained with a sampling rate of 1, the average imaging PSNR reaches 23.50 dB at 0.1 sampling rate, and the image-free classification accuracy reaches up to 95.00\% at a sampling rate of 0.03 and 97.91\% at a sampling rate of 0.1.

</p>
</details>

<details><summary><b>SGUIE-Net: Semantic Attention Guided Underwater Image Enhancement with Multi-Scale Perception</b>
<a href="https://arxiv.org/abs/2201.02832">arxiv:2201.02832</a>
&#x1F4C8; 2 <br>
<p>Qi Qi, Kunqian Li, Haiyong Zheng, Xiang Gao, Guojia Hou, Kun Sun</p></summary>
<p>

**Abstract:** Due to the wavelength-dependent light attenuation, refraction and scattering, underwater images usually suffer from color distortion and blurred details. However, due to the limited number of paired underwater images with undistorted images as reference, training deep enhancement models for diverse degradation types is quite difficult. To boost the performance of data-driven approaches, it is essential to establish more effective learning mechanisms that mine richer supervised information from limited training sample resources. In this paper, we propose a novel underwater image enhancement network, called SGUIE-Net, in which we introduce semantic information as high-level guidance across different images that share common semantic regions. Accordingly, we propose semantic region-wise enhancement module to perceive the degradation of different semantic regions from multiple scales and feed it back to the global attention features extracted from its original scale. This strategy helps to achieve robust and visually pleasant enhancements to different semantic objects, which should thanks to the guidance of semantic information for differentiated enhancement. More importantly, for those degradation types that are not common in the training sample distribution, the guidance connects them with the already well-learned types according to their semantic relevance. Extensive experiments on the publicly available datasets and our proposed dataset demonstrated the impressive performance of SGUIE-Net. The code and proposed dataset are available at: https://trentqq.github.io/SGUIE-Net.html

</p>
</details>

<details><summary><b>CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation</b>
<a href="https://arxiv.org/abs/2201.02831">arxiv:2201.02831</a>
&#x1F4C8; 2 <br>
<p>Reuben Dorent, Aaron Kujawa, Marina Ivory, Spyridon Bakas, Nicola Rieke, Samuel Joutard, Ben Glocker, Jorge Cardoso, Marc Modat, Kayhan Batmanghelich, Arseniy Belkov, Maria Baldeon Calisto, Jae Won Choi, Benoit M. Dawant, Hexin Dong, Sergio Escalera, Yubo Fan, Lasse Hansen, Mattias P. Heinrich, Smriti Joshi, Victoriya Kashtanova, Hyeon Gyu Kim, Satoshi Kondo, Christian N. Kruse, Susana K. Lai-Yuen</p></summary>
<p>

**Abstract:** Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly addressed single-class problems. To tackle these limitations, the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in conjunction with the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large and multi-class benchmark for unsupervised cross-modality DA. The challenge's goal is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are performed using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore, we created an unsupervised cross-modality segmentation benchmark. The training set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105). The aim was to automatically perform unilateral VS and bilateral cochlea segmentation on hrT2 as provided in the testing set (N=137). A total of 16 teams submitted their algorithm for the evaluation phase. The level of performance reached by the top-performing teams is strikingly high (best median Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice - VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an image-to-image translation approach to transform the source-domain images into pseudo-target-domain images. A segmentation network was then trained using these generated images and the manual annotations provided for the source image.

</p>
</details>

<details><summary><b>Classification of Hyperspectral Images by Using Spectral Data and Fully Connected Neural Network</b>
<a href="https://arxiv.org/abs/2201.02821">arxiv:2201.02821</a>
&#x1F4C8; 2 <br>
<p>Zumray Dokur, Tamer Olmez</p></summary>
<p>

**Abstract:** It is observed that high classification performance is achieved for one- and two-dimensional signals by using deep learning methods. In this context, most researchers have tried to classify hyperspectral images by using deep learning methods and classification success over 90% has been achieved for these images. Deep neural networks (DNN) actually consist of two parts: i) Convolutional neural network (CNN) and ii) fully connected neural network (FCNN). While CNN determines the features, FCNN is used in classification. In classification of the hyperspectral images, it is observed that almost all of the researchers used 2D or 3D convolution filters on the spatial data beside spectral data (features). It is convenient to use convolution filters on images or time signals. In hyperspectral images, each pixel is represented by a signature vector which consists of individual features that are independent of each other. Since the order of the features in the vector can be changed, it doesn't make sense to use convolution filters on these features as on time signals. At the same time, since the hyperspectral images do not have a textural structure, there is no need to use spatial data besides spectral data. In this study, hyperspectral images of Indian pines, Salinas, Pavia centre, Pavia university and Botswana are classified by using only fully connected neural network and the spectral data with one dimensional. An average accuracy of 97.5% is achieved for the test sets of all hyperspectral images.

</p>
</details>

<details><summary><b>Attacking Vertical Collaborative Learning System Using Adversarial Dominating Inputs</b>
<a href="https://arxiv.org/abs/2201.02775">arxiv:2201.02775</a>
&#x1F4C8; 2 <br>
<p>Qi Pang, Yuanyuan Yuan, Shuai Wang</p></summary>
<p>

**Abstract:** Vertical collaborative learning system also known as vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-preserving manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individual organizations.
  Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in collaborative learning scenarios.
  We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the resiliency score of "victim" participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.

</p>
</details>

<details><summary><b>Defocus Deblur Microscopy via feature interactive coarse-to-fine network</b>
<a href="https://arxiv.org/abs/2201.02876">arxiv:2201.02876</a>
&#x1F4C8; 1 <br>
<p>Jiahe Wang, Boran Han</p></summary>
<p>

**Abstract:** The clarity of microscopic images is vital in biology research and diagnosis. When taking microscopy images at cell or molecule level, mechanical drift occurs and could be difficult and expansive to counter. Such a problem could be overcome by developing an end-to-end deep learning-based workflow capable of predicting in focused microscopic images from out-of-focused counterparts. In our model, we adopt a structure of multi-level U-net, each level connected head-to-tail with corresponding convolution layers from each other. In contrast to the conventional coarse-to-fine model, our model uses the knowledge distilled from the coarse network transferred to the finer network. We evaluate the performance of our model and found our method to be effective and has a better performance by comparing the results with existing models.

</p>
</details>

<details><summary><b>Reconfigurable Intelligent Surface Enabled Spatial Multiplexing with Fully Convolutional Network</b>
<a href="https://arxiv.org/abs/2201.02834">arxiv:2201.02834</a>
&#x1F4C8; 1 <br>
<p>Bile Peng, Jan-Aike Termöhlen, Cong Sun, Danping He, Ke Guan, Tim Fingscheidt, Eduard A. Jorswieck</p></summary>
<p>

**Abstract:** Reconfigurable intelligent surface (RIS) is an emerging technology for future wireless communication systems. In this work, we consider downlink spatial multiplexing enabled by the RIS for weighted sum-rate (WSR) maximization. In the literature, most solutions use alternating gradient-based optimization, which has moderate performance, high complexity, and limited scalability. We propose to apply a fully convolutional network (FCN) to solve this problem, which was originally designed for semantic segmentation of images. The rectangular shape of the RIS and the spatial correlation of channels with adjacent RIS antennas due to the short distance between them encourage us to apply it for the RIS configuration. We design a set of channel features that includes both cascaded channels via the RIS and the direct channel. In the base station (BS), the differentiable minimum mean squared error (MMSE) precoder is used for pretraining and the weighted minimum mean squared error (WMMSE) precoder is then applied for fine-tuning, which is nondifferentiable, more complex, but achieves a better performance. Evaluation results show that the proposed solution has higher performance and allows for a faster evaluation than the baselines. Hence it scales better to a large number of antennas, advancing the RIS one step closer to practical deployment.

</p>
</details>

<details><summary><b>Extraction of Product Specifications from the Web -- Going Beyond Tables and Lists</b>
<a href="https://arxiv.org/abs/2201.02896">arxiv:2201.02896</a>
&#x1F4C8; 0 <br>
<p>Govind Krishnan Gangadhar, Ashish Kulkarni</p></summary>
<p>

**Abstract:** E-commerce product pages on the web often present product specification data in structured tabular blocks. Extraction of these product attribute-value specifications has benefited applications like product catalogue curation, search, question answering, and others. However, across different Websites, there is a wide variety of HTML elements (like <table>, <ul>, <div>, <span>, <dl> etc.) typically used to render these blocks that makes their automatic extraction a challenge. Most of the current research has focused on extracting product specifications from tables and lists and, therefore, suffers from recall when applied to a large-scale extraction setting. In this paper, we present a product specification extraction approach that goes beyond tables or lists and generalizes across the diverse HTML elements used for rendering specification blocks. Using a combination of hand-coded features and deep learned spatial and token features, we first identify the specification blocks on a product page. We then extract the product attribute-value pairs from these blocks following an approach inspired by wrapper induction. We created a labeled dataset of product specifications extracted from 14,111 diverse specification blocks taken from a range of different product websites. Our experiments show the efficacy of our approach compared to the current specification extraction models and support our claim about its application to large-scale product specification extraction.

</p>
</details>

<details><summary><b>Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy</b>
<a href="https://arxiv.org/abs/2201.02867">arxiv:2201.02867</a>
&#x1F4C8; 0 <br>
<p>Claire Donnat, Axel Levy, Frederic Poitevin, Nina Miolane</p></summary>
<p>

**Abstract:** Recent breakthroughs in high resolution imaging of biomolecules in solution with cryo-electron microscopy (cryo-EM) have unlocked new doors for the reconstruction of molecular volumes, thereby promising further advances in biology, chemistry, and pharmacological research amongst others. Despite significant headway, the immense challenges in cryo-EM data analysis remain legion and intricately inter-disciplinary in nature, requiring insights from physicists, structural biologists, computer scientists, statisticians, and applied mathematicians. Meanwhile, recent next-generation volume reconstruction algorithms that combine generative modeling with end-to-end unsupervised deep learning techniques have shown promising results on simulated data, but still face considerable hurdles when applied to experimental cryo-EM images. In light of the proliferation of such methods and given the interdisciplinary nature of the task, we propose here a critical review of recent advances in the field of deep generative modeling for high resolution cryo-EM volume reconstruction. The present review aims to (i) compare and contrast these new methods, while (ii) presenting them from a perspective and using terminology familiar to scientists in each of the five aforementioned fields with no specific background in cryo-EM. The review begins with an introduction to the mathematical and computational challenges of deep generative models for cryo-EM volume reconstruction, along with an overview of the baseline methodology shared across this class of algorithms. Having established the common thread weaving through these different models, we provide a practical comparison of these state-of-the-art algorithms, highlighting their relative strengths and weaknesses, along with the assumptions that they rely on. This allows us to identify bottlenecks in current methods and avenues for future research.

</p>
</details>


{% endraw %}
Prev: [2022.01.07]({{ '/2022/01/07/2022.01.07.html' | relative_url }})  Next: [2022.01.09]({{ '/2022/01/09/2022.01.09.html' | relative_url }})