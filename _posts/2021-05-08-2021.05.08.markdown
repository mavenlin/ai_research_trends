## Summary for 2021-05-08, created on 2021-12-21


<details><summary><b>FNet: Mixing Tokens with Fourier Transforms</b>
<a href="https://arxiv.org/abs/2105.03824">arxiv:2105.03824</a>
&#x1F4C8; 237 <br>
<p>James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon</p></summary>
<p>

**Abstract:** We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the "efficient" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.

</p>
</details>

<details><summary><b>e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks</b>
<a href="https://arxiv.org/abs/2105.03761">arxiv:2105.03761</a>
&#x1F4C8; 69 <br>
<p>Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, Thomas Lukasiewicz</p></summary>
<p>

**Abstract:** Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER, which learns joint embeddings of images and text, and GPT-2, a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.

</p>
</details>

<details><summary><b>Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</b>
<a href="https://arxiv.org/abs/2105.03654">arxiv:2105.03654</a>
&#x1F4C8; 8 <br>
<p>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu</p></summary>
<p>

**Abstract:** Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.

</p>
</details>

<details><summary><b>Understanding Neural Networks with Logarithm Determinant Entropy Estimator</b>
<a href="https://arxiv.org/abs/2105.03705">arxiv:2105.03705</a>
&#x1F4C8; 4 <br>
<p>Zhanghao Zhouyin, Ding Liu</p></summary>
<p>

**Abstract:** Understanding the informative behaviour of deep neural networks is challenged by misused estimators and the complexity of network structure, which leads to inconsistent observations and diversified interpretation. Here we propose the LogDet estimator -- a reliable matrix-based entropy estimator that approximates Shannon differential entropy. We construct informative measurements based on LogDet estimator, verify our method with comparable experiments and utilize it to analyse neural network behaviour. Our results demonstrate the LogDet estimator overcomes the drawbacks that emerge from highly diverse and degenerated distribution thus is reliable to estimate entropy in neural networks. The Network analysis results also find a functional distinction between shallow and deeper layers, which can help understand the compression phenomenon in the Information bottleneck theory of neural networks.

</p>
</details>

<details><summary><b>Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility</b>
<a href="https://arxiv.org/abs/2105.03692">arxiv:2105.03692</a>
&#x1F4C8; 4 <br>
<p>Charles Jin, Melinda Sun, Martin Rinard</p></summary>
<p>

**Abstract:** A recent line of work has shown that deep networks are highly susceptible to backdoor data poisoning attacks. Specifically, by injecting a small amount of malicious data into the training distribution, an adversary gains the ability to control the model's behavior during inference. In this work, we propose an iterative training procedure for removing poisoned data from the training set. Our approach consists of two steps. We first train an ensemble of weak learners to automatically discover distinct subpopulations in the training set. We then leverage a boosting framework to recover the clean data. Empirically, our method successfully defends against several state-of-the-art backdoor attacks, including both clean and dirty label attacks. We also present results from an independent third-party evaluation including a recent \textit{adaptive} poisoning adversary. The results indicate our approach is competitive with existing defenses against backdoor attacks on deep neural networks, and significantly outperforms the state-of-the-art in several scenarios.

</p>
</details>

<details><summary><b>Learning Image Attacks toward Vision Guided Autonomous Vehicles</b>
<a href="https://arxiv.org/abs/2105.03834">arxiv:2105.03834</a>
&#x1F4C8; 3 <br>
<p>Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris</p></summary>
<p>

**Abstract:** While adversarial neural networks have been shown successful for static image attacks, very few approaches have been developed for attacking online image streams while taking into account the underlying physical dynamics of autonomous vehicles, their mission, and environment. This paper presents an online adversarial machine learning framework that can effectively misguide autonomous vehicles' missions. In the existing image attack methods devised toward autonomous vehicles, optimization steps are repeated for every image frame. This framework removes the need for fully converged optimization at every frame to realize image attacks in real-time. Using reinforcement learning, a generative neural network is trained over a set of image frames to obtain an attack policy that is more robust to dynamic and uncertain environments. A state estimator is introduced for processing image streams to reduce the attack policy's sensitivity to physical variables such as unknown position and velocity. A simulation study is provided to validate the results.

</p>
</details>

<details><summary><b>Slash or burn: Power line and vegetation classification for wildfire prevention</b>
<a href="https://arxiv.org/abs/2105.03804">arxiv:2105.03804</a>
&#x1F4C8; 3 <br>
<p>Austin Park, Farzaneh Rajabi, Ross Weber</p></summary>
<p>

**Abstract:** Electric utilities are struggling to manage increasing wildfire risk in a hotter and drier climate. Utility transmission and distribution lines regularly ignite destructive fires when they make contact with surrounding vegetation. Trimming vegetation to maintain the separation from utility assets is as critical to safety as it is difficult. Each utility has tens of thousands of linear miles to manage, poor knowledge of where those assets are located, and no way to prioritize trimming. Feature-enhanced convolutional neural networks (CNNs) have proven effective in this problem space. Histograms of oriented gradients (HOG) and Hough transforms are used to increase the salience of the linear structures like power lines and poles. Data is frequently taken from drone or satellite footage, but Google Street View offers an even more scalable and lower cost solution. This paper uses $1,320$ images scraped from Street View, transfer learning on popular CNNs, and feature engineering to place images in one of three classes: (1) no utility systems, (2) utility systems with no overgrown vegetation, or (3) utility systems with overgrown vegetation. The CNN output thus yields a prioritized vegetation management system and creates a geotagged map of utility assets as a byproduct. Test set accuracy with reached $80.15\%$ using VGG11 with a trained first layer and classifier, and a model ensemble correctly classified $88.88\%$ of images with risky vegetation overgrowth.

</p>
</details>

<details><summary><b>Stability and Generalization of Stochastic Gradient Methods for Minimax Problems</b>
<a href="https://arxiv.org/abs/2105.03793">arxiv:2105.03793</a>
&#x1F4C8; 3 <br>
<p>Yunwen Lei, Zhenhuan Yang, Tianbao Yang, Yiming Ying</p></summary>
<p>

**Abstract:** Many machine learning problems can be formulated as minimax problems such as Generative Adversarial Networks (GANs), AUC maximization and robust estimation, to mention but a few. A substantial amount of studies are devoted to studying the convergence behavior of their stochastic gradient-type algorithms. In contrast, there is relatively little work on their generalization, i.e., how the learning models built from training examples would behave on test examples. In this paper, we provide a comprehensive generalization analysis of stochastic gradient methods for minimax problems under both convex-concave and nonconvex-nonconcave cases through the lens of algorithmic stability. We establish a quantitative connection between stability and several generalization measures both in expectation and with high probability. For the convex-concave setting, our stability analysis shows that stochastic gradient descent ascent attains optimal generalization bounds for both smooth and nonsmooth minimax problems. We also establish generalization bounds for both weakly-convex-weakly-concave and gradient-dominated problems.

</p>
</details>

<details><summary><b>MetaKernel: Learning Variational Random Features with Limited Labels</b>
<a href="https://arxiv.org/abs/2105.03781">arxiv:2105.03781</a>
&#x1F4C8; 3 <br>
<p>Yingjun Du, Haoliang Sun, Xiantong Zhen, Jun Xu, Yilong Yin, Ling Shao, Cees G. M. Snoek</p></summary>
<p>

**Abstract:** Few-shot learning deals with the fundamental and challenging problem of learning from a few annotated samples, while being able to generalize well on new tasks. The crux of few-shot learning is to extract prior knowledge from related tasks to enable fast adaptation to a new task with a limited amount of data. In this paper, we propose meta-learning kernels with random Fourier features for few-shot learning, we call MetaKernel. Specifically, we propose learning variational random features in a data-driven manner to obtain task-specific kernels by leveraging the shared knowledge provided by related tasks in a meta-learning setting. We treat the random feature basis as the latent variable, which is estimated by variational inference. The shared knowledge from related tasks is incorporated into a context inference of the posterior, which we achieve via a long-short term memory module. To establish more expressive kernels, we deploy conditional normalizing flows based on coupling layers to achieve a richer posterior distribution over random Fourier bases. The resultant kernels are more informative and discriminative, which further improves the few-shot learning. To evaluate our method, we conduct extensive experiments on both few-shot image classification and regression tasks. A thorough ablation study demonstrates that the effectiveness of each introduced component in our method. The benchmark results on fourteen datasets demonstrate MetaKernel consistently delivers at least comparable and often better performance than state-of-the-art alternatives.

</p>
</details>

<details><summary><b>Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics</b>
<a href="https://arxiv.org/abs/2105.03703">arxiv:2105.03703</a>
&#x1F4C8; 3 <br>
<p>Greg Yang, Etai Littwin</p></summary>
<p>

**Abstract:** Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at initialization has an infinite-width limit for a large class of architectures including modern staples such as ResNet and Transformers. However, their analysis does not apply to training. Here, we show the same neural networks (in the so-called NTK parametrization) during training follow a kernel gradient descent dynamics in function space, where the kernel is the infinite-width NTK. This completes the proof of the *architectural universality* of NTK behavior. To achieve this result, we apply the Tensor Programs technique: Write the entire SGD dynamics inside a Tensor Program and analyze it via the Master Theorem. To facilitate this proof, we develop a graphical notation for Tensor Programs.

</p>
</details>

<details><summary><b>Deep learning of nanopore sensing signals using a bi-path network</b>
<a href="https://arxiv.org/abs/2105.03660">arxiv:2105.03660</a>
&#x1F4C8; 3 <br>
<p>Dario Dematties, Chenyu Wen, Mauricio David Pérez, Dian Zhou, Shi-Li Zhang</p></summary>
<p>

**Abstract:** Temporary changes in electrical resistance of a nanopore sensor caused by translocating target analytes are recorded as a sequence of pulses on current traces. Prevalent algorithms for feature extraction in pulse-like signals lack objectivity because empirical amplitude thresholds are user-defined to single out the pulses from the noisy background. Here, we use deep learning for feature extraction based on a bi-path network (B-Net). After training, the B-Net acquires the prototypical pulses and the ability of both pulse recognition and feature extraction without a priori assigned parameters. The B-Net performance is evaluated on generated datasets and further applied to experimental data of DNA and protein translocation. The B-Net results show remarkably small relative errors and stable trends. The B-Net is further shown capable of processing data with a signal-to-noise ratio equal to one, an impossibility for threshold-based algorithms. The developed B-Net is generic for pulse-like signals beyond pulsed nanopore currents.

</p>
</details>

<details><summary><b>Super Solutions of the Model RB</b>
<a href="https://arxiv.org/abs/2105.03831">arxiv:2105.03831</a>
&#x1F4C8; 2 <br>
<p>Guangyan Zhou, Wei Xu</p></summary>
<p>

**Abstract:** The concept of super solution is a special type of generalized solutions with certain degree of robustness and stability. In this paper we consider the $(1,1)$-super solutions of the model RB. Using the first moment method, we establish a "threshold" such that as the constraint density crosses this value, the expected number of $(1,1)$-super solutions goes from $0$ to infinity.

</p>
</details>

<details><summary><b>RBNN: Memory-Efficient Reconfigurable Deep Binary Neural Network with IP Protection for Internet of Things</b>
<a href="https://arxiv.org/abs/2105.03822">arxiv:2105.03822</a>
&#x1F4C8; 2 <br>
<p>Huming Qiu, Hua Ma, Zhi Zhang, Yifeng Zheng, Anmin Fu, Pan Zhou, Yansong Gao, Derek Abbott, Said F. Al-Sarawi</p></summary>
<p>

**Abstract:** Though deep neural network models exhibit outstanding performance for various applications, their large model size and extensive floating-point operations render deployment on mobile computing platforms a major challenge, and, in particular, on Internet of Things devices. One appealing solution is model quantization that reduces the model size and uses integer operations commonly supported by microcontrollers . To this end, a 1-bit quantized DNN model or deep binary neural network maximizes the memory efficiency, where each parameter in a BNN model has only 1-bit. In this paper, we propose a reconfigurable BNN (RBNN) to further amplify the memory efficiency for resource-constrained IoT devices. Generally, the RBNN can be reconfigured on demand to achieve any one of M (M>1) distinct tasks with the same parameter set, thus only a single task determines the memory requirements. In other words, the memory utilization is improved by times M. Our extensive experiments corroborate that up to seven commonly used tasks can co-exist (the value of M can be larger). These tasks with a varying number of classes have no or negligible accuracy drop-off on three binarized popular DNN architectures including VGG, ResNet, and ReActNet. The tasks span across different domains, e.g., computer vision and audio domains validated herein, with the prerequisite that the model architecture can serve those cross-domain tasks. To protect the intellectual property of an RBNN model, the reconfiguration can be controlled by both a user key and a device-unique root key generated by the intrinsic hardware fingerprint. By doing so, an RBNN model can only be used per paid user per authorized device, thus benefiting both the user and the model provider.

</p>
</details>

<details><summary><b>Click-Through Rate Prediction Using Graph Neural Networks and Online Learning</b>
<a href="https://arxiv.org/abs/2105.03811">arxiv:2105.03811</a>
&#x1F4C8; 2 <br>
<p>Farzaneh Rajabi, Jack Siyuan He</p></summary>
<p>

**Abstract:** Recommendation systems have been extensively studied by many literature in the past and are ubiquitous in online advertisement, shopping industry/e-commerce, query suggestions in search engines, and friend recommendation in social networks. Moreover, restaurant/music/product/movie/news/app recommendations are only a few of the applications of a recommender system. A small percent improvement on the CTR prediction accuracy has been mentioned to add millions of dollars of revenue to the advertisement industry. Click-Through-Rate (CTR) prediction is a special version of recommender system in which the goal is predicting whether or not a user is going to click on a recommended item. A content-based recommendation approach takes into account the past history of the user's behavior, i.e. the recommended products and the users reaction to them. So, a personalized model that recommends the right item to the right user at the right time is the key to building such a model. On the other hand, the so-called collaborative filtering approach incorporates the click history of the users who are very similar to a particular user, thereby helping the recommender to come up with a more confident prediction for that particular user by leveraging the wider knowledge of users who share their taste in a connected network of users. In this project, we are interested in building a CTR predictor using Graph Neural Networks complemented by an online learning algorithm that models such dynamic interactions. By framing the problem as a binary classification task, we have evaluated this system both on the offline models (GNN, Deep Factorization Machines) with test-AUC of 0.7417 and on the online learning model with test-AUC of 0.7585 using a sub-sampled version of Criteo public dataset consisting of 10,000 data points.

</p>
</details>

<details><summary><b>Human Gait State Prediction Using Cellular Automata and Classification Using ELM</b>
<a href="https://arxiv.org/abs/2105.03799">arxiv:2105.03799</a>
&#x1F4C8; 2 <br>
<p>Vijay Bhaskar Semwal, Neha Gaud, G. C. Nandi</p></summary>
<p>

**Abstract:** In this research article, we have reported periodic cellular automata rules for different gait state prediction and classification of the gait data using extreme machine Leaning (ELM). This research is the first attempt to use cellular automaton to understand the complexity of bipedal walk. Due to nonlinearity, varying configurations throughout the gait cycle and the passive joint located at the unilateral foot-ground contact in bipedal walk resulting variation of dynamic descriptions and control laws from phase to phase for human gait is making difficult to predict the bipedal walk states. We have designed the cellular automata rules which will predict the next gait state of bipedal steps based on the previous two neighbour states. We have designed cellular automata rules for normal walk. The state prediction will help to correctly design the bipedal walk. The normal walk depends on next two states and has total 8 states. We have considered the current and previous states to predict next state. So we have formulated 16 rules using cellular automata, 8 rules for each leg. The priority order maintained using the fact that if right leg in swing phase then left leg will be in stance phase. To validate the model we have classified the gait Data using ELM [1] and achieved accuracy 60%. We have explored the trajectories and compares with another gait trajectories. Finally we have presented the error analysis for different joints.

</p>
</details>

<details><summary><b>AnomalyHop: An SSL-based Image Anomaly Localization Method</b>
<a href="https://arxiv.org/abs/2105.03797">arxiv:2105.03797</a>
&#x1F4C8; 2 <br>
<p>Kaitai Zhang, Bin Wang, Wei Wang, Fahad Sohrab, Moncef Gabbouj, C. -C. Jay Kuo</p></summary>
<p>

**Abstract:** An image anomaly localization method based on the successive subspace learning (SSL) framework, called AnomalyHop, is proposed in this work. AnomalyHop consists of three modules: 1) feature extraction via successive subspace learning (SSL), 2) normality feature distributions modeling via Gaussian models, and 3) anomaly map generation and fusion. Comparing with state-of-the-art image anomaly localization methods based on deep neural networks (DNNs), AnomalyHop is mathematically transparent, easy to train, and fast in its inference speed. Besides, its area under the ROC curve (ROC-AUC) performance on the MVTec AD dataset is 95.9%, which is among the best of several benchmarking methods. Our codes are publicly available at Github.

</p>
</details>

<details><summary><b>NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer</b>
<a href="https://arxiv.org/abs/2105.03775">arxiv:2105.03775</a>
&#x1F4C8; 2 <br>
<p>Hossein Basafa, Sajad Movahedi, Ali Ebrahimi, Azadeh Shakery, Heshaam Faili</p></summary>
<p>

**Abstract:** This paper presents a technical report of our submission to the 4th task of SemEval-2021, titled: Reading Comprehension of Abstract Meaning. In this task, we want to predict the correct answer based on a question given a context. Usually, contexts are very lengthy and require a large receptive field from the model. Thus, common contextualized language models like BERT miss fine representation and performance due to the limited capacity of the input tokens. To tackle this problem, we used the Longformer model to better process the sequences. Furthermore, we utilized the method proposed in the Longformer benchmark on Wikihop dataset which improved the accuracy on our task data from 23.01% and 22.95% achieved by the baselines for subtask 1 and 2, respectively, to 70.30% and 64.38%.

</p>
</details>

<details><summary><b>Improving Deep Learning Performance for Predicting Large-Scale Porous-Media Flow through Feature Coarsening</b>
<a href="https://arxiv.org/abs/2105.03752">arxiv:2105.03752</a>
&#x1F4C8; 2 <br>
<p>Bicheng Yan, Dylan Robert Harp, Bailian Chen, Rajesh J. Pawar</p></summary>
<p>

**Abstract:** Physics-based simulation for fluid flow in porous media is a computational technology to predict the temporal-spatial evolution of state variables (e.g. pressure) in porous media, and usually requires high computational expense due to its nonlinearity and the scale of the study domain. This letter describes a deep learning (DL) workflow to predict the pressure evolution as fluid flows in large-scale 3D heterogeneous porous media. In particular, we apply feature coarsening technique to extract the most representative information and perform the training and prediction of DL at the coarse scale, and further recover the resolution at the fine scale by 2D piecewise cubic interpolation. We validate the DL approach that is trained from physics-based simulation data to predict pressure field in a field-scale 3D geologic CO_2 storage reservoir. We evaluate the impact of feature coarsening on DL performance, and observe that the feature coarsening can not only decrease training time by >74% and reduce memory consumption by >75%, but also maintains temporal error <1.5%. Besides, the DL workflow provides predictive efficiency with ~1400 times speedup compared to physics-based simulation.

</p>
</details>

<details><summary><b>Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model</b>
<a href="https://arxiv.org/abs/2105.03733">arxiv:2105.03733</a>
&#x1F4C8; 2 <br>
<p>Lingwei Peng, Hui Qian, Zhebang Shen, Chao Zhang, Fei Li</p></summary>
<p>

**Abstract:** Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.

</p>
</details>

<details><summary><b>Protecting Individual Interests across Clusters: Spectral Clustering with Guarantees</b>
<a href="https://arxiv.org/abs/2105.03714">arxiv:2105.03714</a>
&#x1F4C8; 2 <br>
<p>Shubham Gupta, Ambedkar Dukkipati</p></summary>
<p>

**Abstract:** Studies related to fairness in machine learning have recently gained traction due to its ever-expanding role in high-stakes decision making. For example, it may be desirable to ensure that all clusters discovered by an algorithm have high gender diversity. Previously, these problems have been studied under a setting where sensitive attributes, with respect to which fairness conditions impose diversity across clusters, are assumed to be observable; hence, protected groups are readily available. Most often, this may not be true, and diversity or individual interests can manifest as an intrinsic or latent feature of a social network. For example, depending on latent sensitive attributes, individuals interact with each other and represent each other's interests, resulting in a network, which we refer to as a representation graph. Motivated by this, we propose an individual fairness criterion for clustering a graph $\mathcal{G}$ that requires each cluster to contain an adequate number of members connected to the individual under a representation graph $\mathcal{R}$. We devise a spectral clustering algorithm to find fair clusters under a given representation graph. We further propose a variant of the stochastic block model and establish our algorithm's weak consistency under this model. Finally, we present experimental results to corroborate our theoretical findings.

</p>
</details>

<details><summary><b>Business Entity Matching with Siamese Graph Convolutional Networks</b>
<a href="https://arxiv.org/abs/2105.03701">arxiv:2105.03701</a>
&#x1F4C8; 2 <br>
<p>Evgeny Krivosheev, Mattia Atzeni, Katsiaryna Mirylenka, Paolo Scotton, Christoph Miksovic, Anton Zorin</p></summary>
<p>

**Abstract:** Data integration has been studied extensively for decades and approached from different angles. However, this domain still remains largely rule-driven and lacks universal automation. Recent developments in machine learning and in particular deep learning have opened the way to more general and efficient solutions to data-integration tasks. In this paper, we demonstrate an approach that allows modeling and integrating entities by leveraging their relations and contextual information. This is achieved by combining siamese and graph neural networks to effectively propagate information between connected entities and support high scalability. We evaluated our approach on the task of integrating data about business entities, demonstrating that it outperforms both traditional rule-based systems and other deep learning approaches.

</p>
</details>

<details><summary><b>Self-Supervised Adversarial Example Detection by Disentangled Representation</b>
<a href="https://arxiv.org/abs/2105.03689">arxiv:2105.03689</a>
&#x1F4C8; 2 <br>
<p>Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Shengshan Hu, Jinyu Tian, Jiantao Zhou</p></summary>
<p>

**Abstract:** Deep learning models are known to be vulnerable to adversarial examples that are elaborately designed for malicious purposes and are imperceptible to the human perceptual system. Autoencoder, when trained solely over benign examples, has been widely used for (self-supervised) adversarial detection based on the assumption that adversarial examples yield larger reconstruction error. However, because lacking adversarial examples in its training and the too strong generalization ability of autoencoder, this assumption does not always hold true in practice. To alleviate this problem, we explore to detect adversarial examples by disentangled representations of images under the autoencoder structure. By disentangling input images as class features and semantic features, we train an autoencoder, assisted by a discriminator network, over both correctly paired class/semantic features and incorrectly paired class/semantic features to reconstruct benign and counterexamples. This mimics the behavior of adversarial examples and can reduce the unnecessary generalization ability of autoencoder. Compared with the state-of-the-art self-supervised detection methods, our method exhibits better performance in various measurements (i.e., AUC, FPR, TPR) over different datasets (MNIST, Fashion-MNIST and CIFAR-10), different adversarial attack methods (FGSM, BIM, PGD, DeepFool, and CW) and different victim models (8-layer CNN and 16-layer VGG). We compare our method with the state-of-the-art self-supervised detection methods under different adversarial attacks and different victim models (30 attack settings), and it exhibits better performance in various measurements (AUC, FPR, TPR) for most attacks settings. Ideally, AUC is $1$ and our method achieves $0.99+$ on CIFAR-10 for all attacks. Notably, different from other Autoencoder-based detectors, our method can provide resistance to the adaptive adversary.

</p>
</details>

<details><summary><b>HamNet: Conformation-Guided Molecular Representation with Hamiltonian Neural Networks</b>
<a href="https://arxiv.org/abs/2105.03688">arxiv:2105.03688</a>
&#x1F4C8; 2 <br>
<p>Ziyao Li, Shuwen Yang, Guojie Song, Lingsheng Cai</p></summary>
<p>

**Abstract:** Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.

</p>
</details>

<details><summary><b>Nearly Minimax-Optimal Rates for Noisy Sparse Phase Retrieval via Early-Stopped Mirror Descent</b>
<a href="https://arxiv.org/abs/2105.03678">arxiv:2105.03678</a>
&#x1F4C8; 2 <br>
<p>Fan Wu, Patrick Rebeschini</p></summary>
<p>

**Abstract:** This paper studies early-stopped mirror descent applied to noisy sparse phase retrieval, which is the problem of recovering a $k$-sparse signal $\mathbf{x}^\star\in\mathbb{R}^n$ from a set of quadratic Gaussian measurements corrupted by sub-exponential noise. We consider the (non-convex) unregularized empirical risk minimization problem and show that early-stopped mirror descent, when equipped with the hyperbolic entropy mirror map and proper initialization, achieves a nearly minimax-optimal rate of convergence, provided the sample size is at least of order $k^2$ (modulo logarithmic term) and the minimum (in modulus) non-zero entry of the signal is on the order of $\|\mathbf{x}^\star\|_2/\sqrt{k}$. Our theory leads to a simple algorithm that does not rely on explicit regularization or thresholding steps to promote sparsity. More generally, our results establish a connection between mirror descent and sparsity in the non-convex problem of noisy sparse phase retrieval, adding to the literature on early stopping that has mostly focused on non-sparse, Euclidean, and convex settings via gradient descent. Our proof combines a potential-based analysis of mirror descent with a quantitative control on a variational coherence property that we establish along the path of mirror descent, up to a prescribed stopping time.

</p>
</details>

<details><summary><b>Chameleon: A Semi-AutoML framework targeting quick and scalable development and deployment of production-ready ML systems for SMEs</b>
<a href="https://arxiv.org/abs/2105.03669">arxiv:2105.03669</a>
&#x1F4C8; 2 <br>
<p>Johannes Otterbach, Thomas Wollmann</p></summary>
<p>

**Abstract:** Developing, scaling, and deploying modern Machine Learning solutions remains challenging for small- and middle-sized enterprises (SMEs). This is due to a high entry barrier of building and maintaining a dedicated IT team as well as the difficulties of real-world data (RWD) compared to standard benchmark data. To address this challenge, we discuss the implementation and concepts of Chameleon, a semi-AutoML framework. The goal of Chameleon is fast and scalable development and deployment of production-ready machine learning systems into the workflow of SMEs. We first discuss the RWD challenges faced by SMEs. After, we outline the central part of the framework which is a model and loss-function zoo with RWD-relevant defaults. Subsequently, we present how one can use a templatable framework in order to automate the experiment iteration cycle, as well as close the gap between development and deployment. Finally, we touch on our testing framework component allowing us to investigate common model failure modes and support best practices of model deployment governance.

</p>
</details>

<details><summary><b>Interpretable Mixture Density Estimation by use of Differentiable Tree-module</b>
<a href="https://arxiv.org/abs/2105.03616">arxiv:2105.03616</a>
&#x1F4C8; 2 <br>
<p>Ryuichi Kanoh, Tomu Yanabe</p></summary>
<p>

**Abstract:** In order to develop reliable services using machine learning, it is important to understand the uncertainty of the model outputs. Often the probability distribution that the prediction target follows has a complex shape, and a mixture distribution is assumed as a distribution that uncertainty follows. Since the output of mixture density estimation is complicated, its interpretability becomes important when considering its use in real services. In this paper, we propose a method for mixture density estimation that utilizes an interpretable tree structure. Further, a fast inference procedure based on time-invariant information cache achieves both high speed and interpretability.

</p>
</details>

<details><summary><b>Learning to Detect an Odd Restless Markov Arm with a Trembling Hand</b>
<a href="https://arxiv.org/abs/2105.03603">arxiv:2105.03603</a>
&#x1F4C8; 2 <br>
<p>P. N. Karthik, Rajesh Sundaresan</p></summary>
<p>

**Abstract:** This paper studies the problem of finding an anomalous arm in a multi-armed bandit when (a) each arm is a finite-state Markov process, and (b) the arms are restless. Here, anomaly means that the transition probability matrix (TPM) of one of the arms (the odd arm) is different from the common TPM of each of the non-odd arms. The TPMs are unknown to a decision entity that wishes to find the index of the odd arm as quickly as possible, subject to an upper bound on the error probability. We derive a problem instance-specific asymptotic lower bound on the expected time required to find the odd arm index, where the asymptotics is as the error probability vanishes. Further, we devise a policy based on the principle of certainty equivalence, and demonstrate that under a continuous selection assumption and a certain regularity assumption on the TPMs, the policy achieves the lower bound arbitrarily closely. Thus, while the lower bound is shown for all problem instances, the upper bound is shown only for those problem instances satisfying the continuous selection and the regularity assumptions. Our achievability analysis is based on resolving the identifiability problem in the context of a certain lifted countable-state controlled Markov process.

</p>
</details>

<details><summary><b>Class-Incremental Learning for Wireless Device Identification in IoT</b>
<a href="https://arxiv.org/abs/2105.06381">arxiv:2105.06381</a>
&#x1F4C8; 1 <br>
<p>Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song</p></summary>
<p>

**Abstract:** Deep Learning (DL) has been utilized pervasively in the Internet of Things (IoT). One typical application of DL in IoT is device identification from wireless signals, namely Non-cryptographic Device Identification (NDI). However, learning components in NDI systems have to evolve to adapt to operational variations, such a paradigm is termed as Incremental Learning (IL). Various IL algorithms have been proposed and many of them require dedicated space to store the increasing amount of historical data, and therefore, they are not suitable for IoT or mobile applications. However, conventional IL schemes can not provide satisfying performance when historical data are not available. In this paper, we address the IL problem in NDI from a new perspective, firstly, we provide a new metric to measure the degree of topological maturity of DNN models from the degree of conflict of class-specific fingerprints. We discover that an important cause for performance degradation in IL enabled NDI is owing to the conflict of devices' fingerprints. Second, we also show that the conventional IL schemes can lead to low topological maturity of DNN models in NDI systems. Thirdly, we propose a new Channel Separation Enabled Incremental Learning (CSIL) scheme without using historical data, in which our strategy can automatically separate devices' fingerprints in different learning stages and avoid potential conflict. Finally, We evaluated the effectiveness of the proposed framework using real data from ADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in aviation. The proposed framework has the potential to be applied to accurate identification of IoT devices in a variety of IoT applications and services. Data and code available at IEEE Dataport (DOI: 10.21227/1bxc-ke87) and \url{https://github.com/pcwhy/CSIL}}

</p>
</details>

<details><summary><b>Study of List-Based OMP and an Enhanced Model for Direction Finding with Non-Uniform Arrays</b>
<a href="https://arxiv.org/abs/2105.03774">arxiv:2105.03774</a>
&#x1F4C8; 1 <br>
<p>W. S. Leite, R. C. de Lamare</p></summary>
<p>

**Abstract:** This paper proposes an enhanced coarray transformation model (EDCTM) and a mixed greedy maximum likelihood algorithm called List-Based Maximum Likelihood Orthogonal Matching Pursuit (LBML-OMP) for direction-of-arrival estimation with non-uniform linear arrays (NLAs). The proposed EDCTM approach obtains improved estimates when Khatri-Rao product-based models are used to generate difference coarrays under the assumption of uncorrelated sources. In the proposed LBML-OMP technique, for each iteration a set of candidates is generated based on the correlation-maximization between the dictionary and the residue vector. LBML-OMP then chooses the best candidate based on a reduced-complexity asymptotic maximum likelihood decision rule. Simulations show the improved results of EDCTM over existing approaches and that LBML-OMP outperforms existing sparse recovery algorithms as well as Spatial Smoothing Multiple Signal Classification with NLAs.

</p>
</details>

<details><summary><b>Mental Models of Adversarial Machine Learning</b>
<a href="https://arxiv.org/abs/2105.03726">arxiv:2105.03726</a>
&#x1F4C8; 1 <br>
<p>Lukas Bieringer, Kathrin Grosse, Michael Backes, Katharina Krombholz</p></summary>
<p>

**Abstract:** Although machine learning (ML) is widely used in practice, little is known about practitioners' actual understanding of potential security challenges. In this work, we close this substantial gap in the literature and contribute a qualitative study focusing on developers' mental models of the ML pipeline and potentially vulnerable components. Studying mental models has helped in other security fields to discover root causes or improve risk communication. Our study reveals four characteristic ranges in mental models of industrial practitioners. The first range concerns the intertwined relationship of adversarial machine learning (AML) and classical security. The second range describes structural and functional components. The third range expresses individual variations of mental models, which are neither explained by the application nor by the educational background of the corresponding subjects. The fourth range corresponds to the varying levels of technical depth, which are however not determined by our subjects' level of knowledge. Our characteristic ranges have implications for the integration of AML into corporate workflows, security enhancing tools for practitioners, and creating appropriate regulatory frameworks for AML.

</p>
</details>

<details><summary><b>Quantum Machine Learning For Classical Data</b>
<a href="https://arxiv.org/abs/2105.03684">arxiv:2105.03684</a>
&#x1F4C8; 1 <br>
<p>Leonard Wossnig</p></summary>
<p>

**Abstract:** In this dissertation, we study the intersection of quantum computing and supervised machine learning algorithms, which means that we investigate quantum algorithms for supervised machine learning that operate on classical data. This area of research falls under the umbrella of quantum machine learning, a research area of computer science which has recently received wide attention. In particular, we investigate to what extent quantum computers can be used to accelerate supervised machine learning algorithms. The aim of this is to develop a clear understanding of the promises and limitations of the current state of the art of quantum algorithms for supervised machine learning, but also to define directions for future research in this exciting field. We start by looking at supervised quantum machine learning (QML) algorithms through the lens of statistical learning theory. In this framework, we derive novel bounds on the computational complexities of a large set of supervised QML algorithms under the requirement of optimal learning rates. Next, we give a new bound for Hamiltonian simulation of dense Hamiltonians, a major subroutine of most known supervised QML algorithms, and then derive a classical algorithm with nearly the same complexity. We then draw the parallels to recent "quantum-inspired" results, and will explain the implications of these results for quantum machine learning applications. Looking for areas which might bear larger advantages for QML algorithms, we finally propose a novel algorithm for Quantum Boltzmann machines, and argue that quantum algorithms for quantum data are one of the most promising applications for QML with potentially exponential advantage over classical approaches.

</p>
</details>

<details><summary><b>A Crossover That Matches Diverse Parents Together in Evolutionary Algorithms</b>
<a href="https://arxiv.org/abs/2105.03680">arxiv:2105.03680</a>
&#x1F4C8; 1 <br>
<p>Maciej Świechowski</p></summary>
<p>

**Abstract:** Crossover and mutation are the two main operators that lead to new solutions in evolutionary approaches. In this article, a new method of performing the crossover phase is presented. The problem of choice is evolutionary decision tree construction. The method aims at finding such individuals that together complement each other. Hence we say that they are diversely specialized. We propose the way of calculating the so-called complementary fitness. In several empirical experiments, we evaluate the efficacy of the method proposed in four variants and compare it to a fitness-rank-based approach. One variant emerges clearly as the best approach, whereas the remaining ones are below the baseline.

</p>
</details>

<details><summary><b>EZCrop: Energy-Zoned Channels for Robust Output Pruning</b>
<a href="https://arxiv.org/abs/2105.03679">arxiv:2105.03679</a>
&#x1F4C8; 1 <br>
<p>Rui Lin, Jie Ran, Dongpeng Wang, King Hung Chiu, Ngai Wong</p></summary>
<p>

**Abstract:** Recent results have revealed an interesting observation in a trained convolutional neural network (CNN), namely, the rank of a feature map channel matrix remains surprisingly constant despite the input images. This has led to an effective rank-based channel pruning algorithm, yet the constant rank phenomenon remains mysterious and unexplained. This work aims at demystifying and interpreting such rank behavior from a frequency-domain perspective, which as a bonus suggests an extremely efficient Fast Fourier Transform (FFT)-based metric for measuring channel importance without explicitly computing its rank. We achieve remarkable CNN channel pruning based on this analytically sound and computationally efficient metric and adopt it for repetitive pruning to demonstrate robustness via our scheme named Energy-Zoned Channels for Robust Output Pruning (EZCrop), which shows consistently better results than other state-of-the-art channel pruning methods.

</p>
</details>

<details><summary><b>Graph Inference Representation: Learning Graph Positional Embeddings with Anchor Path Encoding</b>
<a href="https://arxiv.org/abs/2105.03821">arxiv:2105.03821</a>
&#x1F4C8; 0 <br>
<p>Yuheng Lu, Jinpeng Chen, ChuXiong Sun, Jie Hu</p></summary>
<p>

**Abstract:** Learning node representations that incorporate information from graph structure benefits wide range of tasks on graph. The majority of existing graph neural networks (GNNs) have limited power in capturing position information for a given node. The idea of positioning nodes with selected anchors has been exploited, yet mainly relying on explicit labeling of distance information. Here we propose Graph Inference Representation (GIR), an anchor based GNN model encoding path information related to pre-selected anchors for each node. Abilities to get position-aware embeddings are theoretically and experimentally investigated on GIR and its core variants. Further, the complementarity between GIRs and typical GNNs is demonstrated. We show that GIRs get outperformed results in position-aware scenarios, and performances on typical GNNs could be improved by fusing GIR embeddings.

</p>
</details>

<details><summary><b>Fine-Grained $ε$-Margin Closed-Form Stabilization of Parametric Hawkes Processes</b>
<a href="https://arxiv.org/abs/2105.03800">arxiv:2105.03800</a>
&#x1F4C8; 0 <br>
<p>Rafael Lima</p></summary>
<p>

**Abstract:** Hawkes Processes have undergone increasing popularity as default tools for modeling self- and mutually exciting interactions of discrete events in continuous-time event streams. A Maximum Likelihood Estimation (MLE) unconstrained optimization procedure over parametrically assumed forms of the triggering kernels of the corresponding intensity function are a widespread cost-effective modeling strategy, particularly suitable for data with few and/or short sequences. However, the MLE optimization lacks guarantees, except for strong assumptions on the parameters of the triggering kernels, and may lead to instability of the resulting parameters .In the present work, we show how a simple stabilization procedure improves the performance of the MLE optimization without these overly restrictive assumptions.This stabilized version of the MLE is shown to outperform traditional methods over sequences of several different lengths.

</p>
</details>

<details><summary><b>Contrastive Attraction and Contrastive Repulsion for Representation Learning</b>
<a href="https://arxiv.org/abs/2105.03746">arxiv:2105.03746</a>
&#x1F4C8; 0 <br>
<p>Huangjie Zheng, Xu Chen, Jiangchao Yao, Hongxia Yang, Chunyuan Li, Ya Zhang, Hao Zhang, Ivor Tsang, Jingren Zhou, Mingyuan Zhou</p></summary>
<p>

**Abstract:** Contrastive learning (CL) is effective in learning data representations without label supervision, where the encoder needs to contrast each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. However, conventional CL is sensitive to how many negative samples are included and how they are selected. Proposed in this paper is a doubly CL strategy that contrasts positive samples and negative ones within themselves separately. We realize this strategy with contrastive attraction and contrastive repulsion (CACR) makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals the connection between CACR and CL from the perspectives of both positive attraction and negative repulsion and shows the benefits in both efficiency and robustness brought by separately contrasting within the sampled positive and negative pairs. Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets in representation learning, but also provides interpretable contrastive weights, demonstrating the efficacy of the proposed doubly contrastive strategy.

</p>
</details>

<details><summary><b>PIM-DRAM: Accelerating Machine Learning Workloads using Processing in Commodity DRAM</b>
<a href="https://arxiv.org/abs/2105.03736">arxiv:2105.03736</a>
&#x1F4C8; 0 <br>
<p>Sourjya Roy, Mustafa Ali, Anand Raghunathan</p></summary>
<p>

**Abstract:** Deep Neural Networks (DNNs) have transformed the field of machine learning and are widely deployed in many applications involving image, video, speech and natural language processing. The increasing compute demands of DNNs have been widely addressed through Graphics Processing Units (GPUs) and specialized accelerators. However, as model sizes grow, these von Neumann architectures require very high memory bandwidth to keep the processing elements utilized as a majority of the data resides in the main memory. Processing in memory has been proposed as a promising solution for the memory wall bottleneck for ML workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to accelerate matrix vector operations in ML workloads. The proposed multiplication primitive adds < 1% area overhead and does not require any change in the DRAM peripherals. Therefore, the proposed multiplication can be easily adopted in commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture, data mapping scheme and dataflow for executing DNNs within DRAM. System evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that the proposed architecture, mapping, and data flow can provide up to 19.5x speedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the memory bottleneck in future generations of DNN hardware.

</p>
</details>

<details><summary><b>Diversifying Neural Text Generation with Part-of-Speech Guided Softmax and Sampling</b>
<a href="https://arxiv.org/abs/2105.03641">arxiv:2105.03641</a>
&#x1F4C8; 0 <br>
<p>Zhixian Yang, Pengxuan Xu, Xiaojun Wan</p></summary>
<p>

**Abstract:** Neural text generation models are likely to suffer from the low-diversity problem. Various decoding strategies and training-based methods have been proposed to promote diversity only by exploiting contextual features, but rarely do they consider incorporating syntactic structure clues. In this work, we propose using linguistic annotation, i.e., part-of-speech (POS), to guide the text generation. In detail, we introduce POS Guided Softmax to explicitly model two posterior probabilities: (i) next-POS, and (ii) next-token from the vocabulary of the target POS. A POS Guided Sampling strategy is further proposed to address the low-diversity problem by enriching the diversity of POS. Extensive experiments and human evaluations demonstrate that, compared with existing state-of-the-art methods, our POS Guided Softmax and Sampling (POSG) can generate more diverse text while maintaining comparable quality.

</p>
</details>

<details><summary><b>A parallel-network continuous quantitative trading model with GARCH and PPO</b>
<a href="https://arxiv.org/abs/2105.03625">arxiv:2105.03625</a>
&#x1F4C8; 0 <br>
<p>Zhishun Wang, Wei Lu, Kaixin Zhang, Tianhao Li, Zixi Zhao</p></summary>
<p>

**Abstract:** It is a difficult task for both professional investors and individual traders continuously making profit in stock market. With the development of computer science and deep reinforcement learning, Buy\&Hold (B\&H) has been oversteped by many artificial intelligence trading algorithms. However, the information and process are not enough, which limit the performance of reinforcement learning algorithms. Thus, we propose a parallel-network continuous quantitative trading model with GARCH and PPO to enrich the basical deep reinforcement learning model, where the deep learning parallel network layers deal with 3 different frequencies data (including GARCH information) and proximal policy optimization (PPO) algorithm interacts actions and rewards with stock trading environment. Experiments in 5 stocks from Chinese stock market show our method achieves more extra profit comparing with basical reinforcement learning methods and bench models.

</p>
</details>


[Next Page]({{ '/2021/05/07/2021.05.07.html' | relative_url }})
