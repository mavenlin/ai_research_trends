Prev: [2022.08.26]({{ '/2022/08/26/2022.08.26.html' | relative_url }})  Next: [2022.08.28]({{ '/2022/08/28/2022.08.28.html' | relative_url }})
{% raw %}
## Summary for 2022-08-27, created on 2022-09-03


<details><summary><b>SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2208.13077">arxiv:2208.13077</a>
&#x1F4C8; 8 <br>
<p>Baihan Lin</p></summary>
<p>

**Abstract:** We propose a recommendation system that suggests treatment strategies to a therapist during the psychotherapy session in real-time. Our system uses a turn-level rating mechanism that predicts the therapeutic outcome by computing a similarity score between the deep embedding of a scoring inventory, and the current sentence that the patient is speaking. The system automatically transcribes a continuous audio stream and separates it into turns of the patient and of the therapist using an online registration-free diarization method. The dialogue pairs along with their computed ratings are then fed into a deep reinforcement learning recommender where the sessions are treated as users and the topics are treated as items. Other than evaluating the empirical advantages of the core components on existing datasets, we demonstrate the effectiveness of this system in a web app.

</p>
</details>

<details><summary><b>Information FOMO: The unhealthy fear of missing out on information. A method for removing misleading data for healthier models</b>
<a href="https://arxiv.org/abs/2208.13080">arxiv:2208.13080</a>
&#x1F4C8; 7 <br>
<p>Ethan Pickering, Themistoklis P. Sapsis</p></summary>
<p>

**Abstract:** Not all data are equal. Misleading or unnecessary data can critically hinder the accuracy of Machine Learning (ML) models. When data is plentiful, misleading effects can be overcome, but in many real-world applications data is sparse and expensive to acquire. We present a method that substantially reduces the data size necessary to accurately train ML models, potentially opening the door for many new, limited-data applications in ML. Our method extracts the most informative data, while ignoring and omitting data that misleads the ML model to inferior generalization properties. Specifically, the method eliminates the phenomena of "double descent", where more data leads to worse performance. This approach brings several key features to the ML community. Notably, the method naturally converges and removes the traditional need to divide the dataset into training, testing, and validation data. Instead, the selection metric inherently assesses testing error. This ensures that key information is never wasted in testing or validation.

</p>
</details>

<details><summary><b>Sub-mW Neuromorphic SNN audio processing applications with Rockpool and Xylo</b>
<a href="https://arxiv.org/abs/2208.12991">arxiv:2208.12991</a>
&#x1F4C8; 7 <br>
<p>Hannah Bos, Dylan Muir</p></summary>
<p>

**Abstract:** Spiking Neural Networks (SNNs) provide an efficient computational mechanism for temporal signal processing, especially when coupled with low-power SNN inference ASICs. SNNs have been historically difficult to configure, lacking a general method for finding solutions for arbitrary tasks. In recent years, gradient-descent optimization methods have been applied to SNNs with increasing ease. SNNs and SNN inference processors therefore offer a good platform for commercial low-power signal processing in energy constrained environments without cloud dependencies. However, to date these methods have not been accessible to ML engineers in industry, requiring graduate-level training to successfully configure a single SNN application. Here we demonstrate a convenient high-level pipeline to design, train and deploy arbitrary temporal signal processing applications to sub-mW SNN inference hardware. We apply a new straightforward SNN architecture designed for temporal signal processing, using a pyramid of synaptic time constants to extract signal features at a range of temporal scales. We demonstrate this architecture on an ambient audio classification task, deployed to the Xylo SNN inference processor in streaming mode. Our application achieves high accuracy (98%) and low latency (100ms) at low power (<4muW inference power). Our approach makes training and deploying SNN applications available to ML engineers with general NN backgrounds, without requiring specific prior experience with spiking NNs. We intend for our approach to make Neuromorphic hardware and SNNs an attractive choice for commercial low-power and edge signal processing applications.

</p>
</details>

<details><summary><b>RL-DistPrivacy: Privacy-Aware Distributed Deep Inference for low latency IoT systems</b>
<a href="https://arxiv.org/abs/2208.13032">arxiv:2208.13032</a>
&#x1F4C8; 6 <br>
<p>Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani</p></summary>
<p>

**Abstract:** Although Deep Neural Networks (DNN) have become the backbone technology of several ubiquitous applications, their deployment in resource-constrained machines, e.g., Internet of Things (IoT) devices, is still challenging. To satisfy the resource requirements of such a paradigm, collaborative deep inference with IoT synergy was introduced. However, the distribution of DNN networks suffers from severe data leakage. Various threats have been presented, including black-box attacks, where malicious participants can recover arbitrary inputs fed into their devices. Although many countermeasures were designed to achieve privacy-preserving DNN, most of them result in additional computation and lower accuracy. In this paper, we present an approach that targets the security of collaborative deep inference via re-thinking the distribution strategy, without sacrificing the model performance. Particularly, we examine different DNN partitions that make the model susceptible to black-box threats and we derive the amount of data that should be allocated per device to hide proprieties of the original input. We formulate this methodology, as an optimization, where we establish a trade-off between the latency of co-inference and the privacy-level of data. Next, to relax the optimal solution, we shape our approach as a Reinforcement Learning (RL) design that supports heterogeneous devices as well as multiple DNNs/datasets.

</p>
</details>

<details><summary><b>Learning Clinical Concepts for Predicting Risk of Progression to Severe COVID-19</b>
<a href="https://arxiv.org/abs/2208.13126">arxiv:2208.13126</a>
&#x1F4C8; 5 <br>
<p>Helen Zhou, Cheng Cheng, Kelly J. Shields, Gursimran Kochhar, Tariq Cheema, Zachary C. Lipton, Jeremy C. Weiss</p></summary>
<p>

**Abstract:** With COVID-19 now pervasive, identification of high-risk individuals is crucial. Using data from a major healthcare provider in Southwestern Pennsylvania, we develop survival models predicting severe COVID-19 progression. In this endeavor, we face a tradeoff between more accurate models relying on many features and less accurate models relying on a few features aligned with clinician intuition. Complicating matters, many EHR features tend to be under-coded, degrading the accuracy of smaller models. In this study, we develop two sets of high-performance risk scores: (i) an unconstrained model built from all available features; and (ii) a pipeline that learns a small set of clinical concepts before training a risk predictor. Learned concepts boost performance over the corresponding features (C-index 0.858 vs. 0.844) and demonstrate improvements over (i) when evaluated out-of-sample (subsequent time periods). Our models outperform previous works (C-index 0.844-0.872 vs. 0.598-0.810).

</p>
</details>

<details><summary><b>On Unsupervised Training of Link Grammar Based Language Models</b>
<a href="https://arxiv.org/abs/2208.13021">arxiv:2208.13021</a>
&#x1F4C8; 5 <br>
<p>Nikolay Mikhaylovskiy</p></summary>
<p>

**Abstract:** In this short note we explore what is needed for the unsupervised training of graph language models based on link grammars. First, we introduce the ter-mination tags formalism required to build a language model based on a link grammar formalism of Sleator and Temperley [21] and discuss the influence of context on the unsupervised learning of link grammars. Second, we pro-pose a statistical link grammar formalism, allowing for statistical language generation. Third, based on the above formalism, we show that the classical dissertation of Yuret [25] on discovery of linguistic relations using lexical at-traction ignores contextual properties of the language, and thus the approach to unsupervised language learning relying just on bigrams is flawed. This correlates well with the unimpressive results in unsupervised training of graph language models based on bigram approach of Yuret.

</p>
</details>

<details><summary><b>Deep Kernel Learning of Dynamical Models from High-Dimensional Noisy Data</b>
<a href="https://arxiv.org/abs/2208.12975">arxiv:2208.12975</a>
&#x1F4C8; 5 <br>
<p>Nicol√≤ Botteghi, Mengwu Guo, Christoph Brune</p></summary>
<p>

**Abstract:** This work proposes a Stochastic Variational Deep Kernel Learning method for the data-driven discovery of low-dimensional dynamical models from high-dimensional noisy data. The framework is composed of an encoder that compresses high-dimensional measurements into low-dimensional state variables, and a latent dynamical model for the state variables that predicts the system evolution over time. The training of the proposed model is carried out in an unsupervised manner, i.e., not relying on labeled data. Our learning method is evaluated on the motion of a pendulum -- a well studied baseline for nonlinear model identification and control with continuous states and control inputs -- measured via high-dimensional noisy RGB images. Results show that the method can effectively denoise measurements, learn compact state representations and latent dynamical models, as well as identify and quantify modeling uncertainties.

</p>
</details>

<details><summary><b>Minimal Feature Analysis for Isolated Digit Recognition for varying encoding rates in noisy environments</b>
<a href="https://arxiv.org/abs/2208.13100">arxiv:2208.13100</a>
&#x1F4C8; 4 <br>
<p>Muskan Garg, Naveen Aggarwal</p></summary>
<p>

**Abstract:** This research work is about recent development made in speech recognition. In this research work, analysis of isolated digit recognition in the presence of different bit rates and at different noise levels has been performed. This research work has been carried using audacity and HTK toolkit. Hidden Markov Model (HMM) is the recognition model which was used to perform this experiment. The feature extraction techniques used are Mel Frequency Cepstrum coefficient (MFCC), Linear Predictive Coding (LPC), perceptual linear predictive (PLP), mel spectrum (MELSPEC), filter bank (FBANK). There were three types of different noise levels which have been considered for testing of data. These include random noise, fan noise and random noise in real time environment. This was done to analyse the best environment which can used for real time applications. Further, five different types of commonly used bit rates at different sampling rates were considered to find out the most optimum bit rate.

</p>
</details>

<details><summary><b>Label-Efficient Self-Training for Attribute Extraction from Semi-Structured Web Documents</b>
<a href="https://arxiv.org/abs/2208.13086">arxiv:2208.13086</a>
&#x1F4C8; 4 <br>
<p>Ritesh Sarkhel, Binxuan Huang, Colin Lockard, Prashant Shiralkar</p></summary>
<p>

**Abstract:** Extracting structured information from HTML documents is a long-studied problem with a broad range of applications, including knowledge base construction, faceted search, and personalized recommendation. Prior works rely on a few human-labeled web pages from each target website or thousands of human-labeled web pages from some seed websites to train a transferable extraction model that generalizes on unseen target websites. Noisy content, low site-level consistency, and lack of inter-annotator agreement make labeling web pages a time-consuming and expensive ordeal. We develop LEAST -- a Label-Efficient Self-Training method for Semi-Structured Web Documents to overcome these limitations. LEAST utilizes a few human-labeled pages to pseudo-annotate a large number of unlabeled web pages from the target vertical. It trains a transferable web-extraction model on both human-labeled and pseudo-labeled samples using self-training. To mitigate error propagation due to noisy training samples, LEAST re-weights each training sample based on its estimated label accuracy and incorporates it in training. To the best of our knowledge, this is the first work to propose end-to-end training for transferable web extraction models utilizing only a few human-labeled pages. Experiments on a large-scale public dataset show that using less than ten human-labeled pages from each seed website for training, a LEAST-trained model outperforms previous state-of-the-art by more than 26 average F1 points on unseen websites, reducing the number of human-labeled pages to achieve similar performance by more than 10x.

</p>
</details>

<details><summary><b>Lossy Image Compression with Quantized Hierarchical VAEs</b>
<a href="https://arxiv.org/abs/2208.13056">arxiv:2208.13056</a>
&#x1F4C8; 4 <br>
<p>Zhihao Duan, Ming Lu, Zhan Ma, Fengqing Zhu</p></summary>
<p>

**Abstract:** Recent work has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting from ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding for image compression. Along with improved neural network blocks, we present a powerful and efficient class of lossy image coders, outperforming previous methods on natural image (lossy) compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs.

</p>
</details>

<details><summary><b>Improving debris flow evacuation alerts in Taiwan using machine learning</b>
<a href="https://arxiv.org/abs/2208.13027">arxiv:2208.13027</a>
&#x1F4C8; 4 <br>
<p>Yi-Lin Tsai, Jeremy Irvin, Suhas Chundi, Jo√£o Estacio Gaspar Araujo, Andrew Y. Ng, Christopher B. Field, Peter K. Kitanidis</p></summary>
<p>

**Abstract:** Taiwan has the highest susceptibility to and fatalities from debris flows worldwide. The existing debris flow warning system in Taiwan, which uses a time-weighted measure of rainfall, leads to alerts when the measure exceeds a predefined threshold. However, this system generates many false alarms and misses a substantial fraction of the actual debris flows. Towards improving this system, we implemented five machine learning models that input historical rainfall data and predict whether a debris flow will occur within a selected time. We found that a random forest model performed the best among the five models and outperformed the existing system in Taiwan. Furthermore, we identified the rainfall trajectories strongly related to debris flow occurrences and explored trade-offs between the risks of missing debris flows versus frequent false alerts. These results suggest the potential for machine learning models trained on hourly rainfall data alone to save lives while reducing false alerts.

</p>
</details>

<details><summary><b>A scalable pipeline for COVID-19: the case study of Germany, Czechia and Poland</b>
<a href="https://arxiv.org/abs/2208.12928">arxiv:2208.12928</a>
&#x1F4C8; 4 <br>
<p>Wildan Abdussalam, Adam Mertel, Kai Fan, Lennart Sch√ºler, Weronika Schlechte-We≈Çnicz, Justin M. Calabrese</p></summary>
<p>

**Abstract:** Throughout the coronavirus disease 2019 (COVID-19) pandemic, decision makers have relied on forecasting models to determine and implement non-pharmaceutical interventions (NPI). In building the forecasting models, continuously updated datasets from various stakeholders including developers, analysts, and testers are required to provide precise predictions. Here we report the design of a scalable pipeline which serves as a data synchronization to support inter-country top-down spatiotemporal observations and forecasting models of COVID-19, named the where2test, for Germany, Czechia and Poland. We have built an operational data store (ODS) using PostgreSQL to continuously consolidate datasets from multiple data sources, perform collaborative work, facilitate high performance data analysis, and trace changes. The ODS has been built not only to store the COVID-19 data from Germany, Czechia, and Poland but also other areas. Employing the dimensional fact model, a schema of metadata is capable of synchronizing the various structures of data from those regions, and is scalable to the entire world. Next, the ODS is populated using batch Extract, Transfer, and Load (ETL) jobs. The SQL queries are subsequently created to reduce the need for pre-processing data for users. The data can then support not only forecasting using a version-controlled Arima-Holt model and other analyses to support decision making, but also risk calculator and optimisation apps. The data synchronization runs at a daily interval, which is displayed at https://www.where2test.de.

</p>
</details>

<details><summary><b>Normality-Guided Distributional Reinforcement Learning for Continuous Control</b>
<a href="https://arxiv.org/abs/2208.13125">arxiv:2208.13125</a>
&#x1F4C8; 3 <br>
<p>Ju-Seung Byun, Andrew Perrault</p></summary>
<p>

**Abstract:** Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) methods instead model the value distribution, which has been shown to improve performance in many settings. In this paper, we model the value distribution as approximately normal using the Markov Chain central limit theorem. We analytically compute quantile bars to provide a new DRL target that is informed by the decrease in standard deviation that occurs over the course of an episode. In addition, we suggest an exploration strategy based on how closely the learned value distribution resembles the target normal distribution to make the value function more accurate for better policy improvement. The approach we outline is compatible with many DRL structures. We use proximal policy optimization as a testbed and show that both the normality-guided target and exploration bonus produce performance improvements. We demonstrate our method outperforms DRL baselines on a number of continuous control tasks.

</p>
</details>

<details><summary><b>Spatial Relation Graph and Graph Convolutional Network for Object Goal Navigation</b>
<a href="https://arxiv.org/abs/2208.13031">arxiv:2208.13031</a>
&#x1F4C8; 3 <br>
<p>D. A. Sasi Kiran, Kritika Anand, Chaitanya Kharyal, Gulshan Kumar, Nandiraju Gireesh, Snehasis Banerjee, Ruddra dev Roychoudhury, Mohan Sridharan, Brojeshwar Bhowmick, Madhava Krishna</p></summary>
<p>

**Abstract:** This paper describes a framework for the object-goal navigation task, which requires a robot to find and move to the closest instance of a target object class from a random starting position. The framework uses a history of robot trajectories to learn a Spatial Relational Graph (SRG) and Graph Convolutional Network (GCN)-based embeddings for the likelihood of proximity of different semantically-labeled regions and the occurrence of different object classes in these regions. To locate a target object instance during evaluation, the robot uses Bayesian inference and the SRG to estimate the visible regions, and uses the learned GCN embeddings to rank visible regions and select the region to explore next.

</p>
</details>

<details><summary><b>6D Robotic Assembly Based on RGB-only Object Pose Estimation</b>
<a href="https://arxiv.org/abs/2208.12986">arxiv:2208.12986</a>
&#x1F4C8; 3 <br>
<p>Bowen Fu, Sek Kun Leong, Xiaocong Lian, Xiangyang Ji</p></summary>
<p>

**Abstract:** Vision-based robotic assembly is a crucial yet challenging task as the interaction with multiple objects requires high levels of precision. In this paper, we propose an integrated 6D robotic system to perceive, grasp, manipulate and assemble blocks with tight tolerances. Aiming to provide an off-the-shelf RGB-only solution, our system is built upon a monocular 6D object pose estimation network trained solely with synthetic images leveraging physically-based rendering. Subsequently, pose-guided 6D transformation along with collision-free assembly is proposed to construct any designed structure with arbitrary initial poses. Our novel 3-axis calibration operation further enhances the precision and robustness by disentangling 6D pose estimation and robotic assembly. Both quantitative and qualitative results demonstrate the effectiveness of our proposed 6D robotic assembly system.

</p>
</details>

<details><summary><b>An Access Control Method with Secret Key for Semantic Segmentation Models</b>
<a href="https://arxiv.org/abs/2208.13135">arxiv:2208.13135</a>
&#x1F4C8; 2 <br>
<p>Teru Nagamori, Ryota Iijima, Hitoshi Kiya</p></summary>
<p>

**Abstract:** A novel method for access control with a secret key is proposed to protect models from unauthorized access in this paper. We focus on semantic segmentation models with the vision transformer (ViT), called segmentation transformer (SETR). Most existing access control methods focus on image classification tasks, or they are limited to CNNs. By using a patch embedding structure that ViT has, trained models and test images can be efficiently encrypted with a secret key, and then semantic segmentation tasks are carried out in the encrypted domain. In an experiment, the method is confirmed to provide the same accuracy as that of using plain images without any encryption to authorized users with a correct key and also to provide an extremely degraded accuracy to unauthorized users.

</p>
</details>

<details><summary><b>An Empirical Study on the Usage of Automated Machine Learning Tools</b>
<a href="https://arxiv.org/abs/2208.13116">arxiv:2208.13116</a>
&#x1F4C8; 2 <br>
<p>Forough Majidi, Moses Openja, Foutse Khomh, Heng Li</p></summary>
<p>

**Abstract:** The popularity of automated machine learning (AutoML) tools in different domains has increased over the past few years. Machine learning (ML) practitioners use AutoML tools to automate and optimize the process of feature engineering, model training, and hyperparameter optimization and so on. Recent work performed qualitative studies on practitioners' experiences of using AutoML tools and compared different AutoML tools based on their performance and provided features, but none of the existing work studied the practices of using AutoML tools in real-world projects at a large scale. Therefore, we conducted an empirical study to understand how ML practitioners use AutoML tools in their projects. To this end, we examined the top 10 most used AutoML tools and their respective usages in a large number of open-source project repositories hosted on GitHub. The results of our study show 1) which AutoML tools are mostly used by ML practitioners and 2) the characteristics of the repositories that use these AutoML tools. Also, we identified the purpose of using AutoML tools (e.g. model parameter sampling, search space management, model evaluation/error-analysis, Data/ feature transformation, and data labeling) and the stages of the ML pipeline (e.g. feature engineering) where AutoML tools are used. Finally, we report how often AutoML tools are used together in the same source code files. We hope our results can help ML practitioners learn about different AutoML tools and their usages, so that they can pick the right tool for their purposes. Besides, AutoML tool developers can benefit from our findings to gain insight into the usages of their tools and improve their tools to better fit the users' usages and needs.

</p>
</details>

<details><summary><b>Improving Electricity Market Economy via Closed-Loop Predict-and-Optimize</b>
<a href="https://arxiv.org/abs/2208.13065">arxiv:2208.13065</a>
&#x1F4C8; 2 <br>
<p>Xianbang Chen, Yikui Liu, Lei Wu</p></summary>
<p>

**Abstract:** The electricity market clearing is usually implemented via an open-loop predict-then-optimize (O-PO) process: it first predicts the available power of renewable energy sources (RES) and the system reserve requirements; then, given the predictions, the markets are cleared via optimization models, i.e., unit commitment (UC) and economic dispatch (ED), to pursue the optimal electricity market economy. However, the market economy could suffer from the open-loop process because its predictions may be overly myopic to the optimizations, i.e., the predictions seek to improve the immediate statistical forecasting errors instead of the ultimate market economy. To this end, this paper proposes a closed-loop predict-and-optimize (C-PO) framework based on the tri-level mixed-integer programming, which trains economy-oriented predictors tailored for the market-clearing optimization to improve the ultimate market economy. Specifically, the upper level trains the economy-oriented RES and reserve predictors according to their induced market economy; the middle and lower levels, with given predictions, mimic the market-clearing process and feed the induced market economy results back to the upper level. The trained economy-oriented predictors are then embedded into the UC model, forming a prescriptive UC model that can simultaneously provide RES-reserve predictions and UC decisions with enhanced market economy. Numerical case studies on an IEEE 118-bus system illustrate potential economic and practical advantages of C-PO over O-PO, robust UC, and stochastic UC.

</p>
</details>

<details><summary><b>A Federated Learning-enabled Smart Street Light Monitoring Application: Benefits and Future Challenges</b>
<a href="https://arxiv.org/abs/2208.12996">arxiv:2208.12996</a>
&#x1F4C8; 2 <br>
<p>Diya Anand, Ioannis Mavromatis, Pietro Carnelli, Aftab Khan</p></summary>
<p>

**Abstract:** Data-enabled cities are recently accelerated and enhanced with automated learning for improved Smart Cities applications. In the context of an Internet of Things (IoT) ecosystem, the data communication is frequently costly, inefficient, not scalable and lacks security. Federated Learning (FL) plays a pivotal role in providing privacy-preserving and communication efficient Machine Learning (ML) frameworks. In this paper we evaluate the feasibility of FL in the context of a Smart Cities Street Light Monitoring application. FL is evaluated against benchmarks of centralised and (fully) personalised machine learning techniques for the classification task of the lampposts operation. Incorporating FL in such a scenario shows minimal performance reduction in terms of the classification task, but huge improvements in the communication cost and the privacy preserving. These outcomes strengthen FL's viability and potential for IoT applications.

</p>
</details>

<details><summary><b>Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers</b>
<a href="https://arxiv.org/abs/2208.13113">arxiv:2208.13113</a>
&#x1F4C8; 1 <br>
<p>Youbao Tang, Ning Zhang, Yirui Wang, Shenghua He, Mei Han, Jing Xiao, Ruei-Sung Lin</p></summary>
<p>

**Abstract:** Automatically measuring lesion/tumor size with RECIST (Response Evaluation Criteria In Solid Tumors) diameters and segmentation is important for computer-aided diagnosis. Although it has been studied in recent years, there is still space to improve its accuracy and robustness, such as (1) enhancing features by incorporating rich contextual information while keeping a high spatial resolution and (2) involving new tasks and losses for joint optimization. To reach this goal, this paper proposes a transformer-based network (MeaFormer, Measurement transFormer) for lesion RECIST diameter prediction and segmentation (LRDPS). It is formulated as three correlative and complementary tasks: lesion segmentation, heatmap prediction, and keypoint regression. To the best of our knowledge, it is the first time to use keypoint regression for RECIST diameter prediction. MeaFormer can enhance high-resolution features by employing transformers to capture their long-range dependencies. Two consistency losses are introduced to explicitly build relationships among these tasks for better optimization. Experiments show that MeaFormer achieves the state-of-the-art performance of LRDPS on the large-scale DeepLesion dataset and produces promising results of two downstream clinic-relevant tasks, i.e., 3D lesion segmentation and RECIST assessment in longitudinal studies.

</p>
</details>

<details><summary><b>Object Goal Navigation using Data Regularized Q-Learning</b>
<a href="https://arxiv.org/abs/2208.13009">arxiv:2208.13009</a>
&#x1F4C8; 1 <br>
<p>Nandiraju Gireesh, D. A. Sasi Kiran, Snehasis Banerjee, Mohan Sridharan, Brojeshwar Bhowmick, Madhava Krishna</p></summary>
<p>

**Abstract:** Object Goal Navigation requires a robot to find and navigate to an instance of a target object class in a previously unseen environment. Our framework incrementally builds a semantic map of the environment over time, and then repeatedly selects a long-term goal ('where to go') based on the semantic map to locate the target object instance. Long-term goal selection is formulated as a vision-based deep reinforcement learning problem. Specifically, an Encoder Network is trained to extract high-level features from a semantic map and select a long-term goal. In addition, we incorporate data augmentation and Q-function regularization to make the long-term goal selection more effective. We report experimental results using the photo-realistic Gibson benchmark dataset in the AI Habitat 3D simulation environment to demonstrate substantial performance improvement on standard measures in comparison with a state of the art data-driven baseline.

</p>
</details>

<details><summary><b>Neural Observer with Lyapunov Stability Guarantee for Uncertain Nonlinear Systems</b>
<a href="https://arxiv.org/abs/2208.13006">arxiv:2208.13006</a>
&#x1F4C8; 1 <br>
<p>Song Chen, Tehuan Chen, Chao Xu, Jian Chu</p></summary>
<p>

**Abstract:** In this paper, we propose a novel nonlinear observer, called the neural observer, for observation tasks of linear time-invariant (LTI) systems and uncertain nonlinear systems by introducing the neural network (NN) into the design of observers. By exploring the method of NN representation to the NN mapping vector, we derive stability analyses (e.g., exponential convergence rate) of LTI and uncertain nonlinear systems that pave the way to solve observation problems using linear matrix inequalities (LMIs) only. Remarkably, the neural observer designed for uncertain systems is based on the ideology of the active disturbance rejection control (ADRC), which can measure the uncertainty in real-time. The LMI results are also significant since we reveal that the observability and controllability of system matrices are required for the existence of solutions of LMIs. Finally, we verify the availability of neural observers on three simulation cases, including the X-29A aircraft model, the nonlinear pendulum, and the four-wheel steering vehicle.

</p>
</details>

<details><summary><b>On Biased Behavior of GANs for Face Verification</b>
<a href="https://arxiv.org/abs/2208.13061">arxiv:2208.13061</a>
&#x1F4C8; 0 <br>
<p>Sasikanth Kotti, Mayank Vatsa, Richa Singh</p></summary>
<p>

**Abstract:** Deep Learning systems need large data for training. Datasets for training face verification systems are difficult to obtain and prone to privacy issues. Synthetic data generated by generative models such as GANs can be a good alternative. However, we show that data generated from GANs are prone to bias and fairness issues. Specifically, GANs trained on FFHQ dataset show biased behavior towards generating white faces in the age group of 20-29. We also demonstrate that synthetic faces cause disparate impact, specifically for race attribute, when used for fine tuning face verification systems.

</p>
</details>


{% endraw %}
Prev: [2022.08.26]({{ '/2022/08/26/2022.08.26.html' | relative_url }})  Next: [2022.08.28]({{ '/2022/08/28/2022.08.28.html' | relative_url }})