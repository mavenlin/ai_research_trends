## Summary for 2021-04-11, created on 2021-12-22


<details><summary><b>A Deep Learning Based Cost Model for Automatic Code Optimization</b>
<a href="https://arxiv.org/abs/2104.04955">arxiv:2104.04955</a>
&#x1F4C8; 184 <br>
<p>Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel Abdous, Taha Arbaoui, Karima Benatchba, Saman Amarasinghe</p></summary>
<p>

**Abstract:** Enabling compilers to automatically optimize code has been a longstanding goal for the compiler community. Efficiently solving this problem requires using precise cost models. These models predict whether applying a sequence of code transformations reduces the execution time of the program. Building an analytical cost model to do so is hard in modern x86 architectures due to the complexity of the microarchitecture. In this paper, we present a novel deep learning based cost model for automatic code optimization. This model was integrated in a search method and implemented in the Tiramisu compiler to select the best code transformations. The input of the proposed model is a set of simple features representing the unoptimized code and a sequence of code transformations. The model predicts the speedup expected when the code transformations are applied. Unlike previous models, the proposed one works on full programs and does not rely on any heavy feature engineering. The proposed model has only 16% of mean absolute percentage error in predicting speedups on full programs. The proposed model enables Tiramisu to automatically find code transformations that match or are better than state-of-the-art compilers without requiring the same level of heavy feature engineering required by those compilers.

</p>
</details>

<details><summary><b>StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</b>
<a href="https://arxiv.org/abs/2104.05196">arxiv:2104.05196</a>
&#x1F4C8; 20 <br>
<p>Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency</p></summary>
<p>

**Abstract:** Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.

</p>
</details>

<details><summary><b>Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning</b>
<a href="https://arxiv.org/abs/2104.04975">arxiv:2104.04975</a>
&#x1F4C8; 11 <br>
<p>Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, Mohammad Emtiyaz Khan</p></summary>
<p>

**Abstract:** Marginal-likelihood based model-selection, even though promising, is rarely used in deep learning due to estimation difficulties. Instead, most approaches rely on validation data, which may not be readily available. In this work, we present a scalable marginal-likelihood estimation method to select both hyperparameters and network architectures, based on the training data alone. Some hyperparameters can be estimated online during training, simplifying the procedure. Our marginal-likelihood estimate is based on Laplace's method and Gauss-Newton approximations to the Hessian, and it outperforms cross-validation and manual-tuning on standard regression and image classification datasets, especially in terms of calibration and out-of-distribution detection. Our work shows that marginal likelihoods can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings).

</p>
</details>

<details><summary><b>Conversational Semantic Role Labeling</b>
<a href="https://arxiv.org/abs/2104.04947">arxiv:2104.04947</a>
&#x1F4C8; 9 <br>
<p>Kun Xu, Han Wu, Linfeng Song, Haisong Zhang, Linqi Song, Dong Yu</p></summary>
<p>

**Abstract:** Semantic role labeling (SRL) aims to extract the arguments for each predicate in an input sentence. Traditional SRL can fail to analyze dialogues because it only works on every single sentence, while ellipsis and anaphora frequently occur in dialogues. To address this problem, we propose the conversational SRL task, where an argument can be the dialogue participants, a phrase in the dialogue history or the current sentence. As the existing SRL datasets are in the sentence level, we manually annotate semantic roles for 3,000 chit-chat dialogues (27,198 sentences) to boost the research in this direction. Experiments show that while traditional SRL systems (even with the help of coreference resolution or rewriting) perform poorly for analyzing dialogues, modeling dialogue histories and participants greatly helps the performance, indicating that adapting SRL to conversations is very promising for universal dialogue understanding. Our initial study by applying CSRL to two mainstream conversational tasks, dialogue response generation and dialogue context rewriting, also confirms the usefulness of CSRL.

</p>
</details>

<details><summary><b>Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog</b>
<a href="https://arxiv.org/abs/2104.04923">arxiv:2104.04923</a>
&#x1F4C8; 6 <br>
<p>Arun Babu, Akshat Shrivastava, Armen Aghajanyan, Ahmed Aly, Angela Fan, Marjan Ghazvininejad</p></summary>
<p>

**Abstract:** Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets. Our code is available at https://github.com/facebookresearch/pytext

</p>
</details>

<details><summary><b>GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion</b>
<a href="https://arxiv.org/abs/2104.05177">arxiv:2104.05177</a>
&#x1F4C8; 5 <br>
<p>Cheng Chi, Shuran Song</p></summary>
<p>

**Abstract:** This paper tackles the task of category-level pose estimation for garments. With a near infinite degree of freedom, a garment's full configuration (i.e., poses) is often described by the per-vertex 3D locations of its entire 3D surface. However, garments are also commonly subject to extreme cases of self-occlusion, especially when folded or crumpled, making it challenging to perceive their full 3D surface. To address these challenges, we propose GarmentNets, where the key idea is to formulate the deformable object pose estimation problem as a shape completion task in the canonical space. This canonical space is defined across garments instances within a category, therefore, specifies the shared category-level pose. By mapping the observed partial surface to the canonical space and completing it in this space, the output representation describes the garment's full configuration using a complete 3D mesh with the per-vertex canonical coordinate label. To properly handle the thin 3D structure presented on garments, we proposed a novel 3D shape representation using the generalized winding number field. Experiments demonstrate that GarmentNets is able to generalize to unseen garment instances and achieve significantly better performance compared to alternative approaches.

</p>
</details>

<details><summary><b>Constructing Contrastive samples via Summarization for Text Classification with limited annotations</b>
<a href="https://arxiv.org/abs/2104.05094">arxiv:2104.05094</a>
&#x1F4C8; 5 <br>
<p>Yangkai Du, Tengfei Ma, Lingfei Wu, Fangli Xu, Xuhong Zhang, Bo Long, Shouling Ji</p></summary>
<p>

**Abstract:** Contrastive Learning has emerged as a powerful representation learning method and facilitates various downstream tasks especially when supervised data is limited. How to construct efficient contrastive samples through data augmentation is key to its success. Unlike vision tasks, the data augmentation method for contrastive learning has not been investigated sufficiently in language tasks. In this paper, we propose a novel approach to construct contrastive samples for language tasks using text summarization. We use these samples for supervised contrastive learning to gain better text representations which greatly benefit text classification tasks with limited annotations. To further improve the method, we mix up samples from different classes and add an extra regularization, named Mixsum, in addition to the cross-entropy-loss. Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG News, and IMDb) demonstrate the effectiveness of the proposed contrastive learning framework with summarization-based data augmentation and Mixsum regularization.

</p>
</details>

<details><summary><b>Pose Invariant Person Re-Identification using Robust Pose-transformation GAN</b>
<a href="https://arxiv.org/abs/2105.00930">arxiv:2105.00930</a>
&#x1F4C8; 4 <br>
<p>Arnab Karmakar, Deepak Mishra</p></summary>
<p>

**Abstract:** The objective of person re-identification (re-ID) is to retrieve a person's images from an image gallery, given a single instance of the person of interest. Despite several advancements, learning discriminative identity-sensitive and viewpoint invariant features for robust Person Re-identification is a major challenge owing to the large pose variation of humans. This paper proposes a re-ID pipeline that utilizes the image generation capability of Generative Adversarial Networks combined with pose clustering and feature fusion to achieve pose invariant feature learning. The objective is to model a given person under different viewpoints and large pose changes and extract the most discriminative features from all the appearances. The pose transformational GAN (pt-GAN) module is trained to generate a person's image in any given pose. In order to identify the most significant poses for discriminative feature extraction, a Pose Clustering module is proposed. The given instance of the person is modelled in varying poses and these features are effectively combined through the Feature Fusion Network. The final re-ID model consisting of these 3 sub-blocks, alleviates the pose dependence in person re-ID. Also, The proposed model is robust to occlusion, scale, rotation and illumination, providing a framework for viewpoint invariant feature learning. The proposed method outperforms the state-of-the-art GAN based models in 4 benchmark datasets. It also surpasses the state-of-the-art models that report higher re-ID accuracy in terms of improvement over baseline.

</p>
</details>

<details><summary><b>The Many Faces of 1-Lipschitz Neural Networks</b>
<a href="https://arxiv.org/abs/2104.05097">arxiv:2104.05097</a>
&#x1F4C8; 4 <br>
<p>Louis Béthune, Alberto González-Sanz, Franck Mamalet, Mathieu Serrurier</p></summary>
<p>

**Abstract:** Lipschitz constrained models have been used to solve specifics deep learning problems such as the estimation of Wasserstein distance for GAN, or the training of neural networks robust to adversarial attacks. Regardless the novel and effective algorithms to build such 1-Lipschitz networks, their usage remains marginal, and they are commonly considered as less expressive and less able to fit properly the data than their unconstrained counterpart.
  The goal of the paper is to demonstrate that, despite being empirically harder to train, 1-Lipschitz neural networks are theoretically better grounded than unconstrained ones when it comes to classification. To achieve that we recall some results about 1-Lipschitz function in the scope of deep learning and we extend and illustrate them to derive general properties for classification.
  First, we show that 1-Lipschitz neural network can fit arbitrarily difficult frontier making them as expressive as classical ones. When minimizing the log loss, we prove that the optimization problem under Lipschitz constraint is well posed and have a minimum, whereas regular neural networks can diverge even on remarkably simple situations. Then, we study the link between classification with 1-Lipschitz network and optimal transport thanks to regularized versions of Kantorovich-Rubinstein duality theory. Last, we derive preliminary bounds on their VC dimension.

</p>
</details>

<details><summary><b>The World as a Graph: Improving El Niño Forecasts with Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2104.05089">arxiv:2104.05089</a>
&#x1F4C8; 3 <br>
<p>Salva Rühling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest Pokropek, Willa Potosnak, Suyash Bire, Salomey Osei, Björn Lütjens</p></summary>
<p>

**Abstract:** Deep learning-based models have recently outperformed state-of-the-art seasonal forecasting models, such as for predicting El Niño-Southern Oscillation (ENSO). However, current deep learning models are based on convolutional neural networks which are difficult to interpret and can fail to model large-scale atmospheric patterns. In comparison, graph neural networks (GNNs) are capable of modeling large-scale spatial dependencies and are more interpretable due to the explicit modeling of information flow through edge connections. We propose the first application of graph neural networks to seasonal forecasting. We design a novel graph connectivity learning module that enables our GNN model to learn large-scale spatial interactions jointly with the actual ENSO forecasting task. Our model, \graphino, outperforms state-of-the-art deep learning-based models for forecasts up to six months ahead. Additionally, we show that our model is more interpretable as it learns sensible connectivity structures that correlate with the ENSO anomaly pattern.

</p>
</details>

<details><summary><b>CoPE: Conditional image generation using Polynomial Expansions</b>
<a href="https://arxiv.org/abs/2104.05077">arxiv:2104.05077</a>
&#x1F4C8; 3 <br>
<p>Grigorios G Chrysos, Markos Georgopoulos, Yannis Panagakis</p></summary>
<p>

**Abstract:** Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks. The source code of CoPE is available at \url{https://github.com/grigorisg9gr/polynomial_nets_for_conditional_generation}.

</p>
</details>

<details><summary><b>ALT-MAS: A Data-Efficient Framework for Active Testing of Machine Learning Algorithms</b>
<a href="https://arxiv.org/abs/2104.04999">arxiv:2104.04999</a>
&#x1F4C8; 3 <br>
<p>Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh</p></summary>
<p>

**Abstract:** Machine learning models are being used extensively in many important areas, but there is no guarantee a model will always perform well or as its developers intended. Understanding the correctness of a model is crucial to prevent potential failures that may have significant detrimental impact in critical application areas. In this paper, we propose a novel framework to efficiently test a machine learning model using only a small amount of labeled test data. The idea is to estimate the metrics of interest for a model-under-test using Bayesian neural network (BNN). We develop a novel data augmentation method helping to train the BNN to achieve high accuracy. We also devise a theoretic information based sampling strategy to sample data points so as to achieve accurate estimations for the metrics of interest. Finally, we conduct an extensive set of experiments to test various machine learning models for different types of metrics. Our experiments show that the metrics estimations by our method are significantly better than existing baselines.

</p>
</details>

<details><summary><b>Unsupervised Learning of Explainable Parse Trees for Improved Generalisation</b>
<a href="https://arxiv.org/abs/2104.04998">arxiv:2104.04998</a>
&#x1F4C8; 3 <br>
<p>Atul Sahay, Ayush Maheshwari, Ritesh Kumar, Ganesh Ramakrishnan, Manjesh Kumar Hanawal, Kavi Arya</p></summary>
<p>

**Abstract:** Recursive neural networks (RvNN) have been shown useful for learning sentence representations and helped achieve competitive performance on several natural language inference tasks. However, recent RvNN-based models fail to learn simple grammar and meaningful semantics in their intermediate tree representation. In this work, we propose an attention mechanism over Tree-LSTMs to learn more meaningful and explainable parse tree structures. We also demonstrate the superior performance of our proposed model on natural language inference, semantic relatedness, and sentiment analysis tasks and compare them with other state-of-the-art RvNN based methods. Further, we present a detailed qualitative and quantitative analysis of the learned parse trees and show that the discovered linguistic structures are more explainable, semantically meaningful, and grammatically correct than recent approaches. The source code of the paper is available at https://github.com/atul04/Explainable-Latent-Structures-Using-Attention.

</p>
</details>

<details><summary><b>AutoGL: A Library for Automated Graph Learning</b>
<a href="https://arxiv.org/abs/2104.04987">arxiv:2104.04987</a>
&#x1F4C8; 3 <br>
<p>Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang Zhang, Yijian Qin, Jiyan Jiang, Xin Wang, Wenwu Zhu</p></summary>
<p>

**Abstract:** Recent years have witnessed an upsurge of research interests and applications of machine learning on graphs. Automated machine learning (AutoML) on graphs is on the horizon to automatically design the optimal machine learning algorithm for a given graph task. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose an automated machine learning pipeline for graph data containing four modules: auto feature engineering, model training, hyper-parameter optimization, and auto ensemble. For each module, we provide numerous state-of-the-art methods and flexible base classes and APIs, which allow easy customization. We further provide experimental results to showcase the usage of our AutoGL library.

</p>
</details>

<details><summary><b>Memory Capacity of Neural Turing Machines with Matrix Representation</b>
<a href="https://arxiv.org/abs/2104.07454">arxiv:2104.07454</a>
&#x1F4C8; 2 <br>
<p>Animesh Renanse, Rohitash Chandra, Alok Sharma</p></summary>
<p>

**Abstract:** It is well known that recurrent neural networks (RNNs) faced limitations in learning long-term dependencies that have been addressed by memory structures in long short-term memory (LSTM) networks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data and has the potential to provide better memory structures when compared to canonical neural networks that use vector representation. Neural Turing machines (NTMs) are novel RNNs that implement notion of programmable computers with neural network controllers to feature algorithms that have copying, sorting, and associative recall tasks. In this paper, we study the augmentation of memory capacity with a matrix representation of RNNs and NTMs (MatNTMs). We investigate if matrix representation has a better memory capacity than the vector representations in conventional neural networks. We use a probabilistic model of the memory capacity using Fisher information and investigate how the memory capacity for matrix representation networks are limited under various constraints, and in general, without any constraints. In the case of memory capacity without any constraints, we found that the upper bound on memory capacity to be $N^2$ for an $N\times N$ state matrix. The results from our experiments using synthetic algorithmic tasks show that MatNTMs have a better learning capacity when compared to its counterparts.

</p>
</details>

<details><summary><b>Detecting COVID-19 and Community Acquired Pneumonia using Chest CT scan images with Deep Learning</b>
<a href="https://arxiv.org/abs/2104.05121">arxiv:2104.05121</a>
&#x1F4C8; 2 <br>
<p>Shubham Chaudhary,  Sadbhawna, Vinit Jakhetiya, Badri N Subudhi, Ujjwal Baid, Sharath Chandra Guntuku</p></summary>
<p>

**Abstract:** We propose a two-stage Convolutional Neural Network (CNN) based classification framework for detecting COVID-19 and Community-Acquired Pneumonia (CAP) using the chest Computed Tomography (CT) scan images. In the first stage, an infection - COVID-19 or CAP, is detected using a pre-trained DenseNet architecture. Then, in the second stage, a fine-grained three-way classification is done using EfficientNet architecture. The proposed COVID+CAP-CNN framework achieved a slice-level classification accuracy of over 94% at identifying COVID-19 and CAP. Further, the proposed framework has the potential to be an initial screening tool for differential diagnosis of COVID-19 and CAP, achieving a validation accuracy of over 89.3% at the finer three-way COVID-19, CAP, and healthy classification. Within the IEEE ICASSP 2021 Signal Processing Grand Challenge (SPGC) on COVID-19 Diagnosis, our proposed two-stage classification framework achieved an overall accuracy of 90% and sensitivity of .857, .9, and .942 at distinguishing COVID-19, CAP, and normal individuals respectively, to rank first in the evaluation. Code and model weights are available at https://github.com/shubhamchaudhary2015/ct_covid19_cap_cnn

</p>
</details>

<details><summary><b>iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo Matching on FPGA Platform</b>
<a href="https://arxiv.org/abs/2104.05112">arxiv:2104.05112</a>
&#x1F4C8; 2 <br>
<p>Tian Gao, Zishen Wan, Yuyang Zhang, Bo Yu, Yanjun Zhang, Shaoshan Liu, Arijit Raychowdhury</p></summary>
<p>

**Abstract:** Stereo matching is a critical task for robot navigation and autonomous vehicles, providing the depth estimation of surroundings. Among all stereo matching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best tradeoffs between efficiency and accuracy. However, due to the inherent iterative process and unpredictable memory access pattern, ELAS can only run at 1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on low-power platforms. In this paper, we propose an energy-efficient architecture for real-time ELAS-based stereo matching on FPGA platform. Moreover, the original computational-intensive and irregular triangulation module is reformed in a regular manner with points interpolation, which is much more hardware-friendly. Optimizations, including memory management, parallelism, and pipelining, are further utilized to reduce memory footprint and improve throughput. Compared with Intel i7 CPU and the state-of-the-art CPU+FPGA implementation, our FPGA realization achieves up to 38.4x and 3.32x frame rate improvement, and up to 27.1x and 1.13x energy efficiency improvement, respectively.

</p>
</details>

<details><summary><b>Research on Optimization Method of Multi-scale Fish Target Fast Detection Network</b>
<a href="https://arxiv.org/abs/2104.05050">arxiv:2104.05050</a>
&#x1F4C8; 2 <br>
<p>Yang Liu, Shengmao Zhang, Fei Wang, Wei Fan, Guohua Zou, Jing Bo</p></summary>
<p>

**Abstract:** The fish target detection algorithm lacks a good quality data set, and the algorithm achieves real-time detection with lower power consumption on embedded devices, and it is difficult to balance the calculation speed and identification ability. To this end, this paper collected and annotated a data set named "Aquarium Fish" of 84 fishes containing 10042 images, and based on this data set, proposed a multi-scale input fast fish target detection network (BTP-yoloV3) and its optimization method. The experiment uses Depthwise convolution to redesign the backbone of the yoloV4 network, which reduces the amount of calculation by 94.1%, and the test accuracy is 92.34%. Then, the training model is enhanced with MixUp, CutMix, and mosaic to increase the test accuracy by 1.27%; Finally, use the mish, swish, and ELU activation functions to increase the test accuracy by 0.76%. As a result, the accuracy of testing the network with 2000 fish images reached 94.37%, and the computational complexity of the network BFLOPS was only 5.47. Comparing the YoloV3~4, MobileNetV2-yoloV3, and YoloV3-tiny networks of migration learning on this data set. The results show that BTP-Yolov3 has smaller model parameters, faster calculation speed, and lower energy consumption during operation while ensuring the calculation accuracy. It provides a certain reference value for the practical application of neural network.

</p>
</details>

<details><summary><b>Learning representations with end-to-end models for improved remaining useful life prognostics</b>
<a href="https://arxiv.org/abs/2104.05049">arxiv:2104.05049</a>
&#x1F4C8; 2 <br>
<p>Alaaeddine Chaoub, Alexandre Voisin, Christophe Cerisara, Benoît Iung</p></summary>
<p>

**Abstract:** The remaining Useful Life (RUL) of equipment is defined as the duration between the current time and its failure. An accurate and reliable prognostic of the remaining useful life provides decision-makers with valuable information to adopt an appropriate maintenance strategy to maximize equipment utilization and avoid costly breakdowns. In this work, we propose an end-to-end deep learning model based on multi-layer perceptron and long short-term memory layers (LSTM) to predict the RUL. After normalization of all data, inputs are fed directly to an MLP layers for feature learning, then to an LSTM layer to capture temporal dependencies, and finally to other MLP layers for RUL prognostic. The proposed architecture is tested on the NASA commercial modular aero-propulsion system simulation (C-MAPSS) dataset. Despite its simplicity with respect to other recently proposed models, the model developed outperforms them with a significant decrease in the competition score and in the root mean square error score between the predicted and the gold value of the RUL. In this paper, we will discuss how the proposed end-to-end model is able to achieve such good results and compare it to other deep learning and state-of-the-art methods.

</p>
</details>

<details><summary><b>Rank-R FNN: A Tensor-Based Learning Model for High-Order Data Classification</b>
<a href="https://arxiv.org/abs/2104.05048">arxiv:2104.05048</a>
&#x1F4C8; 2 <br>
<p>Konstantinos Makantasis, Alexandros Georgogiannis, Athanasios Voulodimos, Ioannis Georgoulas, Anastasios Doulamis, Nikolaos Doulamis</p></summary>
<p>

**Abstract:** An increasing number of emerging applications in data science and engineering are based on multidimensional and structurally rich data. The irregularities, however, of high-dimensional data often compromise the effectiveness of standard machine learning algorithms. We hereby propose the Rank-R Feedforward Neural Network (FNN), a tensor-based nonlinear learning model that imposes Canonical/Polyadic decomposition on its parameters, thereby offering two core advantages compared to typical machine learning methods. First, it handles inputs as multilinear arrays, bypassing the need for vectorization, and can thus fully exploit the structural information along every data dimension. Moreover, the number of the model's trainable parameters is substantially reduced, making it very efficient for small sample setting problems. We establish the universal approximation and learnability properties of Rank-R FNN, and we validate its performance on real-world hyperspectral datasets. Experimental evaluations show that Rank-R FNN is a computationally inexpensive alternative of ordinary FNN that achieves state-of-the-art performance on higher-order tensor data.

</p>
</details>

<details><summary><b>TedNet: A Pytorch Toolkit for Tensor Decomposition Networks</b>
<a href="https://arxiv.org/abs/2104.05018">arxiv:2104.05018</a>
&#x1F4C8; 2 <br>
<p>Yu Pan, Maolin Wang, Zenglin Xu</p></summary>
<p>

**Abstract:** Tensor Decomposition Networks (TDNs) prevail for their inherent compact architectures. To give more researchers a flexible way to exploit TDNs, we present a Pytorch toolkit named TedNet. TedNet implements 5 kinds of tensor decomposition(i.e., CANDECOMP/PARAFAC (CP), Block-Term Tucker (BTT), Tucker-2, Tensor Train (TT) and Tensor Ring (TR) on traditional deep neural layers, the convolutional layer and the fully-connected layer. By utilizing the basic layers, it is simple to construct a variety of TDNs. TedNet is available at https://github.com/tnbar/tednet.

</p>
</details>

<details><summary><b>Saddlepoints in Unsupervised Least Squares</b>
<a href="https://arxiv.org/abs/2104.05000">arxiv:2104.05000</a>
&#x1F4C8; 2 <br>
<p>Samuel Gerber</p></summary>
<p>

**Abstract:** This paper sheds light on the risk landscape of unsupervised least squares in the context of deep auto-encoding neural nets. We formally establish an equivalence between unsupervised least squares and principal manifolds. This link provides insight into the risk landscape of auto--encoding under the mean squared error, in particular all non-trivial critical points are saddlepoints. Finding saddlepoints is in itself difficult, overcomplete auto-encoding poses the additional challenge that the saddlepoints are degenerate. Within this context we discuss regularization of auto-encoders, in particular bottleneck, denoising and contraction auto-encoding and propose a new optimization strategy that can be framed as particular form of contractive regularization.

</p>
</details>

<details><summary><b>Supervised Feature Selection Techniques in Network Intrusion Detection: a Critical Review</b>
<a href="https://arxiv.org/abs/2104.04958">arxiv:2104.04958</a>
&#x1F4C8; 2 <br>
<p>Mario Di Mauro, Giovanni Galatro, Giancarlo Fortino, Antonio Liotta</p></summary>
<p>

**Abstract:** Machine Learning (ML) techniques are becoming an invaluable support for network intrusion detection, especially in revealing anomalous flows, which often hide cyber-threats. Typically, ML algorithms are exploited to classify/recognize data traffic on the basis of statistical features such as inter-arrival times, packets length distribution, mean number of flows, etc. Dealing with the vast diversity and number of features that typically characterize data traffic is a hard problem. This results in the following issues: i) the presence of so many features leads to lengthy training processes (particularly when features are highly correlated), while prediction accuracy does not proportionally improve; ii) some of the features may introduce bias during the classification process, particularly those that have scarce relation with the data traffic to be classified. To this end, by reducing the feature space and retaining only the most significant features, Feature Selection (FS) becomes a crucial pre-processing step in network management and, specifically, for the purposes of network intrusion detection. In this review paper, we complement other surveys in multiple ways: i) evaluating more recent datasets (updated w.r.t. obsolete KDD 99) by means of a designed-from-scratch Python-based procedure; ii) providing a synopsis of most credited FS approaches in the field of intrusion detection, including Multi-Objective Evolutionary techniques; iii) assessing various experimental analyses such as feature correlation, time complexity, and performance. Our comparisons offer useful guidelines to network/security managers who are considering the incorporation of ML concepts into network intrusion detection, where trade-offs between performance and resource consumption are crucial.

</p>
</details>

<details><summary><b>Fine-Grained Attention for Weakly Supervised Object Localization</b>
<a href="https://arxiv.org/abs/2104.04952">arxiv:2104.04952</a>
&#x1F4C8; 2 <br>
<p>Junghyo Sohn, Eunjin Jeon, Wonsik Jung, Eunsong Kang, Heung-Il Suk</p></summary>
<p>

**Abstract:** Although recent advances in deep learning accelerated an improvement in a weakly supervised object localization (WSOL) task, there are still challenges to identify the entire body of an object, rather than only discriminative parts. In this paper, we propose a novel residual fine-grained attention (RFGA) module that autonomously excites the less activated regions of an object by utilizing information distributed over channels and locations within feature maps in combination with a residual operation. To be specific, we devise a series of mechanisms of triple-view attention representation, attention expansion, and feature calibration. Unlike other attention-based WSOL methods that learn a coarse attention map, having the same values across elements in feature maps, our proposed RFGA learns fine-grained values in an attention map by assigning different attention values for each of the elements. We validated the superiority of our proposed RFGA module by comparing it with the recent methods in the literature over three datasets. Further, we analyzed the effect of each mechanism in our RFGA and visualized attention maps to get insights.

</p>
</details>

<details><summary><b>Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation</b>
<a href="https://arxiv.org/abs/2104.04945">arxiv:2104.04945</a>
&#x1F4C8; 2 <br>
<p>Tomasz Szandala</p></summary>
<p>

**Abstract:** In this paper, an enhancement technique for the class activation mapping methods such as gradient-weighted class activation maps or excitation backpropagation is proposed to present the visual explanations of decisions from convolutional neural network-based models. The proposed idea, called Gradual Extrapolation, can supplement any method that generates a heatmap picture by sharpening the output. Instead of producing a coarse localization map that highlights the important predictive regions in the image, the proposed method outputs the specific shape that most contributes to the model output. Thus, the proposed method improves the accuracy of saliency maps. The effect has been achieved by the gradual propagation of the crude map obtained in the deep layer through all preceding layers with respect to their activations. In validation tests conducted on a selected set of images, the faithfulness, interpretability, and applicability of the method are evaluated. The proposed technique significantly improves the localization detection of the neural networks attention at low additional computational costs. Furthermore, the proposed method is applicable to a variety deep neural network models. The code for the method can be found at https://github.com/szandala/gradual-extrapolation

</p>
</details>

<details><summary><b>A Graph Convolutional Neural Network based Framework for Estimating Future Citations Count of Research Articles</b>
<a href="https://arxiv.org/abs/2104.04939">arxiv:2104.04939</a>
&#x1F4C8; 2 <br>
<p>Abdul Wahid, Rajesh Sharma, Chandra Sekhara Rao Annavarapu</p></summary>
<p>

**Abstract:** Scientific publications play a vital role in the career of a researcher. However, some articles become more popular than others among the research community and subsequently drive future research directions. One of the indicative signs of popular articles is the number of citations an article receives. The citation count, which is also the basis with various other metrics, such as the journal impact factor score, the $h$-index, is an essential measure for assessing a scientific paper's quality. In this work, we proposed a Graph Convolutional Network (GCN) based framework for estimating future research publication citations for both the short-term (1-year) and long-term (for 5-years and 10-years) duration. We have tested our proposed approach over the AMiner dataset, specifically on research articles from the computer science domain, consisting of more than 0.8 million articles.

</p>
</details>

<details><summary><b>Edge-Aware Image Compression using Deep Learning-based Super-resolution Network</b>
<a href="https://arxiv.org/abs/2104.04926">arxiv:2104.04926</a>
&#x1F4C8; 2 <br>
<p>Dipti Mishra, Satish Kumar Singh, Rajat Kumar Singh, Krishna Preetham</p></summary>
<p>

**Abstract:** We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works & (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%, 61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%, 10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%, and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively, compared to the same approaches. A similar type of improvement is achieved with other datasets also.

</p>
</details>

<details><summary><b>Advances on image interpolation based on ant colony algorithm</b>
<a href="https://arxiv.org/abs/2104.12863">arxiv:2104.12863</a>
&#x1F4C8; 1 <br>
<p>Olivier Rukundo, Hanqiang Cao</p></summary>
<p>

**Abstract:** This paper presents an advance on image interpolation based on ant colony algorithm (AACA) for high-resolution image scaling. The difference between the proposed algorithm and the previously proposed optimization of bilinear interpolation based on ant colony algorithm (OBACA) is that AACA uses global weighting, whereas OBACA uses a local weighting scheme. The strength of the proposed global weighting of the AACA algorithm depends on employing solely the pheromone matrix information present on any group of four adjacent pixels to decide which case deserves a maximum global weight value or not. Experimental results are further provided to show the higher performance of the proposed AACA algorithm with reference to the algorithms mentioned in this paper.

</p>
</details>

<details><summary><b>Prediction of Apophis Asteroid Flyby Optimal Trajectories and Data Fusion of Earth-Apophis Mission Launch Windows using Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2104.06249">arxiv:2104.06249</a>
&#x1F4C8; 1 <br>
<p>Manuel Ntumba, Saurabh Gore, Jean-Baptiste Awanyo</p></summary>
<p>

**Abstract:** In recent years, understanding asteroids has shifted from light worlds to geological worlds by exploring modern spacecraft and advanced radar and telescopic surveys. However, flyby in 2029 will be an opportunity to conduct an internal geophysical study and test the current hypothesis on the effects of tidal forces on asteroids. The Earth-Apophis mission is driven by additional factors and scientific goals beyond the unique opportunity for natural experimentation. However, the internal geophysical structures remain largely unknown. Understanding the strength and internal integrity of asteroids is not just a matter of scientific curiosity. It is a practical imperative to advance knowledge for planetary defense against the possibility of an asteroid impact. This paper presents a conceptual robotics system required for efficiency at every stage from entry to post-landing and for asteroid monitoring. In short, asteroid surveillance missions are futuristic frontiers, with the potential for technological growth that could revolutionize space exploration. Advanced space technologies and robotic systems are needed to minimize risk and prepare these technologies for future missions. A neural network model is implemented to track and predict asteroids' orbits. Advanced algorithms are also needed to numerically predict orbital events to minimize error

</p>
</details>

<details><summary><b>QZNs: Quantum Z-numbers</b>
<a href="https://arxiv.org/abs/2104.05190">arxiv:2104.05190</a>
&#x1F4C8; 1 <br>
<p>Jixiang Deng, Yong Deng</p></summary>
<p>

**Abstract:** Because of the efficiency of modeling fuzziness and vagueness, Z-number plays an important role in real practice. However, Z-numbers, defined in the real number field, lack the ability to process the quantum information in quantum environment. It is reasonable to generalize Z-number into its quantum counterpart. In this paper, we propose quantum Z-numbers (QZNs), which are the quantum generalization of Z-numbers. In addition, seven basic quantum fuzzy operations of QZNs and their corresponding quantum circuits are presented and illustrated by numerical examples. Moreover, based on QZNs, a novel quantum multi-attributes decision making (MADM) algorithm is proposed and applied in medical diagnosis. The results show that, with the help of quantum computation, the proposed algorithm can make diagnoses correctly and efficiently.

</p>
</details>

<details><summary><b>Automated Mechanism Design for Classification with Partial Verification</b>
<a href="https://arxiv.org/abs/2104.05182">arxiv:2104.05182</a>
&#x1F4C8; 1 <br>
<p>Hanrui Zhang, Yu Cheng, Vincent Conitzer</p></summary>
<p>

**Abstract:** We study the problem of automated mechanism design with partial verification, where each type can (mis)report only a restricted set of types (rather than any other type), induced by the principal's limited verification power. We prove hardness results when the revelation principle does not necessarily hold, as well as when types have even minimally different preferences. In light of these hardness results, we focus on truthful mechanisms in the setting where all types share the same preference over outcomes, which is motivated by applications in, e.g., strategic classification. We present a number of algorithmic and structural results, including an efficient algorithm for finding optimal deterministic truthful mechanisms, which also implies a faster algorithm for finding optimal randomized truthful mechanisms via a characterization based on convexity. We then consider a more general setting, where the principal's cost is a function of the combination of outcomes assigned to each type. In particular, we focus on the case where the cost function is submodular, and give generalizations of essentially all our results in the classical setting where the cost function is additive. Our results provide a relatively complete picture for automated mechanism design with partial verification.

</p>
</details>

<details><summary><b>A Non-Negative Matrix Factorization Game</b>
<a href="https://arxiv.org/abs/2104.05069">arxiv:2104.05069</a>
&#x1F4C8; 1 <br>
<p>Satpreet H. Singh</p></summary>
<p>

**Abstract:** We present a novel game-theoretic formulation of Non-Negative Matrix Factorization (NNMF), a popular data-analysis method with many scientific and engineering applications. The game-theoretic formulation is shown to have favorable scaling and parallelization properties, while retaining reconstruction and convergence performance comparable to the traditional Multiplicative Updates algorithm.

</p>
</details>

<details><summary><b>Dynamic Modeling of User Preferences for Stable Recommendations</b>
<a href="https://arxiv.org/abs/2104.05047">arxiv:2104.05047</a>
&#x1F4C8; 1 <br>
<p>Oluwafemi Olaleke, Ivan Oseledets, Evgeny Frolov</p></summary>
<p>

**Abstract:** In domains where users tend to develop long-term preferences that do not change too frequently, the stability of recommendations is an important factor of the perceived quality of a recommender system. In such cases, unstable recommendations may lead to poor personalization experience and distrust, driving users away from a recommendation service. We propose an incremental learning scheme that mitigates such problems through the dynamic modeling approach. It incorporates a generalized matrix form of a partial differential equation integrator that yields a dynamic low-rank approximation of time-dependent matrices representing user preferences. The scheme allows extending the famous PureSVD approach to time-aware settings and significantly improves its stability without sacrificing the accuracy in standard top-$n$ recommendations tasks.

</p>
</details>

<details><summary><b>Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</b>
<a href="https://arxiv.org/abs/2104.05158">arxiv:2104.05158</a>
&#x1F4C8; 0 <br>
<p>Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng</p></summary>
<p>

**Abstract:** Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.

</p>
</details>

<details><summary><b>Machine Learning Approach to Uncovering Residential Energy Consumption Patterns Based on Socioeconomic and Smart Meter Data</b>
<a href="https://arxiv.org/abs/2104.05154">arxiv:2104.05154</a>
&#x1F4C8; 0 <br>
<p>Wenjun Tang, Hao Wang, Xian-Long Lee, Hong-Tzer Yang</p></summary>
<p>

**Abstract:** The smart meter data analysis contributes to better planning and operations for the power system. This study aims to identify the drivers of residential energy consumption patterns from the socioeconomic perspective based on the consumption and demographic data using machine learning. We model consumption patterns by representative loads and reveal the relationship between load patterns and socioeconomic characteristics. Specifically, we analyze the real-world smart meter data and extract load patterns by clustering in a robust way. We further identify the influencing socioeconomic attributes on load patterns to improve our method's interpretability. The relationship between consumers' load patterns and selected socioeconomic features is characterized via machine learning models. The findings are as follows. (1) Twelve load clusters, consisting of six for weekdays and six for weekends, exhibit a diverse pattern of lifestyle and a difference between weekdays and weekends. (2) Among various socioeconomic features, age and education level are suggested to influence the load patterns. (3) Our proposed analytical model using feature selection and machine learning is proved to be more effective than XGBoost and conventional neural network model in mapping the relationship between load patterns and socioeconomic features.

</p>
</details>


[Next Page]({{ '/2021/04/10/2021.04.10.html' | relative_url }})
