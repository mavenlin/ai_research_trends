## Summary for 2021-08-05, created on 2021-12-18


<details><summary><b>Sketch Your Own GAN</b>
<a href="https://arxiv.org/abs/2108.02774">arxiv:2108.02774</a>
&#x1F4C8; 254 <br>
<p>Sheng-Yu Wang, David Bau, Jun-Yan Zhu</p></summary>
<p>

**Abstract:** Can a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model's output to match the user sketches through a cross-domain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model's diversity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing.

</p>
</details>

<details><summary><b>Video Contrastive Learning with Global Context</b>
<a href="https://arxiv.org/abs/2108.02722">arxiv:2108.02722</a>
&#x1F4C8; 55 <br>
<p>Haofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe, Sören Schwertfeger, Cyrill Stachniss, Mu Li</p></summary>
<p>

**Abstract:** Contrastive learning has revolutionized self-supervised image representation learning field, and recently been adapted to video domain. One of the greatest advantages of contrastive learning is that it allows us to flexibly define powerful loss objectives as long as we can find a reasonable way to formulate positive and negative samples to contrast. However, existing approaches rely heavily on the short-range spatiotemporal salience to form clip-level contrastive signals, thus limit themselves from using global context. In this paper, we propose a new video-level contrastive learning method based on segments to formulate positive pairs. Our formulation is able to capture global context in a video, thus robust to temporal content change. We also incorporate a temporal order regularization term to enforce the inherent sequential structure of videos. Extensive experiments show that our video-level contrastive learning framework (VCLR) is able to outperform previous state-of-the-arts on five video datasets for downstream action classification, action localization and video retrieval. Code is available at https://github.com/amazon-research/video-contrastive-learning.

</p>
</details>

<details><summary><b>Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System</b>
<a href="https://arxiv.org/abs/2108.02776">arxiv:2108.02776</a>
&#x1F4C8; 45 <br>
<p>Yukiya Hono, Kei Hashimoto, Keiichiro Oura, Yoshihiko Nankaku, Keiichi Tokuda</p></summary>
<p>

**Abstract:** This paper presents Sinsy, a deep neural network (DNN)-based singing voice synthesis (SVS) system. In recent years, DNNs have been utilized in statistical parametric SVS systems, and DNN-based SVS systems have demonstrated better performance than conventional hidden Markov model-based ones. SVS systems are required to synthesize a singing voice with pitch and timing that strictly follow a given musical score. Additionally, singing expressions that are not described on the musical score, such as vibrato and timing fluctuations, should be reproduced. The proposed system is composed of four modules: a time-lag model, a duration model, an acoustic model, and a vocoder, and singing voices can be synthesized taking these characteristics of singing voices into account. To better model a singing voice, the proposed system incorporates improved approaches to modeling pitch and vibrato and better training criteria into the acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive neural vocoder with robustness for the pitch, into our systems to generate a high-fidelity singing voice waveform. Moreover, we propose automatic pitch correction techniques for DNN-based SVS to synthesize singing voices with correct pitch even if the training data has out-of-tune phrases. Experimental results show our system can synthesize a singing voice with better timing, more natural vibrato, and correct pitch, and it can achieve better mean opinion scores in subjective evaluation tests.

</p>
</details>

<details><summary><b>Unifying Nonlocal Blocks for Neural Networks</b>
<a href="https://arxiv.org/abs/2108.02451">arxiv:2108.02451</a>
&#x1F4C8; 21 <br>
<p>Lei Zhu, Qi She, Duo Li, Yanye Lu, Xuejing Kang, Jie Hu, Changhu Wang</p></summary>
<p>

**Abstract:** The nonlocal-based blocks are designed for capturing long-range spatial-temporal dependencies in computer vision tasks. Although having shown excellent performance, they still lack the mechanism to encode the rich, structured information among elements in an image or video. In this paper, to theoretically analyze the property of these nonlocal-based blocks, we provide a new perspective to interpret them, where we view them as a set of graph filters generated on a fully-connected graph. Specifically, when choosing the Chebyshev graph filter, a unified formulation can be derived for explaining and analyzing the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage, double attention block). Furthermore, by concerning the property of spectral, we propose an efficient and robust spectral nonlocal block, which can be more robust and flexible to catch long-range dependencies when inserted into deep neural networks than the existing nonlocal blocks. Experimental results demonstrate the clear-cut improvements and practical applicabilities of our method on image classification, action recognition, semantic segmentation, and person re-identification tasks.

</p>
</details>

<details><summary><b>Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents</b>
<a href="https://arxiv.org/abs/2108.02899">arxiv:2108.02899</a>
&#x1F4C8; 15 <br>
<p>Amit Gupte, Alexey Romanov, Sahitya Mantravadi, Dalitso Banda, Jianjie Liu, Raza Khan, Lakshmanan Ramu Meenal, Benjamin Han, Soundar Srinivasan</p></summary>
<p>

**Abstract:** Document digitization is essential for the digital transformation of our societies, yet a crucial step in the process, Optical Character Recognition (OCR), is still not perfect. Even commercial OCR systems can produce questionable output depending on the fidelity of the scanned documents. In this paper, we demonstrate an effective framework for mitigating OCR errors for any downstream NLP task, using Named Entity Recognition (NER) as an example. We first address the data scarcity problem for model training by constructing a document synthesis pipeline, generating realistic but degraded data with NER labels. We measure the NER accuracy drop at various degradation levels and show that a text restoration model, trained on the degraded data, significantly closes the NER accuracy gaps caused by OCR errors, including on an out-of-domain dataset. For the benefit of the community, we have made the document synthesis pipeline available as an open-source project.

</p>
</details>

<details><summary><b>Beyond No Regret: Instance-Dependent PAC Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2108.02717">arxiv:2108.02717</a>
&#x1F4C8; 10 <br>
<p>Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson</p></summary>
<p>

**Abstract:** The theory of reinforcement learning has focused on two fundamental problems: achieving low regret, and identifying $ε$-optimal policies. While a simple reduction allows one to apply a low-regret algorithm to obtain an $ε$-optimal policy and achieve the worst-case optimal rate, it is unknown whether low-regret algorithms can obtain the instance-optimal rate for policy identification. We show that this is not possible -- there exists a fundamental tradeoff between achieving low regret and identifying an $ε$-optimal policy at the instance-optimal rate.
  Motivated by our negative finding, we propose a new measure of instance-dependent sample complexity for PAC tabular reinforcement learning which explicitly accounts for the attainable state visitation distributions in the underlying MDP. We then propose and analyze a novel, planning-based algorithm which attains this sample complexity -- yielding a complexity which scales with the suboptimality gaps and the ``reachability'' of a state. We show that our algorithm is nearly minimax optimal, and on several examples that our instance-dependent sample complexity offers significant improvements over worst-case bounds.

</p>
</details>

<details><summary><b>Is it Fake? News Disinformation Detection on South African News Websites</b>
<a href="https://arxiv.org/abs/2108.02941">arxiv:2108.02941</a>
&#x1F4C8; 9 <br>
<p>Harm de Wet, Vukosi Marivate</p></summary>
<p>

**Abstract:** Disinformation through fake news is an ongoing problem in our society and has become easily spread through social media. The most cost and time effective way to filter these large amounts of data is to use a combination of human and technical interventions to identify it. From a technical perspective, Natural Language Processing (NLP) is widely used in detecting fake news. Social media companies use NLP techniques to identify the fake news and warn their users, but fake news may still slip through undetected. It is especially a problem in more localised contexts (outside the United States of America). How do we adjust fake news detection systems to work better for local contexts such as in South Africa. In this work we investigate fake news detection on South African websites. We curate a dataset of South African fake news and then train detection models. We contrast this with using widely available fake news datasets (from mostly USA website). We also explore making the datasets more diverse by combining them and observe the differences in behaviour in writing between nations' fake news using interpretable machine learning.

</p>
</details>

<details><summary><b>MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion</b>
<a href="https://arxiv.org/abs/2108.02448">arxiv:2108.02448</a>
&#x1F4C8; 9 <br>
<p>Weihao Yuan, Rui Fan, Michael Yu Wang, Qifeng Chen</p></summary>
<p>

**Abstract:** We design a multiscopic vision system that utilizes a low-cost monocular RGB camera to acquire accurate depth estimation. Unlike multi-view stereo with images captured at unconstrained camera poses, the proposed system controls the motion of a camera to capture a sequence of images in horizontally or vertically aligned positions with the same parallax. In this system, we propose a new heuristic method and a robust learning-based method to fuse multiple cost volumes between the reference image and its surrounding images. To obtain training data, we build a synthetic dataset with multiscopic images. The experiments on the real-world Middlebury dataset and real robot demonstration show that our multiscopic vision system outperforms traditional two-frame stereo matching methods in depth estimation. Our code and dataset are available at https://sites.google.com/view/multiscopic.

</p>
</details>

<details><summary><b>Interpolation can hurt robust generalization even when there is no noise</b>
<a href="https://arxiv.org/abs/2108.02883">arxiv:2108.02883</a>
&#x1F4C8; 8 <br>
<p>Konstantin Donhauser, Alexandru Ţifrea, Michael Aerni, Reinhard Heckel, Fanny Yang</p></summary>
<p>

**Abstract:** Numerous recent works show that overparameterization implicitly reduces variance for min-norm interpolators and max-margin classifiers. These findings suggest that ridge regularization has vanishing benefits in high dimensions. We challenge this narrative by showing that, even in the absence of noise, avoiding interpolation through ridge regularization can significantly improve generalization. We prove this phenomenon for the robust risk of both linear regression and classification and hence provide the first theoretical result on robust overfitting.

</p>
</details>

<details><summary><b>WeChat Neural Machine Translation Systems for WMT21</b>
<a href="https://arxiv.org/abs/2108.02401">arxiv:2108.02401</a>
&#x1F4C8; 8 <br>
<p>Xianfeng Zeng, Yijin Liu, Ernan Li, Qiu Ran, Fandong Meng, Peng Li, Jinan Xu, Jie Zhou</p></summary>
<p>

**Abstract:** This paper introduces WeChat AI's participation in WMT 2021 shared news translation task on English->Chinese, English->Japanese, Japanese->English and English->German. Our systems are based on the Transformer (Vaswani et al., 2017) with several novel and effective variants. In our experiments, we employ data filtering, large-scale synthetic data generation (i.e., back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge transfer), advanced finetuning approaches, and boosted Self-BLEU based model ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 case-sensitive BLEU scores on English->Chinese, English->Japanese, Japanese->English and English->German, respectively. The BLEU scores of English->Chinese, English->Japanese and Japanese->English are the highest among all submissions, and that of English->German is the highest among all constrained submissions.

</p>
</details>

<details><summary><b>Adapting to Function Difficulty and Growth Conditions in Private Optimization</b>
<a href="https://arxiv.org/abs/2108.02391">arxiv:2108.02391</a>
&#x1F4C8; 8 <br>
<p>Hilal Asi, Daniel Levy, John Duchi</p></summary>
<p>

**Abstract:** We develop algorithms for private stochastic convex optimization that adapt to the hardness of the specific function we wish to optimize. While previous work provide worst-case bounds for arbitrary convex functions, it is often the case that the function at hand belongs to a smaller class that enjoys faster rates. Concretely, we show that for functions exhibiting $κ$-growth around the optimum, i.e., $f(x) \ge f(x^*) + λκ^{-1} \|x-x^*\|_2^κ$ for $κ> 1$, our algorithms improve upon the standard ${\sqrt{d}}/{n\varepsilon}$ privacy rate to the faster $({\sqrt{d}}/{n\varepsilon})^{\tfracκ{κ- 1}}$. Crucially, they achieve these rates without knowledge of the growth constant $κ$ of the function. Our algorithms build upon the inverse sensitivity mechanism, which adapts to instance difficulty (Asi & Duchi, 2020), and recent localization techniques in private optimization (Feldman et al., 2020). We complement our algorithms with matching lower bounds for these function classes and demonstrate that our adaptive algorithm is \emph{simultaneously} (minimax) optimal over all $κ\ge 1+c$ whenever $c = Θ(1)$.

</p>
</details>

<details><summary><b>Self-supervised optimization of random material microstructures in the small-data regime</b>
<a href="https://arxiv.org/abs/2108.02606">arxiv:2108.02606</a>
&#x1F4C8; 7 <br>
<p>Maximilian Rixner, Phaedon-Stelios Koutsourelakis</p></summary>
<p>

**Abstract:** While the forward and backward modeling of the process-structure-property chain has received a lot of attention from the materials community, fewer efforts have taken into consideration uncertainties. Those arise from a multitude of sources and their quantification and integration in the inversion process are essential in meeting the materials design objectives. The first contribution of this paper is a flexible, fully probabilistic formulation of such optimization problems that accounts for the uncertainty in the process-structure and structure-property linkages and enables the identification of optimal, high-dimensional, process parameters. We employ a probabilistic, data-driven surrogate for the structure-property link which expedites computations and enables handling of non-differential objectives. We couple this with a novel active learning strategy, i.e. a self-supervised collection of data, which significantly improves accuracy while requiring small amounts of training data. We demonstrate its efficacy in optimizing the mechanical and thermal properties of two-phase, random media but envision its applicability encompasses a wide variety of microstructure-sensitive design problems.

</p>
</details>

<details><summary><b>Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data</b>
<a href="https://arxiv.org/abs/2108.04085">arxiv:2108.04085</a>
&#x1F4C8; 6 <br>
<p>Christophe Bonneville, Christopher J. Earls</p></summary>
<p>

**Abstract:** Scientific machine learning has been successfully applied to inverse problems and PDE discovery in computational physics. One caveat concerning current methods is the need for large amounts of ("clean") data, in order to characterize the full system response and discover underlying physical models. Bayesian methods may be particularly promising for overcoming these challenges, as they are naturally less sensitive to the negative effects of sparse and noisy data. In this paper, we propose to use Bayesian neural networks (BNN) in order to: 1) Recover the full system states from measurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian Monte-Carlo to sample the posterior distribution of a deep and dense BNN, and show that it is possible to accurately capture physics of varying complexity, without overfitting. 2) Recover the parameters instantiating the underlying partial differential equation (PDE) governing the physical system. Using the trained BNN, as a surrogate of the system response, we generate datasets of derivatives that are potentially comprising the latent PDE governing the observed system and then perform a sequential threshold Bayesian linear regression (STBLR), between the successive derivatives in space and time, to recover the original PDE parameters. We take advantage of the confidence intervals within the BNN outputs, and introduce the spatial derivatives cumulative variance into the STBLR likelihood, to mitigate the influence of highly uncertain derivative data points; thus allowing for more accurate parameter discovery. We demonstrate our approach on a handful of example, in applied physics and non-linear dynamics.

</p>
</details>

<details><summary><b>Learning to Elect</b>
<a href="https://arxiv.org/abs/2108.02768">arxiv:2108.02768</a>
&#x1F4C8; 6 <br>
<p>Cem Anil, Xuchan Bao</p></summary>
<p>

**Abstract:** Voting systems have a wide range of applications including recommender systems, web search, product design and elections. Limited by the lack of general-purpose analytical tools, it is difficult to hand-engineer desirable voting rules for each use case. For this reason, it is appealing to automatically discover voting rules geared towards each scenario. In this paper, we show that set-input neural network architectures such as Set Transformers, fully-connected graph networks and DeepSets are both theoretically and empirically well-suited for learning voting rules. In particular, we show that these network models can not only mimic a number of existing voting rules to compelling accuracy -- both position-based (such as Plurality and Borda) and comparison-based (such as Kemeny, Copeland and Maximin) -- but also discover near-optimal voting rules that maximize different social welfare functions. Furthermore, the learned voting rules generalize well to different voter utility distributions and election sizes unseen during training.

</p>
</details>

<details><summary><b>A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities</b>
<a href="https://arxiv.org/abs/2108.02594">arxiv:2108.02594</a>
&#x1F4C8; 6 <br>
<p>Shanaka Perera, Virginia Aglietti, Theodoros Damoulas</p></summary>
<p>

**Abstract:** We study the problem of estimating potential revenue or demand at business facilities and understanding its generating mechanism. This problem arises in different fields such as operation research or urban science, and more generally, it is crucial for businesses' planning and decision making. We develop a Bayesian spatial interaction model, henceforth BSIM, which provides probabilistic predictions about revenues generated by a particular business location provided their features and the potential customers' characteristics in a given region. BSIM explicitly accounts for the competition among the competitive facilities through a probability value determined by evaluating a store-specific Gaussian distribution at a given customer location. We propose a scalable variational inference framework that, while being significantly faster than competing Markov Chain Monte Carlo inference schemes, exhibits comparable performances in terms of parameters identification and uncertainty quantification. We demonstrate the benefits of BSIM in various synthetic settings characterised by an increasing number of stores and customers. Finally, we construct a real-world, large spatial dataset for pub activities in London, UK, which includes over 1,500 pubs and 150,000 customer regions. We demonstrate how BSIM outperforms competing approaches on this large dataset in terms of prediction performances while providing results that are both interpretable and consistent with related indicators observed for the London region.

</p>
</details>

<details><summary><b>Finetuning Pretrained Transformers into Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2108.02446">arxiv:2108.02446</a>
&#x1F4C8; 6 <br>
<p>Seongmin Park, Jihwa Lee</p></summary>
<p>

**Abstract:** Text variational autoencoders (VAEs) are notorious for posterior collapse, a phenomenon where the model's decoder learns to ignore signals from the encoder. Because posterior collapse is known to be exacerbated by expressive decoders, Transformers have seen limited adoption as components of text VAEs. Existing studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et al., 2021) mitigate posterior collapse using massive pretraining, a technique unavailable to most of the research community without extensive computing resources. We present a simple two-phase training scheme to convert a sequence-to-sequence Transformer into a VAE with just finetuning. The resulting language model is competitive with massively pretrained Transformer-based VAEs in some internal metrics while falling short on others. To facilitate training we comprehensively explore the impact of common posterior collapse alleviation techniques in the literature. We release our code for reproducability.

</p>
</details>

<details><summary><b>Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles</b>
<a href="https://arxiv.org/abs/2108.02940">arxiv:2108.02940</a>
&#x1F4C8; 5 <br>
<p>Jindi Zhang, Yang Lou, Jianping Wang, Kui Wu, Kejie Lu, Xiaohua Jia</p></summary>
<p>

**Abstract:** In recent years, many deep learning models have been adopted in autonomous driving. At the same time, these models introduce new vulnerabilities that may compromise the safety of autonomous vehicles. Specifically, recent studies have demonstrated that adversarial attacks can cause a significant decline in detection precision of deep learning-based 3D object detection models. Although driving safety is the ultimate concern for autonomous driving, there is no comprehensive study on the linkage between the performance of deep learning models and the driving safety of autonomous vehicles under adversarial attacks. In this paper, we investigate the impact of two primary types of adversarial attacks, perturbation attacks and patch attacks, on the driving safety of vision-based autonomous vehicles rather than the detection precision of deep learning models. In particular, we consider two state-of-the-art models in vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving safety, we propose an end-to-end evaluation framework with a set of driving safety performance metrics. By analyzing the results of our extensive evaluation experiments, we find that (1) the attack's impact on the driving safety of autonomous vehicles and the attack's impact on the precision of 3D object detectors are decoupled, and (2) the DSGN model demonstrates stronger robustness to adversarial attacks than the Stereo R-CNN model. In addition, we further investigate the causes behind the two findings with an ablation study. The findings of this paper provide a new perspective to evaluate adversarial attacks and guide the selection of deep learning models in autonomous driving.

</p>
</details>

<details><summary><b>Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist</b>
<a href="https://arxiv.org/abs/2108.02904">arxiv:2108.02904</a>
&#x1F4C8; 5 <br>
<p>Alexander Trott, Sunil Srinivasa, Douwe van der Wal, Sebastien Haneuse, Stephan Zheng</p></summary>
<p>

**Abstract:** Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world.

</p>
</details>

<details><summary><b>Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene</b>
<a href="https://arxiv.org/abs/2108.02846">arxiv:2108.02846</a>
&#x1F4C8; 5 <br>
<p>Qi Wu, Cheng-Ju Wu, Yixin Zhu, Jungseock Joo</p></summary>
<p>

**Abstract:** Human-robot collaboration is an essential research topic in artificial intelligence (AI), enabling researchers to devise cognitive AI systems and affords an intuitive means for users to interact with the robot. Of note, communication plays a central role. To date, prior studies in embodied agent navigation have only demonstrated that human languages facilitate communication by instructions in natural languages. Nevertheless, a plethora of other forms of communication is left unexplored. In fact, human communication originated in gestures and oftentimes is delivered through multimodal cues, e.g. "go there" with a pointing gesture. To bridge the gap and fill in the missing dimension of communication in embodied agent navigation, we propose investigating the effects of using gestures as the communicative interface instead of verbal cues. Specifically, we develop a VR-based 3D simulation environment, named Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human player is placed in the same virtual scene and shepherds the artificial agent using only gestures. The agent is tasked to solve the navigation problem guided by natural gestures with unknown semantics; we do not use any predefined gestures due to the diversity and versatile nature of human gestures. We argue that learning the semantics of natural gestures is mutually beneficial to learning the navigation task--learn to communicate and communicate to learn. In a series of experiments, we demonstrate that human gesture cues, even without predefined semantics, improve the object-goal navigation for an embodied agent, outperforming various state-of-the-art methods.

</p>
</details>

<details><summary><b>Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina</b>
<a href="https://arxiv.org/abs/2108.02798">arxiv:2108.02798</a>
&#x1F4C8; 5 <br>
<p>Jan Kukačka, Anja Zenz, Marcel Kollovieh, Dominik Jüstel, Vasilis Ntziachristos</p></summary>
<p>

**Abstract:** Fundus photography is the primary method for retinal imaging and essential for diabetic retinopathy prevention. Automated segmentation of fundus photographs would improve the quality, capacity, and cost-effectiveness of eye care screening programs. However, current segmentation methods are not robust towards the diversity in imaging conditions and pathologies typical for real-world clinical applications. To overcome these limitations, we utilized contrastive self-supervised learning to exploit the large variety of unlabeled fundus images in the publicly available EyePACS dataset. We pre-trained an encoder of a U-Net, which we later fine-tuned on several retinal vessel and lesion segmentation datasets. We demonstrate for the first time that by using contrastive self-supervised learning, the pre-trained network can recognize blood vessels, optic disc, fovea, and various lesions without being provided any labels. Furthermore, when fine-tuned on a downstream blood vessel segmentation task, such pre-trained networks achieve state-of-the-art performance on images from different datasets. Additionally, the pre-training also leads to shorter training times and an improved few-shot performance on both blood vessel and lesion segmentation tasks. Altogether, our results showcase the benefits of contrastive self-supervised pre-training which can play a crucial role in real-world clinical applications requiring robust models able to adapt to new devices with only a few annotated samples.

</p>
</details>

<details><summary><b>VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search</b>
<a href="https://arxiv.org/abs/2108.02725">arxiv:2108.02725</a>
&#x1F4C8; 5 <br>
<p>Shaunak Mishra, Mikhail Kuznetsov, Gaurav Srivastava, Maxim Sviridenko</p></summary>
<p>

**Abstract:** Numerous online stock image libraries offer high quality yet copyright free images for use in marketing campaigns. To assist advertisers in navigating such third party libraries, we study the problem of automatically fetching relevant ad images given the ad text (via a short textual query for images). Motivated by our observations in logged data on ad image search queries (given ad text), we formulate a keyword extraction problem, where a keyword extracted from the ad text (or its augmented version) serves as the ad image query. In this context, we propose VisualTextRank: an unsupervised method to (i) augment input ad text using semantically similar ads, and (ii) extract the image query from the augmented ad text. VisualTextRank builds on prior work on graph based context extraction (biased TextRank in particular) by leveraging both the text and image of similar ads for better keyword extraction, and using advertiser category specific biasing with sentence-BERT embeddings. Using data collected from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search feature for onboarding advertisers, we demonstrate the superiority of VisualTextRank compared to competitive keyword extraction baselines (including an $11\%$ accuracy lift over biased TextRank). For the case when the stock image library is restricted to English queries, we show the effectiveness of VisualTextRank on multilingual ads (translated to English) while leveraging semantically similar English ads. Online tests with a simplified version of VisualTextRank led to a 28.7% increase in the usage of stock image search, and a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native ad platform.

</p>
</details>

<details><summary><b>Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images</b>
<a href="https://arxiv.org/abs/2108.02704">arxiv:2108.02704</a>
&#x1F4C8; 5 <br>
<p>Juan P. Vigueras-Guillén, Joan Lasenby, Frank Seeliger</p></summary>
<p>

**Abstract:** Regularization in convolutional neural networks (CNNs) is usually addressed with dropout layers. However, dropout is sometimes detrimental in the convolutional part of a CNN as it simply sets to zero a percentage of pixels in the feature maps, adding unrepresentative examples during training. Here, we propose a CNN layer that performs regularization by applying random rotations of reflections to a small percentage of feature maps after every convolutional layer. We prove how this concept is beneficial for images with orientational symmetries, such as in medical images, as it provides a certain degree of rotational invariance. We tested this method in two datasets, a patch-based set of histopathology images (PatchCamelyon) to perform classification using a generic DenseNet, and a set of specular microscopy images of the corneal endothelium to perform segmentation using a tailored U-net, improving the performance in both cases.

</p>
</details>

<details><summary><b>Learning to Design and Construct Bridge without Blueprint</b>
<a href="https://arxiv.org/abs/2108.02439">arxiv:2108.02439</a>
&#x1F4C8; 5 <br>
<p>Yunfei Li, Tao Kong, Lei Li, Yifeng Li, Yi Wu</p></summary>
<p>

**Abstract:** Autonomous assembly has been a desired functionality of many intelligent robot systems. We study a new challenging assembly task, designing and constructing a bridge without a blueprint. In this task, the robot needs to first design a feasible bridge architecture for arbitrarily wide cliffs and then manipulate the blocks reliably to construct a stable bridge according to the proposed design. In this paper, we propose a bi-level approach to tackle this task. At the high level, the system learns a bridge blueprint policy in a physical simulator using deep reinforcement learning and curriculum learning. A policy is represented as an attention-based neural network with object-centric input, which enables generalization to different numbers of blocks and cliff widths. For low-level control, we implement a motion-planning-based policy for real-robot motion control, which can be directly combined with a trained blueprint policy for real-world bridge construction without tuning. In our field study, our bi-level robot system demonstrates the capability of manipulating blocks to construct a diverse set of bridges with different architectures.

</p>
</details>

<details><summary><b>Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation</b>
<a href="https://arxiv.org/abs/2108.02425">arxiv:2108.02425</a>
&#x1F4C8; 5 <br>
<p>Yiming Li, Tao Kong, Ruihang Chu, Yifeng Li, Peng Wang, Lei Li</p></summary>
<p>

**Abstract:** Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl

</p>
</details>

<details><summary><b>PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction</b>
<a href="https://arxiv.org/abs/2108.02424">arxiv:2108.02424</a>
&#x1F4C8; 5 <br>
<p>Tiange Wang, Zijun Zhang, Kwok-Leung Tsui</p></summary>
<p>

**Abstract:** Accurate forecasting of traffic conditions is critical for improving safety, stability, and efficiency of a city transportation system. In reality, it is challenging to produce accurate traffic forecasts due to the complex and dynamic spatiotemporal correlations. Most existing works only consider partial characteristics and features of traffic data, and result in unsatisfactory performances on modeling and forecasting. In this paper, we propose a periodic spatial-temporal deep neural network (PSTN) with three pivotal modules to improve the forecasting performance of traffic conditions through a novel integration of three types of information. First, the historical traffic information is folded and fed into a module consisting of a graph convolutional network and a temporal convolutional network. Second, the recent traffic information together with the historical output passes through the second module consisting of a graph convolutional network and a gated recurrent unit framework. Finally, a multi-layer perceptron is applied to process the auxiliary road attributes and output the final predictions. Experimental results on two publicly accessible real-world urban traffic data sets show that the proposed PSTN outperforms the state-of-the-art benchmarks by significant margins for short-term traffic conditions forecasting

</p>
</details>

<details><summary><b>Online Model-Free Reinforcement Learning for the Automatic Control of a Flexible Wing Aircraft</b>
<a href="https://arxiv.org/abs/2108.02393">arxiv:2108.02393</a>
&#x1F4C8; 5 <br>
<p>Mohammed Abouheaf, Wail Gueaieb, Frank Lewis</p></summary>
<p>

**Abstract:** The control problem of the flexible wing aircraft is challenging due to the prevailing and high nonlinear deformations in the flexible wing system. This urged for new control mechanisms that are robust to the real-time variations in the wing's aerodynamics. An online control mechanism based on a value iteration reinforcement learning process is developed for flexible wing aerial structures. It employs a model-free control policy framework and a guaranteed convergent adaptive learning architecture to solve the system's Bellman optimality equation. A Riccati equation is derived and shown to be equivalent to solving the underlying Bellman equation. The online reinforcement learning solution is implemented using means of an adaptive-critic mechanism. The controller is proven to be asymptotically stable in the Lyapunov sense. It is assessed through computer simulations and its superior performance is demonstrated on two scenarios under different operating conditions.

</p>
</details>

<details><summary><b>TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding</b>
<a href="https://arxiv.org/abs/2108.02388">arxiv:2108.02388</a>
&#x1F4C8; 5 <br>
<p>Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu</p></summary>
<p>

**Abstract:** Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task.

</p>
</details>

<details><summary><b>Predicting Status of Pre and Post M&A Deals Using Machine Learning and Deep Learning Techniques</b>
<a href="https://arxiv.org/abs/2110.09315">arxiv:2110.09315</a>
&#x1F4C8; 4 <br>
<p>Tugce Karatas, Ali Hirsa</p></summary>
<p>

**Abstract:** Risk arbitrage or merger arbitrage is a well-known investment strategy that speculates on the success of M&A deals. Prediction of the deal status in advance is of great importance for risk arbitrageurs. If a deal is mistakenly classified as a completed deal, then enormous cost can be incurred as a result of investing in target company shares. On the contrary, risk arbitrageurs may lose the opportunity of making profit. In this paper, we present an ML and DL based methodology for takeover success prediction problem. We initially apply various ML techniques for data preprocessing such as kNN for data imputation, PCA for lower dimensional representation of numerical variables, MCA for categorical variables, and LSTM autoencoder for sentiment scores. We experiment with different cost functions, different evaluation metrics, and oversampling techniques to address class imbalance in our dataset. We then implement feedforward neural networks to predict the success of the deal status. Our preliminary results indicate that our methodology outperforms the benchmark models such as logit and weighted logit models. We also integrate sentiment scores into our methodology using different model architectures, but our preliminary results show that the performance is not changing much compared to the simple FFNN framework. We will explore different architectures and employ a thorough hyperparameter tuning for sentiment scores as a future work.

</p>
</details>

<details><summary><b>COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging</b>
<a href="https://arxiv.org/abs/2108.03131">arxiv:2108.03131</a>
&#x1F4C8; 4 <br>
<p>Alexander MacLean, Saad Abbasi, Ashkan Ebadi, Andy Zhao, Maya Pavlova, Hayden Gunraj, Pengcheng Xi, Sonny Kohli, Alexander Wong</p></summary>
<p>

**Abstract:** The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of life globally, and a critical factor in mitigating its effects is screening individuals for infections, thereby allowing for both proper treatment for those individuals as well as action to be taken to prevent further spread of the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a screening tool as it is a much cheaper and easier to apply imaging modality than others that are traditionally used for pulmonary examinations, namely chest x-ray and computed tomography. Given the scarcity of expert radiologists for interpreting POCUS examinations in many highly affected regions around the world, low-cost deep learning-driven clinical decision support solutions can have a large impact during the on-going pandemic. Motivated by this, we introduce COVID-Net US, a highly efficient, self-attention deep convolutional neural network design tailored for COVID-19 screening from lung POCUS images. Experimental results show that the proposed COVID-Net US can achieve an AUC of over 0.98 while achieving 353X lower architectural complexity, 62X lower computational complexity, and 14.3X faster inference times on a Raspberry Pi. Clinical validation was also conducted, where select cases were reviewed and reported on by a practicing clinician (20 years of clinical practice) specializing in intensive care (ICU) and 15 years of expertise in POCUS interpretation. To advocate affordable healthcare and artificial intelligence for resource-constrained environments, we have made COVID-Net US open source and publicly available as part of the COVID-Net open source initiative.

</p>
</details>

<details><summary><b>Hate Speech Detection in Roman Urdu</b>
<a href="https://arxiv.org/abs/2108.02830">arxiv:2108.02830</a>
&#x1F4C8; 4 <br>
<p>Moin Khan, Khurram Shahzad, Kamran Malik</p></summary>
<p>

**Abstract:** Hate speech is a specific type of controversial content that is widely legislated as a crime that must be identified and blocked. However, due to the sheer volume and velocity of the Twitter data stream, hate speech detection cannot be performed manually. To address this issue, several studies have been conducted for hate speech detection in European languages, whereas little attention has been paid to low-resource South Asian languages, making the social media vulnerable for millions of users. In particular, to the best of our knowledge, no study has been conducted for hate speech detection in Roman Urdu text, which is widely used in the sub-continent. In this study, we have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, we have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, we have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets.

</p>
</details>

<details><summary><b>Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications</b>
<a href="https://arxiv.org/abs/2108.02818">arxiv:2108.02818</a>
&#x1F4C8; 4 <br>
<p>Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, Miles Brundage</p></summary>
<p>

**Abstract:** Recently, there have been breakthroughs in computer vision ("CV") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.

</p>
</details>

<details><summary><b>Quantum Topological Data Analysis with Linear Depth and Exponential Speedup</b>
<a href="https://arxiv.org/abs/2108.02811">arxiv:2108.02811</a>
&#x1F4C8; 4 <br>
<p>Shashanka Ubaru, Ismail Yunus Akhalwaya, Mark S. Squillante, Kenneth L. Clarkson, Lior Horesh</p></summary>
<p>

**Abstract:** Quantum computing offers the potential of exponential speedups for certain classical computations. Over the last decade, many quantum machine learning (QML) algorithms have been proposed as candidates for such exponential improvements. However, two issues unravel the hope of exponential speedup for some of these QML algorithms: the data-loading problem and, more recently, the stunning dequantization results of Tang et al. A third issue, namely the fault-tolerance requirements of most QML algorithms, has further hindered their practical realization. The quantum topological data analysis (QTDA) algorithm of Lloyd, Garnerone and Zanardi was one of the first QML algorithms that convincingly offered an expected exponential speedup. From the outset, it did not suffer from the data-loading problem. A recent result has also shown that the generalized problem solved by this algorithm is likely classically intractable, and would therefore be immune to any dequantization efforts. However, the QTDA algorithm of Lloyd et~al. has a time complexity of $O(n^4/(ε^2 δ))$ (where $n$ is the number of data points, $ε$ is the error tolerance, and $δ$ is the smallest nonzero eigenvalue of the restricted Laplacian) and requires fault-tolerant quantum computing, which has not yet been achieved. In this paper, we completely overhaul the QTDA algorithm to achieve an improved exponential speedup and depth complexity of $O(n\log(1/(δε)))$. Our approach includes three key innovations: (a) an efficient realization of the combinatorial Laplacian as a sum of Pauli operators; (b) a quantum rejection sampling approach to restrict the superposition to the simplices in the complex; and (c) a stochastic rank estimation method to estimate the Betti numbers. We present a theoretical error analysis, and the circuit and computational time and depth complexities for Betti number estimation.

</p>
</details>

<details><summary><b>A Low Rank Promoting Prior for Unsupervised Contrastive Learning</b>
<a href="https://arxiv.org/abs/2108.02696">arxiv:2108.02696</a>
&#x1F4C8; 4 <br>
<p>Yu Wang, Jingyang Lin, Qi Cai, Yingwei Pan, Ting Yao, Hongyang Chao, Tao Mei</p></summary>
<p>

**Abstract:** Unsupervised learning is just at a tipping point where it could really take off. Among these approaches, contrastive learning has seen tremendous progress and led to state-of-the-art performance. In this paper, we construct a novel probabilistic graphical model that effectively incorporates the low rank promoting prior into the framework of contrastive learning, referred to as LORAC. In contrast to the existing conventional self-supervised approaches that only considers independent learning, our hypothesis explicitly requires that all the samples belonging to the same instance class lie on the same subspace with small dimension. This heuristic poses particular joint learning constraints to reduce the degree of freedom of the problem during the search of the optimal network parameterization. Most importantly, we argue that the low rank prior employed here is not unique, and many different priors can be invoked in a similar probabilistic way, corresponding to different hypotheses about underlying truth behind the contrastive features. Empirical evidences show that the proposed algorithm clearly surpasses the state-of-the-art approaches on multiple benchmarks, including image classification, object detection, instance segmentation and keypoint detection.

</p>
</details>

<details><summary><b>Redesigning Fully Convolutional DenseUNets for Large Histopathology Images</b>
<a href="https://arxiv.org/abs/2108.02676">arxiv:2108.02676</a>
&#x1F4C8; 4 <br>
<p>Juan P. Vigueras-Guillén, Joan Lasenby, Frank Seeliger</p></summary>
<p>

**Abstract:** The automated segmentation of cancer tissue in histopathology images can help clinicians to detect, diagnose, and analyze such disease. Different from other natural images used in many convolutional networks for benchmark, histopathology images can be extremely large, and the cancerous patterns can reach beyond 1000 pixels. Therefore, the well-known networks in the literature were never conceived to handle these peculiarities. In this work, we propose a Fully Convolutional DenseUNet that is particularly designed to solve histopathology problems. We evaluated our network in two public pathology datasets published as challenges in the recent MICCAI 2019: binary segmentation in colon cancer images (DigestPath2019), and multi-class segmentation in prostate cancer images (Gleason2019), achieving similar and better results than the winners of the challenges, respectively. Furthermore, we discussed some good practices in the training setup to yield the best performance and the main challenges in these histopathology datasets.

</p>
</details>

<details><summary><b>A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective</b>
<a href="https://arxiv.org/abs/2108.02656">arxiv:2108.02656</a>
&#x1F4C8; 4 <br>
<p>Wei-Wen Hsu, Yongfang Wu, Chang Hao, Yu-Ling Hou, Xiang Gao, Yun Shao, Xueli Zhang, Tao He, Yanhong Tai</p></summary>
<p>

**Abstract:** Objective: We develop a computer-aided diagnosis (CAD) system using deep learning approaches for lesion detection and classification on whole-slide images (WSIs) with breast cancer. The deep features being distinguishing in classification from the convolutional neural networks (CNN) are demonstrated in this study to provide comprehensive interpretability for the proposed CAD system using pathological knowledge. Methods: In the experiment, a total of 186 slides of WSIs were collected and classified into three categories: Non-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma (IDC). Instead of conducting pixel-wise classification into three classes directly, we designed a hierarchical framework with the multi-view scheme that performs lesion detection for region proposal at higher magnification first and then conducts lesion classification at lower magnification for each detected lesion. Results: The slide-level accuracy rate for three-category classification reaches 90.8% (99/109) through 5-fold cross-validation and achieves 94.8% (73/77) on the testing set. The experimental results show that the morphological characteristics and co-occurrence properties learned by the deep learning models for lesion classification are accordant with the clinical rules in diagnosis. Conclusion: The pathological interpretability of the deep features not only enhances the reliability of the proposed CAD system to gain acceptance from medical specialists, but also facilitates the development of deep learning frameworks for various tasks in pathology. Significance: This paper presents a CAD system for pathological image analysis, which fills the clinical requirements and can be accepted by medical specialists with providing its interpretability from the pathological perspective.

</p>
</details>

<details><summary><b>Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration</b>
<a href="https://arxiv.org/abs/2108.02613">arxiv:2108.02613</a>
&#x1F4C8; 4 <br>
<p>Haobo Jiang, Jin Xie, Jianjun Qian, Jian Yang</p></summary>
<p>

**Abstract:** Point cloud registration is a fundamental problem in 3D computer vision. In this paper, we cast point cloud registration into a planning problem in reinforcement learning, which can seek the transformation between the source and target point clouds through trial and error. By modeling the point cloud registration process as a Markov decision process (MDP), we develop a latent dynamic model of point clouds, consisting of a transformation network and evaluation network. The transformation network aims to predict the new transformed feature of the point cloud after performing a rigid transformation (i.e., action) on it while the evaluation network aims to predict the alignment precision between the transformed source point cloud and target point cloud as the reward signal. Once the dynamic model of the point cloud is trained, we employ the cross-entropy method (CEM) to iteratively update the planning policy by maximizing the rewards in the point cloud registration process. Thus, the optimal policy, i.e., the transformation between the source and target point clouds, can be obtained via gradually narrowing the search space of the transformation. Experimental results on ModelNet40 and 7Scene benchmark datasets demonstrate that our method can yield good registration performance in an unsupervised manner.

</p>
</details>

<details><summary><b>Shape Modeling with Spline Partitions</b>
<a href="https://arxiv.org/abs/2108.02507">arxiv:2108.02507</a>
&#x1F4C8; 4 <br>
<p>Shufei Ge, Shijia Wang, Lloyd Elliott</p></summary>
<p>

**Abstract:** Shape modelling (with methods that output shapes) is a new and important task in Bayesian nonparametrics and bioinformatics. In this work, we focus on Bayesian nonparametric methods for capturing shapes by partitioning a space using curves. In related work, the classical Mondrian process is used to partition spaces recursively with axis-aligned cuts, and is widely applied in multi-dimensional and relational data. The Mondrian process outputs hyper-rectangles. Recently, the random tessellation process was introduced as a generalization of the Mondrian process, partitioning a domain with non-axis aligned cuts in an arbitrary dimensional space, and outputting polytopes. Motivated by these processes, in this work, we propose a novel parallelized Bayesian nonparametric approach to partition a domain with curves, enabling complex data-shapes to be acquired. We apply our method to HIV-1-infected human macrophage image dataset, and also simulated datasets sets to illustrate our approach. We compare to support vector machines, random forests and state-of-the-art computer vision methods such as simple linear iterative clustering super pixel image segmentation. We develop an R package that is available at \url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.

</p>
</details>

<details><summary><b>Imperceptible Adversarial Examples by Spatial Chroma-Shift</b>
<a href="https://arxiv.org/abs/2108.02502">arxiv:2108.02502</a>
&#x1F4C8; 4 <br>
<p>Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu, Alptekin Temizel</p></summary>
<p>

**Abstract:** Deep Neural Networks have been shown to be vulnerable to various kinds of adversarial perturbations. In addition to widely studied additive noise based perturbations, adversarial examples can also be created by applying a per pixel spatial drift on input images. While spatial transformation based adversarial examples look more natural to human observers due to absence of additive noise, they still possess visible distortions caused by spatial transformations. Since the human vision is more sensitive to the distortions in the luminance compared to those in chrominance channels, which is one of the main ideas behind the lossy visual multimedia compression standards, we propose a spatial transformation based perturbation method to create adversarial examples by only modifying the color components of an input image. While having competitive fooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets, examples created with the proposed method have better scores with regards to various perceptual quality metrics. Human visual perception studies validate that the examples are more natural looking and often indistinguishable from their original counterparts.

</p>
</details>

<details><summary><b>AutoLL: Automatic Linear Layout of Graphs based on Deep Neural Network</b>
<a href="https://arxiv.org/abs/2108.02431">arxiv:2108.02431</a>
&#x1F4C8; 4 <br>
<p>Chihiro Watanabe, Taiji Suzuki</p></summary>
<p>

**Abstract:** Linear layouts are a graph visualization method that can be used to capture an entry pattern in an adjacency matrix of a given graph. By reordering the node indices of the original adjacency matrix, linear layouts provide knowledge of latent graph structures. Conventional linear layout methods commonly aim to find an optimal reordering solution based on predefined features of a given matrix and loss function. However, prior knowledge of the appropriate features to use or structural patterns in a given adjacency matrix is not always available. In such a case, performing the reordering based on data-driven feature extraction without assuming a specific structure in an adjacency matrix is preferable. Recently, a neural-network-based matrix reordering method called DeepTMR has been proposed to perform this function. However, it is limited to a two-mode reordering (i.e., the rows and columns are reordered separately) and it cannot be applied in the one-mode setting (i.e., the same node order is used for reordering both rows and columns), owing to the characteristics of its model architecture. In this study, we extend DeepTMR and propose a new one-mode linear layout method referred to as AutoLL. We developed two types of neural network models, AutoLL-D and AutoLL-U, for reordering directed and undirected networks, respectively. To perform one-mode reordering, these AutoLL models have specific encoder architectures, which extract node features from an observed adjacency matrix. We conducted both qualitative and quantitative evaluations of the proposed approach, and the experimental results demonstrate its effectiveness.

</p>
</details>

<details><summary><b>Spotify Danceability and Popularity Analysis using SAP</b>
<a href="https://arxiv.org/abs/2108.02370">arxiv:2108.02370</a>
&#x1F4C8; 4 <br>
<p>Virginia Ochi, Ricardo Estrada, Teezal Gaji, Wendy Gadea, Emily Duong</p></summary>
<p>

**Abstract:** Our analysis reviews and visualizes the audio features and popularity of songs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally sourced from Spotify API, consists of multiple Excel files containing information relevant to our visualization and regression analysis. The exercise seeks to determine the connection between the popularity of the songs and the danceability. Insights to be included and factored as part of our analysis include song energy, valence, BPM, release date, and year.

</p>
</details>

<details><summary><b>An Interpretable Probabilistic Model for Short-Term Solar Power Forecasting Using Natural Gradient Boosting</b>
<a href="https://arxiv.org/abs/2108.04058">arxiv:2108.04058</a>
&#x1F4C8; 3 <br>
<p>Georgios Mitrentsis, Hendrik Lens</p></summary>
<p>

**Abstract:** The stochastic nature of photovoltaic (PV) power has led both academia and industry to a large amount of research work aiming at the development of accurate PV power forecasting models. However, most of those models are based on machine learning algorithms and are considered as black boxes which do not provide any insight or explanation about their predictions. Therefore, their direct implementation in environments, where transparency is required, and the trust associated with their predictions may be questioned. To this end, we propose a two stage probabilistic forecasting framework able to generate highly accurate, reliable, and sharp forecasts yet offering full transparency on both the point forecasts and the prediction intervals (PIs). In the first stage, we exploit natural gradient boosting (NGBoost) for yielding probabilistic forecasts while in the second stage, we calculate the Shapley additive explanation (SHAP) values in order to fully understand why a prediction was made. To highlight the performance and the applicability of the proposed framework, real data from two PV parks located in Southern Germany are employed. Initially, the natural gradient boosting is thoroughly compared with two state-of-the-art algorithms, namely Gaussian process and lower upper bound estimation, in a wide range of forecasting metrics. Secondly, a detailed analysis of the model's complex nonlinear relationships and interaction effects between the various features is presented. The latter allows us to interpret the model, identify some learned physical properties, explain individual predictions, reduce the computational requirements for the training without jeopardizing the model accuracy, detect possible bugs, and gain trust in the model. Finally, we conclude that the model was able to develop nonlinear relationships following human logic and intuition based on learned physical properties.

</p>
</details>

<details><summary><b>Interpretable Visual Understanding with Cognitive Attention Network</b>
<a href="https://arxiv.org/abs/2108.02924">arxiv:2108.02924</a>
&#x1F4C8; 3 <br>
<p>Xuejiao Tang, Wenbin Zhang, Yi Yu, Kea Turner, Tyler Derr, Mengyu Wang, Eirini Ntoutsi</p></summary>
<p>

**Abstract:** While image understanding on recognition-level has achieved remarkable advancements, reliable visual scene understanding requires comprehensive image understanding on recognition-level but also cognition-level, which calls for exploiting the multi-source information as well as learning different levels of understanding and extensive commonsense knowledge. In this paper, we propose a novel Cognitive Attention Network (CAN) for visual commonsense reasoning to achieve interpretable visual understanding. Specifically, we first introduce an image-text fusion module to fuse information from images and text collectively. Second, a novel inference module is designed to encode commonsense among image, query and response. Extensive experiments on large-scale Visual Commonsense Reasoning (VCR) benchmark dataset demonstrate the effectiveness of our approach. The implementation is publicly available at https://github.com/tanjatang/CAN

</p>
</details>

<details><summary><b>A Data Augmented Approach to Transfer Learning for Covid-19 Detection</b>
<a href="https://arxiv.org/abs/2108.02870">arxiv:2108.02870</a>
&#x1F4C8; 3 <br>
<p>Shagufta Henna, Aparna Reji</p></summary>
<p>

**Abstract:** Covid-19 detection at an early stage can aid in an effective treatment and isolation plan to prevent its spread. Recently, transfer learning has been used for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major limitations inherent to these proposed methods is limited labeled dataset size that affects the reliability of Covid-19 diagnosis and disease progression. In this work, we demonstrate that how we can augment limited X-ray images data by using Contrast limited adaptive histogram equalization (CLAHE) to train the last layer of the pre-trained deep learning models to mitigate the bias of transfer learning for Covid-19 detection. We transfer learned various pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18, and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset. The experiment results reveal that the CLAHE-based augmentation to various pre-trained deep learning models significantly improves the model efficiency. The pre-trained VCG-16 model with CLAHEbased augmented images achieves a sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when trained on non-augmented data. Other models demonstrate a value of less than 60% when trained on non-augmented data. Our results reveal that the sample bias can negatively impact the performance of transfer learning which is significantly improved by using CLAHE-based augmentation.

</p>
</details>

<details><summary><b>3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning</b>
<a href="https://arxiv.org/abs/2108.02858">arxiv:2108.02858</a>
&#x1F4C8; 3 <br>
<p>Yue Sun, Zhuoming Huang, Honggang Zhang, Zhi Cao, Deqiang Xu</p></summary>
<p>

**Abstract:** mmWave radar has been shown as an effective sensing technique in low visibility, smoke, dusty, and dense fog environment. However tapping the potential of radar sensing to reconstruct 3D object shapes remains a great challenge, due to the characteristics of radar data such as sparsity, low resolution, specularity, high noise, and multi-path induced shadow reflections and artifacts. In this paper we propose 3D Reconstruction and Imaging via mmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D shape of an object in dense detailed point cloud format, based on sparse raw mmWave radar intensity data. The architecture consists of two back-to-back conditional GAN deep neural networks: the first generator network generates 2D depth images based on raw radar intensity data, and the second generator network outputs 3D point clouds based on the results of the first generator. The architecture exploits both convolutional neural network's convolutional operation (that extracts local structure neighborhood information) and the efficiency and detailed geometry capture capability of point clouds (other than costly voxelization of 3D space or distance fields). Our experiments have demonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its performance improvement over standard techniques.

</p>
</details>

<details><summary><b>GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena</b>
<a href="https://arxiv.org/abs/2108.02854">arxiv:2108.02854</a>
&#x1F4C8; 3 <br>
<p>Eva Vanmassenhove, Johanna Monti</p></summary>
<p>

**Abstract:** Languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked. These cross-linguistic differences can lead to ambiguities that are difficult to resolve, especially for sentence-level MT systems. The identification of ambiguity and its subsequent resolution is a challenging task for which currently there aren't any specific resources or challenge sets available. In this paper, we introduce gENder-IT, an English--Italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the English source side and multiple gender alternative translations, where needed, on the Italian target side.

</p>
</details>

<details><summary><b>Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting</b>
<a href="https://arxiv.org/abs/2108.02853">arxiv:2108.02853</a>
&#x1F4C8; 3 <br>
<p>Tugce Karatas, Federico Klinkert, Ali Hirsa</p></summary>
<p>

**Abstract:** Institutional investors have been increasing the allocation of the illiquid alternative assets such as private equity funds in their portfolios, yet there exists a very limited literature on cash flow forecasting of illiquid alternative assets. The net cash flow of private equity funds typically follow a J-curve pattern, however the timing and the size of the contributions and distributions depend on the investment opportunities. In this paper, we develop a benchmark model and present two novel approaches (direct vs. indirect) to predict the cash flows of private equity funds. We introduce a sliding window approach to apply on our cash flow data because different vintage year funds contain different lengths of cash flow information. We then pass the data to an LSTM/ GRU model to predict the future cash flows either directly or indirectly (based on the benchmark model). We further integrate macroeconomic indicators into our data, which allows us to consider the impact of market environment on cash flows and to apply stress testing. Our results indicate that the direct model is easier to implement compared to the benchmark model and the indirect model, but still the predicted cash flows align better with the actual cash flows. We also show that macroeconomic variables improve the performance of the direct model whereas the impact is not obvious on the indirect model.

</p>
</details>

<details><summary><b>Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning</b>
<a href="https://arxiv.org/abs/2108.02832">arxiv:2108.02832</a>
&#x1F4C8; 3 <br>
<p>Akash Gupta, Padmaja Jonnalagedda, Bir Bhanu, Amit K. Roy-Chowdhury</p></summary>
<p>

**Abstract:** Most of the existing works in supervised spatio-temporal video super-resolution (STVSR) heavily rely on a large-scale external dataset consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these methods make a prior assumption that the low-resolution video is obtained by down-scaling the high-resolution video using a known degradation kernel, which does not hold in practical settings. Another problem with these methods is that they cannot exploit instance-specific internal information of video at testing time. Recently, deep internal learning approaches have gained attention due to their ability to utilize the instance-specific statistics of a video. However, these methods have a large inference time as they require thousands of gradient updates to learn the intrinsic structure of the data. In this work, we presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as well as internal, information through meta-transfer learning and internal learning, respectively. Specifically, meta-learning is employed to obtain adaptive parameters, using a large-scale external dataset, that can adapt quickly to the novel condition (degradation model) of the given test video during the internal learning task, thereby exploiting external and internal information of a video for super-resolution. The model trained using our approach can quickly adapt to a specific video condition with only a few gradient updates, which reduces the inference time significantly. Extensive experiments on standard datasets demonstrate that our method performs favorably against various state-of-the-art approaches.

</p>
</details>

<details><summary><b>Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2108.02743">arxiv:2108.02743</a>
&#x1F4C8; 3 <br>
<p>Canyu Yang, Dennis Eschweiler, Johannes Stegmaier</p></summary>
<p>

**Abstract:** Recent developments in fluorescence microscopy allow capturing high-resolution 3D images over time for living model organisms. To be able to image even large specimens, techniques like multi-view light-sheet imaging record different orientations at each time point that can then be fused into a single high-quality volume. Based on measured point spread functions (PSF), deconvolution and content fusion are able to largely revert the inevitable degradation occurring during the imaging process. Classical multi-view deconvolution and fusion methods mainly use iterative procedures and content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been deployed to approach 3D single-view deconvolution microscopy, but the multi-view case waits to be studied. We investigated the efficacy of CNN-based multi-view deconvolution and fusion with two synthetic data sets that mimic developing embryos and involve either two or four complementary 3D views. Compared with classical state-of-the-art methods, the proposed semi- and self-supervised models achieve competitive and superior deconvolution and fusion quality in the two-view and quad-view cases, respectively.

</p>
</details>

<details><summary><b>Fairness Properties of Face Recognition and Obfuscation Systems</b>
<a href="https://arxiv.org/abs/2108.02707">arxiv:2108.02707</a>
&#x1F4C8; 3 <br>
<p>Harrison Rosenberg, Brian Tang, Kassem Fawaz, Somesh Jha</p></summary>
<p>

**Abstract:** The proliferation of automated face recognition in various commercial and government sectors has caused significant privacy concerns for individuals. A recent, popular approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems. Face obfuscation systems generate imperceptible perturbations, when added to an image, cause the face recognition system to misidentify the user. The key to these approaches is the generation of perturbations using a pre-trained metric embedding network followed by their application to an online system, whose model might be proprietary. This dependence of face obfuscation on metric embedding networks, which are known to be unfair in the context of face recognition, surfaces the question of demographic fairness -- \textit{are there demographic disparities in the performance of face obfuscation systems?} To address this question, we perform an analytical and empirical exploration of the performance of recent face obfuscation systems that rely on deep embedding networks. We find that metric embedding networks are demographically aware; they cluster faces in the embedding space based on their demographic attributes. We observe that this effect carries through to face obfuscation systems: faces belonging to minority groups incur reduced utility compared to those from majority groups. For example, the disparity in average obfuscation success rate on the online Face++ API can reach up to 20 percentage points. We present an intuitive analytical model to provide insights into these phenomena.

</p>
</details>

<details><summary><b>Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware</b>
<a href="https://arxiv.org/abs/2108.02671">arxiv:2108.02671</a>
&#x1F4C8; 3 <br>
<p>Julia Hornauer, Lazaros Nalpantidis, Vasileios Belagiannis</p></summary>
<p>

**Abstract:** Real-world perception systems in many cases build on hardware with limited resources to adhere to cost and power limitations of their carrying system. Deploying deep neural networks on resource-constrained hardware became possible with model compression techniques, as well as efficient and hardware-aware architecture design. However, model adaptation is additionally required due to the diverse operation environments. In this work, we address the problem of training deep neural networks on resource-constrained hardware in the context of visual domain adaptation. We select the task of monocular depth estimation where our goal is to transform a pre-trained model to the target's domain data. While the source domain includes labels, we assume an unlabelled target domain, as it happens in real-world applications. Then, we present an adversarial learning approach that is adapted for training on the device with limited resources. Since visual domain adaptation, i.e. neural network training, has not been previously explored for resource-constrained hardware, we present the first feasibility study for image-based depth estimation. Our experiments show that visual domain adaptation is relevant only for efficient network architectures and training sets at the order of a few hundred samples. Models and code are publicly available.

</p>
</details>

<details><summary><b>Parallel Capsule Networks for Classification of White Blood Cells</b>
<a href="https://arxiv.org/abs/2108.02644">arxiv:2108.02644</a>
&#x1F4C8; 3 <br>
<p>Juan P. Vigueras-Guillén, Arijit Patra, Ola Engkvist, Frank Seeliger</p></summary>
<p>

**Abstract:** Capsule Networks (CapsNets) is a machine learning architecture proposed to overcome some of the shortcomings of convolutional neural networks (CNNs). However, CapsNets have mainly outperformed CNNs in datasets where images are small and/or the objects to identify have minimal background noise. In this work, we present a new architecture, parallel CapsNets, which exploits the concept of branching the network to isolate certain capsules, allowing each branch to identify different entities. We applied our concept to the two current types of CapsNet architectures, studying the performance for networks with different layers of capsules. We tested our design in a public, highly unbalanced dataset of acute myeloid leukaemia images (15 classes). Our experiments showed that conventional CapsNets show similar performance than our baseline CNN (ResNeXt-50) but depict instability problems. In contrast, parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better rotational invariance than both, conventional CapsNets and ResNeXt-50.

</p>
</details>

<details><summary><b>EENLP: Cross-lingual Eastern European NLP Index</b>
<a href="https://arxiv.org/abs/2108.02605">arxiv:2108.02605</a>
&#x1F4C8; 3 <br>
<p>Alexey Tikhonov, Alex Malkhasov, Andrey Manoshin, George Dima, Réka Cserháti, Md. Sadek Hossain Asif, Matt Sárdi</p></summary>
<p>

**Abstract:** This report presents the results of the EENLP project, done as a part of EEML 2021 summer school.
  It presents a broad index of NLP resources for Eastern European languages, which, we hope, could be helpful for the NLP community; several new hand-crafted cross-lingual datasets focused on Eastern European languages, and a sketch evaluation of cross-lingual transfer learning abilities of several modern multilingual Transformer-based models.

</p>
</details>

<details><summary><b>DeepScanner: a Robotic System for Automated 2D Object Dataset Collection with Annotations</b>
<a href="https://arxiv.org/abs/2108.02555">arxiv:2108.02555</a>
&#x1F4C8; 3 <br>
<p>Valery Ilin, Ivan Kalinov, Pavel Karpyshev, Dzmitry Tsetserukou</p></summary>
<p>

**Abstract:** In the proposed study, we describe the possibility of automated dataset collection using an articulated robot. The proposed technology reduces the number of pixel errors on a polygonal dataset and the time spent on manual labeling of 2D objects. The paper describes a novel automatic dataset collection and annotation system, and compares the results of automated and manual dataset labeling. Our approach increases the speed of data labeling 240-fold, and improves the accuracy compared to manual labeling 13-fold. We also present a comparison of metrics for training a neural network on a manually annotated and an automatically collected dataset.

</p>
</details>

<details><summary><b>Bambara Language Dataset for Sentiment Analysis</b>
<a href="https://arxiv.org/abs/2108.02524">arxiv:2108.02524</a>
&#x1F4C8; 3 <br>
<p>Mountaga Diallo, Chayma Fourati, Hatem Haddad</p></summary>
<p>

**Abstract:** For easier communication, posting, or commenting on each others posts, people use their dialects. In Africa, various languages and dialects exist. However, they are still underrepresented and not fully exploited for analytical studies and research purposes. In order to perform approaches like Machine Learning and Deep Learning, datasets are required. One of the African languages is Bambara, used by citizens in different countries. However, no previous work on datasets for this language was performed for Sentiment Analysis. In this paper, we present the first common-crawl-based Bambara dialectal dataset dedicated for Sentiment Analysis, available freely for Natural Language Processing research purposes.

</p>
</details>

<details><summary><b>Poison Ink: Robust and Invisible Backdoor Attack</b>
<a href="https://arxiv.org/abs/2108.02488">arxiv:2108.02488</a>
&#x1F4C8; 3 <br>
<p>Jie Zhang, Dongdong Chen, Jing Liao, Qidong Huang, Gang Hua, Weiming Zhang, Nenghai Yu</p></summary>
<p>

**Abstract:** Recent research shows deep neural networks are vulnerable to different types of attacks, such as adversarial attack, data poisoning attack and backdoor attack. Among them, backdoor attack is the most cunning one and can occur in almost every stage of deep learning pipeline. Therefore, backdoor attack has attracted lots of interests from both academia and industry. However, most existing backdoor attack methods are either visible or fragile to some effortless pre-processing such as common data transformations. To address these limitations, we propose a robust and invisible backdoor attack called "Poison Ink". Concretely, we first leverage the image structures as target poisoning areas, and fill them with poison ink (information) to generate the trigger pattern. As the image structure can keep its semantic meaning during the data transformation, such trigger pattern is inherently robust to data transformations. Then we leverage a deep injection network to embed such trigger pattern into the cover image to achieve stealthiness. Compared to existing popular backdoor attack methods, Poison Ink outperforms both in stealthiness and robustness. Through extensive experiments, we demonstrate Poison Ink is not only general to different datasets and network architectures, but also flexible for different attack scenarios. Besides, it also has very strong resistance against many state-of-the-art defense techniques.

</p>
</details>

<details><summary><b>LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection</b>
<a href="https://arxiv.org/abs/2108.02455">arxiv:2108.02455</a>
&#x1F4C8; 3 <br>
<p>Cui Xie, Hao Guo, Junyu Dong</p></summary>
<p>

**Abstract:** Ocean fronts can cause the accumulation of nutrients and affect the propagation of underwater sound, so high-precision ocean front detection is of great significance to the marine fishery and national defense fields. However, the current ocean front detection methods either have low detection accuracy or most can only detect the occurrence of ocean front by binary classification, rarely considering the differences of the characteristics of multiple ocean fronts in different sea areas. In order to solve the above problems, we propose a semantic segmentation network called location and seasonality enhanced network (LSENet) for multi-class ocean fronts detection at pixel level. In this network, we first design a channel supervision unit structure, which integrates the seasonal characteristics of the ocean front itself and the contextual information to improve the detection accuracy. We also introduce a location attention mechanism to adaptively assign attention weights to the fronts according to their frequently occurred sea area, which can further improve the accuracy of multi-class ocean front detection. Compared with other semantic segmentation methods and current representative ocean front detection method, the experimental results demonstrate convincingly that our method is more effective.

</p>
</details>

<details><summary><b>Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents</b>
<a href="https://arxiv.org/abs/2108.10078">arxiv:2108.10078</a>
&#x1F4C8; 2 <br>
<p>Ling Zhang, Jian Cao, Yuan Zhang, Bohan Zhou, Shuo Feng</p></summary>
<p>

**Abstract:** Spiking neural network (SNN), compared with depth neural network (DNN), has faster processing speed, lower energy consumption and more biological interpretability, which is expected to approach Strong AI. Reinforcement learning is similar to learning in biology. It is of great significance to study the combination of SNN and RL. We propose the reinforcement learning method of spike distillation network (SDN) with STBP. This method uses distillation to effectively avoid the weakness of STBP, which can achieve SOTA performance in classification, and can obtain a smaller, faster convergence and lower power consumption SNN reinforcement learning model. Experiments show that our method can converge faster than traditional SNN reinforcement learning and DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN 200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which proves that SDN has lower power consumption than DNN, and the power consumption of SDN is more than 600 times lower than DNN on large-scale devices. SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning.

</p>
</details>

<details><summary><b>Adaptive Residue-wise Profile Fusion for Low Homologous Protein SecondaryStructure Prediction Using External Knowledge</b>
<a href="https://arxiv.org/abs/2108.04176">arxiv:2108.04176</a>
&#x1F4C8; 2 <br>
<p>Qin Wang, Jun Wei, Boyuan Wang, Zhen Li1, Sheng Wang, Shuguang Cu</p></summary>
<p>

**Abstract:** Protein secondary structure prediction (PSSP) is essential for protein function analysis. However, for low homologous proteins, the PSSP suffers from insufficient input features. In this paper, we explicitly import external self-supervised knowledge for low homologous PSSP under the guidance of residue-wise profile fusion. In practice, we firstly demonstrate the superiority of profile over Position-Specific Scoring Matrix (PSSM) for low homologous PSSP. Based on this observation, we introduce the novel self-supervised BERT features as the pseudo profile, which implicitly involves the residue distribution in all native discovered sequences as the complementary features. Further-more, a novel residue-wise attention is specially designed to adaptively fuse different features (i.e.,original low-quality profile, BERT based pseudo profile), which not only takes full advantage of each feature but also avoids noise disturbance. Be-sides, the feature consistency loss is proposed to accelerate the model learning from multiple semantic levels. Extensive experiments confirm that our method outperforms state-of-the-arts (i.e.,4.7%forextremely low homologous cases on BC40 dataset).

</p>
</details>

<details><summary><b>End-to-end Neural Video Coding Using a Compound Spatiotemporal Representation</b>
<a href="https://arxiv.org/abs/2108.04103">arxiv:2108.04103</a>
&#x1F4C8; 2 <br>
<p>Haojie Liu, Ming Lu, Zhiqi Chen, Xun Cao, Zhan Ma, Yao Wang</p></summary>
<p>

**Abstract:** Recent years have witnessed rapid advances in learnt video coding. Most algorithms have solely relied on the vector-based motion representation and resampling (e.g., optical flow based bilinear sampling) for exploiting the inter frame redundancy. In spite of the great success of adaptive kernel-based resampling (e.g., adaptive convolutions and deformable convolutions) in video prediction for uncompressed videos, integrating such approaches with rate-distortion optimization for inter frame coding has been less successful. Recognizing that each resampling solution offers unique advantages in regions with different motion and texture characteristics, we propose a hybrid motion compensation (HMC) method that adaptively combines the predictions generated by these two approaches. Specifically, we generate a compound spatiotemporal representation (CSTR) through a recurrent information aggregation (RIA) module using information from the current and multiple past frames. We further design a one-to-many decoder pipeline to generate multiple predictions from the CSTR, including vector-based resampling, adaptive kernel-based resampling, compensation mode selection maps and texture enhancements, and combines them adaptively to achieve more accurate inter prediction. Experiments show that our proposed inter coding system can provide better motion-compensated prediction and is more robust to occlusions and complex motions. Together with jointly trained intra coder and residual coder, the overall learnt hybrid coder yields the state-of-the-art coding efficiency in low-delay scenario, compared to the traditional H.264/AVC and H.265/HEVC, as well as recently published learning-based methods, in terms of both PSNR and MS-SSIM metrics.

</p>
</details>

<details><summary><b>Determining Sentencing Recommendations and Patentability Using a Machine Learning Trained Expert System</b>
<a href="https://arxiv.org/abs/2108.04088">arxiv:2108.04088</a>
&#x1F4C8; 2 <br>
<p>Logan Brown, Reid Pezewski, Jeremy Straub</p></summary>
<p>

**Abstract:** This paper presents two studies that use a machine learning expert system (MLES). One focuses on a system to advise to United States federal judges for regarding consistent federal criminal sentencing, based on both the federal sentencing guidelines and offender characteristics. The other study aims to develop a system that could prospectively assist the U.S. Patent and Trademark Office automate their patentability assessment process. Both studies use a machine learning-trained rule-fact expert system network to accept input variables for training and presentation and output a scaled variable that represents the system recommendation (e.g., the sentence length or the patentability assessment). This paper presents and compares the rule-fact networks that have been developed for these projects. It explains the decision-making process underlying the structures used for both networks and the pre-processing of data that was needed and performed. It also, through comparing the two systems, discusses how different methods can be used with the MLES system.

</p>
</details>

<details><summary><b>Reinforcement Learning for Intelligent Healthcare Systems: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2108.04087">arxiv:2108.04087</a>
&#x1F4C8; 2 <br>
<p>Alaa Awad Abdellatif, Naram Mhaisen, Zina Chkirbene, Amr Mohamed, Aiman Erbad, Mohsen Guizani</p></summary>
<p>

**Abstract:** The rapid increase in the percentage of chronic disease patients along with the recent pandemic pose immediate threats on healthcare expenditure and elevate causes of death. This calls for transforming healthcare systems away from one-on-one patient treatment into intelligent health systems, to improve services, access and scalability, while reducing costs. Reinforcement Learning (RL) has witnessed an intrinsic breakthrough in solving a variety of complex problems for diverse applications and services. Thus, we conduct in this paper a comprehensive survey of the recent models and techniques of RL that have been developed/used for supporting Intelligent-healthcare (I-health) systems. This paper can guide the readers to deeply understand the state-of-the-art regarding the use of RL in the context of I-health. Specifically, we first present an overview for the I-health systems challenges, architecture, and how RL can benefit these systems. We then review the background and mathematical modeling of different RL, Deep RL (DRL), and multi-agent RL models. After that, we provide a deep literature review for the applications of RL in I-health systems. In particular, three main areas have been tackled, i.e., edge intelligence, smart core network, and dynamic treatment regimes. Finally, we highlight emerging challenges and outline future research directions in driving the future success of RL in I-health systems, which opens the door for exploring some interesting and unsolved problems.

</p>
</details>

<details><summary><b>Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications</b>
<a href="https://arxiv.org/abs/2108.02892">arxiv:2108.02892</a>
&#x1F4C8; 2 <br>
<p>Khoi Khac Nguyen, Antonino Masaracchia, Cheng Yin, Long D. Nguyen, Octavia A. Dobre, Trung Q. Duong</p></summary>
<p>

**Abstract:** In this paper, we propose a deep reinforcement learning (DRL) approach for solving the optimisation problem of the network's sum-rate in device-to-device (D2D) communications supported by an intelligent reflecting surface (IRS). The IRS is deployed to mitigate the interference and enhance the signal between the D2D transmitter and the associated D2D receiver. Our objective is to jointly optimise the transmit power at the D2D transmitter and the phase shift matrix at the IRS to maximise the network sum-rate. We formulate a Markov decision process and then propose the proximal policy optimisation for solving the maximisation game. Simulation results show impressive performance in terms of the achievable rate and processing time.

</p>
</details>

<details><summary><b>User Scheduling for Federated Learning Through Over-the-Air Computation</b>
<a href="https://arxiv.org/abs/2108.02891">arxiv:2108.02891</a>
&#x1F4C8; 2 <br>
<p>Xiang Ma, Haijian Sun, Qun Wang, Rose Qingyang Hu</p></summary>
<p>

**Abstract:** A new machine learning (ML) technique termed as federated learning (FL) aims to preserve data at the edge devices and to only exchange ML model parameters in the learning process. FL not only reduces the communication needs but also helps to protect the local privacy. Although FL has these advantages, it can still experience large communication latency when there are massive edge devices connected to the central parameter server (PS) and/or millions of model parameters involved in the learning process. Over-the-air computation (AirComp) is capable of computing while transmitting data by allowing multiple devices to send data simultaneously by using analog modulation. To achieve good performance in FL through AirComp, user scheduling plays a critical role. In this paper, we investigate and compare different user scheduling policies, which are based on various criteria such as wireless channel conditions and the significance of model updates. Receiver beamforming is applied to minimize the mean-square-error (MSE) of the distortion of function aggregation result via AirComp. Simulation results show that scheduling based on the significance of model updates has smaller fluctuations in the training process while scheduling based on channel condition has the advantage on energy efficiency.

</p>
</details>

<details><summary><b>Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques</b>
<a href="https://arxiv.org/abs/2108.02838">arxiv:2108.02838</a>
&#x1F4C8; 2 <br>
<p>Tugce Karatas, Ali Hirsa</p></summary>
<p>

**Abstract:** Market indicators such as CPI and GDP have been widely used over decades to identify the stage of business cycles and also investment attractiveness of sectors given market conditions. In this paper, we propose a two-stage methodology that consists of predicting ETF prices for each sector using market indicators and ranking sectors based on their predicted rate of returns. We initially start with choosing sector specific macroeconomic indicators and implement Recursive Feature Elimination algorithm to select the most important features for each sector. Using our prediction tool, we implement different Recurrent Neural Networks models to predict the future ETF prices for each sector. We then rank the sectors based on their predicted rate of returns. We select the best performing model by evaluating the annualized return, annualized Sharpe ratio, and Calmar ratio of the portfolios that includes the top four ranked sectors chosen by the model. We also test the robustness of the model performance with respect to lookback windows and look ahead windows. Our empirical results show that our methodology beats the equally weighted portfolio performance even in the long run. We also find that Echo State Networks exhibits an outstanding performance compared to other models yet it is faster to implement compared to other RNN models.

</p>
</details>

<details><summary><b>Deep learning for inverse problems with unknown operator</b>
<a href="https://arxiv.org/abs/2108.02744">arxiv:2108.02744</a>
&#x1F4C8; 2 <br>
<p>Miguel del Alamo</p></summary>
<p>

**Abstract:** We consider ill-posed inverse problems where the forward operator $T$ is unknown, and instead we have access to training data consisting of functions $f_i$ and their noisy images $Tf_i$. This is a practically relevant and challenging problem which current methods are able to solve only under strong assumptions on the training set. Here we propose a new method that requires minimal assumptions on the data, and prove reconstruction rates that depend on the number of training points and the noise level. We show that, in the regime of "many" training data, the method is minimax optimal. The proposed method employs a type of convolutional neural networks (U-nets) and empirical risk minimization in order to "fit" the unknown operator. In a nutshell, our approach is based on two ideas: the first is to relate U-nets to multiscale decompositions such as wavelets, thereby linking them to the existing theory, and the second is to use the hierarchical structure of U-nets and the low number of parameters of convolutional neural nets to prove entropy bounds that are practically useful. A significant difference with the existing works on neural networks in nonparametric statistics is that we use them to approximate operators and not functions, which we argue is mathematically more natural and technically more convenient.

</p>
</details>

<details><summary><b>Deep Reinforcement Learning for Continuous Docking Control of Autonomous Underwater Vehicles: A Benchmarking Study</b>
<a href="https://arxiv.org/abs/2108.02665">arxiv:2108.02665</a>
&#x1F4C8; 2 <br>
<p>Mihir Patil, Bilal Wehbe, Matias Valdenegro-Toro</p></summary>
<p>

**Abstract:** Docking control of an autonomous underwater vehicle (AUV) is a task that is integral to achieving persistent long term autonomy. This work explores the application of state-of-the-art model-free deep reinforcement learning (DRL) approaches to the task of AUV docking in the continuous domain. We provide a detailed formulation of the reward function, utilized to successfully dock the AUV onto a fixed docking platform. A major contribution that distinguishes our work from the previous approaches is the usage of a physics simulator to define and simulate the underwater environment as well as the DeepLeng AUV. We propose a new reward function formulation for the docking task, incorporating several components, that outperforms previous reward formulations. We evaluate proximal policy optimization (PPO), twin delayed deep deterministic policy gradients (TD3) and soft actor-critic (SAC) in combination with our reward function. Our evaluation yielded results that conclusively show the TD3 agent to be most efficient and consistent in terms of docking the AUV, over multiple evaluation runs it achieved a 100% success rate and episode return of 10667.1 +- 688.8. We also show how our reward function formulation improves over the state of the art.

</p>
</details>

<details><summary><b>Reducing Unintended Bias of ML Models on Tabular and Textual Data</b>
<a href="https://arxiv.org/abs/2108.02662">arxiv:2108.02662</a>
&#x1F4C8; 2 <br>
<p>Guilherme Alves, Maxime Amblard, Fabien Bernier, Miguel Couceiro, Amedeo Napoli</p></summary>
<p>

**Abstract:** Unintended biases in machine learning (ML) models are among the major concerns that must be addressed to maintain public trust in ML. In this paper, we address process fairness of ML models that consists in reducing the dependence of models on sensitive features, without compromising their performance. We revisit the framework FixOut that is inspired in the approach "fairness through unawareness" to build fairer models. We introduce several improvements such as automating the choice of FixOut's parameters. Also, FixOut was originally proposed to improve fairness of ML models on tabular data. We also demonstrate the feasibility of FixOut's workflow for models on textual data. We present several experimental results that illustrate the fact that FixOut improves process fairness on different classification settings.

</p>
</details>

<details><summary><b>An ASP-based Solution to the Chemotherapy Treatment Scheduling problem</b>
<a href="https://arxiv.org/abs/2108.02637">arxiv:2108.02637</a>
&#x1F4C8; 2 <br>
<p>Carmine Dodaro, Giuseppe Galatà, Andrea Grioni, Marco Maratea, Marco Mochi, Ivan Porro</p></summary>
<p>

**Abstract:** The problem of scheduling chemotherapy treatments in oncology clinics is a complex problem, given that the solution has to satisfy (as much as possible) several requirements such as the cyclic nature of chemotherapy treatment plans, maintaining a constant number of patients, and the availability of resources, e.g., treatment time, nurses, and drugs. At the same time, realizing a satisfying schedule is of upmost importance for obtaining the best health outcomes. In this paper we first consider a specific instance of the problem which is employed in the San Martino Hospital in Genova, Italy, and present a solution to the problem based on Answer Set Programming (ASP). Then, we enrich the problem and the related ASP encoding considering further features often employed in other hospitals, desirable also in S. Martino, and/or considered in related papers. Results of an experimental analysis, conducted on the real data provided by the San Martino Hospital, show that ASP is an effective solving methodology also for this important scheduling problem. Under consideration for acceptance in TPLP.

</p>
</details>

<details><summary><b>Using a Collated Cybersecurity Dataset for Machine Learning and Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2108.02618">arxiv:2108.02618</a>
&#x1F4C8; 2 <br>
<p>Erik Hemberg, Una-May O'Reilly</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) and Machine Learning (ML) algorithms can support the span of indicator-level, e.g. anomaly detection, to behavioral level cyber security modeling and inference. This contribution is based on a dataset named BRON which is amalgamated from public threat and vulnerability behavioral sources. We demonstrate how BRON can support prediction of related threat techniques and attack patterns. We also discuss other AI and ML uses of BRON to exploit its behavioral knowledge.

</p>
</details>

<details><summary><b>Tikhonov Regularization of Circle-Valued Signals</b>
<a href="https://arxiv.org/abs/2108.02602">arxiv:2108.02602</a>
&#x1F4C8; 2 <br>
<p>Laurent Condat</p></summary>
<p>

**Abstract:** It is common to have to process signals or images whose values are cyclic and can be represented as points on the complex circle, like wrapped phases, angles, orientations, or color hues. We consider a Tikhonov-type regularization model to smoothen or interpolate circle-valued signals defined on arbitrary graphs. We propose a convex relaxation of this nonconvex problem as a semidefinite program, and an efficient algorithm to solve it.

</p>
</details>

<details><summary><b>Performer Identification From Symbolic Representation of Music Using Statistical Models</b>
<a href="https://arxiv.org/abs/2108.02576">arxiv:2108.02576</a>
&#x1F4C8; 2 <br>
<p>Syed Rifat Mahmud Rafee, Gyorgy Fazekas, Geraint A. ~Wiggins</p></summary>
<p>

**Abstract:** Music Performers have their own idiosyncratic way of interpreting a musical piece. A group of skilled performers playing the same piece of music would likely to inject their unique artistic styles in their performances. The variations of the tempo, timing, dynamics, articulation etc. from the actual notated music are what make the performers unique in their performances. This study presents a dataset consisting of four movements of Schubert's ``Sonata in B-flat major, D.960" performed by nine virtuoso pianists individually. We proposed and extracted a set of expressive features that are able to capture the characteristics of an individual performer's style. We then present a performer identification method based on the similarity of feature distribution, given a set of piano performances. The identification is done considering each feature individually as well as a fusion of the features. Results show that the proposed method achieved a precision of 0.903 using fusion features. Moreover, the onset time deviation feature shows promising result when considered individually.

</p>
</details>

<details><summary><b>Fairer Chess: A Reversal of Two Opening Moves in Chess Creates Balance Between White and Black</b>
<a href="https://arxiv.org/abs/2108.02547">arxiv:2108.02547</a>
&#x1F4C8; 2 <br>
<p>Steven J. Brams, Mehmet S. Ismail</p></summary>
<p>

**Abstract:** Unlike tic-tac-toe or checkers, in which optimal play leads to a draw, it is not known whether optimal play in chess ends in a win for White, a win for Black, or a draw. But after White moves first in chess, if Black has a double move followed by a double move of White and then alternating play, play is more balanced because White does not always tie or lead in moves. Symbolically, Balanced Alternation gives the following move sequence: After White's (W) initial move, first Black (B) and then White each have two moves in a row (BBWW), followed by the alternating sequence, beginning with W, which altogether can be written as WB/BW/WB/WB/WB... (the slashes separate alternating pairs of moves). Except for reversal of the 3rd and 4th moves from WB to BW, this is the standard chess sequence. Because Balanced Alternation lies between the standard sequence, which favors White, and a comparable sequence that favors Black, it is highly likely to produce a draw with optimal play, rendering chess fairer. This conclusion is supported by a computer analysis of chess openings and how they would play out under Balanced Alternation.

</p>
</details>

<details><summary><b>Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks</b>
<a href="https://arxiv.org/abs/2108.02517">arxiv:2108.02517</a>
&#x1F4C8; 2 <br>
<p>Sawan Singh Mahara, Shruti M., B. N. Bharath</p></summary>
<p>

**Abstract:** Federated Learning (FL) has evolved as a promising technique to handle distributed machine learning across edge devices. A single neural network (NN) that optimises a global objective is generally learned in most work in FL, which could be suboptimal for edge devices. Although works finding a NN personalised for edge device specific tasks exist, they lack generalisation and/or convergence guarantees. In this paper, a novel communication efficient FL algorithm for personalised learning in a wireless setting with guarantees is presented. The algorithm relies on finding a ``better`` empirical estimate of losses at each device, using a weighted average of the losses across different devices. It is devised from a Probably Approximately Correct (PAC) bound on the true loss in terms of the proposed empirical loss and is bounded by (i) the Rademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a signed gradient feedback to find a personalised NN at each device, it is also proven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate of the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed algorithm outperforms locally trained devices as well as the conventionally used FedAvg and FedSGD algorithms under practical SNR regimes.

</p>
</details>

<details><summary><b>Improved Speech Emotion Recognition using Transfer Learning and Spectrogram Augmentation</b>
<a href="https://arxiv.org/abs/2108.02510">arxiv:2108.02510</a>
&#x1F4C8; 2 <br>
<p>Sarala Padi, Seyed Omid Sadjadi, Dinesh Manocha, Ram D. Sriram</p></summary>
<p>

**Abstract:** Automatic speech emotion recognition (SER) is a challenging task that plays a crucial role in natural human-computer interaction. One of the main challenges in SER is data scarcity, i.e., insufficient amounts of carefully labeled data to build and fully explore complex deep learning models for emotion classification. This paper aims to address this challenge using a transfer learning strategy combined with spectrogram augmentation. Specifically, we propose a transfer learning approach that leverages a pre-trained residual network (ResNet) model including a statistics pooling layer from speaker recognition trained using large amounts of speaker-labeled data. The statistics pooling layer enables the model to efficiently process variable-length input, thereby eliminating the need for sequence truncation which is commonly used in SER systems. In addition, we adopt a spectrogram augmentation technique to generate additional training data samples by applying random time-frequency masks to log-mel spectrograms to mitigate overfitting and improve the generalization of emotion recognition models. We evaluate the effectiveness of our proposed approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that the transfer learning and spectrogram augmentation approaches improve the SER performance, and when combined achieve state-of-the-art results.

</p>
</details>

<details><summary><b>RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging</b>
<a href="https://arxiv.org/abs/2108.02508">arxiv:2108.02508</a>
&#x1F4C8; 2 <br>
<p>Narinder Singh Punn, Sonali Agarwal</p></summary>
<p>

**Abstract:** The advancements in deep learning technologies have produced immense contributions to biomedical image analysis applications. With breast cancer being the common deadliest disease among women, early detection is the key means to improve survivability. Medical imaging like ultrasound presents an excellent visual representation of the functioning of the organs; however, for any radiologist analysing such scans is challenging and time consuming which delays the diagnosis process. Although various deep learning based approaches are proposed that achieved promising results, the present article introduces an efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet) model with minimal training parameters for tumor segmentation using breast ultrasound imaging to further improve the segmentation performance of varying tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception depth-wise separable convolution and hybrid pooling (max pooling and spectral pooling) layers. In addition, cross-spatial attention filters are added to suppress the irrelevant features and focus on the target structure. The segmentation performance of the proposed model is validated on two publicly available datasets using standard segmentation evaluation metrics, where it outperformed the other state-of-the-art segmentation models.

</p>
</details>

<details><summary><b>Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection</b>
<a href="https://arxiv.org/abs/2108.02501">arxiv:2108.02501</a>
&#x1F4C8; 2 <br>
<p>Tungyu Wu, Youting Wang</p></summary>
<p>

**Abstract:** For the highly imbalanced credit card fraud detection problem, most existing methods either use data augmentation methods or conventional machine learning models, while neural network-based anomaly detection approaches are lacking. Furthermore, few studies have employed AI interpretability tools to investigate the feature importance of transaction data, which is crucial for the black-box fraud detection module. Considering these two points together, we propose a novel anomaly detection framework for credit card fraud detection as well as a model-explaining module responsible for prediction explanations. The fraud detection model is composed of two deep neural networks, which are trained in an unsupervised and adversarial manner. Precisely, the generator is an AutoEncoder aiming to reconstruct genuine transaction data, while the discriminator is a fully-connected network for fraud detection. The explanation module has three white-box explainers in charge of interpretations of the AutoEncoder, discriminator, and the whole detection model, respectively. Experimental results show the state-of-the-art performances of our fraud detection model on the benchmark dataset compared with baselines. In addition, prediction analyses by three explainers are presented, offering a clear perspective on how each feature of an instance of interest contributes to the final model output.

</p>
</details>

<details><summary><b>MixLacune: Segmentation of lacunes of presumed vascular origin</b>
<a href="https://arxiv.org/abs/2108.02483">arxiv:2108.02483</a>
&#x1F4C8; 2 <br>
<p>Denis Kutnar, Bas H. M. van der Velden, Marta Girones Sanguesa, Mirjam I. Geerlings, J. Matthijs Biesbroek, Hugo J. Kuijf</p></summary>
<p>

**Abstract:** Lacunes of presumed vascular origin are fluid-filled cavities of between 3 - 15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes relies on manual annotation or semi-automatic / interactive approaches; and almost no automatic methods exist for this task. In this work, we present a two-stage approach to segment lacunes of presumed vascular origin: (1) detection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data originates from Task 3 of the "Where is VALDO?" challenge and consists of 40 training subjects. We report the mean DICE on the training set of 0.83 and on the validation set of 0.84. Source code is available at: https://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune can be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .

</p>
</details>

<details><summary><b>MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds</b>
<a href="https://arxiv.org/abs/2108.02482">arxiv:2108.02482</a>
&#x1F4C8; 2 <br>
<p>Marta Girones Sanguesa, Denis Kutnar, Bas H. M. van der Velden, Hugo J. Kuijf</p></summary>
<p>

**Abstract:** Cerebral microbleeds are small, dark, round lesions that can be visualised on T2*-weighted MRI or other sequences sensitive to susceptibility effects. In this work, we propose a multi-stage approach to both microbleed detection and segmentation. First, possible microbleed locations are detected with a Mask R-CNN technique. Second, at each possible microbleed location, a simple U-Net performs the final segmentation. This work used the 72 subjects as training data provided by the "Where is VALDO?" challenge of MICCAI 2021.

</p>
</details>

<details><summary><b>A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare</b>
<a href="https://arxiv.org/abs/2108.02428">arxiv:2108.02428</a>
&#x1F4C8; 2 <br>
<p>Andrei Velichko</p></summary>
<p>

**Abstract:** Edge computing is a fast-growing and much needed technology in healthcare. The problem of implementing artificial intelligence on edge devices is the complexity and high resource intensity of the most known neural network data analysis methods and algorithms. The difficulty of implementing these methods on low-power microcontrollers with small memory size calls for the development of new effective algorithms for neural networks. This study presents a new method for analyzing medical data based on the LogNNet neural network, which uses chaotic mappings to transform input information. The method effectively solves classification problems and calculates risk factors for the presence of a disease in a patient according to a set of medical health indicators. The efficiency of LogNNet in assessing perinatal risk is illustrated on cardiotocogram data obtained from the UC Irvine machine learning repository. The classification accuracy reaches ~91% with the ~3-10 kB of RAM used on the Arduino microcontroller. Using the LogNNet network trained on a publicly available database of the Israeli Ministry of Health, a service concept for COVID-19 express testing is provided. A classification accuracy of ~95% is achieved, and ~0.6 kB of RAM is used. In all examples, the model is tested using standard classification quality metrics: precision, recall, and F1-measure. The LogNNet architecture allows the implementation of artificial intelligence on medical peripherals of the Internet of Things with low RAM resources and can be used in clinical decision support systems.

</p>
</details>

<details><summary><b>Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems</b>
<a href="https://arxiv.org/abs/2108.04059">arxiv:2108.04059</a>
&#x1F4C8; 1 <br>
<p>Andres Gomez, Andreas Tretter, Pascal Alexander Hager, Praveenth Sanmugarajah, Luca Benini, Lothar Thiele</p></summary>
<p>

**Abstract:** Sensing systems powered by energy harvesting have traditionally been designed to tolerate long periods without energy. As the Internet of Things (IoT) evolves towards a more transient and opportunistic execution paradigm, reducing energy storage costs will be key for its economic and ecologic viability. However, decreasing energy storage in harvesting systems introduces reliability issues. Transducers only produce intermittent energy at low voltage and current levels, making guaranteed task completion a challenge. Existing ad hoc methods overcome this by buffering enough energy either for single tasks, incurring large data-retention overheads, or for one full application cycle, requiring a large energy buffer. We present Julienning: an automated method for optimizing the total energy cost of batteryless applications. Using a custom specification model, developers can describe transient applications as a set of atomically executed kernels with explicit data dependencies. Our optimization flow can partition data- and energy-intensive applications into multiple execution cycles with bounded energy consumption. By leveraging interkernel data dependencies, these energy-bounded execution cycles minimize the number of system activations and nonvolatile data transfers, and thus the total energy overhead. We validate our methodology with two batteryless cameras running energy-intensive machine learning applications. Results demonstrate that compared to ad hoc solutions, our method can reduce the required energy storage by over 94% while only incurring a 0.12% energy overhead.

</p>
</details>

<details><summary><b>RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2108.02889">arxiv:2108.02889</a>
&#x1F4C8; 1 <br>
<p>Khoi Khac Nguyen, Antonino Masaracchia, Tan Do-Duy, H. Vincent Poor, Trung Q. Duong</p></summary>
<p>

**Abstract:** Many of the devices used in Internet-of-Things (IoT) applications are energy-limited, and thus supplying energy while maintaining seamless connectivity for IoT devices is of considerable importance. In this context, we propose a simultaneous wireless power transfer and information transmission scheme for IoT devices with support from reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a first phase, IoT devices harvest energy from the UAV through wireless power transfer; and then in a second phase, the UAV collects data from the IoT devices through information transmission. To characterise the agility of the UAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at maximizing the total network sum-rate, we jointly optimize the trajectory of the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS. We formulate a Markov decision process and propose two deep reinforcement learning algorithms to solve the optimization problem of maximizing the total network sum-rate. Numerical results illustrate the effectiveness of the UAV's flying path optimization and the network's throughput of our proposed techniques compared with other benchmark schemes. Given the strict requirements of the RIS and UAV, the significant improvement in processing time and throughput performance demonstrates that our proposed scheme is well applicable for practical IoT applications.

</p>
</details>

<details><summary><b>Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2108.02837">arxiv:2108.02837</a>
&#x1F4C8; 1 <br>
<p>Jaber Rezaei Mianroodi, Shahed Rezaei, Nima H. Siboni, Bai-Xiang Xu, Dierk Raabe</p></summary>
<p>

**Abstract:** The elastic properties of materials derive from their electronic and atomic nature. However, simulating bulk materials fully at these scales is not feasible, so that typically homogenized continuum descriptions are used instead. A seamless and lossless transition of the constitutive description of the elastic response of materials between these two scales has been so far elusive. Here we show how this problem can be overcome by using Artificial Intelligence (AI). A Convolutional Neural Network (CNN) model is trained, by taking the structure image of a nanoporous material as input and the corresponding elasticity tensor, calculated from Molecular Statics (MS), as output. Trained with the atomistic data, the CNN model captures the size- and pore-dependency of the material's elastic properties which, on the physics side, can stem from surfaces and non-local effects. Such effects are often ignored in upscaling from atomistic to classical continuum theory. To demonstrate the accuracy and the efficiency of the trained CNN model, a Finite Element Method (FEM) based result of an elastically deformed nanoporous beam equipped with the CNN as constitutive law is compared with that by a full atomistic simulation. The good agreement between the atomistic simulations and the FEM-AI combination for a system with size and surface effects establishes a new lossless scale bridging approach to such problems. The trained CNN model deviates from the atomistic result by 9.6\% for porosity scenarios of up to 90\% but it is about 230 times faster than the MS calculation and does not require to change simulation methods between different scales. The efficiency of the CNN evaluation together with the preservation of important atomistic effects makes the trained model an effective atomistically-informed constitutive model for macroscopic simulations of nanoporous materials and solving of inverse problems.

</p>
</details>

<details><summary><b>THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy</b>
<a href="https://arxiv.org/abs/2108.02817">arxiv:2108.02817</a>
&#x1F4C8; 1 <br>
<p>Carla Floricel, Nafiul Nipu, Mikayla Biggs, Andrew Wentzel, Guadalupe Canahuate, Lisanne Van Dijk, Abdallah Mohamed, C. David Fuller, G. Elisabeta Marai</p></summary>
<p>

**Abstract:** Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.

</p>
</details>

<details><summary><b>Role-based lateral movement detection with unsupervised learning</b>
<a href="https://arxiv.org/abs/2108.02713">arxiv:2108.02713</a>
&#x1F4C8; 1 <br>
<p>Brian A. Powell</p></summary>
<p>

**Abstract:** Adversarial lateral movement via compromised accounts remains difficult to discover via traditional rule-based defenses because it generally lacks explicit indicators of compromise. We propose a behavior-based, unsupervised framework comprising two methods of lateral movement detection on enterprise networks: one aimed at generic lateral movement via either exploit or authenticated connections, and one targeting the specific techniques of process injection and hijacking. The first method is based on the premise that the role of a system---the functions it performs on the network---determines the roles of the systems it should make connections with. The adversary meanwhile might move between any systems whatever, possibly seeking out systems with unusual roles that facilitate certain accesses. We use unsupervised learning to cluster systems according to role and identify connections to systems with novel roles as potentially malicious. The second method is based on the premise that the temporal patterns of inter-system processes that facilitate these connections depend on the roles of the systems involved. If a process is compromised by an attacker, these normal patterns might be disrupted in discernible ways. We apply frequent-itemset mining to process sequences to establish regular patterns of communication between systems based on role, and identify rare process sequences as signalling potentially malicious connections.

</p>
</details>

<details><summary><b>Using Metamorphic Relations to Verify and Enhance Artcode Classification</b>
<a href="https://arxiv.org/abs/2108.02694">arxiv:2108.02694</a>
&#x1F4C8; 1 <br>
<p>Liming Xu, Dave Towey, Andrew French, Steve Benford, Zhi Quan Zhou, Tsong Yueh Chen</p></summary>
<p>

**Abstract:** Software testing is often hindered where it is impossible or impractical to determine the correctness of the behaviour or output of the software under test (SUT), a situation known as the oracle problem. An example of an area facing the oracle problem is automatic image classification, using machine learning to classify an input image as one of a set of predefined classes. An approach to software testing that alleviates the oracle problem is metamorphic testing (MT). While traditional software testing examines the correctness of individual test cases, MT instead examines the relations amongst multiple executions of test cases and their outputs. These relations are called metamorphic relations (MRs): if an MR is found to be violated, then a fault must exist in the SUT. This paper examines the problem of classifying images containing visually hidden markers called Artcodes, and applies MT to verify and enhance the trained classifiers. This paper further examines two MRs, Separation and Occlusion, and reports on their capability in verifying the image classification using one-way analysis of variance (ANOVA) in conjunction with three other statistical analysis methods: t-test (for unequal variances), Kruskal-Wallis test, and Dunnett's test. In addition to our previously-studied classifier, that used Random Forests, we introduce a new classifier that uses a support vector machine, and present its MR-augmented version. Experimental evaluations across a number of performance metrics show that the augmented classifiers can achieve better performance than non-augmented classifiers. This paper also analyses how the enhanced performance is obtained.

</p>
</details>

<details><summary><b>Redatuming physical systems using symmetric autoencoders</b>
<a href="https://arxiv.org/abs/2108.02537">arxiv:2108.02537</a>
&#x1F4C8; 1 <br>
<p>Pawan Bharadwaj, Matthew Li, Laurent Demanet</p></summary>
<p>

**Abstract:** This paper considers physical systems described by hidden states and indirectly observed through repeated measurements corrupted by unmodeled nuisance parameters. A network-based representation learns to disentangle the coherent information (relative to the state) from the incoherent nuisance information (relative to the sensing). Instead of physical models, the representation uses symmetry and stochastic regularization to inform an autoencoder architecture called SymAE. It enables redatuming, i.e., creating virtual data instances where the nuisances are uniformized across measurements.

</p>
</details>

<details><summary><b>On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement</b>
<a href="https://arxiv.org/abs/2108.02505">arxiv:2108.02505</a>
&#x1F4C8; 1 <br>
<p>Jose Jurandir Alves Esteves, Amina Boubendir, Fabrice Guillemin, Pierre Sens</p></summary>
<p>

**Abstract:** The evaluation of the impact of using Machine Learning in the management of softwarized networks is considered in multiple research works. Beyond that, we propose to evaluate the robustness of online learning for optimal network slice placement. A major assumption to this study is to consider that slice request arrivals are non-stationary. In this context, we simulate unpredictable network load variations and compare two Deep Reinforcement Learning (DRL) algorithms: a pure DRL-based algorithm and a heuristically controlled DRL as a hybrid DRL-heuristic algorithm, to assess the impact of these unpredictable changes of traffic load on the algorithms performance. We conduct extensive simulations of a large-scale operator infrastructure. The evaluation results show that the proposed hybrid DRL-heuristic approach is more robust and reliable in case of unpredictable network load changes than pure DRL as it reduces the performance degradation. These results are follow-ups for a series of recent research we have performed showing that the proposed hybrid DRL-heuristic approach is efficient and more adapted to real network scenarios than pure DRL.

</p>
</details>

<details><summary><b>DRL-based Slice Placement Under Non-Stationary Conditions</b>
<a href="https://arxiv.org/abs/2108.02495">arxiv:2108.02495</a>
&#x1F4C8; 1 <br>
<p>Jose Jurandir Alves Esteves, Amina Boubendir, Fabrice Guillemin, Pierre Sens</p></summary>
<p>

**Abstract:** We consider online learning for optimal network slice placement under the assumption that slice requests arrive according to a non-stationary Poisson process. We propose a framework based on Deep Reinforcement Learning (DRL) combined with a heuristic to design algorithms. We specifically design two pure-DRL algorithms and two families of hybrid DRL-heuristic algorithms. To validate their performance, we perform extensive simulations in the context of a large-scale operator infrastructure. The evaluation results show that the proposed hybrid DRL-heuristic algorithms require three orders of magnitude of learning episodes less than pure-DRL to achieve convergence. This result indicates that the proposed hybrid DRL-heuristic approach is more reliable than pure-DRL in a real non-stationary network scenario.

</p>
</details>


[Next Page](2021/2021-08/2021-08-04.md)
