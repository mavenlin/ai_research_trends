## Summary for 2021-10-05, created on 2021-12-16


<details><summary><b>Autoregressive Diffusion Models</b>
<a href="https://arxiv.org/abs/2110.02037">arxiv:2110.02037</a>
&#x1F4C8; 111 <br>
<p>Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans</p></summary>
<p>

**Abstract:** We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.

</p>
</details>

<details><summary><b>Exploring the Limits of Large Scale Pre-training</b>
<a href="https://arxiv.org/abs/2110.02095">arxiv:2110.02095</a>
&#x1F4C8; 105 <br>
<p>Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi</p></summary>
<p>

**Abstract:** Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work, we systematically study this phenomena and establish that, as we increase the upstream accuracy, the performance of downstream tasks saturates. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the saturation behavior we observe is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, to have a better downstream performance, we need to hurt upstream accuracy.

</p>
</details>

<details><summary><b>Replay-Guided Adversarial Environment Design</b>
<a href="https://arxiv.org/abs/2110.02439">arxiv:2110.02439</a>
&#x1F4C8; 62 <br>
<p>Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, Tim Rockt√§schel</p></summary>
<p>

**Abstract:** Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR$^{\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR$^{\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework.

</p>
</details>

<details><summary><b>Deep reinforcement learning for guidewire navigation in coronary artery phantom</b>
<a href="https://arxiv.org/abs/2110.01840">arxiv:2110.01840</a>
&#x1F4C8; 51 <br>
<p>Jihoon Kweon, Kyunghwan Kim, Chaehyuk Lee, Hwi Kwon, Jinwoo Park, Kyoseok Song, Young In Kim, Jeeone Park, Inwook Back, Jae-Hyung Roh, Youngjin Moon, Jaesoon Choi, Young-Hak Kim</p></summary>
<p>

**Abstract:** In percutaneous intervention for treatment of coronary plaques, guidewire navigation is a primary procedure for stent delivery. Steering a flexible guidewire within coronary arteries requires considerable training, and the non-linearity between the control operation and the movement of the guidewire makes precise manipulation difficult. Here, we introduce a deep reinforcement learning(RL) framework for autonomous guidewire navigation in a robot-assisted coronary intervention. Using Rainbow, a segment-wise learning approach is applied to determine how best to accelerate training using human demonstrations with deep Q-learning from demonstrations (DQfD), transfer learning, and weight initialization. `State' for RL is customized as a focus window near the guidewire tip, and subgoals are placed to mitigate a sparse reward problem. The RL agent improves performance, eventually enabling the guidewire to reach all valid targets in `stable' phase. Our framework opens anew direction in the automation of robot-assisted intervention, providing guidance on RL in physical spaces involving mechanical fatigue.

</p>
</details>

<details><summary><b>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</b>
<a href="https://arxiv.org/abs/2110.02178">arxiv:2110.02178</a>
&#x1F4C8; 45 <br>
<p>Sachin Mehta, Mohammad Rastegari</p></summary>
<p>

**Abstract:** Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than Mo-bileNetv3 for a similar number of parameters.

</p>
</details>

<details><summary><b>Applying Machine Learning to Study Fluid Mechanics</b>
<a href="https://arxiv.org/abs/2110.02083">arxiv:2110.02083</a>
&#x1F4C8; 30 <br>
<p>Steven L. Brunton</p></summary>
<p>

**Abstract:** This paper provides a short overview of how to use machine learning to build data-driven models in fluid mechanics. The process of machine learning is broken down into five stages: (1) formulating a problem to model, (2) collecting and curating training data to inform the model, (3) choosing an architecture with which to represent the model, (4) designing a loss function to assess the performance of the model, and (5) selecting and implementing an optimization algorithm to train the model. At each stage, we discuss how prior physical knowledge may be embedding into the process, with specific examples from the field of fluid mechanics.

</p>
</details>

<details><summary><b>Data Augmentation Approaches in Natural Language Processing: A Survey</b>
<a href="https://arxiv.org/abs/2110.01852">arxiv:2110.01852</a>
&#x1F4C8; 27 <br>
<p>Bohan Li, Yutai Hou, Wanxiang Che</p></summary>
<p>

**Abstract:** As an effective strategy, data augmentation (DA) alleviates data scarcity scenarios where deep learning techniques may fail. It is widely applied in computer vision then introduced to natural language processing and achieves improvements in many tasks. One of the main focuses of the DA methods is to improve the diversity of training data, thereby helping the model to better generalize to unseen testing data. In this survey, we frame DA methods into three categories based on the diversity of augmented data, including paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods in detail according to the above categories. Further, we also introduce their applications in NLP tasks as well as the challenges.

</p>
</details>

<details><summary><b>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</b>
<a href="https://arxiv.org/abs/2110.02386">arxiv:2110.02386</a>
&#x1F4C8; 22 <br>
<p>Karthikeyan K, Aalok Sathe, Somak Aditya, Monojit Choudhury</p></summary>
<p>

**Abstract:** Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.

</p>
</details>

<details><summary><b>Spatial Context Awareness for Unsupervised Change Detection in Optical Satellite Images</b>
<a href="https://arxiv.org/abs/2110.02068">arxiv:2110.02068</a>
&#x1F4C8; 21 <br>
<p>Lukas Kondmann, Aysim Toker, Sudipan Saha, Bernhard Sch√∂lkopf, Laura Leal-Taix√©, Xiao Xiang Zhu</p></summary>
<p>

**Abstract:** Detecting changes on the ground in multitemporal Earth observation data is one of the key problems in remote sensing. In this paper, we introduce Sibling Regression for Optical Change detection (SiROC), an unsupervised method for change detection in optical satellite images with medium and high resolution. SiROC is a spatial context-based method that models a pixel as a linear combination of its distant neighbors. It uses this model to analyze differences in the pixel and its spatial context-based predictions in subsequent time periods for change detection. We combine this spatial context-based change detection with ensembling over mutually exclusive neighborhoods and transitioning from pixel to object-level changes with morphological operations. SiROC achieves competitive performance for change detection with medium-resolution Sentinel-2 and high-resolution Planetscope imagery on four datasets. Besides accurate predictions without the need for training, SiROC also provides a well-calibrated uncertainty of its predictions. This makes the method especially useful in conjunction with deep-learning based methods for applications such as pseudo-labeling.

</p>
</details>

<details><summary><b>Influence-Balanced Loss for Imbalanced Visual Classification</b>
<a href="https://arxiv.org/abs/2110.02444">arxiv:2110.02444</a>
&#x1F4C8; 15 <br>
<p>Seulki Park, Jongin Lim, Younghan Jeon, Jin Young Choi</p></summary>
<p>

**Abstract:** In this paper, we propose a balancing training method to address problems in imbalanced data learning. To this end, we derive a new loss used in the balancing training phase that alleviates the influence of samples that cause an overfitted decision boundary. The proposed loss efficiently improves the performance of any type of imbalance learning methods. In experiments on multiple benchmark data sets, we demonstrate the validity of our method and reveal that the proposed loss outperforms the state-of-the-art cost-sensitive loss methods. Furthermore, since our loss is not restricted to a specific task, model, or training method, it can be easily used in combination with other recent re-sampling, meta-learning, and cost-sensitive learning methods for class-imbalance problems.

</p>
</details>

<details><summary><b>PoNet: Pooling Network for Efficient Token Mixing in Long Sequences</b>
<a href="https://arxiv.org/abs/2110.02442">arxiv:2110.02442</a>
&#x1F4C8; 10 <br>
<p>Chao-Hong Tan, Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Zhen-Hua Ling</p></summary>
<p>

**Abstract:** Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 96.0% of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations.

</p>
</details>

<details><summary><b>Robustness modularity in complex networks</b>
<a href="https://arxiv.org/abs/2110.02297">arxiv:2110.02297</a>
&#x1F4C8; 9 <br>
<p>Filipi N. Silva, Aiiad Albeshri, Vijey Thayananthan, Wadee Alhalabi, Santo Fortunato</p></summary>
<p>

**Abstract:** A basic question in network community detection is how modular a given network is. This is usually addressed by evaluating the quality of partitions detected in the network. The Girvan-Newman (GN) modularity function is the standard way to make this assessment, but it has a number of drawbacks. Most importantly, it is not clearly interpretable, given that the measure can take relatively large values on partitions of random networks without communities. Here we propose a new measure based on the concept of robustness: modularity is the probability to find trivial partitions when the structure of the network is randomly perturbed. This concept can be implemented for any clustering algorithm capable of telling when a group structure is absent. Tests on artificial and real graphs reveal that robustness modularity can be used to assess and compare the strength of the community structure of different networks. We also introduce two other quality functions: modularity difference, a suitably normalized version of the GN modularity; information modularity, a measure of distance based on information compression. Both measures are strongly correlated with robustness modularity, and are promising options as well.

</p>
</details>

<details><summary><b>The Potential of Machine Learning to Enhance Computational Fluid Dynamics</b>
<a href="https://arxiv.org/abs/2110.02085">arxiv:2110.02085</a>
&#x1F4C8; 9 <br>
<p>Ricardo Vinuesa, Steven L. Brunton</p></summary>
<p>

**Abstract:** Machine learning is rapidly becoming a core technology for scientific computing, with numerous opportunities to advance the field of computational fluid dynamics. This paper highlights some of the areas of highest potential impact, including to accelerate direct numerical simulations, to improve turbulence closure modelling, and to develop enhanced reduced-order models. In each of these areas, it is possible to improve machine learning capabilities by incorporating physics into the process, and in turn, to improve the simulation of fluids to uncover new physical understanding. Despite the promise of machine learning described here, we also note that classical methods are often more efficient for many tasks. We also emphasize that in order to harness the full potential of machine learning to improve computational fluid dynamics, it is essential for the community to continue to establish benchmark systems and best practices for open-source software, data sharing, and reproducible research.

</p>
</details>

<details><summary><b>Coarsening Optimization for Differentiable Programming</b>
<a href="https://arxiv.org/abs/2110.02307">arxiv:2110.02307</a>
&#x1F4C8; 8 <br>
<p>Xipeng Shen, Guoqiang Zhang, Irene Dea, Samantha Andow, Emilio Arroyo-Fang, Neal Gafter, Johann George, Melissa Grueter, Erik Meijer, Steffi Stumpos, Alanna Tempest, Christy Warden, Shannon Yang</p></summary>
<p>

**Abstract:** This paper presents a novel optimization for differentiable programming named coarsening optimization. It offers a systematic way to synergize symbolic differentiation and algorithmic differentiation (AD). Through it, the granularity of the computations differentiated by each step in AD can become much larger than a single operation, and hence lead to much reduced runtime computations and data allocations in AD. To circumvent the difficulties that control flow creates to symbolic differentiation in coarsening, this work introduces phi-calculus, a novel method to allow symbolic reasoning and differentiation of computations that involve branches and loops. It further avoids "expression swell" in symbolic differentiation and balance reuse and coarsening through the design of reuse-centric segment of interest identification. Experiments on a collection of real-world applications show that coarsening optimization is effective in speeding up AD, producing several times to two orders of magnitude speedups.

</p>
</details>

<details><summary><b>A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection</b>
<a href="https://arxiv.org/abs/2110.04079">arxiv:2110.04079</a>
&#x1F4C8; 7 <br>
<p>Yongqi Dong, Sandeep Patil, Bart van Arem, Haneen Farah</p></summary>
<p>

**Abstract:** Reliable and accurate lane detection is of vital importance for the safe performance of Lane Keeping Assistance and Lane Departure Warning systems. However, under certain challenging peculiar circumstances, it is difficult to get satisfactory performance in accurately detecting the lanes from one single image which is often the case in current literature. Since lane markings are continuous lines, the lanes that are difficult to be accurately detected in the single current image can potentially be better deduced if information from previous frames is incorporated. This study proposes a novel hybrid spatial-temporal sequence-to-one deep learning architecture making full use of the spatial-temporal information in multiple continuous image frames to detect lane markings in the very last current frame. Specifically, the hybrid model integrates the single image feature extraction module with the spatial convolutional neural network (SCNN) embedded for excavating spatial features and relationships in one single image, the spatial-temporal feature integration module with spatial-temporal recurrent neural network (ST-RNN), which can capture the spatial-temporal correlations and time dependencies among image sequences, and the encoder-decoder structure, which makes this image segmentation problem work in an end-to-end supervised learning format. Extensive experiments reveal that the proposed model can effectively handle challenging driving scenes and outperforms available state-of-the-art methods with a large margin.

</p>
</details>

<details><summary><b>Influencing Towards Stable Multi-Agent Interactions</b>
<a href="https://arxiv.org/abs/2110.08229">arxiv:2110.08229</a>
&#x1F4C8; 6 <br>
<p>Woodrow Z. Wang, Andy Shih, Annie Xie, Dorsa Sadigh</p></summary>
<p>

**Abstract:** Learning in multi-agent environments is difficult due to the non-stationarity introduced by an opponent's or partner's changing behaviors. Instead of reactively adapting to the other agent's (opponent or partner) behavior, we propose an algorithm to proactively influence the other agent's strategy to stabilize -- which can restrain the non-stationarity caused by the other agent. We learn a low-dimensional latent representation of the other agent's strategy and the dynamics of how the latent strategy evolves with respect to our robot's behavior. With this learned dynamics model, we can define an unsupervised stability reward to train our robot to deliberately influence the other agent to stabilize towards a single strategy. We demonstrate the effectiveness of stabilizing in improving efficiency of maximizing the task reward in a variety of simulated environments, including autonomous driving, emergent communication, and robotic manipulation. We show qualitative results on our website: https://sites.google.com/view/stable-marl/.

</p>
</details>

<details><summary><b>Live Multi-Streaming and Donation Recommendations via Coupled Donation-Response Tensor Factorization</b>
<a href="https://arxiv.org/abs/2110.06117">arxiv:2110.06117</a>
&#x1F4C8; 6 <br>
<p>Hsu-Chao Lai, Jui-Yi Tsai, Hong-Han Shuai, Jiun-Long Huang, Wang-Chien Lee, De-Nian Yang</p></summary>
<p>

**Abstract:** In contrast to traditional online videos, live multi-streaming supports real-time social interactions between multiple streamers and viewers, such as donations. However, donation and multi-streaming channel recommendations are challenging due to complicated streamer and viewer relations, asymmetric communications, and the tradeoff between personal interests and group interactions. In this paper, we introduce Multi-Stream Party (MSP) and formulate a new multi-streaming recommendation problem, called Donation and MSP Recommendation (DAMRec). We propose Multi-stream Party Recommender System (MARS) to extract latent features via socio-temporal coupled donation-response tensor factorization for donation and MSP recommendations. Experimental results on Twitch and Douyu manifest that MARS significantly outperforms existing recommenders by at least 38.8% in terms of hit ratio and mean average precision.

</p>
</details>

<details><summary><b>Multi-Object Tracking with Deep Learning Ensemble for Unmanned Aerial System Applications</b>
<a href="https://arxiv.org/abs/2110.02044">arxiv:2110.02044</a>
&#x1F4C8; 6 <br>
<p>Wanlin Xie, Jaime Ide, Daniel Izadi, Sean Banger, Thayne Walker, Ryan Ceresani, Dylan Spagnuolo, Christopher Guagliano, Henry Diaz, Jason Twedt</p></summary>
<p>

**Abstract:** Multi-object tracking (MOT) is a crucial component of situational awareness in military defense applications. With the growing use of unmanned aerial systems (UASs), MOT methods for aerial surveillance is in high demand. Application of MOT in UAS presents specific challenges such as moving sensor, changing zoom levels, dynamic background, illumination changes, obscurations and small objects. In this work, we present a robust object tracking architecture aimed to accommodate for the noise in real-time situations. We propose a kinematic prediction model, called Deep Extended Kalman Filter (DeepEKF), in which a sequence-to-sequence architecture is used to predict entity trajectories in latent space. DeepEKF utilizes a learned image embedding along with an attention mechanism trained to weight the importance of areas in an image to predict future states. For the visual scoring, we experiment with different similarity measures to calculate distance based on entity appearances, including a convolutional neural network (CNN) encoder, pre-trained using Siamese networks. In initial evaluation experiments, we show that our method, combining scoring structure of the kinematic and visual models within a MHT framework, has improved performance especially in edge cases where entity motion is unpredictable, or the data presents frames with significant gaps.

</p>
</details>

<details><summary><b>A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction</b>
<a href="https://arxiv.org/abs/2110.03446">arxiv:2110.03446</a>
&#x1F4C8; 5 <br>
<p>Moitreya Chatterjee, Narendra Ahuja, Anoop Cherian</p></summary>
<p>

**Abstract:** Predicting the future frames of a video is a challenging task, in part due to the underlying stochastic real-world phenomena. Prior approaches to solve this task typically estimate a latent prior characterizing this stochasticity, however do not account for the predictive uncertainty of the (deep learning) model. Such approaches often derive the training signal from the mean-squared error (MSE) between the generated frame and the ground truth, which can lead to sub-optimal training, especially when the predictive uncertainty is high. Towards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a stochastic quantification of the model's predictive uncertainty, and use it to weigh the MSE loss. We propose a hierarchical, variational framework to derive NUQ in a principled manner using a deep, Bayesian graphical model. Our experiments on four benchmark stochastic video prediction datasets show that our proposed framework trains more effectively compared to the state-of-the-art models (especially when the training sets are small), while demonstrating better video generation quality and diversity against several evaluation metrics.

</p>
</details>

<details><summary><b>Unifying Likelihood-free Inference with Black-box Sequence Design and Beyond</b>
<a href="https://arxiv.org/abs/2110.03372">arxiv:2110.03372</a>
&#x1F4C8; 5 <br>
<p>Dinghuai Zhang, Jie Fu, Yoshua Bengio, Aaron Courville</p></summary>
<p>

**Abstract:** Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box sequence design, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous drug discovery approaches can be "reinvented" in our framework, and further propose new probabilistic sequence design algorithms. Extensive experiments illustrate the benefits of the proposed methodology.

</p>
</details>

<details><summary><b>Extensions of Karger's Algorithm: Why They Fail in Theory and How They Are Useful in Practice</b>
<a href="https://arxiv.org/abs/2110.02750">arxiv:2110.02750</a>
&#x1F4C8; 5 <br>
<p>Erik Jenner, Enrique Fita Sanmart√≠n, Fred A. Hamprecht</p></summary>
<p>

**Abstract:** The minimum graph cut and minimum $s$-$t$-cut problems are important primitives in the modeling of combinatorial problems in computer science, including in computer vision and machine learning. Some of the most efficient algorithms for finding global minimum cuts are randomized algorithms based on Karger's groundbreaking contraction algorithm. Here, we study whether Karger's algorithm can be successfully generalized to other cut problems. We first prove that a wide class of natural generalizations of Karger's algorithm cannot efficiently solve the $s$-$t$-mincut or the normalized cut problem to optimality. However, we then present a simple new algorithm for seeded segmentation / graph-based semi-supervised learning that is closely based on Karger's original algorithm, showing that for these problems, extensions of Karger's algorithm can be useful. The new algorithm has linear asymptotic runtime and yields a potential that can be interpreted as the posterior probability of a sample belonging to a given seed / class. We clarify its relation to the random walker algorithm / harmonic energy minimization in terms of distributions over spanning forests. On classical problems from seeded image segmentation and graph-based semi-supervised learning on image data, the method performs at least as well as the random walker / harmonic energy minimization / Gaussian processes.

</p>
</details>

<details><summary><b>Noisy Feature Mixup</b>
<a href="https://arxiv.org/abs/2110.02180">arxiv:2110.02180</a>
&#x1F4C8; 5 <br>
<p>Soon Hoe Lim, N. Benjamin Erichson, Francisco Utrera, Winnie Xu, Michael W. Mahoney</p></summary>
<p>

**Abstract:** We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than training with convex combinations of pairs of examples and their labels, we use noise-perturbed convex combinations of pairs of data points in both input and feature space. This method includes mixup and manifold mixup as special cases, but it has additional advantages, including better smoothing of decision boundaries and enabling improved model robustness. We provide theory to understand this as well as the implicit regularization effects of NFM. Our theory is supported by empirical results, demonstrating the advantage of NFM, as compared to mixup and manifold mixup. We show that residual networks and vision transformers trained with NFM have favorable trade-offs between predictive accuracy on clean data and robustness with respect to various types of data perturbation across a range of computer vision benchmark datasets.

</p>
</details>

<details><summary><b>Top-N: Equivariant set and graph generation without exchangeability</b>
<a href="https://arxiv.org/abs/2110.02096">arxiv:2110.02096</a>
&#x1F4C8; 5 <br>
<p>Clement Vignac, Pascal Frossard</p></summary>
<p>

**Abstract:** We consider one-shot probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. These functions can be integrated into variational autoencoders (VAE), generative adversarial networks (GAN) or normalizing flows, and have important applications in drug discovery. Set and graph generation is most commonly performed by generating points (and sometimes edge weights) i.i.d. from a normal distribution, and processing them along with the prior vector using Transformer layers or graph neural networks. This architecture is designed to generate exchangeable distributions (all permutations of a set are equally likely) but it is hard to train due to the stochasticity of i.i.d. generation. We propose a new definition of equivariance and show that exchangeability is in fact unnecessary in VAEs and GANs. We then introduce Top-n, a deterministic, non-exchangeable set creation mechanism which learns to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any VAE or GAN -- it is easier to train and better captures complex dependencies in the data. Top-n outperforms i.i.d generation by 15% at SetMNIST reconstruction, generates sets that are 64% closer to the true distribution on a synthetic molecule-like dataset, and is able to generate more diverse molecules when trained on the classical QM9 dataset. With improved foundations in one-shot generation, our algorithm contributes to the design of more effective molecule generation methods.

</p>
</details>

<details><summary><b>Clustering of the Blendshape Facial Model</b>
<a href="https://arxiv.org/abs/2110.15313">arxiv:2110.15313</a>
&#x1F4C8; 4 <br>
<p>Stevo Rackoviƒá, Cl√°udia Soares, Du≈°an Jakovetiƒá, Zoranka Desnica, Relja Ljubobratoviƒá</p></summary>
<p>

**Abstract:** Digital human animation relies on high-quality 3D models of the human face -- rigs. A face rig must be accurate and, at the same time, fast to compute. One of the most common rigging models is the blendshape model. We present a novel approach for learning the inverse rig parameters at increased accuracy and decreased computational cost at the same time. It is based on a two-fold clustering of the blendshape face model. Our method focuses exclusively on the underlying space of deformation and produces clusters in both the mesh space and the controller space -- something that was not investigated in previous literature. This segmentation finds intuitive and meaningful connections between groups of vertices on the face and deformation controls, and further these segments can be observed independently. A separate model for solving the inverse rig problem is then learned for each segment. Our method is completely unsupervised and highly parallelizable.

</p>
</details>

<details><summary><b>COVIDRead: A Large-scale Question Answering Dataset on COVID-19</b>
<a href="https://arxiv.org/abs/2110.09321">arxiv:2110.09321</a>
&#x1F4C8; 4 <br>
<p>Tanik Saikh, Sovan Kumar Sahoo, Asif Ekbal, Pushpak Bhattacharyya</p></summary>
<p>

**Abstract:** During this pandemic situation, extracting any relevant information related to COVID-19 will be immensely beneficial to the community at large. In this paper, we present a very important resource, COVIDRead, a Stanford Question Answering Dataset (SQuAD) like dataset over more than 100k question-answer pairs. The dataset consists of Context-Answer-Question triples. Primarily the questions from the context are constructed in an automated way. After that, the system-generated questions are manually checked by hu-mans annotators. This is a precious resource that could serve many purposes, ranging from common people queries regarding this very uncommon disease to managing articles by editors/associate editors of a journal. We establish several end-to-end neural network based baseline models that attain the lowest F1 of 32.03% and the highest F1 of 37.19%. To the best of our knowledge, we are the first to provide this kind of QA dataset in such a large volume on COVID-19. This dataset creates a new avenue of carrying out research on COVID-19 by providing a benchmark dataset and a baseline model.

</p>
</details>

<details><summary><b>BI-RADS-Net: An Explainable Multitask Learning Approach for Cancer Diagnosis in Breast Ultrasound Images</b>
<a href="https://arxiv.org/abs/2110.04069">arxiv:2110.04069</a>
&#x1F4C8; 4 <br>
<p>Boyu Zhang, Aleksandar Vakanski, Min Xian</p></summary>
<p>

**Abstract:** In healthcare, it is essential to explain the decision-making process of machine learning models to establish the trustworthiness of clinicians. This paper introduces BI-RADS-Net, a novel explainable deep learning approach for cancer detection in breast ultrasound images. The proposed approach incorporates tasks for explaining and classifying breast tumors, by learning feature representations relevant to clinical diagnosis. Explanations of the predictions (benign or malignant) are provided in terms of morphological features that are used by clinicians for diagnosis and reporting in medical practice. The employed features include the BI-RADS descriptors of shape, orientation, margin, echo pattern, and posterior features. Additionally, our approach predicts the likelihood of malignancy of the findings, which relates to the BI-RADS assessment category reported by clinicians. Experimental validation on a dataset consisting of 1,192 images indicates improved model accuracy, supported by explanations in clinical terms using the BI-RADS lexicon.

</p>
</details>

<details><summary><b>Voice Aging with Audio-Visual Style Transfer</b>
<a href="https://arxiv.org/abs/2110.02411">arxiv:2110.02411</a>
&#x1F4C8; 4 <br>
<p>Justin Wilson, Sunyeong Park, Seunghye J. Wilson, Ming C. Lin</p></summary>
<p>

**Abstract:** Face aging techniques have used generative adversarial networks (GANs) and style transfer learning to transform one's appearance to look younger/older. Identity is maintained by conditioning these generative networks on a learned vector representation of the source content. In this work, we apply a similar approach to age a speaker's voice, referred to as voice aging. We first analyze the classification of a speaker's age by training a convolutional neural network (CNN) on the speaker's voice and face data from Common Voice and VoxCeleb datasets. We generate aged voices from style transfer to transform an input spectrogram to various ages and demonstrate our method on a mobile app.

</p>
</details>

<details><summary><b>FacialFilmroll: High-resolution multi-shot video editing</b>
<a href="https://arxiv.org/abs/2110.02124">arxiv:2110.02124</a>
&#x1F4C8; 4 <br>
<p>Bharath Bhushan Damodaran, Emmanuel Jolly, Gilles Puy, Philippe Henri Gosselin, C√©dric Th√©bault, Junghyun Ahn, Tim Christensen, Paul Ghezzo, Pierre Hellier</p></summary>
<p>

**Abstract:** We present FacialFilmroll, a solution for spatially and temporally consistent editing of faces in one or multiple shots. We build upon unwrap mosaic [Rav-Acha et al. 2008] by specializing it to faces. We leverage recent techniques to fit a 3D face model on monocular videos to (i) improve the quality of the mosaic for edition and (ii) permit the automatic transfer of edits from one shot to other shots of the same actor. We explain how FacialFilmroll is integrated in post-production facility. Finally, we present video editing results using FacialFilmroll on high resolution videos.

</p>
</details>

<details><summary><b>Double Encoder-Decoder Networks for Gastrointestinal Polyp Segmentation</b>
<a href="https://arxiv.org/abs/2110.01939">arxiv:2110.01939</a>
&#x1F4C8; 4 <br>
<p>Adrian Galdran, Gustavo Carneiro, Miguel A. Gonz√°lez Ballester</p></summary>
<p>

**Abstract:** Polyps represent an early sign of the development of Colorectal Cancer. The standard procedure for their detection consists of colonoscopic examination of the gastrointestinal tract. However, the wide range of polyp shapes and visual appearances, as well as the reduced quality of this image modality, turn their automatic identification and segmentation with computational tools into a challenging computer vision task. In this work, we present a new strategy for the delineation of gastrointestinal polyps from endoscopic images based on a direct extension of common encoder-decoder networks for semantic segmentation. In our approach, two pretrained encoder-decoder networks are sequentially stacked: the second network takes as input the concatenation of the original frame and the initial prediction generated by the first network, which acts as an attention mechanism enabling the second network to focus on interesting areas within the image, thereby improving the quality of its predictions. Quantitative evaluation carried out on several polyp segmentation databases shows that double encoder-decoder networks clearly outperform their single encoder-decoder counterparts in all cases. In addition, our best double encoder-decoder combination attains excellent segmentation accuracy and reaches state-of-the-art performance results in all the considered datasets, with a remarkable boost of accuracy on images extracted from datasets not used for training.

</p>
</details>

<details><summary><b>Frequency Aware Face Hallucination Generative Adversarial Network with Semantic Structural Constraint</b>
<a href="https://arxiv.org/abs/2110.01880">arxiv:2110.01880</a>
&#x1F4C8; 4 <br>
<p>Shailza Sharma, Abhinav Dhall, Vinay Kumar</p></summary>
<p>

**Abstract:** In this paper, we address the issue of face hallucination. Most current face hallucination methods rely on two-dimensional facial priors to generate high resolution face images from low resolution face images. These methods are only capable of assimilating global information into the generated image. Still there exist some inherent problems in these methods; such as, local features, subtle structural details and missing depth information in final output image. Present work proposes a Generative Adversarial Network (GAN) based novel progressive Face Hallucination (FH) network to address these issues present among current methods. The generator of the proposed model comprises of FH network and two sub-networks, assisting FH network to generate high resolution images. The first sub-network leverages on explicitly adding high frequency components into the model. To explicitly encode the high frequency components, an auto encoder is proposed to generate high resolution coefficients of Discrete Cosine Transform (DCT). To add three dimensional parametric information into the network, second sub-network is proposed. This network uses a shape model of 3D Morphable Models (3DMM) to add structural constraint to the FH network. Extensive experimentation results in the paper shows that the proposed model outperforms the state-of-the-art methods.

</p>
</details>

<details><summary><b>Hypernetworks for Continual Semi-Supervised Learning</b>
<a href="https://arxiv.org/abs/2110.01856">arxiv:2110.01856</a>
&#x1F4C8; 4 <br>
<p>Dhanajit Brahma, Vinay Kumar Verma, Piyush Rai</p></summary>
<p>

**Abstract:** Learning from data sequentially arriving, possibly in a non i.i.d. way, with changing task distribution over time is called continual learning. Much of the work thus far in continual learning focuses on supervised learning and some recent works on unsupervised learning. In many domains, each task contains a mix of labelled (typically very few) and unlabelled (typically plenty) training examples, which necessitates a semi-supervised learning approach. To address this in a continual learning setting, we propose a framework for semi-supervised continual learning called Meta-Consolidation for Continual Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns the meta-distribution that generates the weights of a semi-supervised auxiliary classifier generative adversarial network $(\textit{Semi-ACGAN})$ as the base network. We consolidate the knowledge of sequential tasks in the hypernetwork, and the base network learns the semi-supervised learning task. Further, we present $\textit{Semi-Split CIFAR-10}$, a new benchmark for continual semi-supervised learning, obtained by modifying the $\textit{Split CIFAR-10}$ dataset, in which the tasks with labelled and unlabelled data arrive sequentially. Our proposed model yields significant improvements in the continual semi-supervised learning setting. We compare the performance of several existing continual learning approaches on the proposed continual semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.

</p>
</details>

<details><summary><b>Numerisation D'un Siecle de Paysage Ferroviaire Fran√ßais : recul du rail, cons√©quences territoriales et co√ªt environnemental</b>
<a href="https://arxiv.org/abs/2111.03433">arxiv:2111.03433</a>
&#x1F4C8; 3 <br>
<p>Robert Jeansoulin</p></summary>
<p>

**Abstract:** The reconstruction of geographical data over a century, allows to figuring out the evolution of the French railway landscape, and how it has been impacted by major events (eg.: WWII), or longer time span processes : industry outsourcing, metropolization, public transport policies or absence of them. This work is resulting from the fusion of several public geographical data (SNCF, IGN), enriched with the computer-assisted addition of multiple data gathered on the Internet (Wikipedia, volunteer geographic information). The dataset compounds almost every rail stations (even simple stops) and railway branch nodes, whose link to their respective rail lines allows to build the underlying consistent graph of the network. Every rail line has a "valid to" date (or approx) so that time evolution can be displayed. The present progress of that reconstruction sums up to roughly 90% of what is expected (exact total unknown). This allows to consider temporal demographic analysis (how many cities and towns served by the railway since 1925 up on today), and environmental simulations as well (CO2 cost by given destination ).

</p>
</details>

<details><summary><b>Dataset Structural Index: Understanding a machine's perspective towards visual data</b>
<a href="https://arxiv.org/abs/2110.04070">arxiv:2110.04070</a>
&#x1F4C8; 3 <br>
<p>Dishant Parikh</p></summary>
<p>

**Abstract:** With advances in vision and perception architectures, we have realized that working with data is equally crucial, if not more, than the algorithms. Till today, we have trained machines based on our knowledge and perspective of the world. The entire concept of Dataset Structural Index(DSI) revolves around understanding a machine`s perspective of the dataset. With DSI, I show two meta values with which we can get more information over a visual dataset and use it to optimize data, create better architectures, and have an ability to guess which model would work best. These two values are the Variety contribution ratio and Similarity matrix. In the paper, I show many applications of DSI, one of which is how the same level of accuracy can be achieved with the same model architectures trained over less amount of data.

</p>
</details>

<details><summary><b>Revisiting SVD to generate powerful Node Embeddings for Recommendation Systems</b>
<a href="https://arxiv.org/abs/2110.03665">arxiv:2110.03665</a>
&#x1F4C8; 3 <br>
<p>Amar Budhiraja</p></summary>
<p>

**Abstract:** Graph Representation Learning (GRL) is an upcoming and promising area in recommendation systems. In this paper, we revisit the Singular Value Decomposition (SVD) of adjacency matrix for embedding generation of users and items and use a two-layer neural network on top of these embeddings to learn relevance between user-item pairs. Inspired by the success of higher-order learning in GRL, we further propose an extension of this method to include two-hop neighbors for SVD through the second order of the adjacency matrix and demonstrate improved performance compared with the simple SVD method which only uses one-hop neighbors. Empirical validation on three publicly available datasets of recommendation system demonstrates that the proposed methods, despite being simple, beat many state-of-the-art methods and for two of three datasets beats all of them up to a margin of 10%. Through our research, we want to shed light on the effectiveness of matrix factorization approaches, specifically SVD, in the deep learning era and show that these methods still contribute as important baselines in recommendation systems.

</p>
</details>

<details><summary><b>Data-Centric AI Requires Rethinking Data Notion</b>
<a href="https://arxiv.org/abs/2110.02491">arxiv:2110.02491</a>
&#x1F4C8; 3 <br>
<p>Mustafa Hajij, Ghada Zamzmi, Karthikeyan Natesan Ramamurthy, Aldo Guzman Saenz</p></summary>
<p>

**Abstract:** The transition towards data-centric AI requires revisiting data notions from mathematical and implementational standpoints to obtain unified data-centric machine learning packages. Towards this end, this work proposes unifying principles offered by categorical and cochain notions of data, and discusses the importance of these principles in data-centric AI transition. In the categorical notion, data is viewed as a mathematical structure that we act upon via morphisms to preserve this structure. As for cochain notion, data can be viewed as a function defined in a discrete domain of interest and acted upon via operators. While these notions are almost orthogonal, they provide a unifying definition to view data, ultimately impacting the way machine learning packages are developed, implemented, and utilized by practitioners.

</p>
</details>

<details><summary><b>The Power of Contrast for Feature Learning: A Theoretical Analysis</b>
<a href="https://arxiv.org/abs/2110.02473">arxiv:2110.02473</a>
&#x1F4C8; 3 <br>
<p>Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, Linjun Zhang</p></summary>
<p>

**Abstract:** Contrastive learning has achieved state-of-the-art performance in various self-supervised learning tasks and even outperforms its supervised counterpart. Despite its empirical success, theoretical understanding of why contrastive learning works is still limited. In this paper, (i) we provably show that contrastive learning outperforms autoencoder, a classical unsupervised learning method, for both feature recovery and downstream tasks; (ii) we also illustrate the role of labeled data in supervised contrastive learning. This provides theoretical support for recent findings that contrastive learning with labels improves the performance of learned representations in the in-domain downstream task, but it can harm the performance in transfer learning. We verify our theory with numerical experiments.

</p>
</details>

<details><summary><b>Feature Selection by a Mechanism Design</b>
<a href="https://arxiv.org/abs/2110.02419">arxiv:2110.02419</a>
&#x1F4C8; 3 <br>
<p>Xingwei Hu</p></summary>
<p>

**Abstract:** In constructing an econometric or statistical model, we pick relevant features or variables from many candidates. A coalitional game is set up to study the selection problem where the players are the candidates and the payoff function is a performance measurement in all possible modeling scenarios. Thus, in theory, an irrelevant feature is equivalent to a dummy player in the game, which contributes nothing to all modeling situations. The hypothesis test of zero mean contribution is the rule to decide a feature is irrelevant or not. In our mechanism design, the end goal perfectly matches the expected model performance with the expected sum of individual marginal effects. Within a class of noninformative likelihood among all modeling opportunities, the matching equation results in a specific valuation for each feature. After estimating the valuation and its standard deviation, we drop any candidate feature if its valuation is not significantly different from zero. In the simulation studies, our new approach significantly outperforms several popular methods used in practice, and its accuracy is robust to the choice of the payoff function.

</p>
</details>

<details><summary><b>CADA: Multi-scale Collaborative Adversarial Domain Adaptation for Unsupervised Optic Disc and Cup Segmentation</b>
<a href="https://arxiv.org/abs/2110.02417">arxiv:2110.02417</a>
&#x1F4C8; 3 <br>
<p>Peng Liu, Charlie T. Tran, Bin Kong, Ruogu Fang</p></summary>
<p>

**Abstract:** The diversity of retinal imaging devices poses a significant challenge: domain shift, which leads to performance degradation when applying the deep learning models trained on one domain to new testing domains. In this paper, we propose a multi-scale input along with multiple domain adaptors applied hierarchically in both feature and output spaces. The proposed training strategy and novel unsupervised domain adaptation framework, called Collaborative Adversarial Domain Adaptation (CADA), can effectively overcome the challenge. Multi-scale inputs can reduce the information loss due to the pooling layers used in the network for feature extraction, while our proposed CADA is an interactive paradigm that presents an exquisite collaborative adaptation through both adversarial learning and ensembling weights at different network layers. In particular, to produce a better prediction for the unlabeled target domain data, we simultaneously achieve domain invariance and model generalizability via adversarial learning at multi-scale outputs from different levels of network layers and maintaining an exponential moving average (EMA) of the historical weights during training. Without annotating any sample from the target domain, multiple adversarial losses in encoder and decoder layers guide the extraction of domain-invariant features to confuse the domain classifier. Meanwhile, the ensembling of weights via EMA reduces the uncertainty of adapting multiple discriminator learning. Comprehensive experimental results demonstrate that our CADA model incorporating multi-scale input training can overcome performance degradation and outperform state-of-the-art domain adaptation methods in segmenting retinal optic disc and cup from fundus images stemming from the REFUGE, Drishti-GS, and Rim-One-r3 datasets.

</p>
</details>

<details><summary><b>Geometric Algebra Attention Networks for Small Point Clouds</b>
<a href="https://arxiv.org/abs/2110.02393">arxiv:2110.02393</a>
&#x1F4C8; 3 <br>
<p>Matthew Spellings</p></summary>
<p>

**Abstract:** Much of the success of deep learning is drawn from building architectures that properly respect underlying symmetry and structure in the data on which they operate - a set of considerations that have been united under the banner of geometric deep learning. Often problems in the physical sciences deal with relatively small sets of points in two- or three-dimensional space wherein translation, rotation, and permutation equivariance are important or even vital for models to be useful in practice. In this work, we present rotation- and permutation-equivariant architectures for deep learning on these small point clouds, composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. We demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology.

</p>
</details>

<details><summary><b>EntQA: Entity Linking as Question Answering</b>
<a href="https://arxiv.org/abs/2110.02369">arxiv:2110.02369</a>
&#x1F4C8; 3 <br>
<p>Wenzheng Zhang, Wenyue Hua, Karl Stratos</p></summary>
<p>

**Abstract:** A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called EntQA, which stands for Entity linking as Question Answering. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.

</p>
</details>

<details><summary><b>Transformer Assisted Convolutional Network for Cell Instance Segmentation</b>
<a href="https://arxiv.org/abs/2110.02270">arxiv:2110.02270</a>
&#x1F4C8; 3 <br>
<p>Deepanshu Pandey, Pradyumna Gupta, Sumit Bhattacharya, Aman Sinha, Rohit Agarwal</p></summary>
<p>

**Abstract:** Region proposal based methods like R-CNN and Faster R-CNN models have proven to be extremely successful in object detection and segmentation tasks. Recently, Transformers have also gained popularity in the domain of Computer Vision, and are being utilised to improve the performance of conventional models. In this paper, we present a relatively new transformer based approach to enhance the performance of the conventional convolutional feature extractor in the existing region proposal based methods. Our approach merges the convolutional feature maps with transformer-based token embeddings by applying a projection operation similar to self-attention in transformers. The results of our experiments show that transformer assisted feature extractor achieves a significant improvement in mIoU (mean Intersection over Union) scores compared to vanilla convolutional backbone.

</p>
</details>

<details><summary><b>Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation</b>
<a href="https://arxiv.org/abs/2110.02196">arxiv:2110.02196</a>
&#x1F4C8; 3 <br>
<p>Dorothy Cheng, Edmund Y. Lam</p></summary>
<p>

**Abstract:** Transfer learning (TL) for medical image segmentation helps deep learning models achieve more accurate performances when there are scarce medical images. This study focuses on completing segmentation of the ribs from lung ultrasound images and finding the best TL technique with U-Net, a convolutional neural network for precise and fast image segmentation. Two approaches of TL were used, using a pre-trained VGG16 model to build the U-Net (V-Unet) and pre-training U-Net network with grayscale natural salient object dataset (X-Unet). Visual results and dice coefficients (DICE) of the models were compared. X-Unet showed more accurate and artifact-free visual performances on the actual mask prediction, despite its lower DICE than V-Unet. A partial-frozen network fine-tuning (FT) technique was also applied to X-Unet to compare results between different FT strategies, which FT all layers slightly outperformed freezing part of the network. The effect of dataset sizes was also evaluated, showing the importance of the combination between TL and data augmentation.

</p>
</details>

<details><summary><b>Secure Aggregation for Buffered Asynchronous Federated Learning</b>
<a href="https://arxiv.org/abs/2110.02177">arxiv:2110.02177</a>
&#x1F4C8; 3 <br>
<p>Jinhyun So, Ramy E. Ali, Ba≈üak G√ºler, A. Salman Avestimehr</p></summary>
<p>

**Abstract:** Federated learning (FL) typically relies on synchronous training, which is slow due to stragglers. While asynchronous training handles stragglers efficiently, it does not ensure privacy due to the incompatibility with the secure aggregation protocols. A buffered asynchronous training protocol known as FedBuff has been proposed recently which bridges the gap between synchronous and asynchronous training to mitigate stragglers and to also ensure privacy simultaneously. FedBuff allows the users to send their updates asynchronously while ensuring privacy by storing the updates in a trusted execution environment (TEE) enabled private buffer. TEEs, however, have limited memory which limits the buffer size. Motivated by this limitation, we develop a buffered asynchronous secure aggregation (BASecAgg) protocol that does not rely on TEEs. The conventional secure aggregation protocols cannot be applied in the buffered asynchronous setting since the buffer may have local models corresponding to different rounds and hence the masks that the users use to protect their models may not cancel out. BASecAgg addresses this challenge by carefully designing the masks such that they cancel out even if they correspond to different rounds. Our convergence analysis and experiments show that BASecAgg almost has the same convergence guarantees as FedBuff without relying on TEEs.

</p>
</details>

<details><summary><b>FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions</b>
<a href="https://arxiv.org/abs/2110.02035">arxiv:2110.02035</a>
&#x1F4C8; 3 <br>
<p>David Amat Ol√≥ndriz, Pon√ß Palau Puigdevall, Adri√† Salvador Palau</p></summary>
<p>

**Abstract:** In this paper we introduce the Food Drinks and groceries Images Multi Lingual (FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M store names, product names descriptions, and collection sections gathered from the Glovo application. The data made available corresponds to food, drinks and groceries products from 37 countries in Europe, the Middle East, Africa and Latin America. The dataset comprehends 33 languages, including 870K samples of languages of countries from Eastern Europe and Western Asia such as Ukrainian and Kazakh, which have been so far underrepresented in publicly available visio-linguistic datasets. The dataset also includes widely spoken languages such as Spanish and English. To assist further research, we include a benchmark over the text-image retrieval task using ADAPT, a SotA existing technique.

</p>
</details>

<details><summary><b>FoodChem: A food-chemical relation extraction model</b>
<a href="https://arxiv.org/abs/2110.02019">arxiv:2110.02019</a>
&#x1F4C8; 3 <br>
<p>Gjorgjina Cenikj, Barbara Korou≈°iƒá Seljak, Tome Eftimov</p></summary>
<p>

**Abstract:** In this paper, we present FoodChem, a new Relation Extraction (RE) model for identifying chemicals present in the composition of food entities, based on textual information provided in biomedical peer-reviewed scientific literature. The RE task is treated as a binary classification problem, aimed at identifying whether the contains relation exists between a food-chemical entity pair. This is accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models. For evaluation purposes, a novel dataset with annotated contains relations in food-chemical entity pairs is generated, in a golden and silver version. The models are integrated into a voting scheme in order to produce the silver version of the dataset which we use for augmenting the individual models, while the manually annotated golden version is used for their evaluation. Out of the three evaluated models, the BioBERT model achieves the best results, with a macro averaged F1 score of 0.902 in the unbalanced augmentation setting.

</p>
</details>

<details><summary><b>Joint inference of multiple graphs with hidden variables from stationary graph signals</b>
<a href="https://arxiv.org/abs/2110.03666">arxiv:2110.03666</a>
&#x1F4C8; 2 <br>
<p>Samuel Rey, Andrei Buciulea, Madeline Navarro, Santiago Segarra, Antonio G. Marques</p></summary>
<p>

**Abstract:** Learning graphs from sets of nodal observations represents a prominent problem formally known as graph topology inference. However, current approaches are limited by typically focusing on inferring single networks, and they assume that observations from all nodes are available. First, many contemporary setups involve multiple related networks, and second, it is often the case that only a subset of nodes is observed while the rest remain hidden. Motivated by these facts, we introduce a joint graph topology inference method that models the influence of the hidden variables. Under the assumptions that the observed signals are stationary on the sought graphs and the graphs are closely related, the joint estimation of multiple networks allows us to exploit such relationships to improve the quality of the learned graphs. Moreover, we confront the challenging problem of modeling the influence of the hidden nodes to minimize their detrimental effect. To obtain an amenable approach, we take advantage of the particular structure of the setup at hand and leverage the similarity between the different graphs, which affects both the observed and the hidden nodes. To test the proposed method, numerical simulations over synthetic and real-world graphs are provided.

</p>
</details>

<details><summary><b>PWG-IDS: An Intrusion Detection Model for Solving Class Imbalance in IIoT Networks Using Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2110.03445">arxiv:2110.03445</a>
&#x1F4C8; 2 <br>
<p>Lei Zhang, Shuaimin Jiang, Xiajiong Shen, Brij B. Gupta, Zhihong Tian</p></summary>
<p>

**Abstract:** With the continuous development of industrial IoT (IIoT) technology, network security is becoming more and more important. And intrusion detection is an important part of its security. However, since the amount of attack traffic is very small compared to normal traffic, this imbalance makes intrusion detection in it very difficult. To address this imbalance, an intrusion detection system called pretraining Wasserstein generative adversarial network intrusion detection system (PWG-IDS) is proposed in this paper. This system is divided into two main modules: 1) In this module, we introduce the pretraining mechanism in the Wasserstein generative adversarial network with gradient penalty (WGAN-GP) for the first time, firstly using the normal network traffic to train the WGAN-GP, and then inputting the imbalance data into the pre-trained WGAN-GP to retrain and generate the final required data. 2) Intrusion detection module: We use LightGBM as the classification algorithm to detect attack traffic in IIoT networks. The experimental results show that our proposed PWG-IDS outperforms other models, with F1-scores of 99% and 89% on the 2 datasets, respectively. And the pretraining mechanism we proposed can also be widely used in other GANs, providing a new way of thinking for the training of GANs.

</p>
</details>

<details><summary><b>Detecting and Quantifying Malicious Activity with Simulation-based Inference</b>
<a href="https://arxiv.org/abs/2110.02483">arxiv:2110.02483</a>
&#x1F4C8; 2 <br>
<p>Andrew Gambardella, Bogdan State, Naeemullah Khan, Leo Tsourides, Philip H. S. Torr, Atƒ±lƒ±m G√ºne≈ü Baydin</p></summary>
<p>

**Abstract:** We propose the use of probabilistic programming techniques to tackle the malicious user identification problem in a recommendation algorithm. Probabilistic programming provides numerous advantages over other techniques, including but not limited to providing a disentangled representation of how malicious users acted under a structured model, as well as allowing for the quantification of damage caused by malicious users. We show experiments in malicious user identification using a model of regular and malicious users interacting with a simple recommendation algorithm, and provide a novel simulation-based measure for quantifying the effects of a user or group of users on its dynamics.

</p>
</details>

<details><summary><b>Exponentially Many Local Minima in Quantum Neural Networks</b>
<a href="https://arxiv.org/abs/2110.02479">arxiv:2110.02479</a>
&#x1F4C8; 2 <br>
<p>Xuchen You, Xiaodi Wu</p></summary>
<p>

**Abstract:** Quantum Neural Networks (QNNs), or the so-called variational quantum circuits, are important quantum applications both because of their similar promises as classical neural networks and because of the feasibility of their implementation on near-term intermediate-size noisy quantum machines (NISQ). However, the training task of QNNs is challenging and much less understood. We conduct a quantitative investigation on the landscape of loss functions of QNNs and identify a class of simple yet extremely hard QNN instances for training. Specifically, we show for typical under-parameterized QNNs, there exists a dataset that induces a loss function with the number of spurious local minima depending exponentially on the number of parameters. Moreover, we show the optimality of our construction by providing an almost matching upper bound on such dependence. While local minima in classical neural networks are due to non-linear activations, in quantum neural networks local minima appear as a result of the quantum interference phenomenon. Finally, we empirically confirm that our constructions can indeed be hard instances in practice with typical gradient-based optimizers, which demonstrates the practical value of our findings.

</p>
</details>

<details><summary><b>Can an AI agent hit a moving target?</b>
<a href="https://arxiv.org/abs/2110.02474">arxiv:2110.02474</a>
&#x1F4C8; 2 <br>
<p> Rui,  Shi</p></summary>
<p>

**Abstract:** As the economies we live in are evolving over time, it is imperative that economic agents in models form expectations that can adjust to changes in the environment. This exercise offers a plausible expectation formation model that connects to computer science, psychology and neural science research on learning and decision-making, and applies it to an economy with a policy regime change. Employing the actor-critic model of reinforcement learning, the agent born in a fresh environment learns through first interacting with the environment. This involves taking exploratory actions and observing the corresponding stimulus signals. This interactive experience is then used to update its subjective belief about the world. I show, through several simulation experiments, that the agent adjusts its subjective belief facing an increase of inflation target. Moreover, the subjective belief evolves according to the agent's experience in the world.

</p>
</details>

<details><summary><b>Post-hoc Models for Performance Estimation of Machine Learning Inference</b>
<a href="https://arxiv.org/abs/2110.02459">arxiv:2110.02459</a>
&#x1F4C8; 2 <br>
<p>Xuechen Zhang, Samet Oymak, Jiasi Chen</p></summary>
<p>

**Abstract:** Estimating how well a machine learning model performs during inference is critical in a variety of scenarios (for example, to quantify uncertainty, or to choose from a library of available models). However, the standard accuracy estimate of softmax confidence is not versatile and cannot reliably predict different performance metrics (e.g., F1-score, recall) or the performance in different application scenarios or input domains. In this work, we systematically generalize performance estimation to a diverse set of metrics and scenarios and discuss generalized notions of uncertainty calibration. We propose the use of post-hoc models to accomplish this goal and investigate design parameters, including the model type, feature engineering, and performance metric, to achieve the best estimation quality. Emphasis is given to object detection problems and, unlike prior work, our approach enables the estimation of per-image metrics such as recall and F1-score. Through extensive experiments with computer vision models and datasets in three use cases -- mobile edge offloading, model selection, and dataset shift -- we find that proposed post-hoc models consistently outperform the standard calibrated confidence baselines. To the best of our knowledge, this is the first work to develop a unified framework to address different performance estimation problems for machine learning inference.

</p>
</details>

<details><summary><b>VC dimension of partially quantized neural networks in the overparametrized regime</b>
<a href="https://arxiv.org/abs/2110.02456">arxiv:2110.02456</a>
&#x1F4C8; 2 <br>
<p>Yutong Wang, Clayton D. Scott</p></summary>
<p>

**Abstract:** Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs match the performance of state-of-the-art full-precision models.

</p>
</details>

<details><summary><b>Ripple Attention for Visual Perception with Sub-quadratic Complexity</b>
<a href="https://arxiv.org/abs/2110.02453">arxiv:2110.02453</a>
&#x1F4C8; 2 <br>
<p>Lin Zheng, Huijie Pan, Lingpeng Kong</p></summary>
<p>

**Abstract:** Transformer architectures are now central to modeling in natural language processing tasks. At its heart is the attention mechanism, which enables effective modeling of long-term dependencies in a sequence. Recently, transformers have been successfully applied in the computer vision domain, where 2D images are first segmented into patches and then treated as 1D sequences. Such linearization, however, impairs the notion of spatial locality in images, which bears important visual clues. To bridge the gap, we propose ripple attention, a sub-quadratic attention mechanism for visual perception. In ripple attention, contributions of different tokens to a query are weighted with respect to their relative spatial distances in the 2D space. To favor correlations with vicinal tokens yet permit long-term dependencies, we derive the spatial weights through a stick-breaking transformation. We further design a dynamic programming algorithm that computes weighted contributions for all queries in linear observed time, taking advantage of the summed-area table and recent advances in linearized attention. Extensive experiments and analyses demonstrate the effectiveness of ripple attention on various visual tasks.

</p>
</details>

<details><summary><b>Task Affinity with Maximum Bipartite Matching in Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2110.02399">arxiv:2110.02399</a>
&#x1F4C8; 2 <br>
<p>Cat P. Le, Juncheng Dong, Mohammadreza Soltani, Vahid Tarokh</p></summary>
<p>

**Abstract:** We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the affinity score to propose a novel algorithm for the few-shot learning problem. In particular, using this score, we find relevant training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning a few-shot model. Results on various few-shot benchmark datasets demonstrate the efficacy of the proposed approach by improving the classification accuracy over the state-of-the-art methods even when using smaller models.

</p>
</details>

<details><summary><b>Fast and Interpretable Consensus Clustering via Minipatch Learning</b>
<a href="https://arxiv.org/abs/2110.02388">arxiv:2110.02388</a>
&#x1F4C8; 2 <br>
<p>Luqin Gan, Genevera I. Allen</p></summary>
<p>

**Abstract:** Consensus clustering has been widely used in bioinformatics and other applications to improve the accuracy, stability and reliability of clustering results. This approach ensembles cluster co-occurrences from multiple clustering runs on subsampled observations. For application to large-scale bioinformatics data, such as to discover cell types from single-cell sequencing data, for example, consensus clustering has two significant drawbacks: (i) computational inefficiency due to repeatedly applying clustering algorithms, and (ii) lack of interpretability into the important features for differentiating clusters. In this paper, we address these two challenges by developing IMPACC: Interpretable MiniPatch Adaptive Consensus Clustering. Our approach adopts three major innovations. We ensemble cluster co-occurrences from tiny subsets of both observations and features, termed minipatches, thus dramatically reducing computation time. Additionally, we develop adaptive sampling schemes for observations, which result in both improved reliability and computational savings, as well as adaptive sampling schemes of features, which leads to interpretable solutions by quickly learning the most relevant features that differentiate clusters. We study our approach on synthetic data and a variety of real large-scale bioinformatics data sets; results show that our approach not only yields more accurate and interpretable cluster solutions, but it also substantially improves computational efficiency compared to standard consensus clustering approaches.

</p>
</details>

<details><summary><b>Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning</b>
<a href="https://arxiv.org/abs/2110.02370">arxiv:2110.02370</a>
&#x1F4C8; 2 <br>
<p>Christopher Michael Rytting, David Wingate</p></summary>
<p>

**Abstract:** Large natural language models (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of \textit{compositional learning}, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task.

</p>
</details>

<details><summary><b>HYPER: Learned Hybrid Trajectory Prediction via Factored Inference and Adaptive Sampling</b>
<a href="https://arxiv.org/abs/2110.02344">arxiv:2110.02344</a>
&#x1F4C8; 2 <br>
<p>Xin Huang, Guy Rosman, Igor Gilitschenski, Ashkan Jasour, Stephen G. McGill, John J. Leonard, Brian C. Williams</p></summary>
<p>

**Abstract:** Modeling multi-modal high-level intent is important for ensuring diversity in trajectory prediction. Existing approaches explore the discrete nature of human intent before predicting continuous trajectories, to improve accuracy and support explainability. However, these approaches often assume the intent to remain fixed over the prediction horizon, which is problematic in practice, especially over longer horizons. To overcome this limitation, we introduce HYPER, a general and expressive hybrid prediction framework that models evolving human intent. By modeling traffic agents as a hybrid discrete-continuous system, our approach is capable of predicting discrete intent changes over time. We learn the probabilistic hybrid model via a maximum likelihood estimation problem and leverage neural proposal distributions to sample adaptively from the exponentially growing discrete space. The overall approach affords a better trade-off between accuracy and coverage. We train and validate our model on the Argoverse dataset, and demonstrate its effectiveness through comprehensive ablation studies and comparisons with state-of-the-art models.

</p>
</details>

<details><summary><b>Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis</b>
<a href="https://arxiv.org/abs/2110.02334">arxiv:2110.02334</a>
&#x1F4C8; 2 <br>
<p>Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, Thamar Solorio</p></summary>
<p>

**Abstract:** Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing user-generated reviews to determine (i) the target being evaluated, (ii) the aspect category to which it belongs, and (iii) the sentiment expressed towards the target and aspect pair. In this article, we propose transforming ABSA into an abstract summary-like conditional text generation task that uses targets, aspects, and polarities to generate auxiliary statements. To demonstrate the efficacy of our task formulation and a proposed system, we fine-tune a pre-trained model for conditional text generation tasks to get new state-of-the-art results on a few restaurant domains and urban neighborhoods domain benchmark datasets.

</p>
</details>

<details><summary><b>On the Impact of Stable Ranks in Deep Nets</b>
<a href="https://arxiv.org/abs/2110.02333">arxiv:2110.02333</a>
&#x1F4C8; 2 <br>
<p>Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee, Georgios Arvanitidis</p></summary>
<p>

**Abstract:** A recent line of work has established intriguing connections between the generalization/compression properties of a deep neural network (DNN) model and the so-called layer weights' stable ranks. Intuitively, the latter are indicators of the effective number of parameters in the net. In this work, we address some natural questions regarding the space of DNNs conditioned on the layers' stable rank, where we study feed-forward dynamics, initialization, training and expressivity. To this end, we first propose a random DNN model with a new sampling scheme based on stable rank. Then, we show how feed-forward maps are affected by the constraint and how training evolves in the overparametrized regime (via Neural Tangent Kernels). Our results imply that stable ranks appear layerwise essentially as linear factors whose effect accumulates exponentially depthwise. Moreover, we provide empirical analysis suggesting that stable rank initialization alone can lead to convergence speed ups.

</p>
</details>

<details><summary><b>OTTR: Off-Road Trajectory Tracking using Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2110.02332">arxiv:2110.02332</a>
&#x1F4C8; 2 <br>
<p>Akhil Nagariya, Dileep Kalathil, Srikanth Saripalli</p></summary>
<p>

**Abstract:** In this work, we present a novel Reinforcement Learning (RL) algorithm for the off-road trajectory tracking problem. Off-road environments involve varying terrain types and elevations, and it is difficult to model the interaction dynamics of specific off-road vehicles with such a diverse and complex environment. Standard RL policies trained on a simulator will fail to operate in such challenging real-world settings. Instead of using a naive domain randomization approach, we propose an innovative supervised-learning based approach for overcoming the sim-to-real gap problem. Our approach efficiently exploits the limited real-world data available to adapt the baseline RL policy obtained using a simple kinematics simulator. This avoids the need for modeling the diverse and complex interaction of the vehicle with off-road environments. We evaluate the performance of the proposed algorithm using two different off-road vehicles, Warthog and Moose. Compared to the standard ILQR approach, our proposed approach achieves a 30% and 50% reduction in cross track error in Warthog and Moose, respectively, by utilizing only 30 minutes of real-world driving data.

</p>
</details>

<details><summary><b>Phoebe: A Learning-based Checkpoint Optimizer</b>
<a href="https://arxiv.org/abs/2110.02313">arxiv:2110.02313</a>
&#x1F4C8; 2 <br>
<p>Yiwen Zhu, Matteo Interlandi, Abhishek Roy, Krishnadhan Das, Hiren Patel, Malay Bag, Hitesh Sharma, Alekh Jindal</p></summary>
<p>

**Abstract:** Easy-to-use programming interfaces paired with cloud-scale processing engines have enabled big data system users to author arbitrarily complex analytical jobs over massive volumes of data. However, as the complexity and scale of analytical jobs increase, they encounter a number of unforeseen problems, hotspots with large intermediate data on temporary storage, longer job recovery time after failures, and worse query optimizer estimates being examples of issues that we are facing at Microsoft.
  To address these issues, we propose Phoebe, an efficient learning-based checkpoint optimizer. Given a set of constraints and an objective function at compile-time, Phoebe is able to determine the decomposition of job plans, and the optimal set of checkpoints to preserve their outputs to durable global storage. Phoebe consists of three machine learning predictors and one optimization module. For each stage of a job, Phoebe makes accurate predictions for: (1) the execution time, (2) the output size, and (3) the start/end time taking into account the inter-stage dependencies. Using these predictions, we formulate checkpoint optimization as an integer programming problem and propose a scalable heuristic algorithm that meets the latency requirement of the production environment.
  We demonstrate the effectiveness of Phoebe in production workloads, and show that we can free the temporary storage on hotspots by more than 70% and restart failed jobs 68% faster on average with minimum performance impact. Phoebe also illustrates that adding multiple sets of checkpoints is not cost-efficient, which dramatically reduces the complexity of the optimization.

</p>
</details>

<details><summary><b>On the Correspondence between Gaussian Processes and Geometric Harmonics</b>
<a href="https://arxiv.org/abs/2110.02296">arxiv:2110.02296</a>
&#x1F4C8; 2 <br>
<p>Felix Dietrich, Juan M. Bello-Rivas, Ioannis G. Kevrekidis</p></summary>
<p>

**Abstract:** We discuss the correspondence between Gaussian process regression and Geometric Harmonics, two similar kernel-based methods that are typically used in different contexts. Research communities surrounding the two concepts often pursue different goals. Results from both camps can be successfully combined, providing alternative interpretations of uncertainty in terms of error estimation, or leading towards accelerated Bayesian Optimization due to dimensionality reduction.

</p>
</details>

<details><summary><b>Co-training an Unsupervised Constituency Parser with Weak Supervision</b>
<a href="https://arxiv.org/abs/2110.02283">arxiv:2110.02283</a>
&#x1F4C8; 2 <br>
<p>Nickil Maveli, Shay B. Cohen</p></summary>
<p>

**Abstract:** We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results.\footnote{For code or data, please contact the authors.}

</p>
</details>

<details><summary><b>Turing approximations, toric isometric embeddings & manifold convolutions</b>
<a href="https://arxiv.org/abs/2110.02279">arxiv:2110.02279</a>
&#x1F4C8; 2 <br>
<p>P. Su√°rez-Serrato</p></summary>
<p>

**Abstract:** Convolutions are fundamental elements in deep learning architectures. Here, we present a theoretical framework for combining extrinsic and intrinsic approaches to manifold convolution through isometric embeddings into tori. In this way, we define a convolution operator for a manifold of arbitrary topology and dimension. We also explain geometric and topological conditions that make some local definitions of convolutions which rely on translating filters along geodesic paths on a manifold, computationally intractable. A result of Alan Turing from 1938 underscores the need for such a toric isometric embedding approach to achieve a global definition of convolution on computable, finite metric space approximations to a smooth manifold.

</p>
</details>

<details><summary><b>SeanNet: Semantic Understanding Network for Localization Under Object Dynamics</b>
<a href="https://arxiv.org/abs/2110.02276">arxiv:2110.02276</a>
&#x1F4C8; 2 <br>
<p>Xiao Li, Yidong Du, Zhen Zeng, Odest Chadwicke Jenkins</p></summary>
<p>

**Abstract:** We aim for domestic robots to operate indoor for long-term service. Under the object-level scene dynamics induced by human daily activities, a robot needs to robustly localize itself in the environment subject to scene uncertainties. Previous works have addressed visual-based localization in static environments, yet the object-level scene dynamics challenge existing methods on long-term deployment of the robot. This paper proposes SEmantic understANding Network (SeanNet) that enables robots to measure the similarity between two scenes on both visual and semantic aspects. We further develop a similarity-based localization method based on SeanNet for monitoring the progress of visual navigation tasks. In our experiments, we benchmarked SeanNet against baselines methods on scene similarity measures, as well as visual navigation performance once integrated with a visual navigator. We demonstrate that SeanNet outperforms all baseline methods, by robustly localizing the robot under object dynamics, thus reliably informing visual navigation about the task status.

</p>
</details>

<details><summary><b>Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints</b>
<a href="https://arxiv.org/abs/2110.02273">arxiv:2110.02273</a>
&#x1F4C8; 2 <br>
<p>Juan Carlos De los Reyes, David Villac√≠s</p></summary>
<p>

**Abstract:** We investigate a family of bilevel imaging learning problems where the lower-level instance corresponds to a convex variational model involving first- and second-order nonsmooth regularizers. By using geometric properties of the primal-dual reformulation of the lower-level problem and introducing suitable changes of variables, we are able to reformulate the original bilevel problems as Mathematical Programs with Complementarity Constraints (MPCC). For the latter, we prove tight constraint qualification conditions (MPCC-MFCQ and partial MPCC-LICQ) and derive Mordukovich (M-) and Strong (S-) stationarity conditions. The S-stationarity system for the MPCC turns also into S-stationarity conditions for the original formulation. Second-order sufficient optimality conditions are derived as well. The proposed reformulation may be extended to problems in function spaces, leading to MPCC's with additional constraints on the gradient of the state. Finally, we report on some numerical results obtained by using the proposed MPCC reformulations together with available large-scale nonlinear programming solvers.

</p>
</details>

<details><summary><b>Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR</b>
<a href="https://arxiv.org/abs/2110.02267">arxiv:2110.02267</a>
&#x1F4C8; 2 <br>
<p>Pablo Ortiz, Simen Burud</p></summary>
<p>

**Abstract:** We study the inclusion of past conversational context through BERT language models into a CTC-based Automatic Speech Recognition (ASR) system via N-best rescoring. We introduce a data-efficient strategy to fine-tune BERT on transcript disambiguation without external data. Our results show word error rate recoveries up to 37.2% with context-augmented BERT rescoring. We do this in low-resource data domains, both in language (Norwegian), tone (spontaneous, conversational), and topics (parliament proceedings and customer service phone calls). We show how the nature of the data greatly affects the performance of context-augmented N-best rescoring.

</p>
</details>

<details><summary><b>Contextual Combinatorial Volatile Bandits via Gaussian Processes</b>
<a href="https://arxiv.org/abs/2110.02248">arxiv:2110.02248</a>
&#x1F4C8; 2 <br>
<p>Andi Nika, Sepehr Elahi, Cem Tekin</p></summary>
<p>

**Abstract:** We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(K\sqrt{T\overlineŒ≥_{T}} )$ regret with high probability, where $\overlineŒ≥_{T}$ is the maximum information gain associated with the set of base arm contexts that appeared in the first $T$ rounds and $K$ is the maximum cardinality of any feasible action over all rounds. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.

</p>
</details>

<details><summary><b>Hybrid Classical-Quantum method for Diabetic Foot Ulcer Classification</b>
<a href="https://arxiv.org/abs/2110.02222">arxiv:2110.02222</a>
&#x1F4C8; 2 <br>
<p>Azadeh Alavi, Hossein Akhoundi</p></summary>
<p>

**Abstract:** Diabetes is a raising problem that affects many people globally. Diabetic patients are at risk of developing foot ulcer that usually leads to limb amputation, causing significant morbidity, and psychological distress. In order to develop a self monitoring mobile application, it is necessary to be able to classify such ulcers into either of the following classes: Infection, Ischaemia, None, or Both. In this work, we compare the performance of a classical transfer-learning-based method, with the performance of a hybrid classical-quantum Classifier on diabetic foot ulcer classification task. As such, we merge the pre-trained Xception network with a multi-class variational classifier. Thus, after modifying and re-training the Xception network, we extract the output of a mid-layer and employ it as deep-features presenters of the given images. Finally, we use those deep-features to train multi-class variational classifier, where each classifier is implemented on an individual variational circuit. The method is then evaluated on the blind test set DFUC2021. The results proves that our proposed hybrid classical-quantum Classifier leads to considerable improvement compared to solely relying on transfer learning concept through training the modified version of Xception network.

</p>
</details>

<details><summary><b>Using Psuedolabels for training Sentiment Classifiers makes the model generalize better across datasets</b>
<a href="https://arxiv.org/abs/2110.02200">arxiv:2110.02200</a>
&#x1F4C8; 2 <br>
<p>Natesh Reddy, Muktabh Mayank Srivastava</p></summary>
<p>

**Abstract:** The problem statement addressed in this work is : For a public sentiment classification API, how can we set up a classifier that works well on different types of data, having limited ability to annotate data from across domains. We show that given a large amount of unannotated data from across different domains and pseudolabels on this dataset generated by a classifier trained on a small annotated dataset from one domain, we can train a sentiment classifier that generalizes better across different datasets.

</p>
</details>

<details><summary><b>Machine learning attack on copy detection patterns: are 1x1 patterns cloneable?</b>
<a href="https://arxiv.org/abs/2110.02176">arxiv:2110.02176</a>
&#x1F4C8; 2 <br>
<p>Roman Chaban, Olga Taran, Joakim Tutt, Taras Holotyak, Slavi Bonev, Slava Voloshynovskiy</p></summary>
<p>

**Abstract:** Nowadays, the modern economy critically requires reliable yet cheap protection solutions against product counterfeiting for the mass market. Copy detection patterns (CDP) are considered as such solution in several applications. It is assumed that being printed at the maximum achievable limit of a printing resolution of an industrial printer with the smallest symbol size 1x1 elements, the CDP cannot be copied with sufficient accuracy and thus are unclonable. In this paper, we challenge this hypothesis and consider a copy attack against the CDP based on machine learning. The experimental based on samples produced on two industrial printers demonstrate that simple detection metrics used in the CDP authentication cannot reliably distinguish the original CDP from their fakes. Thus, the paper calls for a need of careful reconsideration of CDP cloneability and search for new authentication techniques and CDP optimization because of the current attack.

</p>
</details>

<details><summary><b>A Methodology to Identify Cognition Gaps in Visual Recognition Applications Based on Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2110.02080">arxiv:2110.02080</a>
&#x1F4C8; 2 <br>
<p>Hannes Vietz, Tristan Rauch, Andreas L√∂cklin, Nasser Jazdi, Michael Weyrich</p></summary>
<p>

**Abstract:** Developing consistently well performing visual recognition applications based on convolutional neural networks, e.g. for autonomous driving, is very challenging. One of the obstacles during the development is the opaqueness of their cognitive behaviour. A considerable amount of literature has been published which describes irrational behaviour of trained CNNs showcasing gaps in their cognition. In this paper, a methodology is presented that creates worstcase images using image augmentation techniques. If the CNN's cognitive performance on such images is weak while the augmentation techniques are supposedly harmless, a potential gap in the cognition has been found. The presented worst-case image generator is using adversarial search approaches to efficiently identify the most challenging image. This is evaluated with the well-known AlexNet CNN using images depicting a typical driving scenario.

</p>
</details>

<details><summary><b>Sound Event Detection Transformer: An Event-based End-to-End Model for Sound Event Detection</b>
<a href="https://arxiv.org/abs/2110.02011">arxiv:2110.02011</a>
&#x1F4C8; 2 <br>
<p>Zhirong Ye, Xiangdong Wang, Hong Liu, Yueliang Qian, Rui Tao, Long Yan, Kazushige Ouchi</p></summary>
<p>

**Abstract:** Sound event detection (SED) has gained increasing attention with its wide application in surveillance, video indexing, etc. Existing models in SED mainly generate frame-level prediction, converting it into a sequence multi-label classification problem. A critical issue with the frame-based model is that it pursues the best frame-level prediction rather than the best event-level prediction. Besides, it needs post-processing and cannot be trained in an end-to-end way. This paper firstly presents the one-dimensional Detection Transformer (1D-DETR), inspired by Detection Transformer for image object detection. Furthermore, given the characteristics of SED, the audio query branch and a one-to-many matching strategy for fine-tuning the model are added to 1D-DETR to form Sound Event Detection Transformer (SEDT). To our knowledge, SEDT is the first event-based and end-to-end SED model. Experiments are conducted on the URBAN-SED dataset and the DCASE2019 Task4 dataset, and both show that SEDT can achieve competitive performance.

</p>
</details>

<details><summary><b>Distribution Mismatch Correction for Improved Robustness in Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2110.01955">arxiv:2110.01955</a>
&#x1F4C8; 2 <br>
<p>Alexander Fuchs, Christian Knoll, Franz Pernkopf</p></summary>
<p>

**Abstract:** Deep neural networks rely heavily on normalization methods to improve their performance and learning behavior. Although normalization methods spurred the development of increasingly deep and efficient architectures, they also increase the vulnerability with respect to noise and input corruptions. In most applications, however, noise is ubiquitous and diverse; this can often lead to complete failure of machine learning systems as they fail to cope with mismatches between the input distribution during training- and test-time. The most common normalization method, batch normalization, reduces the distribution shift during training but is agnostic to changes in the input distribution during test time. This makes batch normalization prone to performance degradation whenever noise is present during test-time. Sample-based normalization methods can correct linear transformations of the activation distribution but cannot mitigate changes in the distribution shape; this makes the network vulnerable to distribution changes that cannot be reflected in the normalization parameters. We propose an unsupervised non-parametric distribution correction method that adapts the activation distribution of each layer. This reduces the mismatch between the training and test-time distribution by minimizing the 1-D Wasserstein distance. In our experiments, we empirically show that the proposed method effectively reduces the impact of intense image corruptions and thus improves the classification performance without the need for retraining or fine-tuning the model.

</p>
</details>

<details><summary><b>AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset</b>
<a href="https://arxiv.org/abs/2110.01948">arxiv:2110.01948</a>
&#x1F4C8; 2 <br>
<p>Mohamed Seghir Hadj Ameur, Hassina Aliane</p></summary>
<p>

**Abstract:** Coronavirus disease (COVID-19) is an infectious respiratory disease that was first discovered in late December 2019, in Wuhan, China, and then spread worldwide causing a lot of panic and death. Users of social networking sites such as Facebook and Twitter have been focused on reading, publishing, and sharing novelties, tweets, and articles regarding the newly emerging pandemic. A lot of these users often employ sarcasm to convey their intended meaning in a humorous, funny, and indirect way making it hard for computer-based applications to automatically understand and identify their goal and the harm level that they can inflect. Motivated by the emerging need for annotated datasets that tackle these kinds of problems in the context of COVID-19, this paper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19 sarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the practical utility of the built dataset, it has been carefully analyzed and tested using several classification models.

</p>
</details>

<details><summary><b>CNN-based Human Detection for UAVs in Search and Rescue</b>
<a href="https://arxiv.org/abs/2110.01930">arxiv:2110.01930</a>
&#x1F4C8; 2 <br>
<p>Nikite Mesvan</p></summary>
<p>

**Abstract:** The use of Unmanned Aerial Vehicles (UAVs) as a substitute for ordinary vehicles in applications of search and rescue is being studied all over the world due to its flexible mobility and less obstruction, including two main tasks: search and rescue. This paper proposes an approach for the first task of searching and detecting victims using a type of convolutional neural network technique, the Single Shot Detector (SSD) model, with the Quadcopter hardware platform, a type of UAVs. The model used in the research is a pre-trained model and is applied to test on a Raspberry Pi model B, which is attached on a Quadcopter, while a single camera is equipped at the bottom of the Quadcopter to look from above for search and detection. The Quadcopter in this research is a DIY hardware model that uses accelerometer and gyroscope sensors and ultrasonic sensor as the essential components for balancing control, however, these sensors are susceptible to noise caused by the driving forces on the model, such as the vibration of the motors, therefore, the issues about the PID controller, noise processing for the sensors are also mentioned in the paper. Experimental results proved that the Quadcopter is able to stably flight and the SSD model works well on the Raspberry Pi model B with a processing speed of 3 fps and produces the best detection results at the distance of 1 to 20 meters to objects.

</p>
</details>

<details><summary><b>Decentralized Cooperative Lane Changing at Freeway Weaving Areas Using Multi-Agent Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2110.08124">arxiv:2110.08124</a>
&#x1F4C8; 1 <br>
<p>Yi Hou, Peter Graf</p></summary>
<p>

**Abstract:** Frequent lane changes during congestion at freeway bottlenecks such as merge and weaving areas further reduce roadway capacity. The emergence of deep reinforcement learning (RL) and connected and automated vehicle technology provides a possible solution to improve mobility and energy efficiency at freeway bottlenecks through cooperative lane changing. Deep RL is a collection of machine-learning methods that enables an agent to improve its performance by learning from the environment. In this study, a decentralized cooperative lane-changing controller was developed using proximal policy optimization by adopting a multi-agent deep RL paradigm. In the decentralized control strategy, policy learning and action reward are evaluated locally, with each agent (vehicle) getting access to global state information. Multi-agent deep RL requires lower computational resources and is more scalable than single-agent deep RL, making it a powerful tool for time-sensitive applications such as cooperative lane changing. The results of this study show that cooperative lane changing enabled by multi-agent deep RL yields superior performance to human drivers in term of traffic throughput, vehicle speed, number of stops per vehicle, vehicle fuel efficiency, and emissions. The trained RL policy is transferable and can be generalized to uncongested, moderately congested, and extremely congested traffic conditions.

</p>
</details>

<details><summary><b>Investigating Health-Aware Smart-Nudging with Machine Learning to Help People Pursue Healthier Eating-Habits</b>
<a href="https://arxiv.org/abs/2110.07045">arxiv:2110.07045</a>
&#x1F4C8; 1 <br>
<p>Mansura A Khan, Khalil Muhammad, Barry Smyth, David Coyle</p></summary>
<p>

**Abstract:** Food-choices and eating-habits directly contribute to our long-term health. This makes the food recommender system a potential tool to address the global crisis of obesity and malnutrition. Over the past decade, artificial-intelligence and medical researchers became more invested in researching tools that can guide and help people make healthy and thoughtful decisions around food and diet. In many typical (Recommender System) RS domains, smart nudges have been proven effective in shaping users' consumption patterns. In recent years, knowledgeable nudging and incentifying choices started getting attention in the food domain as well. To develop smart nudging for promoting healthier food choices, we combined Machine Learning and RS technology with food-healthiness guidelines from recognized health organizations, such as the World Health Organization, Food Standards Agency, and the National Health Service United Kingdom. In this paper, we discuss our research on, persuasive visualization for making users aware of the healthiness of the recommended recipes. Here, we propose three novel nudging technology, the WHO-BubbleSlider, the FSA-ColorCoading, and the DRCI-MLCP, that encourage users to choose healthier recipes. We also propose a Topic Modeling based portion-size recommendation algorithm. To evaluate our proposed smart-nudges, we conducted an online user study with 96 participants and 92250 recipes. Results showed that, during the food decision-making process, appropriate healthiness cues make users more likely to click, browse, and choose healthier recipes over less healthy ones.

</p>
</details>

<details><summary><b>Convex-Concave Min-Max Stackelberg Games</b>
<a href="https://arxiv.org/abs/2110.05192">arxiv:2110.05192</a>
&#x1F4C8; 1 <br>
<p>Denizalp Goktas, Amy Greenwald</p></summary>
<p>

**Abstract:** Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald's maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate the efficacy and efficiency of our algorithms in practice by computing competitive equilibria in Fisher markets with varying utility structures. Our experiments suggest potential ways to extend our theoretical results, by demonstrating how different smoothness properties can affect the convergence rate of our algorithms.

</p>
</details>

<details><summary><b>Decoding ECoG signal into 3D hand translation using deep learning</b>
<a href="https://arxiv.org/abs/2110.03528">arxiv:2110.03528</a>
&#x1F4C8; 1 <br>
<p>Maciej ≈öliwowski, Matthieu Martin, Antoine Souloumiac, Pierre Blanchart, Tetiana Aksenova</p></summary>
<p>

**Abstract:** Motor brain-computer interfaces (BCIs) are a promising technology that may enable motor-impaired people to interact with their environment. Designing real-time and accurate BCI is crucial to make such devices useful, safe, and easy to use by patients in a real-life environment. Electrocorticography (ECoG)-based BCIs emerge as a good compromise between invasiveness of the recording device and good spatial and temporal resolution of the recorded signal. However, most ECoG signal decoders used to predict continuous hand movements are linear models. These models have a limited representational capacity and may fail to capture the relationship between ECoG signal and continuous hand movements. Deep learning (DL) models, which are state-of-the-art in many problems, could be a solution to better capture this relationship. In this study, we tested several DL-based architectures to predict imagined 3D continuous hand translation using time-frequency features extracted from ECoG signals. The dataset used in the analysis is a part of a long-term clinical trial (ClinicalTrials.gov identifier: NCT02550522) and was acquired during a closed-loop experiment with a tetraplegic subject. The proposed architectures include multilayer perceptron (MLP), convolutional neural networks (CNN), and long short-term memory networks (LSTM). The accuracy of the DL-based and multilinear models was compared offline using cosine similarity. Our results show that CNN-based architectures outperform the current state-of-the-art multilinear model. The best architecture exploited the spatial correlation between neighboring electrodes with CNN and benefited from the sequential character of the desired hand trajectory by using LSTMs. Overall, DL increased the average cosine similarity, compared to the multilinear model, by up to 60%, from 0.189 to 0.302 and from 0.157 to 0.249 for the left and right hand, respectively.

</p>
</details>

<details><summary><b>Unpacking the Black Box: Regulating Algorithmic Decisions</b>
<a href="https://arxiv.org/abs/2110.03443">arxiv:2110.03443</a>
&#x1F4C8; 1 <br>
<p>Laura Blattner, Scott Nelson, Jann Spiess</p></summary>
<p>

**Abstract:** We characterize optimal oversight of algorithms in a world where an agent designs a complex prediction function but a principal is limited in the amount of information she can learn about the prediction function. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the bias induced by misalignment between principal's and agent's preferences is small relative to the uncertainty about the true state of the world. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many post-hoc explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function rather than sources of mis-prediction, which matter for welfare-relevant outcomes. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide first-best solutions. We provide empirical support for our theoretical findings using an application in consumer lending.

</p>
</details>

<details><summary><b>NEWRON: A New Generalization of the Artificial Neuron to Enhance the Interpretability of Neural Networks</b>
<a href="https://arxiv.org/abs/2110.02775">arxiv:2110.02775</a>
&#x1F4C8; 1 <br>
<p>Federico Siciliano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri</p></summary>
<p>

**Abstract:** In this work, we formulate NEWRON: a generalization of the McCulloch-Pitts neuron structure. This new framework aims to explore additional desirable properties of artificial neurons. We show that some specializations of NEWRON allow the network to be interpretable with no change in their expressiveness. By just inspecting the models produced by our NEWRON-based networks, we can understand the rules governing the task. Extensive experiments show that the quality of the generated models is better than traditional interpretable models and in line or better than standard neural networks.

</p>
</details>

<details><summary><b>Pretraining & Reinforcement Learning: Sharpening the Axe Before Cutting the Tree</b>
<a href="https://arxiv.org/abs/2110.02497">arxiv:2110.02497</a>
&#x1F4C8; 1 <br>
<p>Saurav Kadavath, Samuel Paradis, Brian Yao</p></summary>
<p>

**Abstract:** Pretraining is a common technique in deep learning for increasing performance and reducing training time, with promising experimental results in deep reinforcement learning (RL). However, pretraining requires a relevant dataset for training. In this work, we evaluate the effectiveness of pretraining for RL tasks, with and without distracting backgrounds, using both large, publicly available datasets with minimal relevance, as well as case-by-case generated datasets labeled via self-supervision. Results suggest filters learned during training on less relevant datasets render pretraining ineffective, while filters learned during training on the in-distribution datasets reliably reduce RL training time and improve performance after 80k RL training steps. We further investigate, given a limited number of environment steps, how to optimally divide the available steps into pretraining and RL training to maximize RL performance. Our code is available on GitHub

</p>
</details>

<details><summary><b>TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image Enhancement</b>
<a href="https://arxiv.org/abs/2110.02477">arxiv:2110.02477</a>
&#x1F4C8; 1 <br>
<p>Xinxu Wei, Xianshi Zhang, Shisen Wang, Yanlin Huang, Yongjie Li</p></summary>
<p>

**Abstract:** Low-light image enhancement is a challenging low-level computer vision task because after we enhance the brightness of the image, we have to deal with amplified noise, color distortion, detail loss, blurred edges, shadow blocks and halo artifacts. In this paper, we propose a Two-Stage Network with Channel Attention (denoted as TSN-CA) to enhance the brightness of the low-light image and restore the enhanced images from various kinds of degradation. In the first stage, we enhance the brightness of the low-light image in HSV space and use the information of H and S channels to help the recovery of details in V channel. In the second stage, we integrate Channel Attention (CA) mechanism into the skip connection of U-Net in order to restore the brightness-enhanced image from severe kinds of degradation in RGB space. We train and evaluate the performance of our proposed model on the LOL real-world and synthetic datasets. In addition, we test our model on several other commonly used datasets without Ground-Truth. We conduct extensive experiments to demonstrate that our method achieves excellent effect on brightness enhancement as well as denoising, details preservation and halo artifacts elimination. Our method outperforms many other state-of-the-art methods qualitatively and quantitatively.

</p>
</details>

<details><summary><b>SSFL: Tackling Label Deficiency in Federated Learning via Personalized Self-Supervision</b>
<a href="https://arxiv.org/abs/2110.02470">arxiv:2110.02470</a>
&#x1F4C8; 1 <br>
<p>Chaoyang He, Zhengyu Yang, Erum Mushtaq, Sunwoo Lee, Mahdi Soltanolkotabi, Salman Avestimehr</p></summary>
<p>

**Abstract:** Federated Learning (FL) is transforming the ML training ecosystem from a centralized over-the-cloud setting to distributed training over edge devices in order to strengthen data privacy. An essential but rarely studied challenge in FL is label deficiency at the edge. This problem is even more pronounced in FL compared to centralized training due to the fact that FL users are often reluctant to label their private data. Furthermore, due to the heterogeneous nature of the data at edge devices, it is crucial to develop personalized models. In this paper we propose self-supervised federated learning (SSFL), a unified self-supervised and personalized federated learning framework, and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, we demonstrate that the standard FedAvg algorithm is compatible with recent breakthroughs in centralized self-supervised learning such as SimSiam networks. Moreover, to deal with data heterogeneity at the edge devices in this framework, we have innovated a series of algorithms that broaden existing supervised personalization algorithms into the setting of self-supervised learning. We further propose a novel personalized federated self-supervised learning algorithm, Per-SSFL, which balances personalization and consensus by carefully regulating the distance between the local and global representations of data. To provide a comprehensive comparative analysis of all proposed algorithms, we also develop a distributed training system and related evaluation protocol for SSFL. Our findings show that the gap of evaluation accuracy between supervised learning and unsupervised learning in FL is both small and reasonable. The performance comparison indicates the representation regularization-based personalization method is able to outperform other variants.

</p>
</details>

<details><summary><b>BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models</b>
<a href="https://arxiv.org/abs/2110.02467">arxiv:2110.02467</a>
&#x1F4C8; 1 <br>
<p>Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan</p></summary>
<p>

**Abstract:** Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.

</p>
</details>

<details><summary><b>A Deep Learning-based Audio-in-Image Watermarking Scheme</b>
<a href="https://arxiv.org/abs/2110.02436">arxiv:2110.02436</a>
&#x1F4C8; 1 <br>
<p>Arjon Das, Xin Zhong</p></summary>
<p>

**Abstract:** This paper presents a deep learning-based audio-in-image watermarking scheme. Audio-in-image watermarking is the process of covertly embedding and extracting audio watermarks on a cover-image. Using audio watermarks can open up possibilities for different downstream applications. For the purpose of implementing an audio-in-image watermarking that adapts to the demands of increasingly diverse situations, a neural network architecture is designed to automatically learn the watermarking process in an unsupervised manner. In addition, a similarity network is developed to recognize the audio watermarks under distortions, therefore providing robustness to the proposed method. Experimental results have shown high fidelity and robustness of the proposed blind audio-in-image watermarking scheme.

</p>
</details>

<details><summary><b>Federated Distillation of Natural Language Understanding with Confident Sinkhorns</b>
<a href="https://arxiv.org/abs/2110.02432">arxiv:2110.02432</a>
&#x1F4C8; 1 <br>
<p>Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria</p></summary>
<p>

**Abstract:** Enhancing the user experience is an essential task for application service providers. For instance, two users living wide apart may have different tastes of food. A food recommender mobile application installed on an edge device might want to learn from user feedback (reviews) to satisfy the client's needs pertaining to distinct domains. Retrieving user data comes at the cost of privacy while asking for model parameters trained on a user device becomes space inefficient at a large scale. In this work, we propose an approach to learn a central (global) model from the federation of (local) models which are trained on user-devices, without disclosing the local data or model parameters to the server. We propose a federation mechanism for the problems with natural similarity metric between the labels which commonly appear in natural language understanding (NLU) tasks. To learn the global model, the objective is to minimize the optimal transport cost of the global model's predictions from the confident sum of soft-targets assigned by local models. The confidence (a model weighting scheme) score of a model is defined as the L2 distance of a model's prediction from its probability bias. The method improves the global model's performance over the baseline designed on three NLU tasks with intrinsic label space semantics, i.e., fine-grained sentiment analysis, emotion recognition in conversation, and natural language inference. We make our codes public at https://github.com/declare-lab/sinkhorn-loss.

</p>
</details>

<details><summary><b>Explaining Off-Policy Actor-Critic From A Bias-Variance Perspective</b>
<a href="https://arxiv.org/abs/2110.02421">arxiv:2110.02421</a>
&#x1F4C8; 1 <br>
<p>Ting-Han Fan, Peter J. Ramadge</p></summary>
<p>

**Abstract:** Off-policy Actor-Critic algorithms have demonstrated phenomenal experimental performance but still require better explanations. To this end, we show its policy evaluation error on the distribution of transitions decomposes into: a Bellman error, a bias from policy mismatch, and a variance term from sampling. By comparing the magnitude of bias and variance, we explain the success of the Emphasizing Recent Experience sampling and 1/age weighted sampling. Both sampling strategies yield smaller bias and variance and are hence preferable to uniform sampling.

</p>
</details>

<details><summary><b>Tradeoffs in Streaming Binary Classification under Limited Inspection Resources</b>
<a href="https://arxiv.org/abs/2110.02403">arxiv:2110.02403</a>
&#x1F4C8; 1 <br>
<p>Parisa Hassanzadeh, Danial Dervovic, Samuel Assefa, Prashant Reddy, Manuela Veloso</p></summary>
<p>

**Abstract:** Institutions are increasingly relying on machine learning models to identify and alert on abnormal events, such as fraud, cyber attacks and system failures. These alerts often need to be manually investigated by specialists. Given the operational cost of manual inspections, the suspicious events are selected by alerting systems with carefully designed thresholds. In this paper, we consider an imbalanced binary classification problem, where events arrive sequentially and only a limited number of suspicious events can be inspected. We model the event arrivals as a non-homogeneous Poisson process, and compare various suspicious event selection methods including those based on static and adaptive thresholds. For each method, we analytically characterize the tradeoff between the minority-class detection rate and the inspection capacity as a function of the data class imbalance and the classifier confidence score densities. We implement the selection methods on a real public fraud detection dataset and compare the empirical results with analytical bounds. Finally, we investigate how class imbalance and the choice of classifier impact the tradeoff.

</p>
</details>

<details><summary><b>Quantum Semi-Supervised Learning with Quantum Supremacy</b>
<a href="https://arxiv.org/abs/2110.02343">arxiv:2110.02343</a>
&#x1F4C8; 1 <br>
<p>Zhou Shangnan</p></summary>
<p>

**Abstract:** Quantum machine learning promises to efficiently solve important problems. There are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power. We propose a novel framework that resolves both issues: quantum semi-supervised learning. Moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning. In the meantime, we show that naive quantum matrix product estimation algorithm outperforms the best known classical matrix multiplication algorithm. We showcase two concrete quantum semi-supervised learning algorithms: a quantum self-training algorithm named the propagating nearest-neighbor classifier, and the quantum semi-supervised K-means clustering algorithm. By doing time complexity analysis, we conclude that they indeed possess quantum supremacy.

</p>
</details>

<details><summary><b>Task-aware Privacy Preservation for Multi-dimensional Data</b>
<a href="https://arxiv.org/abs/2110.02329">arxiv:2110.02329</a>
&#x1F4C8; 1 <br>
<p>Jiangnan Cheng, Ao Tang, Sandeep Chinchali</p></summary>
<p>

**Abstract:** Local differential privacy (LDP), a state-of-the-art technique for privacy preservation, has been successfully deployed in a few real-world applications. In the future, LDP can be adopted to anonymize richer user data attributes that will be input to more sophisticated machine learning (ML) tasks. However, today's LDP approaches are largely task-agnostic and often lead to sub-optimal performance -- they will simply inject noise to all data attributes according to a given privacy budget, regardless of what features are most relevant for an ultimate task. In this paper, we address how to significantly improve the ultimate task performance for multi-dimensional user data by considering a task-aware privacy preservation problem. The key idea is to use an encoder-decoder framework to learn (and anonymize) a task-relevant latent representation of user data, which gives an analytical near-optimal solution for a linear setting with mean-squared error (MSE) task loss. We also provide an approximate solution through a learning algorithm for general nonlinear cases. Extensive experiments demonstrate that our task-aware approach significantly improves ultimate task accuracy compared to a standard benchmark LDP approach while guaranteeing the same level of privacy.

</p>
</details>

<details><summary><b>Enhancement of Anime Imaging Enlargement using Modified Super-Resolution CNN</b>
<a href="https://arxiv.org/abs/2110.02321">arxiv:2110.02321</a>
&#x1F4C8; 1 <br>
<p>Tanakit Intaniyom, Warinthorn Thananporn, Kuntpong Woraratpanya</p></summary>
<p>

**Abstract:** Anime is a storytelling medium similar to movies and books. Anime images are a kind of artworks, which are almost entirely drawn by hand. Hence, reproducing existing Anime with larger sizes and higher quality images is expensive. Therefore, we proposed a model based on convolutional neural networks to extract outstanding features of images, enlarge those images, and enhance the quality of Anime images. We trained the model with a training set of 160 images and a validation set of 20 images. We tested the trained model with a testing set of 20 images. The experimental results indicated that our model successfully enhanced the image quality with a larger image-size when compared with the common existing image enlargement and the original SRCNN method.

</p>
</details>

<details><summary><b>Measuring chemical likeness of stars with RSCA</b>
<a href="https://arxiv.org/abs/2110.02250">arxiv:2110.02250</a>
&#x1F4C8; 1 <br>
<p>Damien de Mijolla, Melissa K. Ness</p></summary>
<p>

**Abstract:** Identification of chemically similar stars using elemental abundances is core to many pursuits within Galactic archaeology. However, measuring the chemical likeness of stars using abundances directly is limited by systematic imprints of imperfect synthetic spectra in abundance derivation. We present a novel data-driven model that is capable of identifying chemically similar stars from spectra alone. We call this Relevant Scaled Component Analysis (RSCA). RSCA finds a mapping from stellar spectra to a representation that optimizes recovery of known open clusters. By design, RSCA amplifies factors of chemical abundance variation and minimizes those of non-chemical parameters, such as instrument systematics. The resultant representation of stellar spectra can therefore be used for precise measurements of chemical similarity between stars. We validate RSCA using 185 cluster stars in 22 open clusters in the APOGEE survey. We quantify our performance in measuring chemical similarity using a reference set of 151,145 field stars. We find that our representation identifies known stellar siblings more effectively than stellar abundance measurements. Using RSCA, 1.8% of pairs of field stars are as similar as birth siblings, compared to 2.3% when using stellar abundance labels. We find that almost all of the information within spectra leveraged by RSCA fits into a two-dimensional basis, which we link to [Fe/H] and alpha-element abundances. We conclude that chemical tagging of stars to their birth clusters remains prohibitive. However, using the spectra has noticeable gain, and our approach is poised to benefit from larger datasets and improved algorithm designs.

</p>
</details>

<details><summary><b>$Œî$-UQ: Accurate Uncertainty Quantification via Anchor Marginalization</b>
<a href="https://arxiv.org/abs/2110.02197">arxiv:2110.02197</a>
&#x1F4C8; 1 <br>
<p>Rushil Anirudh, Jayaraman J. Thiagarajan</p></summary>
<p>

**Abstract:** We present $Œî$-UQ -- a novel, general-purpose uncertainty estimator using the concept of anchoring in predictive models. Anchoring works by first transforming the input into a tuple consisting of an anchor point drawn from a prior distribution, and a combination of the input sample with the anchor using a pretext encoding scheme. This encoding is such that the original input can be perfectly recovered from the tuple -- regardless of the choice of the anchor. Therefore, any predictive model should be able to predict the target response from the tuple alone (since it implicitly represents the input). Moreover, by varying the anchors for a fixed sample, we can estimate uncertainty in the prediction even using only a single predictive model. We find this uncertainty is deeply connected to improper sampling of the input data, and inherent noise, enabling us to estimate the total uncertainty in any system. With extensive empirical studies on a variety of use-cases, we demonstrate that $Œî$-UQ outperforms several competitive baselines. Specifically, we study model fitting, sequential model optimization, model based inversion in the regression setting and out of distribution detection, & calibration under distribution shifts for classification.

</p>
</details>

<details><summary><b>Blockchain-based Federated Learning: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2110.02182">arxiv:2110.02182</a>
&#x1F4C8; 1 <br>
<p>Zhilin Wang, Qin Hu</p></summary>
<p>

**Abstract:** With the technological advances in machine learning, effective ways are available to process the huge amount of data generated in real life. However, issues of privacy and scalability will constrain the development of machine learning. Federated learning (FL) can prevent privacy leakage by assigning training tasks to multiple clients, thus separating the central server from the local devices. However, FL still suffers from shortcomings such as single-point-failure and malicious data. The emergence of blockchain provides a secure and efficient solution for the deployment of FL. In this paper, we conduct a comprehensive survey of the literature on blockchained FL (BCFL). First, we investigate how blockchain can be applied to federal learning from the perspective of system composition. Then, we analyze the concrete functions of BCFL from the perspective of mechanism design and illustrate what problems blockchain addresses specifically for FL. We also survey the applications of BCFL in reality. Finally, we discuss some challenges and future research directions.

</p>
</details>

<details><summary><b>NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback</b>
<a href="https://arxiv.org/abs/2110.02148">arxiv:2110.02148</a>
&#x1F4C8; 1 <br>
<p>Ruijie Zhou, Soham Deshmukh, Jeremiah Greer, Charles Lee</p></summary>
<p>

**Abstract:** Current research in dialogue systems is focused on conversational assistants working on short conversations in either task-oriented or open domain settings. In this paper, we focus on improving task-based conversational assistants online, primarily those working on document-type conversations (e.g., emails) whose contents may or may not be completely related to the assistant's task. We propose "NARLE" a deep reinforcement learning (RL) framework for improving the natural language understanding (NLU) component of dialogue systems online without the need to collect human labels for customer data. The proposed solution associates user emotion with the assistant's action and uses that to improve NLU models using policy gradients. For two intent classification problems, we empirically show that using reinforcement learning to fine tune the pre-trained supervised learning models improves performance up to 43%. Furthermore, we demonstrate the robustness of the method to partial and noisy implicit feedback.

</p>
</details>

<details><summary><b>A study of first-passage time minimization via Q-learning in heated gridworlds</b>
<a href="https://arxiv.org/abs/2110.02129">arxiv:2110.02129</a>
&#x1F4C8; 1 <br>
<p>M. A. Larchenko, P. Osinenko, G. Yaremenko, V. V. Palyulin</p></summary>
<p>

**Abstract:** Optimization of first-passage times is required in applications ranging from nanobots navigation to market trading. In such settings, one often encounters unevenly distributed noise levels across the environment. We extensively study how a learning agent fares in 1- and 2- dimensional heated gridworlds with an uneven temperature distribution. The results show certain bias effects in agents trained via simple tabular Q-learning, SARSA, Expected SARSA and Double Q-learning. While high learning rate prevents exploration of regions with higher temperature, low enough rate increases the presence of agents in such regions. The discovered peculiarities and biases of temporal-difference-based reinforcement learning methods should be taken into account in real-world physical applications and agent design.

</p>
</details>

<details><summary><b>Adversarial Robustness Verification and Attack Synthesis in Stochastic Systems</b>
<a href="https://arxiv.org/abs/2110.02125">arxiv:2110.02125</a>
&#x1F4C8; 1 <br>
<p>Lisa Oakley, Alina Oprea, Stavros Tripakis</p></summary>
<p>

**Abstract:** Probabilistic model checking is a useful technique for specifying and verifying properties of stochastic systems including randomized protocols and the theoretical underpinnings of reinforcement learning models. However, these methods rely on the assumed structure and probabilities of certain system transitions. These assumptions may be incorrect, and may even be violated in the event that an adversary gains control of some or all components in the system.
  In this paper, motivated by research in adversarial machine learning on adversarial examples, we develop a formal framework for adversarial robustness in systems defined as discrete time Markov chains (DTMCs), and extend to include deterministic, memoryless policies acting in Markov decision processes (MDPs). Our framework includes a flexible approach for specifying several adversarial models with different capabilities to manipulate the system. We outline a class of threat models under which adversaries can perturb system transitions, constrained by an $\varepsilon$ ball around the original transition probabilities and define four specific instances of this threat model.
  We define three main DTMC adversarial robustness problems and present two optimization-based solutions, leveraging traditional and parametric probabilistic model checking techniques. We then evaluate our solutions on two stochastic protocols and a collection of GridWorld case studies, which model an agent acting in an environment described as an MDP. We find that the parametric solution results in fast computation for small parameter spaces. In the case of less restrictive (stronger) adversaries, the number of parameters increases, and directly computing property satisfaction probabilities is more scalable. We demonstrate the usefulness of our definitions and solutions by comparing system outcomes over various properties, threat models, and case studies.

</p>
</details>

<details><summary><b>TENT: Text Classification Based on ENcoding Tree Learning</b>
<a href="https://arxiv.org/abs/2110.02047">arxiv:2110.02047</a>
&#x1F4C8; 1 <br>
<p>Chong Zhang, Junran Wu, He Zhu, Ke Xu</p></summary>
<p>

**Abstract:** Text classification is a primary task in natural language processing (NLP). Recently, graph neural networks (GNNs) have developed rapidly and been applied to text classification tasks. Although more complex models tend to achieve better performance, research highly depends on the computing power of the device used. In this article, we propose TENT (https://github.com/Daisean/TENT) to obtain better text classification performance and reduce the reliance on computing power. Specifically, we first establish a dependency analysis graph for each text and then convert each graph into its corresponding encoding tree. The representation of the entire graph is obtained by updating the representation of the non-leaf nodes in the encoding tree. Experimental results show that our method outperforms other baselines on several datasets while having a simple structure and few parameters.

</p>
</details>

<details><summary><b>Extracting Major Topics of COVID-19 Related Tweets</b>
<a href="https://arxiv.org/abs/2110.01876">arxiv:2110.01876</a>
&#x1F4C8; 1 <br>
<p>Faezeh Azizi, Hamed Vahdat-Nejad, Hamideh Hajiabadi, Mohammad Hossein Khosravi</p></summary>
<p>

**Abstract:** With the outbreak of the Covid-19 virus, the activity of users on Twitter has significantly increased. Some studies have investigated the hot topics of tweets in this period; however, little attention has been paid to presenting and analyzing the spatial and temporal trends of Covid-19 topics. In this study, we use the topic modeling method to extract global topics during the nationwide quarantine periods (March 23 to June 23, 2020) on Covid-19 tweets. We implement the Latent Dirichlet Allocation (LDA) algorithm to extract the topics and then name them with the "reopening", "death cases", "telecommuting", "protests", "anger expression", "masking", "medication", "social distance", "second wave", and "peak of the disease" titles. We additionally analyze temporal trends of the topics for the whole world and four countries. By analyzing the graphs, fascinating results are obtained from altering users' focus on topics over time.

</p>
</details>

<details><summary><b>Cellular Network Radio Propagation Modeling with Deep Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2110.01848">arxiv:2110.01848</a>
&#x1F4C8; 1 <br>
<p>Xin Zhang, Xiujun Shu, Bingwen Zhang, Jie Ren, Lizhou Zhou, Xin Chen</p></summary>
<p>

**Abstract:** Radio propagation modeling and prediction is fundamental for modern cellular network planning and optimization. Conventional radio propagation models fall into two categories. Empirical models, based on coarse statistics, are simple and computationally efficient, but are inaccurate due to oversimplification. Deterministic models, such as ray tracing based on physical laws of wave propagation, are more accurate and site specific. But they have higher computational complexity and are inflexible to utilize site information other than traditional global information system (GIS) maps.
  In this article we present a novel method to model radio propagation using deep convolutional neural networks and report significantly improved performance compared to conventional models. We also lay down the framework for data-driven modeling of radio propagation and enable future research to utilize rich and unconventional information of the site, e.g. satellite photos, to provide more accurate and flexible models.

</p>
</details>

<details><summary><b>Truth-Conditional Captioning of Time Series Data</b>
<a href="https://arxiv.org/abs/2110.01839">arxiv:2110.01839</a>
&#x1F4C8; 1 <br>
<p>Harsh Jhamtani, Taylor Berg-Kirkpatrick</p></summary>
<p>

**Abstract:** In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical contemporary neural models with attention mechanisms can generate fluent output descriptions for this task, they often generate factually incorrect descriptions. We propose a computational model with a truth-conditional architecture which first runs small learned programs on the input time series, then identifies the programs/patterns which hold true for the given input, and finally conditions on only the chosen valid program (rather than the input time series) to generate the output text description. A program in our model is constructed from modules, which are small neural networks that are designed to capture numerical patterns and temporal information. The modules are shared across multiple programs, enabling compositionality as well as efficient learning of module parameters. The modules, as well as the composition of the modules, are unobserved in data, and we learn them in an end-to-end fashion with the only training signal coming from the accompanying natural language text descriptions. We find that the proposed model is able to generate high-precision captions even though we consider a small and simple space of module types.

</p>
</details>

<details><summary><b>Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition</b>
<a href="https://arxiv.org/abs/2110.01833">arxiv:2110.01833</a>
&#x1F4C8; 1 <br>
<p>Jeong-Hoon Lee, Jongeun Choi</p></summary>
<p>

**Abstract:** Deep reinforcement learning has shown its effectiveness in various applications and provides a promising direction for solving tasks with high complexity. In most reinforcement learning algorithms, however, two major issues need to be dealt with - the sample inefficiency and the interpretability of a policy. The former happens when the environment is sparsely rewarded and/or has a long-term credit assignment problem, while the latter becomes a problem when the learned policies are deployed at the customer side product. In this paper, we propose a novel hierarchical reinforcement learning algorithm that mitigates the aforementioned issues by decomposing the original task in a hierarchy and by compounding pretrained primitives with intents. We show how the proposed scheme can be employed in practice by solving a pick and place task with a 6 DoF manipulator.

</p>
</details>

<details><summary><b>Predicting Credit Risk for Unsecured Lending: A Machine Learning Approach</b>
<a href="https://arxiv.org/abs/2110.02206">arxiv:2110.02206</a>
&#x1F4C8; 0 <br>
<p>K. S. Naik</p></summary>
<p>

**Abstract:** Since the 1990s, there have been significant advances in the technology space and the e-Commerce area, leading to an exponential increase in demand for cashless payment solutions. This has led to increased demand for credit cards, bringing along with it the possibility of higher credit defaults and hence higher delinquency rates, over a period of time. The purpose of this research paper is to build a contemporary credit scoring model to forecast credit defaults for unsecured lending (credit cards), by employing machine learning techniques. As much of the customer payments data available to lenders, for forecasting Credit defaults, is imbalanced (skewed), on account of a limited subset of default instances, this poses a challenge for predictive modelling. In this research, this challenge is addressed by deploying Synthetic Minority Oversampling Technique (SMOTE), a proven technique to iron out such imbalances, from a given dataset. On running the research dataset through seven different machine learning models, the results indicate that the Light Gradient Boosting Machine (LGBM) Classifier model outperforms the other six classification techniques. Thus, our research indicates that the LGBM classifier model is better equipped to deliver higher learning speeds, better efficiencies and manage larger data volumes. We expect that deployment of this model will enable better and timely prediction of credit defaults for decision-makers in commercial lending institutions and banks.

</p>
</details>

<details><summary><b>TensorPlan and the Few Actions Lower Bound for Planning in MDPs under Linear Realizability of Optimal Value Functions</b>
<a href="https://arxiv.org/abs/2110.02195">arxiv:2110.02195</a>
&#x1F4C8; 0 <br>
<p>Gell√©rt Weisz, Csaba Szepesv√°ri, Andr√°s Gy√∂rgy</p></summary>
<p>

**Abstract:** We consider the minimax query complexity of online planning with a generative model in fixed-horizon Markov decision processes (MDPs) with linear function approximation. Following recent works, we consider broad classes of problems where either (i) the optimal value function $v^\star$ or (ii) the optimal action-value function $q^\star$ lie in the linear span of some features; or (iii) both $v^\star$ and $q^\star$ lie in the linear span when restricted to the states reachable from the starting state. Recently, Weisz et al. (2021b) showed that under (ii) the minimax query complexity of any planning algorithm is at least exponential in the horizon $H$ or in the feature dimension $d$ when the size $A$ of the action set can be chosen to be exponential in $\min(d,H)$. On the other hand, for the setting (i), Weisz et al. (2021a) introduced TensorPlan, a planner whose query cost is polynomial in all relevant quantities when the number of actions is fixed. Among other things, these two works left open the question whether polynomial query complexity is possible when $A$ is subexponential in $min(d,H)$. In this paper we answer this question in the negative: we show that an exponentially large lower bound holds when $A=Œ©(\min(d^{1/4},H^{1/2}))$, under either (i), (ii) or (iii). In particular, this implies a perhaps surprising exponential separation of query complexity compared to the work of Du et al. (2021) who prove a polynomial upper bound when (iii) holds for all states. Furthermore, we show that the upper bound of TensorPlan can be extended to hold under (iii) and, for MDPs with deterministic transitions and stochastic rewards, also under (ii).

</p>
</details>

<details><summary><b>Inference and De-Noising of Non-Gaussian Particle Distribution Functions: A Generative Modeling Approach</b>
<a href="https://arxiv.org/abs/2110.02153">arxiv:2110.02153</a>
&#x1F4C8; 0 <br>
<p>John Donaghy, Kai Germaschewski</p></summary>
<p>

**Abstract:** The particle-in-cell numerical method of plasma physics balances a trade-off between computational cost and intrinsic noise. Inference on data produced by these simulations generally consists of binning the data to recover the particle distribution function, from which physical processes may be investigated. In addition to containing noise, the distribution function is temporally dynamic and can be non-gaussian and multi-modal, making the task of modeling it difficult. Here we demonstrate the use of normalizing flows to learn a smooth, tractable approximation to the noisy particle distribution function. We demonstrate that the resulting data driven likelihood conserves relevant physics and may be extended to encapsulate the temporal evolution of the distribution function.

</p>
</details>

<details><summary><b>S2 Reducer: High-Performance Sparse Communication to Accelerate Distributed Deep Learning</b>
<a href="https://arxiv.org/abs/2110.02140">arxiv:2110.02140</a>
&#x1F4C8; 0 <br>
<p>Keshi Ge, Yongquan Fu, Zhiquan Lai, Xiaoge Deng, Dongsheng Li</p></summary>
<p>

**Abstract:** Distributed stochastic gradient descent (SGD) approach has been widely used in large-scale deep learning, and the gradient collective method is vital to ensure the training scalability of the distributed deep learning system. Collective communication such as AllReduce has been widely adopted for the distributed SGD process to reduce the communication time. However, AllReduce incurs large bandwidth resources while most gradients are sparse in many cases since many gradient values are zeros and should be efficiently compressed for bandwidth saving. To reduce the sparse gradient communication overhead, we propose Sparse-Sketch Reducer (S2 Reducer), a novel sketch-based sparse gradient aggregation method with convergence guarantees. S2 Reducer reduces the communication cost by only compressing the non-zero gradients with count-sketch and bitmap, and enables the efficient AllReduce operators for parallel SGD training. We perform extensive evaluation against four state-of-the-art methods over five training models. Our results show that S2 Reducer converges to the same accuracy, reduces 81\% sparse communication overhead, and achieves 1.8$ \times $ speedup compared to state-of-the-art approaches.

</p>
</details>

<details><summary><b>NeurWIN: Neural Whittle Index Network For Restless Bandits Via Deep RL</b>
<a href="https://arxiv.org/abs/2110.02128">arxiv:2110.02128</a>
&#x1F4C8; 0 <br>
<p>Khaled Nakhleh, Santosh Ganji, Ping-Chun Hsieh, I-Hong Hou, Srinivas Shakkottai</p></summary>
<p>

**Abstract:** Whittle index policy is a powerful tool to obtain asymptotically optimal solutions for the notoriously intractable problem of restless bandits. However, finding the Whittle indices remains a difficult problem for many practical restless bandits with convoluted transition kernels. This paper proposes NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices for any restless bandits by leveraging mathematical properties of the Whittle indices. We show that a neural network that produces the Whittle index is also one that produces the optimal control for a set of Markov decision problems. This property motivates using deep reinforcement learning for the training of NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance for three recently studied restless bandit problems. Our experiment results show that the performance of NeurWIN is significantly better than other RL algorithms.

</p>
</details>

<details><summary><b>ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models</b>
<a href="https://arxiv.org/abs/2110.02042">arxiv:2110.02042</a>
&#x1F4C8; 0 <br>
<p>Hoai Nam Tran, Udo Kruschwitz</p></summary>
<p>

**Abstract:** This paper describes our approach (ur-iw-hnt) for the Shared Task of GermEval2021 to identify toxic, engaging, and fact-claiming comments. We submitted three runs using an ensembling strategy by majority (hard) voting with multiple different BERT models of three different types: German-based, Twitter-based, and multilingual models. All ensemble models outperform single models, while BERTweet is the winner of all individual models in every subtask. Twitter-based models perform better than GermanBERT models, and multilingual models perform worse but by a small margin.

</p>
</details>

<details><summary><b>Dropout Q-Functions for Doubly Efficient Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2110.02034">arxiv:2110.02034</a>
&#x1F4C8; 0 <br>
<p>Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka</p></summary>
<p>

**Abstract:** Randomized ensemble double Q-learning (REDQ) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called Dr.Q, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that Dr.Q is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ and much better computational efficiency than REDQ and comparable computational efficiency with that of SAC.

</p>
</details>

<details><summary><b>A new harmonium for pattern recognition in survival data</b>
<a href="https://arxiv.org/abs/2110.01960">arxiv:2110.01960</a>
&#x1F4C8; 0 <br>
<p>Hylke C. Donker, Harry J. M. Groen</p></summary>
<p>

**Abstract:** Background: Survival analysis concerns the study of timeline data where the event of interest may remain unobserved (i.e., censored). Studies commonly record more than one type of event, but conventional survival techniques focus on a single event type. We set out to integrate both multiple independently censored time-to-event variables as well as missing observations.
  Methods: An energy-based approach is taken with a bi-partite structure between latent and visible states, commonly known as harmoniums (or restricted Boltzmann machines).
  Results: The present harmonium is shown, both theoretically and experimentally, to capture non-linear patterns between distinct time recordings. We illustrate on real world data that, for a single time-to-event variable, our model is on par with established methods. In addition, we demonstrate that discriminative predictions improve by leveraging an extra time-to-event variable.
  Conclusions: Multiple time-to-event variables can be successfully captured within the harmonium paradigm.

</p>
</details>

<details><summary><b>Continuous-Time Fitted Value Iteration for Robust Policies</b>
<a href="https://arxiv.org/abs/2110.01954">arxiv:2110.01954</a>
&#x1F4C8; 0 <br>
<p>Michael Lutter, Boris Belousov, Shie Mannor, Dieter Fox, Animesh Garg, Jan Peters</p></summary>
<p>

**Abstract:** Solving the Hamilton-Jacobi-Bellman equation is important in many domains including control, robotics and economics. Especially for continuous control, solving this differential equation and its extension the Hamilton-Jacobi-Isaacs equation, is important as it yields the optimal policy that achieves the maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs equation, which includes an adversary controlling the environment and minimizing the reward, the obtained policy is also robust to perturbations of the dynamics. In this paper we propose continuous fitted value iteration (cFVI) and robust fitted value iteration (rFVI). These algorithms leverage the non-linear control-affine dynamics and separable state and action reward of many continuous control problems to derive the optimal policy and optimal adversary in closed form. This analytic expression simplifies the differential equations and enables us to solve for the optimal value function using value iteration for continuous actions and states as well as the adversarial case. Notably, the resulting algorithms do not require discretization of states or actions. We apply the resulting algorithms to the Furuta pendulum and cartpole. We show that both algorithms obtain the optimal policy. The robustness Sim2Real experiments on the physical systems show that the policies successfully achieve the task in the real-world. When changing the masses of the pendulum, we observe that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at https://sites.google.com/view/rfvi

</p>
</details>

<details><summary><b>Classification of high-dimensional data with spiked covariance matrix structure</b>
<a href="https://arxiv.org/abs/2110.01950">arxiv:2110.01950</a>
&#x1F4C8; 0 <br>
<p>Yin-Jen Chen, Minh Tang</p></summary>
<p>

**Abstract:** We study the classification problem for high-dimensional data with $n$ observations on $p$ features where the $p \times p$ covariance matrix $Œ£$ exhibits a spiked eigenvalues structure and the vector $Œ∂$, given by the difference between the whitened mean vectors, is sparse with sparsity at most $s$. We propose an adaptive classifier (adaptive with respect to the sparsity $s$) that first performs dimension reduction on the feature vectors prior to classification in the dimensionally reduced space, i.e., the classifier whitened the data, then screen the features by keeping only those corresponding to the $s$ largest coordinates of $Œ∂$ and finally apply Fisher linear discriminant on the selected features. Leveraging recent results on entrywise matrix perturbation bounds for covariance matrices, we show that the resulting classifier is Bayes optimal whenever $n \rightarrow \infty$ and $s \sqrt{n^{-1} \ln p} \rightarrow 0$. Experimental results on real and synthetic data sets indicate that the proposed classifier is competitive with existing state-of-the-art methods while also selecting a smaller number of features.

</p>
</details>

<details><summary><b>Data-driven Nonlinear Model Reduction to Spectral Submanifolds in Mechanical Systems</b>
<a href="https://arxiv.org/abs/2110.01929">arxiv:2110.01929</a>
&#x1F4C8; 0 <br>
<p>Mattia Cenedese, Joar Ax√•s, Haocheng Yang, Melih Eriten, George Haller</p></summary>
<p>

**Abstract:** While data-driven model reduction techniques are well-established for linearizable mechanical systems, general approaches to reducing non-linearizable systems with multiple coexisting steady states have been unavailable. In this paper, we review such a data-driven nonlinear model reduction methodology based on spectral submanifolds (SSMs). As input, this approach takes observations of unforced nonlinear oscillations to construct normal forms of the dynamics reduced to very low dimensional invariant manifolds. These normal forms capture amplitude-dependent properties and are accurate enough to provide predictions for non-linearizable system response under the additions of external forcing. We illustrate these results on examples from structural vibrations, featuring both synthetic and experimental data.

</p>
</details>

<details><summary><b>Inferring Hidden Structures in Random Graphs</b>
<a href="https://arxiv.org/abs/2110.01901">arxiv:2110.01901</a>
&#x1F4C8; 0 <br>
<p>Wasim Huleihel</p></summary>
<p>

**Abstract:** We study the two inference problems of detecting and recovering an isolated community of \emph{general} structure planted in a random graph. The detection problem is formalized as a hypothesis testing problem, where under the null hypothesis, the graph is a realization of an Erd≈ës-R√©nyi random graph $\mathcal{G}(n,q)$ with edge density $q\in(0,1)$; under the alternative, there is an unknown structure $Œì_k$ on $k$ nodes, planted in $\mathcal{G}(n,q)$, such that it appears as an \emph{induced subgraph}. In case of a successful detection, we are concerned with the task of recovering the corresponding structure. For these problems, we investigate the fundamental limits from both the statistical and computational perspectives. Specifically, we derive lower bounds for detecting/recovering the structure $Œì_k$ in terms of the parameters $(n,k,q)$, as well as certain properties of $Œì_k$, and exhibit computationally unbounded optimal algorithms that achieve these lower bounds. We also consider the problem of testing in polynomial-time. As is customary in many similar structured high-dimensional problems, our model undergoes an "easy-hard-impossible" phase transition and computational constraints can severely penalize the statistical performance. To provide an evidence for this phenomenon, we show that the class of low-degree polynomials algorithms match the statistical performance of the polynomial-time algorithms we develop.

</p>
</details>

<details><summary><b>Random matrices in service of ML footprint: ternary random features with no performance loss</b>
<a href="https://arxiv.org/abs/2110.01899">arxiv:2110.01899</a>
&#x1F4C8; 0 <br>
<p>Hafiz Tiomoko Ali, Zhenyu Liao, Romain Couillet</p></summary>
<p>

**Abstract:** In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\bf K} = \mathbb{E}_{\bf w} \left[œÉ\left({\bf w}^{\sf T}{\bf x}_i\right)œÉ\left({\bf w}^{\sf T}{\bf x}_j\right)\right]_{i,j=1}^n$, with nonlinear function $œÉ(\cdot)$, data ${\bf x}_1, \ldots, {\bf x}_n \in \mathbb{R}^p$, and random projection vector ${\bf w} \in \mathbb{R}^p$ having i.i.d. entries. In a high-dimensional setting where the number of data $n$ and their dimension $p$ are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of ${\bf K}$ is independent of the distribution of the i.i.d.(zero-mean and unit-variance) entries of ${\bf w}$, and only depends on $œÉ(\cdot)$ via its (generalized) Gaussian moments $\mathbb{E}_{z\sim \mathcal N(0,1)}[œÉ'(z)]$ and $\mathbb{E}_{z\sim \mathcal N(0,1)}[œÉ''(z)]$. As a result, for any kernel matrix ${\bf K}$ of the form above, we propose a novel random features technique, called Ternary Random Feature (TRF), that (i) asymptotically yields the same limiting kernel as the original ${\bf K}$ in a spectral sense and (ii) can be computed and stored much more efficiently, by wisely tuning (in a data-dependent manner) the function $œÉ$ and the random vector ${\bf w}$, both taking values in $\{-1,0,1\}$. The computation of the proposed random features requires no multiplication, and a factor of $b$ times less bits for storage compared to classical random features such as random Fourier features, with $b$ the number of bits to store full precision values. Besides, it appears in our experiments on real data that the substantial gains in computation and storage are accompanied with somewhat improved performances compared to state-of-the-art random features compression/quantization methods.

</p>
</details>

<details><summary><b>Permute Me Softly: Learning Soft Permutations for Graph Representations</b>
<a href="https://arxiv.org/abs/2110.01872">arxiv:2110.01872</a>
&#x1F4C8; 0 <br>
<p>Giannis Nikolentzos, George Dasoulas, Michalis Vazirgiannis</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) have recently emerged as a dominant paradigm for machine learning with graphs. Research on GNNs has mainly focused on the family of message passing neural networks (MPNNs). Similar to the Weisfeiler-Leman (WL) test of isomorphism, these models follow an iterative neighborhood aggregation procedure to update vertex representations, and they next compute graph representations by aggregating the representations of the vertices. Although very successful, MPNNs have been studied intensively in the past few years. Thus, there is a need for novel architectures which will allow research in the field to break away from MPNNs. In this paper, we propose a new graph neural network model, so-called $œÄ$-GNN which learns a "soft" permutation (i.e., doubly stochastic) matrix for each graph, and thus projects all graphs into a common vector space. The learned matrices impose a "soft" ordering on the vertices of the input graphs, and based on this ordering, the adjacency matrices are mapped into vectors. These vectors can be fed into fully-connected or convolutional layers to deal with supervised learning tasks. In case of large graphs, to make the model more efficient in terms of running time and memory, we further relax the doubly stochastic matrices to row stochastic matrices. We empirically evaluate the model on graph classification and graph regression datasets and show that it achieves performance competitive with state-of-the-art models.

</p>
</details>

<details><summary><b>DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge Computing</b>
<a href="https://arxiv.org/abs/2110.01863">arxiv:2110.01863</a>
&#x1F4C8; 0 <br>
<p>Baris Yamansavascilar, Ahmet Cihat Baktir, Cagatay Sonmez, Atay Ozgovde, Cem Ersoy</p></summary>
<p>

**Abstract:** The improvements in the edge computing technology pave the road for diversified applications that demand real-time interaction. However, due to the mobility of the end-users and the dynamic edge environment, it becomes challenging to handle the task offloading with high performance. Moreover, since each application in mobile devices has different characteristics, a task orchestrator must be adaptive and have the ability to learn the dynamics of the environment. For this purpose, we develop a deep reinforcement learning based task orchestrator, DeepEdge, which learns to meet different task requirements without needing human interaction even under the heavily-loaded stochastic network conditions in terms of mobile users and applications. Given the dynamic offloading requests and time-varying communication conditions, we successfully model the problem as a Markov process and then apply the Double Deep Q-Network (DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge, we experiment with four different applications including image rendering, infotainment, pervasive health, and augmented reality in the network under various loads. Furthermore, we compare the performance of our agent with the four different task offloading approaches in the literature. Our results show that DeepEdge outperforms its competitors in terms of the percentage of satisfactorily completed tasks.

</p>
</details>

<details><summary><b>KKT Conditions, First-Order and Second-Order Optimization, and Distributed Optimization: Tutorial and Survey</b>
<a href="https://arxiv.org/abs/2110.01858">arxiv:2110.01858</a>
&#x1F4C8; 0 <br>
<p>Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley</p></summary>
<p>

**Abstract:** This is a tutorial and survey paper on Karush-Kuhn-Tucker (KKT) conditions, first-order and second-order numerical optimization, and distributed optimization. After a brief review of history of optimization, we start with some preliminaries on properties of sets, norms, functions, and concepts of optimization. Then, we introduce the optimization problem, standard optimization problems (including linear programming, quadratic programming, and semidefinite programming), and convex problems. We also introduce some techniques such as eliminating inequality, equality, and set constraints, adding slack variables, and epigraph form. We introduce Lagrangian function, dual variables, KKT conditions (including primal feasibility, dual feasibility, weak and strong duality, complementary slackness, and stationarity condition), and solving optimization by method of Lagrange multipliers. Then, we cover first-order optimization including gradient descent, line-search, convergence of gradient methods, momentum, steepest descent, and backpropagation. Other first-order methods are explained, such as accelerated gradient method, stochastic gradient descent, mini-batch gradient descent, stochastic average gradient, stochastic variance reduced gradient, AdaGrad, RMSProp, and Adam optimizer, proximal methods (including proximal mapping, proximal point algorithm, and proximal gradient method), and constrained gradient methods (including projected gradient method, projection onto convex sets, and Frank-Wolfe method). We also cover non-smooth and $\ell_1$ optimization methods including lasso regularization, convex conjugate, Huber function, soft-thresholding, coordinate descent, and subgradient methods. Then, we explain second-order methods including Newton's method for unconstrained, equality constrained, and inequality constrained problems....

</p>
</details>

<details><summary><b>Dataset: Large-scale Urban IoT Activity Data for DDoS Attack Emulation</b>
<a href="https://arxiv.org/abs/2110.01842">arxiv:2110.01842</a>
&#x1F4C8; 0 <br>
<p>Arvin Hekmati, Eugenio Grippo, Bhaskar Krishnamachari</p></summary>
<p>

**Abstract:** As IoT deployments grow in scale for applications such as smart cities, they face increasing cyber-security threats. In particular, as evidenced by the famous Mirai incident and other ongoing threats, large-scale IoT device networks are particularly susceptible to being hijacked and used as botnets to launch distributed denial of service (DDoS) attacks. Real large-scale datasets are needed to train and evaluate the use of machine learning algorithms such as deep neural networks to detect and defend against such DDoS attacks. We present a dataset from an urban IoT deployment of 4060 nodes describing their spatio-temporal activity under benign conditions. We also provide a synthetic DDoS attack generator that injects attack activity into the dataset based on tunable parameters such as number of nodes attacked and duration of attack. We discuss some of the features of the dataset. We also demonstrate the utility of the dataset as well as our synthetic DDoS attack generator by using them for the training and evaluation of a simple multi-label feed-forward neural network that aims to identify which nodes are under attack and when.

</p>
</details>

<details><summary><b>Attention Augmented Convolutional Transformer for Tabular Time-series</b>
<a href="https://arxiv.org/abs/2110.01825">arxiv:2110.01825</a>
&#x1F4C8; 0 <br>
<p>Sharath M Shankaranarayana, Davor Runje</p></summary>
<p>

**Abstract:** Time-series classification is one of the most frequently performed tasks in industrial data science, and one of the most widely used data representation in the industrial setting is tabular representation. In this work, we propose a novel scalable architecture for learning representations from tabular time-series data and subsequently performing downstream tasks such as time-series classification. The representation learning framework is end-to-end, akin to bidirectional encoder representations from transformers (BERT) in language modeling, however, we introduce novel masking technique suitable for pretraining of time-series data. Additionally, we also use one-dimensional convolutions augmented with transformers and explore their effectiveness, since the time-series datasets lend themselves naturally for one-dimensional convolutions. We also propose a novel timestamp embedding technique, which helps in handling both periodic cycles at different time granularity levels, and aperiodic trends present in the time-series data. Our proposed model is end-to-end and can handle both categorical and continuous valued inputs, and does not require any quantization or encoding of continuous features.

</p>
</details>


[Next Page]({{ '/2021/10/04/2021.10.04.html' | relative_url }})
