## Summary for 2021-05-15, created on 2021-12-21


<details><summary><b>Texture Generation with Neural Cellular Automata</b>
<a href="https://arxiv.org/abs/2105.07299">arxiv:2105.07299</a>
&#x1F4C8; 21 <br>
<p>Alexander Mordvintsev, Eyvind Niklasson, Ettore Randazzo</p></summary>
<p>

**Abstract:** Neural Cellular Automata (NCA) have shown a remarkable ability to learn the required rules to "grow" images, classify morphologies, segment images, as well as to do general computation such as path-finding. We believe the inductive prior they introduce lends itself to the generation of textures. Textures in the natural world are often generated by variants of locally interacting reaction-diffusion systems. Human-made textures are likewise often generated in a local manner (textile weaving, for instance) or using rules with local dependencies (regular grids or geometric patterns). We demonstrate learning a texture generator from a single template image, with the generation method being embarrassingly parallel, exhibiting quick convergence and high fidelity of output, and requiring only some minimal assumptions around the underlying state manifold. Furthermore, we investigate properties of the learned models that are both useful and interesting, such as non-stationary dynamics and an inherent robustness to damage. Finally, we make qualitative claims that the behaviour exhibited by the NCA model is a learned, distributed, local algorithm to generate a texture, setting our method apart from existing work on texture generation. We discuss the advantages of such a paradigm.

</p>
</details>

<details><summary><b>Rethinking Skip Connection with Layer Normalization in Transformers and ResNets</b>
<a href="https://arxiv.org/abs/2105.07205">arxiv:2105.07205</a>
&#x1F4C8; 18 <br>
<p>Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, Yuexian Zou</p></summary>
<p>

**Abstract:** Skip connection, is a widely-used technique to improve the performance and the convergence of deep neural networks, which is believed to relieve the difficulty in optimization due to non-linearity by propagating a linear component through the neural network layers. However, from another point of view, it can also be seen as a modulating mechanism between the input and the output, with the input scaled by a pre-defined value one. In this work, we investigate how the scale factors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale will lead to spurious gradient exploding or vanishing in line with the deepness of the models, which could be addressed by normalization, in particular, layer normalization, which induces consistent improvements over the plain skip connection. Inspired by the findings, we further propose to adaptively adjust the scale of the input by recursively applying skip connection with layer normalization, which promotes the performance substantially and generalizes well across diverse tasks including both machine translation and image classification datasets.

</p>
</details>

<details><summary><b>Neural Trees for Learning on Graphs</b>
<a href="https://arxiv.org/abs/2105.07264">arxiv:2105.07264</a>
&#x1F4C8; 13 <br>
<p>Rajat Talak, Siyi Hu, Lisa Peng, Luca Carlone</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach for learning over graphs. Despite this success, existing GNNs are constrained by their local message-passing architecture and are provably limited in their expressive power. In this work, we propose a new GNN architecture -- the Neural Tree. The neural tree architecture does not perform message passing on the input graph, but on a tree-structured graph, called the H-tree, that is constructed from the input graph. Nodes in the H-tree correspond to subgraphs in the input graph, and they are reorganized in a hierarchical manner such that the parent of a node in the H-tree always corresponds to a larger subgraph in the input graph. We show that the neural tree architecture can approximate any smooth probability distribution function over an undirected graph. We also prove that the number of parameters needed to achieve an $ε$-approximation of the distribution function is exponential in the treewidth of the input graph, but linear in its size. We prove that any continuous $\mathcal{G}$-invariant/equivariant function can be approximated by a nonlinear combination of such probability distribution functions over $\mathcal{G}$. We apply the neural tree to semi-supervised node classification in 3D scene graphs, and show that these theoretical properties translate into significant gains in prediction accuracy, over the more traditional GNN architectures. We also show the applicability of the neural tree architecture to citation networks with large treewidth, by using a graph sub-sampling technique.

</p>
</details>

<details><summary><b>Move2Hear: Active Audio-Visual Source Separation</b>
<a href="https://arxiv.org/abs/2105.07142">arxiv:2105.07142</a>
&#x1F4C8; 10 <br>
<p>Sagnik Majumder, Ziad Al-Halah, Kristen Grauman</p></summary>
<p>

**Abstract:** We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent's camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model's ability to find minimal movement sequences with maximal payoff for audio source separation. Project: http://vision.cs.utexas.edu/projects/move2hear.

</p>
</details>

<details><summary><b>Cohort Shapley value for algorithmic fairness</b>
<a href="https://arxiv.org/abs/2105.07168">arxiv:2105.07168</a>
&#x1F4C8; 8 <br>
<p>Masayoshi Mase, Art B. Owen, Benjamin B. Seiler</p></summary>
<p>

**Abstract:** Cohort Shapley value is a model-free method of variable importance grounded in game theory that does not use any unobserved and potentially impossible feature combinations. We use it to evaluate algorithmic fairness, using the well known COMPAS recidivism data as our example. This approach allows one to identify for each individual in a data set the extent to which they were adversely or beneficially affected by their value of a protected attribute such as their race. The method can do this even if race was not one of the original predictors and even if it does not have access to a proprietary algorithm that has made the predictions. The grounding in game theory lets us define aggregate variable importance for a data set consistently with its per subject definitions. We can investigate variable importance for multiple quantities of interest in the fairness literature including false positive predictions.

</p>
</details>

<details><summary><b>Towards a Predictive Processing Implementation of the Common Model of Cognition</b>
<a href="https://arxiv.org/abs/2105.07308">arxiv:2105.07308</a>
&#x1F4C8; 7 <br>
<p>Alexander Ororbia, M. A. Kelly</p></summary>
<p>

**Abstract:** In this article, we present a cognitive architecture that is built from powerful yet simple neural models. Specifically, we describe an implementation of the common model of cognition grounded in neural generative coding and holographic associative memory. The proposed system creates the groundwork for developing agents that learn continually from diverse tasks as well as model human performance at larger scales than what is possible with existant cognitive architectures.

</p>
</details>

<details><summary><b>A Deep Metric Learning Approach to Account Linking</b>
<a href="https://arxiv.org/abs/2105.07263">arxiv:2105.07263</a>
&#x1F4C8; 7 <br>
<p>Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop, Nicholas Andrews</p></summary>
<p>

**Abstract:** We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and metadata of their corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity -- ranging from single posts to entire months of activity -- to a vector space, where samples by the same author map to nearby points. The approach does not require human-annotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.

</p>
</details>

<details><summary><b>Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect Rendering from a Single Image</b>
<a href="https://arxiv.org/abs/2105.07174">arxiv:2105.07174</a>
&#x1F4C8; 6 <br>
<p>Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Anil Kumar Tiwari</p></summary>
<p>

**Abstract:** The Bokeh Effect is one of the most desirable effects in photography for rendering artistic and aesthetic photos. Usually, it requires a DSLR camera with different aperture and shutter settings and certain photography skills to generate this effect. In smartphones, computational methods and additional sensors are used to overcome the physical lens and sensor limitations to achieve such effect. Most of the existing methods utilized additional sensor's data or pretrained network for fine depth estimation of the scene and sometimes use portrait segmentation pretrained network module to segment salient objects in the image. Because of these reasons, networks have many parameters, become runtime intensive and unable to run in mid-range devices. In this paper, we used an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for direct Bokeh effect rendering of images captured from the monocular camera. To further improve the perceptual quality of such effect, a stacked model consisting of two DMSHN modules is also proposed. Our model does not rely on any pretrained network module for Monocular Depth Estimation or Saliency Detection, thus significantly reducing the size of model and run time. Stacked DMSHN achieves state-of-the-art results on a large scale EBB! dataset with around 6x less runtime compared to the current state-of-the-art model in processing HD quality images.

</p>
</details>

<details><summary><b>Real-time Detection of Practical Universal Adversarial Perturbations</b>
<a href="https://arxiv.org/abs/2105.07334">arxiv:2105.07334</a>
&#x1F4C8; 5 <br>
<p>Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Emil C. Lupu</p></summary>
<p>

**Abstract:** Universal Adversarial Perturbations (UAPs) are a prominent class of adversarial examples that exploit the systemic vulnerabilities and enable physically realizable and robust attacks against Deep Neural Networks (DNNs). UAPs generalize across many different inputs; this leads to realistic and effective attacks that can be applied at scale. In this paper we propose HyperNeuron, an efficient and scalable algorithm that allows for the real-time detection of UAPs by identifying suspicious neuron hyper-activations. Our results show the effectiveness of HyperNeuron on multiple tasks (image classification, object detection), against a wide variety of universal attacks, and in realistic scenarios, like perceptual ad-blocking and adversarial patches. HyperNeuron is able to simultaneously detect both adversarial mask and patch UAPs with comparable or better performance than existing UAP defenses whilst introducing a significantly reduced latency of only 0.86 milliseconds per image. This suggests that many realistic and practical universal attacks can be reliably mitigated in real-time, which shows promise for the robust deployment of machine learning systems.

</p>
</details>

<details><summary><b>A brain basis of dynamical intelligence for AI and computational neuroscience</b>
<a href="https://arxiv.org/abs/2105.07284">arxiv:2105.07284</a>
&#x1F4C8; 5 <br>
<p>Joseph D. Monaco, Kanaka Rajan, Grace M. Hwang</p></summary>
<p>

**Abstract:** The deep neural nets of modern artificial intelligence (AI) have not achieved defining features of biological intelligence, including abstraction, causal learning, and energy-efficiency. While scaling to larger models has delivered performance improvements for current applications, more brain-like capacities may demand new theories, models, and methods for designing artificial learning systems. Here, we argue that this opportunity to reassess insights from the brain should stimulate cooperation between AI research and theory-driven computational neuroscience (CN). To motivate a brain basis of neural computation, we present a dynamical view of intelligence from which we elaborate concepts of sparsity in network structure, temporal dynamics, and interactive learning. In particular, we suggest that temporal dynamics, as expressed through neural synchrony, nested oscillations, and flexible sequences, provide a rich computational layer for reading and updating hierarchical models distributed in long-term memory networks. Moreover, embracing agent-centered paradigms in AI and CN will accelerate our understanding of the complex dynamics and behaviors that build useful world models. A convergence of AI/CN theories and objectives will reveal dynamical principles of intelligence for brains and engineered learning systems. This article was inspired by our symposium on dynamical neuroscience and machine learning at the 6th Annual US/NIH BRAIN Initiative Investigators Meeting.

</p>
</details>

<details><summary><b>Calibrating sufficiently</b>
<a href="https://arxiv.org/abs/2105.07283">arxiv:2105.07283</a>
&#x1F4C8; 5 <br>
<p>Dirk Tasche</p></summary>
<p>

**Abstract:** When probabilistic classifiers are trained and calibrated, the so-called grouping loss component of the calibration loss can easily be overlooked. Grouping loss refers to the gap between observable information and information actually exploited in the calibration exercise. We investigate the relation between grouping loss and the concept of sufficiency, identifying comonotonicity as a useful criterion for sufficiency. We revisit the probing reduction approach of Langford & Zadrozny (2005) and find that it produces an estimator of probabilistic classifiers that reduces grouping loss. Finally, we discuss Brier curves as tools to support training and 'sufficient' calibration of probabilistic classifiers.

</p>
</details>

<details><summary><b>Composite Localization for Human Pose Estimation</b>
<a href="https://arxiv.org/abs/2105.07245">arxiv:2105.07245</a>
&#x1F4C8; 5 <br>
<p>ZiFan Chen, Xin Qin, Chao Yang, Li Zhang</p></summary>
<p>

**Abstract:** The existing human pose estimation methods are confronted with inaccurate long-distance regression or high computational cost due to the complex learning objectives. This work proposes a novel deep learning framework for human pose estimation called composite localization to divide the complex learning objective into two simpler ones: a sparse heatmap to find the keypoint's approximate location and two short-distance offsetmaps to obtain its final precise coordinates. To realize the framework, we construct two types of composite localization networks: CLNet-ResNet and CLNet-Hourglass. We evaluate the networks on three benchmark datasets, including the Leeds Sports Pose dataset, the MPII Human Pose dataset, and the COCO keypoints detection dataset. The experimental results show that our CLNet-ResNet50 outperforms SimpleBaseline by 1.14% with about 1/2 GFLOPs. Our CLNet-Hourglass outperforms the original stacked-hourglass by 4.45% on COCO.

</p>
</details>

<details><summary><b>AgeFlow: Conditional Age Progression and Regression with Normalizing Flows</b>
<a href="https://arxiv.org/abs/2105.07239">arxiv:2105.07239</a>
&#x1F4C8; 5 <br>
<p>Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan</p></summary>
<p>

**Abstract:** Age progression and regression aim to synthesize photorealistic appearance of a given face image with aging and rejuvenation effects, respectively. Existing generative adversarial networks (GANs) based methods suffer from the following three major issues: 1) unstable training introducing strong ghost artifacts in the generated faces, 2) unpaired training leading to unexpected changes in facial attributes such as genders and races, and 3) non-bijective age mappings increasing the uncertainty in the face transformation. To overcome these issues, this paper proposes a novel framework, termed AgeFlow, to integrate the advantages of both flow-based models and GANs. The proposed AgeFlow contains three parts: an encoder that maps a given face to a latent space through an invertible neural network, a novel invertible conditional translation module (ICTM) that translates the source latent vector to target one, and a decoder that reconstructs the generated face from the target latent vector using the same encoder network; all parts are invertible achieving bijective age mappings. The novelties of ICTM are two-fold. First, we propose an attribute-aware knowledge distillation to learn the manipulation direction of age progression while keeping other unrelated attributes unchanged, alleviating unexpected changes in facial attributes. Second, we propose to use GANs in the latent space to ensure the learned latent vector indistinguishable from the real ones, which is much easier than traditional use of GANs in the image domain. Experimental results demonstrate superior performance over existing GANs-based methods on two benchmarked datasets. The source code is available at https://github.com/Hzzone/AgeFlow.

</p>
</details>

<details><summary><b>Window-Level is a Strong Denoising Surrogate</b>
<a href="https://arxiv.org/abs/2105.07153">arxiv:2105.07153</a>
&#x1F4C8; 5 <br>
<p>Ayaan Haque, Adam Wang, Abdullah-Al-Zubaer Imran</p></summary>
<p>

**Abstract:** CT image quality is heavily reliant on radiation dose, which causes a trade-off between radiation dose and image quality that affects the subsequent image-based diagnostic performance. However, high radiation can be harmful to both patients and operators. Several (deep learning-based) approaches have been attempted to denoise low dose images. However, those approaches require access to large training sets, specifically the full dose CT images for reference, which can often be difficult to obtain. Self-supervised learning is an emerging alternative for lowering the reference data requirement facilitating unsupervised learning. Currently available self-supervised CT denoising works are either dependent on foreign domain or pretexts are not very task-relevant. To tackle the aforementioned challenges, we propose a novel self-supervised learning approach, namely Self-Supervised Window-Leveling for Image DeNoising (SSWL-IDN), leveraging an innovative, task-relevant, simple, yet effective surrogate -- prediction of the window-leveled equivalent. SSWL-IDN leverages residual learning and a hybrid loss combining perceptual loss and MSE, all incorporated in a VAE framework. Our extensive (in- and cross-domain) experimentation demonstrates the effectiveness of SSWL-IDN in aggressive denoising of CT (abdomen and chest) images acquired at 5\% dose level only.

</p>
</details>

<details><summary><b>Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization</b>
<a href="https://arxiv.org/abs/2105.07331">arxiv:2105.07331</a>
&#x1F4C8; 4 <br>
<p>Haichao Yu, Linjie Yang, Humphrey Shi</p></summary>
<p>

**Abstract:** Post-training quantization methods use a set of calibration data to compute quantization ranges for network parameters and activations. The calibration data usually comes from the training dataset which could be inaccessible due to sensitivity of the data. In this work, we want to study such a problem: can we use out-of-domain data to calibrate the trained networks without knowledge of the original dataset? Specifically, we go beyond the domain of natural images to include drastically different domains such as X-ray images, satellite images and ultrasound images. We find cross-domain calibration leads to surprisingly stable performance of quantized models on 10 tasks in different image domains with 13 different calibration datasets. We also find that the performance of quantized models is correlated with the similarity of the Gram matrices between the source and calibration domains, which can be used as a criterion to choose calibration set for better performance. We believe our research opens the door to borrow cross-domain knowledge for network quantization and compression.

</p>
</details>

<details><summary><b>Regret Analysis of Distributed Online LQR Control for Unknown LTI Systems</b>
<a href="https://arxiv.org/abs/2105.07310">arxiv:2105.07310</a>
&#x1F4C8; 4 <br>
<p>Ting-Jui Chang, Shahin Shahrampour</p></summary>
<p>

**Abstract:** Online learning has recently opened avenues for rethinking classical optimal control beyond time-invariant cost metrics, and online controllers are designed when the performance criteria changes adversarially over time. Inspired by this line of research, we study the distributed online linear quadratic regulator (LQR) problem for linear time-invariant (LTI) systems with unknown dynamics. Consider a multi-agent network where each agent is modeled as a LTI system. The LTI systems are associated with time-varying quadratic costs that are revealed sequentially. The goal of the network is to collectively (i) estimate the unknown dynamics and (ii) compute local control sequences competitive to that of the best centralized policy in hindsight that minimizes the sum of costs for all agents. This problem is formulated as a {\it regret} minimization. We propose a distributed variant of the online LQR algorithm where each agent computes its system estimate during an exploration stage. The agent then applies distributed online gradient descent on a semi-definite programming (SDP) whose feasible set is based on the agent's system estimate. We prove that the regret bound of our proposed algorithm scales $\tilde{O}(T^{2/3})$, implying the consensus of the network over time. We also provide simulation results verifying our theoretical guarantee.

</p>
</details>

<details><summary><b>Adaptive Newton Sketch: Linear-time Optimization with Quadratic Convergence and Effective Hessian Dimensionality</b>
<a href="https://arxiv.org/abs/2105.07291">arxiv:2105.07291</a>
&#x1F4C8; 4 <br>
<p>Jonathan Lacotte, Yifei Wang, Mert Pilanci</p></summary>
<p>

**Abstract:** We propose a randomized algorithm with quadratic convergence rate for convex optimization problems with a self-concordant, composite, strongly convex objective function. Our method is based on performing an approximate Newton step using a random projection of the Hessian. Our first contribution is to show that, at each iteration, the embedding dimension (or sketch size) can be as small as the effective dimension of the Hessian matrix. Leveraging this novel fundamental result, we design an algorithm with a sketch size proportional to the effective dimension and which exhibits a quadratic rate of convergence. This result dramatically improves on the classical linear-quadratic convergence rates of state-of-the-art sub-sampled Newton methods. However, in most practical cases, the effective dimension is not known beforehand, and this raises the question of how to pick a sketch size as small as the effective dimension while preserving a quadratic convergence rate. Our second and main contribution is thus to propose an adaptive sketch size algorithm with quadratic convergence rate and which does not require prior knowledge or estimation of the effective dimension: at each iteration, it starts with a small sketch size, and increases it until quadratic progress is achieved. Importantly, we show that the embedding dimension remains proportional to the effective dimension throughout the entire path and that our method achieves state-of-the-art computational complexity for solving convex optimization programs with a strongly convex component.

</p>
</details>

<details><summary><b>Data-Driven Reachability Analysis from Noisy Data</b>
<a href="https://arxiv.org/abs/2105.07229">arxiv:2105.07229</a>
&#x1F4C8; 4 <br>
<p>Amr Alanwar, Anne Koch, Frank Allgöwer, Karl Henrik Johansson</p></summary>
<p>

**Abstract:** We consider the problem of computing reachable sets directly from noisy data without a given system model. Several reachability algorithms are presented, and their accuracy is shown to depend on the underlying system generating the data. First, an algorithm for computing over-approximated reachable sets based on matrix zonotopes is proposed for linear systems. Constrained matrix zonotopes are introduced to provide less conservative reachable sets at the cost of increased computational expenses and utilized to incorporate prior knowledge about the unknown system model. Then we extend the approach to polynomial systems and under the assumption of Lipschitz continuity to nonlinear systems. Theoretical guarantees are given for these algorithms in that they give a proper over-approximative reachable set containing the true reachable set. Multiple numerical examples show the applicability of the introduced algorithms, and accuracy comparisons are made between algorithms.

</p>
</details>

<details><summary><b>On the Distributional Properties of Adaptive Gradients</b>
<a href="https://arxiv.org/abs/2105.07222">arxiv:2105.07222</a>
&#x1F4C8; 4 <br>
<p>Zhang Zhiyi, Liu Ziyin</p></summary>
<p>

**Abstract:** Adaptive gradient methods have achieved remarkable success in training deep neural networks on a wide variety of tasks. However, not much is known about the mathematical and statistical properties of this family of methods. This work aims at providing a series of theoretical analyses of its statistical properties justified by experiments. In particular, we show that when the underlying gradient obeys a normal distribution, the variance of the magnitude of the \textit{update} is an increasing and bounded function of time and does not diverge. This work suggests that the divergence of variance is not the cause of the need for warm up of the Adam optimizer, contrary to what is believed in the current literature.

</p>
</details>

<details><summary><b>Multi-scale super-resolution generation of low-resolution scanned pathological images</b>
<a href="https://arxiv.org/abs/2105.07200">arxiv:2105.07200</a>
&#x1F4C8; 4 <br>
<p>Kai Sun, Yanhua Gao, Ting Xie, Xun Wang, Qingqing Yang, Le Chen, Kuansong Wang, Gang Yu</p></summary>
<p>

**Abstract:** Background. Digital pathology has aroused widespread interest in modern pathology. The key of digitalization is to scan the whole slide image (WSI) at high magnification. The lager the magnification is, the richer details WSI will provide, but the scanning time is longer and the file size of obtained is larger. Methods. We design a strategy to scan slides with low resolution (5X) and a super-resolution method is proposed to restore the image details when in diagnosis. The method is based on a multi-scale generative adversarial network, which sequentially generates three high-resolution images such as 10X, 20X and 40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are 24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680 and 0.512, which are better than other super-resolution networks. Visual scoring average and standard deviation from three pathologists is 3.63 plus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value of analysis of variance is 0.37, indicating that generated images include sufficient information for diagnosis. The average value of Kappa test is 0.99, meaning the diagnosis of generated images is highly consistent with that of the real images. Conclusion. This proposed method can generate high-quality 10X, 20X, 40X images from 5X images at the same time, in which the time and storage costs of digitalization can be effectively reduced up to 1/64 of the previous costs. The proposed method provides a better alternative for low-cost storage, faster image share of digital pathology. Keywords. Digital pathology; Super-resolution; Low resolution scanning; Low cost

</p>
</details>

<details><summary><b>ExSinGAN: Learning an Explainable Generative Model from a Single Image</b>
<a href="https://arxiv.org/abs/2105.07350">arxiv:2105.07350</a>
&#x1F4C8; 3 <br>
<p>ZiCheng Zhang, CongYing Han, TianDe Guo</p></summary>
<p>

**Abstract:** Generating images from a single sample, as a newly developing branch of image synthesis, has attracted extensive attention. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. On this basis, we design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works.

</p>
</details>

<details><summary><b>Understanding the Effect of Bias in Deep Anomaly Detection</b>
<a href="https://arxiv.org/abs/2105.07346">arxiv:2105.07346</a>
&#x1F4C8; 3 <br>
<p>Ziyu Ye, Yuxin Chen, Haitao Zheng</p></summary>
<p>

**Abstract:** Anomaly detection presents a unique challenge in machine learning, due to the scarcity of labeled anomaly data. Recent work attempts to mitigate such problems by augmenting training of deep anomaly detection models with additional labeled anomaly samples. However, the labeled data often does not align with the target distribution and introduces harmful bias to the trained model. In this paper, we aim to understand the effect of a biased anomaly set on anomaly detection. Concretely, we view anomaly detection as a supervised learning task where the objective is to optimize the recall at a given false positive rate. We formally study the relative scoring bias of an anomaly detector, defined as the difference in performance with respect to a baseline anomaly detector. We establish the first finite sample rates for estimating the relative scoring bias for deep anomaly detection, and empirically validate our theoretical results on both synthetic and real-world datasets. We also provide an extensive empirical study on how a biased training anomaly set affects the anomaly score function and therefore the detection performance on different anomaly classes. Our study demonstrates scenarios in which the biased anomaly set can be useful or problematic, and provides a solid benchmark for future research.

</p>
</details>

<details><summary><b>Self-supervised Learning on Graphs: Contrastive, Generative,or Predictive</b>
<a href="https://arxiv.org/abs/2105.07342">arxiv:2105.07342</a>
&#x1F4C8; 3 <br>
<p>Lirong Wu, Haitao Lin, Zhangyang Gao, Cheng Tan, Stan. Z. Li</p></summary>
<p>

**Abstract:** Deep learning on graphs has recently achieved remarkable success on a variety of tasks, while such success relies heavily on the massive and carefully labeled data. However, precise annotations are generally very expensive and time-consuming. To address this problem, self-supervised learning (SSL) is emerging as a new paradigm for extracting informative knowledge through well-designed pretext tasks without relying on manual labels. In this survey, we extend the concept of SSL, which first emerged in the fields of computer vision and natural language processing, to present a timely and comprehensive review of existing SSL techniques for graph data. Specifically, we divide existing graph SSL methods into three categories: contrastive, generative, and predictive. More importantly, unlike other surveys that only provide a high-level description of published research, we present an additional mathematical summary of existing works in a unified framework. Furthermore, to facilitate methodological development and empirical comparisons, we also summarize the commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental study of various algorithms. Finally, we discuss the technical challenges and potential future directions for improving graph self-supervised learning. Latest advances in graph SSL are summarized in a GitHub repository https://github.com/LirongWu/awesome-graph-self-supervised-learning.

</p>
</details>

<details><summary><b>Annotation Uncertainty in the Context of Grammatical Change</b>
<a href="https://arxiv.org/abs/2105.07270">arxiv:2105.07270</a>
&#x1F4C8; 3 <br>
<p>Marie-Luis Merten, Marcel Wever, Michaela Geierhos, Doris Tophinke, Eyke Hüllermeier</p></summary>
<p>

**Abstract:** This paper elaborates on the notion of uncertainty in the context of annotation in large text corpora, specifically focusing on (but not limited to) historical languages. Such uncertainty might be due to inherent properties of the language, for example, linguistic ambiguity and overlapping categories of linguistic description, but could also be caused by lacking annotation expertise. By examining annotation uncertainty in more detail, we identify the sources and deepen our understanding of the nature and different types of uncertainty encountered in daily annotation practice. Moreover, some practical implications of our theoretical findings are also discussed. Last but not least, this article can be seen as an attempt to reconcile the perspectives of the main scientific disciplines involved in corpus projects, linguistics and computer science, to develop a unified view and to highlight the potential synergies between these disciplines.

</p>
</details>

<details><summary><b>Regret Minimization Experience Replay in Off-Policy Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.07253">arxiv:2105.07253</a>
&#x1F4C8; 3 <br>
<p>Xu-Hui Liu, Zhenghai Xue, Jing-Cheng Pang, Shengyi Jiang, Feng Xu, Yang Yu</p></summary>
<p>

**Abstract:** In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World.

</p>
</details>

<details><summary><b>Make Bipedal Robots Learn How to Imitate</b>
<a href="https://arxiv.org/abs/2105.07193">arxiv:2105.07193</a>
&#x1F4C8; 3 <br>
<p>Vishal Kumar, Sinnu Susan Thomas</p></summary>
<p>

**Abstract:** Bipedal robots do not perform well as humans since they do not learn to walk like we do. In this paper we propose a method to train a bipedal robot to perform some basic movements with the help of imitation learning (IL) in which an instructor will perform the movement and the robot will try to mimic the instructor movement. To the best of our knowledge, this is the first time we train the robot to perform movements with a single video of the instructor and as the training is done based on joint angles the robot will keep its joint angles always in physical limits which in return help in faster training. The joints of the robot are identified by OpenPose architecture and then joint angle data is extracted with the help of angle between three points resulting in a noisy solution. We smooth the data using Savitzky-Golay filter and preserve the Simulatore data anatomy. An ingeniously written Deep Q Network (DQN) is trained with experience replay to make the robot learn to perform the movements as similar as the instructor. The implementation of the paper is made publicly available.

</p>
</details>

<details><summary><b>Learning Control Policies for Imitating Human Gaits</b>
<a href="https://arxiv.org/abs/2106.15273">arxiv:2106.15273</a>
&#x1F4C8; 2 <br>
<p>Utkarsh A. Mishra</p></summary>
<p>

**Abstract:** The work presented in this report introduces a framework aimed towards learning to imitate human gaits. Humans exhibit movements like walking, running, and jumping in the most efficient manner, which served as the source of motivation for this project. Skeletal and Musculoskeletal human models were considered for motions in the sagittal plane, and results from both were compared exhaustively. While skeletal models are driven with motor actuation, musculoskeletal models perform through muscle-tendon actuation. Model-free reinforcement learning algorithms were used to optimize inverse dynamics control actions to satisfy the objective of imitating a reference motion along with secondary objectives of minimizing effort in terms of power spent by motors and metabolic energy consumed by the muscles. On the one hand, the control actions for the motor actuated model is the target joint angles converted into joint torques through a Proportional-Differential controller. While on the other hand, the control actions for the muscle-tendon actuated model is the muscle excitations converted implicitly to muscle activations and then to muscle forces which apply moments on joints. Muscle-tendon actuated models were found to have superiority over motor actuation as they are inherently smooth due to muscle activation dynamics and don't need any external regularizers. Finally, a strategy that was used to obtain an optimal configuration of the significant decision variables in the framework was discussed. All the results and analysis are presented in an illustrative, qualitative, and quantitative manner. Supporting video links are provided in the Appendix.

</p>
</details>

<details><summary><b>Instance Segmentation of Microscopic Foraminifera</b>
<a href="https://arxiv.org/abs/2105.14191">arxiv:2105.14191</a>
&#x1F4C8; 2 <br>
<p>Thomas Haugland Johansen, Steffen Aagaard Sørensen, Kajsa Møllersen, Fred Godtliebsen</p></summary>
<p>

**Abstract:** Foraminifera are single-celled marine organisms that construct shells that remain as fossils in the marine sediments. Classifying and counting these fossils are important in e.g. paleo-oceanographic and -climatological research. However, the identification and counting process has been performed manually since the 1800s and is laborious and time-consuming. In this work, we present a deep learning-based instance segmentation model for classifying, detecting, and segmenting microscopic foraminifera. Our model is based on the Mask R-CNN architecture, using model weight parameters that have learned on the COCO detection dataset. We use a fine-tuning approach to adapt the parameters on a novel object detection dataset of more than 7000 microscopic foraminifera and sediment grains. The model achieves a (COCO-style) average precision of $0.78 \pm 0.00$ on the classification and detection task, and $0.80 \pm 0.00$ on the segmentation task. When the model is evaluated without challenging sediment grain images, the average precision for both tasks increases to $0.84 \pm 0.00$ and $0.86 \pm 0.00$, respectively. Prediction results are analyzed both quantitatively and qualitatively and discussed. Based on our findings we propose several directions for future work, and conclude that our proposed model is an important step towards automating the identification and counting of microscopic foraminifera.

</p>
</details>

<details><summary><b>Explainable Hierarchical Imitation Learning for Robotic Drink Pouring</b>
<a href="https://arxiv.org/abs/2105.07348">arxiv:2105.07348</a>
&#x1F4C8; 2 <br>
<p>Dandan Zhang, Yu Zheng, Qiang Li, Lei Wei, Dongsheng Zhang, Zhengyou Zhang</p></summary>
<p>

**Abstract:** To accurately pour drinks into various containers is an essential skill for service robots. However, drink pouring is a dynamic process and difficult to model. Traditional deep imitation learning techniques for implementing autonomous robotic pouring have an inherent black-box effect and require a large amount of demonstration data for model training. To address these issues, an Explainable Hierarchical Imitation Learning (EHIL) method is proposed in this paper such that a robot can learn high-level general knowledge and execute low-level actions across multiple drink pouring scenarios. Moreover, with EHIL, a logical graph can be constructed for task execution, through which the decision-making process for action generation can be made explainable to users and the causes of failure can be traced out. Based on the logical graph, the framework is manipulable to achieve different targets while the adaptability to unseen scenarios can be achieved in an explainable manner. A series of experiments have been conducted to verify the effectiveness of the proposed method. Results indicate that EHIL outperforms the traditional behavior cloning method in terms of success rate, adaptability, manipulability and explainability.

</p>
</details>

<details><summary><b>Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer</b>
<a href="https://arxiv.org/abs/2105.07322">arxiv:2105.07322</a>
&#x1F4C8; 2 <br>
<p>Arthita Ghosh, Max Ehrlich, Larry Davis, Rama Chellappa</p></summary>
<p>

**Abstract:** Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain.

</p>
</details>

<details><summary><b>Gradient Descent in Materio</b>
<a href="https://arxiv.org/abs/2105.11233">arxiv:2105.11233</a>
&#x1F4C8; 1 <br>
<p>Marcus N. Boon, Hans-Christian Ruiz Euler, Tao Chen, Bram van de Ven, Unai Alegre Ibarra, Peter A. Bobbert, Wilfred G. van der Wiel</p></summary>
<p>

**Abstract:** Deep learning, a multi-layered neural network approach inspired by the brain, has revolutionized machine learning. One of its key enablers has been backpropagation, an algorithm that computes the gradient of a loss function with respect to the weights in the neural network model, in combination with its use in gradient descent. However, the implementation of deep learning in digital computers is intrinsically wasteful, with energy consumption becoming prohibitively high for many applications. This has stimulated the development of specialized hardware, ranging from neuromorphic CMOS integrated circuits and integrated photonic tensor cores to unconventional, material-based computing systems. The learning process in these material systems, taking place, e.g., by artificial evolution or surrogate neural network modelling, is still a complicated and time-consuming process. Here, we demonstrate an efficient and accurate homodyne gradient extraction method for performing gradient descent on the loss function directly in the material system. We demonstrate the method in our recently developed dopant network processing units, where we readily realize all Boolean gates. This shows that gradient descent can in principle be fully implemented in materio using simple electronics, opening up the way to autonomously learning material systems.

</p>
</details>

<details><summary><b>CCMN: A General Framework for Learning with Class-Conditional Multi-Label Noise</b>
<a href="https://arxiv.org/abs/2105.07338">arxiv:2105.07338</a>
&#x1F4C8; 1 <br>
<p>Ming-Kun Xie, Sheng-Jun Huang</p></summary>
<p>

**Abstract:** Class-conditional noise commonly exists in machine learning tasks, where the class label is corrupted with a probability depending on its ground-truth. Many research efforts have been made to improve the model robustness against the class-conditional noise. However, they typically focus on the single label case by assuming that only one label is corrupted. In real applications, an instance is usually associated with multiple labels, which could be corrupted simultaneously with their respective conditional probabilities. In this paper, we formalize this problem as a general framework of learning with Class-Conditional Multi-label Noise (CCMN for short). We establish two unbiased estimators with error bounds for solving the CCMN problems, and further prove that they are consistent with commonly used multi-label loss functions. Finally, a new method for partial multi-label learning is implemented with unbiased estimator under the CCMN framework. Empirical studies on multiple datasets and various evaluation metrics validate the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>LocalNewton: Reducing Communication Bottleneck for Distributed Learning</b>
<a href="https://arxiv.org/abs/2105.07320">arxiv:2105.07320</a>
&#x1F4C8; 1 <br>
<p>Vipul Gupta, Avishek Ghosh, Michal Derezinski, Rajiv Khanna, Kannan Ramchandran, Michael Mahoney</p></summary>
<p>

**Abstract:** To address the communication bottleneck problem in distributed optimization within a master-worker framework, we propose LocalNewton, a distributed second-order algorithm with local averaging. In LocalNewton, the worker machines update their model in every iteration by finding a suitable second-order descent direction using only the data and model stored in their own local memory. We let the workers run multiple such iterations locally and communicate the models to the master node only once every few (say L) iterations. LocalNewton is highly practical since it requires only one hyperparameter, the number L of local iterations. We use novel matrix concentration-based techniques to obtain theoretical guarantees for LocalNewton, and we validate them with detailed empirical evaluation. To enhance practicability, we devise an adaptive scheme to choose L, and we show that this reduces the number of local iterations in worker machines between two model synchronizations as the training proceeds, successively refining the model quality at the master. Via extensive experiments using several real-world datasets with AWS Lambda workers and an AWS EC2 master, we show that LocalNewton requires fewer than 60% of the communication rounds (between master and workers) and less than 40% of the end-to-end running time, compared to state-of-the-art algorithms, to reach the same training~loss.

</p>
</details>

<details><summary><b>Content Analysis Application in Nursing: A Synthetic Knowledge Synthesis Meta-Study</b>
<a href="https://arxiv.org/abs/2105.07189">arxiv:2105.07189</a>
&#x1F4C8; 1 <br>
<p>Helena Blažun Vošner, Peter Kokol, Jernej Završnik, Danica Železnik</p></summary>
<p>

**Abstract:** Theoretical issues: With the explosive growth in the research literature production, the need for new approaches to structure knowledge emerged. Method: Synthetic content analysis was used in our meta-study. Results and discussion: Our meta-study showed that content analysis is frequently used in nursing research in a very wide spectrum of applications. The trend of its use is positive and it is used globally in a variety of research settings. The synthetic content analysis used in our study showed to be a very helpful tool in performing knowledge synthesis, replacing many of the routine activities of conventional synthesis with automated activities this making such studies more economically viable and easier to perform.

</p>
</details>

<details><summary><b>RIDnet: Radiologist-Inspired Deep Neural Network for Low-dose CT Denoising</b>
<a href="https://arxiv.org/abs/2105.07146">arxiv:2105.07146</a>
&#x1F4C8; 1 <br>
<p>Kecheng Chen, Jiayu Sun, Jiang Shen, Jixiang Luo, Xinyu Zhang, Xuelin Pan, Dongsheng Wu, Yue Zhao, Miguel Bento, Yazhou Ren, Xiaorong Pu</p></summary>
<p>

**Abstract:** Being low-level radiation exposure and less harmful to health, low-dose computed tomography (LDCT) has been widely adopted in the early screening of lung cancer and COVID-19. LDCT images inevitably suffer from the degradation problem caused by complex noises. It was reported that, compared with commercial iterative reconstruction methods, deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved competitive performance. Most existing DL-based methods focus on the local information extracted by CNN, while ignoring both explicit non-local and context information (which are leveraged by radiologists). To address this issue, we propose a novel deep learning model named radiologist-inspired deep denoising network (RIDnet) to imitate the workflow of a radiologist reading LDCT images. Concretely, the proposed model explicitly integrates all the local, non-local and context information rather than local information only. Our radiologist-inspired model is potentially favoured by radiologists as a familiar workflow. A double-blind reader study on a public clinical dataset shows that, compared with state-of-the-art methods, our proposed model achieves the most impressive performance in terms of the structural fidelity, the noise suppression and the overall score. As a physicians-inspired model, RIDnet gives a new research roadmap that takes into account the behavior of physicians when designing decision support tools for assisting clinical diagnosis. Models and code are available at https://github.com/tonyckc/RIDnet_demo.

</p>
</details>

<details><summary><b>A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts</b>
<a href="https://arxiv.org/abs/2105.07190">arxiv:2105.07190</a>
&#x1F4C8; 0 <br>
<p>Gesina Schwalbe, Bettina Finzel</p></summary>
<p>

**Abstract:** In the meantime, a wide variety of terminologies, motivations, approaches and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. In the literature many taxonomies for XAI methods of varying level of detail and depth can be found. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts, and provides a taxonomy of XAI methods that is complete with respect to notions present in the current state-of-research. In a structured literature analysis and meta-study we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by in total more than 50 diverse selected example methods, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview on XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.

</p>
</details>

<details><summary><b>BubbleNet: Inferring micro-bubble dynamics with semi-physics-informed deep learning</b>
<a href="https://arxiv.org/abs/2105.07179">arxiv:2105.07179</a>
&#x1F4C8; 0 <br>
<p>Hanfeng Zhai, Quan Zhou, Guohui Hu</p></summary>
<p>

**Abstract:** Micro-bubbles and bubbly flows are widely observed and applied in chemical engineering, medicine, involves deformation, rupture, and collision of bubbles, phase mixture, etc. We study bubble dynamics by setting up two numerical simulation cases: bubbly flow with a single bubble and multiple bubbles, both confined in the microchannel, with parameters corresponding to their medical backgrounds. Both the cases have their medical background applications. Multiphase flow simulation requires high computation accuracy due to possible component losses that may be caused by sparse meshing during the computation. Hence, data-driven methods can be adopted as an useful tool. Based on physics-informed neural networks (PINNs), we propose a novel deep learning framework BubbleNet, which entails three main parts: deep neural networks (DNN) with sub nets for predicting different physics fields; the semi-physics-informed part, with only the fluid continuum condition and the pressure Poisson equation $\mathcal{P}$ encoded within; the time discretized normalizer (TDN), an algorithm to normalize field data per time step before training. We apply the traditional DNN and our BubbleNet to train the coarsened simulation data and predict the physics fields of both the two bubbly flow cases. The BubbleNets are trained for both with and without $\mathcal{P}$, from which we conclude that the 'physics-informed' part can serve as inner supervision. Results indicate our framework can predict the physics fields more accurately, estimating the prediction absolute errors. Our deep learning predictions outperform traditional numerical methods computed with similar data density meshing. The proposed network can potentially be applied to many other engineering fields.

</p>
</details>


[Next Page]({{ '/2021/05/14/2021.05.14.html' | relative_url }})
