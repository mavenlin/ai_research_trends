Prev: [2022.11.13]({{ '/2022/11/13/2022.11.13.html' | relative_url }})  Next: [2022.11.15]({{ '/2022/11/15/2022.11.15.html' | relative_url }})
{% raw %}
## Summary for 2022-11-14, created on 2022-11-24


<details><summary><b>Legged Locomotion in Challenging Terrains using Egocentric Vision</b>
<a href="https://arxiv.org/abs/2211.07638">arxiv:2211.07638</a>
&#x1F4C8; 878 <br>
<p>Ananye Agarwal, Ashish Kumar, Jitendra Malik, Deepak Pathak</p></summary>
<p>

**Abstract:** Animals are capable of precise and agile locomotion using vision. Replicating this ability has been a long-standing goal in robotics. The traditional approach has been to decompose this problem into elevation mapping and foothold planning phases. The elevation mapping, however, is susceptible to failure and large noise artifacts, requires specialized hardware, and is biologically implausible. In this paper, we present the first end-to-end locomotion system capable of traversing stairs, curbs, stepping stones, and gaps. We show this result on a medium-sized quadruped robot using a single front-facing depth camera. The small size of the robot necessitates discovering specialized gait patterns not seen elsewhere. The egocentric camera requires the policy to remember past information to estimate the terrain under its hind feet. We train our policy in simulation. Training has two phases - first, we train a policy using reinforcement learning with a cheap-to-compute variant of depth image and then in phase 2 distill it into the final policy that uses depth using supervised learning. The resulting policy transfers to the real world and is able to run in real-time on the limited compute of the robot. It can traverse a large variety of terrain while being robust to perturbations like pushes, slippery surfaces, and rocky terrain. Videos are at https://vision-locomotion.github.io

</p>
</details>

<details><summary><b>Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces</b>
<a href="https://arxiv.org/abs/2211.07292">arxiv:2211.07292</a>
&#x1F4C8; 252 <br>
<p>Dominic Rampas, Pablo Pernias, Elea Zhong, Marc Aubreville</p></summary>
<p>

**Abstract:** Conditional text-to-image generation has seen countless recent improvements in terms of quality, diversity and fidelity. Nevertheless, most state-of-the-art models require numerous inference steps to produce faithful generations, resulting in performance bottlenecks for end-user applications. In this paper we introduce Paella, a novel text-to-image model requiring less than 10 steps to sample high-fidelity images, using a speed-optimized architecture allowing to sample a single image in less than 500 ms, while having 573M parameters. The model operates on a compressed & quantized latent space, it is conditioned on CLIP embeddings and uses an improved sampling function over previous works. Aside from text-conditional image generation, our model is able to do latent space interpolation and image manipulations such as inpainting, outpainting, and structural editing. We release all of our code and pretrained models at https://github.com/dome272/Paella

</p>
</details>

<details><summary><b>General Intelligence Requires Rethinking Exploration</b>
<a href="https://arxiv.org/abs/2211.07819">arxiv:2211.07819</a>
&#x1F4C8; 112 <br>
<p>Minqi Jiang, Tim Rocktäschel, Edward Grefenstette</p></summary>
<p>

**Abstract:** We are at the cusp of a transition from "learning from data" to "learning what data to learn from" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.

</p>
</details>

<details><summary><b>QueryForm: A Simple Zero-shot Form Entity Query Framework</b>
<a href="https://arxiv.org/abs/2211.07730">arxiv:2211.07730</a>
&#x1F4C8; 85 <br>
<p>Zifeng Wang, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong Su, Hao Zhang, Jennifer Dy, Vincent Perot, Tomas Pfister</p></summary>
<p>

**Abstract:** Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%) zero-shot benchmark, with a smaller model size and no additional image input.

</p>
</details>

<details><summary><b>Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models</b>
<a href="https://arxiv.org/abs/2211.07793">arxiv:2211.07793</a>
&#x1F4C8; 78 <br>
<p>Zhihong Pan, Xin Zhou, Hao Tian</p></summary>
<p>

**Abstract:** Transferring large amount of high resolution images over limited bandwidth is an important but very challenging task. Compressing images using extremely low bitrates (<0.1 bpp) has been studied but it often results in low quality images of heavy artifacts due to the strong constraint in the number of bits available for the compressed data. It is often said that a picture is worth a thousand words but on the other hand, language is very powerful in capturing the essence of an image using short descriptions. With the recent success of diffusion models for text-to-image generation, we propose a generative image compression method that demonstrates the potential of saving an image as a short text embedding which in turn can be used to generate high-fidelity images which is equivalent to the original one perceptually. For a given image, its corresponding text embedding is learned using the same optimization process as the text-to-image diffusion model itself, using a learnable text embedding as input after bypassing the original transformer. The optimization is applied together with a learning compression model to achieve extreme compression of low bitrates <0.1 bpp. Based on our experiments measured by a comprehensive set of image quality metrics, our method outperforms the other state-of-the-art deep learning methods in terms of both perceptual quality and diversity.

</p>
</details>

<details><summary><b>On Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and Model Calibration</b>
<a href="https://arxiv.org/abs/2211.07795">arxiv:2211.07795</a>
&#x1F4C8; 77 <br>
<p>Nauman Dawalatabad, Sameer Khurana, Antoine Laurent, James Glass</p></summary>
<p>

**Abstract:** Pseudo-label (PL) filtering forms a crucial part of Self-Training (ST) methods for unsupervised domain adaptation. Dropout-based Uncertainty-driven Self-Training (DUST) proceeds by first training a teacher model on source domain labeled data. Then, the teacher model is used to provide PLs for the unlabeled target domain data. Finally, we train a student on augmented labeled and pseudo-labeled data. The process is iterative, where the student becomes the teacher for the next DUST iteration. A crucial step that precedes the student model training in each DUST iteration is filtering out noisy PLs that could lead the student model astray. In DUST, we proposed a simple, effective, and theoretically sound PL filtering strategy based on the teacher model's uncertainty about its predictions on unlabeled speech utterances. We estimate the model's uncertainty by computing disagreement amongst multiple samples drawn from the teacher model during inference by injecting noise via dropout. In this work, we show that DUST's PL filtering, as initially used, may fail under severe source and target domain mismatch. We suggest several approaches to eliminate or alleviate this issue. Further, we bring insights from the research in neural network model calibration to DUST and show that a well-calibrated model correlates strongly with a positive outcome of the DUST PL filtering step.

</p>
</details>

<details><summary><b>EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</b>
<a href="https://arxiv.org/abs/2211.07636">arxiv:2211.07636</a>
&#x1F4C8; 60 <br>
<p>Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao</p></summary>
<p>

**Abstract:** We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we will release all the code and models at \url{https://github.com/baaivision/EVA}.

</p>
</details>

<details><summary><b>MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation</b>
<a href="https://arxiv.org/abs/2211.07302">arxiv:2211.07302</a>
&#x1F4C8; 59 <br>
<p>Chang-Bin Jeon, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon, Kyogu Lee</p></summary>
<p>

**Abstract:** Separation of multiple singing voices into each voice is a rarely studied area in music source separation research. The absence of a benchmark dataset has hindered its progress. In this paper, we present an evaluation dataset and provide baseline studies for multiple singing voices separation. First, we introduce MedleyVox, an evaluation dataset for multiple singing voices separation that corresponds to such categories. We specify the problem definition in this dataset by categorizing the problem into i) duet, ii) unison, iii)main vs. rest, and iv) N-singing separation. Second, we present a strategy for construction of multiple singing mixtures using various single-singing datasets. This can be used to obtain training data. Third, we propose the improved super-resolution network (iSRNet). Jointly trained with the Conv-TasNet and the multi-singing mixture construction strategy, the proposed iSRNet achieved comparable performance to ideal time-frequency masks on duet and unison subsets of MedleyVox. Audio samples, the dataset, and codes are available on our GitHub page (https://github.com/jeonchangbin49/MedleyVox).

</p>
</details>

<details><summary><b>Grafting Pre-trained Models for Multimodal Headline Generation</b>
<a href="https://arxiv.org/abs/2211.07210">arxiv:2211.07210</a>
&#x1F4C8; 56 <br>
<p>Lingfeng Qiao, Chen Wu, Ye Liu, Haoyuan Peng, Di Yin, Bo Ren</p></summary>
<p>

**Abstract:** Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained language models and video-language models have achieved significant progress in related downstream tasks. However, none of them can be directly applied to multimodal headline architecture where we need both multimodal encoder and sentence decoder. A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities. In this paper, we propose a novel approach to graft the video encoder from the pre-trained video-language model on the generative pre-trained language model. We also present a consensus fusion mechanism for the integration of different components, via inter/intra modality relation. Empirically, experiments show that the grafted model achieves strong results on a brand-new dataset collected from real-world applications.

</p>
</details>

<details><summary><b>Logical Tasks for Measuring Extrapolation and Rule Comprehension</b>
<a href="https://arxiv.org/abs/2211.07727">arxiv:2211.07727</a>
&#x1F4C8; 40 <br>
<p>Ippei Fujisawa, Ryota Kanai</p></summary>
<p>

**Abstract:** Logical reasoning is essential in a variety of human activities. A representative example of a logical task is mathematics. Recent large-scale models trained on large datasets have been successful in various fields, but their reasoning ability in arithmetic tasks is limited, which we reproduce experimentally. Here, we recast this limitation as not unique to mathematics but common to tasks that require logical operations. We then propose a new set of tasks, termed logical tasks, which will be the next challenge to address. This higher point of view helps the development of inductive biases that have broad impact beyond the solution of individual tasks. We define and characterize logical tasks and discuss system requirements for their solution. Furthermore, we discuss the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias. Finally, we provide directions for solving logical tasks.

</p>
</details>

<details><summary><b>Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts</b>
<a href="https://arxiv.org/abs/2211.07250">arxiv:2211.07250</a>
&#x1F4C8; 40 <br>
<p>Karim M. Ibrahim, Elena V. Epure, Geoffroy Peeters, Gaël Richard</p></summary>
<p>

**Abstract:** As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed user-aware autotaggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, the autotagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from their stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time 1) by leveraging user-aware music autotaggers, and 2) by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a context-aware personalized music retrieval system is feasible, but the performance decreases in the case of new users, new tracks or when the number of context classes increases.

</p>
</details>

<details><summary><b>3D Reconstruction-Based Seed Counting of Sorghum Panicles for Agricultural Inspection</b>
<a href="https://arxiv.org/abs/2211.07748">arxiv:2211.07748</a>
&#x1F4C8; 20 <br>
<p>Harry Freeman, Eric Schneider, Chung Hee Kim, Moonyoung Lee, George Kantor</p></summary>
<p>

**Abstract:** In this paper, we present a method for creating high-quality 3D models of sorghum panicles for phenotyping in breeding experiments. This is achieved with a novel reconstruction approach that uses seeds as semantic landmarks in both 2D and 3D. To evaluate the performance, we develop a new metric for assessing the quality of reconstructed point clouds without having a ground-truth point cloud. Finally, a counting method is presented where the density of seed centers in the 3D model allows 2D counts from multiple views to be effectively combined into a whole-panicle count. We demonstrate that using this method to estimate seed count and weight for sorghum outperforms count extrapolation from 2D images, an approach used in most state of the art methods for seeds and grains of comparable size.

</p>
</details>

<details><summary><b>Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding</b>
<a href="https://arxiv.org/abs/2211.07634">arxiv:2211.07634</a>
&#x1F4C8; 19 <br>
<p>Mirac Suzgun, Luke Melas-Kyriazi, Dan Jurafsky</p></summary>
<p>

**Abstract:** In open-ended natural-language generation, existing text decoding methods typically struggle to produce text which is both diverse and high-quality. Greedy and beam search are known to suffer from text degeneration and linguistic diversity issues, while temperature, top-k, and nucleus sampling often yield diverse but low-quality outputs. In this work, we present crowd sampling, a family of decoding methods based on Bayesian risk minimization, to address this diversity-quality trade-off. Inspired by the principle of "the wisdom of the crowd," crowd sampling seeks to select a candidate from a pool of candidates that has the least expected risk (i.e., highest expected reward) under a generative model according to a given utility function. Crowd sampling can be seen as a generalization of numerous existing methods, including majority voting, and in practice, it can be used as a drop-in replacement for existing sampling methods. Extensive experiments show that crowd sampling delivers improvements of 3-7 ROUGE and BLEU points across a wide range of tasks, including summarization, data-to-text, translation, and textual style transfer, while achieving new state-of-the-art results on WebNLG and WMT'16.

</p>
</details>

<details><summary><b>Agent-State Construction with Auxiliary Inputs</b>
<a href="https://arxiv.org/abs/2211.07805">arxiv:2211.07805</a>
&#x1F4C8; 18 <br>
<p>Ruo Yu Tao, Adam White, Marlos C. Machado</p></summary>
<p>

**Abstract:** In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, are rarely emphasized, and it is not clear what their role or impact is. In this work we explore this idea further, and relate these auxiliary inputs to prior classic approaches to state construction. We present a series of examples illustrating the different ways of using auxiliary inputs for reinforcement learning. We show that these auxiliary inputs can be used to discriminate between observations that would otherwise be aliased, leading to more expressive features that smoothly interpolate between different states. Finally, we show that this approach is complementary to state-of-the-art methods such as recurrent neural networks and truncated back-propagation through time, and acts as a heuristic that facilitates longer temporal credit assignment, leading to better performance.

</p>
</details>

<details><summary><b>A Rigorous Study Of The Deep Taylor Decomposition</b>
<a href="https://arxiv.org/abs/2211.08425">arxiv:2211.08425</a>
&#x1F4C8; 16 <br>
<p>Leon Sixt, Tim Landgraf</p></summary>
<p>

**Abstract:** Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate the DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient$\times$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie in the same linear regions as the input - contrary to a fundamental assumption of the Taylor theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims.

</p>
</details>

<details><summary><b>Seeded iterative clustering for histology region identification</b>
<a href="https://arxiv.org/abs/2211.07425">arxiv:2211.07425</a>
&#x1F4C8; 10 <br>
<p>Eduard Chelebian, Francesco Ciompi, Carolina Wählby</p></summary>
<p>

**Abstract:** Annotations are necessary to develop computer vision algorithms for histopathology, but dense annotations at a high resolution are often time-consuming to make. Deep learning models for segmentation are a way to alleviate the process, but require large amounts of training data, training times and computing power. To address these issues, we present seeded iterative clustering to produce a coarse segmentation densely and at the whole slide level. The algorithm uses precomputed representations as the clustering space and a limited amount of sparse interactive annotations as seeds to iteratively classify image patches. We obtain a fast and effective way of generating dense annotations for whole slide images and a framework that allows the comparison of neural network latent representations in the context of transfer learning.

</p>
</details>

<details><summary><b>Hierarchical Inference of the Lensing Convergence from Photometric Catalogs with Bayesian Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2211.07807">arxiv:2211.07807</a>
&#x1F4C8; 9 <br>
<p>Ji Won Park, Simon Birrer, Madison Ueland, Miles Cranmer, Adriano Agnello, Sebastian Wagner-Carena, Philip J. Marshall, Aaron Roodman, the LSST Dark Energy Science Collaboration</p></summary>
<p>

**Abstract:** We present a Bayesian graph neural network (BGNN) that can estimate the weak lensing convergence ($κ$) from photometric measurements of galaxies along a given line of sight. The method is of particular interest in strong gravitational time delay cosmography (TDC), where characterizing the "external convergence" ($κ_{\rm ext}$) from the lens environment and line of sight is necessary for precise inference of the Hubble constant ($H_0$). Starting from a large-scale simulation with a $κ$ resolution of $\sim$1$'$, we introduce fluctuations on galaxy-galaxy lensing scales of $\sim$1$''$ and extract random sightlines to train our BGNN. We then evaluate the model on test sets with varying degrees of overlap with the training distribution. For each test set of 1,000 sightlines, the BGNN infers the individual $κ$ posteriors, which we combine in a hierarchical Bayesian model to yield constraints on the hyperparameters governing the population. For a test field well sampled by the training set, the BGNN recovers the population mean of $κ$ precisely and without bias, resulting in a contribution to the $H_0$ error budget well under 1\%. In the tails of the training set with sparse samples, the BGNN, which can ingest all available information about each sightline, extracts more $κ$ signal compared to a simplified version of the traditional method based on matching galaxy number counts, which is limited by sample variance. Our hierarchical inference pipeline using BGNNs promises to improve the $κ_{\rm ext}$ characterization for precision TDC. The implementation of our pipeline is available as a public Python package, Node to Joy.

</p>
</details>

<details><summary><b>Redeeming Intrinsic Rewards via Constrained Optimization</b>
<a href="https://arxiv.org/abs/2211.07627">arxiv:2211.07627</a>
&#x1F4C8; 9 <br>
<p>Eric Chen, Zhang-Wei Hong, Joni Pajarinen, Pulkit Agrawal</p></summary>
<p>

**Abstract:** State-of-the-art reinforcement learning (RL) algorithms typically use random sampling (e.g., $ε$-greedy) for exploration, but this method fails on hard exploration tasks like Montezuma's Revenge. To address the challenge of exploration, prior works incentivize exploration by rewarding the agent when it visits novel states. Such intrinsic rewards (also called exploration bonus or curiosity) often lead to excellent performance on hard exploration tasks. However, on easy exploration tasks, the agent gets distracted by intrinsic rewards and performs unnecessary exploration even when sufficient task (also called extrinsic) reward is available. Consequently, such an overly curious agent performs worse than an agent trained with only task reward. Such inconsistency in performance across tasks prevents the widespread use of intrinsic rewards with RL algorithms. We propose a principled constrained optimization procedure called Extrinsic-Intrinsic Policy Optimization (EIPO) that automatically tunes the importance of the intrinsic reward: it suppresses the intrinsic reward when exploration is unnecessary and increases it when exploration is required. The results is superior exploration that does not require manual tuning in balancing the intrinsic reward against the task reward. Consistent performance gains across sixty-one ATARI games validate our claim. The code is available at https://github.com/Improbable-AI/eipo.

</p>
</details>

<details><summary><b>Variational Quantum Algorithms for Chemical Simulation and Drug Discovery</b>
<a href="https://arxiv.org/abs/2211.07854">arxiv:2211.07854</a>
&#x1F4C8; 8 <br>
<p>Hasan Mustafa, Sai Nandan Morapakula, Prateek Jain, Srinjoy Ganguly</p></summary>
<p>

**Abstract:** Quantum computing has gained a lot of attention recently, and scientists have seen potential applications in this field using quantum computing for Cryptography and Communication to Machine Learning and Healthcare. Protein folding has been one of the most interesting areas to study, and it is also one of the biggest problems of biochemistry. Each protein folds distinctively, and the difficulty of finding its stable shape rapidly increases with an increase in the number of amino acids in the chain. A moderate protein has about 100 amino acids, and the number of combinations one needs to verify to find the stable structure is enormous. At some point, the number of these combinations will be so vast that classical computers cannot even attempt to solve them. In this paper, we examine how this problem can be solved with the help of quantum computing using two different algorithms, Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA), using Qiskit Nature. We compare the results of different quantum hardware and simulators and check how error mitigation affects the performance. Further, we make comparisons with SoTA algorithms and evaluate the reliability of the method.

</p>
</details>

<details><summary><b>YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations</b>
<a href="https://arxiv.org/abs/2211.07131">arxiv:2211.07131</a>
&#x1F4C8; 8 <br>
<p>Eunjin Choi, Yoonjin Chung, Seolhee Lee, JongIk Jeon, Taegyun Kwon, Juhan Nam</p></summary>
<p>

**Abstract:** Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.

</p>
</details>

<details><summary><b>Robust Deep Learning for Autonomous Driving</b>
<a href="https://arxiv.org/abs/2211.07772">arxiv:2211.07772</a>
&#x1F4C8; 7 <br>
<p>Charles Corbière</p></summary>
<p>

**Abstract:** The last decade's research in artificial intelligence had a significant impact on the advance of autonomous driving. Yet, safety remains a major concern when it comes to deploying such systems in high-risk environments. The objective of this thesis is to develop methodological tools which provide reliable uncertainty estimates for deep neural networks. First, we introduce a new criterion to reliably estimate model confidence: the true class probability (TCP). We show that TCP offers better properties for failure prediction than current uncertainty measures. Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. The relevance of the proposed approach is validated on image classification and semantic segmentation datasets. Then, we extend our learned confidence approach to the task of domain adaptation where it improves the selection of pseudo-labels in self-training methods. Finally, we tackle the challenge of jointly detecting misclassification and out-of-distributions samples by introducing a new uncertainty measure based on evidential models and defined on the simplex.

</p>
</details>

<details><summary><b>Learning to Optimize with Stochastic Dominance Constraints</b>
<a href="https://arxiv.org/abs/2211.07767">arxiv:2211.07767</a>
&#x1F4C8; 7 <br>
<p>Hanjun Dai, Yuan Xue, Niao He, Bethany Wang, Na Li, Dale Schuurmans, Bo Dai</p></summary>
<p>

**Abstract:** In real-world decision-making, uncertainty is important yet difficult to handle. Stochastic dominance provides a theoretically sound approach for comparing uncertain quantities, but optimization with stochastic dominance constraints is often computationally expensive, which limits practical applicability. In this paper, we develop a simple yet efficient approach for the problem, the Light Stochastic Dominance Solver (light-SD), that leverages useful properties of the Lagrangian. We recast the inner optimization in the Lagrangian as a learning problem for surrogate approximation, which bypasses apparent intractability and leads to tractable updates or even closed-form solutions for gradient calculations. We prove convergence of the algorithm and test it empirically. The proposed light-SD demonstrates superior performance on several representative problems ranging from finance to supply chain management.

</p>
</details>

<details><summary><b>Do Neural Networks Trained with Topological Features Learn Different Internal Representations?</b>
<a href="https://arxiv.org/abs/2211.07697">arxiv:2211.07697</a>
&#x1F4C8; 7 <br>
<p>Sarah McGuire, Shane Jackson, Tegan Emerson, Henry Kvinge</p></summary>
<p>

**Abstract:** There is a growing body of work that leverages features extracted via topological data analysis to train machine learning models. While this field, sometimes known as topological machine learning (TML), has seen some notable successes, an understanding of how the process of learning from topological features differs from the process of learning from raw data is still limited. In this work, we begin to address one component of this larger issue by asking whether a model trained with topological features learns internal representations of data that are fundamentally different than those learned by a model trained with the original raw data. To quantify ``different'', we exploit two popular metrics that can be used to measure the similarity of the hidden representations of data within neural networks, neural stitching and centered kernel alignment. From these we draw a range of conclusions about how training with topological features does and does not change the representations that a model learns. Perhaps unsurprisingly, we find that structurally, the hidden representations of models trained and evaluated on topological features differ substantially compared to those trained and evaluated on the corresponding raw data. On the other hand, our experiments show that in some cases, these representations can be reconciled (at least to the degree required to solve the corresponding task) using a simple affine transformation. We conjecture that this means that neural networks trained on raw data may extract some limited topological features in the process of making predictions.

</p>
</details>

<details><summary><b>What Images are More Memorable to Machines?</b>
<a href="https://arxiv.org/abs/2211.07625">arxiv:2211.07625</a>
&#x1F4C8; 7 <br>
<p>Junlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid</p></summary>
<p>

**Abstract:** This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorialize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that "complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.

</p>
</details>

<details><summary><b>Generalized Stable Weights via Neural Gibbs Density</b>
<a href="https://arxiv.org/abs/2211.07533">arxiv:2211.07533</a>
&#x1F4C8; 7 <br>
<p>Yoshiaki Kitazawa</p></summary>
<p>

**Abstract:** We present a generalized balancing weight method fully available for estimating causal effects for an arbitrary mixture of discrete and continuous interventions. Our weights are trainable through back-propagation, and we give a method for estimating the weights via neural network algorithms. In addition, we also provide a method to measure the performance of our weights by estimating the mutual information for the balanced distribution. Our method is easy to implement with any present deep learning libraries, and the weights from it can be used in most state-of-art supervised algorithms.

</p>
</details>

<details><summary><b>On Analyzing the Role of Image for Visual-enhanced Relation Extraction</b>
<a href="https://arxiv.org/abs/2211.07504">arxiv:2211.07504</a>
&#x1F4C8; 7 <br>
<p>Lei Li, Xiang Chen, Shuofei Qiao, Feiyu Xiong, Huajun Chen, Ningyu Zhang</p></summary>
<p>

**Abstract:** Multimodal relation extraction is an essential task for knowledge graph construction. In this paper, we take an in-depth empirical analysis that indicates the inaccurate information in the visual scene graph leads to poor modal alignment weights, further degrading performance. Moreover, the visual shuffle experiments illustrate that the current approaches may not take full advantage of visual information. Based on the above observation, we further propose a strong baseline with an implicit fine-grained multimodal alignment based on Transformer for multimodal relation extraction. Experimental results demonstrate the better performance of our method. Codes are available at https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal.

</p>
</details>

<details><summary><b>Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions</b>
<a href="https://arxiv.org/abs/2211.07501">arxiv:2211.07501</a>
&#x1F4C8; 7 <br>
<p>Yong-Lu Li, Hongwei Fan, Zuoyu Qiu, Yiming Dou, Liang Xu, Hao-Shu Fang, Peiyang Guo, Haisheng Su, Dongliang Wang, Wei Wu, Cewu Lu</p></summary>
<p>

**Abstract:** Spatio-temporal Human-Object Interaction (ST-HOI) detection aims at detecting HOIs from videos, which is crucial for activity understanding. In daily HOIs, humans often interact with a variety of objects, e.g., holding and touching dozens of household items in cleaning. However, existing whole body-object interaction video benchmarks usually provide limited object classes. Here, we introduce a new benchmark based on AVA: Discovering Interacted Objects (DIO) including 51 interactions and 1,000+ objects. Accordingly, an ST-HOI learning task is proposed expecting vision systems to track human actors, detect interactions and simultaneously discover interacted objects. Even though today's detectors/trackers excel in object detection/tracking tasks, they perform unsatisfied to localize diverse/unseen objects in DIO. This profoundly reveals the limitation of current vision systems and poses a great challenge. Thus, how to leverage spatio-temporal cues to address object discovery is explored, and a Hierarchical Probe Network (HPN) is devised to discover interacted objects utilizing hierarchical spatio-temporal human/context cues. In extensive experiments, HPN demonstrates impressive performance. Data and code are available at https://github.com/DirtyHarryLYL/HAKE-AVA.

</p>
</details>

<details><summary><b>Butterfly Effect Attack: Tiny and Seemingly Unrelated Perturbations for Object Detection</b>
<a href="https://arxiv.org/abs/2211.07483">arxiv:2211.07483</a>
&#x1F4C8; 7 <br>
<p>Nguyen Anh Vu Doan, Arda Yüksel, Chih-Hong Cheng</p></summary>
<p>

**Abstract:** This work aims to explore and identify tiny and seemingly unrelated perturbations of images in object detection that will lead to performance degradation. While tininess can naturally be defined using $L_p$ norms, we characterize the degree of "unrelatedness" of an object by the pixel distance between the occurred perturbation and the object. Triggering errors in prediction while satisfying two objectives can be formulated as a multi-objective optimization problem where we utilize genetic algorithms to guide the search. The result successfully demonstrates that (invisible) perturbations on the right part of the image can drastically change the outcome of object detection on the left. An extensive evaluation reaffirms our conjecture that transformer-based object detection networks are more susceptible to butterfly effects in comparison to single-stage object detection networks such as YOLOv5.

</p>
</details>

<details><summary><b>Interpreting Bias in the Neural Networks: A Peek Into Representational Similarity</b>
<a href="https://arxiv.org/abs/2211.07774">arxiv:2211.07774</a>
&#x1F4C8; 6 <br>
<p>Gnyanesh Bangaru, Lalith Bharadwaj Baru, Kiran Chakravarthula</p></summary>
<p>

**Abstract:** Neural networks trained on standard image classification data sets are shown to be less resistant to data set bias. It is necessary to comprehend the behavior objective function that might correspond to superior performance for data with biases. However, there is little research on the selection of the objective function and its representational structure when trained on data set with biases.
  In this paper, we investigate the performance and internal representational structure of convolution-based neural networks (e.g., ResNets) trained on biased data using various objective functions. We specifically study similarities in representations, using Centered Kernel Alignment (CKA), for different objective functions (probabilistic and margin-based) and offer a comprehensive analysis of the chosen ones.
  According to our findings, ResNets representations obtained with Negative Log Likelihood $(\mathcal{L}_{NLL})$ and Softmax Cross-Entropy ($\mathcal{L}_{SCE}$) as loss functions are equally capable of producing better performance and fine representations on biased data. We note that without progressive representational similarities among the layers of a neural network, the performance is less likely to be robust.

</p>
</details>

<details><summary><b>Advancing the State-of-the-Art for ECG Analysis through Structured State Space Models</b>
<a href="https://arxiv.org/abs/2211.07579">arxiv:2211.07579</a>
&#x1F4C8; 6 <br>
<p>Temesgen Mehari, Nils Strodthoff</p></summary>
<p>

**Abstract:** The field of deep-learning-based ECG analysis has been largely dominated by convolutional architectures. This work explores the prospects of applying the recently introduced structured state space models (SSMs) as a particularly promising approach due to its ability to capture long-term dependencies in time series. We demonstrate that this approach leads to significant improvements over the current state-of-the-art for ECG classification, which we trace back to individual pathologies. Furthermore, the model's ability to capture long-term dependencies allows to shed light on long-standing questions in the literature such as the optimal sampling rate or window size to train classification models. Interestingly, we find no evidence for using data sampled at 500Hz as opposed to 100Hz and no advantages from extending the model's input size beyond 3s. Based on this very promising first assessment, SSMs could develop into a new modeling paradigm for ECG analysis.

</p>
</details>

<details><summary><b>Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations</b>
<a href="https://arxiv.org/abs/2211.07517">arxiv:2211.07517</a>
&#x1F4C8; 6 <br>
<p>Swarnadeep Saha, Peter Hase, Nazneen Rajani, Mohit Bansal</p></summary>
<p>

**Abstract:** Recent work on explainable NLP has shown that few-shot prompting can enable large pretrained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question - "Are LLMs and humans equally good at explaining data labels for both easy and hard samples?" We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models. Supporting code and data are available at https://github.com/swarnaHub/ExplanationHardness

</p>
</details>

<details><summary><b>Group-Equivariant Neural Networks with Fusion Diagrams</b>
<a href="https://arxiv.org/abs/2211.07482">arxiv:2211.07482</a>
&#x1F4C8; 6 <br>
<p>Zimu Li, Han Zheng, Erik Thiede, Junyu Liu, Risi Kondor</p></summary>
<p>

**Abstract:** Many learning tasks in physics and chemistry involve global spatial symmetries as well as permutational symmetry between particles. The standard approach to such problems is equivariant neural networks, which employ tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increases, the bookkeeping associated with ensuring parsimony as well as equivariance quickly becomes nontrivial. In this paper, we propose to use fusion diagrams, a technique widely used in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for use in equivariant neural networks. This yields a diagrammatic approach to constructing new neural network architectures. We show that when applied to particles in a given local neighborhood, the resulting components, which we call fusion blocks, are universal approximators of any continuous equivariant function defined on the neighborhood. As a practical demonstration, we incorporate a fusion block into a pre-existing equivariant architecture (Cormorant) and show that it improves performance on benchmark molecular learning tasks.

</p>
</details>

<details><summary><b>Multi-VQG: Generating Engaging Questions for Multiple Images</b>
<a href="https://arxiv.org/abs/2211.07441">arxiv:2211.07441</a>
&#x1F4C8; 6 <br>
<p>Min-Hsuan Yeh, Vicent Chen, Ting-Hao 'Kenneth' Haung, Lun-Wei Ku</p></summary>
<p>

**Abstract:** Generating engaging content has drawn much recent attention in the NLP community. Asking questions is a natural way to respond to photos and promote awareness. However, most answers to questions in traditional question-answering (QA) datasets are factoids, which reduce individuals' willingness to answer. Furthermore, traditional visual question generation (VQG) confines the source data for question generation to single images, resulting in a limited ability to comprehend time-series information of the underlying event. In this paper, we propose generating engaging questions from multiple images. We present MVQG, a new dataset, and establish a series of baselines, including both end-to-end and dual-stage architectures. Results show that building stories behind the image sequence enables models to generate engaging questions, which confirms our assumption that people typically construct a picture of the event in their minds before asking questions. These results open up an exciting challenge for visual-and-language models to implicitly construct a story behind a series of photos to allow for creativity and experience sharing and hence draw attention to downstream applications.

</p>
</details>

<details><summary><b>Assessing Performance and Fairness Metrics in Face Recognition - Bootstrap Methods</b>
<a href="https://arxiv.org/abs/2211.07245">arxiv:2211.07245</a>
&#x1F4C8; 6 <br>
<p>Jean-Rémy Conti, Stéphan Clémençon</p></summary>
<p>

**Abstract:** The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function in Face Recognition. In order to draw reliable conclusions based on empirical ROC analysis, evaluating accurately the uncertainty related to statistical versions of the ROC curves of interest is necessary. For this purpose, we explain in this paper that, because the True/False Acceptance Rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach is not valid here and that a dedicated recentering technique must be used instead. This is illustrated on real data of face images, when applied to several ROC-based metrics such as popular fairness metrics.

</p>
</details>

<details><summary><b>Rapid Connectionist Speaker Adaptation</b>
<a href="https://arxiv.org/abs/2211.08978">arxiv:2211.08978</a>
&#x1F4C8; 5 <br>
<p>Michael Witbrock, Patrick Haffner</p></summary>
<p>

**Abstract:** We present SVCnet, a system for modelling speaker variability. Encoder Neural Networks specialized for each speech sound produce low dimensionality models of acoustical variation, and these models are further combined into an overall model of voice variability. A training procedure is described which minimizes the dependence of this model on which sounds have been uttered. Using the trained model (SVCnet) and a brief, unconstrained sample of a new speaker's voice, the system produces a Speaker Voice Code that can be used to adapt a recognition system to the new speaker without retraining. A system which combines SVCnet with an MS-TDNN recognizer is described

</p>
</details>

<details><summary><b>Evaluating How Fine-tuning on Bimodal Data Effects Code Generation</b>
<a href="https://arxiv.org/abs/2211.07842">arxiv:2211.07842</a>
&#x1F4C8; 5 <br>
<p>Gabriel Orlanski, Seonhye Yang, Michael Healy</p></summary>
<p>

**Abstract:** Despite the increase in popularity of language models for code generation, it is still unknown how training on bimodal coding forums affects a model's code generation performance and reliability. We, therefore, collect a dataset of over 2.2M StackOverflow questions with answers for finetuning. These fine-tuned models have average $pass@k$ improvements of 54.64% and 85.35% on the HumanEval (Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021) tasks, respectively. This regime further decreases the number of generated programs with both syntax and runtime errors. However, we find that at higher temperatures, there are significant decreases to the model's ability to generate runnable programs despite higher $pass@k$ scores, underscoring the need for better methods of incorporating such data that mitigate these side effects. The code can be found https://github.com/gabeorlanski/bimodalcode-generation

</p>
</details>

<details><summary><b>Denoising Diffusion Models for Out-of-Distribution Detection</b>
<a href="https://arxiv.org/abs/2211.07740">arxiv:2211.07740</a>
&#x1F4C8; 5 <br>
<p>Mark S. Graham, Walter H. L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</p></summary>
<p>

**Abstract:** Out-of-distribution detection is crucial to the safe deployment of machine learning systems. Currently, the state-of-the-art in unsupervised out-of-distribution detection is dominated by generative-based approaches that make use of estimates of the likelihood or other measurements from a generative model. Reconstruction-based methods offer an alternative approach, in which a measure of reconstruction error is used to determine if a sample is out-of-distribution. However, reconstruction-based approaches are less favoured, as they require careful tuning of the model's information bottleneck - such as the size of the latent dimension - to produce good results. In this work, we exploit the view of denoising diffusion probabilistic models (DDPM) as denoising autoencoders where the bottleneck is controlled externally, by means of the amount of noise applied. We propose to use DDPMs to reconstruct an input that has been noised to a range of noise levels, and use the resulting multi-dimensional reconstruction error to classify out-of-distribution inputs. Our approach outperforms not only reconstruction-based methods, but also state-of-the-art generative-based approaches.

</p>
</details>

<details><summary><b>Removing fluid lensing effects from spatial images</b>
<a href="https://arxiv.org/abs/2211.07648">arxiv:2211.07648</a>
&#x1F4C8; 5 <br>
<p>Greg Sabella</p></summary>
<p>

**Abstract:** Shallow water and coastal aquatic ecosystems such as coral reefs and seagrass meadows play a critical role in regulating and understanding Earth's changing climate and biodiversity. They also play an important role in protecting towns and cities from erosion and storm surges. Yet technology used for remote sensing (drones, UAVs, satellites) cannot produce detailed images of these ecosystems. Fluid lensing effects, the distortions caused by surface waves and light on underwater objects, are what makes the remote sensing of these ecosystems a very challenging task. Using machine learning, a proof of concept model was developed that is able to remove most of these effects and produce a clearer more stable image.

</p>
</details>

<details><summary><b>NeurIPS 2022 Competition: Driving SMARTS</b>
<a href="https://arxiv.org/abs/2211.07545">arxiv:2211.07545</a>
&#x1F4C8; 5 <br>
<p>Amir Rasouli, Randy Goebel, Matthew E. Taylor, Iuliia Kotseruba, Soheil Alizadeh, Tianpei Yang, Montgomery Alban, Florian Shkurti, Yuzheng Zhuang, Adam Scibior, Kasra Rezaee, Animesh Garg, David Meger, Jun Luo, Liam Paull, Weinan Zhang, Xinyu Wang, Xi Chen</p></summary>
<p>

**Abstract:** Driving SMARTS is a regular competition designed to tackle problems caused by the distribution shift in dynamic interaction contexts that are prevalent in real-world autonomous driving (AD). The proposed competition supports methodologically diverse solutions, such as reinforcement learning (RL) and offline learning methods, trained on a combination of naturalistic AD data and open-source simulation platform SMARTS. The two-track structure allows focusing on different aspects of the distribution shift. Track 1 is open to any method and will give ML researchers with different backgrounds an opportunity to solve a real-world autonomous driving challenge. Track 2 is designed for strictly offline learning methods. Therefore, direct comparisons can be made between different methods with the aim to identify new promising research directions. The proposed setup consists of 1) realistic traffic generated using real-world data and micro simulators to ensure fidelity of the scenarios, 2) framework accommodating diverse methods for solving the problem, and 3) baseline method. As such it provides a unique opportunity for the principled investigation into various aspects of autonomous vehicle deployment.

</p>
</details>

<details><summary><b>Aspects of scaling and scalability for flow-based sampling of lattice QCD</b>
<a href="https://arxiv.org/abs/2211.07541">arxiv:2211.07541</a>
&#x1F4C8; 5 <br>
<p>Ryan Abbott, Michael S. Albergo, Aleksandar Botev, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Alexander G. D. G. Matthews, Sébastien Racanière, Ali Razavi, Danilo J. Rezende, Fernando Romero-López, Phiala E. Shanahan, Julian M. Urban</p></summary>
<p>

**Abstract:** Recent applications of machine-learned normalizing flows to sampling in lattice field theory suggest that such methods may be able to mitigate critical slowing down and topological freezing. However, these demonstrations have been at the scale of toy models, and it remains to be determined whether they can be applied to state-of-the-art lattice quantum chromodynamics calculations. Assessing the viability of sampling algorithms for lattice field theory at scale has traditionally been accomplished using simple cost scaling laws, but as we discuss in this work, their utility is limited for flow-based approaches. We conclude that flow-based approaches to sampling are better thought of as a broad family of algorithms with different scaling properties, and that scalability must be assessed experimentally.

</p>
</details>

<details><summary><b>Towards a Mathematics Formalisation Assistant using Large Language Models</b>
<a href="https://arxiv.org/abs/2211.07524">arxiv:2211.07524</a>
&#x1F4C8; 5 <br>
<p>Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, Anand Tadipatri</p></summary>
<p>

**Abstract:** Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful input-dependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\% accuracy for $120$ theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of $13$ theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.

</p>
</details>

<details><summary><b>Efficient Contextual Bandits with Knapsacks via Regression</b>
<a href="https://arxiv.org/abs/2211.07484">arxiv:2211.07484</a>
&#x1F4C8; 5 <br>
<p>Aleksandrs Slivkins, Dylan Foster</p></summary>
<p>

**Abstract:** We consider contextual bandits with knapsacks (CBwK), a variant of the contextual bandit which places global constraints on budget consumption. We present a new algorithm that is simple, statistically optimal, and computationally efficient. Our algorithm combines LagrangeBwK (Immorlica et al., FOCS'19), a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML'20), a regression-based technique for contextual bandits. Our analysis emphasizes the modularity of both techniques.

</p>
</details>

<details><summary><b>Temporal patterns in insulin needs for Type 1 diabetes</b>
<a href="https://arxiv.org/abs/2211.07393">arxiv:2211.07393</a>
&#x1F4C8; 5 <br>
<p>Isabella Degen, Zahraa S. Abdallah</p></summary>
<p>

**Abstract:** Type 1 Diabetes (T1D) is a chronic condition where the body produces little or no insulin, a hormone required for the cells to use blood glucose (BG) for energy and to regulate BG levels in the body. Finding the right insulin dose and time remains a complex, challenging and as yet unsolved control task. In this study, we use the OpenAPS Data Commons dataset, which is an extensive dataset collected in real-life conditions, to discover temporal patterns in insulin need driven by well-known factors such as carbohydrates as well as potentially novel factors. We utilised various time series techniques to spot such patterns using matrix profile and multi-variate clustering. The better we understand T1D and the factors impacting insulin needs, the more we can contribute to building data-driven technology for T1D treatments.

</p>
</details>

<details><summary><b>Higher degree sum-of-squares relaxations robust against oblivious outliers</b>
<a href="https://arxiv.org/abs/2211.07327">arxiv:2211.07327</a>
&#x1F4C8; 5 <br>
<p>Tommaso d'Orsi, Rajai Nasser, Gleb Novikov, David Steurer</p></summary>
<p>

**Abstract:** We consider estimation models of the form $Y=X^*+N$, where $X^*$ is some $m$-dimensional signal we wish to recover, and $N$ is symmetrically distributed noise that may be unbounded in all but a small $α$ fraction of the entries. We introduce a family of algorithms that under mild assumptions recover the signal $X^*$ in all estimation problems for which there exists a sum-of-squares algorithm that succeeds in recovering the signal $X^*$ when the noise $N$ is Gaussian. This essentially shows that it is enough to design a sum-of-squares algorithm for an estimation problem with Gaussian noise in order to get the algorithm that works with the symmetric noise model. Our framework extends far beyond previous results on symmetric noise models and is even robust to adversarial perturbations.
  As concrete examples, we investigate two problems for which no efficient algorithms were known to work for heavy-tailed noise: tensor PCA and sparse PCA. For the former, our algorithm recovers the principal component in polynomial time when the signal-to-noise ratio is at least $\tilde{O}(n^{p/4}/α)$, that matches (up to logarithmic factors) current best known algorithmic guarantees for Gaussian noise. For the latter, our algorithm runs in quasipolynomial time and matches the state-of-the-art guarantees for quasipolynomial time algorithms in the case of Gaussian noise. Using a reduction from the planted clique problem, we provide evidence that the quasipolynomial time is likely to be necessary for sparse PCA with symmetric noise.
  In our proofs we use bounds on the covering numbers of sets of pseudo-expectations, which we obtain by certifying in sum-of-squares upper bounds on the Gaussian complexities of sets of solutions. This approach for bounding the covering numbers of sets of pseudo-expectations may be interesting in its own right and may find other application in future works.

</p>
</details>

<details><summary><b>MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets</b>
<a href="https://arxiv.org/abs/2211.07321">arxiv:2211.07321</a>
&#x1F4C8; 5 <br>
<p>Ziyang Ma, Zhisheng Zhen, Changli Tang, Yujin Wang, Xie Chen</p></summary>
<p>

**Abstract:** In this paper, we provide a new perspective on self-supervised speech models from how the self-training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE), without caring about specific pretext tasks. Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL refers to two typical models, HuBERT and data2vec, which use the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with no need for that much data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think that doing multi-task learning on self-supervised speech models from our perspective is a promising trend.

</p>
</details>

<details><summary><b>PAC-Bayesian Meta-Learning: From Theory to Practice</b>
<a href="https://arxiv.org/abs/2211.07206">arxiv:2211.07206</a>
&#x1F4C8; 5 <br>
<p>Jonas Rothfuss, Martin Josifoski, Vincent Fortuin, Andreas Krause</p></summary>
<p>

**Abstract:** Meta-Learning aims to accelerate the learning on new tasks by acquiring useful inductive biases from related data sources. In practice, the number of tasks available for meta-learning is often small. Yet, most of the existing approaches rely on an abundance of meta-training tasks, making them prone to overfitting. How to regularize the meta-learner to ensure generalization to unseen tasks, is a central question in the literature. We provide a theoretical analysis using the PAC-Bayesian framework and derive the first bound for meta-learners with unbounded loss functions. Crucially, our bounds allow us to derive the PAC-optimal hyper-posterior (PACOH) - the closed-form-solution of the PAC-Bayesian meta-learning problem, thereby avoiding the reliance on nested optimization, giving rise to an optimization problem amenable to standard variational methods that scale well. Our experiments show that, when instantiating the PACOH with Gaussian processes and Bayesian Neural Networks as base learners, the resulting methods are more scalable, and yield state-of-the-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates. Finally, thanks to the principled treatment of uncertainty, our meta-learners can also be successfully employed for sequential decision problems.

</p>
</details>

<details><summary><b>Learning-Augmented Model-Based Planning for Visual Exploration</b>
<a href="https://arxiv.org/abs/2211.07898">arxiv:2211.07898</a>
&#x1F4C8; 4 <br>
<p>Yimeng Li, Arnab Debnath, Gregory Stein, Jana Kosecka</p></summary>
<p>

**Abstract:** We consider the problem of time-limited robotic exploration in previously unseen environments where exploration is limited by a predefined amount of time. We propose a novel exploration approach using learning-augmented model-based planning. We generate a set of subgoals associated with frontiers on the current map and derive a Bellman Equation for exploration with these subgoals. Visual sensing and advances in semantic mapping of indoor scenes are exploited for training a deep convolutional neural network to estimate properties associated with each frontier: the expected unobserved area beyond the frontier and the expected timesteps (discretized actions) required to explore it. The proposed model-based planner is guaranteed to explore the whole scene if time permits. We thoroughly evaluate our approach on a large-scale pseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. We compare our approach with classical and more recent RL-based exploration methods, demonstrating its clear advantages in several settings.

</p>
</details>

<details><summary><b>Federated Learning for Healthcare Domain - Pipeline, Applications and Challenges</b>
<a href="https://arxiv.org/abs/2211.07893">arxiv:2211.07893</a>
&#x1F4C8; 4 <br>
<p>Madhura Joshi, Ankit Pal, Malaikannan Sankarasubbu</p></summary>
<p>

**Abstract:** Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.

</p>
</details>

<details><summary><b>Using Human Perception to Regularize Transfer Learning</b>
<a href="https://arxiv.org/abs/2211.07885">arxiv:2211.07885</a>
&#x1F4C8; 4 <br>
<p>Justin Dulay, Walter J. Scheirer</p></summary>
<p>

**Abstract:** Recent trends in the machine learning community show that models with fidelity toward human perceptual measurements perform strongly on vision tasks. Likewise, human behavioral measurements have been used to regularize model performance. But can we transfer latent knowledge gained from this across different learning objectives? In this work, we introduce PERCEP-TL (Perceptual Transfer Learning), a methodology for improving transfer learning with the regularization power of psychophysical labels in models. We demonstrate which models are affected the most by perceptual transfer learning and find that models with high behavioral fidelity -- including vision transformers -- improve the most from this regularization by as much as 1.9\% Top@1 accuracy points. These findings suggest that biologically inspired learning agents can benefit from human behavioral measurements as regularizers and psychophysical learned representations can be transferred to independent evaluation tasks.

</p>
</details>

<details><summary><b>Explainable Action Advising for Multi-Agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2211.07882">arxiv:2211.07882</a>
&#x1F4C8; 4 <br>
<p>Yue Guo, Joseph Campbell, Simon Stepputtis, Ruiyu Li, Dana Hughes, Fei Fang, Katia Sycara</p></summary>
<p>

**Abstract:** Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods.

</p>
</details>

<details><summary><b>Cross-domain Federated Adaptive Prompt Tuning for CLIP</b>
<a href="https://arxiv.org/abs/2211.07864">arxiv:2211.07864</a>
&#x1F4C8; 4 <br>
<p>Shangchao Su, Mingzhao Yang, Bin Li, Xiangyang Xue</p></summary>
<p>

**Abstract:** Federated learning (FL) allows multiple parties to collaboratively train a global model without disclosing their data. Existing research often requires all model parameters to participate in the training procedure. However, with the advent of powerful pre-trained models, it becomes possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for cross-domain federated image classification scenarios with the vision-language pre-trained model, CLIP, which gives play to the strong representation ability in FL. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a global prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and global prompt with the local datasets and the frozen keys. Ultimately, the global aggregation model can assign a personalized prompt to CLIP based on the domain features of each test sample. We perform extensive experiments on two multi-domain image classification datasets. The results show that FedAPT can achieve better performance with less than 10\% of the number of parameters of the fully trained model, and the global model can perform well in different client domains simultaneously.

</p>
</details>

<details><summary><b>Regularized Stein Variational Gradient Flow</b>
<a href="https://arxiv.org/abs/2211.07861">arxiv:2211.07861</a>
&#x1F4C8; 4 <br>
<p>Ye He, Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, Jianfeng Lu</p></summary>
<p>

**Abstract:** The Stein Variational Gradient Descent (SVGD) algorithm is an deterministic particle method for sampling. However, a mean-field analysis reveals that the gradient flow corresponding to the SVGD algorithm (i.e., the Stein Variational Gradient Flow) only provides a constant-order approximation to the Wasserstein Gradient Flow corresponding to the KL-divergence minimization. In this work, we propose the Regularized Stein Variational Gradient Flow which interpolates between the Stein Variational Gradient Flow and the Wasserstein Gradient Flow. We establish various theoretical properties of the Regularized Stein Variational Gradient Flow (and its time-discretization) including convergence to equilibrium, existence and uniqueness of weak solutions, and stability of the solutions. We provide preliminary numerical evidence of the improved performance offered by the regularization.

</p>
</details>

<details><summary><b>Hierarchically Structured Task-Agnostic Continual Learning</b>
<a href="https://arxiv.org/abs/2211.07725">arxiv:2211.07725</a>
&#x1F4C8; 4 <br>
<p>Heinke Hihn, Daniel A. Braun</p></summary>
<p>

**Abstract:** One notable weakness of current machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We derive this principle from a Bayesian perspective and show its connections to previous approaches to continual learning. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Equipped with a diverse and specialized set of parameters, each path can be regarded as a distinct sub-network that learns to solve tasks. To improve expert allocation, we introduce diversity objectives, which we evaluate in additional ablation studies. Importantly, our approach can operate in a task-agnostic way, i.e., it does not require task-specific knowledge, as is the case with many existing continual learning algorithms. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method on continual reinforcement learning and variants of the MNIST, CIFAR-10, and CIFAR-100 datasets.

</p>
</details>

<details><summary><b>Disentangling Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2211.07700">arxiv:2211.07700</a>
&#x1F4C8; 4 <br>
<p>Rafael Pastrana</p></summary>
<p>

**Abstract:** A variational autoencoder (VAE) is a probabilistic machine learning framework for posterior inference that projects an input set of high-dimensional data to a lower-dimensional, latent space. The latent space learned with a VAE offers exciting opportunities to develop new data-driven design processes in creative disciplines, in particular, to automate the generation of multiple novel designs that are aesthetically reminiscent of the input data but that were unseen during training. However, the learned latent space is typically disorganized and entangled: traversing the latent space along a single dimension does not result in changes to single visual attributes of the data. The lack of latent structure impedes designers from deliberately controlling the visual attributes of new designs generated from the latent space. This paper presents an experimental study that investigates latent space disentanglement. We implement three different VAE models from the literature and train them on a publicly available dataset of 60,000 images of hand-written digits. We perform a sensitivity analysis to find a small number of latent dimensions necessary to maximize a lower bound to the log marginal likelihood of the data. Furthermore, we investigate the trade-offs between the quality of the reconstruction of the decoded images and the level of disentanglement of the latent space. We are able to automatically align three latent dimensions with three interpretable visual properties of the digits: line weight, tilt and width. Our experiments suggest that i) increasing the contribution of the Kullback-Leibler divergence between the prior over the latents and the variational distribution to the evidence lower bound, and ii) conditioning input image class enhances the learning of a disentangled latent space with a VAE.

</p>
</details>

<details><summary><b>Learnable Spatio-Temporal Map Embeddings for Deep Inertial Localization</b>
<a href="https://arxiv.org/abs/2211.07635">arxiv:2211.07635</a>
&#x1F4C8; 4 <br>
<p>Dennis Melamed, Karnik Ram, Vivek Roy, Kris Kitani</p></summary>
<p>

**Abstract:** Indoor localization systems often fuse inertial odometry with map information via hand-defined methods to reduce odometry drift, but such methods are sensitive to noise and struggle to generalize across odometry sources. To address the robustness problem in map utilization, we propose a data-driven prior on possible user locations in a map by combining learned spatial map embeddings and temporal odometry embeddings. Our prior learns to encode which map regions are feasible locations for a user more accurately than previous hand-defined methods. This prior leads to a 49% improvement in inertial-only localization accuracy when used in a particle filter. This result is significant, as it shows that our relative positioning method can match the performance of absolute positioning using bluetooth beacons. To show the generalizability of our method, we also show similar improvements using wheel encoder odometry.

</p>
</details>

<details><summary><b>Alternating minimization algorithm with initialization analysis for r-local and k-sparse unlabeled sensing</b>
<a href="https://arxiv.org/abs/2211.07621">arxiv:2211.07621</a>
&#x1F4C8; 4 <br>
<p>Ahmed Abbasi, Abiy Tasissa, Shuchin Aeron</p></summary>
<p>

**Abstract:** The unlabeled sensing problem is to recover an unknown signal from permuted linear measurements. We propose an alternating minimization algorithm with a suitable initialization for the widely considered k-sparse permutation model. Assuming either a Gaussian measurement matrix or a sub-Gaussian signal, we upper bound the initialization error for the r-local and k-sparse permutation models in terms of the block size $r$ and the number of shuffles k, respectively. Our algorithm is computationally scalable and, compared to baseline methods, achieves superior performance on real and synthetic datasets.

</p>
</details>

<details><summary><b>AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT</b>
<a href="https://arxiv.org/abs/2211.07499">arxiv:2211.07499</a>
&#x1F4C8; 4 <br>
<p>Aman Priyanshu, Supriti Vijay</p></summary>
<p>

**Abstract:** Keyword extraction has been an important topic for modern natural language processing. With its applications ranging from ontology generation, fact verification in summarized text, and recommendation systems. While it has had significant data-intensive applications, it is often hampered when the data set is small. Downstream training for keyword extractors is a lengthy process and requires a significant amount of data. Recently, Few-shot Learning (FSL) and Zero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore, we propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM bases by incorporating the concept of regularized attention into a pre-training phase for downstream domain adaptation. As we believe our work has implications to be utilized in the pipeline of FSL/ZSL and keyword extraction, we open-source our code as well as provide the fine-tuning library of the same name AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.

</p>
</details>

<details><summary><b>Implications of Regret on Stability of Linear Dynamical Systems</b>
<a href="https://arxiv.org/abs/2211.07411">arxiv:2211.07411</a>
&#x1F4C8; 4 <br>
<p>Aren Karapetyan, Anastasios Tsiamis, Efe C. Balta, Andrea Iannelli, John Lygeros</p></summary>
<p>

**Abstract:** The setting of an agent making decisions under uncertainty and under dynamic constraints is common for the fields of optimal control, reinforcement learning and recently also for online learning. In the online learning setting, the quality of an agent's decision is often quantified by the concept of regret, comparing the performance of the chosen decisions to the best possible ones in hindsight. While regret is a useful performance measure, when dynamical systems are concerned, it is important to also assess the stability of the closed-loop system for a chosen policy. In this work, we show that for linear state feedback policies and linear systems subject to adversarial disturbances, linear regret implies asymptotic stability in both time-varying and time-invariant settings. Conversely, we also show that bounded input bounded state (BIBS) stability and summability of the state transition matrices imply linear regret.

</p>
</details>

<details><summary><b>Unsupervised Method for Intra-patient Registration of Brain Magnetic Resonance Images based on Objective Function Weighting by Inverse Consistency: Contribution to the BraTS-Reg Challenge</b>
<a href="https://arxiv.org/abs/2211.07386">arxiv:2211.07386</a>
&#x1F4C8; 4 <br>
<p>Marek Wodzinski, Artur Jurgas, Niccolo Marini, Manfredo Atzori, Henning Muller</p></summary>
<p>

**Abstract:** Registration of brain scans with pathologies is difficult, yet important research area. The importance of this task motivated researchers to organize the BraTS-Reg challenge, jointly with IEEE ISBI 2022 and MICCAI 2022 conferences. The organizers introduced the task of aligning pre-operative to follow-up magnetic resonance images of glioma. The main difficulties are connected with the missing data leading to large, nonrigid, and noninvertible deformations. In this work, we describe our contributions to both the editions of the BraTS-Reg challenge. The proposed method is based on combined deep learning and instance optimization approaches. First, the instance optimization enriches the state-of-the-art LapIRN method to improve the generalizability and fine-details preservation. Second, an additional objective function weighting is introduced, based on the inverse consistency. The proposed method is fully unsupervised and exhibits high registration quality and robustness. The quantitative results on the external validation set are: (i) IEEE ISBI 2022 edition: 1.85, and 0.86, (ii) MICCAI 2022 edition: 1.71, and 0.86, in terms of the mean of median absolute error and robustness respectively. The method scored the 1st place during the IEEE ISBI 2022 version of the challenge and the 3rd place during the MICCAI 2022. Future work could transfer the inverse consistency-based weighting directly into the deep network training.

</p>
</details>

<details><summary><b>Finding Skill Neurons in Pre-trained Transformer-based Language Models</b>
<a href="https://arxiv.org/abs/2211.07349">arxiv:2211.07349</a>
&#x1F4C8; 4 <br>
<p>Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li</p></summary>
<p>

**Abstract:** Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron.

</p>
</details>

<details><summary><b>Learning to Model Multimodal Semantic Alignment for Story Visualization</b>
<a href="https://arxiv.org/abs/2211.07289">arxiv:2211.07289</a>
&#x1F4C8; 4 <br>
<p>Bowen Li, Thomas Lukasiewicz</p></summary>
<p>

**Abstract:** Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, where the images should be realistic and keep global consistency across dynamic scenes and characters. Current works face the problem of semantic misalignment because of their fixed architecture and diversity of input modalities. To address this problem, we explore the semantic alignment between text and image representations by learning to match their semantic levels in the GAN-based generative model. More specifically, we introduce dynamic interactions according to learning to dynamically explore various semantic depths and fuse the different-modal information at a matched semantic level, which thus relieves the text-image semantic misalignment problem. Extensive experiments on different datasets demonstrate the improvements of our approach, neither using segmentation masks nor auxiliary captioning networks, on image quality and story consistency, compared with state-of-the-art methods.

</p>
</details>

<details><summary><b>Structured Knowledge Distillation Towards Efficient and Compact Multi-View 3D Detection</b>
<a href="https://arxiv.org/abs/2211.08398">arxiv:2211.08398</a>
&#x1F4C8; 3 <br>
<p>Linfeng Zhang, Yukang Shi, Hung-Shuo Tai, Zhipeng Zhang, Yuan He, Ke Wang, Kaisheng Ma</p></summary>
<p>

**Abstract:** Detecting 3D objects from multi-view images is a fundamental problem in 3D computer vision. Recently, significant breakthrough has been made in multi-view 3D detection tasks. However, the unprecedented detection performance of these vision BEV (bird's-eye-view) detection models is accompanied with enormous parameters and computation, which make them unaffordable on edge devices. To address this problem, in this paper, we propose a structured knowledge distillation framework, aiming to improve the efficiency of modern vision-only BEV detection models. The proposed framework mainly includes: (a) spatial-temporal distillation which distills teacher knowledge of information fusion from different timestamps and views, (b) BEV response distillation which distills teacher response to different pillars, and (c) weight-inheriting which solves the problem of inconsistent inputs between students and teacher in modern transformer architectures. Experimental results show that our method leads to an average improvement of 2.16 mAP and 2.27 NDS on the nuScenes benchmark, outperforming multiple baselines by a large margin.

</p>
</details>

<details><summary><b>Contrastive learning for regression in multi-site brain age prediction</b>
<a href="https://arxiv.org/abs/2211.08326">arxiv:2211.08326</a>
&#x1F4C8; 3 <br>
<p>Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, Pietro Gori</p></summary>
<p>

**Abstract:** Building accurate Deep Learning (DL) models for brain age prediction is a very relevant topic in neuroimaging, as it could help better understand neurodegenerative disorders and find new biomarkers. To estimate accurate and generalizable models, large datasets have been collected, which are often multi-site and multi-scanner. This large heterogeneity negatively affects the generalization performance of DL models since they are prone to overfit site-related noise. Recently, contrastive learning approaches have been shown to be more robust against noise in data or labels. For this reason, we propose a novel contrastive learning regression loss for robust brain age prediction using MRI scans. Our method achieves state-of-the-art performance on the OpenBHB challenge, yielding the best generalization capability and robustness to site-related noise.

</p>
</details>

<details><summary><b>Pretraining ECG Data with Adversarial Masking Improves Model Generalizability for Data-Scarce Tasks</b>
<a href="https://arxiv.org/abs/2211.07889">arxiv:2211.07889</a>
&#x1F4C8; 3 <br>
<p>Jessica Y. Bo, Hen-Wei Huang, Alvin Chan, Giovanni Traverso</p></summary>
<p>

**Abstract:** Medical datasets often face the problem of data scarcity, as ground truth labels must be generated by medical professionals. One mitigation strategy is to pretrain deep learning models on large, unlabelled datasets with self-supervised learning (SSL). Data augmentations are essential for improving the generalizability of SSL-trained models, but they are typically handcrafted and tuned manually. We use an adversarial model to generate masks as augmentations for 12-lead electrocardiogram (ECG) data, where masks learn to occlude diagnostically-relevant regions of the ECGs. Compared to random augmentations, adversarial masking reaches better accuracy when transferring to to two diverse downstream objectives: arrhythmia classification and gender classification. Compared to a state-of-art ECG augmentation method 3KG, adversarial masking performs better in data-scarce regimes, demonstrating the generalizability of our model.

</p>
</details>

<details><summary><b>DINER: Disorder-Invariant Implicit Neural Representation</b>
<a href="https://arxiv.org/abs/2211.07871">arxiv:2211.07871</a>
&#x1F4C8; 3 <br>
<p>Shaowen Xie, Hao Zhu, Zhen Liu, Qi Zhang, You Zhou, Xun Cao, Zhan Ma</p></summary>
<p>

**Abstract:** Implicit neural representation (INR) characterizes the attributes of a signal as a function of corresponding coordinates which emerges as a sharp weapon for solving inverse problems. However, the capacity of INR is limited by the spectral bias in the network training. In this paper, we find that such a frequency-related problem could be largely solved by re-arranging the coordinates of the input signal, for which we propose the disorder-invariant implicit neural representation (DINER) by augmenting a hash-table to a traditional INR backbone. Given discrete signals sharing the same histogram of attributes and different arrangement orders, the hash-table could project the coordinates into the same distribution for which the mapped signal can be better modeled using the subsequent INR network, leading to significantly alleviated spectral bias. Experiments not only reveal the generalization of the DINER for different INR backbones (MLP vs. SIREN) and various tasks (image/video representation, phase retrieval, and refractive index recovery) but also show the superiority over the state-of-the-art algorithms both in quality and speed.

</p>
</details>

<details><summary><b>Adaptive Embedding for Temporal Network</b>
<a href="https://arxiv.org/abs/2211.07866">arxiv:2211.07866</a>
&#x1F4C8; 3 <br>
<p>Haoran Zhang, Junhui Wang</p></summary>
<p>

**Abstract:** Temporal network has become ubiquitous with the rise of online social platform and e-commerce, but largely under investigated in literature. In this paper, we propose a statistical framework for temporal network analysis, leveraging strengths of adaptive network merging, tensor decomposition and point process. A two-step embedding procedure and a regularized maximum likelihood estimate based on Poisson point process is developed, where the initial estimate is based on equal spaced time intervals while the final estimate on the adaptively merging time intervals. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the tensor estimation error in each iteration is established. Through analysis, it is shown that the tensor estimation error is significantly reduced by the proposed method. Extensive numerical experiments also validate this phenomenon, as well as its advantage over other existing competitors. The proposed method is also applied to analyze a militarized interstate dispute dataset, where not only the prediction accuracy increases, but the adaptively merged intervals also lead to clear interpretation.

</p>
</details>

<details><summary><b>Multi-Player Bandits Robust to Adversarial Collisions</b>
<a href="https://arxiv.org/abs/2211.07817">arxiv:2211.07817</a>
&#x1F4C8; 3 <br>
<p>Shivakumar Mahesh, Anshuka Rangi, Haifeng Xu, Long Tran-Thanh</p></summary>
<p>

**Abstract:** Motivated by cognitive radios, stochastic Multi-Player Multi-Armed Bandits has been extensively studied in recent years. In this setting, each player pulls an arm, and receives a reward corresponding to the arm if there is no collision, namely the arm was selected by one single player. Otherwise, the player receives no reward if collision occurs. In this paper, we consider the presence of malicious players (or attackers) who obstruct the cooperative players (or defenders) from maximizing their rewards, by deliberately colliding with them. We provide the first decentralized and robust algorithm RESYNC for defenders whose performance deteriorates gracefully as $\tilde{O}(C)$ as the number of collisions $C$ from the attackers increases. We show that this algorithm is order-optimal by proving a lower bound which scales as $Ω(C)$. This algorithm is agnostic to the algorithm used by the attackers and agnostic to the number of collisions $C$ faced from attackers.

</p>
</details>

<details><summary><b>Quantifying the Impact of Label Noise on Federated Learning</b>
<a href="https://arxiv.org/abs/2211.07816">arxiv:2211.07816</a>
&#x1F4C8; 3 <br>
<p>Shuqi Ke, Chao Huang, Xin Liu</p></summary>
<p>

**Abstract:** Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets while preserving privacy. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. Theoretically speaking, we derive an upper bound for the generalization error that is linear in the clients' label noise level. Empirically speaking, we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. We show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.

</p>
</details>

<details><summary><b>Diffusion Models for Medical Image Analysis: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2211.07804">arxiv:2211.07804</a>
&#x1F4C8; 3 <br>
<p>Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, Dorit Merhof</p></summary>
<p>

**Abstract:** Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples despite their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. To help the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical image analysis. Specifically, we introduce the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modelling frameworks: diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.

</p>
</details>

<details><summary><b>On the Global Convergence of Fitted Q-Iteration with Two-layer Neural Network Parametrization</b>
<a href="https://arxiv.org/abs/2211.07675">arxiv:2211.07675</a>
&#x1F4C8; 3 <br>
<p>Mudit Gaur, Vaneet Aggarwal, Mridul Aggarwal</p></summary>
<p>

**Abstract:** Deep Q-learning based algorithms have been applied successfully in many decision making problems, while their theoretical foundations are not as well understood. In this paper, we study a Fitted Q-Iteration with two-layer ReLU neural network parametrization, and find the sample complexity guarantees for the algorithm. The approach estimates the Q-function in each iteration using a convex optimization problem. We show that this approach achieves a sample complexity of $\tilde{\mathcal{O}}(1/ε^{2})$, which is order-optimal. This result holds for a countable state-space and does not require any assumptions such as a linear or low rank structure on the MDP.

</p>
</details>

<details><summary><b>Machine Learning Performance Analysis to Predict Stroke Based on Imbalanced Medical Dataset</b>
<a href="https://arxiv.org/abs/2211.07652">arxiv:2211.07652</a>
&#x1F4C8; 3 <br>
<p>Yuru Jing</p></summary>
<p>

**Abstract:** Cerebral stroke, the second most substantial cause of death universally, has been a primary public health concern over the last few years. With the help of machine learning techniques, early detection of various stroke alerts is accessible, which can efficiently prevent or diminish the stroke. Medical dataset, however, are frequently unbalanced in their class label, with a tendency to poorly predict minority classes. In this paper, the potential risk factors for stroke are investigated. Moreover, four distinctive approaches are applied to improve the classification of the minority class in the imbalanced stroke dataset, which are the ensemble weight voting classifier, the Synthetic Minority Over-sampling Technique (SMOTE), Principal Component Analysis with K-Means Clustering (PCA-Kmeans), Focal Loss with the Deep Neural Network (DNN) and compare their performance. Through the analysis results, SMOTE and PCA-Kmeans with DNN-Focal Loss work best for the limited size of a large severe imbalanced dataset,which is 2-4 times outperform Kaggle work.

</p>
</details>

<details><summary><b>High-Accuracy Machine Learning Techniques for Functional Connectome Fingerprinting and Cognitive State Decoding</b>
<a href="https://arxiv.org/abs/2211.07507">arxiv:2211.07507</a>
&#x1F4C8; 3 <br>
<p>Andrew Hannum, Mario A. Lopez, Saúl A. Blanco, Richard F. Betzel</p></summary>
<p>

**Abstract:** The human brain is a complex network comprised of functionally and anatomically interconnected brain regions. A growing number of studies have suggested that empirical estimates of brain networks may be useful for discovery of biomarkers of disease and cognitive state. A prerequisite for realizing this aim, however, is that brain networks also serve as reliable markers of an individual. Here, using Human Connectome Project data, we build upon recent studies examining brain-based fingerprints of individual subjects and cognitive states based on cognitively-demanding tasks that assess, for example, working memory, theory of mind, and motor function. Our approach achieves accuracy of up to 99\% for both identification of the subject of an fMRI scan, and for classification of the cognitive state of a previously-unseen subject in a scan. More broadly, we explore the accuracy and reliability of five different machine learning techniques on subject fingerprinting and cognitive state decoding objectives, using functional connectivity data from fMRI scans of a high number of subjects (865) across a number of cognitive states (8). These results represent an advance on existing techniques for functional connectivity-based brain fingerprinting and state decoding. Additionally, 16 different pre-processing pipelines are compared in order to characterize the effects of different aspects of the production of functional connectomes (FCs) on the accuracy of subject and task classification, and to identify possible confounds.

</p>
</details>

<details><summary><b>Parallel Automatic History Matching Algorithm Using Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2211.07434">arxiv:2211.07434</a>
&#x1F4C8; 3 <br>
<p>Omar S. Alolayan, Abdullah O. Alomar, John R. Williams</p></summary>
<p>

**Abstract:** Reformulating the history matching problem from a least-square mathematical optimization problem into a Markov Decision Process introduces a method in which reinforcement learning can be utilized to solve the problem. This method provides a mechanism where an artificial deep neural network agent can interact with the reservoir simulator and find multiple different solutions to the problem. Such formulation allows for solving the problem in parallel by launching multiple concurrent environments enabling the agent to learn simultaneously from all the environments at once, achieving significant speed up.

</p>
</details>

<details><summary><b>Replacing Language Model for Style Transfer</b>
<a href="https://arxiv.org/abs/2211.07343">arxiv:2211.07343</a>
&#x1F4C8; 3 <br>
<p>Pengyu Cheng, Ruineng Li</p></summary>
<p>

**Abstract:** We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer. Our method autoregressively replaces each token in the original sentence with a text span in the target style. In contrast, the new span is generated via a non-autoregressive masked language model. The RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To further control the style of generated sentences, we conduct a style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text style transfer tasks demonstrate the effectiveness of RLM compared with other baselines.

</p>
</details>

<details><summary><b>SVS: Adversarial refinement for sparse novel view synthesis</b>
<a href="https://arxiv.org/abs/2211.07301">arxiv:2211.07301</a>
&#x1F4C8; 3 <br>
<p>Violeta Menéndez González, Andrew Gilbert, Graeme Phillipson, Stephen Jolly, Simon Hadfield</p></summary>
<p>

**Abstract:** This paper proposes Sparse View Synthesis. This is a view synthesis problem where the number of reference views is limited, and the baseline between target and reference view is significant. Under these conditions, current radiance field methods fail catastrophically due to inescapable artifacts such 3D floating blobs, blurring and structural duplication, whenever the number of reference views is limited, or the target view diverges significantly from the reference views.
  Advances in network architecture and loss regularisation are unable to satisfactorily remove these artifacts. The occlusions within the scene ensure that the true contents of these regions is simply not available to the model. In this work, we instead focus on hallucinating plausible scene contents within such regions. To this end we unify radiance field models with adversarial learning and perceptual losses. The resulting system provides up to 60% improvement in perceptual accuracy compared to current state-of-the-art radiance field models on this problem.

</p>
</details>

<details><summary><b>Robustifying Deep Vision Models Through Shape Sensitization</b>
<a href="https://arxiv.org/abs/2211.07277">arxiv:2211.07277</a>
&#x1F4C8; 3 <br>
<p>Aditay Tripathi, Rishubh Singh, Anirban Chakraborty, Pradeep Shenoy</p></summary>
<p>

**Abstract:** Recent work has shown that deep vision models tend to be overly dependent on low-level or "texture" features, leading to poor generalization. Various data augmentation strategies have been proposed to overcome this so-called texture bias in DNNs. We propose a simple, lightweight adversarial augmentation technique that explicitly incentivizes the network to learn holistic shapes for accurate prediction in an object classification setting. Our augmentations superpose edgemaps from one image onto another image with shuffled patches, using a randomly determined mixing proportion, with the image label of the edgemap image. To classify these augmented images, the model needs to not only detect and focus on edges but distinguish between relevant and spurious edges. We show that our augmentations significantly improve classification accuracy and robustness measures on a range of datasets and neural architectures. As an example, for ViT-S, We obtain absolute gains on classification accuracy gains up to 6%. We also obtain gains of up to 28% and 8.5% on natural adversarial and out-of-distribution datasets like ImageNet-A (for ViT-B) and ImageNet-R (for ViT-S), respectively. Analysis using a range of probe datasets shows substantially increased shape sensitivity in our trained models, explaining the observed improvement in robustness and classification accuracy.

</p>
</details>

<details><summary><b>The Best Path Algorithm automatic variables selection via High Dimensional Graphical Models</b>
<a href="https://arxiv.org/abs/2211.07267">arxiv:2211.07267</a>
&#x1F4C8; 3 <br>
<p>Consuelo R. Nava, Luigi Riso, Maria G. Zoia</p></summary>
<p>

**Abstract:** This paper proposes a new algorithm for an automatic variable selection procedure in High Dimensional Graphical Models. The algorithm selects the relevant variables for the node of interest on the basis of mutual information. Several contributions in literature have investigated the use of mutual information in selecting the appropriate number of relevant features in a large data-set, but most of them have focused on binary outcomes or required high computational effort. The algorithm here proposed overcomes these drawbacks as it is an extension of Chow and Liu's algorithm. Once, the probabilistic structure of a High Dimensional Graphical Model is determined via the said algorithm, the best path-step, including variables with the most explanatory/predictive power for a variable of interest, is determined via the computation of the entropy coefficient of determination. The latter, being based on the notion of (symmetric) Kullback-Leibler divergence, turns out to be closely connected to the mutual information of the involved variables. The application of the algorithm to a wide range of real-word and publicly data-sets has highlighted its potential and greater effectiveness compared to alternative extant methods.

</p>
</details>

<details><summary><b>The Role of Local Alignment and Uniformity in Image-Text Contrastive Learning on Medical Images</b>
<a href="https://arxiv.org/abs/2211.07254">arxiv:2211.07254</a>
&#x1F4C8; 3 <br>
<p>Philip Müller, Georgios Kaissis, Daniel Rueckert</p></summary>
<p>

**Abstract:** Image-text contrastive learning has proven effective for pretraining medical image models. When targeting localized downstream tasks like semantic segmentation or object detection, additional local contrastive losses that align image regions with sentences have shown promising results. We study how local contrastive losses are related to global (per-sample) contrastive losses and which effects they have on localized medical downstream tasks. Based on a theoretical comparison, we propose to remove some components of local losses and replace others by a novel distribution prior which enforces uniformity of representations within each sample. We empirically study this approach on chest X-ray tasks and find it to be very effective, outperforming methods without local losses on 12 of 18 tasks.

</p>
</details>

<details><summary><b>SA-DPSGD: Differentially Private Stochastic Gradient Descent based on Simulated Annealing</b>
<a href="https://arxiv.org/abs/2211.07218">arxiv:2211.07218</a>
&#x1F4C8; 3 <br>
<p>Jie Fu, Zhili Chen, XinPeng Ling</p></summary>
<p>

**Abstract:** Differential privacy (DP) provides a formal privacy guarantee that prevents adversaries with access to machine learning models from extracting information about individual training points. Differentially private stochastic gradient descent (DPSGD) is the most popular training method with differential privacy in image recognition. However, existing DPSGD schemes lead to significant performance degradation, which prevents the application of differential privacy. In this paper, we propose a simulated annealing-based differentially private stochastic gradient descent scheme (SA-DPSGD) which accepts a candidate update with a probability that depends both on the update quality and on the number of iterations. Through this random update screening, we make the differentially private gradient descent proceed in the right direction in each iteration, and result in a more accurate model finally. In our experiments, under the same hyperparameters, our scheme achieves test accuracies 98.35%, 87.41% and 60.92% on datasets MNIST, FashionMNIST and CIFAR10, respectively, compared to the state-of-the-art result of 98.12%, 86.33% and 59.34%. Under the freely adjusted hyperparameters, our scheme achieves even higher accuracies, 98.89%, 88.50% and 64.17%. We believe that our method has a great contribution for closing the accuracy gap between private and non-private image classification.

</p>
</details>

<details><summary><b>TriDoNet: A Triple Domain Model-driven Network for CT Metal Artifact Reduction</b>
<a href="https://arxiv.org/abs/2211.07190">arxiv:2211.07190</a>
&#x1F4C8; 3 <br>
<p>Baoshun Shi, Ke Jiang, Shaolei Zhang, Qiusheng Lian, Yanwei Qin</p></summary>
<p>

**Abstract:** Recent deep learning-based methods have achieved promising performance for computed tomography metal artifact reduction (CTMAR). However, most of them suffer from two limitations: (i) the domain knowledge is not fully embedded into the network training; (ii) metal artifacts lack effective representation models. The aforementioned limitations leave room for further performance improvement. Against these issues, we propose a novel triple domain model-driven CTMAR network, termed as TriDoNet, whose network training exploits triple domain knowledge, i.e., the knowledge of the sinogram, CT image, and metal artifact domains. Specifically, to explore the non-local repetitive streaking patterns of metal artifacts, we encode them as an explicit tight frame sparse representation model with adaptive thresholds. Furthermore, we design a contrastive regularization (CR) built upon contrastive learning to exploit clean CT images and metal-affected images as positive and negative samples, respectively. Experimental results show that our TriDoNet can generate superior artifact-reduced CT images.

</p>
</details>

<details><summary><b>DroneNet: Crowd Density Estimation using Self-ONNs for Drones</b>
<a href="https://arxiv.org/abs/2211.07137">arxiv:2211.07137</a>
&#x1F4C8; 3 <br>
<p>Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</p></summary>
<p>

**Abstract:** Video surveillance using drones is both convenient and efficient due to the ease of deployment and unobstructed movement of drones in many scenarios. An interesting application of drone-based video surveillance is to estimate crowd densities (both pedestrians and vehicles) in public places. Deep learning using convolution neural networks (CNNs) is employed for automatic crowd counting and density estimation using images and videos. However, the performance and accuracy of such models typically depend upon the model architecture i.e., deeper CNN models improve accuracy at the cost of increased inference time. In this paper, we propose a novel crowd density estimation model for drones (DroneNet) using Self-organized Operational Neural Networks (Self-ONN). Self-ONN provides efficient learning capabilities with lower computational complexity as compared to CNN-based models. We tested our algorithm on two drone-view public datasets. Our evaluation shows that the proposed DroneNet shows superior performance on an equivalent CNN-based model.

</p>
</details>

<details><summary><b>Demo Abstract: Real-Time Out-of-Distribution Detection on a Mobile Robot</b>
<a href="https://arxiv.org/abs/2211.11520">arxiv:2211.11520</a>
&#x1F4C8; 2 <br>
<p>Michael Yuhas, Arvind Easwaran</p></summary>
<p>

**Abstract:** In a cyber-physical system such as an autonomous vehicle (AV), machine learning (ML) models can be used to navigate and identify objects that may interfere with the vehicle's operation. However, ML models are unlikely to make accurate decisions when presented with data outside their training distribution. Out-of-distribution (OOD) detection can act as a safety monitor for ML models by identifying such samples at run time. However, in safety critical systems like AVs, OOD detection needs to satisfy real-time constraints in addition to functional requirements. In this demonstration, we use a mobile robot as a surrogate for an AV and use an OOD detector to identify potentially hazardous samples. The robot navigates a miniature town using image data and a YOLO object detection network. We show that our OOD detector is capable of identifying OOD images in real-time on an embedded platform concurrently performing object detection and lane following. We also show that it can be used to successfully stop the vehicle in the presence of unknown, novel samples.

</p>
</details>

<details><summary><b>Tire-road friction estimation and uncertainty assessment to improve electric aircraft braking system</b>
<a href="https://arxiv.org/abs/2211.10336">arxiv:2211.10336</a>
&#x1F4C8; 2 <br>
<p>Francesco Crocetti, G. Costante, M. L. Fravolini, P. Valigi</p></summary>
<p>

**Abstract:** The accurate online estimation of the road-friction coefficient is an essential feature for any advanced brake control system. In this study, a data-driven scheme based on a MLP Neural Net is proposed to estimate the optimum friction coefficient as a function of windowed slip-friction measurements. A stochastic NN weights drop-out mechanism is used to online estimate the confidence interval of the estimated best friction coefficient thus providing a characterization of the epistemic uncertainty associated to the NN block. Open loop and closed loop simulations of the landing phase of an aircraft on an unknown surface are used to show the potentiality and efficacy of the proposed robust friction estimation approach.

</p>
</details>

<details><summary><b>Is my automatic audio captioning system so bad? spider-max: a metric to consider several caption candidates</b>
<a href="https://arxiv.org/abs/2211.08983">arxiv:2211.08983</a>
&#x1F4C8; 2 <br>
<p>Etienne Labbé, Thomas Pellegrini, Julien Pinquier</p></summary>
<p>

**Abstract:** Automatic Audio Captioning (AAC) is the task that aims to describe an audio signal using natural language. AAC systems take as input an audio signal and output a free-form text sentence, called a caption. Evaluating such systems is not trivial, since there are many ways to express the same idea. For this reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr, are used to compare a single automatic caption to one or several captions of reference, produced by a human annotator. Nevertheless, an automatic system can produce several caption candidates, either using some randomness in the sentence generation process, or by considering the various competing hypothesized captions during decoding with beam-search, for instance. If we consider an end-user of an AAC system, presenting several captions instead of a single one seems relevant to provide some diversity, similarly to information retrieval systems. In this work, we explore the possibility to consider several predicted captions in the evaluation process instead of one. For this purpose, we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the scores of several caption candidates. To advocate for our metric, we report experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On AudioCaps for example, this system reached a SPIDEr-max value (with 5 candidates) close to the SPIDEr human score of reference.

</p>
</details>

<details><summary><b>ET-AL: Entropy-Targeted Active Learning for Bias Mitigation in Materials Data</b>
<a href="https://arxiv.org/abs/2211.07881">arxiv:2211.07881</a>
&#x1F4C8; 2 <br>
<p>Hengrui Zhang, Wei Wayne Chen, James M. Rondinelli, Wei Chen</p></summary>
<p>

**Abstract:** Growing materials data and data-centric informatics tools drastically promote the discovery and design of materials. While data-driven models, such as machine learning, have drawn much attention and observed significant progress, the quality of data resources is equally important but less studied. In this work, we focus on bias mitigation, an important aspect of materials data quality. Quantifying the diversity of stability in different crystal systems, we propose a metric for measuring structure-stability bias in materials data. To mitigate the bias, we develop an entropy-target active learning (ET-AL) framework, guiding the acquisition of new data so that diversities of underrepresented crystal systems are improved, thus mitigating the bias. With experiments on materials datasets, we demonstrate the capability of ET-AL and the improvement in machine learning models through bias mitigation. The approach is applicable to data-centric informatics in other scientific domains.

</p>
</details>

<details><summary><b>Describing emotions with acoustic property prompts for speech emotion recognition</b>
<a href="https://arxiv.org/abs/2211.07737">arxiv:2211.07737</a>
&#x1F4C8; 2 <br>
<p>Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh</p></summary>
<p>

**Abstract:** Emotions lie on a broad continuum and treating emotions as a discrete number of classes limits the ability of a model to capture the nuances in the continuum. The challenge is how to describe the nuances of emotions and how to enable a model to learn the descriptions. In this work, we devise a method to automatically create a description (or prompt) for a given audio by computing acoustic properties, such as pitch, loudness, speech rate, and articulation rate. We pair a prompt with its corresponding audio using 5 different emotion datasets. We trained a neural network model using these audio-text pairs. Then, we evaluate the model using one more dataset. We investigate how the model can learn to associate the audio with the descriptions, resulting in performance improvement of Speech Emotion Recognition and Speech Audio Retrieval. We expect our findings to motivate research describing the broad continuum of emotion

</p>
</details>

<details><summary><b>An online algorithm for contrastive Principal Component Analysis</b>
<a href="https://arxiv.org/abs/2211.07723">arxiv:2211.07723</a>
&#x1F4C8; 2 <br>
<p>Siavash Golkar, David Lipshutz, Tiberiu Tesileanu, Dmitri B. Chklovskii</p></summary>
<p>

**Abstract:** Finding informative low-dimensional representations that can be computed efficiently in large datasets is an important problem in data analysis. Recently, contrastive Principal Component Analysis (cPCA) was proposed as a more informative generalization of PCA that takes advantage of contrastive learning. However, the performance of cPCA is sensitive to hyper-parameter choice and there is currently no online algorithm for implementing cPCA. Here, we introduce a modified cPCA method, which we denote cPCA*, that is more interpretable and less sensitive to the choice of hyper-parameter. We derive an online algorithm for cPCA* and show that it maps onto a neural network with local learning rules, so it can potentially be implemented in energy efficient neuromorphic hardware. We evaluate the performance of our online algorithm on real datasets and highlight the differences and similarities with the original formulation.

</p>
</details>

<details><summary><b>Millimeter Wave Drones with Cameras: Computer Vision Aided Wireless Beam Prediction</b>
<a href="https://arxiv.org/abs/2211.07569">arxiv:2211.07569</a>
&#x1F4C8; 2 <br>
<p>Gouranga Charan, Andrew Hredzak, Ahmed Alkhateeb</p></summary>
<p>

**Abstract:** Millimeter wave (mmWave) and terahertz (THz) drones have the potential to enable several futuristic applications such as coverage extension, enhanced security monitoring, and disaster management. However, these drones need to deploy large antenna arrays and use narrow directive beams to maintain a sufficient link budget. The large beam training overhead associated with these arrays makes adjusting these narrow beams challenging for highly-mobile drones. To address these challenges, this paper proposes a vision-aided machine learning-based approach that leverages visual data collected from cameras installed on the drones to enable fast and accurate beam prediction. Further, to facilitate the evaluation of the proposed solution, we build a synthetic drone communication dataset consisting of co-existing wireless and visual data. The proposed vision-aided solution achieves a top-$1$ beam prediction accuracy of $\approx 91\%$ and close to $100\%$ top-$3$ accuracy. These results highlight the efficacy of the proposed solution towards enabling highly mobile mmWave/THz drone communication.

</p>
</details>

<details><summary><b>Reinforcement Learning Based Resource Allocation for Network Slices in O-RAN Midhaul</b>
<a href="https://arxiv.org/abs/2211.07466">arxiv:2211.07466</a>
&#x1F4C8; 2 <br>
<p>Nien Fang Cheng, Turgay Pamuklu, Melike Erol-Kantarci</p></summary>
<p>

**Abstract:** Network slicing envisions the 5th generation (5G) mobile network resource allocation to be based on different requirements for different services, such as Ultra-Reliable Low Latency Communication (URLLC) and Enhanced Mobile Broadband (eMBB). Open Radio Access Network (O-RAN), proposes an open and disaggregated concept of RAN by modulizing the functionalities into independent components. Network slicing for O-RAN can significantly improve performance. Therefore, an advanced resource allocation solution for network slicing in O-RAN is proposed in this study by applying Reinforcement Learning (RL). This research demonstrates an RL compatible simplified edge network simulator with three components, user equipment(UE), Edge O-Cloud, and Regional O-Cloud. This simulator is later used to discover how to improve throughput for targeted network slice(s) by dynamically allocating unused bandwidth from other slices. Increasing the throughput for certain network slicing can also benefit the end users with a higher average data rate, peak rate, or shorter transmission time. The results show that the RL model can provide eMBB traffic with a high peak rate and shorter transmission time for URLLC compared to balanced and eMBB focus baselines.

</p>
</details>

<details><summary><b>MR-NOM: Multi-scale Resolution of Neuronal cells in Nissl-stained histological slices via deliberate Over-segmentation and Merging</b>
<a href="https://arxiv.org/abs/2211.07415">arxiv:2211.07415</a>
&#x1F4C8; 2 <br>
<p>Valentina Vadori, Jean-Marie Graïc, Livio Finos, Livio Corain, Antonella Peruffo, Enrico Grisan</p></summary>
<p>

**Abstract:** In comparative neuroanatomy, the characterization of brain cytoarchitecture is critical to a better understanding of brain structure and function, as it helps to distill information on the development, evolution, and distinctive features of different populations. The automatic segmentation of individual brain cells is a primary prerequisite and yet remains challenging. A new method (MR-NOM) was developed for the instance segmentation of cells in Nissl-stained histological images of the brain. MR-NOM exploits a multi-scale approach to deliberately over-segment the cells into superpixels and subsequently merge them via a classifier based on shape, structure, and intensity features. The method was tested on images of the cerebral cortex, proving successful in dealing with cells of varying characteristics that partially touch or overlap, showing better performance than two state-of-the-art methods.

</p>
</details>

<details><summary><b>Multi-center anatomical segmentation with heterogeneous labels via landmark-based models</b>
<a href="https://arxiv.org/abs/2211.07395">arxiv:2211.07395</a>
&#x1F4C8; 2 <br>
<p>Nicolás Gaggion, Maria Vakalopoulou, Diego H. Milone, Enzo Ferrante</p></summary>
<p>

**Abstract:** Learning anatomical segmentation from heterogeneous labels in multi-center datasets is a common situation encountered in clinical scenarios, where certain anatomical structures are only annotated in images coming from particular medical centers, but not in the full database. Here we first show how state-of-the-art pixel-level segmentation models fail in naively learning this task due to domain memorization issues and conflicting labels. We then propose to adopt HybridGNet, a landmark-based segmentation model which learns the available anatomical structures using graph-based representations. By analyzing the latent space learned by both models, we show that HybridGNet naturally learns more domain-invariant feature representations, and provide empirical evidence in the context of chest X-ray multiclass segmentation. We hope these insights will shed light on the training of deep learning models with heterogeneous labels from public and multi-center datasets.

</p>
</details>

<details><summary><b>Follow the Clairvoyant: an Imitation Learning Approach to Optimal Control</b>
<a href="https://arxiv.org/abs/2211.07389">arxiv:2211.07389</a>
&#x1F4C8; 2 <br>
<p>Andrea Martin, Luca Furieri, Florian Dörfler, John Lygeros, Giancarlo Ferrari-Trecate</p></summary>
<p>

**Abstract:** We consider control of dynamical systems through the lens of competitive analysis. Most prior work in this area focuses on minimizing regret, that is, the loss relative to an ideal clairvoyant policy that has noncausal access to past, present, and future disturbances. Motivated by the observation that the optimal cost only provides coarse information about the ideal closed-loop behavior, we instead propose directly minimizing the tracking error relative to the optimal trajectories in hindsight, i.e., imitating the clairvoyant policy. By embracing a system level perspective, we present an efficient optimization-based approach for computing follow-the-clairvoyant (FTC) safe controllers. We prove that these attain minimal regret if no constraints are imposed on the noncausal benchmark. In addition, we present numerical experiments to show that our policy retains the hallmark of competitive algorithms of interpolating between classical $\mathcal{H}_2$ and $\mathcal{H}_\infty$ control laws - while consistently outperforming regret minimization methods in constrained scenarios thanks to the superior ability to chase the clairvoyant.

</p>
</details>

<details><summary><b>CurvPnP: Plug-and-play Blind Image Restoration with Deep Curvature Denoiser</b>
<a href="https://arxiv.org/abs/2211.07286">arxiv:2211.07286</a>
&#x1F4C8; 2 <br>
<p>Yutong Li, Yuping Duan</p></summary>
<p>

**Abstract:** Due to the development of deep learning-based denoisers, the plug-and-play strategy has achieved great success in image restoration problems. However, existing plug-and-play image restoration methods are designed for non-blind Gaussian denoising such as zhang et al (2022), the performance of which visibly deteriorate for unknown noises. To push the limits of plug-and-play image restoration, we propose a novel framework with blind Gaussian prior, which can deal with more complicated image restoration problems in the real world. More specifically, we build up a new image restoration model by regarding the noise level as a variable, which is implemented by a two-stage blind Gaussian denoiser consisting of a noise estimation subnetwork and a denoising subnetwork, where the noise estimation subnetwork provides the noise level to the denoising subnetwork for blind noise removal. We also introduce the curvature map into the encoder-decoder architecture and the supervised attention module to achieve a highly flexible and effective convolutional neural network. The experimental results on image denoising, deblurring and single-image super-resolution are provided to demonstrate the advantages of our deep curvature denoiser and the resulting plug-and-play blind image restoration method over the state-of-the-art model-based and learning-based methods. Our model is shown to be able to recover the fine image details and tiny structures even when the noise level is unknown for different image restoration tasks. The source codes are available at https://github.com/Duanlab123/CurvPnP.

</p>
</details>

<details><summary><b>Multi-Reference Entropy Model for Learned Image Compression</b>
<a href="https://arxiv.org/abs/2211.07273">arxiv:2211.07273</a>
&#x1F4C8; 2 <br>
<p>Wei Jiang, Jiayu Yang, Yongqi Zhai, Ronggang Wang</p></summary>
<p>

**Abstract:** Recently, learned image compression has achieved remarkable performance. Entropy model, which accurately estimates the distribution of latent representation, plays an important role in boosting rate distortion performance. Most entropy models capture correlations in one dimension. However, there are channel-wise, local and global spatial correlations in latent representation. To address this issue, we propose multi-reference entropy models MEM and MEM+ to capture channel, local and global spatial contexts. We divide latent representation into slices. When decoding current slice, we use previously decoded slices as contexts and use attention map of previously decoded slice to predict global correlations in current slice. To capture local contexts, we propose enhanced checkerboard context capturing to avoid performance degradation while retaining two-pass decoding. Based on MEM and MEM+, we propose image compression models MLIC and MLIC+. Extensive experimental evaluations have shown that our MLIC and MLIC+ achieve state-of-the-art performance and they reduce BD-rate by 9.77% and 13.09% on Kodak dataset over VVC when measured in PSNR.

</p>
</details>

<details><summary><b>Shared Loss between Generators of GANs</b>
<a href="https://arxiv.org/abs/2211.07234">arxiv:2211.07234</a>
&#x1F4C8; 2 <br>
<p>Xin Wang</p></summary>
<p>

**Abstract:** Generative adversarial networks are generative models that are capable of replicating the implicit probability distribution of the input data with high accuracy. Traditionally, GANs consist of a Generator and a Discriminator which interact with each other to produce highly realistic artificial data. Traditional GANs fall prey to the mode collapse problem, which means that they are unable to generate the different variations of data present in the input dataset. Recently, multiple generators have been used to produce more realistic output by mitigating the mode collapse problem. We use this multiple generator framework. The novelty in this paper lies in making the generators compete against each other while interacting with the discriminator simultaneously. We show that this causes a dramatic reduction in the training time for GANs without affecting its performance.

</p>
</details>

<details><summary><b>FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model</b>
<a href="https://arxiv.org/abs/2211.07160">arxiv:2211.07160</a>
&#x1F4C8; 2 <br>
<p>Shuo Shao, Wenyuan Yang, Hanlin Gu, Jian Lou, Zhan Qin, Lixin Fan, Qiang Yang, Kui Ren</p></summary>
<p>

**Abstract:** Copyright protection of the Federated Learning (FL) model has become a major concern since malicious clients in FL can stealthily distribute or sell the FL model to other parties. In order to prevent such misbehavior, one must be able to catch the culprit by investigating trace evidence from the model in question. In this paper, we propose FedTracker, the first FL model protection framework that, on one hand, employs global watermarks to verify ownerships of the global model; and on the other hand, embed unique local fingerprints into respective local models to facilitate tracing the model back to the culprit. Furthermore, FedTracker introduces the intuition of Continual Learning (CL) into watermark embedding, and proposes a CL-based watermark mechanism to improve fidelity. Experimental results show that the proposed FedTracker is effective in ownership verification, traceability, fidelity, and robustness.

</p>
</details>

<details><summary><b>WSC-Trans: A 3D network model for automatic multi-structural segmentation of temporal bone CT</b>
<a href="https://arxiv.org/abs/2211.07143">arxiv:2211.07143</a>
&#x1F4C8; 2 <br>
<p>Xin Hua, Zhijiang Du, Hongjian Yu, Jixin Ma, Fanjun Zheng, Cheng Zhang, Qiaohui Lu, Hui Zhao</p></summary>
<p>

**Abstract:** Cochlear implantation is currently the most effective treatment for patients with severe deafness, but mastering cochlear implantation is extremely challenging because the temporal bone has extremely complex and small three-dimensional anatomical structures, and it is important to avoid damaging the corresponding structures when performing surgery. The spatial location of the relevant anatomical tissues within the target area needs to be determined using CT prior to the procedure. Considering that the target structures are too small and complex, the time required for manual segmentation is too long, and it is extremely challenging to segment the temporal bone and its nearby anatomical structures quickly and accurately. To overcome this difficulty, we propose a deep learning-based algorithm, a 3D network model for automatic segmentation of multi-structural targets in temporal bone CT that can automatically segment the cochlea, facial nerve, auditory tubercle, vestibule and semicircular canal. The algorithm combines CNN and Transformer for feature extraction and takes advantage of spatial attention and channel attention mechanisms to further improve the segmentation effect, the experimental results comparing with the results of various existing segmentation algorithms show that the dice similarity scores, Jaccard coefficients of all targets anatomical structures are significantly higher while HD95 and ASSD scores are lower, effectively proving that our method outperforms other advanced methods.

</p>
</details>

<details><summary><b>Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring</b>
<a href="https://arxiv.org/abs/2211.07138">arxiv:2211.07138</a>
&#x1F4C8; 2 <br>
<p>Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Zhihua Xia, Gerald Schaefer, Hui Fang</p></summary>
<p>

**Abstract:** Federated learning (FL) allows multiple participants to collaboratively build deep learning (DL) models without directly sharing data. Consequently, the issue of copyright protection in FL becomes important since unreliable participants may gain access to the jointly trained model. Application of homomorphic encryption (HE) in secure FL framework prevents the central server from accessing plaintext models. Thus, it is no longer feasible to embed the watermark at the central server using existing watermarking schemes. In this paper, we propose a novel client-side FL watermarking scheme to tackle the copyright protection issue in secure FL with HE. To our best knowledge, it is the first scheme to embed the watermark to models under the Secure FL environment. We design a black-box watermarking scheme based on client-side backdooring to embed a pre-designed trigger set into an FL model by a gradient-enhanced embedding method. Additionally, we propose a trigger set construction mechanism to ensure the watermark cannot be forged. Experimental results demonstrate that our proposed scheme delivers outstanding protection performance and robustness against various watermark removal attacks and ambiguity attack.

</p>
</details>

<details><summary><b>Edge-MultiAI: Multi-Tenancy of Latency-Sensitive Deep Learning Applications on Edge</b>
<a href="https://arxiv.org/abs/2211.07130">arxiv:2211.07130</a>
&#x1F4C8; 2 <br>
<p>SM Zobaed, Ali Mokhtari, Jaya Prakash Champati, Mathieu Kourouma, Mohsen Amini Salehi</p></summary>
<p>

**Abstract:** Smart IoT-based systems often desire continuous execution of multiple latency-sensitive Deep Learning (DL) applications. The edge servers serve as the cornerstone of such IoT-based systems, however, their resource limitations hamper the continuous execution of multiple (multi-tenant) DL applications. The challenge is that, DL applications function based on bulky "neural network (NN) models" that cannot be simultaneously maintained in the limited memory space of the edge. Accordingly, the main contribution of this research is to overcome the memory contention challenge, thereby, meeting the latency constraints of the DL applications without compromising their inference accuracy. We propose an efficient NN model management framework, called Edge-MultiAI, that ushers the NN models of the DL applications into the edge memory such that the degree of multi-tenancy and the number of warm-starts are maximized. Edge-MultiAI leverages NN model compression techniques, such as model quantization, and dynamically loads NN models for DL applications to stimulate multi-tenancy on the edge server. We also devise a model management heuristic for Edge-MultiAI, called iWS-BFE, that functions based on the Bayesian theory to predict the inference requests for multi-tenant applications, and uses it to choose the appropriate NN models for loading, hence, increasing the number of warm-start inferences. We evaluate the efficacy and robustness of Edge-MultiAI under various configurations. The results reveal that Edge-MultiAI can stimulate the degree of multi-tenancy on the edge by at least 2X and increase the number of warm-starts by around 60% without any major loss on the inference accuracy of the applications.

</p>
</details>

<details><summary><b>Quality-diversity in dissimilarity spaces</b>
<a href="https://arxiv.org/abs/2211.12337">arxiv:2211.12337</a>
&#x1F4C8; 1 <br>
<p>Steve Huntsman</p></summary>
<p>

**Abstract:** The theory of magnitude provides a mathematical framework for quantifying and maximizing diversity. We apply this framework to formulate quality-diversity algorithms in generic dissimilarity spaces. In particular, we instantiate and demonstrate a very general version of Go-Explore with promising performance.

</p>
</details>

<details><summary><b>Sample-efficient Quantum Born Machine through Coding Rate Reduction</b>
<a href="https://arxiv.org/abs/2211.10418">arxiv:2211.10418</a>
&#x1F4C8; 1 <br>
<p>Pengyuan Zhai</p></summary>
<p>

**Abstract:** The quantum circuit Born machine (QCBM) is a quantum physics inspired implicit generative model naturally suitable for learning binary images, with a potential advantage of modeling discrete distributions that are hard to simulate classically. As data samples are generated quantum-mechanically, QCBMs encompass a unique optimization landscape. However, pioneering works on QCBMs do not consider the practical scenario where only small batch sizes are allowed during training. QCBMs trained with a statistical two-sample test objective in the image space require large amounts of projective measurements to approximate the model distribution well, unpractical for large-scale quantum systems due to the exponential scaling of the probability space. QCBMs trained adversarially against a deep neural network discriminator are proof-of-concept models that face mode collapse. In this work we investigate practical learning of QCBMs. We use the information-theoretic \textit{Maximal Coding Rate Reduction} (MCR$^2$) metric as a second moment matching tool and study its effect on mode collapse in QCBMs. We compute the sampling based gradient of MCR$^2$ with respect to quantum circuit parameters with or without an explicit feature mapping. We experimentally show that matching up to the second moment alone is not sufficient for training the quantum generator, but when combined with the class probability estimation loss, MCR$^2$ is able to resist mode collapse. In addition, we show that adversarially trained neural network kernel for infinite moment matching is also effective against mode collapse. On the Bars and Stripes dataset, our proposed techniques alleviate mode collapse to a larger degree than previous QCBM training schemes, moving one step closer towards practicality and scalability.

</p>
</details>

<details><summary><b>Enabling AI Quality Control via Feature Hierarchical Edge Inference</b>
<a href="https://arxiv.org/abs/2211.07860">arxiv:2211.07860</a>
&#x1F4C8; 1 <br>
<p>Jinhyuk Choi, Seong-Lyun Kim, Seung-Woo Ko</p></summary>
<p>

**Abstract:** With the rise of edge computing, various AI services are expected to be available at a mobile side through the inference based on deep neural network (DNN) operated at the network edge, called edge inference (EI). On the other hand, the resulting AI quality (e.g., mean average precision in objective detection) has been regarded as a given factor, and AI quality control has yet to be explored despite its importance in addressing the diverse demands of different users. This work aims at tackling the issue by proposing a feature hierarchical EI (FHEI), comprising feature network and inference network deployed at an edge server and corresponding mobile, respectively. Specifically, feature network is designed based on feature hierarchy, a one-directional feature dependency with a different scale. A higher scale feature requires more computation and communication loads while it provides a better AI quality. The tradeoff enables FHEI to control AI quality gradually w.r.t. communication and computation loads, leading to deriving a near-to-optimal solution to maximize multi-user AI quality under the constraints of uplink \& downlink transmissions and edge server and mobile computation capabilities. It is verified by extensive simulations that the proposed joint communication-and-computation control on FHEI architecture always outperforms several benchmarks by differentiating each user's AI quality depending on the communication and computation conditions.

</p>
</details>

<details><summary><b>Energy Storage Price Arbitrage via Opportunity Value Function Prediction</b>
<a href="https://arxiv.org/abs/2211.07797">arxiv:2211.07797</a>
&#x1F4C8; 1 <br>
<p>Ningkun Zheng, Xiaoxiang Liu, Bolun Xu, Yuanyuan Shi</p></summary>
<p>

**Abstract:** This paper proposes a novel energy storage price arbitrage algorithm combining supervised learning with dynamic programming. The proposed approach uses a neural network to directly predicts the opportunity cost at different energy storage state-of-charge levels, and then input the predicted opportunity cost into a model-based arbitrage control algorithm for optimal decisions. We generate the historical optimal opportunity value function using price data and a dynamic programming algorithm, then use it as the ground truth and historical price as predictors to train the opportunity value function prediction model. Our method achieves 65% to 90% profit compared to perfect foresight in case studies using different energy storage models and price data from New York State, which significantly outperforms existing model-based and learning-based methods. While guaranteeing high profitability, the algorithm is also light-weighted and can be trained and implemented with minimal computational cost. Our results also show that the learned prediction model has excellent transferability. The prediction model trained using price data from one region also provides good arbitrage results when tested over other regions.

</p>
</details>

<details><summary><b>Adaptive search space decomposition method for pre- and post- buckling analyses of space truss structures</b>
<a href="https://arxiv.org/abs/2211.07519">arxiv:2211.07519</a>
&#x1F4C8; 1 <br>
<p>Varun Ojha, Bartolomeo Panto, Giuseppe Nicosia</p></summary>
<p>

**Abstract:** The paper proposes a novel adaptive search space decomposition method and a novel gradient-free optimization-based formulation for the pre- and post-buckling analyses of space truss structures. Space trusses are often employed in structural engineering to build large steel constructions, such as bridges and domes, whose structural response is characterized by large displacements. Therefore, these structures are vulnerable to progressive collapses due to local or global buckling effects, leading to sudden failures. The method proposed in this paper allows the analysis of the load-equilibrium path of truss structures to permanent and variable loading, including stable and unstable equilibrium stages and explicitly considering geometric nonlinearities. The goal of this work is to determine these equilibrium stages via optimization of the Lagrangian kinematic parameters of the system, determining the global equilibrium. However, this optimization problem is non-trivial due to the undefined parameter domain and the sensitivity and interaction among the Lagrangian parameters. Therefore, we propose formulating this problem as a nonlinear, multimodal, unconstrained, continuous optimization problem and develop a novel adaptive search space decomposition method, which progressively and adaptively re-defines the search domain (hypersphere) to evaluate the equilibrium of the system using a gradient-free optimization algorithm. We tackle three benchmark problems and evaluate a medium-sized test representing a real structural problem in this paper. The results are compared to those available in the literature regarding displacement-load curves and deformed configurations. The accuracy and robustness of the adopted methodology show a high potential of gradient-free algorithms in analyzing space truss structures.

</p>
</details>

<details><summary><b>An Analytics of Culture: Modeling Subjectivity, Scalability, Contextuality, and Temporality</b>
<a href="https://arxiv.org/abs/2211.07460">arxiv:2211.07460</a>
&#x1F4C8; 1 <br>
<p>Nanne van Noord, Melvin Wevers, Tobias Blanke, Julia Noordegraaf, Marcel Worring</p></summary>
<p>

**Abstract:** There is a bidirectional relationship between culture and AI; AI models are increasingly used to analyse culture, thereby shaping our understanding of culture. On the other hand, the models are trained on collections of cultural artifacts thereby implicitly, and not always correctly, encoding expressions of culture. This creates a tension that both limits the use of AI for analysing culture and leads to problems in AI with respect to cultural complex issues such as bias.
  One approach to overcome this tension is to more extensively take into account the intricacies and complexities of culture. We structure our discussion using four concepts that guide humanistic inquiry into culture: subjectivity, scalability, contextuality, and temporality. We focus on these concepts because they have not yet been sufficiently represented in AI research. We believe that possible implementations of these aspects into AI research leads to AI that better captures the complexities of culture. In what follows, we briefly describe these four concepts and their absence in AI research. For each concept, we define possible research challenges.

</p>
</details>

<details><summary><b>Global Performance Guarantees for Neural Network Models of AC Power Flow</b>
<a href="https://arxiv.org/abs/2211.07125">arxiv:2211.07125</a>
&#x1F4C8; 1 <br>
<p>Samuel Chevalier, Spyros Chatzivasileiadis</p></summary>
<p>

**Abstract:** Machine learning can generate black-box surrogate models which are both extremely fast and highly accurate. Rigorously verifying the accuracy of these black-box models, however, is computationally challenging. When it comes to power systems, learning AC power flow is the cornerstone of any machine learning surrogate model wishing to drastically accelerate computations, whether it is for optimization, control, or dynamics. This paper develops for the first time, to our knowledge, a tractable neural network verification procedure which incorporates the ground truth of the \emph{non-linear} AC power flow equations to determine worst-case neural network performance. Our approach, termed Sequential Targeted Tightening (STT), leverages a loosely convexified reformulation of the original verification problem, which is a mixed integer quadratic program (MIQP). Using the sequential addition of targeted cuts, we iteratively tighten our formulation until either the solution is sufficiently tight or a satisfactory performance guarantee has been generated. After learning neural network models of the 14, 57, 118, and 200-bus PGLib test cases, we compare the performance guarantees generated by our STT procedure with ones generated by a state-of-the-art MIQP solver, Gurobi 9.5. We show that STT often generates performance guarantees which are orders of magnitude tighter than the MIQP upper bound.

</p>
</details>

<details><summary><b>Un discours et un public "Gilets Jaunes" au coeur du Grand Débat National? Combinaison des approches IA et textométriques pour l'analyse de discours des plateformes "Grand Débat National" et "Vrai débat"</b>
<a href="https://arxiv.org/abs/2211.11521">arxiv:2211.11521</a>
&#x1F4C8; 0 <br>
<p>Suignard Philippe</p></summary>
<p>

**Abstract:** In this contribution, we propose to analyze the statements coming from two ''civic tech'' platforms-the governmental platform, ''Grand D{é}bat National'' and, its political and algorithmic response proposed by a Yellow Vest collective, ''Vrai D{é}bat''-, by confronting two families of algorithms dedicated to text analysis. We propose to implement, on the one hand, proven approaches in textual data analysis (Reinert/Iramuteq Method) which have recently shown their interest in the analysis of very large corpora and, on the other hand, new methods resulting from the crossroads of the computer worlds, artificial intelligence and automatic language processing. We will examine the methodological solutions for qualifying the social properties of speakers about whom we have little direct information. Finally, we will attempt to present some research questions at the crossroads of the political sociology of public opinion and data science, which such a confrontation opens up.

</p>
</details>

<details><summary><b>Explainer Divergence Scores (EDS): Some Post-Hoc Explanations May be Effective for Detecting Unknown Spurious Correlations</b>
<a href="https://arxiv.org/abs/2211.07650">arxiv:2211.07650</a>
&#x1F4C8; 0 <br>
<p>Shea Cardozo, Gabriel Islas Montero, Dmitry Kazhdan, Botty Dimanov, Maleakhi Wijaya, Mateja Jamnik, Pietro Lio</p></summary>
<p>

**Abstract:** Recent work has suggested post-hoc explainers might be ineffective for detecting spurious correlations in Deep Neural Networks (DNNs). However, we show there are serious weaknesses with the existing evaluation frameworks for this setting. Previously proposed metrics are extremely difficult to interpret and are not directly comparable between explainer methods. To alleviate these constraints, we propose a new evaluation methodology, Explainer Divergence Scores (EDS), grounded in an information theory approach to evaluate explainers. EDS is easy to interpret and naturally comparable across explainers. We use our methodology to compare the detection performance of three different explainers - feature attribution methods, influential examples and concept extraction, on two different image datasets. We discover post-hoc explainers often contain substantial information about a DNN's dependence on spurious artifacts, but in ways often imperceptible to human users. This suggests the need for new techniques that can use this information to better detect a DNN's reliance on spurious correlations.

</p>
</details>

<details><summary><b>Optimizing Stimulus Energy for Cochlear Implants with a Machine Learning Model of the Auditory Nerve</b>
<a href="https://arxiv.org/abs/2211.07285">arxiv:2211.07285</a>
&#x1F4C8; 0 <br>
<p>Jacob de Nobel, Anna V. Kononova, Jeroen Briaire, Johan Frijns, Thomas Bäck</p></summary>
<p>

**Abstract:** Performing simulations with a realistic biophysical auditory nerve fiber model can be very time consuming, due to the complex nature of the calculations involved. Here, a surrogate (approximate) model of such an auditory nerve fiber model was developed using machine learning methods, to perform simulations more efficiently. Several machine learning models were compared, of which a Convolutional Neural Network showed the best performance. In fact, the Convolutional Neural Network was able to emulate the behavior of the auditory nerve fiber model with extremely high similarity ($R^2 > 0.99$), tested under a wide range of experimental conditions, whilst reducing the simulation time by five orders of magnitude. In addition, we introduce a method for randomly generating charge-balanced waveforms using hyperplane projection. In the second part of this paper, the Convolutional Neural Network surrogate model was used by an Evolutionary Algorithm to optimize the shape of the stimulus waveform in terms energy efficiency. The resulting waveforms resemble a positive Gaussian-like peak, preceded by an elongated negative phase. When comparing the energy of the waveforms generated by the Evolutionary Algorithm with the commonly used square wave, energy decreases of 8% - 45% were observed for different pulse durations. These results were validated with the original auditory nerve fiber model, which demonstrates that our proposed surrogate model can be used as its accurate and efficient replacement.

</p>
</details>


{% endraw %}
Prev: [2022.11.13]({{ '/2022/11/13/2022.11.13.html' | relative_url }})  Next: [2022.11.15]({{ '/2022/11/15/2022.11.15.html' | relative_url }})