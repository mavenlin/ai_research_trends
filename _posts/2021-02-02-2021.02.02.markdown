## Summary for 2021-02-02, created on 2021-12-23


<details><summary><b>Size Matters</b>
<a href="https://arxiv.org/abs/2102.01582">arxiv:2102.01582</a>
&#x1F4C8; 134000 <br>
<p>Mats L. Richter, Wolf Byttner, Ulf Krumnack, Ludwdig Schallner, Justin Shenk</p></summary>
<p>

**Abstract:** Fully convolutional neural networks can process input of arbitrary size by applying a combination of downsampling and pooling. However, we find that fully convolutional image classifiers are not agnostic to the input size but rather show significant differences in performance: presenting the same image at different scales can result in different outcomes. A closer look reveals that there is no simple relationship between input size and model performance (no `bigger is better'), but that each each network has a preferred input size, for which it shows best results. We investigate this phenomenon by applying different methods, including spectral analysis of layer activations and probe classifiers, showing that there are characteristic features depending on the network architecture. From this we find that the size of discriminatory features is critically influencing how the inference process is distributed among the layers.

</p>
</details>

<details><summary><b>Report of the Workshop on Program Synthesis for Scientific Computing</b>
<a href="https://arxiv.org/abs/2102.01687">arxiv:2102.01687</a>
&#x1F4C8; 82 <br>
<p>Hal Finkel, Ignacio Laguna</p></summary>
<p>

**Abstract:** Program synthesis is an active research field in academia, national labs, and industry. Yet, work directly applicable to scientific computing, while having some impressive successes, has been limited. This report reviews the relevant areas of program synthesis work for scientific computing, discusses successes to date, and outlines opportunities for future work. This report is the result of the Workshop on Program Synthesis for Scientific Computing was held virtually on August 4-5 2020 (https://prog-synth-science.github.io/2020/).

</p>
</details>

<details><summary><b>The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</b>
<a href="https://arxiv.org/abs/2102.01672">arxiv:2102.01672</a>
&#x1F4C8; 65 <br>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak</p></summary>
<p>

**Abstract:** We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.

</p>
</details>

<details><summary><b>Neural Data Augmentation via Example Extrapolation</b>
<a href="https://arxiv.org/abs/2102.01335">arxiv:2102.01335</a>
&#x1F4C8; 65 <br>
<p>Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, Hyung Won Chung</p></summary>
<p>

**Abstract:** In many applications of machine learning, certain categories of examples may be underrepresented in the training data, causing systems to underperform on such "few-shot" cases at test time. A common remedy is to perform data augmentation, such as by duplicating underrepresented examples, or heuristically synthesizing new examples. But these remedies often fail to cover the full diversity and complexity of real examples.
  We propose a data augmentation approach that performs neural Example Extrapolation (Ex2). Given a handful of exemplars sampled from some distribution, Ex2 synthesizes new examples that also belong to the same distribution. The Ex2 model is learned by simulating the example generation procedure on data-rich slices of the data, and it is applied to underrepresented, few-shot slices.
  We apply Ex2 to a range of language understanding tasks and significantly improve over state-of-the-art methods on multiple few-shot learning benchmarks, including for relation extraction (FewRel) and intent classification + slot filling (SNIPS).

</p>
</details>

<details><summary><b>Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation</b>
<a href="https://arxiv.org/abs/2102.01678">arxiv:2102.01678</a>
&#x1F4C8; 46 <br>
<p>Rikiya Yamashita, Jin Long, Snikitha Banda, Jeanne Shen, Daniel L. Rubin</p></summary>
<p>

**Abstract:** Suboptimal generalization of machine learning models on unseen data is a key challenge which hampers the clinical applicability of such models to medical imaging. Although various methods such as domain adaptation and domain generalization have evolved to combat this challenge, learning robust and generalizable representations is core to medical image understanding, and continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation for histoPathology), a form of data augmentation based on random style transfer from non-medical style source such as artistic paintings, for learning domain-agnostic visual representations in computational pathology. Style transfer replaces the low-level texture content of an image with the uninformative style of randomly selected style source image, while preserving the original high-level semantic content. This improves robustness to domain shift and can be used as a simple yet powerful tool for learning domain-agnostic representations. We demonstrate that STRAP leads to state-of-the-art performance, particularly in the presence of domain shifts, on two particular classification tasks in computational pathology.

</p>
</details>

<details><summary><b>Transfer Learning in Magnetic Resonance Brain Imaging: a Systematic Review</b>
<a href="https://arxiv.org/abs/2102.01530">arxiv:2102.01530</a>
&#x1F4C8; 46 <br>
<p>Juan Miguel Valverde, Vandad Imani, Ali Abdollahzadeh, Riccardo De Feo, Mithilesh Prakash, Robert Ciszek, Jussi Tohka</p></summary>
<p>

**Abstract:** Transfer learning refers to machine learning techniques that focus on acquiring knowledge from related tasks to improve generalization in the tasks of interest. In MRI, transfer learning is important for developing strategies that address the variation in MR images. Additionally, transfer learning is beneficial to re-utilize machine learning models that were trained to solve related tasks to the task of interest. Our goal is to identify research directions, gaps of knowledge, applications, and widely used strategies among the transfer learning approaches applied in MR brain imaging. We performed a systematic literature search for articles that applied transfer learning to MR brain imaging. We screened 433 studies and we categorized and extracted relevant information, including task type, application, and machine learning methods. Furthermore, we closely examined brain MRI-specific transfer learning approaches and other methods that tackled privacy, unseen target domains, and unlabeled data. We found 129 articles that applied transfer learning to brain MRI tasks. The most frequent applications were dementia related classification tasks and brain tumor segmentation. A majority of articles utilized transfer learning on convolutional neural networks (CNNs). Only few approaches were clearly brain MRI specific, considered privacy issues, unseen target domains or unlabeled data. We proposed a new categorization to group specific, widely-used approaches. There is an increasing interest in transfer learning within brain MRI. Public datasets have contributed to the popularity of Alzheimer's diagnostics/prognostics and tumor segmentation. Likewise, the availability of pretrained CNNs has promoted their utilization. Finally, the majority of the surveyed studies did not examine in detail the interpretation of their strategies after applying transfer learning, and did not compare to other approaches.

</p>
</details>

<details><summary><b>Guidance on the Assurance of Machine Learning in Autonomous Systems (AMLAS)</b>
<a href="https://arxiv.org/abs/2102.01564">arxiv:2102.01564</a>
&#x1F4C8; 39 <br>
<p>Richard Hawkins, Colin Paterson, Chiara Picardi, Yan Jia, Radu Calinescu, Ibrahim Habli</p></summary>
<p>

**Abstract:** Machine Learning (ML) is now used in a range of systems with results that are reported to exceed, under certain conditions, human performance. Many of these systems, in domains such as healthcare , automotive and manufacturing, exhibit high degrees of autonomy and are safety critical. Establishing justified confidence in ML forms a core part of the safety case for these systems. In this document we introduce a methodology for the Assurance of Machine Learning for use in Autonomous Systems (AMLAS). AMLAS comprises a set of safety case patterns and a process for (1) systematically integrating safety assurance into the development of ML components and (2) for generating the evidence base for explicitly justifying the acceptable safety of these components when integrated into autonomous system applications.

</p>
</details>

<details><summary><b>Predicting the Time Until a Vehicle Changes the Lane Using LSTM-based Recurrent Neural Networks</b>
<a href="https://arxiv.org/abs/2102.01431">arxiv:2102.01431</a>
&#x1F4C8; 37 <br>
<p>Florian Wirthmüller, Marvin Klimke, Julian Schlechtriemen, Jochen Hipp, Manfred Reichert</p></summary>
<p>

**Abstract:** To plan safe and comfortable trajectories for automated vehicles on highways, accurate predictions of traffic situations are needed. So far, a lot of research effort has been spent on detecting lane change maneuvers rather than on estimating the point in time a lane change actually happens. In practice, however, this temporal information might be even more useful. This paper deals with the development of a system that accurately predicts the time to the next lane change of surrounding vehicles on highways using long short-term memory-based recurrent neural networks. An extensive evaluation based on a large real-world data set shows that our approach is able to make reliable predictions, even in the most challenging situations, with a root mean squared error around 0.7 seconds. Already 3.5 seconds prior to lane changes the predictions become highly accurate, showing a median error of less than 0.25 seconds. In summary, this article forms a fundamental step towards downstreamed highly accurate position predictions.

</p>
</details>

<details><summary><b>Analyzing dynamical disorder for charge transport in organic semiconductors via machine learning</b>
<a href="https://arxiv.org/abs/2102.01479">arxiv:2102.01479</a>
&#x1F4C8; 36 <br>
<p>Patrick Reiser, Manuel Konrad, Artem Fediai, Salvador Léon, Wolfgang Wenzel, Pascal Friederich</p></summary>
<p>

**Abstract:** Organic semiconductors are indispensable for today's display technologies in form of organic light emitting diodes (OLEDs) and further optoelectronic applications. However, organic materials do not reach the same charge carrier mobility as inorganic semiconductors, limiting the efficiency of devices. To find or even design new organic semiconductors with higher charge carrier mobility, computational approaches, in particular multiscale models, are becoming increasingly important. However, such models are computationally very costly, especially when large systems and long time scales are required, which is the case to compute static and dynamic energy disorder, i.e. dominant factor to determine charge transport. Here we overcome this drawback by integrating machine learning models into multiscale simulations. This allows us to obtain unprecedented insight into relevant microscopic materials properties, in particular static and dynamic disorder contributions for a series of application-relevant molecules. We find that static disorder and thus the distribution of shallow traps is highly asymmetrical for many materials, impacting widely considered Gaussian disorder models. We furthermore analyse characteristic energy level fluctuation times and compare them to typical hopping rates to evaluate the importance of dynamic disorder for charge transport. We hope that our findings will significantly improve the accuracy of computational methods used to predict application relevant materials properties of organic semiconductors, and thus make these methods applicable for virtual materials design.

</p>
</details>

<details><summary><b>A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants</b>
<a href="https://arxiv.org/abs/2102.01567">arxiv:2102.01567</a>
&#x1F4C8; 35 <br>
<p>Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, Karthikeyan Shanmugam</p></summary>
<p>

**Abstract:** This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(λ)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(λ)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).

</p>
</details>

<details><summary><b>Robust data-driven discovery of partial differential equations with time-dependent coefficients</b>
<a href="https://arxiv.org/abs/2102.01432">arxiv:2102.01432</a>
&#x1F4C8; 34 <br>
<p>Aoxue Chen, Guang Lin</p></summary>
<p>

**Abstract:** In this work, we propose a robust Bayesian sparse learning algorithm based on Bayesian group Lasso with spike and slab priors for the discovery of partial differential equations with variable coefficients. Using the samples draw from the posterior distribution with a Gibbs sampler, we are able to estimate the values of coefficients, together with their standard errors and confidence intervals. Apart from constructing the error bars, uncertainty quantification can also be employed for designing new criteria of model selection and threshold setting. This enables our method more adjustable and robust in learning equations with time-dependent coefficients. Three criteria are introduced for model selection and threshold setting to identify the correct terms: the root mean square, total error bar, and group error bar. Moreover, three noise filters are integrated with the robust Bayesian sparse learning algorithm for better results with larger noise. Numerical results demonstrate that our method is more robust than sequential grouped threshold ridge regression and group Lasso in noisy situations through three examples.

</p>
</details>

<details><summary><b>Internal Language Model Training for Domain-Adaptive End-to-End Speech Recognition</b>
<a href="https://arxiv.org/abs/2102.01380">arxiv:2102.01380</a>
&#x1F4C8; 34 <br>
<p>Zhong Meng, Naoyuki Kanda, Yashesh Gaur, Sarangarajan Parthasarathy, Eric Sun, Liang Lu, Xie Chen, Jinyu Li, Yifan Gong</p></summary>
<p>

**Abstract:** The efficacy of external language model (LM) integration with existing end-to-end (E2E) automatic speech recognition (ASR) systems can be improved significantly using the internal language model estimation (ILME) method. In this method, the internal LM score is subtracted from the score obtained by interpolating the E2E score with the external LM score, during inference. To improve the ILME-based inference, we propose an internal LM training (ILMT) method to minimize an additional internal LM loss by updating only the E2E model components that affect the internal LM estimation. ILMT encourages the E2E model to form a standalone LM inside its existing components, without sacrificing ASR accuracy. After ILMT, the more modular E2E model with matched training and inference criteria enables a more thorough elimination of the source-domain internal LM, and therefore leads to a more effective integration of the target-domain external LM. Experimented with 30K-hour trained recurrent neural network transducer and attention-based encoder-decoder models, ILMT with ILME-based inference achieves up to 31.5% and 11.4% relative word error rate reductions from standard E2E training with Shallow Fusion on out-of-domain LibriSpeech and in-domain Microsoft production test sets, respectively.

</p>
</details>

<details><summary><b>Global Earth Magnetic Field Modeling and Forecasting with Spherical Harmonics Decomposition</b>
<a href="https://arxiv.org/abs/2102.01447">arxiv:2102.01447</a>
&#x1F4C8; 33 <br>
<p>Panagiotis Tigas, Téo Bloch, Vishal Upendran, Banafsheh Ferdoushi, Mark C. M. Cheung, Siddha Ganju, Ryan M. McGranaghan, Yarin Gal, Asti Bhatt</p></summary>
<p>

**Abstract:** Modeling and forecasting the solar wind-driven global magnetic field perturbations is an open challenge. Current approaches depend on simulations of computationally demanding models like the Magnetohydrodynamics (MHD) model or sampling spatially and temporally through sparse ground-based stations (SuperMAG). In this paper, we develop a Deep Learning model that forecasts in Spherical Harmonics space 2, replacing reliance on MHD models and providing global coverage at one minute cadence, improving over the current state-of-the-art which relies on feature engineering. We evaluate the performance in SuperMAG dataset (improved by 14.53%) and MHD simulations (improved by 24.35%). Additionally, we evaluate the extrapolation performance of the spherical harmonics reconstruction based on sparse ground-based stations (SuperMAG), showing that spherical harmonics can reliably reconstruct the global magnetic field as evaluated on MHD simulation.

</p>
</details>

<details><summary><b>Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge Intelligence via Online Learning</b>
<a href="https://arxiv.org/abs/2102.02638">arxiv:2102.02638</a>
&#x1F4C8; 32 <br>
<p>Letian Zhang, Lixing Chen, Jie Xu</p></summary>
<p>

**Abstract:** Recent breakthroughs in deep learning (DL) have led to the emergence of many intelligent mobile applications and services, but in the meanwhile also pose unprecedented computing challenges on resource-constrained mobile devices. This paper builds a collaborative deep inference system between a resource-constrained mobile device and a powerful edge server, aiming at joining the power of both on-device processing and computation offloading. The basic idea of this system is to partition a deep neural network (DNN) into a front-end part running on the mobile device and a back-end part running on the edge server, with the key challenge being how to locate the optimal partition point to minimize the end-to-end inference delay. Unlike existing efforts on DNN partitioning that rely heavily on a dedicated offline profiling stage to search for the optimal partition point, our system has a built-in online learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn the optimal partition point on-the-fly. Therefore, ANS is able to closely follow the changes of the system environment by generating new knowledge for adaptive decision making. The core of ANS is a novel contextual bandit learning algorithm, called $μ$LinUCB, which not only has provable theoretical learning performance guarantee but also is ultra-lightweight for easy real-world implementation. We implement our system on a video stream object detection testbed to validate the design of ANS and evaluate its performance. The experiments show that ANS significantly outperforms state-of-the-art benchmarks in terms of tracking system changes and reducing the end-to-end inference delay.

</p>
</details>

<details><summary><b>Depth separation beyond radial functions</b>
<a href="https://arxiv.org/abs/2102.01621">arxiv:2102.01621</a>
&#x1F4C8; 28 <br>
<p>Luca Venturi, Samy Jelassi, Tristan Ozuch, Joan Bruna</p></summary>
<p>

**Abstract:** High-dimensional depth separation results for neural networks show that certain functions can be efficiently approximated by two-hidden-layer networks but not by one-hidden-layer ones in high-dimensions $d$. Existing results of this type mainly focus on functions with an underlying radial or one-dimensional structure, which are usually not encountered in practice. The first contribution of this paper is to extend such results to a more general class of functions, namely functions with piece-wise oscillatory structure, by building on the proof strategy of (Eldan and Shamir, 2016). We complement these results by showing that, if the domain radius and the rate of oscillation of the objective function are constant, then approximation by one-hidden-layer networks holds at a $\mathrm{poly}(d)$ rate for any fixed error threshold.
  A common theme in the proofs of depth-separation results is the fact that one-hidden-layer networks fail to approximate high-energy functions whose Fourier representation is spread in the domain. On the other hand, existing approximation results of a function by one-hidden-layer neural networks rely on the function having a sparse Fourier representation. The choice of the domain also represents a source of gaps between upper and lower approximation bounds. Focusing on a fixed approximation domain, namely the sphere $\mathbb{S}^{d-1}$ in dimension $d$, we provide a characterisation of both functions which are efficiently approximable by one-hidden-layer networks and of functions which are provably not, in terms of their Fourier expansion.

</p>
</details>

<details><summary><b>Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint</b>
<a href="https://arxiv.org/abs/2102.08360">arxiv:2102.08360</a>
&#x1F4C8; 27 <br>
<p>Ella Y. Wang, Anirudh Som, Ankita Shukla, Hongjun Choi, Pavan Turaga</p></summary>
<p>

**Abstract:** Deep neural networks have increasingly been used as an auxiliary tool in healthcare applications, due to their ability to improve performance of several diagnosis tasks. However, these methods are not widely adopted in clinical settings due to the practical limitations in the reliability, generalizability, and interpretability of deep learning based systems. As a result, methods have been developed that impose additional constraints during network training to gain more control as well as improve interpretabilty, facilitating their acceptance in healthcare community. In this work, we investigate the benefit of using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases from chest X-ray images. The OS constraint can be written as a simple orthonormality term which is used in conjunction with the standard cross-entropy loss during classification network training. Previous studies have demonstrated significant benefits in applying such constraints to deep learning models. Our findings corroborate these observations, indicating that the orthonormality loss function effectively produces improved semantic localization via GradCAM visualizations, enhanced classification performance, and reduced model calibration error. Our approach achieves an improvement in accuracy of 1.6% and 4.8% for two- and three-class classification, respectively; similar results are found for models with data augmentation applied. In addition to these findings, our work also presents a new application of the OS regularizer in healthcare, increasing the post-hoc interpretability and performance of deep learning models for COVID-19 classification to facilitate adoption of these methods in clinical settings. We also identify the limitations of our strategy that can be explored for further research in future.

</p>
</details>

<details><summary><b>A Data-Driven Approach to Violin Making</b>
<a href="https://arxiv.org/abs/2102.04254">arxiv:2102.04254</a>
&#x1F4C8; 27 <br>
<p>Sebastian Gonzalez, Davide Salvi, Daniel Baeza, Fabio Antonacci, Augusto Sarti</p></summary>
<p>

**Abstract:** Of all the characteristics of a violin, those that concern its shape are probably the most important ones, as the violin maker has complete control over them. Contemporary violin making, however, is still based more on tradition than understanding, and a definitive scientific study of the specific relations that exist between shape and vibrational properties is yet to come and sorely missed. In this article, using standard statistical learning tools, we show that the modal frequencies of violin tops can, in fact, be predicted from geometric parameters, and that artificial intelligence can be successfully applied to traditional violin making. We also study how modal frequencies vary with the thicknesses of the plate (a process often referred to as {\em plate tuning}) and discuss the complexity of this dependency. Finally, we propose a predictive tool for plate tuning, which takes into account material and geometric parameters.

</p>
</details>

<details><summary><b>Agent Incentives: A Causal Perspective</b>
<a href="https://arxiv.org/abs/2102.01685">arxiv:2102.01685</a>
&#x1F4C8; 27 <br>
<p>Tom Everitt, Ryan Carey, Eric Langlois, Pedro A Ortega, Shane Legg</p></summary>
<p>

**Abstract:** We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.

</p>
</details>

<details><summary><b>Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search</b>
<a href="https://arxiv.org/abs/2102.01645">arxiv:2102.01645</a>
&#x1F4C8; 26 <br>
<p>Federico A. Galatolo, Mario G. C. A. Cimino, Gigliola Vaglini</p></summary>
<p>

**Abstract:** In this research work we present CLIP-GLaSS, a novel zero-shot framework to generate an image (or a caption) corresponding to a given caption (or image). CLIP-GLaSS is based on the CLIP neural network, which, given an image and a descriptive caption, provides similar embeddings. Differently, CLIP-GLaSS takes a caption (or an image) as an input, and generates the image (or the caption) whose CLIP embedding is the most similar to the input one. This optimal image (or caption) is produced via a generative network, after an exploration by a genetic algorithm. Promising results are shown, based on the experimentation of the image Generators BigGAN and StyleGAN2, and of the text Generator GPT2

</p>
</details>

<details><summary><b>Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization</b>
<a href="https://arxiv.org/abs/2102.01670">arxiv:2102.01670</a>
&#x1F4C8; 23 <br>
<p>Kale-ab Tessera, Sara Hooker, Benjamin Rosman</p></summary>
<p>

**Abstract:** Training sparse networks to converge to the same performance as dense neural architectures has proven to be elusive. Recent work suggests that initialization is the key. However, while this direction of research has had some success, focusing on initialization alone appears to be inadequate. In this paper, we take a broader view of training sparse networks and consider the role of regularization, optimization, and architecture choices on sparse models. We propose a simple experimental framework, Same Capacity Sparse vs Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and dense networks. Furthermore, we propose a new measure of gradient flow, Effective Gradient Flow (EGF), that better correlates to performance in sparse networks. Using top-line metrics, SC-SDC and EGF, we show that default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, we show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. Our work suggests that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results.

</p>
</details>

<details><summary><b>Policy Analysis using Synthetic Controls in Continuous-Time</b>
<a href="https://arxiv.org/abs/2102.01577">arxiv:2102.01577</a>
&#x1F4C8; 23 <br>
<p>Alexis Bellot, Mihaela van der Schaar</p></summary>
<p>

**Abstract:** Counterfactual estimation using synthetic controls is one of the most successful recent methodological developments in causal inference. Despite its popularity, the current description only considers time series aligned across units and synthetic controls expressed as linear combinations of observed control units. We propose a continuous-time alternative that models the latent counterfactual path explicitly using the formalism of controlled differential equations. This model is directly applicable to the general setting of irregularly-aligned multivariate time series and may be optimized in rich function spaces -- thereby improving on some limitations of existing approaches.

</p>
</details>

<details><summary><b>Medical Datasets Collections for Artificial Intelligence-based Medical Image Analysis</b>
<a href="https://arxiv.org/abs/2102.01549">arxiv:2102.01549</a>
&#x1F4C8; 21 <br>
<p>Yang Wen</p></summary>
<p>

**Abstract:** We collected 32 public datasets, of which 28 for medical imaging and 4 for natural images, to conduct study. The images of these datasets are captured by different cameras, thus vary from each other in modality, frame size and capacity. For data accessibility, we also provide the websites of most datasets and hope this will help the readers reach the datasets.

</p>
</details>

<details><summary><b>Real-time detection of uncalibrated sensors using Neural Networks</b>
<a href="https://arxiv.org/abs/2102.01565">arxiv:2102.01565</a>
&#x1F4C8; 20 <br>
<p>Luis J. Muñoz-Molina, Ignacio Cazorla-Piñar, Juan P. Dominguez-Morales, Fernando Perez-Peña</p></summary>
<p>

**Abstract:** Nowadays, sensors play a major role in several contexts like science, industry and daily life which benefit of their use. However, the retrieved information must be reliable. Anomalies in the behavior of sensors can give rise to critical consequences such as ruining a scientific project or jeopardizing the quality of the production in industrial production lines. One of the more subtle kind of anomalies are uncalibrations. An uncalibration is said to take place when the sensor is not adjusted or standardized by calibration according to a ground truth value. In this work, an online machine-learning based uncalibration detector for temperature, humidity and pressure sensors was developed. This solution integrates an Artificial Neural Network as main component which learns from the behavior of the sensors under calibrated conditions. Then, after trained and deployed, it detects uncalibrations once they take place. The obtained results show that the proposed solution is able to detect uncalibrations for deviation values of 0.25 degrees, 1% RH and 1.5 Pa, respectively. This solution can be adapted to different contexts by means of transfer learning, whose application allows for the addition of new sensors, the deployment into new environments and the retraining of the model with minimum amounts of data.

</p>
</details>

<details><summary><b>Individual dynamic prediction of clinical endpoint from large dimensional longitudinal biomarker history: a landmark approach</b>
<a href="https://arxiv.org/abs/2102.01466">arxiv:2102.01466</a>
&#x1F4C8; 18 <br>
<p>Anthony Devaux, Robin Genuer, Karine Pérès, Cécile Proust-Lima</p></summary>
<p>

**Abstract:** The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history includes much more repeated markers possibly. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers. We combined a landmark approach extended to endogenous markers history with machine learning methods adapted to survival data. Each marker trajectory is modeled using the information collected up to landmark time, and summary variables that best capture the individual trajectories are derived. These summaries and additional covariates are then included in different prediction methods. To handle a possibly large dimensional history, we rely on machine learning methods adapted to survival data, namely regularized regressions and random survival forests, to predict the event from the landmark time, and we show how they can be combined into a superlearner. Then, the performances are evaluated by cross-validation using estimators of Brier Score and the area under the Receiver Operating Characteristic curve adapted to censored data. We demonstrate in a simulation study the benefits of machine learning survival methods over standard survival models, especially in the case of numerous and/or nonlinear relationships between the predictors and the event. We then applied the methodology in two prediction contexts: a clinical context with the prediction of death for patients with primary biliary cholangitis, and a public health context with the prediction of death in the general elderly population at different ages. Our methodology, implemented in R, enables the prediction of an event using the entire longitudinal patient history, even when the number of repeated markers is large. Although introduced with mixed models for the repeated markers and methods for a single right censored time-to-event, our method can be used with any other appropriate modeling technique for the markers and can be easily extended to competing risks setting.

</p>
</details>

<details><summary><b>Prediction of low-keV monochromatic images from polyenergetic CT scans for improved automatic detection of pulmonary embolism</b>
<a href="https://arxiv.org/abs/2102.01445">arxiv:2102.01445</a>
&#x1F4C8; 18 <br>
<p>Constantin Seibold, Matthias A. Fink, Charlotte Goos, Hans-Ulrich Kauczor, Heinz-Peter Schlemmer, Rainer Stiefelhagen, Jens Kleesiek</p></summary>
<p>

**Abstract:** Detector-based spectral computed tomography is a recent dual-energy CT (DECT) technology that offers the possibility of obtaining spectral information. From this spectral data, different types of images can be derived, amongst others virtual monoenergetic (monoE) images. MonoE images potentially exhibit decreased artifacts, improve contrast, and overall contain lower noise values, making them ideal candidates for better delineation and thus improved diagnostic accuracy of vascular abnormalities.
  In this paper, we are training convolutional neural networks~(CNN) that can emulate the generation of monoE images from conventional single energy CT acquisitions. For this task, we investigate several commonly used image-translation methods. We demonstrate that these methods while creating visually similar outputs, lead to a poorer performance when used for automatic classification of pulmonary embolism (PE). We expand on these methods through the use of a multi-task optimization approach, under which the networks achieve improved classification as well as generation results, as reflected by PSNR and SSIM scores. Further, evaluating our proposed framework on a subset of the RSNA-PE challenge data set shows that we are able to improve the Area under the Receiver Operating Characteristic curve (AuROC) in comparison to a naïve classification approach from 0.8142 to 0.8420.

</p>
</details>

<details><summary><b>Predicting Propensity to Vote with Machine Learning</b>
<a href="https://arxiv.org/abs/2102.01535">arxiv:2102.01535</a>
&#x1F4C8; 17 <br>
<p>Rebecca D. Pollard, Sara M. Pollard, Scott Streit</p></summary>
<p>

**Abstract:** We demonstrate that machine learning enables the capability to infer an individual's propensity to vote from their past actions and attributes. This is useful for microtargeting voter outreach, voter education and get-out-the-vote (GOVT) campaigns. Political scientists developed increasingly sophisticated techniques for estimating election outcomes since the late 1940s. Two prior studies similarly used machine learning to predict individual future voting behavior. We built a machine learning environment using TensorFlow, obtained voting data from 2004 to 2018, and then ran three experiments. We show positive results with a Matthews correlation coefficient of 0.39.

</p>
</details>

<details><summary><b>Capacity and quantum geometry of parametrized quantum circuits</b>
<a href="https://arxiv.org/abs/2102.01659">arxiv:2102.01659</a>
&#x1F4C8; 16 <br>
<p>Tobias Haug, Kishor Bharti, M. S. Kim</p></summary>
<p>

**Abstract:** To harness the potential of noisy intermediate-scale quantum devices, it is paramount to find the best type of circuits to run hybrid quantum-classical algorithms. Key candidates are parametrized quantum circuits that can be effectively implemented on current devices. Here, we evaluate the capacity and trainability of these circuits using the geometric structure of the parameter space via the effective quantum dimension, which reveals the expressive power of circuits in general as well as of particular initialization strategies. We assess the expressive power of various popular circuit types and find striking differences depending on the type of entangling gates used. Particular circuits are characterized by scaling laws in their expressiveness. We identify a transition in the quantum geometry of the parameter space, which leads to a decay of the quantum natural gradient for deep circuits. For shallow circuits, the quantum natural gradient can be orders of magnitude larger in value compared to the regular gradient; however, both of them can suffer from vanishing gradients. By tuning a fixed set of circuit parameters to randomized ones, we find a region where the circuit is expressive, but does not suffer from barren plateaus, hinting at a good way to initialize circuits. We show an algorithm that prunes redundant parameters of a circuit without affecting its effective dimension. Our results enhance the understanding of parametrized quantum circuits and can be immediately applied to improve variational quantum algorithms.

</p>
</details>

<details><summary><b>Towards Multi-agent Reinforcement Learning for Wireless Network Protocol Synthesis</b>
<a href="https://arxiv.org/abs/2102.01611">arxiv:2102.01611</a>
&#x1F4C8; 16 <br>
<p>Hrishikesh Dutta, Subir Biswas</p></summary>
<p>

**Abstract:** This paper proposes a multi-agent reinforcement learning based medium access framework for wireless networks. The access problem is formulated as a Markov Decision Process (MDP), and solved using reinforcement learning with every network node acting as a distributed learning agent. The solution components are developed step by step, starting from a single-node access scenario in which a node agent incrementally learns to control MAC layer packet loads for reining in self-collisions. The strategy is then scaled up for multi-node fully-connected scenarios by using more elaborate reward structures. It also demonstrates preliminary feasibility for more general partially connected topologies. It is shown that by learning to adjust MAC layer transmission probabilities, the protocol is not only able to attain theoretical maximum throughput at an optimal load, but unlike classical approaches, it can also retain that maximum throughput at higher loading conditions. Additionally, the mechanism is agnostic to heterogeneous loading while preserving that feature. It is also shown that access priorities of the protocol across nodes can be parametrically adjusted. Finally, it is also shown that the online learning feature of reinforcement learning is able to make the protocol adapt to time-varying loading conditions.

</p>
</details>

<details><summary><b>It's always personal: Using Early Exits for Efficient On-Device CNN Personalisation</b>
<a href="https://arxiv.org/abs/2102.01393">arxiv:2102.01393</a>
&#x1F4C8; 16 <br>
<p>Ilias Leontiadis, Stefanos Laskaridis, Stylianos I. Venieris, Nicholas D. Lane</p></summary>
<p>

**Abstract:** On-device machine learning is becoming a reality thanks to the availability of powerful hardware and model compression techniques. Typically, these models are pretrained on large GPU clusters and have enough parameters to generalise across a wide variety of inputs. In this work, we observe that a much smaller, personalised model can be employed to fit a specific scenario, resulting in both higher accuracy and faster execution. Nevertheless, on-device training is extremely challenging, imposing excessive computational and memory requirements even for flagship smartphones. At the same time, on-device data availability might be limited and samples are most frequently unlabelled. To this end, we introduce PersEPhonEE, a framework that attaches early exits on the model and personalises them on-device. These allow the model to progressively bypass a larger part of the computation as more personalised data become available. Moreover, we introduce an efficient on-device algorithm that trains the early exits in a semi-supervised manner at a fraction of the whole network's personalisation time. Results show that PersEPhonEE boosts accuracy by up to 15.9% while dropping the training cost by up to 2.2x and inference latency by 2.2-3.2x on average for the same accuracy, depending on the availability of labels on-device.

</p>
</details>

<details><summary><b>Directive Explanations for Actionable Explainability in Machine Learning Applications</b>
<a href="https://arxiv.org/abs/2102.02671">arxiv:2102.02671</a>
&#x1F4C8; 15 <br>
<p>Ronal Singh, Paul Dourish, Piers Howe, Tim Miller, Liz Sonenberg, Eduardo Velloso, Frank Vetere</p></summary>
<p>

**Abstract:** This paper investigates the prospects of using directive explanations to assist people in achieving recourse of machine learning decisions. Directive explanations list which specific actions an individual needs to take to achieve their desired outcome. If a machine learning model makes a decision that is detrimental to an individual (e.g. denying a loan application), then it needs to both explain why it made that decision and also explain how the individual could obtain their desired outcome (if possible). At present, this is often done using counterfactual explanations, but such explanations generally do not tell individuals how to act. We assert that counterfactual explanations can be improved by explicitly providing people with actions they could use to achieve their desired goal. This paper makes two contributions. First, we present the results of an online study investigating people's perception of directive explanations. Second, we propose a conceptual model to generate such explanations. Our online study showed a significant preference for directive explanations ($p<0.001$). However, the participants' preferred explanation type was affected by multiple factors, such as individual preferences, social factors, and the feasibility of the directives. Our findings highlight the need for a human-centred and context-specific approach for creating directive explanations.

</p>
</details>

<details><summary><b>Online Learning with Simple Predictors and a Combinatorial Characterization of Minimax in 0/1 Games</b>
<a href="https://arxiv.org/abs/2102.01646">arxiv:2102.01646</a>
&#x1F4C8; 15 <br>
<p>Steve Hanneke, Roi Livni, Shay Moran</p></summary>
<p>

**Abstract:** Which classes can be learned properly in the online model? -- that is, by an algorithm that at each round uses a predictor from the concept class. While there are simple and natural cases where improper learning is necessary, it is natural to ask how complex must the improper predictors be in such cases. Can one always achieve nearly optimal mistake/regret bounds using "simple" predictors?
  In this work, we give a complete characterization of when this is possible, thus settling an open problem which has been studied since the pioneering works of Angluin (1987) and Littlestone (1988). More precisely, given any concept class C and any hypothesis class H, we provide nearly tight bounds (up to a log factor) on the optimal mistake bounds for online learning C using predictors from H. Our bound yields an exponential improvement over the previously best known bound by Chase and Freitag (2020).
  As applications, we give constructive proofs showing that (i) in the realizable setting, a near-optimal mistake bound (up to a constant factor) can be attained by a sparse majority-vote of proper predictors, and (ii) in the agnostic setting, a near-optimal regret bound (up to a log factor) can be attained by a randomized proper algorithm.
  A technical ingredient of our proof which may be of independent interest is a generalization of the celebrated Minimax Theorem (von Neumann, 1928) for binary zero-sum games. A simple game which fails to satisfy Minimax is "Guess the Larger Number", where each player picks a number and the larger number wins. The payoff matrix is infinite triangular. We show this is the only obstruction: if a game does not contain triangular submatrices of unbounded sizes then the Minimax Theorem holds. This generalizes von Neumann's Minimax Theorem by removing requirements of finiteness (or compactness), and captures precisely the games of interest in online learning.

</p>
</details>

<details><summary><b>A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2102.01792">arxiv:2102.01792</a>
&#x1F4C8; 12 <br>
<p>Atefeh Shahroudnejad</p></summary>
<p>

**Abstract:** Recent advancements in machine learning and signal processing domains have resulted in an extensive surge of interest in Deep Neural Networks (DNNs) due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in control systems and medical applications), it is of paramount importance to understand, trust, and in one word "explain" the argument behind deep models' decisions. In many applications, artificial neural networks (including DNNs) are considered as black-box systems, which do not provide sufficient clue on their internal processing actions. Although some recent efforts have been initiated to explain the behaviors and decisions of deep networks, explainable artificial intelligence (XAI) domain, which aims at reasoning about the behavior and decisions of DNNs, is still in its infancy. The aim of this paper is to provide a comprehensive overview on Understanding, Visualization, and Explanation of the internal and overall behavior of DNNs.

</p>
</details>

<details><summary><b>Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym</b>
<a href="https://arxiv.org/abs/2102.02639">arxiv:2102.02639</a>
&#x1F4C8; 10 <br>
<p>Matthew E. Taylor, Nicholas Nissen, Yuan Wang, Neda Navidi</p></summary>
<p>

**Abstract:** Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn.
  OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.

</p>
</details>

<details><summary><b>Towards Robust Neural Networks via Close-loop Control</b>
<a href="https://arxiv.org/abs/2102.01862">arxiv:2102.01862</a>
&#x1F4C8; 10 <br>
<p>Zhuotong Chen, Qianxiao Li, Zheng Zhang</p></summary>
<p>

**Abstract:** Despite their success in massive engineering applications, deep neural networks are vulnerable to various perturbations due to their black-box nature. Recent study has shown that a deep neural network can misclassify the data even if the input data is perturbed by an imperceptible amount. In this paper, we address the robustness issue of neural networks by a novel close-loop control method from the perspective of dynamic systems. Instead of modifying the parameters in a fixed neural network architecture, a close-loop control process is added to generate control signals adaptively for the perturbed or corrupted data. We connect the robustness of neural networks with optimal control using the geometrical information of underlying data to design the control objective. The detailed analysis shows how the embedding manifolds of state trajectory affect error estimation of the proposed method. Our approach can simultaneously maintain the performance on clean data and improve the robustness against many types of data perturbations. It can also further improve the performance of robustly trained neural networks against different perturbations. To the best of our knowledge, this is the first work that improves the robustness of neural networks with close-loop control.

</p>
</details>

<details><summary><b>Unassisted Noise Reduction of Chemical Reaction Data Sets</b>
<a href="https://arxiv.org/abs/2102.01399">arxiv:2102.01399</a>
&#x1F4C8; 10 <br>
<p>Alessandra Toniato, Philippe Schwaller, Antonio Cardinale, Joppe Geluykens, Teodoro Laino</p></summary>
<p>

**Abstract:** Existing deep learning models applied to reaction prediction in organic chemistry can reach high levels of accuracy (> 90% for Natural Language Processing-based ones). With no chemical knowledge embedded than the information learnt from reaction data, the quality of the data sets plays a crucial role in the performance of the prediction models. While human curation is prohibitively expensive, the need for unaided approaches to remove chemically incorrect entries from existing data sets is essential to improve artificial intelligence models' performance in synthetic chemistry tasks. Here we propose a machine learning-based, unassisted approach to remove chemically wrong entries from chemical reaction collections. We applied this method to the collection of chemical reactions Pistachio and to an open data set, both extracted from USPTO (United States Patent Office) patents. Our results show an improved prediction quality for models trained on the cleaned and balanced data sets. For the retrosynthetic models, the round-trip accuracy metric grows by 13 percentage points and the value of the cumulative Jensen Shannon divergence decreases by 30% compared to its original record. The coverage remains high with 97%, and the value of the class-diversity is not affected by the cleaning. The proposed strategy is the first unassisted rule-free technique to address automatic noise reduction in chemical data sets.

</p>
</details>

<details><summary><b>Mining Feature Relationships in Data</b>
<a href="https://arxiv.org/abs/2102.01355">arxiv:2102.01355</a>
&#x1F4C8; 10 <br>
<p>Andrew Lensen</p></summary>
<p>

**Abstract:** When faced with a new dataset, most practitioners begin by performing exploratory data analysis to discover interesting patterns and characteristics within data. Techniques such as association rule mining are commonly applied to uncover relationships between features (attributes) of the data. However, association rules are primarily designed for use on binary or categorical data, due to their use of rule-based machine learning. A large proportion of real-world data is continuous in nature, and discretisation of such data leads to inaccurate and less informative association rules. In this paper, we propose an alternative approach called feature relationship mining (FRM), which uses a genetic programming approach to automatically discover symbolic relationships between continuous or categorical features in data. To the best of our knowledge, our proposed approach is the first such symbolic approach with the goal of explicitly discovering relationships between features. Empirical testing on a variety of real-world datasets shows the proposed method is able to find high-quality, simple feature relationships which can be easily interpreted and which provide clear and non-trivial insight into data.

</p>
</details>

<details><summary><b>Causal Collaborative Filtering</b>
<a href="https://arxiv.org/abs/2102.01868">arxiv:2102.01868</a>
&#x1F4C8; 9 <br>
<p>Shuyuan Xu, Yingqiang Ge, Yunqi Li, Zuohui Fu, Xu Chen, Yongfeng Zhang</p></summary>
<p>

**Abstract:** Recommender systems are important and valuable tools for many personalized services. Collaborative Filtering (CF) algorithms -- among others -- are fundamental algorithms driving the underlying mechanism of personalized recommendation. Many of the traditional CF algorithms are designed based on the fundamental idea of mining or learning correlative patterns from data for matching, including memory-based methods such as user/item-based CF as well as learning-based methods such as matrix factorization and deep learning models. However, advancing from correlative learning to causal learning is an important problem, because causal/counterfactual modeling can help us to think outside of the observational data for user modeling and personalization. In this paper, we propose Causal Collaborative Filtering (CCF) -- a general framework for modeling causality in collaborative filtering and recommendation. We first provide a unified causal view of CF and mathematically show that many of the traditional CF algorithms are actually special cases of CCF under simplified causal graphs. We then propose a conditional intervention approach for $do$-calculus so that we can estimate the causal relations based on observational data. Finally, we further propose a general counterfactual constrained learning framework for estimating the user-item preferences. Experiments are conducted on two types of real-world datasets -- traditional and randomized trial data -- and results show that our framework can improve the recommendation performance of many CF algorithms.

</p>
</details>

<details><summary><b>Automatic analysis of artistic paintings using information-based measures</b>
<a href="https://arxiv.org/abs/2102.01767">arxiv:2102.01767</a>
&#x1F4C8; 9 <br>
<p>Jorge Miguel Silva, Diogo Pratas, Rui Antunes, Sérgio Matos, Armando J. Pinho</p></summary>
<p>

**Abstract:** The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists' style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website (http://panther.web.ua.pt) for fast author characterization and authentication.

</p>
</details>

<details><summary><b>Truly Sparse Neural Networks at Scale</b>
<a href="https://arxiv.org/abs/2102.01732">arxiv:2102.01732</a>
&#x1F4C8; 9 <br>
<p>Selima Curci, Decebal Constantin Mocanu, Mykola Pechenizkiyi</p></summary>
<p>

**Abstract:** Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era.

</p>
</details>

<details><summary><b>Exact Langevin Dynamics with Stochastic Gradients</b>
<a href="https://arxiv.org/abs/2102.01691">arxiv:2102.01691</a>
&#x1F4C8; 9 <br>
<p>Adrià Garriga-Alonso, Vincent Fortuin</p></summary>
<p>

**Abstract:** Stochastic gradient Markov Chain Monte Carlo algorithms are popular samplers for approximate inference, but they are generally biased. We show that many recent versions of these methods (e.g. Chen et al. (2014)) cannot be corrected using Metropolis-Hastings rejection sampling, because their acceptance probability is always zero. We can fix this by employing a sampler with realizable backwards trajectories, such as Gradient-Guided Monte Carlo (Horowitz, 1991), which generalizes stochastic gradient Langevin dynamics (Welling and Teh, 2011) and Hamiltonian Monte Carlo. We show that this sampler can be used with stochastic gradients, yielding nonzero acceptance probabilities, which can be computed even across multiple steps.

</p>
</details>

<details><summary><b>U-LanD: Uncertainty-Driven Video Landmark Detection</b>
<a href="https://arxiv.org/abs/2102.01586">arxiv:2102.01586</a>
&#x1F4C8; 9 <br>
<p>Mohammad H. Jafari, Christina Luong, Michael Tsang, Ang Nan Gu, Nathan Van Woudenberg, Robert Rohling, Teresa Tsang, Purang Abolmaesumi</p></summary>
<p>

**Abstract:** This paper presents U-LanD, a framework for joint detection of key frames and landmarks in videos. We tackle a specifically challenging problem, where training labels are noisy and highly sparse. U-LanD builds upon a pivotal observation: a deep Bayesian landmark detector solely trained on key video frames, has significantly lower predictive uncertainty on those frames vs. other frames in videos. We use this observation as an unsupervised signal to automatically recognize key frames on which we detect landmarks. As a test-bed for our framework, we use ultrasound imaging videos of the heart, where sparse and noisy clinical labels are only available for a single frame in each video. Using data from 4,493 patients, we demonstrate that U-LanD can exceedingly outperform the state-of-the-art non-Bayesian counterpart by a noticeable absolute margin of 42% in R2 score, with almost no overhead imposed on the model size. Our approach is generic and can be potentially applied to other challenging data with noisy and sparse training labels.

</p>
</details>

<details><summary><b>Approximately Solving Mean Field Games via Entropy-Regularized Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.01585">arxiv:2102.01585</a>
&#x1F4C8; 9 <br>
<p>Kai Cui, Heinz Koeppl</p></summary>
<p>

**Abstract:** The recent mean field game (MFG) formalism facilitates otherwise intractable computation of approximate Nash equilibria in many-agent settings. In this paper, we consider discrete-time finite MFGs subject to finite-horizon objectives. We show that all discrete-time finite MFGs with non-constant fixed point operators fail to be contractive as typically assumed in existing MFG literature, barring convergence via fixed point iteration. Instead, we incorporate entropy-regularization and Boltzmann policies into the fixed point iteration. As a result, we obtain provable convergence to approximate fixed points where existing methods fail, and reach the original goal of approximate Nash equilibria. All proposed methods are evaluated with respect to their exploitability, on both instructive examples with tractable exact solutions and high-dimensional problems where exact methods become intractable. In high-dimensional scenarios, we apply established deep reinforcement learning methods and empirically combine fictitious play with our approximations.

</p>
</details>

<details><summary><b>Gaussian Experts Selection using Graphical Models</b>
<a href="https://arxiv.org/abs/2102.01496">arxiv:2102.01496</a>
&#x1F4C8; 9 <br>
<p>Hamed Jalali, Martin Pawelczyk, Gjergji Kasneci</p></summary>
<p>

**Abstract:** Local approximations are popular methods to scale Gaussian processes (GPs) to big data. Local approximations reduce time complexity by dividing the original dataset into subsets and training a local expert on each subset. Aggregating the experts' prediction is done assuming either conditional dependence or independence between the experts. Imposing the \emph{conditional independence assumption} (CI) between the experts renders the aggregation of different expert predictions time efficient at the cost of poor uncertainty quantification. On the other hand, modeling dependent experts can provide precise predictions and uncertainty quantification at the expense of impractically high computational costs. By eliminating weak experts via a theory-guided expert selection step, we substantially reduce the computational cost of aggregating dependent experts while ensuring calibrated uncertainty quantification. We leverage techniques from the literature on undirected graphical models, using sparse precision matrices that encode conditional dependencies between experts to select the most important experts. Moreov

</p>
</details>

<details><summary><b>Graph Classification Based on Skeleton and Component Features</b>
<a href="https://arxiv.org/abs/2102.01428">arxiv:2102.01428</a>
&#x1F4C8; 9 <br>
<p>Xue Liu, Wei Wei, Xiangnan Feng, Xiaobo Cao, Dan Sun</p></summary>
<p>

**Abstract:** Most existing popular methods for learning graph embedding only consider fixed-order global structural features and lack structures hierarchical representation. To address this weakness, we propose a novel graph embedding algorithm named GraphCSC that realizes classification based on skeleton information using fixed-order structures learned in anonymous random walks manner, and component information using different size subgraphs. Two graphs are similar if their skeletons and components are both similar, thus in our model, we integrate both of them together into embeddings as graph homogeneity characterization. We demonstrate our model on different datasets in comparison with a comprehensive list of up-to-date state-of-the-art baselines, and experiments show that our work is superior in real-world graph classification tasks.

</p>
</details>

<details><summary><b>Graph Coarsening with Neural Networks</b>
<a href="https://arxiv.org/abs/2102.01350">arxiv:2102.01350</a>
&#x1F4C8; 9 <br>
<p>Chen Cai, Dingkang Wang, Yusu Wang</p></summary>
<p>

**Abstract:** As large-scale graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven methods in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be sub-optimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size ($25\times$ of training graphs), is adaptive to different losses (differentiable and non-differentiable), and scales to much larger graphs than previous work.

</p>
</details>

<details><summary><b>Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation</b>
<a href="https://arxiv.org/abs/2102.01813">arxiv:2102.01813</a>
&#x1F4C8; 8 <br>
<p>Mingke Xu, Fan Zhang, Xiaodong Cui, Wei Zhang</p></summary>
<p>

**Abstract:** In Speech Emotion Recognition (SER), emotional characteristics often appear in diverse forms of energy patterns in spectrograms. Typical attention neural network classifiers of SER are usually optimized on a fixed attention granularity. In this paper, we apply multiscale area attention in a deep convolutional neural network to attend emotional characteristics with varied granularities and therefore the classifier can benefit from an ensemble of attentions with different scales. To deal with data sparsity, we conduct data augmentation with vocal tract length perturbation (VTLP) to improve the generalization capability of the classifier. Experiments are carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved 79.34% weighted accuracy (WA) and 77.54% unweighted accuracy (UA), which, to the best of our knowledge, is the state of the art on this dataset.

</p>
</details>

<details><summary><b>Building population models for large-scale neural recordings: opportunities and pitfalls</b>
<a href="https://arxiv.org/abs/2102.01807">arxiv:2102.01807</a>
&#x1F4C8; 8 <br>
<p>Cole Hurwitz, Nina Kudryashova, Arno Onken, Matthias H. Hennig</p></summary>
<p>

**Abstract:** Modern recording technologies now enable simultaneous recording from large numbers of neurons. This has driven the development of new statistical models for analyzing and interpreting neural population activity. Here we provide a broad overview of recent developments in this area. We compare and contrast different approaches, highlight strengths and limitations, and discuss biological and mechanistic insights that these methods provide.

</p>
</details>

<details><summary><b>A Novel Transfer Learning-Based Approach for Screening Pre-existing Heart Diseases Using Synchronized ECG Signals and Heart Sounds</b>
<a href="https://arxiv.org/abs/2102.01728">arxiv:2102.01728</a>
&#x1F4C8; 8 <br>
<p>Ramith Hettiarachchi, Udith Haputhanthri, Kithmini Herath, Hasindu Kariyawasam, Shehan Munasinghe, Kithmin Wickramasinghe, Duminda Samarasinghe, Anjula De Silva, Chamira U. S. Edussooriya</p></summary>
<p>

**Abstract:** Diagnosing pre-existing heart diseases early in life is important as it helps prevent complications such as pulmonary hypertension, heart rhythm problems, blood clots, heart failure and sudden cardiac arrest. To identify such diseases, phonocardiogram (PCG) and electrocardiogram (ECG) waveforms convey important information. Therefore, effectively using these two modalities of data has the potential to improve the disease screening process. We evaluate this hypothesis on a subset of the PhysioNet Challenge 2016 Dataset which contains simultaneously acquired PCG and ECG recordings. Our novel Dual-Convolutional Neural Network based approach uses transfer learning to tackle the problem of having limited amounts of simultaneous PCG and ECG data that is publicly available, while having the potential to adapt to larger datasets. In addition, we introduce two main evaluation frameworks named record-wise and sample-wise evaluation which leads to a rich performance evaluation for the transfer learning approach. Comparisons with methods which used single or dual modality data show that our method can lead to better performance. Furthermore, our results show that individually collected ECG or PCG waveforms are able to provide transferable features which could effectively help to make use of a limited number of synchronized PCG and ECG waveforms and still achieve significant classification performance.

</p>
</details>

<details><summary><b>Clustering with Penalty for Joint Occurrence of Objects: Computational Aspects</b>
<a href="https://arxiv.org/abs/2102.01424">arxiv:2102.01424</a>
&#x1F4C8; 8 <br>
<p>Ondřej Sokol, Vladimír Holý</p></summary>
<p>

**Abstract:** The method of Holý, Sokol and Černý (Applied Soft Computing, 2017, Vol. 60, p. 752-762) clusters objects based on their incidence in a large number of given sets. The idea is to minimize the occurrence of multiple objects from the same cluster in the same set. In the current paper, we study computational aspects of the method. First, we prove that the problem of finding the optimal clustering is NP-hard. Second, to numerically find a suitable clustering, we propose to use the genetic algorithm augmented by a renumbering procedure, a fast task-specific local search heuristic and an initial solution based on a simplified model. Third, in a simulation study, we demonstrate that our improvements of the standard genetic algorithm significantly enhance its computational performance.

</p>
</details>

<details><summary><b>aura-net : robust segmentation of phase-contrast microscopy images with few annotations</b>
<a href="https://arxiv.org/abs/2102.01389">arxiv:2102.01389</a>
&#x1F4C8; 8 <br>
<p>Ethan Cohen, Virginie Uhlmann</p></summary>
<p>

**Abstract:** We present AURA-net, a convolutional neural network (CNN) for the segmentation of phase-contrast microscopy images. AURA-net uses transfer learning to accelerate training and Attention mechanisms to help the network focus on relevant image features. In this way, it can be trained efficiently with a very limited amount of annotations. Our network can thus be used to automate the segmentation of datasets that are generally considered too small for deep learning techniques. AURA-net also uses a loss inspired by active contours that is well-adapted to the specificity of phase-contrast images, further improving performance. We show that AURA-net outperforms state-of-the-art alternatives in several small (less than 100images) datasets.

</p>
</details>

<details><summary><b>Fast Exploration of Weight Sharing Opportunities for CNN Compression</b>
<a href="https://arxiv.org/abs/2102.01345">arxiv:2102.01345</a>
&#x1F4C8; 8 <br>
<p>Etienne Dupuis, David Novo, Ian O'Connor, Alberto Bosio</p></summary>
<p>

**Abstract:** The computational workload involved in Convolutional Neural Networks (CNNs) is typically out of reach for low-power embedded devices. There are a large number of approximation techniques to address this problem. These methods have hyper-parameters that need to be optimized for each CNNs using design space exploration (DSE). The goal of this work is to demonstrate that the DSE phase time can easily explode for state of the art CNN. We thus propose the use of an optimized exploration process to drastically reduce the exploration time without sacrificing the quality of the output.

</p>
</details>

<details><summary><b>TAD: Trigger Approximation based Black-box Trojan Detection for AI</b>
<a href="https://arxiv.org/abs/2102.01815">arxiv:2102.01815</a>
&#x1F4C8; 7 <br>
<p>Xinqiao Zhang, Huili Chen, Farinaz Koushanfar</p></summary>
<p>

**Abstract:** An emerging amount of intelligent applications have been developed with the surge of Machine Learning (ML). Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by the stealthy trigger. We call this vulnerable model adversarial artificial intelligence (AI). In this paper, we target to design a robust Trojan detection scheme that inspects whether a pre-trained AI model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose TAD, the first trigger approximation based Trojan detection framework that enables fast and scalable search of the trigger in the input space. Furthermore, TAD can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of the TAD across various datasets and ML models. Empirical results show that TAD achieves a ROC-AUC score of 0:91 on the public TrojAI dataset 1 and the average detection time per model is 7:1 minutes.

</p>
</details>

<details><summary><b>Recent Advances in Adversarial Training for Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2102.01356">arxiv:2102.01356</a>
&#x1F4C8; 7 <br>
<p>Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, Qian Wang</p></summary>
<p>

**Abstract:** Adversarial training is one of the most effective approaches defending against adversarial examples for deep learning models. Unlike other defense strategies, adversarial training aims to promote the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives. Finally, we highlight the challenges which are not fully tackled and present potential future directions.

</p>
</details>

<details><summary><b>Enabling energy efficient machine learning on a Ultra-Low-Power vision sensor for IoT</b>
<a href="https://arxiv.org/abs/2102.01340">arxiv:2102.01340</a>
&#x1F4C8; 7 <br>
<p>Francesco Paissan, Massimo Gottardi, Elisabetta Farella</p></summary>
<p>

**Abstract:** The Internet of Things (IoT) and smart city paradigm includes ubiquitous technology to extract context information in order to return useful services to users and citizens. An essential role in this scenario is often played by computer vision applications, requiring the acquisition of images from specific devices. The need for high-end cameras often penalizes this process since they are power-hungry and ask for high computational resources to be processed. Thus, the availability of novel low-power vision sensors, implementing advanced features like in-hardware motion detection, is crucial for computer vision in the IoT domain. Unfortunately, to be highly energy-efficient, these sensors might worsen the perception performance (e.g., resolution, frame rate, color). Therefore, domain-specific pipelines are usually delivered in order to exploit the full potential of these cameras. This paper presents the development, analysis, and embedded implementation of a realtime detection, classification and tracking pipeline able to exploit the full potential of background filtering Smart Vision Sensors (SVS). The power consumption obtained for the inference - which requires 8ms - is 7.5 mW.

</p>
</details>

<details><summary><b>Anomaly Detection of Time Series with Smoothness-Inducing Sequential Variational Auto-Encoder</b>
<a href="https://arxiv.org/abs/2102.01331">arxiv:2102.01331</a>
&#x1F4C8; 7 <br>
<p>Longyuan Li, Junchi Yan, Haiyang Wang, Yaohui Jin</p></summary>
<p>

**Abstract:** Deep generative models have demonstrated their effectiveness in learning latent representation and modeling complex dependencies of time series. In this paper, we present a Smoothness-Inducing Sequential Variational Auto-Encoder (SISVAE) model for robust estimation and anomaly detection of multi-dimensional time series. Our model is based on Variational Auto-Encoder (VAE), and its backbone is fulfilled by a Recurrent Neural Network to capture latent temporal structures of time series for both generative model and inference model. Specifically, our model parameterizes mean and variance for each time-stamp with flexible neural networks, resulting in a non-stationary model that can work without the assumption of constant noise as commonly made by existing Markov models. However, such a flexibility may cause the model fragile to anomalies. To achieve robust density estimation which can also benefit detection tasks, we propose a smoothness-inducing prior over possible estimations. The proposed prior works as a regularizer that places penalty at non-smooth reconstructions. Our model is learned efficiently with a novel stochastic gradient variational Bayes estimator. In particular, we study two decision criteria for anomaly detection: reconstruction probability and reconstruction error. We show the effectiveness of our model on both synthetic datasets and public real-world benchmarks.

</p>
</details>

<details><summary><b>Exploiting Raw Images for Real-Scene Super-Resolution</b>
<a href="https://arxiv.org/abs/2102.01579">arxiv:2102.01579</a>
&#x1F4C8; 6 <br>
<p>Xiangyu Xu, Yongrui Ma, Wenxiu Sun, Ming-Hsuan Yang</p></summary>
<p>

**Abstract:** Super-resolution is a fundamental problem in computer vision which aims to overcome the spatial limitation of camera sensors. While significant progress has been made in single image super-resolution, most algorithms only perform well on synthetic data, which limits their applications in real scenarios. In this paper, we study the problem of real-scene single image super-resolution to bridge the gap between synthetic data and real captured images. We focus on two issues of existing super-resolution algorithms: lack of realistic training data and insufficient utilization of visual information obtained from cameras. To address the first issue, we propose a method to generate more realistic training data by mimicking the imaging process of digital cameras. For the second issue, we develop a two-branch convolutional neural network to exploit the radiance information originally-recorded in raw images. In addition, we propose a dense channel-attention block for better image restoration as well as a learning-based guided filter network for effective color correction. Our model is able to generalize to different cameras without deliberately training on images from specific camera types. Extensive experiments demonstrate that the proposed algorithm can recover fine details and clear structures, and achieve high-quality results for single image super-resolution in real scenes.

</p>
</details>

<details><summary><b>Metrics and continuity in reinforcement learning</b>
<a href="https://arxiv.org/abs/2102.01514">arxiv:2102.01514</a>
&#x1F4C8; 6 <br>
<p>Charline Le Lan, Marc G. Bellemare, Pablo Samuel Castro</p></summary>
<p>

**Abstract:** In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of "well-behaved" neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered.

</p>
</details>

<details><summary><b>Bayesian Neural Networks for Virtual Flow Metering: An Empirical Study</b>
<a href="https://arxiv.org/abs/2102.01391">arxiv:2102.01391</a>
&#x1F4C8; 6 <br>
<p>Bjarne Grimstad, Mathilde Hotvedt, Anders T. Sandnes, Odd Kolbjørnsen, Lars S. Imsland</p></summary>
<p>

**Abstract:** Recent works have presented promising results from the application of machine learning (ML) to the modeling of flow rates in oil and gas wells. Encouraging results and advantageous properties of ML models, such as computationally cheap evaluation and ease of calibration to new data, have sparked optimism for the development of data-driven virtual flow meters (VFMs). Data-driven VFMs are developed in the small data regime, where it is important to question the uncertainty and robustness of models. The modeling of uncertainty may help to build trust in models, which is a prerequisite for industrial applications. The contribution of this paper is the introduction of a probabilistic VFM based on Bayesian neural networks. Uncertainty in the model and measurements is described, and the paper shows how to perform approximate Bayesian inference using variational inference. The method is studied by modeling on a large and heterogeneous dataset, consisting of 60 wells across five different oil and gas assets. The predictive performance is analyzed on historical and future test data, where an average error of 4-6% and 8-13% is achieved for the 50% best performing models, respectively. Variational inference appears to provide more robust predictions than the reference approach on future data. Prediction performance and uncertainty calibration is explored in detail and discussed in light of four data challenges. The findings motivate the development of alternative strategies to improve the robustness of data-driven VFMs.

</p>
</details>

<details><summary><b>Benchmarking Quantized Neural Networks on FPGAs with FINN</b>
<a href="https://arxiv.org/abs/2102.01341">arxiv:2102.01341</a>
&#x1F4C8; 6 <br>
<p>Quentin Ducasse, Pascal Cotret, Loïc Lagadec, Robert Stewart</p></summary>
<p>

**Abstract:** The ever-growing cost of both training and inference for state-of-the-art neural networks has brought literature to look upon ways to cut off resources used with a minimal impact on accuracy. Using lower precision comes at the cost of negligible loss in accuracy. While training neural networks may require a powerful setup, deploying a network must be possible on low-power and low-resource hardware architectures. Reconfigurable architectures have proven to be more powerful and flexible than GPUs when looking at a specific application. This article aims to assess the impact of mixed-precision when applied to neural networks deployed on FPGAs. While several frameworks exist that create tools to deploy neural networks using reduced-precision, few of them assess the importance of quantization and the framework quality. FINN and Brevitas, two frameworks from Xilinx labs, are used to assess the impact of quantization on neural networks using 2 to 8 bit precisions and weights with several parallelization configurations. Equivalent accuracy can be obtained using lower-precision representation and enough training. However, the compressed network can be better parallelized allowing the deployed network throughput to be 62 times faster. The benchmark set up in this work is available in a public repository (https://github.com/QDucasse/nn benchmark).

</p>
</details>

<details><summary><b>When Noise meets Chaos: Stochastic Resonance in Neurochaos Learning</b>
<a href="https://arxiv.org/abs/2102.01316">arxiv:2102.01316</a>
&#x1F4C8; 6 <br>
<p>Harikrishnan NB, Nithin Nagaraj</p></summary>
<p>

**Abstract:** Chaos and Noise are ubiquitous in the Brain. Inspired by the chaotic firing of neurons and the constructive role of noise in neuronal models, we for the first time connect chaos, noise and learning. In this paper, we demonstrate Stochastic Resonance (SR) phenomenon in Neurochaos Learning (NL). SR manifests at the level of a single neuron of NL and enables efficient subthreshold signal detection. Furthermore, SR is shown to occur in single and multiple neuronal NL architecture for classification tasks - both on simulated and real-world spoken digit datasets. Intermediate levels of noise in neurochaos learning enables peak performance in classification tasks thus highlighting the role of SR in AI applications, especially in brain inspired learning architectures.

</p>
</details>

<details><summary><b>Near-Optimal Offline Reinforcement Learning via Double Variance Reduction</b>
<a href="https://arxiv.org/abs/2102.01748">arxiv:2102.01748</a>
&#x1F4C8; 5 <br>
<p>Ming Yin, Yu Bai, Yu-Xiang Wang</p></summary>
<p>

**Abstract:** We consider the problem of offline reinforcement learning (RL) -- a well-motivated setting of RL that aims at policy optimization using only historical data. Despite its wide applicability, theoretical understandings of offline RL, such as its optimal sample complexity, remain largely open even in basic settings such as \emph{tabular} Markov Decision Processes (MDPs).
  In this paper, we propose Off-Policy Double Variance Reduction (OPDVR), a new variance reduction based algorithm for offline RL. Our main result shows that OPDVR provably identifies an $ε$-optimal policy with $\widetilde{O}(H^2/d_mε^2)$ episodes of offline data in the finite-horizon stationary transition setting, where $H$ is the horizon length and $d_m$ is the minimal marginal state-action distribution induced by the behavior policy. This improves over the best known upper bound by a factor of $H$. Moreover, we establish an information-theoretic lower bound of $Ω(H^2/d_mε^2)$ which certifies that OPDVR is optimal up to logarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal sample complexity under alternative settings such as the finite-horizon MDPs with non-stationary transitions and the infinite horizon MDPs with discounted rewards.

</p>
</details>

<details><summary><b>Majorizing Measures, Sequential Complexities, and Online Learning</b>
<a href="https://arxiv.org/abs/2102.01729">arxiv:2102.01729</a>
&#x1F4C8; 5 <br>
<p>Adam Block, Yuval Dagan, Sasha Rakhlin</p></summary>
<p>

**Abstract:** We introduce the technique of generic chaining and majorizing measures for controlling sequential Rademacher complexity. We relate majorizing measures to the notion of fractional covering numbers, which we show to be dominated in terms of sequential scale-sensitive dimensions in a horizon-independent way, and, under additional complexity assumptions establish a tight control on worst-case sequential Rademacher complexity in terms of the integral of sequential scale-sensitive dimension. Finally, we establish a tight contraction inequality for worst-case sequential Rademacher complexity. The above constitutes the resolution of a number of outstanding open problems in extending the classical theory of empirical processes to the sequential case, and, in turn, establishes sharp results for online learning.

</p>
</details>

<details><summary><b>Bit Error Tolerance Metrics for Binarized Neural Networks</b>
<a href="https://arxiv.org/abs/2102.01344">arxiv:2102.01344</a>
&#x1F4C8; 5 <br>
<p>Sebastian Buschjäger, Jian-Jia Chen, Kuan-Hsun Chen, Mario Günzel, Katharina Morik, Rodion Novkin, Lukas Pfahler, Mikail Yayla</p></summary>
<p>

**Abstract:** To reduce the resource demand of neural network (NN) inference systems, it has been proposed to use approximate memory, in which the supply voltage and the timing parameters are tuned trading accuracy with energy consumption and performance. Tuning these parameters aggressively leads to bit errors, which can be tolerated by NNs when bit flips are injected during training. However, bit flip training, which is the state of the art for achieving bit error tolerance, does not scale well; it leads to massive overheads and cannot be applied for high bit error rates (BERs). Alternative methods to achieve bit error tolerance in NNs are needed, but the underlying principles behind the bit error tolerance of NNs have not been reported yet. With this lack of understanding, further progress in the research on NN bit error tolerance will be restrained.
  In this study, our objective is to investigate the internal changes in the NNs that bit flip training causes, with a focus on binarized NNs (BNNs). To this end, we quantify the properties of bit error tolerant BNNs with two metrics. First, we propose a neuron-level bit error tolerance metric, which calculates the margin between the pre-activation values and batch normalization thresholds. Secondly, to capture the effects of bit error tolerance on the interplay of neurons, we propose an inter-neuron bit error tolerance metric, which measures the importance of each neuron and computes the variance over all importance values. Our experimental results support that these two metrics are strongly related to bit error tolerance.

</p>
</details>

<details><summary><b>A step towards a reinforcement learning de novo genome assembler</b>
<a href="https://arxiv.org/abs/2102.02649">arxiv:2102.02649</a>
&#x1F4C8; 4 <br>
<p>Kleber Padovani, Roberto Xavier, Andre Carvalho, Anna Reali, Annie Chateau, Ronnie Alves</p></summary>
<p>

**Abstract:** The use of reinforcement learning has proven to be very promising for solving complex activities without human supervision during their learning process. However, their successful applications are predominantly focused on fictional and entertainment problems - such as games. Based on the above, this work aims to shed light on the application of reinforcement learning to solve this relevant real-world problem, the genome assembly. By expanding the only approach found in the literature that addresses this problem, we carefully explored the aspects of intelligent agent learning, performed by the Q-learning algorithm, to understand its suitability to be applied in scenarios whose characteristics are more similar to those faced by real genome projects. The improvements proposed here include changing the previously proposed reward system and including state space exploration optimization strategies based on dynamic pruning and mutual collaboration with evolutionary computing. These investigations were tried on 23 new environments with larger inputs than those used previously. All these environments are freely available on the internet for the evolution of this research by the scientific community. The results suggest consistent performance progress using the proposed improvements, however, they also demonstrate the limitations of them, especially related to the high dimensionality of state and action spaces. We also present, later, the paths that can be traced to tackle genome assembly efficiently in real scenarios considering recent, successfully reinforcement learning applications - including deep reinforcement learning - from other domains dealing with high-dimensional inputs.

</p>
</details>

<details><summary><b>Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation</b>
<a href="https://arxiv.org/abs/2102.01818">arxiv:2102.01818</a>
&#x1F4C8; 4 <br>
<p>Aparna Elangovan, Jiayuan He, Karin Verspoor</p></summary>
<p>

**Abstract:** Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model's ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model's ability to memorize versus generalize.

</p>
</details>

<details><summary><b>Multimodal Attention Fusion for Target Speaker Extraction</b>
<a href="https://arxiv.org/abs/2102.01326">arxiv:2102.01326</a>
&#x1F4C8; 4 <br>
<p>Hiroshi Sato, Tsubasa Ochiai, Keisuke Kinoshita, Marc Delcroix, Tomohiro Nakatani, Shoko Araki</p></summary>
<p>

**Abstract:** Target speaker extraction, which aims at extracting a target speaker's voice from a mixture of voices using audio, visual or locational clues, has received much interest. Recently an audio-visual target speaker extraction has been proposed that extracts target speech by using complementary audio and visual clues. Although audio-visual target speaker extraction offers a more stable performance than single modality methods for simulated data, its adaptation towards realistic situations has not been fully explored as well as evaluations on real recorded mixtures. One of the major issues to handle realistic situations is how to make the system robust to clue corruption because in real recordings both clues may not be equally reliable, e.g. visual clues may be affected by occlusions. In this work, we propose a novel attention mechanism for multi-modal fusion and its training methods that enable to effectively capture the reliability of the clues and weight the more reliable ones. Our proposals improve signal to distortion ratio (SDR) by 1.0 dB over conventional fusion mechanisms on simulated data. Moreover, we also record an audio-visual dataset of simultaneous speech with realistic visual clue corruption and show that audio-visual target speaker extraction with our proposals successfully work on real data.

</p>
</details>

<details><summary><b>Optimal Sequential Detection of Signals with Unknown Appearance and Disappearance Points in Time</b>
<a href="https://arxiv.org/abs/2102.01310">arxiv:2102.01310</a>
&#x1F4C8; 4 <br>
<p>Alexander G. Tartakovsky, Nikita R. Berenkov, Alexei E. Kolessa, Igor V. Nikiforov</p></summary>
<p>

**Abstract:** The paper addresses a sequential changepoint detection problem, assuming that the duration of change may be finite and unknown. This problem is of importance for many applications, e.g., for signal and image processing where signals appear and disappear at unknown points in time or space. In contrast to the conventional optimality criterion in quickest change detection that requires minimization of the expected delay to detection for a given average run length to a false alarm, we focus on a reliable maximin change detection criterion of maximizing the minimal probability of detection in a given time (or space) window for a given local maximal probability of false alarm in the prescribed window. We show that the optimal detection procedure is a modified CUSUM procedure. We then compare operating characteristics of this optimal procedure with popular in engineering the Finite Moving Average (FMA) detection algorithm and the ordinary CUSUM procedure using Monte Carlo simulations, which show that typically the later algorithms have almost the same performance as the optimal one. At the same time, the FMA procedure has a substantial advantage -- independence to the intensity of the signal, which is usually unknown. Finally, the FMA algorithm is applied to detecting faint streaks of satellites in optical images.

</p>
</details>

<details><summary><b>Transparent FPGA Acceleration with TensorFlow</b>
<a href="https://arxiv.org/abs/2102.06018">arxiv:2102.06018</a>
&#x1F4C8; 3 <br>
<p>Simon Pfenning, Philipp Holzinger, Marc Reichenbach</p></summary>
<p>

**Abstract:** Today, artificial neural networks are one of the major innovators pushing the progress of machine learning. This has particularly affected the development of neural network accelerating hardware. However, since most of these architectures require specialized toolchains, there is a certain amount of additional effort for developers each time they want to make use of a new deep learning accelerator. Furthermore the flexibility of the device is bound to the architecture itself, as well as to the functionality of the runtime environment.
  In this paper we propose a toolflow using TensorFlow as frontend, thus offering developers the opportunity of using a familiar environment. On the backend we use an FPGA, which is addressable via an HSA runtime environment. In this way we are able to hide the complexity of controlling new hardware from the user, while at the same time maintaining a high amount of flexibility. This can be achieved by our HSA toolflow, since the hardware is not statically configured with the structure of the network. Instead, it can be dynamically reconfigured during runtime with the respective kernels executed by the network and simultaneously from other sources e.g. OpenCL/OpenMP.

</p>
</details>

<details><summary><b>Provably Secure Federated Learning against Malicious Clients</b>
<a href="https://arxiv.org/abs/2102.01854">arxiv:2102.01854</a>
&#x1F4C8; 3 <br>
<p>Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong</p></summary>
<p>

**Abstract:** Federated learning enables clients to collaboratively learn a shared global model without sharing their local training data with a cloud server. However, malicious clients can corrupt the global model to predict incorrect labels for testing examples. Existing defenses against malicious clients leverage Byzantine-robust federated learning methods. However, these methods cannot provably guarantee that the predicted label for a testing example is not affected by malicious clients. We bridge this gap via ensemble federated learning. In particular, given any base federated learning algorithm, we use the algorithm to learn multiple global models, each of which is learnt using a randomly selected subset of clients. When predicting the label of a testing example, we take majority vote among the global models. We show that our ensemble federated learning with any base federated learning algorithm is provably secure against malicious clients. Specifically, the label predicted by our ensemble global model for a testing example is provably not affected by a bounded number of malicious clients. Moreover, we show that our derived bound is tight. We evaluate our method on MNIST and Human Activity Recognition datasets. For instance, our method can achieve a certified accuracy of 88% on MNIST when 20 out of 1,000 clients are malicious.

</p>
</details>

<details><summary><b>Multi-class probabilistic atlas-based whole heart segmentation method in cardiac CT and MRI</b>
<a href="https://arxiv.org/abs/2102.01822">arxiv:2102.01822</a>
&#x1F4C8; 3 <br>
<p>Tarun Kanti Ghosh, Md. Kamrul Hasan, Shidhartho Roy, Md. Ashraful Alam, Eklas Hossain, Mohiuddin Ahmad</p></summary>
<p>

**Abstract:** Accurate and robust whole heart substructure segmentation is crucial in developing clinical applications, such as computer-aided diagnosis and computer-aided surgery. However, segmentation of different heart substructures is challenging because of inadequate edge or boundary information, the complexity of the background and texture, and the diversity in different substructures' sizes and shapes. This article proposes a framework for multi-class whole heart segmentation employing non-rigid registration-based probabilistic atlas incorporating the Bayesian framework. We also propose a non-rigid registration pipeline utilizing a multi-resolution strategy for obtaining the highest attainable mutual information between the moving and fixed images. We further incorporate non-rigid registration into the expectation-maximization algorithm and implement different deep convolutional neural network-based encoder-decoder networks for ablation studies. All the extensive experiments are conducted utilizing the publicly available dataset for the whole heart segmentation containing 20 MRI and 20 CT cardiac images. The proposed approach exhibits an encouraging achievement, yielding a mean volume overlapping error of 14.5 % for CT scans exceeding the state-of-the-art results by a margin of 1.3 % in terms of the same metric. As the proposed approach provides better-results to delineate the different substructures of the heart, it can be a medical diagnostic aiding tool for helping experts with quicker and more accurate results.

</p>
</details>

<details><summary><b>Recurrent Neural Network for MoonBoard Climbing Route Classification and Generation</b>
<a href="https://arxiv.org/abs/2102.01788">arxiv:2102.01788</a>
&#x1F4C8; 3 <br>
<p>Yi-Shiou Duh, Ray Chang</p></summary>
<p>

**Abstract:** Classifying the difficulties of climbing routes and generating new routes are both challenging. Existing machine learning models not only fail to accurately predict a problem's difficulty, but they are also unable to generate reasonable problems. In this work, we introduced "BetaMove", a new move preprocessing pipeline we developed, in order to mimic a human climber's hand sequence. The preprocessed move sequences were then used to train both a route generator and a grade predictor. By preprocessing a MoonBoard problem into a proper move sequence, the accuracy of our grade predictor reaches near human-level performance, and our route generator produces new routes of much better quality compared to previous work. We demonstrated that with BetaMove, we are able to inject human insights into the machine learning problems, and this can be the foundations for future transfer learning on climbing style classification problems.

</p>
</details>

<details><summary><b>Subdimensional Expansion for Multi-objective Multi-agent Path Finding</b>
<a href="https://arxiv.org/abs/2102.01353">arxiv:2102.01353</a>
&#x1F4C8; 3 <br>
<p>Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</p></summary>
<p>

**Abstract:** Conventional multi-agent path planners typically determine a path that optimizes a single objective, such as path length. Many applications, however, may require multiple objectives, say time-to-completion and fuel use, to be simultaneously optimized in the planning process. Often, these criteria may not be readily compared and sometimes lie in competition with each other. Simply applying standard multi-objective search algorithms to multi-agent path finding may prove to be inefficient because the size of the space of possible solutions, i.e., the Pareto-optimal set, can grow exponentially with the number of agents (the dimension of the search space). This paper presents an approach that bypasses this so-called curse of dimensionality by leveraging our prior multi-agent work with a framework called subdimensional expansion. One example of subdimensional expansion, when applied to A*, is called M* and M* was limited to a single objective function. We combine principles of dominance and subdimensional expansion to create a new algorithm named multi-objective M* (MOM*), which dynamically couples agents for planning only when those agents have to "interact" with each other. MOM* computes the complete Pareto-optimal set for multiple agents efficiently and naturally trades off sub-optimal approximations of the Pareto-optimal set and computational efficiency. Our approach is able to find the complete Pareto-optimal set for problem instances with hundreds of solutions which the standard multi-objective A* algorithms could not find within a bounded time.

</p>
</details>

<details><summary><b>A Graph-Constrained Changepoint Learning Approach for Automatic QRS-Complex Detection</b>
<a href="https://arxiv.org/abs/2102.01319">arxiv:2102.01319</a>
&#x1F4C8; 3 <br>
<p>Atiyeh Fotoohinasab, Toby Hocking, Fatemeh Afghah</p></summary>
<p>

**Abstract:** This study presents a new viewpoint on ECG signal analysis by applying a graph-based changepoint detection model to locate R-peak positions. This model is based on a new graph learning algorithm to learn the constraint graph given the labeled ECG data. The proposed learning algorithm starts with a simple initial graph and iteratively edits the graph so that the final graph has the maximum accuracy in R-peak detection. We evaluate the performance of the algorithm on the MIT-BIH Arrhythmia Database. The evaluation results demonstrate that the proposed method can obtain comparable results to other state-of-the-art approaches. The proposed method achieves the overall sensitivity of Sen = 99.64%, positive predictivity of PPR = 99.71%, and detection error rate of DER = 0.19.

</p>
</details>

<details><summary><b>Fake-image detection with Robust Hashing</b>
<a href="https://arxiv.org/abs/2102.01313">arxiv:2102.01313</a>
&#x1F4C8; 3 <br>
<p>Miki Tanaka, Hitoshi Kiya</p></summary>
<p>

**Abstract:** In this paper, we investigate whether robust hashing has a possibility to robustly detect fake-images even when multiple manipulation techniques such as JPEG compression are applied to images for the first time. In an experiment, the proposed fake detection with robust hashing is demonstrated to outperform state-of-the-art one under the use of various datasets including fake images generated with GANs.

</p>
</details>

<details><summary><b>Single-Shell NODDI Using Dictionary Learner Estimated Isotropic Volume Fraction</b>
<a href="https://arxiv.org/abs/2102.02772">arxiv:2102.02772</a>
&#x1F4C8; 2 <br>
<p>Abrar Faiyaz, Marvin Doyley, Giovanni Schifitto, Jianhui Zhong, Md Nasir Uddin</p></summary>
<p>

**Abstract:** Neurite orientation dispersion and density imaging (NODDI) enables the assessment of intracellular, extracellular and free water signals from multi-shell diffusion MRI data. It is an insightful approach to characterize brain tissue microstructure. Single-shell reconstruction for NODDI parameters has been discouraged in previous studies caused by failure when fitting, especially for the neurite density index (NDI). Here, we investigated the possibility of creating robust NODDI parameter maps with single-shell data, using the isotropic volume fraction (fISO) as prior. Prior estimation was made independent of the NODDI model constraint using a dictionary learning approach. First, we used a stochastic sparse dictionary-based network (DictNet) in predicting fISO which is trained with data obtained from in vivo and simulated diffusion MRI data. In single-shell cases, the mean diffusivity (MD) and raw T2 signal with no diffusion weighting (S0) was incorporated in the dictionary for the fISO estimation. Then, the NODDI framework was used with the known fISO to estimate the NDI and orientation dispersion index (ODI). The fISO estimated by our model was compared with other fISO estimators in the simulation. Further, using both synthetic data simulation and human data collected on a 3T scanner, we compared the performance of our dictionary-based learning prior NODDI (DLpN) with the original NODDI for both single-shell and multi-shell data. Our results suggest that DLpN derived NDI and ODI parameters for single-shell protocols are comparable with original multi-shell NODDI, and protocol with b=2000 s/mm2 performs the best (error ~5% in white and grey matter). This may allow NODDI evaluation of studies on single-shell data by multi-shell scanning of two subjects for DictNet fISO training.

</p>
</details>

<details><summary><b>Big Data Analytics Applying the Fusion Approach of Multicriteria Decision Making with Deep Learning Algorithms</b>
<a href="https://arxiv.org/abs/2102.02637">arxiv:2102.02637</a>
&#x1F4C8; 2 <br>
<p>Swarajya Lakshmi V Papineni, Snigdha Yarlagadda, Harita Akkineni, A. Mallikarjuna Reddy</p></summary>
<p>

**Abstract:** Data is evolving with the rapid progress of population and communication for various types of devices such as networks, cloud computing, Internet of Things (IoT), actuators, and sensors. The increment of data and communication content goes with the equivalence of velocity, speed, size, and value to provide the useful and meaningful knowledge that helps to solve the future challenging tasks and latest issues. Besides, multicriteria based decision making is one of the key issues to solve for various issues related to the alternative effects in big data analysis. It tends to find a solution based on the latest machine learning techniques that include algorithms like decision making and deep learning mechanism based on multicriteria in providing insights to big data. On the other hand, the derivations are made for it to go with the approximations to increase the duality of runtime and improve the entire system's potentiality and efficacy. In essence, several fields, including business, agriculture, information technology, and computer science, use deep learning and multicriteria-based decision-making problems. This paper aims to provide various applications that involve the concepts of deep learning techniques and exploiting the multicriteria approaches for issues that are facing in big data analytics by proposing new studies with the fusion approaches of data-driven techniques.

</p>
</details>

<details><summary><b>Deep Autoencoder-based Fuzzy C-Means for Topic Detection</b>
<a href="https://arxiv.org/abs/2102.02636">arxiv:2102.02636</a>
&#x1F4C8; 2 <br>
<p>Hendri Murfi, Natasha Rosaline, Nora Hariadi</p></summary>
<p>

**Abstract:** Topic detection is a process for determining topics from a collection of textual data. One of the topic detection methods is a clustering-based method, which assumes that the centroids are topics. The clustering method has the advantage that it can process data with negative representations. Therefore, the clustering method allows a combination with a broader representation learning method. In this paper, we adopt deep learning for topic detection by using a deep autoencoder and fuzzy c-means called deep autoencoder-based fuzzy c-means (DFCM). The encoder of the autoencoder performs a lower-dimensional representation learning. Fuzzy c-means groups the lower-dimensional representation to identify the centroids. The autoencoder's decoder transforms back the centroids into the original representation to be interpreted as the topics. Our simulation shows that DFCM improves the coherence score of eigenspace-based fuzzy c-means (EFCM) and is comparable to the leading standard methods, i.e., nonnegative matrix factorization (NMF) or latent Dirichlet allocation (LDA).

</p>
</details>

<details><summary><b>Impact of Data Processing on Fairness in Supervised Learning</b>
<a href="https://arxiv.org/abs/2102.01867">arxiv:2102.01867</a>
&#x1F4C8; 2 <br>
<p>Sajad Khodadadian, AmirEmad Ghassami, Negar Kiyavash</p></summary>
<p>

**Abstract:** We study the impact of pre and post processing for reducing discrimination in data-driven decision makers. We first analyze the fundamental trade-off between fairness and accuracy in a pre-processing approach, and propose a design for a pre-processing module based on a convex optimization program, which can be added before the original classifier. This leads to a fundamental lower bound on attainable discrimination, given any acceptable distortion in the outcome. Furthermore, we reformulate an existing post-processing method in terms of our accuracy and fairness measures, which allows comparing post-processing and pre-processing approaches. We show that under some mild conditions, pre-processing outperforms post-processing. Finally, we show that by appropriate choice of the discrimination measure, the optimization problem for both pre and post processing approaches will reduce to a linear program and hence can be solved efficiently.

</p>
</details>

<details><summary><b>UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging with Unpaired Data</b>
<a href="https://arxiv.org/abs/2102.01850">arxiv:2102.01850</a>
&#x1F4C8; 2 <br>
<p>Ru Li, Chuan Wang, Shuaicheng Liu, Jue Wang, Guanghui Liu, Bing Zeng</p></summary>
<p>

**Abstract:** The paper proposes a method to effectively fuse multi-exposure inputs and generates high-quality high dynamic range (HDR) images with unpaired datasets. Deep learning-based HDR image generation methods rely heavily on paired datasets. The ground truth provides information for the network getting HDR images without ghosting. Datasets without ground truth are hard to apply to train deep neural networks. Recently, Generative Adversarial Networks (GAN) have demonstrated their potentials of translating images from source domain X to target domain Y in the absence of paired examples. In this paper, we propose a GAN-based network for solving such problems while generating enjoyable HDR results, named UPHDR-GAN. The proposed method relaxes the constraint of paired dataset and learns the mapping from LDR domain to HDR domain. Although the pair data are missing, UPHDR-GAN can properly handle the ghosting artifacts caused by moving objects or misalignments with the help of modified GAN loss, improved discriminator network and useful initialization phase. The proposed method preserves the details of important regions and improves the total image perceptual quality. Qualitative and quantitative comparisons against other methods demonstrated the superiority of our method.

</p>
</details>

<details><summary><b>A Speaker Verification Backend with Robust Performance across Conditions</b>
<a href="https://arxiv.org/abs/2102.01760">arxiv:2102.01760</a>
&#x1F4C8; 2 <br>
<p>Luciana Ferrer, Mitchell McLaren, Niko Brummer</p></summary>
<p>

**Abstract:** In this paper, we address the problem of speaker verification in conditions unseen or unknown during development. A standard method for speaker verification consists of extracting speaker embeddings with a deep neural network and processing them through a backend composed of probabilistic linear discriminant analysis (PLDA) and global logistic regression score calibration. This method is known to result in systems that work poorly on conditions different from those used to train the calibration model. We propose to modify the standard backend, introducing an adaptive calibrator that uses duration and other automatically extracted side-information to adapt to the conditions of the inputs. The backend is trained discriminatively to optimize binary cross-entropy. When trained on a number of diverse datasets that are labeled only with respect to speaker, the proposed backend consistently and, in some cases, dramatically improves calibration, compared to the standard PLDA approach, on a number of held-out datasets, some of which are markedly different from the training data. Discrimination performance is also consistently improved. We show that joint training of the PLDA and the adaptive calibrator is essential -- the same benefits cannot be achieved when freezing PLDA and fine-tuning the calibrator. To our knowledge, the results in this paper are the first evidence in the literature that it is possible to develop a speaker verification system with robust out-of-the-box performance on a large variety of conditions.

</p>
</details>

<details><summary><b>Distributed Conditional Generative Adversarial Networks (GANs) for Data-Driven Millimeter Wave Communications in UAV Networks</b>
<a href="https://arxiv.org/abs/2102.01751">arxiv:2102.01751</a>
&#x1F4C8; 2 <br>
<p>Qianqian Zhang, Aidin Ferdowsi, Walid Saad, Mehdi Bennis</p></summary>
<p>

**Abstract:** In this paper, a novel framework is proposed to perform data-driven air-to-ground (A2G) channel estimation for millimeter wave (mmWave) communications in an unmanned aerial vehicle (UAV) wireless network. First, an effective channel estimation approach is developed to collect mmWave channel information, allowing each UAV to train a stand-alone channel model via a conditional generative adversarial network (CGAN) along each beamforming direction. Next, in order to expand the application scenarios of the trained channel model into a broader spatial-temporal domain, a cooperative framework, based on a distributed CGAN architecture, is developed, allowing each UAV to collaboratively learn the mmWave channel distribution in a fully-distributed manner. To guarantee an efficient learning process, necessary and sufficient conditions for the optimal UAV network topology that maximizes the learning rate for cooperative channel modeling are derived, and the optimal CGAN learning solution per UAV is subsequently characterized, based on the distributed network structure. Simulation results show that the proposed distributed CGAN approach is robust to the local training error at each UAV. Meanwhile, a larger airborne network size requires more communication resources per UAV to guarantee an efficient learning rate. The results also show that, compared with a stand-alone CGAN without information sharing and two other distributed schemes, namely: A multi-discriminator CGAN and a federated CGAN method, the proposed distributed CGAN approach yields a higher modeling accuracy while learning the environment, and it achieves a larger average data rate in the online performance of UAV downlink mmWave communications.

</p>
</details>

<details><summary><b>Vehicle trajectory prediction in top-view image sequences based on deep learning method</b>
<a href="https://arxiv.org/abs/2102.01749">arxiv:2102.01749</a>
&#x1F4C8; 2 <br>
<p>Zahra Salahshoori Nejad, Hamed Heravi, Ali Rahimpour Jounghani, Abdollah Shahrezaie, Afshin Ebrahimi</p></summary>
<p>

**Abstract:** Annually, a large number of injuries and deaths around the world are related to motor vehicle accidents. This value has recently been reduced to some extent, via the use of driver-assistance systems. Developing driver-assistance systems (i.e., automated driving systems) can play a crucial role in reducing this number. Estimating and predicting surrounding vehicles' movement is essential for an automated vehicle and advanced safety systems. Moreover, predicting the trajectory is influenced by numerous factors, such as drivers' behavior during accidents, history of the vehicle's movement and the surrounding vehicles, and their position on the traffic scene. The vehicle must move over a safe path in traffic and react to other drivers' unpredictable behaviors in the shortest time. Herein, to predict automated vehicles' path, a model with low computational complexity is proposed, which is trained by images taken from the road's aerial image. Our method is based on an encoder-decoder model that utilizes a social tensor to model the effect of the surrounding vehicles' movement on the target vehicle. The proposed model can predict the vehicle's future path in any freeway only by viewing the images related to the history of the target vehicle's movement and its neighbors. Deep learning was used as a tool for extracting the features of these images. Using the HighD database, an image dataset of the road's aerial image was created, and the model's performance was evaluated on this new database. We achieved the RMSE of 1.91 for the next 5 seconds and found that the proposed method had less error than the best path-prediction methods in previous studies.

</p>
</details>

<details><summary><b>On Robustness of Neural Semantic Parsers</b>
<a href="https://arxiv.org/abs/2102.01563">arxiv:2102.01563</a>
&#x1F4C8; 2 <br>
<p>Shuo Huang, Zhuang Li, Lizhen Qu, Lei Pan</p></summary>
<p>

**Abstract:** Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sate-of-the-art parsers' performance on robustness test sets, and evaluating the effect of data augmentation.

</p>
</details>

<details><summary><b>Image Splicing Detection, Localization and Attribution via JPEG Primary Quantization Matrix Estimation and Clustering</b>
<a href="https://arxiv.org/abs/2102.01439">arxiv:2102.01439</a>
&#x1F4C8; 2 <br>
<p>Yakun Niu, Benedetta Tondi, Yao Zhao, Rongrong Ni, Mauro Barni</p></summary>
<p>

**Abstract:** Detection of inconsistencies of double JPEG artefacts across different image regions is often used to detect local image manipulations, like image splicing, and to localize them. In this paper, we move one step further, proposing an end-to-end system that, in addition to detecting and localizing spliced regions, can also distinguish regions coming from different donor images. We assume that both the spliced regions and the background image have undergone a double JPEG compression, and use a local estimate of the primary quantization matrix to distinguish between spliced regions taken from different sources. To do so, we cluster the image blocks according to the estimated primary quantization matrix and refine the result by means of morphological reconstruction. The proposed method can work in a wide variety of settings including aligned and non-aligned double JPEG compression, and regardless of whether the second compression is stronger or weaker than the first one. We validated the proposed approach by means of extensive experiments showing its superior performance with respect to baseline methods working in similar conditions.

</p>
</details>

<details><summary><b>Child-Computer Interaction: Recent Works, New Dataset, and Age Detection</b>
<a href="https://arxiv.org/abs/2102.01405">arxiv:2102.01405</a>
&#x1F4C8; 2 <br>
<p>Ruben Tolosana, Juan Carlos Ruiz-Garcia, Ruben Vera-Rodriguez, Jaime Herreros-Rodriguez, Sergio Romero-Tapiador, Aythami Morales, Julian Fierrez</p></summary>
<p>

**Abstract:** We overview recent research in Child-Computer Interaction and describe our framework ChildCI intended for: i) generating a better understanding of the cognitive and neuromotor development of children while interacting with mobile devices, and ii) enabling new applications in e-learning and e-health, among others. Our framework includes a new mobile application, specific data acquisition protocols, and a first release of the ChildCI dataset (ChildCIdb v1), which is planned to be extended yearly to enable longitudinal studies. In our framework children interact with a tablet device, using both a pen stylus and the finger, performing different tasks that require different levels of neuromotor and cognitive skills. ChildCIdb comprises more than 400 children from 18 months to 8 years old, considering therefore the first three development stages of the Piaget's theory. In addition, and as a demonstration of the potential of the ChildCI framework, we include experimental results for one of the many applications enabled by ChildCIdb: children age detection based on device interaction. Different machine learning approaches are evaluated, proposing a new set of 34 global features to automatically detect age groups, achieving accuracy results over 90% and interesting findings in terms of the type of features more useful for this task.

</p>
</details>

<details><summary><b>Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach</b>
<a href="https://arxiv.org/abs/2103.03817">arxiv:2103.03817</a>
&#x1F4C8; 1 <br>
<p>Amirhossein Shaghaghi, Abolfazl Zakeri, Nader Mokari, Mohammad Reza Javan, Mohammad Behdadfar, Eduard A Jorswieck</p></summary>
<p>

**Abstract:** In this paper, we propose a Zero-Touch, deep reinforcement learning (DRL)-based Proactive Failure Recovery framework called ZT-PFR for stateful network function virtualization (NFV)-enabled networks. To this end, we formulate a resource-efficient optimization problem minimizing the network cost function including resource cost and wrong decision penalty. As a solution, we propose state-of-the-art DRL-based methods such as soft-actor-critic (SAC) and proximal-policy-optimization (PPO). In addition, to train and test our DRL agents, we propose a novel impending-failure model. Moreover, to keep network status information at an acceptable freshness level for appropriate decision-making, we apply the concept of age of information to strike a balance between the event and scheduling based monitoring. Several key systems and DRL algorithm design insights for ZT-PFR are drawn from our analysis and simulation results. For example, we use a hybrid neural network, consisting long short-term memory layers in the DRL agents structure, to capture impending-failures time dependency.

</p>
</details>

<details><summary><b>Optimizing Unlicensed Band Spectrum Sharing With Subspace-Based Pareto Tracing</b>
<a href="https://arxiv.org/abs/2102.09047">arxiv:2102.09047</a>
&#x1F4C8; 1 <br>
<p>Zachary J. Grey, Susanna Mosleh, Jacob D. Rezac, Yao Ma, Jason B. Coder, Andrew M. Dienstfrey</p></summary>
<p>

**Abstract:** To meet the ever-growing demands of data throughput for forthcoming and deployed wireless networks, new wireless technologies like Long-Term Evolution License-Assisted Access (LTE-LAA) operate in shared and unlicensed bands. However, the LAA network must co-exist with incumbent IEEE 802.11 Wi-Fi systems. We consider a coexistence scenario where multiple LAA and Wi-Fi links share an unlicensed band. We aim to improve this coexistence by maximizing the key performance indicators (KPIs) of these networks simultaneously via dimension reduction and multi-criteria optimization. These KPIs are network throughputs as a function of medium access control protocols and physical layer parameters. We perform an exploratory analysis of coexistence behavior by approximating active subspaces to identify low-dimensional structure in the optimization criteria, i.e., few linear combinations of parameters for simultaneously maximizing KPIs. We leverage an aggregate low-dimensional subspace parametrized by approximated active subspaces of throughputs to facilitate multi-criteria optimization. The low-dimensional subspace approximations inform visualizations revealing convex KPIs over mixed active coordinates leading to an analytic Pareto trace of near-optimal solutions.

</p>
</details>

<details><summary><b>Edge-Detect: Edge-centric Network Intrusion Detection using Deep Neural Network</b>
<a href="https://arxiv.org/abs/2102.01873">arxiv:2102.01873</a>
&#x1F4C8; 1 <br>
<p>Praneet Singh, Jishnu Jaykumar, Akhil Pankaj, Reshmi Mitra</p></summary>
<p>

**Abstract:** Edge nodes are crucial for detection against multitudes of cyber attacks on Internet-of-Things endpoints and is set to become part of a multi-billion industry. The resource constraints in this novel network infrastructure tier constricts the deployment of existing Network Intrusion Detection System with Deep Learning models (DLM). We address this issue by developing a novel light, fast and accurate 'Edge-Detect' model, which detects Distributed Denial of Service attack on edge nodes using DLM techniques. Our model can work within resource restrictions i.e. low power, memory and processing capabilities, to produce accurate results at a meaningful pace. It is built by creating layers of Long Short-Term Memory or Gated Recurrent Unit based cells, which are known for their excellent representation of sequential data. We designed a practical data science pipeline with Recurring Neural Network to learn from the network packet behavior in order to identify whether it is normal or attack-oriented. The model evaluation is from deployment on actual edge node represented by Raspberry Pi using current cybersecurity dataset (UNSW2015). Our results demonstrate that in comparison to conventional DLM techniques, our model maintains a high testing accuracy of 99% even with lower resource utilization in terms of cpu and memory. In addition, it is nearly 3 times smaller in size than the state-of-art model and yet requires a much lower testing time.

</p>
</details>

<details><summary><b>Organization of a Latent Space structure in VAE/GAN trained by navigation data</b>
<a href="https://arxiv.org/abs/2102.01852">arxiv:2102.01852</a>
&#x1F4C8; 1 <br>
<p>Hiroki Kojima, Takashi Ikegami</p></summary>
<p>

**Abstract:** We present a novel artificial cognitive mapping system using generative deep neural networks, called variational autoencoder/generative adversarial network (VAE/GAN), which can map input images to latent vectors and generate temporal sequences internally. The results show that the distance of the predicted image is reflected in the distance of the corresponding latent vector after training. This indicates that the latent space is self-organized to reflect the proximity structure of the dataset and may provide a mechanism through which many aspects of cognition are spatially represented. The present study allows the network to internally generate temporal sequences that are analogous to the hippocampal replay/pre-play ability, where VAE produces only near-accurate replays of past experiences, but by introducing GANs, the generated sequences are coupled with instability and novelty.

</p>
</details>

<details><summary><b>Analyzing the barren plateau phenomenon in training quantum neural networks with the ZX-calculus</b>
<a href="https://arxiv.org/abs/2102.01828">arxiv:2102.01828</a>
&#x1F4C8; 1 <br>
<p>Chen Zhao, Xiao-Shan Gao</p></summary>
<p>

**Abstract:** In this paper, we propose a general scheme to analyze the gradient vanishing phenomenon, also known as the barren plateau phenomenon, in training quantum neural networks with the ZX-calculus. More precisely, we extend the barren plateaus theorem from unitary 2-design circuits to any parameterized quantum circuits under certain reasonable assumptions. The main technical contribution of this paper is representing certain integrations as ZX-diagrams and computing them with the ZX-calculus. The method is used to analyze four concrete quantum neural networks with different structures. It is shown that, for the hardware efficient ansatz and the MPS-inspired ansatz, there exist barren plateaus, while for the QCNN ansatz and the tree tensor network ansatz, there exists no barren plateau.

</p>
</details>

<details><summary><b>Safe Search for Stackelberg Equilibria in Extensive-Form Games</b>
<a href="https://arxiv.org/abs/2102.01775">arxiv:2102.01775</a>
&#x1F4C8; 1 <br>
<p>Chun Kai Ling, Noam Brown</p></summary>
<p>

**Abstract:** Stackelberg equilibrium is a solution concept in two-player games where the leader has commitment rights over the follower. In recent years, it has become a cornerstone of many security applications, including airport patrolling and wildlife poaching prevention. Even though many of these settings are sequential in nature, existing techniques pre-compute the entire solution ahead of time. In this paper, we present a theoretically sound and empirically effective way to apply search, which leverages extra online computation to improve a solution, to the computation of Stackelberg equilibria in general-sum games. Instead of the leader attempting to solve the full game upfront, an approximate "blueprint" solution is first computed offline and is then improved online for the particular subgames encountered in actual play. We prove that our search technique is guaranteed to perform no worse than the pre-computed blueprint strategy, and empirically demonstrate that it enables approximately solving significantly larger games compared to purely offline methods. We also show that our search operation may be cast as a smaller Stackelberg problem, making our method complementary to existing algorithms based on strategy generation.

</p>
</details>

<details><summary><b>Deep Convolutional Neural Networks to Predict Mutual Coupling Effects in Metasurfaces</b>
<a href="https://arxiv.org/abs/2102.01761">arxiv:2102.01761</a>
&#x1F4C8; 1 <br>
<p>Sensong An, Bowen Zheng, Mikhail Y. Shalaginov, Hong Tang, Hang Li, Li Zhou, Yunxi Dong, Mohammad Haerinia, Anuradha Murthy Agarwal, Clara Rivero-Baleine, Myungkoo Kang, Kathleen A. Richardson, Tian Gu, Juejun Hu, Clayton Fowler, Hualiang Zhang</p></summary>
<p>

**Abstract:** Metasurfaces have provided a novel and promising platform for the realization of compact and large-scale optical devices. The conventional metasurface design approach assumes periodic boundary conditions for each element, which is inaccurate in most cases since the near-field coupling effects between elements will change when surrounded by non-identical structures. In this paper, we propose a deep learning approach to predict the actual electromagnetic (EM) responses of each target meta-atom placed in a large array with near-field coupling effects taken into account. The predicting neural network takes the physical specifications of the target meta-atom and its neighbors as input, and calculates its phase and amplitude in milliseconds. This approach can be applied to explain metasurfaces' performance deterioration caused by mutual coupling and further used to optimize their efficiencies once combined with optimization algorithms. To demonstrate the efficacy of this methodology, we obtain large improvements in efficiency for a beam deflector and a metalens over the conventional design approach. Moreover, we show the correlations between a metasurface's performance and its design errors caused by mutual coupling are not bound to certain specifications (materials, shapes, etc.). As such, we envision that this approach can be readily applied to explore the mutual coupling effects and improve the performance of various metasurface designs.

</p>
</details>

<details><summary><b>Initial condition assessment for reaction-diffusion glioma growth models: A translational MRI/histology (in)validation study</b>
<a href="https://arxiv.org/abs/2102.01719">arxiv:2102.01719</a>
&#x1F4C8; 1 <br>
<p>Corentin Martens, Laetitia Lebrun, Christine Decaestecker, Thomas Vandamme, Yves-Rémi Van Eycke, Antonin Rovai, Thierry Metens, Olivier Debeir, Serge Goldman, Isabelle Salmon, Gaetan Van Simaeys</p></summary>
<p>

**Abstract:** Diffuse gliomas are highly infiltrative tumors whose early diagnosis and follow-up usually rely on magnetic resonance imaging (MRI). However, the limited sensitivity of this technique makes it impossible to directly assess the extent of the glioma cell invasion, leading to sub-optimal treatment planing. Reaction-diffusion growth models have been proposed for decades to extrapolate glioma cell infiltration beyond margins visible on MRI and predict its spatial-temporal evolution. These models nevertheless require an initial condition, that is the tumor cell density values at every location of the brain at diagnosis time. Several works have proposed to relate the tumor cell density function to abnormality outlines visible on MRI but the underlying assumptions have never been verified so far. In this work we propose to verify these assumptions by stereotactic histological analysis of a non-operated brain with glioblastoma using a tailored 3D-printed slicer. Cell density maps are computed from histological slides using a deep learning approach. The density maps are then registered to a postmortem MR image and related to an MR-derived geodesic distance map to the tumor core. The relation between the edema outlines visible on T2 FLAIR MRI and the distance to the core is also investigated. Our results suggest that (i) the previously suggested exponential decrease of the tumor cell density with the distance to the tumor core is not unreasonable but (ii) the edema outlines may in general not correspond to a cell density iso-contour and (iii) the commonly adopted tumor cell density value at these outlines is likely overestimated. These findings highlight the limitations of using conventional MRI to derive glioma cell density maps and point out the need of validating other methods to initialize reaction-diffusion growth models and make them usable in clinical practice.

</p>
</details>

<details><summary><b>QoS-Aware Power Minimization of Distributed Many-Core Servers using Transfer Q-Learning</b>
<a href="https://arxiv.org/abs/2102.01348">arxiv:2102.01348</a>
&#x1F4C8; 1 <br>
<p>Dainius Jenkus, Fei Xia, Rishad Shafik, Alex Yakovlev</p></summary>
<p>

**Abstract:** Web servers scaled across distributed systems necessitate complex runtime controls for providing quality of service (QoS) guarantees as well as minimizing the energy costs under dynamic workloads. This paper presents a QoS-aware runtime controller using horizontal scaling (node allocation) and vertical scaling (resource allocation within nodes) methods synergistically to provide adaptation to workloads while minimizing the power consumption under QoS constraint (i.e., response time). A horizontal scaling determines the number of active nodes based on workload demands and the required QoS according to a set of rules. Then, it is coupled with vertical scaling using transfer Q-learning, which further tunes power/performance based on workload profile using dynamic voltage/frequency scaling (DVFS). It transfers Q-values within minimally explored states reducing exploration requirements. In addition, the approach exploits a scalable architecture of the many-core server allowing to reuse available knowledge from fully or partially explored nodes. When combined, these methods allow to reduce the exploration time and QoS violations when compared to model-free Q-learning. The technique balances design-time and runtime costs to maximize the portability and operational optimality demonstrated through persistent power reductions with minimal QoS violations under different workload scenarios on heterogeneous multi-processing nodes of a server cluster.

</p>
</details>

<details><summary><b>Dermo-DOCTOR: A framework for concurrent skin lesion detection and recognition using a deep convolutional neural network with end-to-end dual encoders</b>
<a href="https://arxiv.org/abs/2102.01824">arxiv:2102.01824</a>
&#x1F4C8; 0 <br>
<p>Md. Kamrul Hasan, Shidhartho Roy, Chayan Mondal, Md. Ashraful Alam, Md. Toufick E Elahi, Aishwariya Dutta, S. M. Taslim Uddin Raju, Md. Tasnim Jawad, Mohiuddin Ahmad</p></summary>
<p>

**Abstract:** Automated skin lesion analysis for simultaneous detection and recognition is still challenging for inter-class homogeneity and intra-class heterogeneity, leading to low generic capability of a Single Convolutional Neural Network (CNN) with limited datasets. This article proposes an end-to-end deep CNN-based framework for simultaneous detection and recognition of the skin lesions, named Dermo-DOCTOR, consisting of two encoders. The feature maps from two encoders are fused channel-wise, called Fused Feature Map (FFM). The FFM is utilized for decoding in the detection sub-network, concatenating each stage of two encoders' outputs with corresponding decoder layers to retrieve the lost spatial information due to pooling in the encoders. For the recognition sub-network, the outputs of three fully connected layers, utilizing feature maps of two encoders and FFM, are aggregated to obtain a final lesion class. We train and evaluate the proposed Dermo-Doctor utilizing two publicly available benchmark datasets, such as ISIC-2016 and ISIC-2017. The achieved segmentation results exhibit mean intersection over unions of 85.0 % and 80.0 % respectively for ISIC-2016 and ISIC-2017 test datasets. The proposed Dermo-DOCTOR also demonstrates praiseworthy success in lesion recognition, providing the areas under the receiver operating characteristic curves of 0.98 and 0.91 respectively for those two datasets. The experimental results show that the proposed Dermo-DOCTOR outperforms the alternative methods mentioned in the literature, designed for skin lesion detection and recognition. As the Dermo-DOCTOR provides better-results on two different test datasets, even with limited training data, it can be an auspicious computer-aided assistive tool for dermatologists.

</p>
</details>

<details><summary><b>Symmetric Sparse Boolean Matrix Factorization and Applications</b>
<a href="https://arxiv.org/abs/2102.01570">arxiv:2102.01570</a>
&#x1F4C8; 0 <br>
<p>Sitan Chen, Zhao Song, Runzhou Tao, Ruizhe Zhang</p></summary>
<p>

**Abstract:** In this work, we study a variant of nonnegative matrix factorization where we wish to find a symmetric factorization of a given input matrix into a sparse, Boolean matrix. Formally speaking, given $\mathbf{M}\in\mathbb{Z}^{m\times m}$, we want to find $\mathbf{W}\in\{0,1\}^{m\times r}$ such that $\| \mathbf{M} - \mathbf{W}\mathbf{W}^\top \|_0$ is minimized among all $\mathbf{W}$ for which each row is $k$-sparse. This question turns out to be closely related to a number of questions like recovering a hypergraph from its line graph, as well as reconstruction attacks for private neural network training.
  As this problem is hard in the worst-case, we study a natural average-case variant that arises in the context of these reconstruction attacks: $\mathbf{M} = \mathbf{W}\mathbf{W}^{\top}$ for $\mathbf{W}$ a random Boolean matrix with $k$-sparse rows, and the goal is to recover $\mathbf{W}$ up to column permutation. Equivalently, this can be thought of as recovering a uniformly random $k$-uniform hypergraph from its line graph.
  Our main result is a polynomial-time algorithm for this problem based on bootstrapping higher-order information about $\mathbf{W}$ and then decomposing an appropriate tensor. The key ingredient in our analysis, which may be of independent interest, is to show that such a matrix $\mathbf{W}$ has full column rank with high probability as soon as $m = \widetildeΩ(r)$, which we do using tools from Littlewood-Offord theory and estimates for binary Krawtchouk polynomials.

</p>
</details>

<details><summary><b>An Abstraction-based Method to Check Multi-Agent Deep Reinforcement-Learning Behaviors</b>
<a href="https://arxiv.org/abs/2102.01434">arxiv:2102.01434</a>
&#x1F4C8; 0 <br>
<p>Pierre El Mqirmi, Francesco Belardinelli, Borja G. León</p></summary>
<p>

**Abstract:** Multi-agent reinforcement learning (RL) often struggles to ensure the safe behaviours of the learning agents, and therefore it is generally not adapted to safety-critical applications. To address this issue, we present a methodology that combines formal verification with (deep) RL algorithms to guarantee the satisfaction of formally-specified safety constraints both in training and testing. The approach we propose expresses the constraints to verify in Probabilistic Computation Tree Logic (PCTL) and builds an abstract representation of the system to reduce the complexity of the verification step. This abstract model allows for model checking techniques to identify a set of abstract policies that meet the safety constraints expressed in PCTL. Then, the agents' behaviours are restricted according to these safe abstract policies. We provide formal guarantees that by using this method, the actions of the agents always meet the safety constraints, and provide a procedure to generate an abstract model automatically. We empirically evaluate and show the effectiveness of our method in a multi-agent environment.

</p>
</details>

<details><summary><b>Generalized Facial Manipulation Detection with Edge Region Feature Extraction</b>
<a href="https://arxiv.org/abs/2102.01381">arxiv:2102.01381</a>
&#x1F4C8; 0 <br>
<p>Dong-Keon Kim, Kwangsu Kim</p></summary>
<p>

**Abstract:** This paper presents a generalized and robust face manipulation detection method based on the edge region features appearing in images. Most contemporary face synthesis processes include color awkwardness reduction but damage the natural fingerprint in the edge region. In addition, these color correction processes do not proceed in the non-face background region. We also observe that the synthesis process does not consider the natural properties of the image appearing in the time domain. Considering these observations, we propose a facial forensic framework that utilizes pixel-level color features appearing in the edge region of the whole image. Furthermore, our framework includes a 3D-CNN classification model that interprets the extracted color features spatially and temporally. Unlike other existing studies, we conduct authenticity determination by considering all features extracted from multiple frames within one video. Through extensive experiments, including real-world scenarios to evaluate generalized detection ability, we show that our framework outperforms state-of-the-art facial manipulation detection technologies in terms of accuracy and robustness.

</p>
</details>

<details><summary><b>Probabilistic Trust Intervals for Out of Distribution Detection</b>
<a href="https://arxiv.org/abs/2102.01336">arxiv:2102.01336</a>
&#x1F4C8; 0 <br>
<p>Gagandeep Singh, Deepak Mishra</p></summary>
<p>

**Abstract:** Building neural network classifiers with an ability to distinguish between in and out-of distribution inputs is an important step towards faithful deep learning systems. Some of the successful approaches for this, resort to architectural novelties, such as ensembles, with increased complexities in terms of the number of parameters and training procedures. Whereas some other approaches make use of surrogate samples, which are easy to create and work as proxies for actual out-of-distribution (OOD) samples, to train the networks for OOD detection. In this paper, we propose a very simple approach for enhancing the ability of a pretrained network to detect OOD inputs without even altering the original parameter values. We define a probabilistic trust interval for each weight parameter of the network and optimize its size according to the in-distribution (ID) inputs. It allows the network to sample additional weight values along with the original values at the time of inference and use the observed disagreement among the corresponding outputs for OOD detection. In order to capture the disagreement effectively, we also propose a measure and establish its suitability using empirical evidence. Our approach outperforms the existing state-of-the-art methods on various OOD datasets by considerable margins without using any real or surrogate OOD samples. We also analyze the performance of our approach on adversarial and corrupted inputs such as CIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as well. By using fundamental theorem of calculus on neural networks, we explain why our technique doesn't need to observe OOD samples during training to achieve results better than the previous works.

</p>
</details>


[Next Page]({{ '/2021/02/01/2021.02.01.html' | relative_url }})
