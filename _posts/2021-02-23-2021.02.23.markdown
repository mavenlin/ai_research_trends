Prev: [2021.02.22]({{ '/2021/02/22/2021.02.22.html' | relative_url }})  Next: [2021.02.24]({{ '/2021/02/24/2021.02.24.html' | relative_url }})
{% raw %}
## Summary for 2021-02-23, created on 2021-12-24


<details><summary><b>On Sexual Selection</b>
<a href="https://arxiv.org/abs/2102.11667">arxiv:2102.11667</a>
&#x1F4C8; 994 <br>
<p>Larry Bull</p></summary>
<p>

**Abstract:** Sexual selection is a fundamental aspect of evolution for all eukaryotic organisms with mating types. This paper suggests intersexual selection is best viewed as a mechanism to compensate for the unavoidable dynamics of coevolution between sexes that emerge with isogamy. Using the NK model of fitness landscapes, the conditions under which allosomes emerge are first explored. This extends previous work on the evolution of sex where the fitness landscape smoothing of a rudimentary form of the Baldwin effect is suggested as the underlying cause. The NKCS model of coevolution is then used to show how varying fitness landscape size, ruggedness, and connectedness can vary the conditions under which a very simple sexual selection mechanism proves beneficial. This is found to be the case whether one or both sexes exploit sexual selection.

</p>
</details>

<details><summary><b>Solving high-dimensional parabolic PDEs using the tensor train format</b>
<a href="https://arxiv.org/abs/2102.11830">arxiv:2102.11830</a>
&#x1F4C8; 71 <br>
<p>Lorenz Richter, Leon Sallandt, Nikolas NÃ¼sken</p></summary>
<p>

**Abstract:** High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid-based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.

</p>
</details>

<details><summary><b>Generative Modelling of BRDF Textures from Flash Images</b>
<a href="https://arxiv.org/abs/2102.11861">arxiv:2102.11861</a>
&#x1F4C8; 70 <br>
<p>Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel</p></summary>
<p>

**Abstract:** We learn a latent space for easy capture, consistent interpolation, and efficient reproduction of visual material appearance. When users provide a photo of a stationary natural material captured under flashlight illumination, first it is converted into a latent material code. Then, in the second step, conditioned on the material code, our method produces an infinite and diverse spatial field of BRDF model parameters (diffuse albedo, normals, roughness, specular albedo) that subsequently allows rendering in complex scenes and illuminations, matching the appearance of the input photograph. Technically, we jointly embed all flash images into a latent space using a convolutional encoder, and -- conditioned on these latent codes -- convert random spatial fields into fields of BRDF parameters using a convolutional neural network (CNN). We condition these BRDF parameters to match the visual characteristics (statistics and spectra of visual features) of the input under matching light. A user study compares our approach favorably to previous work, even those with access to BRDF supervision.

</p>
</details>

<details><summary><b>Image Completion via Inference in Deep Generative Models</b>
<a href="https://arxiv.org/abs/2102.12037">arxiv:2102.12037</a>
&#x1F4C8; 34 <br>
<p>William Harvey, Saeid Naderiparizi, Frank Wood</p></summary>
<p>

**Abstract:** We consider image completion from the perspective of amortized inference in an image generative model. We leverage recent state of the art variational auto-encoder architectures that have been shown to produce photo-realistic natural images at non-trivial resolutions. Through amortized inference in such a model we can train neural artifacts that produce diverse, realistic image completions even when the vast majority of an image is missing. We demonstrate superior sample quality and diversity compared to prior art on the CIFAR-10 and FFHQ-256 datasets. We conclude by describing and demonstrating an application that requires an in-painting model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.

</p>
</details>

<details><summary><b>Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective</b>
<a href="https://arxiv.org/abs/2102.11535">arxiv:2102.11535</a>
&#x1F4C8; 24 <br>
<p>Wuyang Chen, Xinyu Gong, Zhangyang Wang</p></summary>
<p>

**Abstract:** Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? We provide an affirmative answer, by proposing a novel framework called training-free neural architecture search (TE-NAS). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK) and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks and can be computed without any training and any label. We show that: (1) these two measurements imply the trainability and expressivity of a neural network; (2) they strongly correlate with the network's test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in bridging the theoretical findings of deep networks and practical impacts in real NAS applications. Code is available at: https://github.com/VITA-Group/TENAS.

</p>
</details>

<details><summary><b>ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2102.11600">arxiv:2102.11600</a>
&#x1F4C8; 23 <br>
<p>Jungmin Kwon, Jeongseop Kim, Hyunseo Park, In Kwon Choi</p></summary>
<p>

**Abstract:** Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.

</p>
</details>

<details><summary><b>Unsupervised Brain Anomaly Detection and Segmentation with Transformers</b>
<a href="https://arxiv.org/abs/2102.11650">arxiv:2102.11650</a>
&#x1F4C8; 22 <br>
<p>Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, Robert Gray, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</p></summary>
<p>

**Abstract:** Pathological brain appearances may be so heterogeneous as to be intelligible only as anomalies, defined by their deviation from normality rather than any specific pathological characteristic. Amongst the hardest tasks in medical imaging, detecting such anomalies requires models of the normal brain that combine compactness with the expressivity of the complex, long-range interactions that characterise its structural organisation. These are requirements transformers have arguably greater potential to satisfy than other current candidate architectures, but their application has been inhibited by their demands on data and computational resource. Here we combine the latent representation of vector quantised variational autoencoders with an ensemble of autoregressive transformers to enable unsupervised anomaly detection and segmentation defined by deviation from healthy brain imaging data, achievable at low computational cost, within relative modest data regimes. We compare our method to current state-of-the-art approaches across a series of experiments involving synthetic and real pathological lesions. On real lesions, we train our models on 15,000 radiologically normal participants from UK Biobank, and evaluate performance on four different brain MR datasets with small vessel disease, demyelinating lesions, and tumours. We demonstrate superior anomaly detection performance both image-wise and pixel-wise, achievable without post-processing. These results draw attention to the potential of transformers in this most challenging of imaging tasks.

</p>
</details>

<details><summary><b>Learning with User-Level Privacy</b>
<a href="https://arxiv.org/abs/2102.11845">arxiv:2102.11845</a>
&#x1F4C8; 19 <br>
<p>Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh</p></summary>
<p>

**Abstract:** We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user's entire contribution ($m \ge 1$ samples), providing more stringent but more realistic protection against information leaks. We show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy cost decreases at a faster $O(1/n)$ rate. We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius $Ï$ of the distribution rather than the entire range.

</p>
</details>

<details><summary><b>HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search</b>
<a href="https://arxiv.org/abs/2102.11646">arxiv:2102.11646</a>
&#x1F4C8; 10 <br>
<p>Niv Nayman, Yonathan Aflalo, Asaf Noy, Lihi Zelnik-Manor</p></summary>
<p>

**Abstract:** Realistic use of neural networks often requires adhering to multiple constraints on latency, energy and memory among others. A popular approach to find fitting networks is through constrained Neural Architecture Search (NAS), however, previous methods enforce the constraint only softly. Therefore, the resulting networks do not exactly adhere to the resource constraint and their accuracy is harmed. In this work we resolve this by introducing Hard Constrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate formulation of the expected resource requirement and a scalable search method that satisfies the hard constraint throughout the search. Our experiments show that HardCoRe-NAS generates state-of-the-art architectures, surpassing other NAS methods, while strictly satisfying the hard resource constraints without any tuning required.

</p>
</details>

<details><summary><b>SeqNet: Learning Descriptors for Sequence-based Hierarchical Place Recognition</b>
<a href="https://arxiv.org/abs/2102.11603">arxiv:2102.11603</a>
&#x1F4C8; 10 <br>
<p>Sourav Garg, Michael Milford</p></summary>
<p>

**Abstract:** Visual Place Recognition (VPR) is the task of matching current visual imagery from a camera to images stored in a reference map of the environment. While initial VPR systems used simple direct image methods or hand-crafted visual features, recent work has focused on learning more powerful visual features and further improving performance through either some form of sequential matcher / filter or a hierarchical matching process. In both cases the performance of the initial single-image based system is still far from perfect, putting significant pressure on the sequence matching or (in the case of hierarchical systems) pose refinement stages. In this paper we present a novel hybrid system that creates a high performance initial match hypothesis generator using short learnt sequential descriptors, which enable selective control sequential score aggregation using single image learnt descriptors. Sequential descriptors are generated using a temporal convolutional network dubbed SeqNet, encoding short image sequences using 1-D convolutions, which are then matched against the corresponding temporal descriptors from the reference dataset to provide an ordered list of place match hypotheses. We then perform selective sequential score aggregation using shortlisted single image learnt descriptors from a separate pipeline to produce an overall place match hypothesis. Comprehensive experiments on challenging benchmark datasets demonstrate the proposed method outperforming recent state-of-the-art methods using the same amount of sequential information. Source code and supplementary material can be found at https://github.com/oravus/seqNet.

</p>
</details>

<details><summary><b>Adversarial Robustness with Non-uniform Perturbations</b>
<a href="https://arxiv.org/abs/2102.12002">arxiv:2102.12002</a>
&#x1F4C8; 9 <br>
<p>Ecenaz Erdemir, Jeffrey Bickford, Luca Melis, Sergul Aydore</p></summary>
<p>

**Abstract:** Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the requirement of imperceptibility. However, uniform perturbations do not result in realistic AEs in domains such as malware, finance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies. The key idea of our proposed approach is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. We propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. Using experimental datasets for malware classification, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certification utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certification.

</p>
</details>

<details><summary><b>Understanding and Mitigating Accuracy Disparity in Regression</b>
<a href="https://arxiv.org/abs/2102.12013">arxiv:2102.12013</a>
&#x1F4C8; 8 <br>
<p>Jianfeng Chi, Yuan Tian, Geoffrey J. Gordon, Han Zhao</p></summary>
<p>

**Abstract:** With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity in prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.

</p>
</details>

<details><summary><b>Meta-Learned Attribute Self-Gating for Continual Generalized Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2102.11856">arxiv:2102.11856</a>
&#x1F4C8; 8 <br>
<p>Vinay Kumar Verma, Kevin Liang, Nikhil Mehta, Lawrence Carin</p></summary>
<p>

**Abstract:** Zero-shot learning (ZSL) has been shown to be a promising approach to generalizing a model to categories unseen during training by leveraging class attributes, but challenges still remain. Recently, methods using generative models to combat bias towards classes seen during training have pushed the state of the art of ZSL, but these generative models can be slow or computationally expensive to train. Additionally, while many previous ZSL methods assume a one-time adaptation to unseen classes, in reality, the world is always changing, necessitating a constant adjustment for deployed models. Models unprepared to handle a sequential stream of data are likely to experience catastrophic forgetting. We propose a meta-continual zero-shot learning (MCZSL) approach to address both these issues. In particular, by pairing self-gating of attributes and scaled class normalization with meta-learning based training, we are able to outperform state-of-the-art results while being able to train our models substantially faster ($>100\times$) than expensive generative-based approaches. We demonstrate this by performing experiments on five standard ZSL datasets (CUB, aPY, AWA1, AWA2 and SUN) in both generalized zero-shot learning and generalized continual zero-shot learning settings.

</p>
</details>

<details><summary><b>Deep Policy Dynamic Programming for Vehicle Routing Problems</b>
<a href="https://arxiv.org/abs/2102.11756">arxiv:2102.11756</a>
&#x1F4C8; 8 <br>
<p>Wouter Kool, Herke van Hoof, Joaquim Gromicho, Max Welling</p></summary>
<p>

**Abstract:** Routing problems are a class of combinatorial problems with many practical applications. Recently, end-to-end deep learning methods have been proposed to learn approximate solution heuristics for such problems. In contrast, classical dynamic programming (DP) algorithms guarantee optimal solutions, but scale badly with the problem size. We propose Deep Policy Dynamic Programming (DPDP), which aims to combine the strengths of learned neural heuristics with those of DP algorithms. DPDP prioritizes and restricts the DP state space using a policy derived from a deep neural network, which is trained to predict edges from example solutions. We evaluate our framework on the travelling salesman problem (TSP), the vehicle routing problem (VRP) and TSP with time windows (TSPTW) and show that the neural policy improves the performance of (restricted) DP algorithms, making them competitive to strong alternatives such as LKH, while also outperforming most other 'neural approaches' for solving TSPs, VRPs and TSPTWs with 100 nodes.

</p>
</details>

<details><summary><b>Accelerating Recursive Partition-Based Causal Structure Learning</b>
<a href="https://arxiv.org/abs/2102.11545">arxiv:2102.11545</a>
&#x1F4C8; 8 <br>
<p>Md. Musfiqur Rahman, Ayman Rasheed, Md. Mosaddek Khan, Mohammad Ali Javidian, Pooyan Jamshidi, Md. Mamun-Or-Rashid</p></summary>
<p>

**Abstract:** Causal structure discovery from observational data is fundamental to the causal understanding of autonomous systems such as medical decision support systems, advertising campaigns and self-driving cars. This is essential to solve well-known causal decision making and prediction problems associated with those real-world applications. Recently, recursive causal discovery algorithms have gained particular attention among the research community due to their ability to provide good results by using Conditional Independent (CI) tests in smaller sub-problems. However, each of such algorithms needs a refinement function to remove undesired causal relations of the discovered graphs. Notably, with the increase of the problem size, the computation cost (i.e., the number of CI-tests) of the refinement function makes an algorithm expensive to deploy in practice. This paper proposes a generic causal structure refinement strategy that can locate the undesired relations with a small number of CI-tests, thus speeding up the algorithm for large and complex problems. We theoretically prove the correctness of our algorithm. We then empirically evaluate its performance against the state-of-the-art algorithms in terms of solution quality and completion time in synthetic and real datasets.

</p>
</details>

<details><summary><b>Deep Video Prediction for Time Series Forecasting</b>
<a href="https://arxiv.org/abs/2102.12061">arxiv:2102.12061</a>
&#x1F4C8; 7 <br>
<p>Zhen Zeng, Tucker Balch, Manuela Veloso</p></summary>
<p>

**Abstract:** Time series forecasting is essential for decision making in many domains. In this work, we address the challenge of predicting prices evolution among multiple potentially interacting financial assets. A solution to this problem has obvious importance for governments, banks, and investors. Statistical methods such as Auto Regressive Integrated Moving Average (ARIMA) are widely applied to these problems. In this paper, we propose to approach economic time series forecasting of multiple financial assets in a novel way via video prediction. Given past prices of multiple potentially interacting financial assets, we aim to predict the prices evolution in the future. Instead of treating the snapshot of prices at each time point as a vector, we spatially layout these prices in 2D as an image, such that we can harness the power of CNNs in learning a latent representation for these financial assets. Thus, the history of these prices becomes a sequence of images, and our goal becomes predicting future images. We build on a state-of-the-art video prediction method for forecasting future images. Our experiments involve the prediction task of the price evolution of nine financial assets traded in U.S. stock markets. The proposed method outperforms baselines including ARIMA, Prophet, and variations of the proposed method, demonstrating the benefits of harnessing the power of CNNs in the problem of economic time series forecasting.

</p>
</details>

<details><summary><b>Decision Rule Elicitation for Domain Adaptation</b>
<a href="https://arxiv.org/abs/2102.11539">arxiv:2102.11539</a>
&#x1F4C8; 7 <br>
<p>Alexander Nikitin, Samuel Kaski</p></summary>
<p>

**Abstract:** Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or to provide feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. In simulated and real-user studies, we show that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert's knowledge to the AI model.

</p>
</details>

<details><summary><b>Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games</b>
<a href="https://arxiv.org/abs/2102.11494">arxiv:2102.11494</a>
&#x1F4C8; 7 <br>
<p>Yu Bai, Chi Jin, Huan Wang, Caiming Xiong</p></summary>
<p>

**Abstract:** Real world applications such as economics and policy making often involve solving multi-agent games with two unique features: (1) The agents are inherently asymmetric and partitioned into leaders and followers; (2) The agents have different reward functions, thus the game is general-sum. The majority of existing results in this field focuses on either symmetric solution concepts (e.g. Nash equilibrium) or zero-sum games. It remains open how to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash equilibrium -- in general-sum games efficiently from noisy samples.
  This paper initiates the theoretical study of sample-efficient learning of the Stackelberg equilibrium, in the bandit feedback setting where we only observe noisy samples of the reward. We consider three representative two-player general-sum games: bandit games, bandit-reinforcement learning (bandit-RL) games, and linear bandit games. In all these games, we identify a fundamental gap between the exact value of the Stackelberg equilibrium and its estimated version using finitely many noisy samples, which can not be closed information-theoretically regardless of the algorithm. We then establish sharp positive results on sample-efficient learning of Stackelberg equilibrium with value optimal up to the gap identified above, with matching lower bounds in the dependency on the gap, error tolerance, and the size of the action spaces. Overall, our results unveil unique challenges in learning Stackelberg equilibria under noisy bandit feedback, which we hope could shed light on future research on this topic.

</p>
</details>

<details><summary><b>Modular Design Patterns for Hybrid Learning and Reasoning Systems: a taxonomy, patterns and use cases</b>
<a href="https://arxiv.org/abs/2102.11965">arxiv:2102.11965</a>
&#x1F4C8; 6 <br>
<p>Michael van Bekkum, Maaike de Boer, Frank van Harmelen, AndrÃ© Meyer-Vitali, Annette ten Teije</p></summary>
<p>

**Abstract:** The unification of statistical (data-driven) and symbolic (knowledge-driven) methods is widely recognised as one of the key challenges of modern AI. Recent years have seen large number of publications on such hybrid neuro-symbolic AI systems. That rapidly growing literature is highly diverse and mostly empirical, and is lacking a unifying view of the large variety of these hybrid systems. In this paper we analyse a large body of recent literature and we propose a set of modular design patterns for such hybrid, neuro-symbolic systems. We are able to describe the architecture of a very large number of hybrid systems by composing only a small set of elementary patterns as building blocks.
  The main contributions of this paper are: 1) a taxonomically organised vocabulary to describe both processes and data structures used in hybrid systems; 2) a set of 15+ design patterns for hybrid AI systems, organised in a set of elementary patterns and a set of compositional patterns; 3) an application of these design patterns in two realistic use-cases for hybrid AI systems. Our patterns reveal similarities between systems that were not recognised until now. Finally, our design patterns extend and refine Kautz' earlier attempt at categorising neuro-symbolic architectures.

</p>
</details>

<details><summary><b>Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality</b>
<a href="https://arxiv.org/abs/2102.11866">arxiv:2102.11866</a>
&#x1F4C8; 6 <br>
<p>Tengyu Xu, Zhuoran Yang, Zhaoran Wang, Yingbin Liang</p></summary>
<p>

**Abstract:** Designing off-policy reinforcement learning algorithms is typically a very challenging task, because a desirable iteration update often involves an expectation over an on-policy distribution. Prior off-policy actor-critic (AC) algorithms have introduced a new critic that uses the density ratio for adjusting the distribution mismatch in order to stabilize the convergence, but at the cost of potentially introducing high biases due to the estimation errors of both the density ratio and value function. In this paper, we develop a doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take advantage of learned nuisance functions to reduce estimation errors. Moreover, DR-Off-PAC adopts a single timescale structure, in which both actor and critics are updated simultaneously with constant stepsize, and is thus more sample efficient than prior algorithms that adopt either two timescale or nested-loop structure. We study the finite-time convergence rate and characterize the sample complexity for DR-Off-PAC to attain an $Îµ$-accurate optimal policy. We also show that the overall convergence of DR-Off-PAC is doubly robust to the approximation errors that depend only on the expressive power of approximation functions. To the best of our knowledge, our study establishes the first overall sample complexity analysis for a single time-scale off-policy AC algorithm.

</p>
</details>

<details><summary><b>Data Fusion for Audiovisual Speaker Localization: Extending Dynamic Stream Weights to the Spatial Domain</b>
<a href="https://arxiv.org/abs/2102.11588">arxiv:2102.11588</a>
&#x1F4C8; 6 <br>
<p>Julio Wissing, Benedikt Boenninghoff, Dorothea Kolossa, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki, Christopher Schymura</p></summary>
<p>

**Abstract:** Estimating the positions of multiple speakers can be helpful for tasks like automatic speech recognition or speaker diarization. Both applications benefit from a known speaker position when, for instance, applying beamforming or assigning unique speaker identities. Recently, several approaches utilizing acoustic signals augmented with visual data have been proposed for this task. However, both the acoustic and the visual modality may be corrupted in specific spatial regions, for instance due to poor lighting conditions or to the presence of background noise. This paper proposes a novel audiovisual data fusion framework for speaker localization by assigning individual dynamic stream weights to specific regions in the localization space. This fusion is achieved via a neural network, which combines the predictions of individual audio and video trackers based on their time- and location-dependent reliability. A performance evaluation using audiovisual recordings yields promising results, with the proposed fusion approach outperforming all baseline models.

</p>
</details>

<details><summary><b>ROAD: The ROad event Awareness Dataset for Autonomous Driving</b>
<a href="https://arxiv.org/abs/2102.11585">arxiv:2102.11585</a>
&#x1F4C8; 6 <br>
<p>Gurkirt Singh, Stephen Akrigg, Manuele Di Maio, Valentina Fontana, Reza Javanmard Alitappeh, Suman Saha, Kossar Jeddisaravi, Farzad Yousefi, Jacob Culley, Tom Nicholson, Jordan Omokeowa, Salman Khan, Stanislao Grazioso, Andrew Bradley, Giuseppe Di Gironimo, Fabio Cuzzolin</p></summary>
<p>

**Abstract:** Humans approach driving in a holistic fashion which entails, in particular, understanding road events and their evolution. Injecting these capabilities in an autonomous vehicle has thus the potential to take situational awareness and decision making closer to human-level performance. To this purpose, we introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge the first of its kind. ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by a moving agent, the action(s) it performs and the corresponding scene locations. ROAD comprises 22 videos, originally from the Oxford RobotCar Dataset, annotated with bounding boxes showing the location in the image plane of each road event. We also provide as baseline a new incremental algorithm for online road event awareness, based on inflating RetinaNet along time, which achieves a mean average precision of 16.8% and 6.1% for frame-level and video-level event detection, respectively, at 50% overlap. Though promising, these figures highlight the challenges faced by situation awareness in autonomous driving. Finally, ROAD allows scholars to investigate exciting tasks such as complex (road) activity detection, future road event anticipation and the modelling of sentient road agents in terms of mental states. Dataset can be obtained from https://github.com/gurkirt/road-dataset and baseline code from https://github.com/gurkirt/3D-RetinaNet.

</p>
</details>

<details><summary><b>On the Minimal Error of Empirical Risk Minimization</b>
<a href="https://arxiv.org/abs/2102.12066">arxiv:2102.12066</a>
&#x1F4C8; 5 <br>
<p>Gil Kur, Alexander Rakhlin</p></summary>
<p>

**Abstract:** We study the minimal error of the Empirical Risk Minimization (ERM) procedure in the task of regression, both in the random and the fixed design settings. Our sharp lower bounds shed light on the possibility (or impossibility) of adapting to simplicity of the model generating the data. In the fixed design setting, we show that the error is governed by the global complexity of the entire class. In contrast, in random design, ERM may only adapt to simpler models if the local neighborhoods around the regression function are nearly as complex as the class itself, a somewhat counter-intuitive conclusion. We provide sharp lower bounds for performance of ERM for both Donsker and non-Donsker classes. We also discuss our results through the lens of recent studies on interpolation in overparameterized models.

</p>
</details>

<details><summary><b>PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset</b>
<a href="https://arxiv.org/abs/2102.12010">arxiv:2102.12010</a>
&#x1F4C8; 5 <br>
<p>Jean-Luc DÃ©ziel, Pierre Merriaux, Francis Tremblay, Dave Lessard, Dominique Plourde, Julien Stanguennec, Pierre Goulet, Pierre Olivier</p></summary>
<p>

**Abstract:** Leddar PixSet is a new publicly available dataset (dataset.leddartech.com) for autonomous driving research and development. One key novelty of this dataset is the presence of full-waveform data from the Leddar Pixell sensor, a solid-state flash LiDAR. Full-waveform data has been shown to improve the performance of perception algorithms in airborne applications but is yet to be demonstrated for terrestrial applications such as autonomous driving. The PixSet dataset contains approximately 29k frames from 97 sequences recorded in high-density urban areas, using a set of various sensors (cameras, LiDARs, radar, IMU, etc.) Each frame has been manually annotated with 3D bounding boxes.

</p>
</details>

<details><summary><b>Ray-based framework for state identification in quantum dot devices</b>
<a href="https://arxiv.org/abs/2102.11784">arxiv:2102.11784</a>
&#x1F4C8; 5 <br>
<p>Justyna P. Zwolak, Thomas McJunkin, Sandesh S. Kalantre, Samuel F. Neyens, E. R. MacQuarrie, Mark A. Eriksson, Jacob M. Taylor</p></summary>
<p>

**Abstract:** Quantum dots (QDs) defined with electrostatic gates are a leading platform for a scalable quantum computing implementation. However, with increasing numbers of qubits, the complexity of the control parameter space also grows. Traditional measurement techniques, relying on complete or near-complete exploration via two-parameter scans (images) of the device response, quickly become impractical with increasing numbers of gates. Here we propose to circumvent this challenge by introducing a measurement technique relying on one-dimensional projections of the device response in the multidimensional parameter space. Dubbed the ``ray-based classification (RBC) framework,'' we use this machine learning approach to implement a classifier for QD states, enabling automated recognition of qubit-relevant parameter regimes. We show that RBC surpasses the 82 % accuracy benchmark from the experimental implementation of image-based classification techniques from prior work while reducing the number of measurement points needed by up to 70 %. The reduction in measurement cost is a significant gain for time-intensive QD measurements and is a step forward toward the scalability of these devices. We also discuss how the RBC-based optimizer, which tunes the device to a multiqubit regime, performs when tuning in the two-dimensional and three-dimensional parameter spaces defined by plunger and barrier gates that control the QDs.This work provides experimental validation of both efficient state identification and optimization with machine learning techniques for non-traditional measurements in quantum systems with high-dimensional parameter spaces and time-intensive measurements.

</p>
</details>

<details><summary><b>Learning Large-scale Location Embedding From Human Mobility Trajectories with Graphs</b>
<a href="https://arxiv.org/abs/2103.00483">arxiv:2103.00483</a>
&#x1F4C8; 4 <br>
<p>Chenyu Tian, Yuchun Zhang, Zefeng Weng, Xiusen Gu, Wai Kin Victor Chan</p></summary>
<p>

**Abstract:** An increasing amount of location-based service (LBS) data is being accumulated and helps to study urban dynamics and human mobility. GPS coordinates and other location indicators are normally low dimensional and only representing spatial proximity, thus difficult to be effectively utilized by machine learning models in Geo-aware applications. Existing location embedding methods are mostly tailored for specific problems that are taken place within areas of interest. When it comes to the scale of a city or even a country, existing approaches always suffer from extensive computational cost and significant data sparsity. Different from existing studies, we propose to learn representations through a GCN-aided skip-gram model named GCN-L2V by considering both spatial connection and human mobility. With a flow graph and a spatial graph, it embeds context information into vector representations. GCN-L2V is able to capture relationships among locations and provide a better notion of similarity in a spatial environment. Across quantitative experiments and case studies, we empirically demonstrate that representations learned by GCN-L2V are effective. As far as we know, this is the first study that provides a fine-grained location embedding at the city level using only LBS records. GCN-L2V is a general-purpose embedding model with high flexibility and can be applied in down-streaming Geo-aware applications.

</p>
</details>

<details><summary><b>Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic</b>
<a href="https://arxiv.org/abs/2102.12855">arxiv:2102.12855</a>
&#x1F4C8; 4 <br>
<p>Mingyu Cai, Mohammadhosein Hasanbeig, Shaoping Xiao, Alessandro Abate, Zhen Kan</p></summary>
<p>

**Abstract:** This paper investigates the motion planning of autonomous dynamical systems modeled by Markov decision processes (MDP) with unknown transition probabilities over continuous state and action spaces. Linear temporal logic (LTL) is used to specify high-level tasks over infinite horizon, which can be converted into a limit deterministic generalized BÃ¼chi automaton (LDGBA) with several accepting sets. The novelty is to design an embedded product MDP (EP-MDP) between the LDGBA and the MDP by incorporating a synchronous tracking-frontier function to record unvisited accepting sets of the automaton, and to facilitate the satisfaction of the accepting conditions. The proposed LDGBA-based reward shaping and discounting schemes for the model-free reinforcement learning (RL) only depend on the EP-MDP states and can overcome the issues of sparse rewards. Rigorous analysis shows that any RL method that optimizes the expected discounted return is guaranteed to find an optimal policy whose traces maximize the satisfaction probability. A modular deep deterministic policy gradient (DDPG) is then developed to generate such policies over continuous state and action spaces. The performance of our framework is evaluated via an array of OpenAI gym environments.

</p>
</details>

<details><summary><b>Feature Importance Explanations for Temporal Black-Box Models</b>
<a href="https://arxiv.org/abs/2102.11934">arxiv:2102.11934</a>
&#x1F4C8; 4 <br>
<p>Akshay Sood, Mark Craven</p></summary>
<p>

**Abstract:** Models in the supervised learning framework may capture rich and complex representations over the features that are hard for humans to interpret. Existing methods to explain such models are often specific to architectures and data where the features do not have a time-varying component. In this work, we propose TIME, a method to explain models that are inherently temporal in nature. Our approach (i) uses a model-agnostic permutation-based approach to analyze global feature importance, (ii) identifies the importance of salient features with respect to their temporal ordering as well as localized windows of influence, and (iii) uses hypothesis testing to provide statistical rigor.

</p>
</details>

<details><summary><b>Grounded Relational Inference: Domain Knowledge Driven Explainable Autonomous Driving</b>
<a href="https://arxiv.org/abs/2102.11905">arxiv:2102.11905</a>
&#x1F4C8; 4 <br>
<p>Chen Tang, Nishan Srishankar, Sujitha Martin, Masayoshi Tomizuka</p></summary>
<p>

**Abstract:** Explainability is essential for autonomous vehicles and other robotics systems interacting with humans and other objects during operation. Humans need to understand and anticipate the actions taken by the machines for trustful and safe cooperation. In this work, we aim to enable the explainability of an autonomous driving system at the design stage by incorporating expert domain knowledge into the model. We propose Grounded Relational Inference (GRI). It models an interactive system's underlying dynamics by inferring an interaction graph representing the agents' relations. We ensure an interpretable interaction graph by grounding the relational latent space into semantic behaviors defined with expert domain knowledge. We demonstrate that it can model interactive traffic scenarios under both simulation and real-world settings, and generate interpretable graphs explaining the vehicle's behavior by their interactions.

</p>
</details>

<details><summary><b>Quantum Cross Entropy and Maximum Likelihood Principle</b>
<a href="https://arxiv.org/abs/2102.11887">arxiv:2102.11887</a>
&#x1F4C8; 4 <br>
<p>Zhou Shangnan, Yixu Wang</p></summary>
<p>

**Abstract:** Quantum machine learning is an emerging field at the intersection of machine learning and quantum computing. Classical cross entropy plays a central role in machine learning. We define its quantum generalization, the quantum cross entropy, prove its lower bounds, and investigate its relation to quantum fidelity. In the classical case, minimizing cross entropy is equivalent to maximizing likelihood. In the quantum case, when the quantum cross entropy is constructed from quantum data undisturbed by quantum measurements, this relation holds. Classical cross entropy is equal to negative log-likelihood. When we obtain quantum cross entropy through empirical density matrix based on measurement outcomes, the quantum cross entropy is lower-bounded by negative log-likelihood. These two different scenarios illustrate the information loss when making quantum measurements. We conclude that to achieve the goal of full quantum machine learning, it is crucial to utilize the deferred measurement principle.

</p>
</details>

<details><summary><b>Probabilistic Spatial Analysis in Quantitative Microscopy with Uncertainty-Aware Cell Detection using Deep Bayesian Regression of Density Maps</b>
<a href="https://arxiv.org/abs/2102.11865">arxiv:2102.11865</a>
&#x1F4C8; 4 <br>
<p>Alvaro Gomariz, Tiziano Portenier, CÃ©sar Nombela-Arrieta, Orcun Goksel</p></summary>
<p>

**Abstract:** 3D microscopy is key in the investigation of diverse biological systems, and the ever increasing availability of large datasets demands automatic cell identification methods that not only are accurate, but also can imply the uncertainty in their predictions to inform about potential errors and hence confidence in conclusions using them. While conventional deep learning methods often yield deterministic results, advances in deep Bayesian learning allow for accurate predictions with a probabilistic interpretation in numerous image classification and segmentation tasks. It is however nontrivial to extend such Bayesian methods to cell detection, which requires specialized learning frameworks. In particular, regression of density maps is a popular successful approach for extracting cell coordinates from local peaks in a postprocessing step, which hinders any meaningful probabilistic output. We herein propose a deep learning-based cell detection framework that can operate on large microscopy images and outputs desired probabilistic predictions by (i) integrating Bayesian techniques for the regression of uncertainty-aware density maps, where peak detection can be applied to generate cell proposals, and (ii) learning a mapping from the numerous proposals to a probabilistic space that is calibrated, i.e. accurately represents the chances of a successful prediction. Utilizing such calibrated predictions, we propose a probabilistic spatial analysis with Monte-Carlo sampling. We demonstrate this in revising an existing description of the distribution of a mesenchymal stromal cell type within the bone marrow, where our proposed methods allow us to reveal spatial patterns that are otherwise undetectable. Introducing such probabilistic analysis in quantitative microscopy pipelines will allow for reporting confidence intervals for testing biological hypotheses of spatial distributions.

</p>
</details>

<details><summary><b>Automated Discovery of Adaptive Attacks on Adversarial Defenses</b>
<a href="https://arxiv.org/abs/2102.11860">arxiv:2102.11860</a>
&#x1F4C8; 4 <br>
<p>Chengyuan Yao, Pavol Bielik, Petar Tsankov, Martin Vechev</p></summary>
<p>

**Abstract:** Reliable evaluation of adversarial defenses is a challenging task, currently limited to an expert who manually crafts attacks that exploit the defense's inner workings or approaches based on an ensemble of fixed attacks, none of which may be effective for the specific defense at hand. Our key observation is that adaptive attacks are composed of reusable building blocks that can be formalized in a search space and used to automatically discover attacks for unknown defenses. We evaluated our approach on 24 adversarial defenses and show that it outperforms AutoAttack, the current state-of-the-art tool for reliable evaluation of adversarial defenses: our tool discovered significantly stronger attacks by producing 3.0\%-50.8\% additional adversarial examples for 10 models, while obtaining attacks with slightly stronger or similar strength for the remaining models.

</p>
</details>

<details><summary><b>Quantum Entropic Causal Inference</b>
<a href="https://arxiv.org/abs/2102.11764">arxiv:2102.11764</a>
&#x1F4C8; 4 <br>
<p>Mohammad Ali Javidian, Vaneet Aggarwal, Fanglin Bao, Zubin Jacob</p></summary>
<p>

**Abstract:** The class of problems in causal inference which seeks to isolate causal correlations solely from observational data even without interventions has come to the forefront of machine learning, neuroscience and social sciences. As new large scale quantum systems go online, it opens interesting questions of whether a quantum framework exists on isolating causal correlations without any interventions on a quantum system. We put forth a theoretical framework for merging quantum information science and causal inference by exploiting entropic principles. At the root of our approach is the proposition that the true causal direction minimizes the entropy of exogenous variables in a non-local hidden variable theory. The proposed framework uses a quantum causal structural equation model to build the connection between two fields: entropic causal inference and the quantum marginal problem. First, inspired by the definition of geometric quantum discord, we fill the gap between classical and quantum conditional density matrices to define quantum causal models. Subsequently, using a greedy approach, we develop a scalable algorithm for quantum entropic causal inference unifying classical and quantum causality in a principled way. We apply our proposed algorithm to an experimentally relevant scenario of identifying the subsystem impacted by noise starting from an entangled state. This successful inference on a synthetic quantum dataset can have practical applications in identifying originators of malicious activity on future multi-node quantum networks as well as quantum error correction. As quantum datasets and systems grow in complexity, our framework can play a foundational role in bringing observational causal inference from the classical to the quantum domain.

</p>
</details>

<details><summary><b>A Simulation-Based Test of Identifiability for Bayesian Causal Inference</b>
<a href="https://arxiv.org/abs/2102.11761">arxiv:2102.11761</a>
&#x1F4C8; 4 <br>
<p>Sam Witty, David Jensen, Vikash Mansinghka</p></summary>
<p>

**Abstract:** This paper introduces a procedure for testing the identifiability of Bayesian models for causal inference. Although the do-calculus is sound and complete given a causal graph, many practical assumptions cannot be expressed in terms of graph structure alone, such as the assumptions required by instrumental variable designs, regression discontinuity designs, and within-subjects designs. We present simulation-based identifiability (SBI), a fully automated identification test based on a particle optimization scheme with simulated observations. This approach expresses causal assumptions as priors over functions in a structural causal model, including flexible priors using Gaussian processes. We prove that SBI is asymptotically sound and complete, and produces practical finite-sample bounds. We also show empirically that SBI agrees with known results in graph-based identification as well as with widely-held intuitions for designs in which graph-based methods are inconclusive.

</p>
</details>

<details><summary><b>EBMs Trained with Maximum Likelihood are Generator Models Trained with a Self-adverserial Loss</b>
<a href="https://arxiv.org/abs/2102.11757">arxiv:2102.11757</a>
&#x1F4C8; 4 <br>
<p>Zhisheng Xiao, Qing Yan, Yali Amit</p></summary>
<p>

**Abstract:** Maximum likelihood estimation is widely used in training Energy-based models (EBMs). Training requires samples from an unnormalized distribution, which is usually intractable, and in practice, these are obtained by MCMC algorithms such as Langevin dynamics. However, since MCMC in high-dimensional space converges extremely slowly, the current understanding of maximum likelihood training, which assumes approximate samples from the model can be drawn, is problematic. In this paper, we try to understand this training procedure by replacing Langevin dynamics with deterministic solutions of the associated gradient descent ODE. Doing so allows us to study the density induced by the dynamics (if the dynamics are invertible), and connect with GANs by treating the dynamics as generator models, the initial values as latent variables and the loss as optimizing a critic defined by the very same energy that determines the generator through its gradient. Hence the term - self-adversarial loss. We show that reintroducing the noise in the dynamics does not lead to a qualitative change in the behavior, and merely reduces the quality of the generator. We thus show that EBM training is effectively a self-adversarial procedure rather than maximum likelihood estimation.

</p>
</details>

<details><summary><b>Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed</b>
<a href="https://arxiv.org/abs/2102.11742">arxiv:2102.11742</a>
&#x1F4C8; 4 <br>
<p>Maria Refinetti, Sebastian Goldt, Florent Krzakala, Lenka ZdeborovÃ¡</p></summary>
<p>

**Abstract:** A recent series of theoretical works showed that the dynamics of neural networks with a certain initialisation are well-captured by kernel methods. Concurrent empirical work demonstrated that kernel methods can come close to the performance of neural networks on some image classification tasks. These results raise the question of whether neural networks only learn successfully if kernels also learn successfully, despite neural networks being more expressive. Here, we show theoretically that two-layer neural networks (2LNN) with only a few hidden neurons can beat the performance of kernel learning on a simple Gaussian mixture classification task. We study the high-dimensional limit where the number of samples is linearly proportional to the input dimension, and show that while small 2LNN achieve near-optimal performance on this task, lazy training approaches such as random features and kernel methods do not. Our analysis is based on the derivation of a closed set of equations that track the learning dynamics of the 2LNN and thus allow to extract the asymptotic performance of the network as a function of signal-to-noise ratio and other hyperparameters. We finally illustrate how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.

</p>
</details>

<details><summary><b>Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization</b>
<a href="https://arxiv.org/abs/2102.11537">arxiv:2102.11537</a>
&#x1F4C8; 4 <br>
<p>Peiyuan Zhang, Antonio Orvieto, Hadi Daneshmand, Thomas Hofmann, Roy Smith</p></summary>
<p>

**Abstract:** Viewing optimization methods as numerical integrators for ordinary differential equations (ODEs) provides a thought-provoking modern framework for studying accelerated first-order optimizers. In this literature, acceleration is often supposed to be linked to the quality of the integrator (accuracy, energy preservation, symplecticity). In this work, we propose a novel ordinary differential equation that questions this connection: both the explicit and the semi-implicit (a.k.a symplectic) Euler discretizations on this ODE lead to an accelerated algorithm for convex programming. Although semi-implicit methods are well-known in numerical analysis to enjoy many desirable features for the integration of physical systems, our findings show that these properties do not necessarily relate to acceleration.

</p>
</details>

<details><summary><b>Comparative evaluation of CNN architectures for Image Caption Generation</b>
<a href="https://arxiv.org/abs/2102.11506">arxiv:2102.11506</a>
&#x1F4C8; 4 <br>
<p>Sulabh Katiyar, Samir Kumar Borgohain</p></summary>
<p>

**Abstract:** Aided by recent advances in Deep Learning, Image Caption Generation has seen tremendous progress over the last few years. Most methods use transfer learning to extract visual information, in the form of image features, with the help of pre-trained Convolutional Neural Network models followed by transformation of the visual information using a Caption Generator module to generate the output sentences. Different methods have used different Convolutional Neural Network Architectures and, to the best of our knowledge, there is no systematic study which compares the relative efficacy of different Convolutional Neural Network architectures for extracting the visual information. In this work, we have evaluated 17 different Convolutional Neural Networks on two popular Image Caption Generation frameworks: the first based on Neural Image Caption (NIC) generation model and the second based on Soft-Attention framework. We observe that model complexity of Convolutional Neural Network, as measured by number of parameters, and the accuracy of the model on Object Recognition task does not necessarily co-relate with its efficacy on feature extraction for Image Caption Generation task.

</p>
</details>

<details><summary><b>Sleep Apnea and Respiratory Anomaly Detection from a Wearable Band and Oxygen Saturation</b>
<a href="https://arxiv.org/abs/2102.13473">arxiv:2102.13473</a>
&#x1F4C8; 3 <br>
<p>Wolfgang Ganglberger, Abigail A. Bucklin, Ryan A. Tesh, Madalena Da Silva Cardoso, Haoqi Sun, Michael J. Leone, Luis Paixao, Ezhil Panneerselvam, Elissa M. Ye, B. Taylor Thompson, Oluwaseun Akeju, David Kuller, Robert J. Thomas, M. Brandon Westover</p></summary>
<p>

**Abstract:** Objective: Sleep related respiratory abnormalities are typically detected using polysomnography. There is a need in general medicine and critical care for a more convenient method to automatically detect sleep apnea from a simple, easy-to-wear device. The objective is to automatically detect abnormal respiration and estimate the Apnea-Hypopnea-Index (AHI) with a wearable respiratory device, compared to an SpO2 signal or polysomnography using a large (n = 412) dataset serving as ground truth. Methods: Simultaneously recorded polysomnographic (PSG) and wearable respiratory effort data were used to train and evaluate models in a cross-validation fashion. Time domain and complexity features were extracted, important features were identified, and a random forest model employed to detect events and predict AHI. Four models were trained: one each using the respiratory features only, a feature from the SpO2 (%)-signal only, and two additional models that use the respiratory features and the SpO2 (%)-feature, one allowing a time lag of 30 seconds between the two signals. Results: Event-based classification resulted in areas under the receiver operating characteristic curves of 0.94, 0.86, 0.82, and areas under the precision-recall curves of 0.48, 0.32, 0.51 for the models using respiration and SpO2, respiration-only, and SpO2-only respectively. Correlation between expert-labelled and predicted AHI was 0.96, 0.78, and 0.93, respectively. Conclusions: A wearable respiratory effort signal with or without SpO2 predicted AHI accurately. Given the large dataset and rigorous testing design, we expect our models are generalizable to evaluating respiration in a variety of environments, such as at home and in critical care.

</p>
</details>

<details><summary><b>Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography</b>
<a href="https://arxiv.org/abs/2102.12040">arxiv:2102.12040</a>
&#x1F4C8; 3 <br>
<p>Xuefeng Du, Haohan Wang, Zhenxi Zhu, Xiangrui Zeng, Yi-Wei Chang, Jing Zhang, Min Xu</p></summary>
<p>

**Abstract:** Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that visualizes the structural and spatial organization of macromolecules at a near-native state in single cells, which has broad applications in life science. However, the systematic structural recognition and recovery of macromolecules captured by cryo-ET are difficult due to high structural complexity and imaging limits. Deep learning based subtomogram classification have played critical roles for such tasks. As supervised approaches, however, their performance relies on sufficient and laborious annotation on a large training dataset.
  Results: To alleviate this major labeling burden, we proposed a Hybrid Active Learning (HAL) framework for querying subtomograms for labelling from a large unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select the subtomograms that have the most uncertain predictions. Moreover, to mitigate the sampling bias caused by such strategy, a discriminator is introduced to judge if a certain subtomogram is labeled or unlabeled and subsequently the model queries the subtomogram that have higher probabilities to be unlabeled. Additionally, HAL introduces a subset sampling strategy to improve the diversity of the query set, so that the information overlap is decreased between the queried batches and the algorithmic efficiency is improved. Our experiments on subtomogram classification tasks using both simulated and real data demonstrate that we can achieve comparable testing performance (on average only 3% accuracy drop) by using less than 30% of the labeled subtomograms, which shows a very promising result for subtomogram classification task with limited labeling resources.

</p>
</details>

<details><summary><b>Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.12017">arxiv:2102.12017</a>
&#x1F4C8; 3 <br>
<p>Isaac J. Sledge, Darshan W. Bryner, Jose C. Principe</p></summary>
<p>

**Abstract:** Reinforcement learning in large-scale environments is challenging due to the many possible actions that can be taken in specific situations. We have previously developed a means of constraining, and hence speeding up, the search process through the use of motion primitives; motion primitives are sequences of pre-specified actions taken across a state series. As a byproduct of this work, we have found that if the motion primitives' motions and actions are labeled, then the search can be sped up further. Since motion primitives may initially lack such details, we propose a theoretically viewpoint-insensitive and speed-insensitive means of automatically annotating the underlying motions and actions. We do this through a differential-geometric, spatio-temporal kinematics descriptor, which analyzes how the poses of entities in two motion sequences change over time. We use this descriptor in conjunction with a weighted-nearest-neighbor classifier to label the primitives using a limited set of training examples.
  In our experiments, we achieve high motion and action annotation rates for human-action-derived primitives with as few as one training sample. We also demonstrate that reinforcement learning using accurately labeled trajectories leads to high-performing policies more quickly than standard reinforcement learning techniques. This is partly because motion primitives encode prior domain knowledge and preempt the need to re-discover that knowledge during training. It is also because agents can leverage the labels to systematically ignore action classes that do not facilitate task objectives, thereby reducing the action space.

</p>
</details>

<details><summary><b>Improving Deep Learning Sound Events Classifiers using Gram Matrix Feature-wise Correlations</b>
<a href="https://arxiv.org/abs/2102.11771">arxiv:2102.11771</a>
&#x1F4C8; 3 <br>
<p>Antonio Joia Neto, Andre G C Pacheco, Diogo C Luvizon</p></summary>
<p>

**Abstract:** In this paper, we propose a new Sound Event Classification (SEC) method which is inspired in recent works for out-of-distribution detection. In our method, we analyse all the activations of a generic CNN in order to produce feature representations using Gram Matrices. The similarity metrics are evaluated considering all possible classes, and the final prediction is defined as the class that minimizes the deviation with respect to the features seeing during training. The proposed approach can be applied to any CNN and our experimental evaluation of four different architectures on two datasets demonstrated that our method consistently improves the baseline models.

</p>
</details>

<details><summary><b>Paraphrases do not explain word analogies</b>
<a href="https://arxiv.org/abs/2102.11749">arxiv:2102.11749</a>
&#x1F4C8; 3 <br>
<p>Louis Fournier, Ewan Dunbar</p></summary>
<p>

**Abstract:** Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between "jump" and "jumped" will be in a similar direction to that of "walk" and "walked," and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically.

</p>
</details>

<details><summary><b>Uncertainty-aware Generalized Adaptive CycleGAN</b>
<a href="https://arxiv.org/abs/2102.11747">arxiv:2102.11747</a>
&#x1F4C8; 3 <br>
<p>Uddeshya Upadhyay, Yanbei Chen, Zeynep Akata</p></summary>
<p>

**Abstract:** Unpaired image-to-image translation refers to learning inter-image-domain mapping in an unsupervised manner. Existing methods often learn deterministic mappings without explicitly modelling the robustness to outliers or predictive uncertainty, leading to performance degradation when encountering unseen out-of-distribution (OOD) patterns at test time. To address this limitation, we propose a novel probabilistic method called Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC), which models the per-pixel residual by generalized Gaussian distribution, capable of modelling heavy-tailed distributions. We compare our model with a wide variety of state-of-the-art methods on two challenging tasks: unpaired image denoising in the natural image and unpaired modality prorogation in medical image domains. Experimental results demonstrate that our model offers superior image generation quality compared to recent methods in terms of quantitative metrics such as signal-to-noise ratio and structural similarity. Our model also exhibits stronger robustness towards OOD test data.

</p>
</details>

<details><summary><b>Cell abundance aware deep learning for cell detection on highly imbalanced pathological data</b>
<a href="https://arxiv.org/abs/2102.11677">arxiv:2102.11677</a>
&#x1F4C8; 3 <br>
<p>Yeman Brhane Hagos, Catherine SY Lecat, Dominic Patel, Lydia Lee, Thien-An Tran, Manuel Rodriguez- Justo, Kwee Yong, Yinyin Yuan</p></summary>
<p>

**Abstract:** Automated analysis of tissue sections allows a better understanding of disease biology and may reveal biomarkers that could guide prognosis or treatment selection. In digital pathology, less abundant cell types can be of biological significance, but their scarcity can result in biased and sub-optimal cell detection model. To minimize the effect of cell imbalance on cell detection, we proposed a deep learning pipeline that considers the abundance of cell types during model training. Cell weight images were generated, which assign larger weights to less abundant cells and used the weights to regularize Dice overlap loss function. The model was trained and evaluated on myeloma bone marrow trephine samples. Our model obtained a cell detection F1-score of 0.78, a 2% increase compared to baseline models, and it outperformed baseline models at detecting rare cell types. We found that scaling deep learning loss function by the abundance of cells improves cell detection performance. Our results demonstrate the importance of incorporating domain knowledge on deep learning methods for pathological data with class imbalance.

</p>
</details>

<details><summary><b>Deterministic Neural Networks with Inductive Biases Capture Epistemic and Aleatoric Uncertainty</b>
<a href="https://arxiv.org/abs/2102.11582">arxiv:2102.11582</a>
&#x1F4C8; 3 <br>
<p>Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, Yarin Gal</p></summary>
<p>

**Abstract:** We show that a single softmax neural net with minimal changes can beat the uncertainty predictions of Deep Ensembles and other more complex single-forward-pass uncertainty approaches. Standard softmax neural nets suffer from feature collapse and extrapolate arbitrarily for OoD points. This results in arbitrary softmax entropies for OoD points which can have high entropy, low, or anything in between, thus cannot capture epistemic uncertainty reliably. We prove that this failure lies at the core of "why" Deep Ensemble Uncertainty works well. Instead of using softmax entropy, we show that with appropriate inductive biases softmax neural nets trained with maximum likelihood reliably capture epistemic uncertainty through their feature-space density. This density is obtained using simple Gaussian Discriminant Analysis, but it cannot represent aleatoric uncertainty reliably. We show that it is necessary to combine feature-space density with softmax entropy to disentangle uncertainties well. We evaluate the epistemic uncertainty quality on active learning and OoD detection, achieving SOTA ~98 AUROC on CIFAR-10 vs SVHN without fine-tuning on OoD data.

</p>
</details>

<details><summary><b>Multi-Knowledge Fusion for New Feature Generation in Generalized Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2102.11566">arxiv:2102.11566</a>
&#x1F4C8; 3 <br>
<p>Hongxin Xiang, Cheng Xie, Ting Zeng, Yun Yang</p></summary>
<p>

**Abstract:** Suffering from the semantic insufficiency and domain-shift problems, most of existing state-of-the-art methods fail to achieve satisfactory results for Zero-Shot Learning (ZSL). In order to alleviate these problems, we propose a novel generative ZSL method to learn more generalized features from multi-knowledge with continuously generated new semantics in semantic-to-visual embedding. In our approach, the proposed Multi-Knowledge Fusion Network (MKFNet) takes different semantic features from multi-knowledge as input, which enables more relevant semantic features to be trained for semantic-to-visual embedding, and finally generates more generalized visual features by adaptively fusing visual features from different knowledge domain. The proposed New Feature Generator (NFG) with adaptive genetic strategy is used to enrich semantic information on the one hand, and on the other hand it greatly improves the intersection of visual feature generated by MKFNet and unseen visual faetures. Empirically, we show that our approach can achieve significantly better performance compared to existing state-of-the-art methods on a large number of benchmarks for several ZSL tasks, including traditional ZSL, generalized ZSL and zero-shot retrieval.

</p>
</details>

<details><summary><b>SISE-PC: Semi-supervised Image Subsampling for Explainable Pathology</b>
<a href="https://arxiv.org/abs/2102.11560">arxiv:2102.11560</a>
&#x1F4C8; 3 <br>
<p>Sohini Roychowdhury, Kwok Sun Tang, Mohith Ashok, Anoop Sanka</p></summary>
<p>

**Abstract:** Although automated pathology classification using deep learning (DL) has proved to be predictively efficient, DL methods are found to be data and compute cost intensive. In this work, we aim to reduce DL training costs by pre-training a Resnet feature extractor using SimCLR contrastive loss for latent encoding of OCT images. We propose a novel active learning framework that identifies a minimal sub-sampled dataset containing the most uncertain OCT image samples using label propagation on the SimCLR latent encodings. The pre-trained Resnet model is then fine-tuned with the labelled minimal sub-sampled data and the underlying pathological sites are visually explained. Our framework identifies upto 2% of OCT images to be most uncertain that need prioritized specialist attention and that can fine-tune a Resnet model to achieve upto 97% classification accuracy. The proposed method can be extended to other medical images to minimize prediction costs.

</p>
</details>

<details><summary><b>Model-Attentive Ensemble Learning for Sequence Modeling</b>
<a href="https://arxiv.org/abs/2102.11500">arxiv:2102.11500</a>
&#x1F4C8; 3 <br>
<p>Victor D. Bourgin, Ioana Bica, Mihaela van der Schaar</p></summary>
<p>

**Abstract:** Medical time-series datasets have unique characteristics that make prediction tasks challenging. Most notably, patient trajectories often contain longitudinal variations in their input-output relationships, generally referred to as temporal conditional shift. Designing sequence models capable of adapting to such time-varying distributions remains a prevailing problem. To address this we present Model-Attentive Ensemble learning for Sequence modeling (MAES). MAES is a mixture of time-series experts which leverages an attention-based gating mechanism to specialize the experts on different sequence dynamics and adaptively weight their predictions. We demonstrate that MAES significantly out-performs popular sequence models on datasets subject to temporal shift.

</p>
</details>

<details><summary><b>The Power of $D$-hops in Matching Power-Law Graphs</b>
<a href="https://arxiv.org/abs/2102.12975">arxiv:2102.12975</a>
&#x1F4C8; 2 <br>
<p>Liren Yu, Jiaming Xu, Xiaojun Lin</p></summary>
<p>

**Abstract:** This paper studies seeded graph matching for power-law graphs. Assume that two edge-correlated graphs are independently edge-sampled from a common parent graph with a power-law degree distribution. A set of correctly matched vertex-pairs is chosen at random and revealed as initial seeds. Our goal is to use the seeds to recover the remaining latent vertex correspondence between the two graphs. Departing from the existing approaches that focus on the use of high-degree seeds in $1$-hop neighborhoods, we develop an efficient algorithm that exploits the low-degree seeds in suitably-defined $D$-hop neighborhoods. Specifically, we first match a set of vertex-pairs with appropriate degrees (which we refer to as the first slice) based on the number of low-degree seeds in their $D$-hop neighborhoods. This significantly reduces the number of initial seeds needed to trigger a cascading process to match the rest of the graphs. Under the Chung-Lu random graph model with $n$ vertices, max degree $Î(\sqrt{n})$, and the power-law exponent $2<Î²<3$, we show that as soon as $D> \frac{4-Î²}{3-Î²}$, by optimally choosing the first slice, with high probability our algorithm can correctly match a constant fraction of the true pairs without any error, provided with only $Î©((\log n)^{4-Î²})$ initial seeds. Our result achieves an exponential reduction in the seed size requirement, as the best previously known result requires $n^{1/2+Îµ}$ seeds (for any small constant $Îµ>0$). Performance evaluation with synthetic and real data further corroborates the improved performance of our algorithm.

</p>
</details>

<details><summary><b>Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing</b>
<a href="https://arxiv.org/abs/2102.12060">arxiv:2102.12060</a>
&#x1F4C8; 2 <br>
<p>Sarah Wiegreffe, Ana MarasoviÄ</p></summary>
<p>

**Abstract:** Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.

</p>
</details>

<details><summary><b>Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation</b>
<a href="https://arxiv.org/abs/2102.12056">arxiv:2102.12056</a>
&#x1F4C8; 2 <br>
<p>Changfa Shi, Min Xian, Xiancheng Zhou, Haotian Wang, Heng-Da Cheng</p></summary>
<p>

**Abstract:** Liver segmentation from abdominal CT images is an essential step for liver cancer computer-aided diagnosis and surgical planning. However, both the accuracy and robustness of existing liver segmentation methods cannot meet the requirements of clinical applications. In particular, for the common clinical cases where the liver tissue contains major pathology, current segmentation methods show poor performance. In this paper, we propose a novel low-rank tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that achieves accurate and robust pathological liver segmentation of CT images. Firstly, we propose a multi-slice LRTD scheme to recover the underlying low-rank structure embedded in 3D medical images. It performs the LRTD on small image segments consisting of multiple consecutive image slices. Then, we present an LRTD-based atlas construction method to generate tumor-free liver atlases that mitigates the performance degradation of liver segmentation due to the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to derive patient-specific liver atlases for each test image, and to achieve accurate pairwise image registration and label propagation. Extensive experiments on three public databases of pathological liver cases validate the effectiveness of the proposed method. Both qualitative and quantitative results demonstrate that, in the presence of major pathology, the proposed method is more accurate and robust than state-of-the-art methods.

</p>
</details>

<details><summary><b>The Sensitivity of Word Embeddings-based Author Detection Models to Semantic-preserving Adversarial Perturbations</b>
<a href="https://arxiv.org/abs/2102.11917">arxiv:2102.11917</a>
&#x1F4C8; 2 <br>
<p>Jeremiah Duncan, Fabian Fallas, Chris Gropp, Emily Herron, Maria Mahbub, Paula Olaya, Eduardo Ponce, Tabitha K. Samuel, Daniel Schultz, Sudarshan Srinivasan, Maofeng Tang, Viktor Zenkov, Quan Zhou, Edmon Begoli</p></summary>
<p>

**Abstract:** Authorship analysis is an important subject in the field of natural language processing. It allows the detection of the most likely writer of articles, news, books, or messages. This technique has multiple uses in tasks related to authorship attribution, detection of plagiarism, style analysis, sources of misinformation, etc. The focus of this paper is to explore the limitations and sensitiveness of established approaches to adversarial manipulations of inputs. To this end, and using those established techniques, we first developed an experimental frame-work for author detection and input perturbations. Next, we experimentally evaluated the performance of the authorship detection model to a collection of semantic-preserving adversarial perturbations of input narratives. Finally, we compare and analyze the effects of different perturbation strategies, input and model configurations, and the effects of these on the author detection model.

</p>
</details>

<details><summary><b>Neural ranking models for document retrieval</b>
<a href="https://arxiv.org/abs/2102.11903">arxiv:2102.11903</a>
&#x1F4C8; 2 <br>
<p>Mohamed Trabelsi, Zhiyu Chen, Brian D. Davison, Jeff Heflin</p></summary>
<p>

**Abstract:** Ranking models are the main components of information retrieval systems. Several approaches to ranking are based on traditional machine learning algorithms using a set of hand-crafted features. Recently, researchers have leveraged deep learning models in information retrieval. These models are trained end-to-end to extract features from the raw data for ranking tasks, so that they overcome the limitations of hand-crafted features. A variety of deep learning models have been proposed, and each model presents a set of neural network components to extract features that are used for ranking. In this paper, we compare the proposed models in the literature along different dimensions in order to understand the major contributions and limitations of each model. In our discussion of the literature, we analyze the promising neural components, and propose future research directions. We also show the analogy between document retrieval and other retrieval tasks where the items to be ranked are structured documents, answers, images and videos.

</p>
</details>

<details><summary><b>An Explainable Artificial Intelligence Approach for Unsupervised Fault Detection and Diagnosis in Rotating Machinery</b>
<a href="https://arxiv.org/abs/2102.11848">arxiv:2102.11848</a>
&#x1F4C8; 2 <br>
<p>Lucas Costa Brito, Gian Antonio Susto, Jorge Nei Brito, Marcus Antonio Viana Duarte</p></summary>
<p>

**Abstract:** The monitoring of rotating machinery is an essential task in today's production processes. Currently, several machine learning and deep learning-based modules have achieved excellent results in fault detection and diagnosis. Nevertheless, to further increase user adoption and diffusion of such technologies, users and human experts must be provided with explanations and insights by the modules. Another issue is related, in most cases, with the unavailability of labeled historical data that makes the use of supervised models unfeasible. Therefore, a new approach for fault detection and diagnosis in rotating machinery is here proposed. The methodology consists of three parts: feature extraction, fault detection and fault diagnosis. In the first part, the vibration features in the time and frequency domains are extracted. Secondly, in the fault detection, the presence of fault is verified in an unsupervised manner based on anomaly detection algorithms. The modularity of the methodology allows different algorithms to be implemented. Finally, in fault diagnosis, Shapley Additive Explanations (SHAP), a technique to interpret black-box models, is used. Through the feature importance ranking obtained by the model explainability, the fault diagnosis is performed. Two tools for diagnosis are proposed, namely: unsupervised classification and root cause analysis. The effectiveness of the proposed approach is shown on three datasets containing different mechanical faults in rotating machinery. The study also presents a comparison between models used in machine learning explainability: SHAP and Local Depth-based Feature Importance for the Isolation Forest (Local- DIFFI). Lastly, an analysis of several state-of-art anomaly detection algorithms in rotating machinery is included.

</p>
</details>

<details><summary><b>Federated Learning for Physical Layer Design</b>
<a href="https://arxiv.org/abs/2102.11777">arxiv:2102.11777</a>
&#x1F4C8; 2 <br>
<p>Ahmet M. Elbir, Anastasios K. Papazafeiropoulos, Symeon Chatzinotas</p></summary>
<p>

**Abstract:** Model-free techniques, such as machine learning (ML), have recently attracted much interest towards the physical layer design, e.g., symbol detection, channel estimation, and beamforming. Most of these ML techniques employ centralized learning (CL) schemes and assume the availability of datasets at a parameter server (PS), demanding the transmission of data from edge devices, such as mobile phones, to the PS. Exploiting the data generated at the edge, federated learning (FL) has been proposed recently as a distributed learning scheme, in which each device computes the model parameters and sends them to the PS for model aggregation while the datasets are kept intact at the edge. Thus, FL is more communication-efficient and privacy-preserving than CL and applicable to the wireless communication scenarios, wherein the data are generated at the edge devices. This article presents the recent advances in FL-based training for physical layer design problems. Compared to CL, the effectiveness of FL is presented in terms of communication overhead with a slight performance loss in the learning accuracy. The design challenges, such as model, data, and hardware complexity, are also discussed in detail along with possible solutions.

</p>
</details>

<details><summary><b>School of hard knocks: Curriculum analysis for Pommerman with a fixed computational budget</b>
<a href="https://arxiv.org/abs/2102.11762">arxiv:2102.11762</a>
&#x1F4C8; 2 <br>
<p>Omkar Shelke, Hardik Meisheri, Harshad Khadilkar</p></summary>
<p>

**Abstract:** Pommerman is a hybrid cooperative/adversarial multi-agent environment, with challenging characteristics in terms of partial observability, limited or no communication, sparse and delayed rewards, and restrictive computational time limits. This makes it a challenging environment for reinforcement learning (RL) approaches. In this paper, we focus on developing a curriculum for learning a robust and promising policy in a constrained computational budget of 100,000 games, starting from a fixed base policy (which is itself trained to imitate a noisy expert policy). All RL algorithms starting from the base policy use vanilla proximal-policy optimization (PPO) with the same reward function, and the only difference between their training is the mix and sequence of opponent policies. One expects that beginning training with simpler opponents and then gradually increasing the opponent difficulty will facilitate faster learning, leading to more robust policies compared against a baseline where all available opponent policies are introduced from the start. We test this hypothesis and show that within constrained computational budgets, it is in fact better to "learn in the school of hard knocks", i.e., against all available opponent policies nearly from the start. We also include ablation studies where we study the effect of modifying the base environment properties of ammo and bomb blast strength on the agent performance.

</p>
</details>

<details><summary><b>Weakly-supervised multi-class object localization using only object counts as labels</b>
<a href="https://arxiv.org/abs/2102.11743">arxiv:2102.11743</a>
&#x1F4C8; 2 <br>
<p>Kyle Mills, Isaac Tamblyn</p></summary>
<p>

**Abstract:** We demonstrate the use of an extensive deep neural network to localize instances of objects in images. The EDNN is naturally able to accurately perform multi-class counting using only ground truth count values as labels. Without providing any conceptual information, object annotations, or pixel segmentation information, the neural network is able to formulate its own conceptual representation of the items in the image. Using images labelled with only the counts of the objects present,the structure of the extensive deep neural network can be exploited to perform localization of the objects within the visual field. We demonstrate that a trained EDNN can be used to count objects in images much larger than those on which it was trained. In order to demonstrate our technique, we introduce seven new data sets: five progressively harder MNIST digit-counting data sets, and two datasets of 3d-rendered rubber ducks in various situations. On most of these datasets, the EDNN achieves greater than 99% test set accuracy in counting objects.

</p>
</details>

<details><summary><b>Recurrent Model Predictive Control</b>
<a href="https://arxiv.org/abs/2102.11736">arxiv:2102.11736</a>
&#x1F4C8; 2 <br>
<p>Zhengyu Liu, Jingliang Duan, Wenxuan Wang, Shengbo Eben Li, Yuming Yin, Ziyu Lin, Qi Sun, Bo Cheng</p></summary>
<p>

**Abstract:** This paper proposes an off-line algorithm, called Recurrent Model Predictive Control (RMPC), to solve general nonlinear finite-horizon optimal control problems. Unlike traditional Model Predictive Control (MPC) algorithms, it can make full use of the current computing resources and adaptively select the longest model prediction horizon. Our algorithm employs a recurrent function to approximate the optimal policy, which maps the system states and reference values directly to the control inputs. The number of prediction steps is equal to the number of recurrent cycles of the learned policy function. With an arbitrary initial policy function, the proposed RMPC algorithm can converge to the optimal policy by directly minimizing the designed loss function. We further prove the convergence and optimality of the RMPC algorithm thorough Bellman optimality principle, and demonstrate its generality and efficiency using two numerical examples.

</p>
</details>

<details><summary><b>Deep Unrolled Network for Video Super-Resolution</b>
<a href="https://arxiv.org/abs/2102.11720">arxiv:2102.11720</a>
&#x1F4C8; 2 <br>
<p>Benjamin Naoto Chiche, Arnaud Woiselle, Joana Frontera-Pons, Jean-Luc Starck</p></summary>
<p>

**Abstract:** Video super-resolution (VSR) aims to reconstruct a sequence of high-resolution (HR) images from their corresponding low-resolution (LR) versions. Traditionally, solving a VSR problem has been based on iterative algorithms that can exploit prior knowledge on image formation and assumptions on the motion. However, these classical methods struggle at incorporating complex statistics from natural images. Furthermore, VSR has recently benefited from the improvement brought by deep learning (DL) algorithms. These techniques can efficiently learn spatial patterns from large collections of images. Yet, they fail to incorporate some knowledge about the image formation model, which limits their flexibility. Unrolled optimization algorithms, developed for inverse problems resolution, allow to include prior information into deep learning architectures. They have been used mainly for single image restoration tasks. Adapting an unrolled neural network structure can bring the following benefits. First, this may increase performance of the super-resolution task. Then, this gives neural networks better interpretability. Finally, this allows flexibility in learning a single model to nonblindly deal with multiple degradations. In this paper, we propose a new VSR neural network based on unrolled optimization techniques and discuss its performance.

</p>
</details>

<details><summary><b>A Novel Deep Learning Method for Textual Sentiment Analysis</b>
<a href="https://arxiv.org/abs/2102.11651">arxiv:2102.11651</a>
&#x1F4C8; 2 <br>
<p>Hossein Sadr, Mozhdeh Nazari Solimandarabi, Mir Mohsen Pedram, Mohammad Teshnehlab</p></summary>
<p>

**Abstract:** Sentiment analysis is known as one of the most crucial tasks in the field of natural language processing and Convolutional Neural Network (CNN) is one of those prominent models that is commonly used for this aim. Although convolutional neural networks have obtained remarkable results in recent years, they are still confronted with some limitations. Firstly, they consider that all words in a sentence have equal contributions in the sentence meaning representation and are not able to extract informative words. Secondly, they require a large number of training data to obtain considerable results while they have many parameters that must be accurately adjusted. To this end, a convolutional neural network integrated with a hierarchical attention layer is proposed which is able to extract informative words and assign them higher weight. Moreover, the effect of transfer learning that transfers knowledge learned in the source domain to the target domain with the aim of improving the performance is also explored. Based on the empirical results, the proposed model not only has higher classification accuracy and can extract informative words but also applying incremental transfer learning can significantly enhance the classification performance.

</p>
</details>

<details><summary><b>Enhancing Data-Free Adversarial Distillation with Activation Regularization and Virtual Interpolation</b>
<a href="https://arxiv.org/abs/2102.11638">arxiv:2102.11638</a>
&#x1F4C8; 2 <br>
<p>Xiaoyang Qu, Jianzong Wang, Jing Xiao</p></summary>
<p>

**Abstract:** Knowledge distillation refers to a technique of transferring the knowledge from a large learned model or an ensemble of learned models to a small model. This method relies on access to the original training set, which might not always be available. A possible solution is a data-free adversarial distillation framework, which deploys a generative network to transfer the teacher model's knowledge to the student model. However, the data generation efficiency is low in the data-free adversarial distillation. We add an activation regularizer and a virtual interpolation method to improve the data generation efficiency. The activation regularizer enables the students to match the teacher's predictions close to activation boundaries and decision boundaries. The virtual interpolation method can generate virtual samples and labels in-between decision boundaries. Our experiments show that our approach surpasses state-of-the-art data-free distillation methods. The student model can achieve 95.42% accuracy on CIFAR-10 and 77.05% accuracy on CIFAR-100 without any original training data. Our model's accuracy is 13.8% higher than the state-of-the-art data-free method on CIFAR-100.

</p>
</details>

<details><summary><b>Scaling up learning with GAIT-prop</b>
<a href="https://arxiv.org/abs/2102.11598">arxiv:2102.11598</a>
&#x1F4C8; 2 <br>
<p>Sander Dalm, Nasir Ahmad, Luca Ambrogioni, Marcel van Gerven</p></summary>
<p>

**Abstract:** Backpropagation of error (BP) is a widely used and highly successful learning algorithm. However, its reliance on non-local information in propagating error gradients makes it seem an unlikely candidate for learning in the brain. In the last decade, a number of investigations have been carried out focused upon determining whether alternative more biologically plausible computations can be used to approximate BP. This work builds on such a local learning algorithm - Gradient Adjusted Incremental Target Propagation (GAIT-prop) - which has recently been shown to approximate BP in a manner which appears biologically plausible. This method constructs local, layer-wise weight update targets in order to enable plausible credit assignment. However, in deep networks, the local weight updates computed by GAIT-prop can deviate from BP for a number of reasons. Here, we provide and test methods to overcome such sources of error. In particular, we adaptively rescale the locally-computed errors and show that this significantly increases the performance and stability of the GAIT-prop algorithm when applied to the CIFAR-10 dataset.

</p>
</details>

<details><summary><b>Classification of Breast Cancer Lesions in Ultrasound Images by using Attention Layer and loss Ensembles in Deep Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2102.11519">arxiv:2102.11519</a>
&#x1F4C8; 2 <br>
<p>Elham Yousef Kalaf, Ata Jodeiri, Seyed Kamaledin Setarehdan, Ng Wei Lin, Kartini Binti Rahman, Nur Aishah Taib, Sarinder Kaur Dhillon</p></summary>
<p>

**Abstract:** Reliable classification of benign and malignant lesions in breast ultrasound images can provide an effective and relatively low cost method for early diagnosis of breast cancer. The accuracy of the diagnosis is however highly dependent on the quality of the ultrasound systems and the experience of the users (radiologists). The leverage in deep convolutional neural network approaches provided solutions in efficient analysis of breast ultrasound images. In this study, we proposed a new framework for classification of breast cancer lesions by use of an attention module in modified VGG16 architecture. We also proposed new ensembled loss function which is the combination of binary cross-entropy and logarithm of the hyperbolic cosine loss to improve the model discrepancy between classified lesions and its labels. Networks trained from pretrained ImageNet weights, and subsequently fine-tuned with ultrasound datasets. The proposed model in this study outperformed other modified VGG16 architectures with the accuracy of 93% and also the results are competitive with other state of the art frameworks for classification of breast cancer lesions. In this study, we employed transfer learning approaches with the pre-trained VGG16 architecture. Different CNN models for classification task were trained to predict benign or malignant lesions in breast ultrasound images. Our Experimental results show that the choice of loss function is highly important in classification task and by adding an attention block we could empower the performance our model.

</p>
</details>

<details><summary><b>A Quantitative Metric for Privacy Leakage in Federated Learning</b>
<a href="https://arxiv.org/abs/2102.13472">arxiv:2102.13472</a>
&#x1F4C8; 1 <br>
<p>Yong Liu, Xinghua Zhu, Jianzong Wang, Jing Xiao</p></summary>
<p>

**Abstract:** In the federated learning system, parameter gradients are shared among participants and the central modulator, while the original data never leave their protected source domain. However, the gradient itself might carry enough information for precise inference of the original data. By reporting their parameter gradients to the central server, client datasets are exposed to inference attacks from adversaries. In this paper, we propose a quantitative metric based on mutual information for clients to evaluate the potential risk of information leakage in their gradients. Mutual information has received increasing attention in the machine learning and data mining community over the past few years. However, existing mutual information estimation methods cannot handle high-dimensional variables. In this paper, we propose a novel method to approximate the mutual information between the high-dimensional gradients and batched input data. Experimental results show that the proposed metric reliably reflect the extent of information leakage in federated learning. In addition, using the proposed metric, we investigate the influential factors of risk level. It is proven that, the risk of information leakage is related to the status of the task model, as well as the inherent data distribution.

</p>
</details>

<details><summary><b>A Genetic Algorithm with Tree-structured Mutation for Hyperparameter Optimisation of Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2102.11995">arxiv:2102.11995</a>
&#x1F4C8; 1 <br>
<p>Yingfang Yuan, Wenjun Wang, Wei Pang</p></summary>
<p>

**Abstract:** In recent years, graph neural networks (GNNs) have gained increasing attention, as they possess the excellent capability of processing graph-related problems. In practice, hyperparameter optimisation (HPO) is critical for GNNs to achieve satisfactory results, but this process is costly because the evaluations of different hyperparameter settings require excessively training many GNNs. Many approaches have been proposed for HPO, which aims to identify promising hyperparameters efficiently. In particular, the genetic algorithm (GA) for HPO has been explored, which treats GNNs as a black-box model, of which only the outputs can be observed given a set of hyperparameters. However, because GNN models are sophisticated and the evaluations of hyperparameters on GNNs are expensive, GA requires advanced techniques to balance the exploration and exploitation of the search and make the optimisation more effective given limited computational resources. Therefore, we proposed a tree-structured mutation strategy for GA to alleviate this issue. Meanwhile, we reviewed the recent HPO works, which gives room for the idea of tree-structure to develop, and we hope our approach can further improve these HPO methods in the future.

</p>
</details>

<details><summary><b>Dont Just Divide; Polarize and Conquer!</b>
<a href="https://arxiv.org/abs/2102.11872">arxiv:2102.11872</a>
&#x1F4C8; 1 <br>
<p>Shivin Srivastava, Siddharth Bhatia, Lingxiao Huang, Lim Jun Heng, Kenji Kawaguchi, Vaibhav Rajan</p></summary>
<p>

**Abstract:** In data containing heterogeneous subpopulations, classification performance benefits from incorporating the knowledge of cluster structure in the classifier. Previous methods for such combined clustering and classification are either 1) classifier-specific and not generic, or 2) independently perform clustering and classifier training, which may not form clusters that can potentially benefit classifier performance. The question of how to perform clustering to improve the performance of classifiers trained on the clusters has received scant attention in previous literature, despite its importance in several real-world applications. In this paper, we design a simple and efficient classification algorithm called Clustering Aware Classification (CAC), to find clusters that are well suited for being used as training datasets by classifiers for each underlying subpopulation. Our experiments on synthetic and real benchmark datasets demonstrate the efficacy of CAC over previous methods for combined clustering and classification.

</p>
</details>

<details><summary><b>Deep Unitary Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2102.11855">arxiv:2102.11855</a>
&#x1F4C8; 1 <br>
<p>Hao-Yuan Chang, Kang L. Wang</p></summary>
<p>

**Abstract:** Deep neural networks can suffer from the exploding and vanishing activation problem, in which the networks fail to train properly because the neural signals either amplify or attenuate across the layers and become saturated. While other normalization methods aim to fix the stated problem, most of them have inference speed penalties in those applications that require running averages of the neural activations. Here we extend the unitary framework based on Lie algebra to neural networks of any dimensionalities, overcoming the major constraints of the prior arts that limit synaptic weights to be square matrices. Our proposed unitary convolutional neural networks deliver up to 32% faster inference speeds and up to 50% reduction in permanent hard disk space while maintaining competitive prediction accuracy.

</p>
</details>

<details><summary><b>Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems</b>
<a href="https://arxiv.org/abs/2102.11755">arxiv:2102.11755</a>
&#x1F4C8; 1 <br>
<p>Stefano Sarao Mannelli, Pierfrancesco Urbani</p></summary>
<p>

**Abstract:** The optimization step in many machine learning problems rarely relies on vanilla gradient descent but it is common practice to use momentum-based accelerated methods. Despite these algorithms being widely applied to arbitrary loss functions, their behaviour in generically non-convex, high dimensional landscapes is poorly understood. In this work, we use dynamical mean field theory techniques to describe analytically the average dynamics of these methods in a prototypical non-convex model: the (spiked) matrix-tensor model. We derive a closed set of equations that describe the behaviour of heavy-ball momentum and Nesterov acceleration in the infinite dimensional limit. By numerical integration of these equations, we observe that these methods speed up the dynamics but do not improve the algorithmic threshold with respect to gradient descent in the spiked model.

</p>
</details>

<details><summary><b>Multi-Space Evolutionary Search for Large-Scale Optimization</b>
<a href="https://arxiv.org/abs/2102.11693">arxiv:2102.11693</a>
&#x1F4C8; 1 <br>
<p>Liang Feng, Qingxia Shang, Yaqing Hou, Kay Chen Tan, Yew-Soon Ong</p></summary>
<p>

**Abstract:** In recent years, to improve the evolutionary algorithms used to solve optimization problems involving a large number of decision variables, many attempts have been made to simplify the problem solution space of a given problem for the evolutionary search. In the literature, the existing approaches can generally be categorized as decomposition-based methods and dimension-reduction-based methods. The former decomposes a large-scale problem into several smaller subproblems, while the latter transforms the original high-dimensional solution space into a low-dimensional space. However, it is worth noting that a given large-scale optimization problem may not always be decomposable, and it is also difficult to guarantee that the global optimum of the original problem is preserved in the reduced low-dimensional problem space. This paper thus proposes a new search paradigm, namely the multi-space evolutionary search, to enhance the existing evolutionary search methods for solving large-scale optimization problems. In contrast to existing approaches that perform an evolutionary search in a single search space, the proposed paradigm is designed to conduct a search in multiple solution spaces that are derived from the given problem, each possessing a unique landscape. The proposed paradigm makes no assumptions about the large-scale optimization problem of interest, such as that the problem is decomposable or that a certain relationship exists among the decision variables. To verify the efficacy of the proposed paradigm, comprehensive empirical studies in comparison to four state-of-the-art algorithms were conducted using the CEC2013 large-scale benchmark problems.

</p>
</details>

<details><summary><b>Models we Can Trust: Toward a Systematic Discipline of (Agent-Based) Model Interpretation and Validation</b>
<a href="https://arxiv.org/abs/2102.11615">arxiv:2102.11615</a>
&#x1F4C8; 1 <br>
<p>Gabriel Istrate</p></summary>
<p>

**Abstract:** We advocate the development of a discipline of interacting with and extracting information from models, both mathematical (e.g. game-theoretic ones) and computational (e.g. agent-based models). We outline some directions for the development of a such a discipline:
  - the development of logical frameworks for the systematic formal specification of stylized facts and social mechanisms in (mathematical and computational) social science. Such frameworks would bring to attention new issues, such as phase transitions, i.e. dramatical changes in the validity of the stylized facts beyond some critical values in parameter space. We argue that such statements are useful for those logical frameworks describing properties of ABM.
  - the adaptation of tools from the theory of reactive systems (such as bisimulation) to obtain practically relevant notions of two systems "having the same behavior".
  - the systematic development of an adversarial theory of model perturbations, that investigates the robustness of conclusions derived from models of social behavior to variations in several features of the social dynamics. These may include: activation order, the underlying social network, individual agent behavior.

</p>
</details>

<details><summary><b>EscapeWildFire: Assisting People to Escape Wildfires in Real-Time</b>
<a href="https://arxiv.org/abs/2102.11558">arxiv:2102.11558</a>
&#x1F4C8; 1 <br>
<p>Andreas Kamilaris, Jean-Baptiste Filippi, Chirag Padubidri, Jesper Provoost, Savvas Karatsiolis, Ian Cole, Wouter Couwenbergh, Evi Demetriou</p></summary>
<p>

**Abstract:** Over the past couple of decades, the number of wildfires and area of land burned around the world has been steadily increasing, partly due to climatic changes and global warming. Therefore, there is a high probability that more people will be exposed to and endangered by forest fires. Hence there is an urgent need to design pervasive systems that effectively assist people and guide them to safety during wildfires. This paper presents EscapeWildFire, a mobile application connected to a backend system which models and predicts wildfire geographical progression, assisting citizens to escape wildfires in real-time. A small pilot indicates the correctness of the system. The code is open-source; fire authorities around the world are encouraged to adopt this approach.

</p>
</details>

<details><summary><b>Oriole: Thwarting Privacy against Trustworthy Deep Learning Models</b>
<a href="https://arxiv.org/abs/2102.11502">arxiv:2102.11502</a>
&#x1F4C8; 1 <br>
<p>Liuqiao Chen, Hu Wang, Benjamin Zi Hao Zhao, Minhui Xue, Haifeng Qian</p></summary>
<p>

**Abstract:** Deep Neural Networks have achieved unprecedented success in the field of face recognition such that any individual can crawl the data of others from the Internet without their explicit permission for the purpose of training high-precision face recognition models, creating a serious violation of privacy. Recently, a well-known system named Fawkes (published in USENIX Security 2020) claimed this privacy threat can be neutralized by uploading cloaked user images instead of their original images. In this paper, we present Oriole, a system that combines the advantages of data poisoning attacks and evasion attacks, to thwart the protection offered by Fawkes, by training the attacker face recognition model with multi-cloaked images generated by Oriole. Consequently, the face recognition accuracy of the attack model is maintained and the weaknesses of Fawkes are revealed. Experimental results show that our proposed Oriole system is able to effectively interfere with the performance of the Fawkes system to achieve promising attacking results. Our ablation study highlights multiple principal factors that affect the performance of the Oriole system, including the DSSIM perturbation budget, the ratio of leaked clean user images, and the numbers of multi-cloaks for each uncloaked image. We also identify and discuss at length the vulnerabilities of Fawkes. We hope that the new methodology presented in this paper will inform the security community of a need to design more robust privacy-preserving deep learning models.

</p>
</details>

<details><summary><b>Learner-Private Convex Optimization</b>
<a href="https://arxiv.org/abs/2102.11976">arxiv:2102.11976</a>
&#x1F4C8; 0 <br>
<p>Jiaming Xu, Kuang Xu, Dana Yang</p></summary>
<p>

**Abstract:** Convex optimization with feedback is a framework where a learner relies on iterative queries and feedback to arrive at the minimizer of a convex function. It has gained considerable popularity thanks to its scalability in large-scale optimization and machine learning. The repeated interactions, however, expose the learner to privacy risks from eavesdropping adversaries that observe the submitted queries. In this paper, we study how to optimally obfuscate the learner's queries in convex optimization with first-order feedback, so that their learned optimal value is provably difficult to estimate for an eavesdropping adversary. We consider two formulations of learner privacy: a Bayesian formulation in which the convex function is drawn randomly, and a minimax formulation in which the function is fixed and the adversary's probability of error is measured with respect to a minimax criterion.
  Suppose that the learner wishes to ensure the adversary cannot estimate accurately with probability greater than $1/L$ for some $L>0$. Our main results show that the query complexity overhead is additive in $L$ in the minimax formulation, but multiplicative in $L$ in the Bayesian formulation. Compared to existing learner-private sequential learning models with binary feedback, our results apply to the significantly richer family of general convex functions with full-gradient feedback. Our proofs learn on tools from the theory of Dirichlet processes, as well as a novel strategy designed for measuring information leakage under a full-gradient oracle.

</p>
</details>

<details><summary><b>Arguments for the Unsuitability of Convolutional Neural Networks for Non--Local Tasks</b>
<a href="https://arxiv.org/abs/2102.11944">arxiv:2102.11944</a>
&#x1F4C8; 0 <br>
<p>Sebastian Stabinger, David Peer, Antonio RodrÃ­guez-SÃ¡nchez</p></summary>
<p>

**Abstract:** Convolutional neural networks have established themselves over the past years as the state of the art method for image classification, and for many datasets, they even surpass humans in categorizing images. Unfortunately, the same architectures perform much worse when they have to compare parts of an image to each other to correctly classify this image.
  Until now, no well-formed theoretical argument has been presented to explain this deficiency. In this paper, we will argue that convolutional layers are of little use for such problems, since comparison tasks are global by nature, but convolutional layers are local by design. We will use this insight to reformulate a comparison task into a sorting task and use findings on sorting networks to propose a lower bound for the number of parameters a neural network needs to solve comparison tasks in a generalizable way. We will use this lower bound to argue that attention, as well as iterative/recurrent processing, is needed to prevent a combinatorial explosion.

</p>
</details>

<details><summary><b>Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others</b>
<a href="https://arxiv.org/abs/2102.11938">arxiv:2102.11938</a>
&#x1F4C8; 0 <br>
<p>Kanishk Gandhi, Gala Stojnic, Brenden M. Lake, Moira R. Dillon</p></summary>
<p>

**Abstract:** To achieve human-like common sense about everyday life, machine learning systems must understand and reason about the goals, preferences, and actions of other agents in the environment. By the end of their first year of life, human infants intuitively achieve such common sense, and these cognitive achievements lay the foundation for humans' rich and complex understanding of the mental states of others. Can machines achieve generalizable, commonsense reasoning about other agents like human infants? The Baby Intuitions Benchmark (BIB) challenges machines to predict the plausibility of an agent's behavior based on the underlying causes of its actions. Because BIB's content and paradigm are adopted from developmental cognitive science, BIB allows for direct comparison between human and machine performance. Nevertheless, recently proposed, deep-learning-based agency reasoning models fail to show infant-like reasoning, leaving BIB an open challenge.

</p>
</details>

<details><summary><b>Greedy-Step Off-Policy Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2102.11717">arxiv:2102.11717</a>
&#x1F4C8; 0 <br>
<p>Yuhui Wang, Qingyuan Wu, Pengcheng He, Xiaoyang Tan</p></summary>
<p>

**Abstract:** Most of the policy evaluation algorithms are based on the theories of Bellman Expectation and Optimality Equation, which derive two popular approaches - Policy Iteration (PI) and Value Iteration (VI). However, multi-step bootstrapping is often at cross-purposes with and off-policy learning in PI-based methods due to the large variance of multi-step off-policy correction. In contrast, VI-based methods are naturally off-policy but subject to one-step learning.In this paper, we deduce a novel multi-step Bellman Optimality Equation by utilizing a latent structure of multi-step bootstrapping with the optimal value function. Via this new equation, we derive a new multi-step value iteration method that converges to the optimal value function with exponential contraction rate $\mathcal{O}(Î³^n)$ but only linear computational complexity. Moreover, it can naturally derive a suite of multi-step off-policy algorithms that can safely utilize data collected by arbitrary policies without correction.Experiments reveal that the proposed methods are reliable, easy to implement and achieve state-of-the-art performance on a series of standard benchmark datasets.

</p>
</details>

<details><summary><b>A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix</b>
<a href="https://arxiv.org/abs/2102.11658">arxiv:2102.11658</a>
&#x1F4C8; 0 <br>
<p>Chihiro Watanabe, Taiji Suzuki</p></summary>
<p>

**Abstract:** Biclustering is a method for detecting homogeneous submatrices in a given observed matrix, and it is an effective tool for relational data analysis. Although there are many studies that estimate the underlying bicluster structure of a matrix, few have enabled us to determine the appropriate number of biclusters in an observed matrix. Recently, a statistical test on the number of biclusters has been proposed for a regular-grid bicluster structure, where we assume that the latent bicluster structure can be represented by row-column clustering. However, when the latent bicluster structure does not satisfy such regular-grid assumption, the previous test requires a larger number of biclusters than necessary (i.e., a finer bicluster structure than necessary) for the null hypothesis to be accepted, which is not desirable in terms of interpreting the accepted bicluster structure. In this study, we propose a new statistical test on the number of biclusters that does not require the regular-grid assumption and derive the asymptotic behavior of the proposed test statistic in both null and alternative cases. We illustrate the effectiveness of the proposed method by applying it to both synthetic and practical relational data matrices.

</p>
</details>

<details><summary><b>Artificial Intelligence as an Anti-Corruption Tool (AI-ACT) -- Potentials and Pitfalls for Top-down and Bottom-up Approaches</b>
<a href="https://arxiv.org/abs/2102.11567">arxiv:2102.11567</a>
&#x1F4C8; 0 <br>
<p>Nils KÃ¶bis, Christopher Starke, Iyad Rahwan</p></summary>
<p>

**Abstract:** Corruption continues to be one of the biggest societal challenges of our time. New hope is placed in Artificial Intelligence (AI) to serve as an unbiased anti-corruption agent. Ever more available (open) government data paired with unprecedented performance of such algorithms render AI the next frontier in anti-corruption. Summarizing existing efforts to use AI-based anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance research and policy. It outlines why AI presents a unique tool for top-down and bottom-up anti-corruption approaches. For both approaches, we outline in detail how AI-ACT present different potentials and pitfalls for (a) input data, (b) algorithmic design, and (c) institutional implementation. Finally, we venture a look into the future and flesh out key questions that need to be addressed to develop AI-ACT while considering citizens' views, hence putting "society in the loop".

</p>
</details>


{% endraw %}
Prev: [2021.02.22]({{ '/2021/02/22/2021.02.22.html' | relative_url }})  Next: [2021.02.24]({{ '/2021/02/24/2021.02.24.html' | relative_url }})