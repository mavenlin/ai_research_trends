## Summary for 2021-03-12, created on 2021-12-23


<details><summary><b>A Unified Game-Theoretic Interpretation of Adversarial Robustness</b>
<a href="https://arxiv.org/abs/2103.07364">arxiv:2103.07364</a>
&#x1F4C8; 46 <br>
<p>Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, Quanshi Zhang</p></summary>
<p>

**Abstract:** This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing defense methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features.

</p>
</details>

<details><summary><b>Approximating How Single Head Attention Learns</b>
<a href="https://arxiv.org/abs/2103.07601">arxiv:2103.07601</a>
&#x1F4C8; 45 <br>
<p>Charlie Snell, Ruiqi Zhong, Dan Klein, Jacob Steinhardt</p></summary>
<p>

**Abstract:** Why do models often attend to salient words, and how does this evolve throughout training? We approximate model training as a two stage process: early on in training when the attention weights are uniform, the model learns to translate individual input word `i` to `o` if they co-occur frequently. Later, the model learns to attend to `i` while the correct output is $o$ because it knows `i` translates to `o`. To formalize, we define a model property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i` translates to `o`), and claim that it drives the learning of the attention. This claim is supported by the fact that before the attention mechanism is learned, KTIW can be learned from word co-occurrence statistics, but not the other way around. Particularly, we can construct a training distribution that makes KTIW hard to learn, the learning of the attention fails, and the model cannot even learn the simple task of copying the input words to the output. Our approximation explains why models sometimes attend to salient words, and inspires a toy example where a multi-head attention model can overcome the above hard training distribution by improving learning dynamics rather than expressiveness. We end by discussing the limitation of our approximation framework and suggest future directions.

</p>
</details>

<details><summary><b>Genetic Improvement of Routing Protocols for Delay Tolerant Networks</b>
<a href="https://arxiv.org/abs/2103.07428">arxiv:2103.07428</a>
&#x1F4C8; 26 <br>
<p>Michela Lorandi, Leonardo Lucio Custode, Giovanni Iacca</p></summary>
<p>

**Abstract:** Routing plays a fundamental role in network applications, but it is especially challenging in Delay Tolerant Networks (DTNs). These are a kind of mobile ad hoc networks made of e.g. (possibly, unmanned) vehicles and humans where, despite a lack of continuous connectivity, data must be transmitted while the network conditions change due to the nodes' mobility. In these contexts, routing is NP-hard and is usually solved by heuristic "store and forward" replication-based approaches, where multiple copies of the same message are moved and stored across nodes in the hope that at least one will reach its destination. Still, the existing routing protocols produce relatively low delivery probabilities. Here, we genetically improve two routing protocols widely adopted in DTNs, namely Epidemic and PRoPHET, in the attempt to optimize their delivery probability. First, we dissect them into their fundamental components, i.e., functionalities such as checking if a node can transfer data, or sending messages to all connections. Then, we apply Genetic Improvement (GI) to manipulate these components as terminal nodes of evolving trees. We apply this methodology, in silico, to six test cases of urban networks made of hundreds of nodes, and find that GI produces consistent gains in delivery probability in four cases. We then verify if this improvement entails a worsening of other relevant network metrics, such as latency and buffer time. Finally, we compare the logics of the best evolved protocols with those of the baseline protocols, and we discuss the generalizability of the results across test cases.

</p>
</details>

<details><summary><b>Latent Space Explorations of Singing Voice Synthesis using DDSP</b>
<a href="https://arxiv.org/abs/2103.07197">arxiv:2103.07197</a>
&#x1F4C8; 22 <br>
<p>Juan Alonso, Cumhur Erkut</p></summary>
<p>

**Abstract:** Machine learning based singing voice models require large datasets and lengthy training times. In this work we present a lightweight architecture, based on the Differentiable Digital Signal Processing (DDSP) library, that is able to output song-like utterances conditioned only on pitch and amplitude, after twelve hours of training using small datasets of unprocessed audio. The results are promising, as both the melody and the singer's voice are recognizable. In addition, we present two zero-configuration tools to train new models and experiment with them. Currently we are exploring the latent space representation, which is included in the DDSP library, but not in the original DDSP examples. Our results indicate that the latent space improves both the identification of the singer as well as the comprehension of the lyrics. Our code is available at https://github.com/juanalonso/DDSP-singing-experiments with links to the zero-configuration notebooks, and our sound examples are at https://juanalonso.github.io/DDSP-singing-experiments/ .

</p>
</details>

<details><summary><b>Text Mining of Stocktwits Data for Predicting Stock Prices</b>
<a href="https://arxiv.org/abs/2103.16388">arxiv:2103.16388</a>
&#x1F4C8; 13 <br>
<p>Mukul Jaggi, Priyanka Mandal, Shreya Narang, Usman Naseem, Matloob Khushi</p></summary>
<p>

**Abstract:** Stock price prediction can be made more efficient by considering the price fluctuations and understanding the sentiments of people. A limited number of models understand financial jargon or have labelled datasets concerning stock price change. To overcome this challenge, we introduced FinALBERT, an ALBERT based model trained to handle financial domain text classification tasks by labelling Stocktwits text data based on stock price change. We collected Stocktwits data for over ten years for 25 different companies, including the major five FAANG (Facebook, Amazon, Apple, Netflix, Google). These datasets were labelled with three labelling techniques based on stock price changes. Our proposed model FinALBERT is fine-tuned with these labels to achieve optimal results. We experimented with the labelled dataset by training it on traditional machine learning, BERT, and FinBERT models, which helped us understand how these labels behaved with different model architectures. Our labelling method competitive advantage is that it can help analyse the historical data effectively, and the mathematical function can be easily customised to predict stock movement.

</p>
</details>

<details><summary><b>Student-Teacher Learning from Clean Inputs to Noisy Inputs</b>
<a href="https://arxiv.org/abs/2103.07600">arxiv:2103.07600</a>
&#x1F4C8; 12 <br>
<p>Guanzhe Hong, Zhiyuan Mao, Xiaojun Lin, Stanley H. Chan</p></summary>
<p>

**Abstract:** Feature-based student-teacher learning, a training method that encourages the student's hidden features to mimic those of the teacher network, is empirically successful in transferring the knowledge from a pre-trained teacher network to the student network. Furthermore, recent empirical results demonstrate that, the teacher's features can boost the student network's generalization even when the student's input sample is corrupted by noise. However, there is a lack of theoretical insights into why and when this method of transferring knowledge can be successful between such heterogeneous tasks. We analyze this method theoretically using deep linear networks, and experimentally using nonlinear networks. We identify three vital factors to the success of the method: (1) whether the student is trained to zero training loss; (2) how knowledgeable the teacher is on the clean-input problem; (3) how the teacher decomposes its knowledge in its hidden features. Lack of proper control in any of the three factors leads to failure of the student-teacher learning method.

</p>
</details>

<details><summary><b>Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks</b>
<a href="https://arxiv.org/abs/2103.12593">arxiv:2103.12593</a>
&#x1F4C8; 10 <br>
<p>Bojian Yin, Federico Corradi, Sander M. Bohte</p></summary>
<p>

**Abstract:** Inspired by more detailed modeling of biological neurons, Spiking neural networks (SNNs) have been investigated both as more biologically plausible and potentially more powerful models of neural computation, and also with the aim of extracting biological neurons' energy efficiency; the performance of such networks however has remained lacking compared to classical artificial neural networks (ANNs). Here, we demonstrate how a novel surrogate gradient combined with recurrent networks of tunable and adaptive spiking neurons yields state-of-the-art for SNNs on challenging benchmarks in the time-domain, like speech and gesture recognition. This also exceeds the performance of standard classical recurrent neural networks (RNNs) and approaches that of the best modern ANNs. As these SNNs exhibit sparse spiking, we show that they theoretically are one to three orders of magnitude more computationally efficient compared to RNNs with comparable performance. Together, this positions SNNs as an attractive solution for AI hardware implementations.

</p>
</details>

<details><summary><b>Cross-Domain Similarity Learning for Face Recognition in Unseen Domains</b>
<a href="https://arxiv.org/abs/2103.07503">arxiv:2103.07503</a>
&#x1F4C8; 10 <br>
<p>Masoud Faraki, Xiang Yu, Yi-Hsuan Tsai, Yumin Suh, Manmohan Chandraker</p></summary>
<p>

**Abstract:** Face recognition models trained under the assumption of identical training and test distributions often suffer from poor generalization when faced with unknown variations, such as a novel ethnicity or unpredictable individual make-ups during test time. In this paper, we introduce a novel cross-domain metric learning loss, which we dub Cross-Domain Triplet (CDT) loss, to improve face recognition in unseen domains. The CDT loss encourages learning semantically meaningful features by enforcing compact feature clusters of identities from one domain, where the compactness is measured by underlying similarity metrics that belong to another training domain with different statistics. Intuitively, it discriminatively correlates explicit metrics derived from one domain, with triplet samples from another domain in a unified loss function to be minimized within a network, which leads to better alignment of the training domains. The network parameters are further enforced to learn generalized features under domain shift, in a model-agnostic learning pipeline. Unlike the recent work of Meta Face Recognition, our method does not require careful hard-pair sample mining and filtering strategy during training. Extensive experiments on various face recognition benchmarks show the superiority of our method in handling variations, compared to baseline and the state-of-the-art methods.

</p>
</details>

<details><summary><b>Continual Learning for Recurrent Neural Networks: an Empirical Evaluation</b>
<a href="https://arxiv.org/abs/2103.07492">arxiv:2103.07492</a>
&#x1F4C8; 9 <br>
<p>Andrea Cossu, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu</p></summary>
<p>

**Abstract:** Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.

</p>
</details>

<details><summary><b>Generating and Characterizing Scenarios for Safety Testing of Autonomous Vehicles</b>
<a href="https://arxiv.org/abs/2103.07403">arxiv:2103.07403</a>
&#x1F4C8; 9 <br>
<p>Zahra Ghodsi, Siva Kumar Sastry Hari, Iuri Frosio, Timothy Tsai, Alejandro Troccoli, Stephen W. Keckler, Siddharth Garg, Anima Anandkumar</p></summary>
<p>

**Abstract:** Extracting interesting scenarios from real-world data as well as generating failure cases is important for the development and testing of autonomous systems. We propose efficient mechanisms to both characterize and generate testing scenarios using a state-of-the-art driving simulator. For any scenario, our method generates a set of possible driving paths and identifies all the possible safe driving trajectories that can be taken starting at different times, to compute metrics that quantify the complexity of the scenario. We use our method to characterize real driving data from the Next Generation Simulation (NGSIM) project, as well as adversarial scenarios generated in simulation. We rank the scenarios by defining metrics based on the complexity of avoiding accidents and provide insights into how the AV could have minimized the probability of incurring an accident. We demonstrate a strong correlation between the proposed metrics and human intuition.

</p>
</details>

<details><summary><b>VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts</b>
<a href="https://arxiv.org/abs/2103.07292">arxiv:2103.07292</a>
&#x1F4C8; 9 <br>
<p>Matthew J. Vowels, Necati Cihan Camgoz, Richard Bowden</p></summary>
<p>

**Abstract:** Disentangled representations support a range of downstream tasks including causal reasoning, generative modeling, and fair machine learning. Unfortunately, disentanglement has been shown to be impossible without the incorporation of supervision or inductive bias. Given that supervision is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an unsupervised, deep State-Space-Model for Video Disentanglement (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts decoder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including identity and dynamics transfer, sequence generation, Fréchet Inception Distance, and factor classification. VDSM provides state-of-the-art performance and exceeds adversarial methods, even when the methods use additional supervision.

</p>
</details>

<details><summary><b>Real-time Timbre Transfer and Sound Synthesis using DDSP</b>
<a href="https://arxiv.org/abs/2103.07220">arxiv:2103.07220</a>
&#x1F4C8; 9 <br>
<p>Francesco Ganis, Erik Frej Knudesn, Søren V. K. Lyster, Robin Otterbein, David Südholt, Cumhur Erkut</p></summary>
<p>

**Abstract:** Neural audio synthesis is an actively researched topic, having yielded a wide range of techniques that leverages machine learning architectures. Google Magenta elaborated a novel approach called Differential Digital Signal Processing (DDSP) that incorporates deep neural networks with preconditioned digital signal processing techniques, reaching state-of-the-art results especially in timbre transfer applications. However, most of these techniques, including the DDSP, are generally not applicable in real-time constraints, making them ineligible in a musical workflow. In this paper, we present a real-time implementation of the DDSP library embedded in a virtual synthesizer as a plug-in that can be used in a Digital Audio Workstation. We focused on timbre transfer from learned representations of real instruments to arbitrary sound inputs as well as controlling these models by MIDI. Furthermore, we developed a GUI for intuitive high-level controls which can be used for post-processing and manipulating the parameters estimated by the neural network. We have conducted a user experience test with seven participants online. The results indicated that our users found the interface appealing, easy to understand, and worth exploring further. At the same time, we have identified issues in the timbre transfer quality, in some components we did not implement, and in installation and distribution of our plugin. The next iteration of our design will address these issues. Our real-time MATLAB and JUCE implementations are available at https://github.com/SMC704/juce-ddsp and https://github.com/SMC704/matlab-ddsp , respectively.

</p>
</details>

<details><summary><b>Learnable Companding Quantization for Accurate Low-bit Neural Networks</b>
<a href="https://arxiv.org/abs/2103.07156">arxiv:2103.07156</a>
&#x1F4C8; 7 <br>
<p>Kohei Yamamoto</p></summary>
<p>

**Abstract:** Quantizing deep neural networks is an effective method for reducing memory consumption and improving inference speed, and is thus useful for implementation in resource-constrained devices. However, it is still hard for extremely low-bit models to achieve accuracy comparable with that of full-precision models. To address this issue, we propose learnable companding quantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and 4-bit models. LCQ jointly optimizes model weights and learnable companding functions that can flexibly and non-uniformly control the quantization levels of weights and activations. We also present a new weight normalization technique that allows more stable training for quantization. Experimental results show that LCQ outperforms conventional state-of-the-art methods and narrows the gap between quantized and full-precision models for image classification and object detection tasks. Notably, the 2-bit ResNet-50 model on ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%, allowing LCQ to further exploit the potential of non-uniform quantization.

</p>
</details>

<details><summary><b>Slip-Based Autonomous ZUPT through Gaussian Process to Improve Planetary Rover Localization</b>
<a href="https://arxiv.org/abs/2103.07587">arxiv:2103.07587</a>
&#x1F4C8; 6 <br>
<p>Cagri Kilic, Nicholas Ohi, Yu Gu, Jason N. Gross</p></summary>
<p>

**Abstract:** The zero-velocity update (ZUPT) algorithm provides valuable state information to maintain the inertial navigation system (INS) reliability when stationary conditions are satisfied. Employing ZUPT along with leveraging non-holonomic constraints can greatly benefit wheeled mobile robot dead-reckoning localization accuracy. However, determining how often they should be employed requires consideration to balance localization accuracy and traversal rate for planetary rovers. To address this, we investigate when to autonomously initiate stops to improve wheel-inertial odometry (WIO) localization performance with ZUPT. To do this, we propose a 3D dead-reckoning approach that predicts wheel slippage while the rover is in motion and forecasts the appropriate time to stop without changing any rover hardware or major rover operations. We validate with field tests that our approach is viable on different terrain types and achieves a 3D localization accuracy of more than 97% over 650 m drives on rough terrain.

</p>
</details>

<details><summary><b>A Review on Semi-Supervised Relation Extraction</b>
<a href="https://arxiv.org/abs/2103.07575">arxiv:2103.07575</a>
&#x1F4C8; 6 <br>
<p>Yusen Lin</p></summary>
<p>

**Abstract:** Relation extraction (RE) plays an important role in extracting knowledge from unstructured text but requires a large amount of labeled corpus. To reduce the expensive annotation efforts, semisupervised learning aims to leverage both labeled and unlabeled data. In this paper, we review and compare three typical methods in semi-supervised RE with deep learning or meta-learning: self-ensembling, which forces consistent under perturbations but may confront insufficient supervision; self-training, which iteratively generates pseudo labels and retrain itself with the enlarged labeled set; dual learning, which leverages a primal task and a dual task to give mutual feedback. Mean-teacher (Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al., 2019) are elaborated as the representatives to alleviate the weakness of these three methods, respectively.

</p>
</details>

<details><summary><b>Cooperative Learning of Zero-Shot Machine Reading Comprehension</b>
<a href="https://arxiv.org/abs/2103.07449">arxiv:2103.07449</a>
&#x1F4C8; 6 <br>
<p>Hongyin Luo, Shang-Wen Li, Seunghak Yu, James Glass</p></summary>
<p>

**Abstract:** Pretrained language models have significantly improved the performance of down-stream language understanding tasks, including extractive question answering, by providing high-quality contextualized word embeddings. However, learning question answering models still need large-scaled data annotation in specific domains. In this work, we propose a cooperative, self-play learning framework, REGEX, for question generation and answering. REGEX is built upon a masked answer extraction task with an interactive learning environment containing an answer entity REcognizer, a question Generator, and an answer EXtractor. Given a passage with a masked entity, the generator generates a question around the entity, and the extractor is trained to extract the masked entity with the generated question and raw texts. The framework allows the training of question generation and answering models on any text corpora without annotation. We further leverage a reinforcement learning technique to reward generating high-quality questions and to improve the answer extraction model's performance. Experiment results show that REGEX outperforms the state-of-the-art (SOTA) pretrained language models and zero-shot approaches on standard question-answering benchmarks, and yields the new SOTA performance under the zero-shot setting.

</p>
</details>

<details><summary><b>Towards Risk Modeling for Collaborative AI</b>
<a href="https://arxiv.org/abs/2103.07460">arxiv:2103.07460</a>
&#x1F4C8; 5 <br>
<p>Matteo Camilli, Michael Felderer, Andrea Giusti, Dominik T. Matt, Anna Perini, Barbara Russo, Angelo Susi</p></summary>
<p>

**Abstract:** Collaborative AI systems aim at working together with humans in a shared space to achieve a common goal. This setting imposes potentially hazardous circumstances due to contacts that could harm human beings. Thus, building such systems with strong assurances of compliance with requirements domain specific standards and regulations is of greatest importance. Challenges associated with the achievement of this goal become even more severe when such systems rely on machine learning components rather than such as top-down rule-based AI. In this paper, we introduce a risk modeling approach tailored to Collaborative AI systems. The risk model includes goals, risk events and domain specific indicators that potentially expose humans to hazards. The risk model is then leveraged to drive assurance methods that feed in turn the risk model through insights extracted from run-time evidence. Our envisioned approach is described by means of a running example in the domain of Industry 4.0, where a robotic arm endowed with a visual perception component, implemented with machine learning, collaborates with a human operator for a production-relevant task.

</p>
</details>

<details><summary><b>Multiview Sensing With Unknown Permutations: An Optimal Transport Approach</b>
<a href="https://arxiv.org/abs/2103.07458">arxiv:2103.07458</a>
&#x1F4C8; 5 <br>
<p>Yanting Ma, Petros T. Boufounos, Hassan Mansour, Shuchin Aeron</p></summary>
<p>

**Abstract:** In several applications, including imaging of deformable objects while in motion, simultaneous localization and mapping, and unlabeled sensing, we encounter the problem of recovering a signal that is measured subject to unknown permutations. In this paper we take a fresh look at this problem through the lens of optimal transport (OT). In particular, we recognize that in most practical applications the unknown permutations are not arbitrary but some are more likely to occur than others. We exploit this by introducing a regularization function that promotes the more likely permutations in the solution. We show that, even though the general problem is not convex, an appropriate relaxation of the resulting regularized problem allows us to exploit the well-developed machinery of OT and develop a tractable algorithm.

</p>
</details>

<details><summary><b>Process Comparison Using Object-Centric Process Cubes</b>
<a href="https://arxiv.org/abs/2103.07184">arxiv:2103.07184</a>
&#x1F4C8; 5 <br>
<p>Anahita Farhang Ghahfarokhi, Alessandro Berti, Wil M. P. van der Aalst</p></summary>
<p>

**Abstract:** Process mining provides ways to analyze business processes. Common process mining techniques consider the process as a whole. However, in real-life business processes, different behaviors exist that make the overall process too complex to interpret. Process comparison is a branch of process mining that isolates different behaviors of the process from each other by using process cubes. Process cubes organize event data using different dimensions. Each cell contains a set of events that can be used as an input to apply process mining techniques. Existing work on process cubes assume single case notions. However, in real processes, several case notions (e.g., order, item, package, etc.) are intertwined. Object-centric process mining is a new branch of process mining addressing multiple case notions in a process. To make a bridge between object-centric process mining and process comparison, we propose a process cube framework, which supports process cube operations such as slice and dice on object-centric event logs. To facilitate the comparison, the framework is integrated with several object-centric process discovery approaches.

</p>
</details>

<details><summary><b>Feature Learning for Stock Price Prediction Shows a Significant Role of Analyst Rating</b>
<a href="https://arxiv.org/abs/2103.09106">arxiv:2103.09106</a>
&#x1F4C8; 4 <br>
<p>Jaideep Singh, Matloob Khushi</p></summary>
<p>

**Abstract:** To reject the Efficient Market Hypothesis a set of 5 technical indicators and 23 fundamental indicators was identified to establish the possibility of generating excess returns on the stock market. Leveraging these data points and various classification machine learning models, trading data of the 505 equities on the US S&P500 over the past 20 years was analysed to develop a classifier effective for our cause. From any given day, we were able to predict the direction of change in price by 1% up to 10 days in the future. The predictions had an overall accuracy of 83.62% with a precision of 85% for buy signals and a recall of 100% for sell signals. Moreover, we grouped equities by their sector and repeated the experiment to see if grouping similar assets together positively effected the results but concluded that it showed no significant improvements in the performance rejecting the idea of sector-based analysis. Also, using feature ranking we could identify an even smaller set of 6 indicators while maintaining similar accuracies as that from the original 28 features and also uncovered the importance of buy, hold and sell analyst ratings as they came out to be the top contributors in the model. Finally, to evaluate the effectiveness of the classifier in real-life situations, it was backtested on FAANG equities using a modest trading strategy where it generated high returns of above 60% over the term of the testing dataset. In conclusion, our proposed methodology with the combination of purposefully picked features shows an improvement over the previous studies, and our model predicts the direction of 1% price changes on the 10th day with high confidence and with enough buffer to even build a robotic trading system.

</p>
</details>

<details><summary><b>SMOTE-ENC: A novel SMOTE-based method to generate synthetic data for nominal and continuous features</b>
<a href="https://arxiv.org/abs/2103.07612">arxiv:2103.07612</a>
&#x1F4C8; 4 <br>
<p>Mimi Mukherjee, Matloob Khushi</p></summary>
<p>

**Abstract:** Real world datasets are heavily skewed where some classes are significantly outnumbered by the other classes. In these situations, machine learning algorithms fail to achieve substantial efficacy while predicting these under-represented instances. To solve this problem, many variations of synthetic minority over-sampling methods (SMOTE) have been proposed to balance the dataset which deals with continuous features. However, for datasets with both nominal and continuous features, SMOTE-NC is the only SMOTE-based over-sampling technique to balance the data. In this paper, we present a novel minority over-sampling method, SMOTE-ENC (SMOTE - Encoded Nominal and Continuous), in which, nominal features are encoded as numeric values and the difference between two such numeric value reflects the amount of change of association with minority class. Our experiments show that the classification model using SMOTE-ENC method offers better prediction than model using SMOTE-NC when the dataset has a substantial number of nominal features and also when there is some association between the categorical features and the target class. Additionally, our proposed method addressed one of the major limitations of SMOTE-NC algorithm. SMOTE-NC can be applied only on mixed datasets that have features consisting of both continuous and nominal features and cannot function if all the features of the dataset are nominal. Our novel method has been generalized to be applied on both mixed datasets and on nominal only datasets. The code is available from mkhushi.github.io

</p>
</details>

<details><summary><b>Medical data wrangling with sequential variational autoencoders</b>
<a href="https://arxiv.org/abs/2103.07206">arxiv:2103.07206</a>
&#x1F4C8; 4 <br>
<p>Daniel Barrejón, Pablo M. Olmos, Antonio Artés-Rodríguez</p></summary>
<p>

**Abstract:** Medical data sets are usually corrupted by noise and missing data. These missing patterns are commonly assumed to be completely random, but in medical scenarios, the reality is that these patterns occur in bursts due to sensors that are off for some time or data collected in a misaligned uneven fashion, among other causes. This paper proposes to model medical data records with heterogeneous data types and bursty missing data using sequential variational autoencoders (VAEs). In particular, we propose a new methodology, the Shi-VAE, which extends the capabilities of VAEs to sequential streams of data with missing observations. We compare our model against state-of-the-art solutions in an intensive care unit database (ICU) and a dataset of passive human monitoring. Furthermore, we find that standard error metrics such as RMSE are not conclusive enough to assess temporal models and include in our analysis the cross-correlation between the ground truth and the imputed signal. We show that Shi-VAE achieves the best performance in terms of using both metrics, with lower computational complexity than the GP-VAE model, which is the state-of-the-art method for medical records.

</p>
</details>

<details><summary><b>Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability</b>
<a href="https://arxiv.org/abs/2103.07162">arxiv:2103.07162</a>
&#x1F4C8; 4 <br>
<p>Wei-Tsung Kao, Hung-Yi Lee</p></summary>
<p>

**Abstract:** This paper investigates whether the power of the models pre-trained on text data, such as BERT, can be transferred to general token sequence classification applications. To verify pre-trained models' transferability, we test the pre-trained models on text classification tasks with meanings of tokens mismatches, and real-world non-text token sequence classification data, including amino acid, DNA, and music. We find that even on non-text data, the models pre-trained on text converge faster, perform better than the randomly initialized models, and only slightly worse than the models using task-specific knowledge. We also find that the representations of the text and non-text pre-trained models share non-trivial similarities.

</p>
</details>

<details><summary><b>Causal Markov Boundaries</b>
<a href="https://arxiv.org/abs/2103.07560">arxiv:2103.07560</a>
&#x1F4C8; 3 <br>
<p>Sofia Triantafillou, Fattaneh Jabbari, Greg Cooper</p></summary>
<p>

**Abstract:** Feature selection is an important problem in machine learning, which aims to select variables that lead to an optimal predictive model. In this paper, we focus on feature selection for post-intervention outcome prediction from pre-intervention variables. We are motivated by healthcare settings, where the goal is often to select the treatment that will maximize a specific patient's outcome; however, we often do not have sufficient randomized control trial data to identify well the conditional treatment effect. We show how we can use observational data to improve feature selection and effect estimation in two cases: (a) using observational data when we know the causal graph, and (b) when we do not know the causal graph but have observational and limited experimental data. Our paper extends the notion of Markov boundary to treatment-outcome pairs. We provide theoretical guarantees for the methods we introduce. In simulated data, we show that combining observational and experimental data improves feature selection and effect estimation.

</p>
</details>

<details><summary><b>Meta-Modeling of Assembly Contingencies and Planning for Repair</b>
<a href="https://arxiv.org/abs/2103.07544">arxiv:2103.07544</a>
&#x1F4C8; 3 <br>
<p>Priyam Parashar, Aayush Naik, Jiaming Hu, Henrik I. Christensen</p></summary>
<p>

**Abstract:** The World Robotics Challenge (2018 & 2020) was designed to challenge teams to design systems that are easy to adapt to new tasks and to ensure robust operation in a semi-structured environment. We present a layered strategy to transform missions into tasks and actions and provide a set of strategies to address simple and complex failures. We propose a model for characterizing failures using this model and discuss repairs. Simple failures are by far the most common in our WRC system and we also present how we repaired them.

</p>
</details>

<details><summary><b>Adversarial Graph Disentanglement</b>
<a href="https://arxiv.org/abs/2103.07295">arxiv:2103.07295</a>
&#x1F4C8; 3 <br>
<p>Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Shuiwang Ji, Jian Cheng, Yao Zhao</p></summary>
<p>

**Abstract:** A real-world graph has a complex topological structure, which is often formed by the interaction of different latent factors. Disentanglement of these latent factors can effectively improve the robustness and expressiveness of node representation of graph. However, most existing methods lack consideration of the intrinsic differences in relations between nodes caused by factor entanglement. In this paper, we propose an Adversarial Disentangled Graph Convolutional Network (ADGCN) for disentangled graph representation learning. Specifically, a component-specific aggregation approach is proposed to achieve micro-disentanglement by inferring latent components that caused the links between nodes. On the basis of micro-disentanglement, we further propose a macro-disentanglement adversarial regularizer to improve the separability among component distributions, thus restricting the interdependence among components. Additionally, to reveal the topological graph structure, a diversity-preserving node sampling approach is proposed, by which the graph structure can be progressively refined in a way of local structure awareness. The experimental results on various real-world graph data verify that our ADGCN obtains more favorable performance over currently available alternatives.

</p>
</details>

<details><summary><b>Patient-specific virtual spine straightening and vertebra inpainting: An automatic framework for osteoplasty planning</b>
<a href="https://arxiv.org/abs/2103.07279">arxiv:2103.07279</a>
&#x1F4C8; 3 <br>
<p>Christina Bukas, Bailiang Jian, Luis F. Rodriguez Venegas, Francesca De Benetti, Sebastian Ruehling, Anjany Sekuboyina, Jens Gempt, Jan S. Kirschke, Marie Piraud, Johannes Oberreuter, Nassir Navab, Thomas Wendler</p></summary>
<p>

**Abstract:** Symptomatic spinal vertebral compression fractures (VCFs) often require osteoplasty treatment. A cement-like material is injected into the bone to stabilize the fracture, restore the vertebral body height and alleviate pain. Leakage is a common complication and may occur due to too much cement being injected. In this work, we propose an automated patient-specific framework that can allow physicians to calculate an upper bound of cement for the injection and estimate the optimal outcome of osteoplasty. The framework uses the patient CT scan and the fractured vertebra label to build a virtual healthy spine using a high-level approach. Firstly, the fractured spine is segmented with a three-step Convolution Neural Network (CNN) architecture. Next, a per-vertebra rigid registration to a healthy spine atlas restores its curvature. Finally, a GAN-based inpainting approach replaces the fractured vertebra with an estimation of its original shape. Based on this outcome, we then estimate the maximum amount of bone cement for injection. We evaluate our framework by comparing the virtual vertebrae volumes of ten patients to their healthy equivalent and report an average error of 3.88$\pm$7.63\%. The presented pipeline offers a first approach to a personalized automatic high-level framework for planning osteoplasty procedures.

</p>
</details>

<details><summary><b>Longitudinal Quantitative Assessment of COVID-19 Infection Progression from Chest CTs</b>
<a href="https://arxiv.org/abs/2103.07240">arxiv:2103.07240</a>
&#x1F4C8; 3 <br>
<p>Seong Tae Kim, Leili Goli, Magdalini Paschali, Ashkan Khakzar, Matthias Keicher, Tobias Czempiel, Egon Burian, Rickmer Braren, Nassir Navab, Thomas Wendler</p></summary>
<p>

**Abstract:** Chest computed tomography (CT) has played an essential diagnostic role in assessing patients with COVID-19 by showing disease-specific image features such as ground-glass opacity and consolidation. Image segmentation methods have proven to help quantify the disease burden and even help predict the outcome. The availability of longitudinal CT series may also result in an efficient and effective method to reliably assess the progression of COVID-19, monitor the healing process and the response to different therapeutic strategies. In this paper, we propose a new framework to identify infection at a voxel level (identification of healthy lung, consolidation, and ground-glass opacity) and visualize the progression of COVID-19 using sequential low-dose non-contrast CT scans. In particular, we devise a longitudinal segmentation network that utilizes the reference scan information to improve the performance of disease identification. Experimental results on a clinical longitudinal dataset collected in our institution show the effectiveness of the proposed method compared to the static deep neural networks for disease quantification.

</p>
</details>

<details><summary><b>In the light of feature distributions: moment matching for Neural Style Transfer</b>
<a href="https://arxiv.org/abs/2103.07208">arxiv:2103.07208</a>
&#x1F4C8; 3 <br>
<p>Nikolai Kalischek, Jan Dirk Wegner, Konrad Schindler</p></summary>
<p>

**Abstract:** Style transfer aims to render the content of a given image in the graphical/artistic style of another image. The fundamental concept underlying NeuralStyle Transfer (NST) is to interpret style as a distribution in the feature space of a Convolutional Neural Network, such that a desired style can be achieved by matching its feature distribution. We show that most current implementations of that concept have important theoretical and practical limitations, as they only partially align the feature distributions. We propose a novel approach that matches the distributions more precisely, thus reproducing the desired style more faithfully, while still being computationally efficient. Specifically, we adapt the dual form of Central Moment Discrepancy (CMD), as recently proposed for domain adaptation, to minimize the difference between the target style and the feature distribution of the output image. The dual interpretation of this metric explicitly matches all higher-order centralized moments and is therefore a natural extension of existing NST methods that only take into account the first and second moments. Our experiments confirm that the strong theoretical properties also translate to visually better style transfer, and better disentangle style from semantic image content.

</p>
</details>

<details><summary><b>Dynamic Acoustic Unit Augmentation With BPE-Dropout for Low-Resource End-to-End Speech Recognition</b>
<a href="https://arxiv.org/abs/2103.07186">arxiv:2103.07186</a>
&#x1F4C8; 3 <br>
<p>Aleksandr Laptev, Andrei Andrusenko, Ivan Podluzhny, Anton Mitrofanov, Ivan Medennikov, Yuri Matveev</p></summary>
<p>

**Abstract:** With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. Researchers and industry prefer to use end-to-end ASR systems for on-device speech recognition tasks. This is because end-to-end systems can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Another challenging task associated with speech assistants is personalization, which mainly lies in handling out-of-vocabulary (OOV) words. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. To address the aforementioned problems, we propose a method of dynamic acoustic unit augmentation based on the BPE-dropout technique. It non-deterministically tokenizes utterances to extend the token's contexts and to regularize their distribution for the model's recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative WER and 25% relative F-score) at no additional computational cost. Owing to the use of BPE-dropout, our monolingual Turkish Conformer established a competitive result with 22.2% character error rate (CER) and 38.9% word error rate (WER), which is close to the best published multilingual system.

</p>
</details>

<details><summary><b>Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging</b>
<a href="https://arxiv.org/abs/2103.07152">arxiv:2103.07152</a>
&#x1F4C8; 3 <br>
<p>Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, Guangming Shi</p></summary>
<p>

**Abstract:** In coded aperture snapshot spectral imaging (CASSI) system, the real-world hyperspectral image (HSI) can be reconstructed from the captured compressive image in a snapshot. Model-based HSI reconstruction methods employed hand-crafted priors to solve the reconstruction problem, but most of which achieved limited success due to the poor representation capability of these hand-crafted priors. Deep learning based methods learning the mappings between the compressive images and the HSIs directly achieved much better results. Yet, it is nontrivial to design a powerful deep network heuristically for achieving satisfied results. In this paper, we propose a novel HSI reconstruction method based on the Maximum a Posterior (MAP) estimation framework using learned Gaussian Scale Mixture (GSM) prior. Different from existing GSM models using hand-crafted scale priors (e.g., the Jeffrey's prior), we propose to learn the scale prior through a deep convolutional neural network (DCNN). Furthermore, we also propose to estimate the local means of the GSM models by the DCNN. All the parameters of the MAP estimation algorithm and the DCNN parameters are jointly optimized through end-to-end training. Extensive experimental results on both synthetic and real datasets demonstrate that the proposed method outperforms existing state-of-the-art methods. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm.

</p>
</details>

<details><summary><b>Comparing the Performance of NLP Toolkits and Evaluation measures in Legal Tech</b>
<a href="https://arxiv.org/abs/2103.11792">arxiv:2103.11792</a>
&#x1F4C8; 2 <br>
<p>Muhammad Zohaib Khan</p></summary>
<p>

**Abstract:** Recent developments in Natural Language Processing have led to the introduction of state-of-the-art Neural Language Models, enabled with unsupervised transferable learning, using different pretraining objectives. While these models achieve excellent results on the downstream NLP tasks, various domain adaptation techniques can improve their performance on domain-specific tasks. We compare and analyze the pretrained Neural Language Models, XLNet (autoregressive), and BERT (autoencoder) on the Legal Tasks. Results show that XLNet Model performs better on our Sequence Classification task of Legal Opinions Classification, whereas BERT produces better results on the NER task. We use domain-specific pretraining and additional legal vocabulary to adapt BERT Model further to the Legal Domain. We prepared multiple variants of the BERT Model, using both methods and their combination. Comparing our variants of the BERT Model, specializing in the Legal Domain, we conclude that both additional pretraining and vocabulary techniques enhance the BERT model's performance on the Legal Opinions Classification task. Additional legal vocabulary improves BERT's performance on the NER task. Combining the pretraining and vocabulary techniques further improves the final results. Our Legal-Vocab-BERT Model gives the best results on the Legal Opinions Task, outperforming the larger pretrained general Language Models, i.e., BERT-Base and XLNet-Base.

</p>
</details>

<details><summary><b>RL-Controller: a reinforcement learning framework for active structural control</b>
<a href="https://arxiv.org/abs/2103.07616">arxiv:2103.07616</a>
&#x1F4C8; 2 <br>
<p>Soheila Sadeghi Eshkevari, Soheil Sadeghi Eshkevari, Debarshi Sen, Shamim N. Pakzad</p></summary>
<p>

**Abstract:** To maintain structural integrity and functionality during the designed life cycle of a structure, engineers are expected to accommodate for natural hazards as well as operational load levels. Active control systems are an efficient solution for structural response control when a structure is subjected to unexpected extreme loads. However, development of these systems through traditional means is limited by their model dependent nature. Recent advancements in adaptive learning methods, in particular, reinforcement learning (RL), for real-time decision making problems, along with rapid growth in high-performance computational resources, help structural engineers to transform the classic model-based active control problem to a purely data-driven one. In this paper, we present a novel RL-based approach for designing active controllers by introducing RL-Controller, a flexible and scalable simulation environment. The RL-Controller includes attributes and functionalities that are defined to model active structural control mechanisms in detail. We show that the proposed framework is easily trainable for a five story benchmark building with 65% reductions on average in inter story drifts (ISD) when subjected to strong ground motions. In a comparative study with LQG active control method, we demonstrate that the proposed model-free algorithm learns more optimal actuator forcing strategies that yield higher performance, e.g., 25% more ISD reductions on average with respect to LQG, without using prior information about the mechanical properties of the system.

</p>
</details>

<details><summary><b>Conceptual capacity and effective complexity of neural networks</b>
<a href="https://arxiv.org/abs/2103.07614">arxiv:2103.07614</a>
&#x1F4C8; 2 <br>
<p>Lech Szymanski, Brendan McCane, Craig Atkinson</p></summary>
<p>

**Abstract:** We propose a complexity measure of a neural network mapping function based on the diversity of the set of tangent spaces from different inputs. Treating each tangent space as a linear PAC concept we use an entropy-based measure of the bundle of concepts in order to estimate the conceptual capacity of the network. The theoretical maximal capacity of a ReLU network is equivalent to the number of its neurons. In practice however, due to correlations between neuron activities within the network, the actual capacity can be remarkably small, even for very big networks. Empirical evaluations show that this new measure is correlated with the complexity of the mapping function and thus the generalisation capabilities of the corresponding network. It captures the effective, as oppose to the theoretical, complexity of the network function. We also showcase some uses of the proposed measure for analysis and comparison of trained neural network models.

</p>
</details>

<details><summary><b>How to Train Your Flare Prediction Model: Revisiting Robust Sampling of Rare Events</b>
<a href="https://arxiv.org/abs/2103.07542">arxiv:2103.07542</a>
&#x1F4C8; 2 <br>
<p>Azim Ahmadzadeh, Berkay Aydin, Manolis K. Georgoulis, Dustin J. Kempton, Sushant S. Mahajan, Rafal A. Angryk</p></summary>
<p>

**Abstract:** We present a case study of solar flare forecasting by means of metadata feature time series, by treating it as a prominent class-imbalance and temporally coherent problem. Taking full advantage of pre-flare time series in solar active regions is made possible via the Space Weather Analytics for Solar Flares (SWAN-SF) benchmark dataset; a partitioned collection of multivariate time series of active region properties comprising 4075 regions and spanning over 9 years of the Solar Dynamics Observatory (SDO) period of operations. We showcase the general concept of temporal coherence triggered by the demand of continuity in time series forecasting and show that lack of proper understanding of this effect may spuriously enhance models' performance. We further address another well-known challenge in rare event prediction, namely, the class-imbalance issue. The SWAN-SF is an appropriate dataset for this, with a 60:1 imbalance ratio for GOES M- and X-class flares and a 800:1 for X-class flares against flare-quiet instances. We revisit the main remedies for these challenges and present several experiments to illustrate the exact impact that each of these remedies may have on performance. Moreover, we acknowledge that some basic data manipulation tasks such as data normalization and cross validation may also impact the performance -- we discuss these problems as well. In this framework we also review the primary advantages and disadvantages of using true skill statistic and Heidke skill score, as two widely used performance verification metrics for the flare forecasting task. In conclusion, we show and advocate for the benefits of time series vs. point-in-time forecasting, provided that the above challenges are measurably and quantitatively addressed.

</p>
</details>

<details><summary><b>On Incorporating Forecasts into Linear State Space Model Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2103.07533">arxiv:2103.07533</a>
&#x1F4C8; 2 <br>
<p>Jacques A. de Chalendar, Peter W. Glynn</p></summary>
<p>

**Abstract:** Weather forecast information will very likely find increasing application in the control of future energy systems. In this paper, we introduce an augmented state space model formulation with linear dynamics, within which one can incorporate forecast information that is dynamically revealed alongside the evolution of the underlying state variable. We use the martingale model for forecast evolution (MMFE) to enforce the necessary consistency properties that must govern the joint evolution of forecasts with the underlying state. The formulation also generates jointly Markovian dynamics that give rise to Markov decision processes (MDPs) that remain computationally tractable. This paper is the first to enforce MMFE consistency requirements within an MDP formulation that preserves tractability.

</p>
</details>

<details><summary><b>Private Cross-Silo Federated Learning for Extracting Vaccine Adverse Event Mentions</b>
<a href="https://arxiv.org/abs/2103.07491">arxiv:2103.07491</a>
&#x1F4C8; 2 <br>
<p>Pallika Kanani, Virendra J. Marathe, Daniel Peterson, Rave Harpaz, Steve Bright</p></summary>
<p>

**Abstract:** Federated Learning (FL) is quickly becoming a goto distributed training paradigm for users to jointly train a global model without physically sharing their data. Users can indirectly contribute to, and directly benefit from a much larger aggregate data corpus used to train the global model. However, literature on successful application of FL in real-world problem settings is somewhat sparse. In this paper, we describe our experience applying a FL based solution to the Named Entity Recognition (NER) task for an adverse event detection application in the context of mass scale vaccination programs. We present a comprehensive empirical analysis of various dimensions of benefits gained with FL based training. Furthermore, we investigate effects of tighter Differential Privacy (DP) constraints in highly sensitive settings where federation users must enforce Local DP to ensure strict privacy guarantees. We show that local DP can severely cripple the global model's prediction accuracy, thus dis-incentivizing users from participating in the federation. In response, we demonstrate how recent innovation on personalization methods can help significantly recover the lost accuracy. We focus our analysis on the Federated Fine-Tuning algorithm, FedFT, and prove that it is not PAC Identifiable, thus making it even more attractive for FL-based training.

</p>
</details>

<details><summary><b>Machine Learning Assisted Orthonormal Basis Selection for Functional Data Analysis</b>
<a href="https://arxiv.org/abs/2103.07453">arxiv:2103.07453</a>
&#x1F4C8; 2 <br>
<p>Rani Basna, Hiba Nassar, Krzysztof Podgórski</p></summary>
<p>

**Abstract:** In implementations of the functional data methods, the effect of the initial choice of an orthonormal basis has not gained much attention in the past. Typically, several standard bases such as Fourier, wavelets, splines, etc. are considered to transform observed functional data and a choice is made without any formal criteria indicating which of the bases is preferable for the initial transformation of the data into functions. In an attempt to address this issue, we propose a strictly data-driven method of orthogonal basis selection. The method uses recently introduced orthogonal spline bases called the splinets obtained by efficient orthogonalization of the B-splines. The algorithm learns from the data in the machine learning style to efficiently place knots. The optimality criterion is based on the average (per functional data point) mean square error and is utilized both in the learning algorithms and in comparison studies. The latter indicates efficiency that is particularly evident for the sparse functional data and to a lesser degree in analyses of responses to complex physical systems.

</p>
</details>

<details><summary><b>Radiomic Deformation and Textural Heterogeneity (R-DepTH) Descriptor to characterize Tumor Field Effect: Application to Survival Prediction in Glioblastoma</b>
<a href="https://arxiv.org/abs/2103.07423">arxiv:2103.07423</a>
&#x1F4C8; 2 <br>
<p>Marwa Ismail, Prateek Prasanna, Kaustav Bera, Volodymyr Statsevych, Virginia Hill, Gagandeep Singh, Sasan Partovi, Niha Beig, Sean McGarry, Peter Laviolette, Manmeet Ahluwalia, Anant Madabhushi, Pallavi Tiwari</p></summary>
<p>

**Abstract:** The concept of tumor field effect implies that cancer is a systemic disease with its impact way beyond the visible tumor confines. For instance, in Glioblastoma (GBM), an aggressive brain tumor, the increase in intracranial pressure due to tumor burden often leads to brain herniation and poor outcomes. Our work is based on the rationale that highly aggressive tumors tend to grow uncontrollably, leading to pronounced biomechanical tissue deformations in the normal parenchyma, which when combined with local morphological differences in the tumor confines on MRI scans, will comprehensively capture tumor field effect. Specifically, we present an integrated MRI-based descriptor, radiomic-Deformation and Textural Heterogeneity (r-DepTH). This descriptor comprises measurements of the subtle perturbations in tissue deformations throughout the surrounding normal parenchyma due to mass effect. This involves non-rigidly aligning the patients MRI scans to a healthy atlas via diffeomorphic registration. The resulting inverse mapping is used to obtain the deformation field magnitudes in the normal parenchyma. These measurements are then combined with a 3D texture descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (COLLAGE), which captures the morphological heterogeneity within the tumor confines, on MRI scans. R-DepTH, on N = 207 GBM cases (training set (St) = 128, testing set (Sv) = 79), demonstrated improved prognosis of overall survival by categorizing patients into low- (prolonged survival) and high-risk (poor survival) groups (on St, p-value = 0.0000035, and on Sv, p-value = 0.0024). R-DepTH descriptor may serve as a comprehensive MRI-based prognostic marker of disease aggressiveness and survival in solid tumors.

</p>
</details>

<details><summary><b>Optimal sequential decision making with probabilistic digital twins</b>
<a href="https://arxiv.org/abs/2103.07405">arxiv:2103.07405</a>
&#x1F4C8; 2 <br>
<p>Christian Agrell, Kristina Rognlien Dahl, Andreas Hafver</p></summary>
<p>

**Abstract:** Digital twins are emerging in many industries, typically consisting of simulation models and data associated with a specific physical system. One of the main reasons for developing a digital twin, is to enable the simulation of possible consequences of a given action, without the need to interfere with the physical system itself. Physical systems of interest, and the environments they operate in, do not always behave deterministically. Moreover, information about the system and its environment is typically incomplete or imperfect. Probabilistic representations of systems and environments may therefore be called for, especially to support decisions in application areas where actions may have severe consequences.
  In this paper we introduce the probabilistic digital twin (PDT). We will start by discussing how epistemic uncertainty can be treated using measure theory, by modelling epistemic information via $σ$-algebras. Based on this, we give a formal definition of how epistemic uncertainty can be updated in a PDT. We then study the problem of optimal sequential decision making. That is, we consider the case where the outcome of each decision may inform the next. Within the PDT framework, we formulate this optimization problem. We discuss how this problem may be solved (at least in theory) via the maximum principle method or the dynamic programming principle. However, due to the curse of dimensionality, these methods are often not tractable in practice. To mend this, we propose a generic approximate solution using deep reinforcement learning together with neural networks defined on sets. We illustrate the method on a practical problem, considering optimal information gathering for the estimation of a failure probability.

</p>
</details>

<details><summary><b>Automating the GDPR Compliance Assessment for Cross-border Personal Data Transfers in Android Applications</b>
<a href="https://arxiv.org/abs/2103.07297">arxiv:2103.07297</a>
&#x1F4C8; 2 <br>
<p>Danny S. Guamán, Xavier Ferrer, Jose M. del Alamo, Jose Such</p></summary>
<p>

**Abstract:** The General Data Protection Regulation (GDPR) aims to ensure that all personal data processing activities are fair and transparent for the European Union (EU) citizens, regardless of whether these are carried out within the EU or anywhere else. To this end, it sets strict requirements to transfer personal data outside the EU. However, checking these requirements is a daunting task for supervisory authorities, particularly in the mobile app domain due to the huge number of apps available and their dynamic nature. In this paper, we propose a fully automated method to assess the compliance of mobile apps with the GDPR requirements for cross-border personal data transfers. We have applied the method to the top-free 10,080 apps from the Google Play Store. The results reveal that there is still a very significant gap between what app providers and third-party recipients do in practice and what is intended by the GDPR. A substantial 56% of analysed apps are potentially non-compliant with the GDPR cross-border transfer requirements.

</p>
</details>

<details><summary><b>Modelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning</b>
<a href="https://arxiv.org/abs/2103.07276">arxiv:2103.07276</a>
&#x1F4C8; 2 <br>
<p>C. Chalmers, P. Fergus, S. Wich, S. N. Longmore</p></summary>
<p>

**Abstract:** For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74.

</p>
</details>

<details><summary><b>Adversarial Machine Learning Security Problems for 6G: mmWave Beam Prediction Use-Case</b>
<a href="https://arxiv.org/abs/2103.07268">arxiv:2103.07268</a>
&#x1F4C8; 2 <br>
<p>Evren Catak, Ferhat Ozgur Catak, Arild Moldsvor</p></summary>
<p>

**Abstract:** 6G is the next generation for the communication systems. In recent years, machine learning algorithms have been applied widely in various fields such as health, transportation, and the autonomous car. The predictive algorithms will be used in 6G problems. With the rapid developments of deep learning techniques, it is critical to take the security concern into account to apply the algorithms. While machine learning offers significant advantages for 6G, AI models' security is ignored. Since it has many applications in the real world, security is a vital part of the algorithms. This paper has proposed a mitigation method for adversarial attacks against proposed 6G machine learning models for the millimeter-wave (mmWave) beam prediction with adversarial learning. The main idea behind adversarial attacks against machine learning models is to produce faulty results by manipulating trained deep learning models for 6G applications for mmWave beam prediction use case. We have also presented the adversarial learning mitigation method's performance for 6G security in millimeter-wave beam prediction application with fast gradient sign method attack. The mean square errors of the defended model and undefended model are very close.

</p>
</details>

<details><summary><b>Robust and generalizable embryo selection based on artificial intelligence and time-lapse image sequences</b>
<a href="https://arxiv.org/abs/2103.07262">arxiv:2103.07262</a>
&#x1F4C8; 2 <br>
<p>Jørgen Berntsen, Jens Rimestad, Jacob Theilgaard Lassen, Dang Tran, Mikkel Fly Kragh</p></summary>
<p>

**Abstract:** Assessing and selecting the most viable embryos for transfer is an essential part of in vitro fertilization (IVF). In recent years, several approaches have been made to improve and automate the procedure using artificial intelligence (AI) and deep learning. Based on images of embryos with known implantation data (KID), AI models have been trained to automatically score embryos related to their chance of achieving a successful implantation. However, as of now, only limited research has been conducted to evaluate how embryo selection models generalize to new clinics and how they perform in subgroup analyses across various conditions. In this paper, we investigate how a deep learning-based embryo selection model using only time-lapse image sequences performs across different patient ages and clinical conditions, and how it correlates with traditional morphokinetic parameters. The model was trained and evaluated based on a large dataset from 18 IVF centers consisting of 115,832 embryos, of which 14,644 embryos were transferred KID embryos. In an independent test set, the AI model sorted KID embryos with an area under the curve (AUC) of a receiver operating characteristic curve of 0.67 and all embryos with an AUC of 0.95. A clinic hold-out test showed that the model generalized to new clinics with an AUC range of 0.60-0.75 for KID embryos. Across different subgroups of age, insemination method, incubation time, and transfer protocol, the AUC ranged between 0.63 and 0.69. Furthermore, model predictions correlated positively with blastocyst grading and negatively with direct cleavages. The fully automated iDAScore v1.0 model was shown to perform at least as good as a state-of-the-art manual embryo selection model. Moreover, full automatization of embryo scoring implies fewer manual evaluations and eliminates biases due to inter- and intraobserver variation.

</p>
</details>

<details><summary><b>Sentinel: A Hyper-Heuristic for the Generation of Mutant Reduction Strategies</b>
<a href="https://arxiv.org/abs/2103.07241">arxiv:2103.07241</a>
&#x1F4C8; 2 <br>
<p>Giovani Guizzo, Federica Sarro, Jens Krinke, Silvia Regina Vergilio</p></summary>
<p>

**Abstract:** Mutation testing is an effective approach to evaluate and strengthen software test suites, but its adoption is currently limited by the mutants' execution computational cost. Several strategies have been proposed to reduce this cost (a.k.a. mutation cost reduction strategies), however none of them has proven to be effective for all scenarios since they often need an ad-hoc manual selection and configuration depending on the software under test (SUT). In this paper, we propose a novel multi-objective evolutionary hyper-heuristic approach, dubbed Sentinel, to automate the generation of optimal cost reduction strategies for every new SUT. We evaluate Sentinel by carrying out a thorough empirical study involving 40 releases of 10 open-source real-world software systems and both baseline and state-of-the-art strategies as a benchmark. We execute a total of 4,800 experiments, and evaluate their results with both quality indicators and statistical significance tests, following the most recent best practice in the literature. The results show that strategies generated by Sentinel outperform the baseline strategies in 95% of the cases always with large effect sizes. They also obtain statistically significantly better results than state-of-the-art strategies in 88% of the cases, with large effect sizes for 95% of them. Also, our study reveals that the mutation strategies generated by Sentinel for a given software version can be used without any loss in quality for subsequently developed versions in 95% of the cases. These results show that Sentinel is able to automatically generate mutation strategies that reduce mutation testing cost without affecting its testing effectiveness (i.e. mutation score), thus taking off from the tester's shoulders the burden of manually selecting and configuring strategies for each SUT.

</p>
</details>

<details><summary><b>Sequential Random Network for Fine-grained Image Classification</b>
<a href="https://arxiv.org/abs/2103.07230">arxiv:2103.07230</a>
&#x1F4C8; 2 <br>
<p>Chaorong Li, Malu Zhang, Wei Huang, Fengqing Qin, Anping Zeng, Yuanyuan Huang</p></summary>
<p>

**Abstract:** Deep Convolutional Neural Network (DCNN) and Transformer have achieved remarkable successes in image recognition. However, their performance in fine-grained image recognition is still difficult to meet the requirements of actual needs. This paper proposes a Sequence Random Network (SRN) to enhance the performance of DCNN. The output of DCNN is one-dimensional features. This one-dimensional feature abstractly represents image information, but it does not express well the detailed information of image. To address this issue, we use the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks (called BiLSTM-TDN), to further process DCNN one-dimensional features for highlighting the detail information of image. After the feature transform by BiLSTM-TDN, the recognition performance has been greatly improved. We conducted the experiments on six fine-grained image datasets. Except for FGVC-Aircraft, the accuracies of the proposed methods on the other datasets exceeded 99%. Experimental results show that BiLSTM-TDN is far superior to the existing state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended to other models, such as Transformer.

</p>
</details>

<details><summary><b>BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized Neural Networks</b>
<a href="https://arxiv.org/abs/2103.07224">arxiv:2103.07224</a>
&#x1F4C8; 2 <br>
<p>Yedi Zhang, Zhe Zhao, Guangke Chen, Fu Song, Taolue Chen</p></summary>
<p>

**Abstract:** Verifying and explaining the behavior of neural networks is becoming increasingly important, especially when they are deployed in safety-critical applications. In this paper, we study verification problems for Binarized Neural Networks (BNNs), the 1-bit quantization of general real-numbered neural networks. Our approach is to encode BNNs into Binary Decision Diagrams (BDDs), which is done by exploiting the internal structure of the BNNs. In particular, we translate the input-output relation of blocks in BNNs to cardinality constraints which are then encoded by BDDs. Based on the encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. We demonstrate the application of our framework by providing quantitative robustness analysis and interpretability for BNNs. We implement a prototype tool BDD4BNN and carry out extensive experiments which confirm the effectiveness and efficiency of our approach.

</p>
</details>

<details><summary><b>Auction Based Clustered Federated Learning in Mobile Edge Computing System</b>
<a href="https://arxiv.org/abs/2103.07150">arxiv:2103.07150</a>
&#x1F4C8; 2 <br>
<p>Renhao Lu, Weizhe Zhang, Qiong Li, Xiaoxiong Zhong, Athanasios V. Vasilakos</p></summary>
<p>

**Abstract:** In recent years, mobile clients' computing ability and storage capacity have greatly improved, efficiently dealing with some applications locally. Federated learning is a promising distributed machine learning solution that uses local computing and local data to train the Artificial Intelligence (AI) model. Combining local computing and federated learning can train a powerful AI model under the premise of ensuring local data privacy while making full use of mobile clients' resources. However, the heterogeneity of local data, that is, Non-independent and identical distribution (Non-IID) and imbalance of local data size, may bring a bottleneck hindering the application of federated learning in mobile edge computing (MEC) system. Inspired by this, we propose a cluster-based clients selection method that can generate a federated virtual dataset that satisfies the global distribution to offset the impact of data heterogeneity and proved that the proposed scheme could converge to an approximate optimal solution. Based on the clustering method, we propose an auction-based clients selection scheme within each cluster that fully considers the system's energy heterogeneity and gives the Nash equilibrium solution of the proposed scheme for balance the energy consumption and improving the convergence rate. The simulation results show that our proposed selection methods and auction-based federated learning can achieve better performance with the Convolutional Neural Network model (CNN) under different data distributions.

</p>
</details>

<details><summary><b>Learning spectro-temporal representations of complex sounds with parameterized neural networks</b>
<a href="https://arxiv.org/abs/2103.07125">arxiv:2103.07125</a>
&#x1F4C8; 2 <br>
<p>Rachid Riad, Julien Karadayi, Anne-Catherine Bachoud-Lévi, Emmanuel Dupoux</p></summary>
<p>

**Abstract:** Deep Learning models have become potential candidates for auditory neuroscience research, thanks to their recent successes on a variety of auditory tasks. Yet, these models often lack interpretability to fully understand the exact computations that have been performed. Here, we proposed a parametrized neural network layer, that computes specific spectro-temporal modulations based on Gabor kernels (Learnable STRFs) and that is fully interpretable. We evaluated predictive capabilities of this layer on Speech Activity Detection, Speaker Verification, Urban Sound Classification and Zebra Finch Call Type Classification. We found out that models based on Learnable STRFs are on par for all tasks with different toplines, and obtain the best performance for Speech Activity Detection. As this layer is fully interpretable, we used quantitative measures to describe the distribution of the learned spectro-temporal modulations. The filters adapted to each task and focused mostly on low temporal and spectral modulations. The analyses show that the filters learned on human speech have similar spectro-temporal parameters as the ones measured directly in the human auditory cortex. Finally, we observed that the tasks organized in a meaningful way: the human vocalizations tasks closer to each other and bird vocalizations far away from human vocalizations and urban sounds tasks.

</p>
</details>

<details><summary><b>SuperMeshing: A New Deep Learning Architecture for Increasing the Mesh Density of Metal Forming Stress Field with Attention Mechanism and Perceptual Features</b>
<a href="https://arxiv.org/abs/2104.09276">arxiv:2104.09276</a>
&#x1F4C8; 1 <br>
<p>Qingfeng Xu, Zhenguo Nie, Handing Xu, Haosu Zhou, Xinjun Liu</p></summary>
<p>

**Abstract:** In stress field analysis, the finite element analysis is a crucial approach, in which the mesh-density has a significant impact on the results. High mesh density usually contributes authentic to simulation results but costs more computing resources, leading to curtailing efficiency during the design process. To eliminate this drawback, we propose a new data-driven mesh-density boost model named SuperMeshingNet that strengthens the advantages of finite element analysis (FEA) with low mesh-density as inputs to the deep learning model, which consisting of Res-UNet architecture, to acquire high-density stress field instantaneously, shortening computing time and cost automatically. Moreover, the attention mechanism and the perceptual features are utilized, enhancing the performance of SuperMeshingNet. Compared to the baseline that applied the linear interpolation method, SuperMeshingNet achieves a prominent reduction in the mean squared error (MSE) and mean absolute error (MAE) on test data, which contains prior unseen cases. Based on the data set of metal forming, the comparable experiments are proceeded to demonstrate the high quality and superior precision of the reconstructed results generated by our model. The well-trained model can successfully show more excellent performance than the baseline and other methods on the multiple scaled mesh-density, including $2\times$, $4\times$, and $8\times$. With the refined result owning broaden scaling of mesh density and high precision, the FEA process can be accelerated with seldom cost on computation resources. We publicly share our work with full detail of implementation at https://github.com/zhenguonie/2021_SuperMeshing_2D_Metal_Forming

</p>
</details>

<details><summary><b>OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning</b>
<a href="https://arxiv.org/abs/2103.09055">arxiv:2103.09055</a>
&#x1F4C8; 1 <br>
<p>Hantian Zhang, Xu Chu, Abolfazl Asudeh, Shamkant B. Navathe</p></summary>
<p>

**Abstract:** Machine learning (ML) is increasingly being used to make decisions in our society. ML models, however, can be unfair to certain demographic groups (e.g., African Americans or females) according to various fairness metrics. Existing techniques for producing fair ML models either are limited to the type of fairness constraints they can handle (e.g., preprocessing) or require nontrivial modifications to downstream ML training algorithms (e.g., in-processing).
  We propose a declarative system OmniFair for supporting group fairness in ML. OmniFair features a declarative interface for users to specify desired group fairness constraints and supports all commonly used group fairness notions, including statistical parity, equalized odds, and predictive parity. OmniFair is also model-agnostic in the sense that it does not require modifications to a chosen ML algorithm. OmniFair also supports enforcing multiple user declared fairness constraints simultaneously while most previous techniques cannot. The algorithms in OmniFair maximize model accuracy while meeting the specified fairness constraints, and their efficiency is optimized based on the theoretically provable monotonicity property regarding the trade-off between accuracy and fairness that is unique to our system.
  We conduct experiments on commonly used datasets that exhibit bias against minority groups in the fairness literature. We show that OmniFair is more versatile than existing algorithmic fairness approaches in terms of both supported fairness constraints and downstream ML models. OmniFair reduces the accuracy loss by up to $94.8\%$ compared with the second best method. OmniFair also achieves similar running time to preprocessing methods, and is up to $270\times$ faster than in-processing methods.

</p>
</details>

<details><summary><b>Untrained networks for compressive lensless photography</b>
<a href="https://arxiv.org/abs/2103.07609">arxiv:2103.07609</a>
&#x1F4C8; 1 <br>
<p>Kristina Monakhova, Vi Tran, Grace Kuo, Laura Waller</p></summary>
<p>

**Abstract:** Compressive lensless imagers enable novel applications in an extremely compact device, requiring only a phase or amplitude mask placed close to the sensor. They have been demonstrated for 2D and 3D microscopy, single-shot video, and single-shot hyperspectral imaging; in each of these cases, a compressive-sensing-based inverse problem is solved in order to recover a 3D data-cube from a 2D measurement. Typically, this is accomplished using convex optimization and hand-picked priors. Alternatively, deep learning-based reconstruction methods offer the promise of better priors, but require many thousands of ground truth training pairs, which can be difficult or impossible to acquire. In this work, we propose the use of untrained networks for compressive image recovery. Our approach does not require any labeled training data, but instead uses the measurement itself to update the network weights. We demonstrate our untrained approach on lensless compressive 2D imaging as well as single-shot high-speed video recovery using the camera's rolling shutter, and single-shot hyperspectral imaging. We provide simulation and experimental verification, showing that our method results in improved image quality over existing methods.

</p>
</details>

<details><summary><b>Mining Artifacts in Mycelium SEM Micrographs</b>
<a href="https://arxiv.org/abs/2103.07573">arxiv:2103.07573</a>
&#x1F4C8; 1 <br>
<p>Thaicia Stona de Almeida</p></summary>
<p>

**Abstract:** Mycelium is a promising biomaterial based on fungal mycelium, a highly porous, nanofibrous structure. Scanning electron micrographs are used to characterize its network, but the currently available tools for nanofibrous microstructures do not contemplate the particularities of biomaterials. The adoption of a software for artificial nanofibrous in mycelium characterization adds the uncertainty of imaging artifact formation to the analysis. The reported work combines supervised and unsupervised machine learning methods to automate the identification of artifacts in the mapped pores of mycelium microstructure.
  Keywords: Machine learning; unsupervised learning; image processing; mycelium; microstructure informatics

</p>
</details>

<details><summary><b>Efficient reconstruction of depth three circuits with top fan-in two</b>
<a href="https://arxiv.org/abs/2103.07445">arxiv:2103.07445</a>
&#x1F4C8; 1 <br>
<p>Gaurav Sinha</p></summary>
<p>

**Abstract:** We develop efficient randomized algorithms to solve the black-box reconstruction problem for polynomials over finite fields, computable by depth three arithmetic circuits with alternating addition/multiplication gates, such that output gate is an addition gate with in-degree two. These circuits compute polynomials of form $G\times(T_1 + T_2)$, where $G,T_1,T_2$ are product of affine forms, and polynomials $T_1,T_2$ have no common factors. Rank of such a circuit is defined as dimension of vector space spanned by all affine factors of $T_1$ and $T_2$. For any polynomial $f$ computable by such a circuit, $rank(f)$ is defined to be the minimum rank of any such circuit computing it. Our work develops randomized reconstruction algorithms which take as input black-box access to a polynomial $f$ (over finite field $\mathbb{F}$), computable by such a circuit. Here are the results.
  1 [Low rank]: When $5\leq rank(f) = O(\log^3 d)$, it runs in time $(nd^{\log^3d}\log |\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a depth three circuit computing $f$, with top addition gate having in-degree $\leq d^{rank(f)}$.
  2 [High rank]: When $rank(f) = Ω(\log^3 d)$, it runs in time $(nd\log |\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a depth three circuit computing $f$, with top addition gate having in-degree two.
  Ours is the first blackbox reconstruction algorithm for this circuit class, that runs in time polynomial in $\log |\mathbb{F}|$. This problem has been mentioned as an open problem in [GKL12] (STOC 2012)

</p>
</details>

<details><summary><b>Hyperspectral Image Denoising and Anomaly Detection Based on Low-rank and Sparse Representations</b>
<a href="https://arxiv.org/abs/2103.07437">arxiv:2103.07437</a>
&#x1F4C8; 1 <br>
<p>Lina Zhuang, Lianru Gao, Bing Zhang, Xiyou Fu, Jose M. Bioucas-Dias</p></summary>
<p>

**Abstract:** Hyperspectral imaging measures the amount of electromagnetic energy across the instantaneous field of view at a very high resolution in hundreds or thousands of spectral channels. This enables objects to be detected and the identification of materials that have subtle differences between them. However, the increase in spectral resolution often means that there is a decrease in the number of photons received in each channel, which means that the noise linked to the image formation process is greater. This degradation limits the quality of the extracted information and its potential applications. Thus, denoising is a fundamental problem in hyperspectral image (HSI) processing. As images of natural scenes with highly correlated spectral channels, HSIs are characterized by a high level of self-similarity and can be well approximated by low-rank representations. These characteristics underlie the state-of-the-art methods used in HSI denoising. However, where there are rarely occurring pixel types, the denoising performance of these methods is not optimal, and the subsequent detection of these pixels may be compromised. To address these hurdles, in this article, we introduce RhyDe (Robust hyperspectral Denoising), a powerful HSI denoiser, which implements explicit low-rank representation, promotes self-similarity, and, by using a form of collaborative sparsity, preserves rare pixels. The denoising and detection effectiveness of the proposed robust HSI denoiser is illustrated using semireal and real data.

</p>
</details>

<details><summary><b>Hippocampal formation-inspired probabilistic generative model</b>
<a href="https://arxiv.org/abs/2103.07356">arxiv:2103.07356</a>
&#x1F4C8; 1 <br>
<p>Akira Taniguchi, Ayako Fukawa, Hiroshi Yamakawa</p></summary>
<p>

**Abstract:** We tackle the challenging task of bridging the gap between the neuroscientific knowledge of hippocampal formation (HPF) and the engineering knowledge of robotics and artificial intelligence. Simultaneous localization and mapping (SLAM) has already been realized in robotics as a basic function for spatial cognition. In this study, we aim to investigate how the SLAM functionality corresponds to the HPF. To this end, a hypothesis based on a literature review is suggested and a direction for its verification is presented, without performing any new simulations. We survey HPF models and various computational ones, including brain-inspired SLAM, spatial concept formation, and deep generative models. Furthermore, we discuss the relationship between the findings of HPF in neuroscience and SLAM in robotics. Thereby, A hippocampal formation-inspired probabilistic generative model (PGM) was constructed using a methodology for constructing a brain reference architecture. We propose an HPF-PGM as a computational model based on a modification of the conventional SLAM model, which is designed to be highly consistent with the anatomical structure and functions of the HPF. By referencing the brain, we suggest the importance of the integration of egocentric/allocentric information from the entorhinal cortex to the hippocampus and the use of discrete-event queues.

</p>
</details>

<details><summary><b>Second-Order Component Analysis for Fault Detection</b>
<a href="https://arxiv.org/abs/2103.07303">arxiv:2103.07303</a>
&#x1F4C8; 1 <br>
<p>Peng Jingchao, Zhao Haitao, Hu Zhengwei</p></summary>
<p>

**Abstract:** Process monitoring based on neural networks is getting more and more attention. Compared with classical neural networks, high-order neural networks have natural advantages in dealing with heteroscedastic data. However, high-order neural networks might bring the risk of overfitting and learning both the key information from original data and noises or anomalies. Orthogonal constraints can greatly reduce correlations between extracted features, thereby reducing the overfitting risk. This paper proposes a novel fault detection method called second-order component analysis (SCA). SCA rules out the heteroscedasticity of pro-cess data by optimizing a second-order autoencoder with orthogonal constraints. In order to deal with this constrained optimization problem, a geometric conjugate gradient algorithm is adopted in this paper, which performs geometric optimization on the combination of Stiefel manifold and Euclidean manifold. Extensive experiments on the Tennessee-Eastman benchmark pro-cess show that SCA outperforms PCA, KPCA, and autoencoder in missed detection rate (MDR) and false alarm rate (FAR).

</p>
</details>

<details><summary><b>Your most telling friends: Propagating latent ideological features on Twitter using neighborhood coherence</b>
<a href="https://arxiv.org/abs/2103.07250">arxiv:2103.07250</a>
&#x1F4C8; 1 <br>
<p>Pedro Ramaciotti Morales, Jean-Philippe Cointet, Julio Laborde</p></summary>
<p>

**Abstract:** Multidimensional scaling in networks allows for the discovery of latent information about their structure by embedding nodes in some feature space. Ideological scaling for users in social networks such as Twitter is an example, but similar settings can include diverse applications in other networks and even media platforms or e-commerce. A growing literature of ideology scaling methods in social networks restricts the scaling procedure to nodes that provide interpretability of the feature space: on Twitter, it is common to consider the sub-network of parliamentarians and their followers. This allows to interpret inferred latent features as indices for ideology-related concepts inspecting the position of members of parliament. While effective in inferring meaningful features, this is generally restrained to these sub-networks, limiting interesting applications such as country-wide measurement of polarization and its evolution. We propose two methods to propagate ideological features beyond these sub-networks: one based on homophily (linked users have similar ideology), and the other on structural similarity (nodes with similar neighborhoods have similar ideologies). In our methods, we leverage the concept of neighborhood ideological coherence as a parameter for propagation. Using Twitter data, we produce an ideological scaling for 370K users, and analyze the two families of propagation methods on a population of 6.5M users. We find that, when coherence is considered, the ideology of a user is better estimated from those with similar neighborhoods, than from their immediate neighbors.

</p>
</details>

<details><summary><b>Knowledge- and Data-driven Services for Energy Systems using Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2103.07248">arxiv:2103.07248</a>
&#x1F4C8; 1 <br>
<p>Francesco Fusco, Bradley Eck, Robert Gormally, Mark Purcell, Seshu Tirupathi</p></summary>
<p>

**Abstract:** The transition away from carbon-based energy sources poses several challenges for the operation of electricity distribution systems. Increasing shares of distributed energy resources (e.g. renewable energy generators, electric vehicles) and internet-connected sensing and control devices (e.g. smart heating and cooling) require new tools to support accurate, datadriven decision making. Modelling the effect of such growing complexity in the electrical grid is possible in principle using state-of-the-art power-power flow models. In practice, the detailed information needed for these physical simulations may be unknown or prohibitively expensive to obtain. Hence, datadriven approaches to power systems modelling, including feedforward neural networks and auto-encoders, have been studied to leverage the increasing availability of sensor data, but have seen limited practical adoption due to lack of transparency and inefficiencies on large-scale problems. Our work addresses this gap by proposing a data- and knowledge-driven probabilistic graphical model for energy systems based on the framework of graph neural networks (GNNs). The model can explicitly factor in domain knowledge, in the form of grid topology or physics constraints, thus resulting in sparser architectures and much smaller parameters dimensionality when compared with traditional machine-learning models with similar accuracy. Results obtained from a real-world smart-grid demonstration project show how the GNN was used to inform grid congestion predictions and market bidding services for a distribution system operator participating in an energy flexibility market.

</p>
</details>

<details><summary><b>GA for feature selection of EEG heterogeneous data</b>
<a href="https://arxiv.org/abs/2103.07117">arxiv:2103.07117</a>
&#x1F4C8; 1 <br>
<p>Aurora Saibene, Francesca Gasparini</p></summary>
<p>

**Abstract:** The electroencephalographic (EEG) signals provide highly informative data on brain activities and functions. However, their heterogeneity and high dimensionality may represent an obstacle for their interpretation. The introduction of a priori knowledge seems the best option to mitigate high dimensionality problems, but could lose some information and patterns present in the data, while data heterogeneity remains an open issue that often makes generalization difficult. In this study, we propose a genetic algorithm (GA) for feature selection that can be used with a supervised or unsupervised approach. Our proposal considers three different fitness functions without relying on expert knowledge. Starting from two publicly available datasets on cognitive workload and motor movement/imagery, the EEG signals are processed, normalized and their features computed in the time, frequency and time-frequency domains. The feature vector selection is performed by applying our GA proposal and compared with two benchmarking techniques. The results show that different combinations of our proposal achieve better results in respect to the benchmark in terms of overall performance and feature reduction. Moreover, the proposed GA, based on a novel fitness function here presented, outperforms the benchmark when the two different datasets considered are merged together, showing the effectiveness of our proposal on heterogeneous data.

</p>
</details>

<details><summary><b>Explaining Network Intrusion Detection System Using Explainable AI Framework</b>
<a href="https://arxiv.org/abs/2103.07110">arxiv:2103.07110</a>
&#x1F4C8; 1 <br>
<p>Shraddha Mane, Dattaraj Rao</p></summary>
<p>

**Abstract:** Cybersecurity is a domain where the data distribution is constantly changing with attackers exploring newer patterns to attack cyber infrastructure. Intrusion detection system is one of the important layers in cyber safety in today's world. Machine learning based network intrusion detection systems started showing effective results in recent years. With deep learning models, detection rates of network intrusion detection system are improved. More accurate the model, more the complexity and hence less the interpretability. Deep neural networks are complex and hard to interpret which makes difficult to use them in production as reasons behind their decisions are unknown. In this paper, we have used deep neural network for network intrusion detection and also proposed explainable AI framework to add transparency at every stage of machine learning pipeline. This is done by leveraging Explainable AI algorithms which focus on making ML models less of black boxes by providing explanations as to why a prediction is made. Explanations give us measurable factors as to what features influence the prediction of a cyberattack and to what degree. These explanations are generated from SHAP, LIME, Contrastive Explanations Method, ProtoDash and Boolean Decision Rules via Column Generation. We apply these approaches to NSL KDD dataset for intrusion detection system and demonstrate results.

</p>
</details>

<details><summary><b>Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification</b>
<a href="https://arxiv.org/abs/2103.11794">arxiv:2103.11794</a>
&#x1F4C8; 0 <br>
<p>Xiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong He, Bowen Zhou</p></summary>
<p>

**Abstract:** Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks(GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from differ-ent parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN mod-els to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble mod-els without adding model parameters.

</p>
</details>

<details><summary><b>Beyond $\log^2(T)$ Regret for Decentralized Bandits in Matching Markets</b>
<a href="https://arxiv.org/abs/2103.07501">arxiv:2103.07501</a>
&#x1F4C8; 0 <br>
<p>Soumya Basu, Karthik Abinav Sankararaman, Abishek Sankararaman</p></summary>
<p>

**Abstract:** We design decentralized algorithms for regret minimization in the two-sided matching market with one-sided bandit feedback that significantly improves upon the prior works (Liu et al. 2020a, 2020b, Sankararaman et al. 2020). First, for general markets, for any $\varepsilon > 0$, we design an algorithm that achieves a $O(\log^{1+\varepsilon}(T))$ regret to the agent-optimal stable matching, with unknown time horizon $T$, improving upon the $O(\log^{2}(T))$ regret achieved in (Liu et al. 2020b). Second, we provide the optimal $Θ(\log(T))$ agent-optimal regret for markets satisfying uniqueness consistency -- markets where leaving participants don't alter the original stable matching. Previously, $Θ(\log(T))$ regret was achievable (Sankararaman et al. 2020, Liu et al. 2020b) in the much restricted serial dictatorship setting, when all arms have the same preference over the agents. We propose a phase-based algorithm, wherein each phase, besides deleting the globally communicated dominated arms the agents locally delete arms with which they collide often. This local deletion is pivotal in breaking deadlocks arising from rank heterogeneity of agents across arms. We further demonstrate the superiority of our algorithm over existing works through simulations.

</p>
</details>

<details><summary><b>A New Training Framework for Deep Neural Network</b>
<a href="https://arxiv.org/abs/2103.07350">arxiv:2103.07350</a>
&#x1F4C8; 0 <br>
<p>Zhenyan Hou, Wenxuan Fan</p></summary>
<p>

**Abstract:** Knowledge distillation is the process of transferring the knowledge from a large model to a small model. In this process, the small model learns the generalization ability of the large model and retains the performance close to that of the large model. Knowledge distillation provides a training means to migrate the knowledge of models, facilitating model deployment and speeding up inference. However, previous distillation methods require pre-trained teacher models, which still bring computational and storage overheads. In this paper, a novel general training framework called Self Distillation (SD) is proposed. We demonstrate the effectiveness of our method by enumerating its performance improvements in diverse tasks and benchmark datasets.

</p>
</details>

<details><summary><b>Explainable AI by BAPC -- Before and After correction Parameter Comparison</b>
<a href="https://arxiv.org/abs/2103.07155">arxiv:2103.07155</a>
&#x1F4C8; 0 <br>
<p>Florian Sobieczky, Salma Mahmoud, Simon Neugebauer, Lukas Rippitsch, Manuela Geiß</p></summary>
<p>

**Abstract:** By means of a local surrogate approach, an analytical method to yield explanations of AI-predictions in the framework of regression models is defined. In the case of the AI-model producing additive corrections to the predictions of a base model, the explanations are delivered in the form of a shift of its interpretable parameters as long as the AI- predictions are small in a rigorously defined sense. Criteria are formulated giving a precise relation between lost accuracy and lacking model fidelity. Two applications show how physical or econometric parameters may be used to interpret the action of neural network and random forest models in the sense of the underlying base model. This is an extended version of our paper presented at the ISM 2020 conference, where we first introduced our new approach BAPC.

</p>
</details>


[Next Page]({{ '/2021/03/11/2021.03.11.html' | relative_url }})
