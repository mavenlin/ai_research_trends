Prev: [2021.12.28]({{ '/2021/12/28/2021.12.28.html' | relative_url }})  Next: [2021.12.30]({{ '/2021/12/30/2021.12.30.html' | relative_url }})
{% raw %}
## Summary for 2021-12-29, created on 2022-01-08


<details><summary><b>StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2</b>
<a href="https://arxiv.org/abs/2112.14683">arxiv:2112.14683</a>
&#x1F4C8; 145 <br>
<p>Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny</p></summary>
<p>

**Abstract:** Videos show continuous events, yet most - if not all - video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be - time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image and video discriminators pair and propose to use a single hypernetwork-based one. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just 5% more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model achieves state-of-the-art results on four modern 256$^2$ video synthesis benchmarks and one 1024$^2$ resolution one. Videos and the source code are available at the project website: https://universome.github.io/stylegan-v.

</p>
</details>

<details><summary><b>Retrieving Black-box Optimal Images from External Databases</b>
<a href="https://arxiv.org/abs/2112.14921">arxiv:2112.14921</a>
&#x1F4C8; 44 <br>
<p>Ryoma Sato</p></summary>
<p>

**Abstract:** Suppose we have a black-box function (e.g., deep neural network) that takes an image as input and outputs a value that indicates preference. How can we retrieve optimal images with respect to this function from an external database on the Internet? Standard retrieval problems in the literature (e.g., item recommendations) assume that an algorithm has full access to the set of items. In other words, such algorithms are designed for service providers. In this paper, we consider the retrieval problem under different assumptions. Specifically, we consider how users with limited access to an image database can retrieve images using their own black-box functions. This formulation enables a flexible and finer-grained image search defined by each user. We assume the user can access the database through a search query with tight API limits. Therefore, a user needs to efficiently retrieve optimal images in terms of the number of queries. We propose an efficient retrieval algorithm Tiara for this problem. In the experiments, we confirm that our proposed method performs better than several baselines under various settings.

</p>
</details>

<details><summary><b>Disentanglement and Generalization Under Correlation Shifts</b>
<a href="https://arxiv.org/abs/2112.14754">arxiv:2112.14754</a>
&#x1F4C8; 9 <br>
<p>Christina M. Funke, Paul Vicol, Kuan-Chieh Wang, Matthias KÃ¼mmerer, Richard Zemel, Matthias Bethge</p></summary>
<p>

**Abstract:** Correlations between factors of variation are prevalent in real-world data. Machine learning algorithms may benefit from exploiting such correlations, as they can increase predictive performance on noisy data. However, often such correlations are not robust (e.g., they may change between domains, datasets, or applications) and we wish to avoid exploiting them. Disentanglement methods aim to learn representations which capture different factors of variation in latent subspaces. A common approach involves minimizing the mutual information between latent subspaces, such that each encodes a single underlying attribute. However, this fails when attributes are correlated. We solve this problem by enforcing independence between subspaces conditioned on the available attributes, which allows us to remove only dependencies that are not due to the correlation structure present in the training data. We achieve this via an adversarial approach to minimize the conditional mutual information (CMI) between subspaces with respect to categorical variables. We first show theoretically that CMI minimization is a good objective for robust disentanglement on linear problems with Gaussian data. We then apply our method on real-world datasets based on MNIST and CelebA, and show that it yields models that are disentangled and robust under correlation shift, including in weakly supervised settings.

</p>
</details>

<details><summary><b>Implementation of Convolutional Neural Network Architecture on 3D Multiparametric Magnetic Resonance Imaging for Prostate Cancer Diagnosis</b>
<a href="https://arxiv.org/abs/2112.14644">arxiv:2112.14644</a>
&#x1F4C8; 8 <br>
<p>Ping-Chang Lin, Teodora Szasz, Hakizumwami B. Runesha</p></summary>
<p>

**Abstract:** Prostate cancer is one of the most common causes of cancer deaths in men. There is a growing demand for noninvasively and accurately diagnostic methods that facilitate the current standard prostate cancer risk assessment in clinical practice. Still, developing computer-aided classification tools in prostate cancer diagnostics from multiparametric magnetic resonance images continues to be a challenge. In this work, we propose a novel deep learning approach for automatic classification of prostate lesions in the corresponding magnetic resonance images by constructing a two-stage multimodal multi-stream convolutional neural network (CNN)-based architecture framework. Without implementing sophisticated image preprocessing steps or third-party software, our framework achieved the classification performance with the area under a Receiver Operating Characteristic (ROC) curve value of 0.87. The result outperformed most of the submitted methods and shared the highest value reported by the PROSTATEx Challenge organizer. Our proposed CNN-based framework reflects the potential of assisting medical image interpretation in prostate cancer and reducing unnecessary biopsies.

</p>
</details>

<details><summary><b>A transfer learning enhanced the physics-informed neural network model for vortex-induced vibration</b>
<a href="https://arxiv.org/abs/2112.14448">arxiv:2112.14448</a>
&#x1F4C8; 7 <br>
<p>Hesheng Tang, Hu Yang, Yangyang Liao, Liyu Xie</p></summary>
<p>

**Abstract:** Vortex-induced vibration (VIV) is a typical nonlinear fluid-structure interaction phenomenon, which widely exists in practical engineering (the flexible riser, the bridge and the aircraft wing, etc). The conventional finite element model (FEM)-based and data-driven approaches for VIV analysis often suffer from the challenges of the computational cost and acquisition of datasets. This paper proposed a transfer learning enhanced the physics-informed neural network (PINN) model to study the VIV (2D). The physics-informed neural network, when used in conjunction with the transfer learning method, enhances learning efficiency and keeps predictability in the target task by common characteristics knowledge from the source model without requiring a huge quantity of datasets. The datasets obtained from VIV experiment are divided evenly two parts (source domain and target domain), to evaluate the performance of the model. The results show that the proposed method match closely with the results available in the literature using conventional PINN algorithms even though the quantity of datasets acquired in training model gradually becomes smaller. The application of the model can break the limitation of monitoring equipment and methods in the practical projects, and promote the in-depth study of VIV.

</p>
</details>

<details><summary><b>A Color Image Steganography Based on Frequency Sub-band Selection</b>
<a href="https://arxiv.org/abs/2112.14437">arxiv:2112.14437</a>
&#x1F4C8; 7 <br>
<p>Hai Su, Shan Yang, Shuqing Zhang, Songsen Yu</p></summary>
<p>

**Abstract:** Color image steganography based on deep learning is the art of hiding information in the color image. Among them, image hiding steganography(hiding image with image) has attracted much attention in recent years because of its great steganographic capacity. However, images generated by image hiding steganography may show some obvious color distortion or artificial texture traces. We propose a color image steganographic model based on frequency sub-band selection to solve the above problems. Firstly, we discuss the relationship between the characteristics of different color spaces/frequency sub-bands and the generated image quality. Then, we select the B channel of the RGB image as the embedding channel and the high-frequency sub-band as the embedding domain. DWT(discrete wavelet transformation) transforms B channel information and secret gray image into frequency domain information, and then the secret image is embedded and extracted in the frequency domain. Comprehensive experiments demonstrate that images generated by our model have better image quality, and the imperceptibility is significantly increased.

</p>
</details>

<details><summary><b>HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution</b>
<a href="https://arxiv.org/abs/2112.14608">arxiv:2112.14608</a>
&#x1F4C8; 6 <br>
<p>Chaoxiong Wu, Jiaojiao Li, Rui Song, Yunsong Li, Qian Du</p></summary>
<p>

**Abstract:** Spectral super-resolution (SSR) refers to the hyperspectral image (HSI) recovery from an RGB counterpart. Due to the one-to-many nature of the SSR problem, a single RGB image can be reprojected to many HSIs. The key to tackle this illposed problem is to plug into multi-source prior information such as the natural RGB spatial context-prior, deep feature-prior or inherent HSI statistical-prior, etc., so as to improve the confidence and fidelity of reconstructed spectra. However, most current approaches only consider the general and limited priors in their designing the customized convolutional neural networks (CNNs), which leads to the inability to effectively alleviate the degree of ill-posedness. To address the problematic issues, we propose a novel holistic prior-embedded relation network (HPRN) for SSR. Basically, the core framework is delicately assembled by several multi-residual relation blocks (MRBs) that fully facilitate the transmission and utilization of the low-frequency content prior of RGB signals. Innovatively, the semantic prior of RGB input is introduced to identify category attributes and a semantic-driven spatial relation module (SSRM) is put forward to perform the feature aggregation among the clustered similar characteristics using a semantic-embedded relation matrix. Additionally, we develop a transformer-based channel relation module (TCRM), which breaks the habit of employing scalars as the descriptors of channel-wise relations in the previous deep feature-prior and replaces them with certain vectors, together with Transformerstyle feature interactions, supporting the representations to be more discriminative. In order to maintain the mathematical correlation and spectral consistency between hyperspectral bands, the second-order prior constraints (SOPC) are incorporated into the loss function to guide the HSI reconstruction process.

</p>
</details>

<details><summary><b>DDPG car-following model with real-world human driving experience in CARLA</b>
<a href="https://arxiv.org/abs/2112.14602">arxiv:2112.14602</a>
&#x1F4C8; 6 <br>
<p>Dianzhao Li, Ostap Okhrin</p></summary>
<p>

**Abstract:** In the autonomous driving field, the fusion of human knowledge into Deep Reinforcement Learning (DRL) is often based on the human demonstration recorded in the simulated environment. This limits the generalization and the feasibility of application in real-world traffic. We proposed a two-stage DRL method, that learns from real-world human driving to achieve performance that is superior to the pure DRL agent. Training a DRL agent is done within a framework for CARLA with Robot Operating System (ROS). For evaluation, we designed different real-world driving scenarios to compare the proposed two-stage DRL agent with the pure DRL agent. After extracting the 'good' behavior from the human driver, such as anticipation in a signalized intersection, the agent becomes more efficient and drives safer, which makes this autonomous agent more adapt to Human-Robot Interaction (HRI) traffic.

</p>
</details>

<details><summary><b>Res2NetFuse: A Fusion Method for Infrared and Visible Images</b>
<a href="https://arxiv.org/abs/2112.14540">arxiv:2112.14540</a>
&#x1F4C8; 6 <br>
<p>Xu Song, Xiao-Jun Wu, Hui Li, Jun Sun, Vasile Palade</p></summary>
<p>

**Abstract:** This paper presents a novel Res2Net-based fusion framework for infrared and visible images. The proposed fusion model has three parts: an encoder, a fusion layer and a decoder, respectively. The Res2Net-based encoder is used to extract multi-scale features of source images, the paper introducing a new training strategy for training a Res2Net-based encoder that uses only a single image. Then, a new fusion strategy is developed based on the attention model. Finally, the fused image is reconstructed by the decoder. The proposed approach is also analyzed in detail. Experiments show that our method achieves state-of-the-art fusion performance in objective and subjective assessment by comparing with the existing methods.

</p>
</details>

<details><summary><b>The impact of students behaviour, their approach, emotions and problem difficulty level on the performance prediction, evaluation and overall learning process during online coding activities</b>
<a href="https://arxiv.org/abs/2112.14407">arxiv:2112.14407</a>
&#x1F4C8; 6 <br>
<p>Dr. Hardik Patel, Dr. Purvi Koringa</p></summary>
<p>

**Abstract:** Learning process while solving coding problems is quite complex to understand. It is extremely important to understand the skills which are required and gained during learning to code. As a first step to understand the students behaviour and approach during learning coding, two online coding assignments or competitions are conducted with a 1-hour time limit. A survey has been conducted at the end of each coding test and answers to different questions have been collected. In depth statistical analysis is done to understand the learning process while solving the coding problems. It involves lots of parameters including students behaviour, their approach and difficulty level of coding problems. The inclusion of mood and emotions related questions can improve overall prediction performance but difficulty level matters in the submission status prediction. Two coding assignments or competitions are analyzed through in-depth research on 229 (first coding competition dataset) and 325 (second coding competition dataset) data points. The primary results are promising and these results give in depth insights about how learning to solve coding problems is affected by students behaviour, their approach, emotions and problem difficulty level.

</p>
</details>

<details><summary><b>Overcoming Mode Collapse with Adaptive Multi Adversarial Training</b>
<a href="https://arxiv.org/abs/2112.14406">arxiv:2112.14406</a>
&#x1F4C8; 6 <br>
<p>Karttikeya Mangalam, Rohin Garg</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) are a class of generative models used for various applications, but they have been known to suffer from the mode collapse problem, in which some modes of the target distribution are ignored by the generator. Investigative study using a new data generation procedure indicates that the mode collapse of the generator is driven by the discriminator's inability to maintain classification accuracy on previously seen samples, a phenomenon called Catastrophic Forgetting in continual learning. Motivated by this observation, we introduce a novel training procedure that adaptively spawns additional discriminators to remember previous modes of generation. On several datasets, we show that our training scheme can be plugged-in to existing GAN frameworks to mitigate mode collapse and improve standard metrics for GAN evaluation.

</p>
</details>

<details><summary><b>Polyak-Ruppert Averaged Q-Leaning is Statistically Efficient</b>
<a href="https://arxiv.org/abs/2112.14582">arxiv:2112.14582</a>
&#x1F4C8; 5 <br>
<p>Xiang Li, Wenhao Yang, Zhihua Zhang, Michael I. Jordan</p></summary>
<p>

**Abstract:** We study synchronous Q-learning with Polyak-Ruppert averaging (a.k.a., averaged Q-leaning) in a $Î³$-discounted MDP. We establish asymptotic normality for the averaged iteration $\bar{\boldsymbol{Q}}_T$. Furthermore, we show that $\bar{\boldsymbol{Q}}_T$ is actually a regular asymptotically linear (RAL) estimator for the optimal Q-value function $\boldsymbol{Q}^*$ with the most efficient influence function. It implies the averaged Q-learning iteration has the smallest asymptotic variance among all RAL estimators. In addition, we present a non-asymptotic analysis for the $\ell_{\infty}$ error $\mathbb{E}\|\bar{\boldsymbol{Q}}_T-\boldsymbol{Q}^*\|_{\infty}$, showing it matches the instance-dependent lower bound as well as the optimal minimax complexity lower bound. As a byproduct, we find the Bellman noise has sub-Gaussian coordinates with variance $\mathcal{O}((1-Î³)^{-1})$ instead of the prevailing $\mathcal{O}((1-Î³)^{-2})$ under the standard bounded reward assumption. The sub-Gaussian result has potential to improve the sample complexity of many RL algorithms. In short, our theoretical analysis shows averaged Q-Leaning is statistically efficient.

</p>
</details>

<details><summary><b>Fine-Tuning Transformers: Vocabulary Transfer</b>
<a href="https://arxiv.org/abs/2112.14569">arxiv:2112.14569</a>
&#x1F4C8; 5 <br>
<p>Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, Ivan P. Yamshchikov</p></summary>
<p>

**Abstract:** Transformers are responsible for the vast majority of recent advances in natural language processing. The majority of practical natural language processing applications of these models is typically enabled through transfer learning. This paper studies if corpus-specific tokenization used for fine-tuning improves the resulting performance of the model. Through a series of experiments, we demonstrate that such tokenization combined with the initialization and fine-tuning strategy for the vocabulary tokens speeds up the transfer and boosts the performance of the fine-tuned model. We call this aspect of transfer facilitation vocabulary transfer.

</p>
</details>

<details><summary><b>Onsite Non-Line-of-Sight Imaging via Online Calibrations</b>
<a href="https://arxiv.org/abs/2112.14555">arxiv:2112.14555</a>
&#x1F4C8; 5 <br>
<p>Zhengqing Pan, Ruiqian Li, Tian Gao, Zi Wang, Ping Liu, Siyuan Shen, Tao Wu, Jingyi Yu, Shiying Li</p></summary>
<p>

**Abstract:** There has been an increasing interest in deploying non-line-of-sight (NLOS) imaging systems for recovering objects behind an obstacle. Existing solutions generally pre-calibrate the system before scanning the hidden objects. Onsite adjustments of the occluder, object and scanning pattern require re-calibration. We present an online calibration technique that directly decouples the acquired transients at onsite scanning into the LOS and hidden components. We use the former to directly (re)calibrate the system upon changes of scene/obstacle configurations, scanning regions, and scanning patterns whereas the latter for hidden object recovery via spatial, frequency or learning based techniques. Our technique avoids using auxiliary calibration apparatus such as mirrors or checkerboards and supports both laboratory validations and real-world deployments.

</p>
</details>

<details><summary><b>Two-phase training mitigates class imbalance for camera trap image classification with CNNs</b>
<a href="https://arxiv.org/abs/2112.14491">arxiv:2112.14491</a>
&#x1F4C8; 5 <br>
<p>Farjad Malik, Simon Wouters, Ruben Cartuyvels, Erfan Ghadery, Marie-Francine Moens</p></summary>
<p>

**Abstract:** By leveraging deep learning to automatically classify camera trap images, ecologists can monitor biodiversity conservation efforts and the effects of climate change on ecosystems more efficiently. Due to the imbalanced class-distribution of camera trap datasets, current models are biased towards the majority classes. As a result, they obtain good performance for a few majority classes but poor performance for many minority classes. We used two-phase training to increase the performance for these minority classes. We trained, next to a baseline model, four models that implemented a different versions of two-phase training on a subset of the highly imbalanced Snapshot Serengeti dataset. Our results suggest that two-phase training can improve performance for many minority classes, with limited loss in performance for the other classes. We find that two-phase training based on majority undersampling increases class-specific F1-scores up to 3.0%. We also find that two-phase training outperforms using only oversampling or undersampling by 6.1% in F1-score on average. Finally, we find that a combination of over- and undersampling leads to a better performance than using them individually.

</p>
</details>

<details><summary><b>Explainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2112.14466">arxiv:2112.14466</a>
&#x1F4C8; 5 <br>
<p>Kacper Sokol, Peter Flach</p></summary>
<p>

**Abstract:** Explainable artificial intelligence and interpretable machine learning are research fields growing in importance. Yet, the underlying concepts remain somewhat elusive and lack generally agreed definitions. While recent inspiration from social sciences has refocused the work on needs and expectations of human recipients, the field still misses a concrete conceptualisation. We take steps towards addressing this challenge by reviewing the philosophical and social foundations of human explainability, which we then translate into the technological realm. In particular, we scrutinise the notion of algorithmic black boxes and the spectrum of understanding determined by explanatory processes and explainees' background knowledge. This approach allows us to define explainability as (logical) reasoning applied to transparent insights (into black boxes) interpreted under certain background knowledge - a process that engenders understanding in explainees. We then employ this conceptualisation to revisit the much disputed trade-off between transparency and predictive power and its implications for ante-hoc and post-hoc explainers as well as fairness and accountability engendered by explainability. We furthermore discuss components of the machine learning workflow that may be in need of interpretability, building on a range of ideas from human-centred explainability, with a focus on explainees, contrastive statements and explanatory processes. Our discussion reconciles and complements current research to help better navigate open questions - rather than attempting to address any individual issue - thus laying a solid foundation for a grounded discussion and future progress of explainable artificial intelligence and interpretable machine learning. We conclude with a summary of our findings, revisiting the human-centred explanatory process needed to achieve the desired level of algorithmic transparency.

</p>
</details>

<details><summary><b>EiFFFeL: Enforcing Fairness in Forests by Flipping Leaves</b>
<a href="https://arxiv.org/abs/2112.14435">arxiv:2112.14435</a>
&#x1F4C8; 5 <br>
<p>Seyum Assefa Abebe, Claudio Lucchese, Salvatore Orlando</p></summary>
<p>

**Abstract:** Nowadays Machine Learning (ML) techniques are extensively adopted in many socially sensitive systems, thus requiring to carefully study the fairness of the decisions taken by such systems. Many approaches have been proposed to address and to make sure there is no bias against individuals or specific groups which might originally come from biased training datasets or algorithm design. In this regard, we propose a fairness enforcing approach called EiFFFeL:Enforcing Fairness in Forests by Flipping Leaves which exploits tree-based or leaf-based post-processing strategies to relabel leaves of selected decision trees of a given forest. Experimental results show that our approach achieves a user defined group fairness degree without losing a significant amount of accuracy.

</p>
</details>

<details><summary><b>Few-shot Backdoor Defense Using Shapley Estimation</b>
<a href="https://arxiv.org/abs/2112.14889">arxiv:2112.14889</a>
&#x1F4C8; 4 <br>
<p>Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao</p></summary>
<p>

**Abstract:** Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and $Îµ$-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods.

</p>
</details>

<details><summary><b>Multivariate Trend Filtering for Lattice Data</b>
<a href="https://arxiv.org/abs/2112.14758">arxiv:2112.14758</a>
&#x1F4C8; 4 <br>
<p>Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Addison J. Hu, Ryan J. Tibshirani</p></summary>
<p>

**Abstract:** We study a multivariate version of trend filtering, called Kronecker trend filtering or KTF, for the case in which the design points form a lattice in $d$ dimensions. KTF is a natural extension of univariate trend filtering (Steidl et al., 2006; Kim et al., 2009; Tibshirani, 2014), and is defined by minimizing a penalized least squares problem whose penalty term sums the absolute (higher-order) differences of the parameter to be estimated along each of the coordinate directions. The corresponding penalty operator can be written in terms of Kronecker products of univariate trend filtering penalty operators, hence the name Kronecker trend filtering. Equivalently, one can view KTF in terms of an $\ell_1$-penalized basis regression problem where the basis functions are tensor products of falling factorial functions, a piecewise polynomial (discrete spline) basis that underlies univariate trend filtering.
  This paper is a unification and extension of the results in Sadhanala et al. (2016, 2017). We develop a complete set of theoretical results that describe the behavior of $k^{\mathrm{th}}$ order Kronecker trend filtering in $d$ dimensions, for every $k \geq 0$ and $d \geq 1$. This reveals a number of interesting phenomena, including the dominance of KTF over linear smoothers in estimating heterogeneously smooth functions, and a phase transition at $d=2(k+1)$, a boundary past which (on the high dimension-to-smoothness side) linear smoothers fail to be consistent entirely. We also leverage recent results on discrete splines from Tibshirani (2020), in particular, discrete spline interpolation results that enable us to extend the KTF estimate to any off-lattice location in constant-time (independent of the size of the lattice $n$).

</p>
</details>

<details><summary><b>Nonconvex Stochastic Scaled-Gradient Descent and Generalized Eigenvector Problems</b>
<a href="https://arxiv.org/abs/2112.14738">arxiv:2112.14738</a>
&#x1F4C8; 4 <br>
<p>Chris Junchi Li, Michael I. Jordan</p></summary>
<p>

**Abstract:** Motivated by the problem of online canonical correlation analysis, we propose the \emph{Stochastic Scaled-Gradient Descent} (SSGD) algorithm for minimizing the expectation of a stochastic function over a generic Riemannian manifold. SSGD generalizes the idea of projected stochastic gradient descent and allows the use of scaled stochastic gradients instead of stochastic gradients. In the special case of a spherical constraint, which arises in generalized eigenvector problems, we establish a nonasymptotic finite-sample bound of $\sqrt{1/T}$, and show that this rate is minimax optimal, up to a polylogarithmic factor of relevant parameters. On the asymptotic side, a novel trajectory-averaging argument allows us to achieve local asymptotic normality with a rate that matches that of Ruppert-Polyak-Juditsky averaging. We bring these ideas together in an application to online canonical correlation analysis, deriving, for the first time in the literature, an optimal one-time-scale algorithm with an explicit rate of local asymptotic convergence to normality. Numerical studies of canonical correlation analysis are also provided for synthetic data.

</p>
</details>

<details><summary><b>Sequential Episodic Control</b>
<a href="https://arxiv.org/abs/2112.14734">arxiv:2112.14734</a>
&#x1F4C8; 4 <br>
<p>Ismael T. Freire, AdriÃ¡n F. Amil, Paul F. M. J. Verschure</p></summary>
<p>

**Abstract:** State of the art deep reinforcement learning algorithms are sample inefficient due to the large number of episodes they require to achieve asymptotic performance. Episodic Reinforcement Learning (ERL) algorithms, inspired by the mammalian hippocampus, typically use extended memory systems to bootstrap learning from past events to overcome this sample-inefficiency problem. However, such memory augmentations are often used as mere buffers, from which isolated past experiences are drawn to learn from in an offline fashion (e.g., replay). Here, we demonstrate that including a bias in the acquired memory content derived from the order of episodic sampling improves both the sample and memory efficiency of an episodic control algorithm. We test our Sequential Episodic Control (SEC) model in a foraging task to show that storing and using integrated episodes as event sequences leads to faster learning with fewer memory requirements as opposed to a standard ERL benchmark, Model-Free Episodic Control, that buffers isolated events only. We also study the effect of memory constraints and forgetting on the sequential and non-sequential version of the SEC algorithm. Furthermore, we discuss how a hippocampal-like fast memory system could bootstrap slow cortical and subcortical learning subserving habit formation in the mammalian brain.

</p>
</details>

<details><summary><b>Monte Carlo EM for Deep Time Series Anomaly Detection</b>
<a href="https://arxiv.org/abs/2112.14436">arxiv:2112.14436</a>
&#x1F4C8; 4 <br>
<p>FranÃ§ois-Xavier Aubet, Daniel ZÃ¼gner, Jan Gasthaus</p></summary>
<p>

**Abstract:** Time series data are often corrupted by outliers or other kinds of anomalies. Identifying the anomalous points can be a goal on its own (anomaly detection), or a means to improving performance of other time series tasks (e.g. forecasting). Recent deep-learning-based approaches to anomaly detection and forecasting commonly assume that the proportion of anomalies in the training data is small enough to ignore, and treat the unlabeled data as coming from the nominal data distribution. We present a simple yet effective technique for augmenting existing time series models so that they explicitly account for anomalies in the training data. By augmenting the training data with a latent anomaly indicator variable whose distribution is inferred while training the underlying model using Monte Carlo EM, our method simultaneously infers anomalous points while improving model performance on nominal data. We demonstrate the effectiveness of the approach by combining it with a simple feed-forward forecasting model. We investigate how anomalies in the train set affect the training of forecasting models, which are commonly used for time series anomaly detection, and show that our method improves the training of the model.

</p>
</details>

<details><summary><b>Machine Learning Methods for Spectral Efficiency Prediction in Massive MIMO Systems</b>
<a href="https://arxiv.org/abs/2112.14423">arxiv:2112.14423</a>
&#x1F4C8; 4 <br>
<p>Evgeny Bobrov, Sergey Troshin, Nadezhda Chirkova, Ekaterina Lobacheva, Sviatoslav Panchenko, Dmitry Vetrov, Dmitry Kropotov</p></summary>
<p>

**Abstract:** Channel decoding, channel detection, channel assessment, and resource management for wireless multiple-input multiple-output (MIMO) systems are all examples of problems where machine learning (ML) can be successfully applied. In this paper, we study several ML approaches to solve the problem of estimating the spectral efficiency (SE) value for a certain precoding scheme, preferably in the shortest possible time. The best results in terms of mean average percentage error (MAPE) are obtained with gradient boosting over sorted features, while linear models demonstrate worse prediction quality. Neural networks perform similarly to gradient boosting, but they are more resource- and time-consuming because of hyperparameter tuning and frequent retraining. We investigate the practical applicability of the proposed algorithms in a wide range of scenarios generated by the Quadriga simulator. In almost all scenarios, the MAPE achieved using gradient boosting and neural networks is less than 10\%.

</p>
</details>

<details><summary><b>Runway Extraction and Improved Mapping from Space Imagery</b>
<a href="https://arxiv.org/abs/2201.00848">arxiv:2201.00848</a>
&#x1F4C8; 3 <br>
<p>David A. Noever</p></summary>
<p>

**Abstract:** Change detection methods applied to monitoring key infrastructure like airport runways represent an important capability for disaster relief and urban planning. The present work identifies two generative adversarial networks (GAN) architectures that translate reversibly between plausible runway maps and satellite imagery. We illustrate the training capability using paired images (satellite-map) from the same point of view and using the Pix2Pix architecture or conditional GANs. In the absence of available pairs, we likewise show that CycleGAN architectures with four network heads (discriminator-generator pairs) can also provide effective style transfer from raw image pixels to outline or feature maps. To emphasize the runway and tarmac boundaries, we experimentally show that the traditional grey-tan map palette is not a required training input but can be augmented by higher contrast mapping palettes (red-black) for sharper runway boundaries. We preview a potentially novel use case (called "sketch2satellite") where a human roughly draws the current runway boundaries and automates the machine output of plausible satellite images. Finally, we identify examples of faulty runway maps where the published satellite and mapped runways disagree but an automated update renders the correct map using GANs.

</p>
</details>

<details><summary><b>An Efficient and Accurate Rough Set for Feature Selection, Classification and Knowledge Representation</b>
<a href="https://arxiv.org/abs/2201.00436">arxiv:2201.00436</a>
&#x1F4C8; 3 <br>
<p>Shuyin Xia, Xinyu Bai, Guoyin Wang, Deyu Meng, Xinbo Gao, Zizhong Chen, Elisabeth Giem</p></summary>
<p>

**Abstract:** This paper present a strong data mining method based on rough set, which can realize feature selection, classification and knowledge representation at the same time. Rough set has good interpretability, and is a popular method for feature selections. But low efficiency and low accuracy are its main drawbacks that limits its application ability. In this paper,corresponding to the accuracy, we first find the ineffectiveness of rough set because of overfitting, especially in processing noise attribute, and propose a robust measurement for an attribute, called relative importance.we proposed the concept of "rough concept tree" for knowledge representation and classification. Experimental results on public benchmark data sets show that the proposed framework achieves higher accurcy than seven popular or the state-of-the-art feature selection methods.

</p>
</details>

<details><summary><b>The SAMME.C2 algorithm for severely imbalanced multi-class classification</b>
<a href="https://arxiv.org/abs/2112.14868">arxiv:2112.14868</a>
&#x1F4C8; 3 <br>
<p>Banghee So, Emiliano A. Valdez</p></summary>
<p>

**Abstract:** Classification predictive modeling involves the accurate assignment of observations in a dataset to target classes or categories. There is an increasing growth of real-world classification problems with severely imbalanced class distributions. In this case, minority classes have much fewer observations to learn from than those from majority classes. Despite this sparsity, a minority class is often considered the more interesting class yet developing a scientific learning algorithm suitable for the observations presents countless challenges. In this article, we suggest a novel multi-class classification algorithm specialized to handle severely imbalanced classes based on the method we refer to as SAMME.C2. It blends the flexible mechanics of the boosting techniques from SAMME algorithm, a multi-class classifier, and Ada.C2 algorithm, a cost-sensitive binary classifier designed to address highly class imbalances. Not only do we provide the resulting algorithm but we also establish scientific and statistical formulation of our proposed SAMME.C2 algorithm. Through numerical experiments examining various degrees of classifier difficulty, we demonstrate consistent superior performance of our proposed model.

</p>
</details>

<details><summary><b>Application of Hierarchical Temporal Memory Theory for Document Categorization</b>
<a href="https://arxiv.org/abs/2112.14820">arxiv:2112.14820</a>
&#x1F4C8; 3 <br>
<p>Deven Shah, Pinak Ghate, Manali Paranjape, Amit Kumar</p></summary>
<p>

**Abstract:** The current work intends to study the performance of the Hierarchical Temporal Memory(HTM) theory for automated classification of text as well as documents. HTM is a biologically inspired theory based on the working principles of the human neocortex. The current study intends to provide an alternative framework for document categorization using the Spatial Pooler learning algorithm in the HTM Theory. As HTM accepts only a stream of binary data as input, Latent Semantic Indexing(LSI) technique is used for extracting the top features from the input and converting them into binary format. The Spatial Pooler algorithm converts the binary input into sparse patterns with similar input text having overlapping spatial patterns making it easy for classifying the patterns into categories. The results obtained prove that HTM theory, although is in its nascent stages, performs at par with most of the popular machine learning based classifiers.

</p>
</details>

<details><summary><b>Active Learning-Based Optimization of Scientific Experimental Design</b>
<a href="https://arxiv.org/abs/2112.14811">arxiv:2112.14811</a>
&#x1F4C8; 3 <br>
<p>Ruoyu Wang</p></summary>
<p>

**Abstract:** Active learning (AL) is a machine learning algorithm that can achieve greater accuracy with fewer labeled training instances, for having the ability to ask oracles to label the most valuable unlabeled data chosen iteratively and heuristically by query strategies. Scientific experiments nowadays, though becoming increasingly automated, are still suffering from human involvement in the designing process and the exhaustive search in the experimental space. This article performs a retrospective study on a drug response dataset using the proposed AL scheme comprised of the matrix factorization method of alternating least square (ALS) and deep neural networks (DNN). This article also proposes an AL query strategy based on expected loss minimization. As a result, the retrospective study demonstrates that scientific experimental design, instead of being manually set, can be optimized by AL, and the proposed query strategy ELM sampling shows better experimental performance than other ones such as random sampling and uncertainty sampling.

</p>
</details>

<details><summary><b>Learning Inception Attention for Image Synthesis and Image Recognition</b>
<a href="https://arxiv.org/abs/2112.14804">arxiv:2112.14804</a>
&#x1F4C8; 3 <br>
<p>Jianghao Shen, Tianfu Wu</p></summary>
<p>

**Abstract:** Image synthesis and image recognition have witnessed remarkable progress, but often at the expense of computationally expensive training and inference. Learning lightweight yet expressive deep model has emerged as an important and interesting direction. Inspired by the well-known split-transform-aggregate design heuristic in the Inception building block, this paper proposes a Skip-Layer Inception Module (SLIM) that facilitates efficient learning of image synthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger alternative to the well-known ResNeXts for image recognition. In SLIM, the input feature map is first split into a number of groups (e.g., 4).Each group is then transformed to a latent style vector(via channel-wise attention) and a latent spatial mask (via spatial attention). The learned latent masks and latent style vectors are aggregated to modulate the target feature map. For generative learning, SLIM is built on a recently proposed lightweight Generative Adversarial Networks (i.e., FastGANs) which present a skip-layer excitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM achieves better performance than the SLE work and other related methods. For one-shot image synthesis tasks, it shows stronger capability of preserving images structures than prior arts such as the SinGANs. For image classification tasks, the proposed SLIM is used as a drop-in replacement for convolution layers in ResNets (resulting in ResNeXt-like models) and achieves better accuracy in theImageNet-1000 dataset, with significantly smaller model complexity

</p>
</details>

<details><summary><b>An additive graphical model for discrete data</b>
<a href="https://arxiv.org/abs/2112.14674">arxiv:2112.14674</a>
&#x1F4C8; 3 <br>
<p>Jun Tao, Bing Li, Lingzhou Xue</p></summary>
<p>

**Abstract:** We introduce a nonparametric graphical model for discrete node variables based on additive conditional independence. Additive conditional independence is a three way statistical relation that shares similar properties with conditional independence by satisfying the semi-graphoid axioms. Based on this relation we build an additive graphical model for discrete variables that does not suffer from the restriction of a parametric model such as the Ising model. We develop an estimator of the new graphical model via the penalized estimation of the discrete version of the additive precision operator and establish the consistency of the estimator under the ultrahigh-dimensional setting. Along with these methodological developments, we also exploit the properties of discrete random variables to uncover a deeper relation between additive conditional independence and conditional independence than previously known. The new graphical model reduces to a conditional independence graphical model under certain sparsity conditions. We conduct simulation experiments and analysis of an HIV antiretroviral therapy data set to compare the new method with existing ones.

</p>
</details>

<details><summary><b>Universal Online Learning with Bounded Loss: Reduction to Binary Classification</b>
<a href="https://arxiv.org/abs/2112.14638">arxiv:2112.14638</a>
&#x1F4C8; 3 <br>
<p>MoÃ¯se Blanchard, Romain Cosson</p></summary>
<p>

**Abstract:** We study universal consistency of non-i.i.d. processes in the context of online learning. A stochastic process is said to admit universal consistency if there exists a learner that achieves vanishing average loss for any measurable response function on this process. When the loss function is unbounded, Blanchard et al. showed that the only processes admitting strong universal consistency are those taking a finite number of values almost surely. However, when the loss function is bounded, the class of processes admitting strong universal consistency is much richer and its characterization could be dependent on the response setting (Hanneke). In this paper, we show that this class of processes is independent from the response setting thereby closing an open question (Hanneke, Open Problem 3). Specifically, we show that the class of processes that admit universal online learning is the same for binary classification as for multiclass classification with countable number of classes. Consequently, any output setting with bounded loss can be reduced to binary classification. Our reduction is constructive and practical. Indeed, we show that the nearest neighbor algorithm is transported by our construction. For binary classification on a process admitting strong universal learning, we prove that nearest neighbor successfully learns at least all finite unions of intervals.

</p>
</details>

<details><summary><b>Isotuning With Applications To Scale-Free Online Learning</b>
<a href="https://arxiv.org/abs/2112.14586">arxiv:2112.14586</a>
&#x1F4C8; 3 <br>
<p>Laurent Orseau, Marcus Hutter</p></summary>
<p>

**Abstract:** We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain centered bounds for many algorithms, to prevent the regret bounds from being vacuous when the domain is overly large or only partially constrained. The last tool, null updates, prevents the algorithm from performing overly large updates, which could result in unbounded regret, or even invalid updates. We develop a general theory using these tools and apply it to several standard algorithms. In particular, we (almost entirely) restore the adaptivity to small losses of FTRL for unbounded domains, design and prove scale-free adaptive guarantees for a variant of Mirror Descent (at least when the Bregman divergence is convex in its second argument), extend Adapt-ML-Prod to scale-free guarantees, and provide several other minor contributions about Prod, AdaHedge, BOA and Soft-Bayes.

</p>
</details>

<details><summary><b>Active Learning of Quantum System Hamiltonians yields Query Advantage</b>
<a href="https://arxiv.org/abs/2112.14553">arxiv:2112.14553</a>
&#x1F4C8; 3 <br>
<p>Arkopal Dutt, Edwin Pednault, Chai Wah Wu, Sarah Sheldon, John Smolin, Lev Bishop, Isaac L. Chuang</p></summary>
<p>

**Abstract:** Hamiltonian learning is an important procedure in quantum system identification, calibration, and successful operation of quantum computers. Through queries to the quantum system, this procedure seeks to obtain the parameters of a given Hamiltonian model and description of noise sources. Standard techniques for Hamiltonian learning require careful design of queries and $O(Îµ^{-2})$ queries in achieving learning error $Îµ$ due to the standard quantum limit. With the goal of efficiently and accurately estimating the Hamiltonian parameters within learning error $Îµ$ through minimal queries, we introduce an active learner that is given an initial set of training examples and the ability to interactively query the quantum system to generate new training data. We formally specify and experimentally assess the performance of this Hamiltonian active learning (HAL) algorithm for learning the six parameters of a two-qubit cross-resonance Hamiltonian on four different superconducting IBM Quantum devices. Compared with standard techniques for the same problem and a specified learning error, HAL achieves up to a $99.8\%$ reduction in queries required, and a $99.1\%$ reduction over the comparable non-adaptive learning algorithm. Moreover, with access to prior information on a subset of Hamiltonian parameters and given the ability to select queries with linearly (or exponentially) longer system interaction times during learning, HAL can exceed the standard quantum limit and achieve Heisenberg (or super-Heisenberg) limited convergence rates during learning.

</p>
</details>

<details><summary><b>GPS: A Policy-driven Sampling Approach for Graph Representation Learning</b>
<a href="https://arxiv.org/abs/2112.14482">arxiv:2112.14482</a>
&#x1F4C8; 3 <br>
<p>Tiehua Zhang, Yuze Liu, Xin Chen, Xiaowei Huang, Feng Zhu, Xi Zheng</p></summary>
<p>

**Abstract:** Graph representation learning has drawn increasing attention in recent years, especially for learning the low dimensional embedding at both node and graph level for classification and recommendations tasks. To enable learning the representation on the large-scale graph data in the real world, numerous research has focused on developing different sampling strategies to facilitate the training process. Herein, we propose an adaptive Graph Policy-driven Sampling model (GPS), where the influence of each node in the local neighborhood is realized through the adaptive correlation calculation. Specifically, the selections of the neighbors are guided by an adaptive policy algorithm, contributing directly to the message aggregation, node embedding updating, and graph level readout steps. We then conduct comprehensive experiments against baseline methods on graph classification tasks from various perspectives. Our proposed model outperforms the existing ones by 3%-8% on several vital benchmarks, achieving state-of-the-art performance in real-world datasets.

</p>
</details>

<details><summary><b>Baihe: SysML Framework for AI-driven Databases</b>
<a href="https://arxiv.org/abs/2112.14460">arxiv:2112.14460</a>
&#x1F4C8; 3 <br>
<p>Andreas Pfadler, Rong Zhu, Wei Chen, Botong Huang, Tianjing Zeng, Bolin Ding, Jingren Zhou</p></summary>
<p>

**Abstract:** We present Baihe, a SysML Framework for AI-driven Databases. Using Baihe, an existing relational database system may be retrofitted to use learned components for query optimization or other common tasks, such as e.g. learned structure for indexing. To ensure the practicality and real world applicability of Baihe, its high level architecture is based on the following requirements: separation from the core system, minimal third party dependencies, Robustness, stability and fault tolerance, as well as stability and configurability. Based on the high level architecture, we then describe a concrete implementation of Baihe for PostgreSQL and present example use cases for learned query optimizers. To serve both practitioners, as well as researchers in the DB and AI4DB community Baihe for PostgreSQL will be released under open source license.

</p>
</details>

<details><summary><b>Control Theoretic Analysis of Temporal Difference Learning</b>
<a href="https://arxiv.org/abs/2112.14417">arxiv:2112.14417</a>
&#x1F4C8; 3 <br>
<p>Donghwan Lee</p></summary>
<p>

**Abstract:** The goal of this paper is to investigate a control theoretic analysis of linear stochastic iterative algorithm and temporal difference (TD) learning. TD-learning is a linear stochastic iterative algorithm to estimate the value function of a given policy for a Markov decision process, which is one of the most popular and fundamental reinforcement learning algorithms. While there has been a series of successful works in theoretical analysis of TD-learning, it was not until recently that researchers found some guarantees on its statistical efficiency. In this paper, we propose a control theoretic finite-time analysis TD-learning, which exploits standard notions in linear system control communities. Therefore, the proposed work provides additional insights on TD-learning and reinforcement learning with simple concepts and analysis tools in control theory.

</p>
</details>

<details><summary><b>PowerGraph: Using neural networks and principal components to multivariate statistical power trade-offs</b>
<a href="https://arxiv.org/abs/2201.00719">arxiv:2201.00719</a>
&#x1F4C8; 2 <br>
<p>Ajinkya K Mulay, Sean Lane, Erin Hennes</p></summary>
<p>

**Abstract:** It is increasingly acknowledged that a priori statistical power estimation for planned studies with multiple model parameters is inherently a multivariate problem. Power for individual parameters of interest cannot be reliably estimated univariately because sampling variably in, correlation with, and variance explained relative to one parameter will impact the power for another parameter, all usual univariate considerations being equal. Explicit solutions in such cases, especially for models with many parameters, are either impractical or impossible to solve, leaving researchers with the prevailing method of simulating power. However, point estimates for a vector of model parameters are uncertain, and the impact of inaccuracy is unknown. In such cases, sensitivity analysis is recommended such that multiple combinations of possible observable parameter vectors are simulated to understand power trade-offs. A limitation to this approach is that it is computationally expensive to generate sufficient sensitivity combinations to accurately map the power trade-off function in increasingly high dimensional spaces for the models that social scientists estimate. This paper explores the efficient estimation and graphing of statistical power for a study over varying model parameter combinations. Optimally powering a study is crucial to ensure a minimum probability of finding the hypothesized effect. We first demonstrate the impact of varying parameter values on power for specific hypotheses of interest and quantify the computational intensity of computing such a graph for a given level of precision. Finally, we propose a simple and generalizable machine learning inspired solution to cut the computational cost to less than 7\% of what could be called a brute force approach. [abridged]

</p>
</details>

<details><summary><b>VDPC: Variational Density Peak Clustering Algorithm</b>
<a href="https://arxiv.org/abs/2201.00641">arxiv:2201.00641</a>
&#x1F4C8; 2 <br>
<p>Yizhang Wang, Di Wang, You Zhou, Xiaofeng Zhang, Chai Quek</p></summary>
<p>

**Abstract:** The widely applied density peak clustering (DPC) algorithm makes an intuitive cluster formation assumption that cluster centers are often surrounded by data points with lower local density and far away from other data points with higher local density. However, this assumption suffers from one limitation that it is often problematic when identifying clusters with lower density because they might be easily merged into other clusters with higher density. As a result, DPC may not be able to identify clusters with variational density. To address this issue, we propose a variational density peak clustering (VDPC) algorithm, which is designed to systematically and autonomously perform the clustering task on datasets with various types of density distributions. Specifically, we first propose a novel method to identify the representatives among all data points and construct initial clusters based on the identified representatives for further analysis of the clusters' property. Furthermore, we divide all data points into different levels according to their local density and propose a unified clustering framework by combining the advantages of both DPC and DBSCAN. Thus, all the identified initial clusters spreading across different density levels are systematically processed to form the final clusters. To evaluate the effectiveness of the proposed VDPC algorithm, we conduct extensive experiments using 20 datasets including eight synthetic, six real-world and six image datasets. The experimental results show that VDPC outperforms two classical algorithms (i.e., DPC and DBSCAN) and four state-of-the-art extended DPC algorithms.

</p>
</details>

<details><summary><b>Artificial Intelligence and Statistical Techniques in Short-Term Load Forecasting: A Review</b>
<a href="https://arxiv.org/abs/2201.00437">arxiv:2201.00437</a>
&#x1F4C8; 2 <br>
<p>Ali Bou Nassif, Bassel Soudan, Mohammad Azzeh, Imtinan Attilli, Omar AlMulla</p></summary>
<p>

**Abstract:** Electrical utilities depend on short-term demand forecasting to proactively adjust production and distribution in anticipation of major variations. This systematic review analyzes 240 works published in scholarly journals between 2000 and 2019 that focus on applying Artificial Intelligence (AI), statistical, and hybrid models to short-term load forecasting (STLF). This work represents the most comprehensive review of works on this subject to date. A complete analysis of the literature is conducted to identify the most popular and accurate techniques as well as existing gaps. The findings show that although Artificial Neural Networks (ANN) continue to be the most commonly used standalone technique, researchers have been exceedingly opting for hybrid combinations of different techniques to leverage the combined advantages of individual methods. The review demonstrates that it is commonly possible with these hybrid combinations to achieve prediction accuracy exceeding 99%. The most successful duration for short-term forecasting has been identified as prediction for a duration of one day at an hourly interval. The review has identified a deficiency in access to datasets needed for training of the models. A significant gap has been identified in researching regions other than Asia, Europe, North America, and Australia.

</p>
</details>

<details><summary><b>A Literature Review on Length of Stay Prediction for Stroke Patients using Machine Learning and Statistical Approaches</b>
<a href="https://arxiv.org/abs/2201.00005">arxiv:2201.00005</a>
&#x1F4C8; 2 <br>
<p>Ola Alkhatib, Ayman Alahmar</p></summary>
<p>

**Abstract:** Hospital length of stay (LOS) is one of the most essential healthcare metrics that reflects the hospital quality of service and helps improve hospital scheduling and management. LOS prediction helps in cost management because patients who remain in hospitals usually do so in hospital units where resources are severely limited. In this study, we reviewed papers on LOS prediction using machine learning and statistical approaches. Our literature review considers research studies that focus on LOS prediction for stroke patients. Some of the surveyed studies revealed that authors reached contradicting conclusions. For example, the age of the patient was considered an important predictor of LOS for stroke patients in some studies, while other studies concluded that age was not a significant factor. Therefore, additional research is required in this domain to further understand the predictors of LOS for stroke patients.

</p>
</details>

<details><summary><b>A Unified and Constructive Framework for the Universality of Neural Networks</b>
<a href="https://arxiv.org/abs/2112.14877">arxiv:2112.14877</a>
&#x1F4C8; 2 <br>
<p>Tan Bui-Thanh</p></summary>
<p>

**Abstract:** One of the reasons that many neural networks are capable of replicating complicated tasks or functions is their universality property. The past few decades have seen many attempts in providing constructive proofs for single or class of neural networks. This paper is an effort to provide a unified and constructive framework for the universality of a large class of activations including most of existing activations and beyond. At the heart of the framework is the concept of neural network approximate identity. It turns out that most of existing activations are neural network approximate identity, and thus universal in the space of continuous of functions on compacta. The framework induces several advantages. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first unified attempt that is valid for most of existing activations. Third, as a by product, the framework provides the first university proof for some of the existing activation functions including Mish, SiLU, ELU, GELU, and etc. Fourth, it discovers new activations with guaranteed universality property. Indeed, any activation\textemdash whose $\k$th derivative, with $\k$ being an integer, is integrable and essentially bounded\textemdash is universal. Fifth, for a given activation and error tolerance, the framework provides precisely the architecture of the corresponding one-hidden neural network with predetermined number of neuron, and the values of weights/biases.

</p>
</details>

<details><summary><b>Local Quadratic Convergence of Stochastic Gradient Descent with Adaptive Step Size</b>
<a href="https://arxiv.org/abs/2112.14872">arxiv:2112.14872</a>
&#x1F4C8; 2 <br>
<p>Adityanarayanan Radhakrishnan, Mikhail Belkin, Caroline Uhler</p></summary>
<p>

**Abstract:** Establishing a fast rate of convergence for optimization methods is crucial to their applicability in practice. With the increasing popularity of deep learning over the past decade, stochastic gradient descent and its adaptive variants (e.g. Adagrad, Adam, etc.) have become prominent methods of choice for machine learning practitioners. While a large number of works have demonstrated that these first order optimization methods can achieve sub-linear or linear convergence, we establish local quadratic convergence for stochastic gradient descent with adaptive step size for problems such as matrix inversion.

</p>
</details>

<details><summary><b>A sampling-based approach for efficient clustering in large datasets</b>
<a href="https://arxiv.org/abs/2112.14793">arxiv:2112.14793</a>
&#x1F4C8; 2 <br>
<p>Georgios Exarchakis, Omar Oubari, Gregor Lenz</p></summary>
<p>

**Abstract:** We propose a simple and efficient clustering method for high-dimensional data with a large number of clusters. Our algorithm achieves high-performance by evaluating distances of datapoints with a subset of the cluster centres. Our contribution is substantially more efficient than k-means as it does not require an all to all comparison of data points and clusters. We show that the optimal solutions of our approximation are the same as in the exact solution. However, our approach is considerably more efficient at extracting these clusters compared to the state-of-the-art. We compare our approximation with the exact k-means and alternative approximation approaches on a series of standardised clustering tasks. For the evaluation, we consider the algorithmic complexity, including number of operations to convergence, and the stability of the results.

</p>
</details>

<details><summary><b>Efficient Belief Space Planning in High-Dimensional State Spaces using PIVOT: Predictive Incremental Variable Ordering Tactic</b>
<a href="https://arxiv.org/abs/2112.14428">arxiv:2112.14428</a>
&#x1F4C8; 2 <br>
<p>Khen Elimelech, Vadim Indelman</p></summary>
<p>

**Abstract:** In this work, we examine the problem of online decision making under uncertainty, which we formulate as planning in the belief space. Maintaining beliefs (i.e., distributions) over high-dimensional states (e.g., entire trajectories) was not only shown to significantly improve accuracy, but also allows planning with information-theoretic objectives, as required for the tasks of active SLAM and information gathering. Nonetheless, planning under this "smoothing" paradigm holds a high computational complexity, which makes it challenging for online solution. Thus, we suggest the following idea: before planning, perform a standalone state variable reordering procedure on the initial belief, and "push forwards" all the predicted loop closing variables. Since the initial variable order determines which subset of them would be affected by incoming updates, such reordering allows us to minimize the total number of affected variables, and reduce the computational complexity of candidate evaluation during planning. We call this approach PIVOT: Predictive Incremental Variable Ordering Tactic. Applying this tactic can also improve the state inference efficiency; if we maintain the PIVOT order after the planning session, then we should similarly reduce the cost of loop closures, when they actually occur. To demonstrate its effectiveness, we applied PIVOT in a realistic active SLAM simulation, where we managed to significantly reduce the computation time of both the planning and inference sessions. The approach is applicable to general distributions, and induces no loss in accuracy.

</p>
</details>

<details><summary><b>Data-Driven Computational Methods for the Domain of Attraction and Zubov's Equation</b>
<a href="https://arxiv.org/abs/2112.14415">arxiv:2112.14415</a>
&#x1F4C8; 2 <br>
<p>Wei Kang, Kai Sun, Liang Xu</p></summary>
<p>

**Abstract:** This paper deals with a special type of Lyapunov functions, namely the solution of Zubov's equation. Such a function can be used to characterize the domain of attraction for systems of ordinary differential equations. We derive and prove an integral form solution to Zubov's equation. For numerical computation, we develop two data-driven methods. One is based on the integration of an augmented system of differential equations; and the other one is based on deep learning. The former is effective for systems with a relatively low state space dimension and the latter is developed for high dimensional problems. The deep learning method is applied to a New England 10-generator power system model. We prove that a neural network approximation exists for the Lyapunov function of power systems such that the approximation error is a cubic polynomial of the number of generators. The error convergence rate as a function of n, the number of neurons, is proved.

</p>
</details>

<details><summary><b>Deep learning for location based beamforming with NLOS channels</b>
<a href="https://arxiv.org/abs/2201.01386">arxiv:2201.01386</a>
&#x1F4C8; 1 <br>
<p>Luc Le Magoarou, Taha Yassine, StÃ©phane Paquelet, Matthieu CrussiÃ¨re</p></summary>
<p>

**Abstract:** Massive MIMO systems are highly efficient but critically rely on accurate channel state information (CSI) at the base station in order to determine appropriate precoders. CSI acquisition requires sending pilot symbols which induce an important overhead. In this paper, a method whose objective is to determine an appropriate precoder from the knowledge of the user's location only is proposed. Such a way to determine precoders is known as location based beamforming. It allows to reduce or even eliminate the need for pilot symbols, depending on how the location is obtained. the proposed method learns a direct mapping from location to precoder in a supervised way. It involves a neural network with a specific structure based on random Fourier features allowing to learn functions containing high spatial frequencies. It is assessed empirically and yields promising results on realistic synthetic channels. As opposed to previously proposed methods, it allows to handle both line-of-sight (LOS) and non-line-of-sight (NLOS) channels.

</p>
</details>

<details><summary><b>Neural Myerson Auction for Truthful and Energy-Efficient Autonomous Aerial Data Delivery</b>
<a href="https://arxiv.org/abs/2201.01170">arxiv:2201.01170</a>
&#x1F4C8; 1 <br>
<p>Haemin Lee, Sean Kwon, Soyi Jung, Joongheon Kim</p></summary>
<p>

**Abstract:** A successful deployment of drones provides an ideal solution for surveillance systems. Using drones for surveillance can provide access to areas that may be difficult or impossible to reach by humans or in-land vehicles gathering images or video recordings of a specific target in their coverage. Therefore, we introduces a data delivery drone to transfer collected surveillance data in harsh communication conditions. This paper proposes a Myerson auction-based asynchronous data delivery in an aerial distributed data platform in surveillance systems taking battery limitation and long flight constraints into account. In this paper, multiple delivery drones compete to offer data transfer to a single fixed-location surveillance drone. Our proposed Myerson auction-based algorithm, which uses the truthful second-price auction (SPA) as a baseline, is to maximize the seller's revenue while meeting several desirable properties, i.e., individual rationality and incentive compatibility while pursuing truthful operations. On top of these SPA-based operations, a deep learning-based framework is additionally designed for delivery performance improvements.

</p>
</details>

<details><summary><b>Feature matching as improved transfer learning technique for wearable EEG</b>
<a href="https://arxiv.org/abs/2201.00644">arxiv:2201.00644</a>
&#x1F4C8; 1 <br>
<p>Elisabeth R. M. Heremans, Huy Phan, Amir H. Ansari, Pascal BorzÃ©e, Bertien Buyse, Dries Testelmans, Maarten De Vos</p></summary>
<p>

**Abstract:** Objective: With the rapid rise of wearable sleep monitoring devices with non-conventional electrode configurations, there is a need for automated algorithms that can perform sleep staging on configurations with small amounts of labeled data. Transfer learning has the ability to adapt neural network weights from a source modality (e.g. standard electrode configuration) to a new target modality (e.g. non-conventional electrode configuration). Methods: We propose feature matching, a new transfer learning strategy as an alternative to the commonly used finetuning approach. This method consists of training a model with larger amounts of data from the source modality and few paired samples of source and target modality. For those paired samples, the model extracts features of the target modality, matching these to the features from the corresponding samples of the source modality. Results: We compare feature matching to finetuning for three different target domains, with two different neural network architectures, and with varying amounts of training data. Particularly on small cohorts (i.e. 2 - 5 labeled recordings in the non-conventional recording setting), feature matching systematically outperforms finetuning with mean relative differences in accuracy ranging from 0.4% to 4.7% for the different scenarios and datasets. Conclusion: Our findings suggest that feature matching outperforms finetuning as a transfer learning approach, especially in very low data regimes. Significance: As such, we conclude that feature matching is a promising new method for wearable sleep staging with novel devices.

</p>
</details>

<details><summary><b>A Survey of Deep Learning Techniques for Dynamic Branch Prediction</b>
<a href="https://arxiv.org/abs/2112.14911">arxiv:2112.14911</a>
&#x1F4C8; 1 <br>
<p>Rinu Joseph</p></summary>
<p>

**Abstract:** Branch prediction is an architectural feature that speeds up the execution of branch instruction on pipeline processors and reduces the cost of branching. Recent advancements of Deep Learning (DL) in the post Moore's Law era is accelerating areas of automated chip design, low-power computer architectures, and much more. Traditional computer architecture design and algorithms could benefit from dynamic predictors based on deep learning algorithms which learns from experience by optimizing its parameters on large number of data. In this survey paper, we focus on traditional branch prediction algorithms, analyzes its limitations, and presents a literature survey of how deep learning techniques can be applied to create dynamic branch predictors capable of predicting conditional branch instructions. Prior surveys in this field focus on dynamic branch prediction techniques based on neural network perceptrons. We plan to improve the survey based on latest research in DL and advanced Machine Learning (ML) based branch predictors.

</p>
</details>

<details><summary><b>PINNs for the Solution of the Hyperbolic Buckley-Leverett Problem with a Non-convex Flux Function</b>
<a href="https://arxiv.org/abs/2112.14826">arxiv:2112.14826</a>
&#x1F4C8; 1 <br>
<p>Waleed Diab, Mohammed Al Kobaisi</p></summary>
<p>

**Abstract:** The displacement of two immiscible fluids is a common problem in fluid flow in porous media. Such a problem can be posed as a partial differential equation (PDE) in what is commonly referred to as a Buckley-Leverett (B-L) problem. The B-L problem is a non-linear hyperbolic conservation law that is known to be notoriously difficult to solve using traditional numerical methods. Here, we address the forward hyperbolic B-L problem with a nonconvex flux function using physics-informed neural networks (PINNs). The contributions of this paper are twofold. First, we present a PINN approach to solve the hyperbolic B-L problem by embedding the Oleinik entropy condition into the neural network residual. We do not use a diffusion term (artificial viscosity) in the residual-loss, but we rely on the strong form of the PDE. Second, we use the Adam optimizer with residual-based adaptive refinement (RAR) algorithm to achieve an ultra-low loss without weighting. Our solution method can accurately capture the shock-front and produce an accurate overall solution. We report a L2 validation error of 2 x 10-2 and a L2 loss of 1x 10-6. The proposed method does not require any additional regularization or weighting of losses to obtain such accurate solution.

</p>
</details>

<details><summary><b>Graph Neural Networks for Communication Networks: Context, Use Cases and Opportunities</b>
<a href="https://arxiv.org/abs/2112.14792">arxiv:2112.14792</a>
&#x1F4C8; 1 <br>
<p>JosÃ© SuÃ¡rez-Varela, Paul Almasan, Miquel Ferriol-GalmÃ©s, Krzysztof Rusek, Fabien Geyer, Xiangle Cheng, Xiang Shi, Shihan Xiao, Franco Scarselli, Albert Cabellos-Aparicio, Pere Barlet-Ros</p></summary>
<p>

**Abstract:** Graph neural networks (GNN) have shown outstanding applications in many fields where data is fundamentally represented as graphs (e.g., chemistry, biology, recommendation systems). In this vein, communication networks comprise many fundamental components that are naturally represented in a graph-structured manner (e.g., topology, configurations, traffic flows). This position article presents GNNs as a fundamental tool for modeling, control and management of communication networks. GNNs represent a new generation of data-driven models that can accurately learn and reproduce the complex behaviors behind real networks. As a result, such models can be applied to a wide variety of networking use cases, such as planning, online optimization, or troubleshooting. The main advantage of GNNs over traditional neural networks lies in its unprecedented generalization capabilities when applied to other networks and configurations unseen during training, which is a critical feature for achieving practical data-driven solutions for networking. This article comprises a brief tutorial on GNNs and their possible applications to communication networks. To showcase the potential of this technology, we present two use cases with state-of-the-art GNN models respectively applied to wired and wireless networks. Lastly, we delve into the key open challenges and opportunities yet to be explored in this novel research area.

</p>
</details>

<details><summary><b>Learning nonlinear dynamics in synchronization of knowledge-based leader-following networks</b>
<a href="https://arxiv.org/abs/2112.14676">arxiv:2112.14676</a>
&#x1F4C8; 1 <br>
<p>Shimin Wang, Xiangyu Meng, Hongwei Zhang, Frank L. Lewis</p></summary>
<p>

**Abstract:** Knowledge-based leader-following synchronization problem of heterogeneous nonlinear multi-agent systems is challenging since the leader's dynamic information is unknown to all follower nodes. This paper proposes a learning-based fully distributed observer for a class of nonlinear leader systems, which can simultaneously learn the leader's dynamics and states. The class of leader dynamics considered here does not require a bounded Jacobian matrix. Based on this learning-based distributed observer, we further synthesize an adaptive distributed control law for solving the leader-following synchronization problem of multiple Euler-Lagrange systems subject to an uncertain nonlinear leader system. The results are illustrated by a simulation example.

</p>
</details>

<details><summary><b>Dynamic programming with partial information to overcome navigational uncertainty in a nautical environment</b>
<a href="https://arxiv.org/abs/2112.14657">arxiv:2112.14657</a>
&#x1F4C8; 1 <br>
<p>Chris Beeler, Xinkai Li, Mark Crowley, Maia Fraser, Isaac Tamblyn</p></summary>
<p>

**Abstract:** Using a toy nautical navigation environment, we show that dynamic programming can be used when only partial information about a partially observed Markov decision process (POMDP) is known. By incorporating uncertainty into our model, we show that navigation policies can be constructed that maintain safety. Adding controlled sensing methods, we show that these policies can also lower measurement costs at the same time.

</p>
</details>

<details><summary><b>Challenges and approaches for mitigating byzantine attacks in federated learning</b>
<a href="https://arxiv.org/abs/2112.14468">arxiv:2112.14468</a>
&#x1F4C8; 1 <br>
<p>Shengshan Hu, Jianrong Lu, Wei Wan, Leo Yu Zhang</p></summary>
<p>

**Abstract:** Recently emerged federated learning (FL) is an attractive distributed learning framework in which numerous wireless end-user devices can train a global model with the data remained autochthonous. Compared with the traditional machine learning framework that collects user data for centralized storage, which brings huge communication burden and concerns about data privacy, this approach can not only save the network bandwidth but also protect the data privacy. Despite the promising prospect, byzantine attack, an intractable threat in conventional distributed network, is discovered to be rather efficacious against FL as well. In this paper, we conduct a comprehensive investigation of the state-of-the-art strategies for defending against byzantine attacks in FL. We first provide a taxonomy for the existing defense solutions according to the techniques they used, followed by an across-the-board comparison and discussion. Then we propose a new byzantine attack method called weight attack to defeat those defense schemes, and conduct experiments to demonstrate its threat. The results show that existing defense solutions, although abundant, are still far from fully protecting FL. Finally, we indicate possible countermeasures for weight attack, and highlight several challenges and future research directions for mitigating byzantine attacks in FL.

</p>
</details>

<details><summary><b>End-to-End Autoencoder Communications with Optimized Interference Suppression</b>
<a href="https://arxiv.org/abs/2201.01388">arxiv:2201.01388</a>
&#x1F4C8; 0 <br>
<p>Kemal Davaslioglu, Tugba Erpek, Yalin E. Sagduyu</p></summary>
<p>

**Abstract:** An end-to-end communications system based on Orthogonal Frequency Division Multiplexing (OFDM) is modeled as an autoencoder (AE) for which the transmitter (coding and modulation) and receiver (demodulation and decoding) are represented as deep neural networks (DNNs) of the encoder and decoder, respectively. This AE communications approach is shown to outperform conventional communications in terms of bit error rate (BER) under practical scenarios regarding channel and interference effects as well as training data and embedded implementation constraints. A generative adversarial network (GAN) is trained to augment the training data when there is not enough training data available. Also, the performance is evaluated in terms of the DNN model quantization and the corresponding memory requirements for embedded implementation. Then, interference training and randomized smoothing are introduced to train the AE communications to operate under unknown and dynamic interference (jamming) effects on potentially multiple OFDM symbols. Relative to conventional communications, up to 36 dB interference suppression for a channel reuse of four can be achieved by the AE communications with interference training and randomized smoothing. AE communications is also extended to the multiple-input multiple-output (MIMO) case and its BER performance gain with and without interference effects is demonstrated compared to conventional MIMO communications.

</p>
</details>

<details><summary><b>DeePN$^2$: A deep learning-based non-Newtonian hydrodynamic model</b>
<a href="https://arxiv.org/abs/2112.14798">arxiv:2112.14798</a>
&#x1F4C8; 0 <br>
<p>Lidong Fang, Pei Ge, Lei Zhang, Huan Lei, Weinan E</p></summary>
<p>

**Abstract:** A long standing problem in the modeling of non-Newtonian hydrodynamics is the availability of reliable and interpretable hydrodynamic models that faithfully encode the underlying micro-scale polymer dynamics. The main complication arises from the long polymer relaxation time, the complex molecular structure, and heterogeneous interaction. DeePN$^2$, a deep learning-based non-Newtonian hydrodynamic model, has been proposed and has shown some success in systematically passing the micro-scale structural mechanics information to the macro-scale hydrodynamics for suspensions with simple polymer conformation and bond potential. The model retains a multi-scaled nature by mapping the polymer configurations into a set of symmetry-preserving macro-scale features. The extended constitutive laws for these macro-scale features can be directly learned from the kinetics of their micro-scale counterparts. In this paper, we carry out further study of DeePN$^2$ using more complex micro-structural models. We show that DeePN$^2$ can faithfully capture the broadly overlooked viscoelastic differences arising from the specific molecular structural mechanics without human intervention.

</p>
</details>


{% endraw %}
Prev: [2021.12.28]({{ '/2021/12/28/2021.12.28.html' | relative_url }})  Next: [2021.12.30]({{ '/2021/12/30/2021.12.30.html' | relative_url }})