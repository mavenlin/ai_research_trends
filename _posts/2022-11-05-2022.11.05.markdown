Prev: [2022.11.04]({{ '/2022/11/04/2022.11.04.html' | relative_url }})  Next: [2022.11.06]({{ '/2022/11/06/2022.11.06.html' | relative_url }})
{% raw %}
## Summary for 2022-11-05, created on 2022-11-12


<details><summary><b>Toward Neural Network Simulation of Variational Quantum Algorithms</b>
<a href="https://arxiv.org/abs/2211.02929">arxiv:2211.02929</a>
&#x1F4C8; 76 <br>
<p>Oliver Knitter, James Stokes, Shravan Veerapaneni</p></summary>
<p>

**Abstract:** Variational quantum algorithms (VQAs) utilize a hybrid quantum-classical architecture to recast problems of high-dimensional linear algebra as ones of stochastic optimization. Despite the promise of leveraging near- to intermediate-term quantum resources to accelerate this task, the computational advantage of VQAs over wholly classical algorithms has not been firmly established. For instance, while the variational quantum eigensolver (VQE) has been developed to approximate low-lying eigenmodes of high-dimensional sparse linear operators, analogous classical optimization algorithms exist in the variational Monte Carlo (VMC) literature, utilizing neural networks in place of quantum circuits to represent quantum states. In this paper we ask if classical stochastic optimization algorithms can be constructed paralleling other VQAs, focusing on the example of the variational quantum linear solver (VQLS). We find that such a construction can be applied to the VQLS, yielding a paradigm that could theoretically extend to other VQAs of similar form.

</p>
</details>

<details><summary><b>Learning the shape of protein micro-environments with a holographic convolutional neural network</b>
<a href="https://arxiv.org/abs/2211.02936">arxiv:2211.02936</a>
&#x1F4C8; 32 <br>
<p>Michael N. Pun, Andrew Ivanov, Quinn Bellamy, Zachary Montague, Colin LaMont, Philip Bradley, Jakub Otwinowski, Armita Nourmohammad</p></summary>
<p>

**Abstract:** Proteins play a central role in biology from immune recognition to brain activity. While major advances in machine learning have improved our ability to predict protein structure from sequence, determining protein function from structure remains a major challenge. Here, we introduce Holographic Convolutional Neural Network (H-CNN) for proteins, which is a physically motivated machine learning approach to model amino acid preferences in protein structures. H-CNN reflects physical interactions in a protein structure and recapitulates the functional information stored in evolutionary data. H-CNN accurately predicts the impact of mutations on protein function, including stability and binding of protein complexes. Our interpretable computational model for protein structure-function maps could guide design of novel proteins with desired function.

</p>
</details>

<details><summary><b>Improved Kidney Stone Recognition Through Attention and Multi-View Feature Fusion Strategies</b>
<a href="https://arxiv.org/abs/2211.02967">arxiv:2211.02967</a>
&#x1F4C8; 14 <br>
<p>Elias Villalvazo-Avila, Francisco Lopez-Tiro, Jonathan El-Beze, Jacques Hubert, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul</p></summary>
<p>

**Abstract:** This contribution presents a deep learning method for the extraction and fusion of information relating to kidney stone fragments acquired from different viewpoints of the endoscope. Surface and section fragment images are jointly used during the training of the classifier to improve the discrimination power of the features by adding attention layers at the end of each convolutional block. This approach is specifically designed to mimic the morpho-constitutional analysis performed in ex-vivo by biologists to visually identify kidney stones by inspecting both views. The addition of attention mechanisms to the backbone improved the results of single view extraction backbones by 4% on average. Moreover, in comparison to the state-of-the-art, the fusion of the deep features improved the overall results up to 11% in terms of kidney stone classification accuracy.

</p>
</details>

<details><summary><b>Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing</b>
<a href="https://arxiv.org/abs/2211.03017">arxiv:2211.03017</a>
&#x1F4C8; 7 <br>
<p>Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Jiaxiang Zheng, Rui Tang, Hujun Bao, Rui Wang</p></summary>
<p>

**Abstract:** Indoor scenes typically exhibit complex, spatially-varying appearance from global illumination, making inverse rendering a challenging ill-posed problem. This work presents an end-to-end, learning-based inverse rendering framework incorporating differentiable Monte Carlo raytracing with importance sampling. The framework takes a single image as input to jointly recover the underlying geometry, spatially-varying lighting, and photorealistic materials. Specifically, we introduce a physically-based differentiable rendering layer with screen-space ray tracing, resulting in more realistic specular reflections that match the input photo. In addition, we create a large-scale, photorealistic indoor scene dataset with significantly richer details like complex furniture and dedicated decorations. Further, we design a novel out-of-view lighting network with uncertainty-aware refinement leveraging hypernetwork-based neural radiance fields to predict lighting outside the view of the input photo. Through extensive evaluations on common benchmark datasets, we demonstrate superior inverse rendering quality of our method compared to state-of-the-art baselines, enabling various applications such as complex object insertion and material editing with high fidelity. Code and data will be made available at \url{https://jingsenzhu.github.io/invrend}.

</p>
</details>

<details><summary><b>Physics Informed Machine Learning for Chemistry Tabulation</b>
<a href="https://arxiv.org/abs/2211.03022">arxiv:2211.03022</a>
&#x1F4C8; 6 <br>
<p>Amol Salunkhe, Dwyer Deighan, Paul Desjardin, Varun Chandola</p></summary>
<p>

**Abstract:** Modeling of turbulent combustion system requires modeling the underlying chemistry and the turbulent flow. Solving both systems simultaneously is computationally prohibitive. Instead, given the difference in scales at which the two sub-systems evolve, the two sub-systems are typically (re)solved separately. Popular approaches such as the Flamelet Generated Manifolds (FGM) use a two-step strategy where the governing reaction kinetics are pre-computed and mapped to a low-dimensional manifold, characterized by a few reaction progress variables (model reduction) and the manifold is then ``looked-up'' during the runtime to estimate the high-dimensional system state by the flow system. While existing works have focused on these two steps independently, in this work we show that joint learning of the progress variables and the look--up model, can yield more accurate results. We build on the base formulation and implementation ChemTab to include the dynamically generated Themochemical State Variables (Lower Dimensional Dynamic Source Terms). We discuss the challenges in the implementation of this deep neural network architecture and experimentally demonstrate it's superior performance.

</p>
</details>

<details><summary><b>A Comparison of Automatic Labelling Approaches for Sentiment Analysis</b>
<a href="https://arxiv.org/abs/2211.02976">arxiv:2211.02976</a>
&#x1F4C8; 6 <br>
<p>Sumana Biswas, Karen Young, Josephine Griffith</p></summary>
<p>

**Abstract:** Labelling a large quantity of social media data for the task of supervised machine learning is not only time-consuming but also difficult and expensive. On the other hand, the accuracy of supervised machine learning models is strongly related to the quality of the labelled data on which they train, and automatic sentiment labelling techniques could reduce the time and cost of human labelling. We have compared three automatic sentiment labelling techniques: TextBlob, Vader, and Afinn to assign sentiments to tweets without any human assistance. We compare three scenarios: one uses training and testing datasets with existing ground truth labels; the second experiment uses automatic labels as training and testing datasets; and the third experiment uses three automatic labelling techniques to label the training dataset and uses the ground truth labels for testing. The experiments were evaluated on two Twitter datasets: SemEval-2013 (DS-1) and SemEval-2016 (DS-2). Results show that the Afinn labelling technique obtains the highest accuracy of 80.17% (DS-1) and 80.05% (DS-2) using a BiLSTM deep learning model. These findings imply that automatic text labelling could provide significant benefits, and suggest a feasible alternative to the time and cost of human labelling efforts.

</p>
</details>

<details><summary><b>Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block</b>
<a href="https://arxiv.org/abs/2211.02940">arxiv:2211.02940</a>
&#x1F4C8; 6 <br>
<p>Yunhao Chen, Yunjie Zhu, Zihui Yan, Lifang Chen</p></summary>
<p>

**Abstract:** Recently, massive architectures based on Convolutional Neural Network (CNN) and self-attention mechanisms have become necessary for audio classification. While these techniques are state-of-the-art, these works' effectiveness can only be guaranteed with huge computational costs and parameters, large amounts of data augmentation, transfer from large datasets and some other tricks. By utilizing the lightweight nature of audio, we propose an efficient network structure called Paired Inverse Pyramid Structure (PIP) and a network called Paired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\% of Environmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset and 93.2\% of Music Genre Classification (MGC) on the GTAZN dataset, with only 1 million parameters. Both of the results are achieved without data augmentation or model transfer. Public code is available at: https://github.com/JNAIC/PIPMN

</p>
</details>

<details><summary><b>Grassmann Manifold Flow</b>
<a href="https://arxiv.org/abs/2211.02900">arxiv:2211.02900</a>
&#x1F4C8; 6 <br>
<p>Ryoma Yataka, Masashi Shiraishi</p></summary>
<p>

**Abstract:** Recently, studies on machine learning have focused on methods that use symmetry implicit in a specific manifold as an inductive bias. In particular, approaches using Grassmann manifolds have been found to exhibit effective performance in fields such as point cloud and image set analysis. However, there is a lack of research on the construction of general learning models to learn distributions on the Grassmann manifold. In this paper, we lay the theoretical foundations for learning distributions on the Grassmann manifold via continuous normalizing flows. Experimental results show that the proposed method can generate high-quality samples by capturing the data structure. Further, the proposed method significantly outperformed state-of-the-art methods in terms of log-likelihood or evidence lower bound. The results obtained are expected to usher in further research in this field of study.

</p>
</details>

<details><summary><b>FLock: Defending Malicious Behaviors in Federated Learning with Blockchain</b>
<a href="https://arxiv.org/abs/2211.04344">arxiv:2211.04344</a>
&#x1F4C8; 5 <br>
<p>Nanqing Dong, Jiahao Sun, Zhipeng Wang, Shuoying Zhang, Shuhao Zheng</p></summary>
<p>

**Abstract:** Federated learning (FL) is a promising way to allow multiple data owners (clients) to collaboratively train machine learning models without compromising data privacy. Yet, existing FL solutions usually rely on a centralized aggregator for model weight aggregation, while assuming clients are honest. Even if data privacy can still be preserved, the problem of single-point failure and data poisoning attack from malicious clients remains unresolved. To tackle this challenge, we propose to use distributed ledger technology (DLT) to achieve FLock, a secure and reliable decentralized Federated Learning system built on blockchain. To guarantee model quality, we design a novel peer-to-peer (P2P) review and reward/slash mechanism to detect and deter malicious clients, powered by on-chain smart contracts. The reward/slash mechanism, in addition, serves as incentives for participants to honestly upload and review model parameters in the FLock system. FLock thus improves the performance and the robustness of FL systems in a fully P2P manner.

</p>
</details>

<details><summary><b>High-Fidelity Simulation and Novel Data Analysis of the Bubble Creation and Sound Generation Processes in Breaking Waves</b>
<a href="https://arxiv.org/abs/2211.03024">arxiv:2211.03024</a>
&#x1F4C8; 5 <br>
<p>Qiang Gao, Grant B. Deane, Saswata Basak, Umberto Bitencourt, Lian Shen</p></summary>
<p>

**Abstract:** Recent increases in computing power have enabled the numerical simulation of many complex flow problems that are of practical and strategic interest for naval applications. A noticeable area of advancement is the computation of turbulent, two-phase flows resulting from wave breaking and other multiphase flow processes such as cavitation that can generate underwater sound and entrain bubbles in ship wakes, among other effects. Although advanced flow solvers are sophisticated and are capable of simulating high Reynolds number flows on large numbers of grid points, challenges in data analysis remain. Specifically, there is a critical need to transform highly resolved flow fields described on fine grids at discrete time steps into physically resolved features for which the flow dynamics can be understood and utilized in naval applications. This paper presents our recent efforts in this field. In previous works, we developed a novel algorithm to track bubbles in breaking wave simulations and to interpret their dynamical behavior over time (Gao et al., 2021a). We also discovered a new physical mechanism driving bubble production within breaking wave crests (Gao et al., 2021b) and developed a model to relate bubble behaviors to underwater sound generation (Gao et al., 2021c). In this work, we applied our bubble tracking algorithm to the breaking waves simulations and investigated the bubble trajectories, bubble creation mechanisms, and bubble acoustics based on our previous works.

</p>
</details>

<details><summary><b>PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training</b>
<a href="https://arxiv.org/abs/2211.02816">arxiv:2211.02816</a>
&#x1F4C8; 5 <br>
<p>Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du</p></summary>
<p>

**Abstract:** Fact verification has attracted a lot of research attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and disinformation online can sway one's opinion and affect one's actions. While fact-checking is a hard task in general, in many cases, false statements can be easily debunked based on analytics over tables with reliable information. Hence, table-based fact verification has recently emerged as an important and growing research area. Yet, progress has been limited due to the lack of datasets that can be used to pre-train language models (LMs) to be aware of common table operations, such as aggregating a column or comparing tuples. To bridge this gap, in this paper we introduce PASTA, a novel state-of-the-art framework for table-based fact verification via pre-training with synthesized sentence-table cloze questions. In particular, we design six types of common sentence-table cloze tasks, including Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique, based on which we synthesize a large corpus consisting of 1.2 million sentence-table pairs from WikiTables. PASTA uses a recent pre-trained LM, DeBERTaV3, and further pretrains it on our corpus. Our experimental results show that PASTA achieves new state-of-the-art performance on two table-based fact verification benchmarks: TabFact and SEM-TAB-FACTS. In particular, on the complex set of TabFact, which contains multiple operations, PASTA largely outperforms the previous state of the art by 4.7 points (85.6% vs. 80.9%), and the gap between PASTA and human performance on the small TabFact test set is narrowed to just 1.5 points (90.6% vs. 92.1%).

</p>
</details>

<details><summary><b>Quantum Deep Dreaming: A Novel Approach for Quantum Circuit Design</b>
<a href="https://arxiv.org/abs/2211.04343">arxiv:2211.04343</a>
&#x1F4C8; 4 <br>
<p>Romi Lifshitz</p></summary>
<p>

**Abstract:** One of the challenges currently facing the quantum computing community is the design of quantum circuits which can efficiently run on near-term quantum computers, known as the quantum compiling problem. Algorithms such as the Variational Quantum Eigensolver (VQE), Quantum Approximate Optimization Algorithm (QAOA), and Quantum Architecture Search (QAS) have been shown to generate or find optimal near-term quantum circuits. However, these methods are computationally expensive and yield little insight into the circuit design process. In this paper, we propose Quantum Deep Dreaming (QDD), an algorithm that generates optimal quantum circuit architectures for specified objectives, such as ground state preparation, while providing insight into the circuit design process. In QDD, we first train a neural network to predict some property of a quantum circuit (such as VQE energy). Then, we employ the Deep Dreaming technique on the trained network to iteratively update an initial circuit to achieve a target property value (such as ground state VQE energy). Importantly, this iterative updating allows us to analyze the intermediate circuits of the dreaming process and gain insights into the circuit features that the network is modifying during dreaming. We demonstrate that QDD successfully generates, or 'dreams', circuits of six qubits close to ground state energy (Transverse Field Ising Model VQE energy) and that dreaming analysis yields circuit design insights. QDD is designed to optimize circuits with any target property and can be applied to circuit design problems both within and outside of quantum chemistry. Hence, QDD lays the foundation for the future discovery of optimized quantum circuits and for increased interpretability of automated quantum algorithm design.

</p>
</details>

<details><summary><b>Toward Human-AI Co-creation to Accelerate Material Discovery</b>
<a href="https://arxiv.org/abs/2211.04257">arxiv:2211.04257</a>
&#x1F4C8; 4 <br>
<p>Dmitry Zubarev, Carlos Raoni Mendes, Emilio Vital Brazil, Renato Cerqueira, Kristin Schmidt, Vinicius Segura, Juliana Jansen Ferreira, Dan Sanders</p></summary>
<p>

**Abstract:** There is an increasing need in our society to achieve faster advances in Science to tackle urgent problems, such as climate changes, environmental hazards, sustainable energy systems, pandemics, among others. In certain domains like chemistry, scientific discovery carries the extra burden of assessing risks of the proposed novel solutions before moving to the experimental stage. Despite several recent advances in Machine Learning and AI to address some of these challenges, there is still a gap in technologies to support end-to-end discovery applications, integrating the myriad of available technologies into a coherent, orchestrated, yet flexible discovery process. Such applications need to handle complex knowledge management at scale, enabling knowledge consumption and production in a timely and efficient way for subject matter experts (SMEs). Furthermore, the discovery of novel functional materials strongly relies on the development of exploration strategies in the chemical space. For instance, generative models have gained attention within the scientific community due to their ability to generate enormous volumes of novel molecules across material domains. These models exhibit extreme creativity that often translates in low viability of the generated candidates. In this work, we propose a workbench framework that aims at enabling the human-AI co-creation to reduce the time until the first discovery and the opportunity costs involved. This framework relies on a knowledge base with domain and process knowledge, and user-interaction components to acquire knowledge and advise the SMEs. Currently,the framework supports four main activities: generative modeling, dataset triage, molecule adjudication, and risk assessment.

</p>
</details>

<details><summary><b>On learning history based policies for controlling Markov decision processes</b>
<a href="https://arxiv.org/abs/2211.03011">arxiv:2211.03011</a>
&#x1F4C8; 4 <br>
<p>Gandharv Patil, Aditya Mahajan, Doina Precup</p></summary>
<p>

**Abstract:** Reinforcementlearning(RL)folkloresuggeststhathistory-basedfunctionapproximationmethods,suchas recurrent neural nets or history-based state abstraction, perform better than their memory-less counterparts, due to the fact that function approximation in Markov decision processes (MDP) can be viewed as inducing a Partially observable MDP. However, there has been little formal analysis of such history-based algorithms, as most existing frameworks focus exclusively on memory-less features. In this paper, we introduce a theoretical framework for studying the behaviour of RL algorithms that learn to control an MDP using history-based feature abstraction mappings. Furthermore, we use this framework to design a practical RL algorithm and we numerically evaluate its effectiveness on a set of continuous control tasks.

</p>
</details>

<details><summary><b>A Comparative Analysis of the Face Recognition Methods in Video Surveillance Scenarios</b>
<a href="https://arxiv.org/abs/2211.02952">arxiv:2211.02952</a>
&#x1F4C8; 4 <br>
<p>Eker Onur, Bal Murat</p></summary>
<p>

**Abstract:** Facial recognition is fundamental for a wide variety of security systems operating in real-time applications. In video surveillance based face recognition, face images are typically captured over multiple frames in uncontrolled conditions; where head pose, illumination, shadowing, motion blur and focus change over the sequence. We can generalize that the three fundamental operations involved in the facial recognition tasks: face detection, face alignment and face recognition. This study presents comparative benchmark tables for the state-of-art face recognition methods by testing them with same backbone architecture in order to focus only on the face recognition solution instead of network architecture. For this purpose, we constructed a video surveillance dataset of face IDs that has high age variance, intra-class variance (face make-up, beard, etc.) with native surveillance facial imagery data for evaluation. On the other hand, this work discovers the best recognition methods for different conditions like non-masked faces, masked faces, and faces with glasses.

</p>
</details>

<details><summary><b>Prototypical quadruplet for few-shot class incremental learning</b>
<a href="https://arxiv.org/abs/2211.02947">arxiv:2211.02947</a>
&#x1F4C8; 4 <br>
<p>Sanchar Palit, Biplab Banerjee, Subhasis Chaudhuri</p></summary>
<p>

**Abstract:** Many modern computer vision algorithms suffer from two major bottlenecks: scarcity of data and learning new tasks incrementally. While training the model with new batches of data the model looses it's ability to classify the previous data judiciously which is termed as catastrophic forgetting. Conventional methods have tried to mitigate catastrophic forgetting of the previously learned data while the training at the current session has been compromised. The state-of-the-art generative replay based approaches use complicated structures such as generative adversarial network (GAN) to deal with catastrophic forgetting. Additionally, training a GAN with few samples may lead to instability. In this work, we present a novel method to deal with these two major hurdles. Our method identifies a better embedding space with an improved contrasting loss to make classification more robust. Moreover, our approach is able to retain previously acquired knowledge in the embedding space even when trained with new classes. We update previous session class prototypes while training in such a way that it is able to represent the true class mean. This is of prime importance as our classification rule is based on the nearest class mean classification strategy. We have demonstrated our results by showing that the embedding space remains intact after training the model with new classes. We showed that our method preformed better than the existing state-of-the-art algorithms in terms of accuracy across different sessions.

</p>
</details>

<details><summary><b>Accurate and Reliable Methods for 5G UAV Jamming Identification With Calibrated Uncertainty</b>
<a href="https://arxiv.org/abs/2211.02924">arxiv:2211.02924</a>
&#x1F4C8; 4 <br>
<p>Hamed Farkhari, Joseanne Viana, Pedro Sebastiao, Luis Miguel Campos, Luis Bernardo, Rui Dinis, Sarang Kahvazadeh</p></summary>
<p>

**Abstract:** Only increasing accuracy without considering uncertainty may negatively impact Deep Neural Network (DNN) decision-making and decrease its reliability. This paper proposes five combined preprocessing and post-processing methods for time-series binary classification problems that simultaneously increase the accuracy and reliability of DNN outputs applied in a 5G UAV security dataset. These techniques use DNN outputs as input parameters and process them in different ways. Two methods use a well-known Machine Learning (ML) algorithm as a complement, and the other three use only confidence values that the DNN estimates. We compare seven different metrics, such as the Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Mean Confidence (MC), Mean Accuracy (MA), Normalized Negative Log Likelihood (NLL), Brier Score Loss (BSL), and Reliability Score (RS) and the tradeoffs between them to evaluate the proposed hybrid algorithms. First, we show that the eXtreme Gradient Boosting (XGB) classifier might not be reliable for binary classification under the conditions this work presents. Second, we demonstrate that at least one of the potential methods can achieve better results than the classification in the DNN softmax layer. Finally, we show that the prospective methods may improve accuracy and reliability with better uncertainty calibration based on the assumption that the RS determines the difference between MC and MA metrics, and this difference should be zero to increase reliability. For example, Method 3 presents the best RS of 0.65 even when compared to the XGB classifier, which achieves RS of 7.22.

</p>
</details>

<details><summary><b>antGLasso: An Efficient Tensor Graphical Lasso Algorithm</b>
<a href="https://arxiv.org/abs/2211.02920">arxiv:2211.02920</a>
&#x1F4C8; 4 <br>
<p>Bailey Andrew, David Westhead, Luisa Cutillo</p></summary>
<p>

**Abstract:** The class of bigraphical lasso algorithms (and, more broadly, 'tensor'-graphical lasso algorithms) has been used to estimate dependency structures within matrix and tensor data. However, all current methods to do so take prohibitively long on modestly sized datasets. We present a novel tensor-graphical lasso algorithm that analytically estimates the dependency structure, unlike its iterative predecessors. This provides a speedup of multiple orders of magnitude, allowing this class of algorithms to be used on large, real-world datasets.

</p>
</details>

<details><summary><b>A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs</b>
<a href="https://arxiv.org/abs/2211.02918">arxiv:2211.02918</a>
&#x1F4C8; 4 <br>
<p>Xiao Chi</p></summary>
<p>

**Abstract:** Epistemic graphs generalize the epistemic approach to probabilistic argumentation and tackle the uncertainties in and between arguments. A framework was proposed to generate epistemic constraints from data using a two-way generalization method in the perspective of only considering the beliefs of participants without considering the nature of relations represented in an epistemic graph. The deficiency of original framework is that it is unable to learn rules using tighter constraints, and the learnt rules might be counterintuitive. Meanwhile, when dealing with more restricted values, the filtering computational complexity will increase sharply, and the time performance would become unreasonable. This paper introduces a filtering-based approach using a multiple-way generalization step to generate a set of rational rules based on both the beliefs of each agent on different arguments and the epistemic graph corresponding to the epistemic constraints. This approach is able to generated rational rules with multiple restricted values in higher efficiency. Meanwhile, we have proposed a standard to analyze the rationality of a dataset based on the postulates of deciding rational rules. We evaluate the filtering-based approach on two suitable data bases. The empirical results show that the filtering-based approach performs well with a better efficiency comparing to the original framework, and rules generated from the improved approach are ensured to be rational.

</p>
</details>

<details><summary><b>New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound</b>
<a href="https://arxiv.org/abs/2211.02912">arxiv:2211.02912</a>
&#x1F4C8; 4 <br>
<p>Arushi Gupta, Nikunj Saunshi, Dingli Yu, Kaifeng Lyu, Sanjeev Arora</p></summary>
<p>

**Abstract:** Saliency methods compute heat maps that highlight portions of an input that were most {\em important} for the label assigned to it by a deep net. Evaluations of saliency methods convert this heat map into a new {\em masked input} by retaining the $k$ highest-ranked pixels of the original input and replacing the rest with \textquotedblleft uninformative\textquotedblright\ pixels, and checking if the net's output is mostly unchanged. This is usually seen as an {\em explanation} of the output, but the current paper highlights reasons why this inference of causality may be suspect. Inspired by logic concepts of {\em completeness \& soundness}, it observes that the above type of evaluation focuses on completeness of the explanation, but ignores soundness. New evaluation metrics are introduced to capture both notions, while staying in an {\em intrinsic} framework -- i.e., using the dataset and the net, but no separately trained nets, human evaluations, etc. A simple saliency method is described that matches or outperforms prior methods in the evaluations. Experiments also suggest new intrinsic justifications, based on soundness, for popular heuristic tricks such as TV regularization and upsampling.

</p>
</details>

<details><summary><b>ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme</b>
<a href="https://arxiv.org/abs/2211.02906">arxiv:2211.02906</a>
&#x1F4C8; 4 <br>
<p>Mario Chahoud, Hani Sami, Azzam Mourad, Safa Otoum, Hadi Otrok, Jamal Bentahar, Mohsen Guizani</p></summary>
<p>

**Abstract:** In this paper, we increase the availability and integration of devices in the learning process to enhance the convergence of federated learning (FL) models. To address the issue of having all the data in one location, federated learning, which maintains the ability to learn over decentralized data sets, combines privacy and technology. Until the model converges, the server combines the updated weights obtained from each dataset over a number of rounds. The majority of the literature suggested client selection techniques to accelerate convergence and boost accuracy. However, none of the existing proposals have focused on the flexibility to deploy and select clients as needed, wherever and whenever that may be. Due to the extremely dynamic surroundings, some devices are actually not available to serve as clients in FL, which affects the availability of data for learning and the applicability of the existing solution for client selection. In this paper, we address the aforementioned limitations by introducing an On-Demand-FL, a client deployment approach for FL, offering more volume and heterogeneity of data in the learning process. We make use of the containerization technology such as Docker to build efficient environments using IoT and mobile devices serving as volunteers. Furthermore, Kubernetes is used for orchestration. The Genetic algorithm (GA) is used to solve the multi-objective optimization problem due to its evolutionary strategy. The performed experiments using the Mobile Data Challenge (MDC) dataset and the Localfed framework illustrate the relevance of the proposed approach and the efficiency of the on-the-fly deployment of clients whenever and wherever needed with less discarded rounds and more available data.

</p>
</details>

<details><summary><b>Textual Manifold-based Defense Against Natural Language Adversarial Examples</b>
<a href="https://arxiv.org/abs/2211.02878">arxiv:2211.02878</a>
&#x1F4C8; 4 <br>
<p>Dang Minh Nguyen, Luu Anh Tuan</p></summary>
<p>

**Abstract:** Recent studies on adversarial images have shown that they tend to leave the underlying low-dimensional data manifold, making them significantly more challenging for current models to make correct predictions. This so-called off-manifold conjecture has inspired a novel line of defenses against adversarial attacks on images. In this study, we find a similar phenomenon occurs in the contextualized embedding space induced by pretrained language models, in which adversarial texts tend to have their embeddings diverge from the manifold of natural ones. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that projects text embeddings onto an approximated embedding manifold before classification. It reduces the complexity of potential adversarial examples, which ultimately enhances the robustness of the protected model. Through extensive experiments, our method consistently and significantly outperforms previous defenses under various attack settings without trading off clean accuracy. To the best of our knowledge, this is the first NLP defense that leverages the manifold structure against adversarial attacks. Our code is available at \url{https://github.com/dangne/tmd}.

</p>
</details>

<details><summary><b>A review of TinyML</b>
<a href="https://arxiv.org/abs/2211.04448">arxiv:2211.04448</a>
&#x1F4C8; 3 <br>
<p>Harsha Yelchuri, Rashmi R</p></summary>
<p>

**Abstract:** In this current technological world, the application of machine learning is becoming ubiquitous. Incorporating machine learning algorithms on extremely low-power and inexpensive embedded devices at the edge level is now possible due to the combination of the Internet of Things (IoT) and edge computing. To estimate an outcome, traditional machine learning demands vast amounts of resources. The TinyML concept for embedded machine learning attempts to push such diversity from usual high-end approaches to low-end applications. TinyML is a rapidly expanding interdisciplinary topic at the convergence of machine learning, software, and hardware centered on deploying deep neural network models on embedded (micro-controller-driven) systems. TinyML will pave the way for novel edge-level services and applications that survive on distributed edge inferring and independent decision-making rather than server computation. In this paper, we explore TinyML's methodology, how TinyML can benefit a few specific industrial fields, its obstacles, and its future scope.

</p>
</details>

<details><summary><b>Inside Out: Transforming Images of Lab-Grown Plants for Machine Learning Applications in Agriculture</b>
<a href="https://arxiv.org/abs/2211.02972">arxiv:2211.02972</a>
&#x1F4C8; 3 <br>
<p>A. E. Krosney, P. Sotoodeh, C. J. Henry, M. A. Beck, C. P. Bidinosti</p></summary>
<p>

**Abstract:** Machine learning tasks often require a significant amount of training data for the resultant network to perform suitably for a given problem in any domain. In agriculture, dataset sizes are further limited by phenotypical differences between two plants of the same genotype, often as a result of differing growing conditions. Synthetically-augmented datasets have shown promise in improving existing models when real data is not available. In this paper, we employ a contrastive unpaired translation (CUT) generative adversarial network (GAN) and simple image processing techniques to translate indoor plant images to appear as field images. While we train our network to translate an image containing only a single plant, we show that our method is easily extendable to produce multiple-plant field images. Furthermore, we use our synthetic multi-plant images to train several YoloV5 nano object detection models to perform the task of plant detection and measure the accuracy of the model on real field data images. Including training data generated by the CUT-GAN leads to better plant detection performance compared to a network trained solely on real data.

</p>
</details>

<details><summary><b>Can Ensemble of Classifiers Provide Better Recognition Results in Packaging Activity?</b>
<a href="https://arxiv.org/abs/2211.02965">arxiv:2211.02965</a>
&#x1F4C8; 3 <br>
<p>A. H. M. Nazmus Sakib, Promit Basak, Syed Doha Uddin, Shahamat Mustavi Tasin, Md Atiqur Rahman Ahad</p></summary>
<p>

**Abstract:** Skeleton-based Motion Capture (MoCap) systems have been widely used in the game and film industry for mimicking complex human actions for a long time. MoCap data has also proved its effectiveness in human activity recognition tasks. However, it is a quite challenging task for smaller datasets. The lack of such data for industrial activities further adds to the difficulties. In this work, we have proposed an ensemble-based machine learning methodology that is targeted to work better on MoCap datasets. The experiments have been performed on the MoCap data given in the Bento Packaging Activity Recognition Challenge 2021. Bento is a Japanese word that resembles lunch-box. Upon processing the raw MoCap data at first, we have achieved an astonishing accuracy of 98% on 10-fold Cross-Validation and 82% on Leave-One-Out-Cross-Validation by using the proposed ensemble model.

</p>
</details>

<details><summary><b>Predicting Treatment Adherence of Tuberculosis Patients at Scale</b>
<a href="https://arxiv.org/abs/2211.02943">arxiv:2211.02943</a>
&#x1F4C8; 3 <br>
<p>Mihir Kulkarni, Satvik Golechha, Rishi Raj, Jithin Sreedharan, Ankit Bhardwaj, Santanu Rathod, Bhavin Vadera, Jayakrishna Kurada, Sanjay Mattoo, Rajendra Joshi, Kirankumar Rade, Alpan Raval</p></summary>
<p>

**Abstract:** Tuberculosis (TB), an infectious bacterial disease, is a significant cause of death, especially in low-income countries, with an estimated ten million new cases reported globally in $2020$. While TB is treatable, non-adherence to the medication regimen is a significant cause of morbidity and mortality. Thus, proactively identifying patients at risk of dropping off their medication regimen enables corrective measures to mitigate adverse outcomes. Using a proxy measure of extreme non-adherence and a dataset of nearly $700,000$ patients from four states in India, we formulate and solve the machine learning (ML) problem of early prediction of non-adherence based on a custom rank-based metric. We train ML models and evaluate against baselines, achieving a $\sim 100\%$ lift over rule-based baselines and $\sim 214\%$ over a random classifier, taking into account country-wide large-scale future deployment. We deal with various issues in the process, including data quality, high-cardinality categorical data, low target prevalence, distribution shift, variation across cohorts, algorithmic fairness, and the need for robustness and explainability. Our findings indicate that risk stratification of non-adherent patients is a viable, deployable-at-scale ML solution.

</p>
</details>

<details><summary><b>Quantization Adaptor for Bit-Level Deep Learning-Based Massive MIMO CSI Feedback</b>
<a href="https://arxiv.org/abs/2211.02937">arxiv:2211.02937</a>
&#x1F4C8; 3 <br>
<p>Xudong Zhang, Zhilin Lu, Rui Zeng, Jintao Wang</p></summary>
<p>

**Abstract:** In massive multiple-input multiple-output (MIMO) systems, the user equipment (UE) needs to feed the channel state information (CSI) back to the base station (BS) for the following beamforming. But the large scale of antennas in massive MIMO systems causes huge feedback overhead. Deep learning (DL) based methods can compress the CSI at the UE and recover it at the BS, which reduces the feedback cost significantly. But the compressed CSI must be quantized into bit streams for transmission. In this paper, we propose an adaptor-assisted quantization strategy for bit-level DL-based CSI feedback. First, we design a network-aided adaptor and an advanced training scheme to adaptively improve the quantization and reconstruction accuracy. Moreover, for easy practical employment, we introduce the expert knowledge of data distribution and propose a pluggable and cost-free adaptor scheme. Experiments show that compared with the state-of-the-art feedback quantization method, this adaptor-aided quantization strategy can achieve better quantization accuracy and reconstruction performance with less or no additional cost. The open-source codes are available at https://github.com/zhang-xd18/QCRNet.

</p>
</details>

<details><summary><b>Lightweight 3D Convolutional Neural Network for Schizophrenia diagnosis using MRI Images and Ensemble Bagging Classifier</b>
<a href="https://arxiv.org/abs/2211.02868">arxiv:2211.02868</a>
&#x1F4C8; 3 <br>
<p>P Supriya Patro, Tripti Goel, S A VaraPrasad, M Tanveer, R Murugan</p></summary>
<p>

**Abstract:** Structural alterations have been thoroughly investigated in the brain during the early onset of schizophrenia (SCZ) with the development of neuroimaging methods. The objective of the paper is an efficient classification of SCZ in 2 different classes: Cognitive Normal (CN), and SCZ using magnetic resonance imaging (MRI) images. This paper proposed a lightweight 3D convolutional neural network (CNN) based framework for SCZ diagnosis using MRI images. In the proposed model, lightweight 3D CNN is used to extract both spatial and spectral features simultaneously from 3D volume MRI scans, and classification is done using an ensemble bagging classifier. Ensemble bagging classifier contributes to preventing overfitting, reduces variance, and improves the model's accuracy. The proposed algorithm is tested on datasets taken from three benchmark databases available as open-source: MCICShare, COBRE, and fBRINPhase-II. These datasets have undergone preprocessing steps to register all the MRI images to the standard template and reduce the artifacts. The model achieves the highest accuracy 92.22%, sensitivity 94.44%, specificity 90%, precision 90.43%, recall 94.44%, F1-score 92.39% and G-mean 92.19% as compared to the current state-of-the-art techniques. The performance metrics evidenced the use of this model to assist the clinicians for automatic accurate diagnosis of SCZ.

</p>
</details>

<details><summary><b>EventEA: Benchmarking Entity Alignment for Event-centric Knowledge Graphs</b>
<a href="https://arxiv.org/abs/2211.02817">arxiv:2211.02817</a>
&#x1F4C8; 3 <br>
<p>Xiaobin Tian, Zequn Sun, Guangyao Li, Wei Hu</p></summary>
<p>

**Abstract:** Entity alignment is to find identical entities in different knowledge graphs (KGs) that refer to the same real-world object. Embedding-based entity alignment techniques have been drawing a lot of attention recently because they can help solve the issue of symbolic heterogeneity in different KGs. However, in this paper, we show that the progress made in the past was due to biased and unchallenging evaluation. We highlight two major flaws in existing datasets that favor embedding-based entity alignment techniques, i.e., the isomorphic graph structures in relation triples and the weak heterogeneity in attribute triples. Towards a critical evaluation of embedding-based entity alignment methods, we construct a new dataset with heterogeneous relations and attributes based on event-centric KGs. We conduct extensive experiments to evaluate existing popular methods, and find that they fail to achieve promising performance. As a new approach to this difficult problem, we propose a time-aware literal encoder for entity alignment. The dataset and source code are publicly available to foster future research. Our work calls for more effective and practical embedding-based solutions to entity alignment.

</p>
</details>

<details><summary><b>A Comprehensive Survey of Regression Based Loss Functions for Time Series Forecasting</b>
<a href="https://arxiv.org/abs/2211.02989">arxiv:2211.02989</a>
&#x1F4C8; 2 <br>
<p>Aryan Jadon, Avinash Patil, Shruti Jadon</p></summary>
<p>

**Abstract:** Time Series Forecasting has been an active area of research due to its many applications ranging from network usage prediction, resource allocation, anomaly detection, and predictive maintenance. Numerous publications published in the last five years have proposed diverse sets of objective loss functions to address cases such as biased data, long-term forecasting, multicollinear features, etc. In this paper, we have summarized 14 well-known regression loss functions commonly used for time series forecasting and listed out the circumstances where their application can aid in faster and better model convergence. We have also demonstrated how certain categories of loss functions perform well across all data sets and can be considered as a baseline objective function in circumstances where the distribution of the data is unknown. Our code is available at GitHub: https://github.com/aryan-jadon/Regression-Loss-Functions-in-Time-Series-Forecasting-Tensorflow.

</p>
</details>

<details><summary><b>Unsupervised Machine Learning for Explainable Medicare Fraud Detection</b>
<a href="https://arxiv.org/abs/2211.02927">arxiv:2211.02927</a>
&#x1F4C8; 2 <br>
<p>Shubhranshu Shekhar, Jetson Leder-Luis, Leman Akoglu</p></summary>
<p>

**Abstract:** The US federal government spends more than a trillion dollars per year on health care, largely provided by private third parties and reimbursed by the government. A major concern in this system is overbilling, waste and fraud by providers, who face incentives to misreport on their claims in order to receive higher payments. In this paper, we develop novel machine learning tools to identify providers that overbill Medicare, the US federal health insurance program for elderly adults and the disabled. Using large-scale Medicare claims data, we identify patterns consistent with fraud or overbilling among inpatient hospitalizations. Our proposed approach for Medicare fraud detection is fully unsupervised, not relying on any labeled training data, and is explainable to end users, providing reasoning and interpretable insights into the potentially suspicious behavior of the flagged providers. Data from the Department of Justice on providers facing anti-fraud lawsuits and several case studies validate our approach and findings both quantitatively and qualitatively.

</p>
</details>

<details><summary><b>Neural multi-event forecasting on spatio-temporal point processes using probabilistically enriched transformers</b>
<a href="https://arxiv.org/abs/2211.02922">arxiv:2211.02922</a>
&#x1F4C8; 2 <br>
<p>Negar Erfanian, Santiago Segarra, Maarten de Hoop</p></summary>
<p>

**Abstract:** Predicting discrete events in time and space has many scientific applications, such as predicting hazardous earthquakes and outbreaks of infectious diseases. History-dependent spatio-temporal Hawkes processes are often used to mathematically model these point events. However, previous approaches have faced numerous challenges, particularly when attempting to forecast one or multiple future events. In this work, we propose a new neural architecture for multi-event forecasting of spatio-temporal point processes, utilizing transformers, augmented with normalizing flows and probabilistic layers. Our network makes batched predictions of complex history-dependent spatio-temporal distributions of future discrete events, achieving state-of-the-art performance on a variety of benchmark datasets including the South California Earthquakes, Citibike, Covid-19, and Hawkes synthetic pinwheel datasets. More generally, we illustrate how our network can be applied to any dataset of discrete events with associated markers, even when no underlying physics is known.

</p>
</details>

<details><summary><b>ESKNet-An enhanced adaptive selection kernel convolution for breast tumors segmentation</b>
<a href="https://arxiv.org/abs/2211.02915">arxiv:2211.02915</a>
&#x1F4C8; 2 <br>
<p>Gongping Chen, Jianxun Zhang, Yuming Liu, Jingjing Yin, Xiaotao Yin, Liang Cui, Yu Dai</p></summary>
<p>

**Abstract:** Breast cancer is one of the common cancers that endanger the health of women globally. Accurate target lesion segmentation is essential for early clinical intervention and postoperative follow-up. Recently, many convolutional neural networks (CNNs) have been proposed to segment breast tumors from ultrasound images. However, the complex ultrasound pattern and the variable tumor shape and size bring challenges to the accurate segmentation of the breast lesion. Motivated by the selective kernel convolution, we introduce an enhanced selective kernel convolution for breast tumor segmentation, which integrates multiple feature map region representations and adaptively recalibrates the weights of these feature map regions from the channel and spatial dimensions. This region recalibration strategy enables the network to focus more on high-contributing region features and mitigate the perturbation of less useful regions. Finally, the enhanced selective kernel convolution is integrated into U-net with deep supervision constraints to adaptively capture the robust representation of breast tumors. Extensive experiments with twelve state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance in breast ultrasound images.

</p>
</details>

<details><summary><b>HAQJSK: Hierarchical-Aligned Quantum Jensen-Shannon Kernels for Graph Classification</b>
<a href="https://arxiv.org/abs/2211.02904">arxiv:2211.02904</a>
&#x1F4C8; 2 <br>
<p>Lu Bai, Lixin Cui, Yue Wang, Ming Li, Edwin R. Hancock</p></summary>
<p>

**Abstract:** In this work, we propose a family of novel quantum kernels, namely the Hierarchical Aligned Quantum Jensen-Shannon Kernels (HAQJSK), for un-attributed graphs. Different from most existing classical graph kernels, the proposed HAQJSK kernels can incorporate hierarchical aligned structure information between graphs and transform graphs of random sizes into fixed-sized aligned graph structures, i.e., the Hierarchical Transitive Aligned Adjacency Matrix of vertices and the Hierarchical Transitive Aligned Density Matrix of the Continuous-Time Quantum Walk (CTQW). For a pair of graphs to hand, the resulting HAQJSK kernels are defined by measuring the Quantum Jensen-Shannon Divergence (QJSD) between their transitive aligned graph structures. We show that the proposed HAQJSK kernels not only reflect richer intrinsic global graph characteristics in terms of the CTQW, but also address the drawback of neglecting structural correspondence information arising in most existing R-convolution kernels. Furthermore, unlike the previous Quantum Jensen-Shannon Kernels associated with the QJSD and the CTQW, the proposed HAQJSK kernels can simultaneously guarantee the properties of permutation invariant and positive definiteness, explaining the theoretical advantages of the HAQJSK kernels. Experiments indicate the effectiveness of the proposed kernels.

</p>
</details>

<details><summary><b>Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing</b>
<a href="https://arxiv.org/abs/2211.02899">arxiv:2211.02899</a>
&#x1F4C8; 2 <br>
<p>Rui Yu, Yifeng Li, Wenpeng Lu, Longbing Cao</p></summary>
<p>

**Abstract:** In natural language processing (NLP), the context of a word or sentence plays an essential role. Contextual information such as the semantic representation of a passage or historical dialogue forms an essential part of a conversation and a precise understanding of the present phrase or sentence. However, the standard attention mechanisms typically generate weights using query and key but ignore context, forming a Bi-Attention framework, despite their great success in modeling sequence alignment. This Bi-Attention mechanism does not explicitly model the interactions between the contexts, queries and keys of target sequences, missing important contextual information and resulting in poor attention performance. Accordingly, a novel and general triple-attention (Tri-Attention) framework expands the standard Bi-Attention mechanism and explicitly interacts query, key, and context by incorporating context as the third dimension in calculating relevance scores. Four variants of Tri-Attention are generated by expanding the two-dimensional vector-based additive, dot-product, scaled dot-product, and bilinear operations in Bi-Attention to the tensor operations for Tri-Attention. Extensive experiments on three NLP tasks demonstrate that Tri-Attention outperforms about 30 state-of-the-art non-attention, standard Bi-Attention, contextual Bi-Attention approaches and pretrained neural language models1.

</p>
</details>

<details><summary><b>Learning Product Graphs from Spectral Templates</b>
<a href="https://arxiv.org/abs/2211.02893">arxiv:2211.02893</a>
&#x1F4C8; 2 <br>
<p>Aref Einizade, Sepideh Hajipour Sardouie</p></summary>
<p>

**Abstract:** Graph Learning (GL) is at the core of inference and analysis of connections in data mining and machine learning (ML). By observing a dataset of graph signals, and considering specific assumptions, Graph Signal Processing (GSP) tools can provide practical constraints in the GL approach. One applicable constraint can infer a graph with desired frequency signatures, i.e., spectral templates. However, a severe computational burden is a challenging barrier, especially for inference from high-dimensional graph signals. To address this issue and in the case of the underlying graph having graph product structure, we propose learning product (high dimensional) graphs from product spectral templates with significantly reduced complexity rather than learning them directly from high-dimensional graph signals, which, to the best of our knowledge, has not been addressed in the related areas. In contrast to the rare current approaches, our approach can learn all types of product graphs (with more than two graphs) without knowing the type of graph products and has fewer parameters. Experimental results on both the synthetic and real-world data, i.e., brain signal analysis and multi-view object images, illustrate explainable and meaningful factor graphs supported by expert-related research, as well as outperforming the rare current restricted approaches.

</p>
</details>

<details><summary><b>Pitfalls of Climate Network Construction: A Statistical Perspective</b>
<a href="https://arxiv.org/abs/2211.02888">arxiv:2211.02888</a>
&#x1F4C8; 2 <br>
<p>Moritz Haas, Bedartha Goswami, Ulrike von Luxburg</p></summary>
<p>

**Abstract:** Network-based analyses of dynamical systems have become increasingly popular in climate science. Here we address network construction from a statistical perspective and highlight the often ignored fact that the calculated correlation values are only empirical estimates. To measure spurious behaviour as deviation from a ground truth network, we simulate time-dependent isotropic random fields on the sphere and apply common network construction techniques. We find several ways in which the uncertainty stemming from the estimation procedure has major impact on network characteristics. When the data has locally coherent correlation structure, spurious link bundle teleconnections and spurious high-degree clusters have to be expected. Anisotropic estimation variance can also induce severe biases into empirical networks. We validate our findings with ERA5 reanalysis data. Moreover we explain why commonly applied resampling procedures are inappropriate for significance evaluation and propose a statistically more meaningful ensemble construction framework. By communicating which difficulties arise in estimation from scarce data and by presenting which design decisions increase robustness, we hope to contribute to more reliable climate network construction in the future.

</p>
</details>

<details><summary><b>Deep Learning for Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes</b>
<a href="https://arxiv.org/abs/2211.02869">arxiv:2211.02869</a>
&#x1F4C8; 2 <br>
<p>Vanessa Boehm, Wei Ji Leong, Ragini Bal Mahesh, Ioannis Prapas, Edoardo Nemni, Freddie Kalaitzis, Siddha Ganju, Raul Ramos-Pollan</p></summary>
<p>

**Abstract:** With climate change predicted to increase the likelihood of landslide events, there is a growing need for rapid landslide detection technologies that help inform emergency responses. Synthetic Aperture Radar (SAR) is a remote sensing technique that can provide measurements of affected areas independent of weather or lighting conditions. Usage of SAR, however, is hindered by domain knowledge that is necessary for the pre-processing steps and its interpretation requires expert knowledge. We provide simplified, pre-processed, machine-learning ready SAR datacubes for four globally located landslide events obtained from several Sentinel-1 satellite passes before and after a landslide triggering event together with segmentation maps of the landslides. From this dataset, using the Hokkaido, Japan datacube, we study the feasibility of SAR-based landslide detection with supervised deep learning (DL). Our results demonstrate that DL models can be used to detect landslides from SAR data, achieving an Area under the Precision-Recall curve exceeding 0.7. We find that additional satellite visits enhance detection performance, but that early detection is possible when SAR data is combined with terrain information from a digital elevation model. This can be especially useful for time-critical emergency interventions. Code is made publicly available at https://github.com/iprapas/landslide-sar-unet.

</p>
</details>

<details><summary><b>Feature Selection for Classification with QAOA</b>
<a href="https://arxiv.org/abs/2211.02861">arxiv:2211.02861</a>
&#x1F4C8; 2 <br>
<p>Gloria Turati, Maurizio Ferrari Dacrema, Paolo Cremonesi</p></summary>
<p>

**Abstract:** Feature selection is of great importance in Machine Learning, where it can be used to reduce the dimensionality of classification, ranking and prediction problems. The removal of redundant and noisy features can improve both the accuracy and scalability of the trained models. However, feature selection is a computationally expensive task with a solution space that grows combinatorically. In this work, we consider in particular a quadratic feature selection problem that can be tackled with the Quantum Approximate Optimization Algorithm (QAOA), already employed in combinatorial optimization. First we represent the feature selection problem with the QUBO formulation, which is then mapped to an Ising spin Hamiltonian. Then we apply QAOA with the goal of finding the ground state of this Hamiltonian, which corresponds to the optimal selection of features. In our experiments, we consider seven different real-world datasets with dimensionality up to 21 and run QAOA on both a quantum simulator and, for small datasets, the 7-qubit IBM (ibm-perth) quantum computer. We use the set of selected features to train a classification model and evaluate its accuracy. Our analysis shows that it is possible to tackle the feature selection problem with QAOA and that currently available quantum devices can be used effectively. Future studies could test a wider range of classification models as well as improve the effectiveness of QAOA by exploring better performing optimizers for its classical step.

</p>
</details>

<details><summary><b>Towards a methodology for addressing missingness in datasets, with an application to demographic health datasets</b>
<a href="https://arxiv.org/abs/2211.02856">arxiv:2211.02856</a>
&#x1F4C8; 2 <br>
<p>Gift Khangamwa, Terence L. van Zyl, Clint J. van Alten</p></summary>
<p>

**Abstract:** Missing data is a common concern in health datasets, and its impact on good decision-making processes is well documented. Our study's contribution is a methodology for tackling missing data problems using a combination of synthetic dataset generation, missing data imputation and deep learning methods to resolve missing data challenges. Specifically, we conducted a series of experiments with these objectives; $a)$ generating a realistic synthetic dataset, $b)$ simulating data missingness, $c)$ recovering the missing data, and $d)$ analyzing imputation performance. Our methodology used a gaussian mixture model whose parameters were learned from a cleaned subset of a real demographic and health dataset to generate the synthetic data. We simulated various missingness degrees ranging from $10 \%$, $20 \%$, $30 \%$, and $40\%$ under the missing completely at random scheme MCAR. We used an integrated performance analysis framework involving clustering, classification and direct imputation analysis. Our results show that models trained on synthetic and imputed datasets could make predictions with an accuracy of $83 \%$ and $80 \%$ on $a) $ an unseen real dataset and $b)$ an unseen reserved synthetic test dataset, respectively. Moreover, the models that used the DAE method for imputed yielded the lowest log loss an indication of good performance, even though the accuracy measures were slightly lower. In conclusion, our work demonstrates that using our methodology, one can reverse engineer a solution to resolve missingness on an unseen dataset with missingness. Moreover, though we used a health dataset, our methodology can be utilized in other contexts.

</p>
</details>

<details><summary><b>Learning Fabric Manipulation in the Real World with Human Videos</b>
<a href="https://arxiv.org/abs/2211.02832">arxiv:2211.02832</a>
&#x1F4C8; 2 <br>
<p>Robert Lee, Jad Abou-Chakra, Fangyi Zhang, Peter Corke</p></summary>
<p>

**Abstract:** Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by human hands, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a sample-efficient pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric folding task, showing that our policy can reliably reach folded states from crumpled initial configurations.

</p>
</details>

<details><summary><b>A Robust and Low Complexity Deep Learning Model for Remote Sensing Image Classification</b>
<a href="https://arxiv.org/abs/2211.02820">arxiv:2211.02820</a>
&#x1F4C8; 2 <br>
<p>Cam Le, Lam Pham, Nghia NVN, Truong Nguyen, Le Hong Trang</p></summary>
<p>

**Abstract:** In this paper, we present a robust and low complexity deep learning model for Remote Sensing Image Classification (RSIC), the task of identifying the scene of a remote sensing image. In particular, we firstly evaluate different low complexity and benchmark deep neural networks: MobileNetV1, MobileNetV2, NASNetMobile, and EfficientNetB0, which present the number of trainable parameters lower than 5 Million (M). After indicating best network architecture, we further improve the network performance by applying attention schemes to multiple feature maps extracted from middle layers of the network. To deal with the issue of increasing the model footprint as using attention schemes, we apply the quantization technique to satisfies the number trainable parameter of the model lower than 5 M. By conducting extensive experiments on the benchmark datasets NWPU-RESISC45, we achieve a robust and low-complexity model, which is very competitive to the state-of-the-art systems and potential for real-life applications on edge devices.

</p>
</details>

<details><summary><b>Knowledge Retrieval using Foon</b>
<a href="https://arxiv.org/abs/2211.03790">arxiv:2211.03790</a>
&#x1F4C8; 1 <br>
<p>Vara Bhavya Sri Malli</p></summary>
<p>

**Abstract:** Flexible task planning is still a significant challenge for robots. The inability of robots to creatively adapt their task plans to new or unforeseen challenges is largely attributable to their limited understanding of their activities and the environment. Cooking, for example, requires a person to occasionally take risks that a robot would find extremely dangerous. We may obtain manipulation sequences by employing knowledge that is drawn from numerous video sources thanks to knowledge retrieval through graph search.

</p>
</details>

<details><summary><b>Differentiable Neural Computers with Memory Demon</b>
<a href="https://arxiv.org/abs/2211.02987">arxiv:2211.02987</a>
&#x1F4C8; 1 <br>
<p>Ari Azarafrooz</p></summary>
<p>

**Abstract:** A Differentiable Neural Computer (DNC) is a neural network with an external memory which allows for iterative content modification via read, write and delete operations.
  We show that information theoretic properties of the memory contents play an important role in the performance of such architectures. We introduce a novel concept of memory demon to DNC architectures which modifies the memory contents implicitly via additive input encoding. The goal of the memory demon is to maximize the expected sum of mutual information of the consecutive external memory contents.

</p>
</details>

<details><summary><b>Efficient Cavity Searching for Gene Network of Influenza A Virus</b>
<a href="https://arxiv.org/abs/2211.02935">arxiv:2211.02935</a>
&#x1F4C8; 1 <br>
<p>Junjie Li, Jietong Zhao, Yanqing Su, Jiahao Shen, Yaohua Liu, Xinyue Fan, Zheng Kou</p></summary>
<p>

**Abstract:** High order structures (cavities and cliques) of the gene network of influenza A virus reveal tight associations among viruses during evolution and are key signals that indicate viral cross-species infection and cause pandemics. As indicators for sensing the dynamic changes of viral genes, these higher order structures have been the focus of attention in the field of virology. However, the size of the viral gene network is usually huge, and searching these structures in the networks introduces unacceptable delay. To mitigate this issue, in this paper, we propose a simple-yet-effective model named HyperSearch based on deep learning to search cavities in a computable complex network for influenza virus genetics. Extensive experiments conducted on a public influenza virus dataset demonstrate the effectiveness of HyperSearch over other advanced deep-learning methods without any elaborated model crafting. Moreover, HyperSearch can finish the search works in minutes while 0-1 programming takes days. Since the proposed method is simple and easy to be transferred to other complex networks, HyperSearch has the potential to facilitate the monitoring of dynamic changes in viral genes and help humans keep up with the pace of virus mutations.

</p>
</details>

<details><summary><b>1-D Convolutional Graph Convolutional Networks for Fault Detection in Distributed Energy Systems</b>
<a href="https://arxiv.org/abs/2211.02930">arxiv:2211.02930</a>
&#x1F4C8; 1 <br>
<p>Bang L. H. Nguyen, Tuyen Vu, Thai-Thanh Nguyen, Mayank Panwar, Rob Hovsapian</p></summary>
<p>

**Abstract:** This paper presents a 1-D convolutional graph neural network for fault detection in microgrids. The combination of 1-D convolutional neural networks (1D-CNN) and graph convolutional networks (GCN) helps extract both spatial-temporal correlations from the voltage measurements in microgrids. The fault detection scheme includes fault event detection, fault type and phase classification, and fault location. There are five neural network model training to handle these tasks. Transfer learning and fine-tuning are applied to reduce training efforts. The combined recurrent graph convolutional neural networks (1D-CGCN) is compared with the traditional ANN structure on the Potsdam 13-bus microgrid dataset. The achievable accuracy of 99.27%, 98.1%, 98.75%, and 95.6% for fault detection, fault type classification, fault phase identification, and fault location respectively.

</p>
</details>


{% endraw %}
Prev: [2022.11.04]({{ '/2022/11/04/2022.11.04.html' | relative_url }})  Next: [2022.11.06]({{ '/2022/11/06/2022.11.06.html' | relative_url }})