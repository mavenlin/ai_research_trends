Prev: [2021.01.10]({{ '/2021/01/10/2021.01.10.html' | relative_url }})  Next: [2021.01.12]({{ '/2021/01/12/2021.01.12.html' | relative_url }})
{% raw %}
## Summary for 2021-01-11, created on 2021-12-24


<details><summary><b>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</b>
<a href="https://arxiv.org/abs/2101.03961">arxiv:2101.03961</a>
&#x1F4C8; 237 <br>
<p>William Fedus, Barret Zoph, Noam Shazeer</p></summary>
<p>

**Abstract:** In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.

</p>
</details>

<details><summary><b>Technology Readiness Levels for Machine Learning Systems</b>
<a href="https://arxiv.org/abs/2101.03989">arxiv:2101.03989</a>
&#x1F4C8; 59 <br>
<p>Alexander Lavin, Ciarán M. Gilligan-Lee, Alessya Visnjic, Siddha Ganju, Dava Newman, Atılım Güneş Baydin, Sujoy Ganguly, Danny Lange, Amit Sharma, Stephan Zheng, Eric P. Xing, Adam Gibson, James Parr, Chris Mattmann, Yarin Gal</p></summary>
<p>

**Abstract:** The development and deployment of machine learning (ML) systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, where mission critical measures and robustness are ingrained in the development process. Drawing on experience in both spacecraft engineering and ML (from research through product across domain areas), we have developed a proven systems engineering approach for machine learning development and deployment. Our "Machine Learning Technology Readiness Levels" (MLTRL) framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for ML workflows, including key distinctions from traditional software engineering. Even more, MLTRL defines a lingua franca for people across teams and organizations to work collaboratively on artificial intelligence and machine learning technologies. Here we describe the framework and elucidate it with several real world use-cases of developing ML methods from basic research through productization and deployment, in areas such as medical diagnostics, consumer computer vision, satellite imagery, and particle physics.

</p>
</details>

<details><summary><b>A Bayesian neural network predicts the dissolution of compact planetary systems</b>
<a href="https://arxiv.org/abs/2101.04117">arxiv:2101.04117</a>
&#x1F4C8; 42 <br>
<p>Miles Cranmer, Daniel Tamayo, Hanno Rein, Peter Battaglia, Samuel Hadden, Philip J. Armitage, Shirley Ho, David N. Spergel</p></summary>
<p>

**Abstract:** Despite over three hundred years of effort, no solutions exist for predicting when a general planetary configuration will become unstable. We introduce a deep learning architecture to push forward this problem for compact systems. While current machine learning algorithms in this area rely on scientist-derived instability metrics, our new technique learns its own metrics from scratch, enabled by a novel internal structure inspired from dynamics theory. Our Bayesian neural network model can accurately predict not only if, but also when a compact planetary system with three or more planets will go unstable. Our model, trained directly from short N-body time series of raw orbital elements, is more than two orders of magnitude more accurate at predicting instability times than analytical estimators, while also reducing the bias of existing machine learning algorithms by nearly a factor of three. Despite being trained on compact resonant and near-resonant three-planet configurations, the model demonstrates robust generalization to both non-resonant and higher multiplicity configurations, in the latter case outperforming models fit to that specific set of integrations. The model computes instability estimates up to five orders of magnitude faster than a numerical integrator, and unlike previous efforts provides confidence intervals on its predictions. Our inference model is publicly available in the SPOCK package, with training code open-sourced.

</p>
</details>

<details><summary><b>Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals</b>
<a href="https://arxiv.org/abs/2101.03737">arxiv:2101.03737</a>
&#x1F4C8; 21 <br>
<p>Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, Ji-Rong Wen</p></summary>
<p>

**Abstract:** Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities that are multiple hops away in the Knowledge Base (KB) from the entities in the question. A major challenge is the lack of supervision signals at intermediate steps. Therefore, multi-hop KBQA algorithms can only receive the feedback from the final answer, which makes the learning unstable or ineffective.
  To address this challenge, we propose a novel teacher-student approach for the multi-hop KBQA task. In our approach, the student network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network. The major novelty lies in the design of the teacher network, where we utilize both forward and backward reasoning to enhance the learning of intermediate entity distributions. By considering bidirectional reasoning, the teacher network can produce more reliable intermediate supervision signals, which can alleviate the issue of spurious reasoning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our approach on the KBQA task. The code to reproduce our analysis is available at https://github.com/RichardHGL/WSDM2021_NSM.

</p>
</details>

<details><summary><b>Machine Learning Uncertainty as a Design Material: A Post-Phenomenological Inquiry</b>
<a href="https://arxiv.org/abs/2101.04035">arxiv:2101.04035</a>
&#x1F4C8; 10 <br>
<p>Jesse Josua Benjamin, Arne Berger, Nick Merrill, James Pierce</p></summary>
<p>

**Abstract:** Design research is important for understanding and interrogating how emerging technologies shape human experience. However, design research with Machine Learning (ML) is relatively underdeveloped. Crucially, designers have not found a grasp on ML uncertainty as a design opportunity rather than an obstacle. The technical literature points to data and model uncertainties as two main properties of ML. Through post-phenomenology, we position uncertainty as one defining material attribute of ML processes which mediate human experience. To understand ML uncertainty as a design material, we investigate four design research case studies involving ML. We derive three provocative concepts: thingly uncertainty: ML-driven artefacts have uncertain, variable relations to their environments; pattern leakage: ML uncertainty can lead to patterns shaping the world they are meant to represent; and futures creep: ML technologies texture human relations to time with uncertainty. Finally, we outline design research trajectories and sketch a post-phenomenological approach to human-ML relations.

</p>
</details>

<details><summary><b>Fits and Starts: Enterprise Use of AutoML and the Role of Humans in the Loop</b>
<a href="https://arxiv.org/abs/2101.04296">arxiv:2101.04296</a>
&#x1F4C8; 9 <br>
<p>Anamaria Crisan, Brittany Fiore-Gartland</p></summary>
<p>

**Abstract:** AutoML systems can speed up routine data science work and make machine learning available to those without expertise in statistics and computer science. These systems have gained traction in enterprise settings where pools of skilled data workers are limited. In this study, we conduct interviews with 29 individuals from organizations of different sizes to characterize how they currently use, or intend to use, AutoML systems in their data science work. Our investigation also captures how data visualization is used in conjunction with AutoML systems. Our findings identify three usage scenarios for AutoML that resulted in a framework summarizing the level of automation desired by data workers with different levels of expertise. We surfaced the tension between speed and human oversight and found that data visualization can do a poor job balancing the two. Our findings have implications for the design and implementation of human-in-the-loop visual analytics approaches.

</p>
</details>

<details><summary><b>Challenges and approaches to time-series forecasting in data center telemetry: A Survey</b>
<a href="https://arxiv.org/abs/2101.04224">arxiv:2101.04224</a>
&#x1F4C8; 8 <br>
<p>Shruti Jadon, Jan Kanty Milczek, Ajit Patankar</p></summary>
<p>

**Abstract:** Time-series forecasting has been an important research domain for so many years. Its applications include ECG predictions, sales forecasting, weather conditions, even COVID-19 spread predictions. These applications have motivated many researchers to figure out an optimal forecasting approach, but the modeling approach also changes as the application domain changes. This work has focused on reviewing different forecasting approaches for telemetry data predictions collected at data centers. Forecasting of telemetry data is a critical feature of network and data center management products. However, there are multiple options of forecasting approaches that range from a simple linear statistical model to high capacity deep learning architectures. In this paper, we attempted to summarize and evaluate the performance of well known time series forecasting techniques. We hope that this evaluation provides a comprehensive summary to innovate in forecasting approaches for telemetry data.

</p>
</details>

<details><summary><b>Explain and Predict, and then Predict Again</b>
<a href="https://arxiv.org/abs/2101.04109">arxiv:2101.04109</a>
&#x1F4C8; 8 <br>
<p>Zijian Zhang, Koustav Rudra, Avishek Anand</p></summary>
<p>

**Abstract:** A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, that uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. And then we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- fact verification, sentiment classification, and QA -- and find that we substantially outperform existing approaches.

</p>
</details>

<details><summary><b>Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation</b>
<a href="https://arxiv.org/abs/2101.04108">arxiv:2101.04108</a>
&#x1F4C8; 8 <br>
<p>Umang Gupta, Aaron M Ferber, Bistra Dilkina, Greg Ver Steeg</p></summary>
<p>

**Abstract:** Controlling bias in training datasets is vital for ensuring equal treatment, or parity, between different groups in downstream applications. A naive solution is to transform the data so that it is statistically independent of group membership, but this may throw away too much information when a reasonable compromise between fairness and accuracy is desired. Another common approach is to limit the ability of a particular adversary who seeks to maximize parity. Unfortunately, representations produced by adversarial approaches may still retain biases as their efficacy is tied to the complexity of the adversary used during training. To this end, we theoretically establish that by limiting the mutual information between representations and protected attributes, we can assuredly control the parity of any downstream classifier. We demonstrate an effective method for controlling parity through mutual information based on contrastive information estimators and show that they outperform approaches that rely on variational bounds based on complex generative models. We test our approach on UCI Adult and Heritage Health datasets and demonstrate that our approach provides more informative representations across a range of desired parity thresholds while providing strong theoretical guarantees on the parity of any downstream algorithm.

</p>
</details>

<details><summary><b>Towards glass-box CNNs</b>
<a href="https://arxiv.org/abs/2101.10443">arxiv:2101.10443</a>
&#x1F4C8; 7 <br>
<p>Piduguralla Manaswini, Jignesh S. Bhatt</p></summary>
<p>

**Abstract:** Convolution neural networks (CNNs) are brain-inspired architectures popular for their ability to train and relearn visually complex tasks. It is incremental and scalable; however, CNN is mostly treated as black-box and involves multiple trial & error runs. We observe that CNN constructs powerful internal representations that help achieve state-of-the-art performance. Here we propose three layer glass-box (analytical) CNN for two-class image classifcation problems. First is a representation layer that encompasses both the class information (group invariant) and symmetric transformations (group equivariant) of input images. It is then passed through dimension reduction layer (PCA). Finally the compact yet complete representation is provided to a classifer. Analytical machine learning classifers and multilayer perceptrons are used to assess sensitivity. Proposed glass-box CNN is compared with equivariance of AlexNet (CNN) internal representation for better understanding and dissemination of results. In future, we would like to construct glass-box CNN for multiclass visually complex tasks.

</p>
</details>

<details><summary><b>Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2101.04097">arxiv:2101.04097</a>
&#x1F4C8; 7 <br>
<p>Adrià Garriga-Alonso, Mark van der Wilk</p></summary>
<p>

**Abstract:** Infinite width limits of deep neural networks often have tractable forms. They have been used to analyse the behaviour of finite networks, as well as being useful methods in their own right. When investigating infinitely wide convolutional neural networks (CNNs), it was observed that the correlations arising from spatial weight sharing disappear in the infinite limit. This is undesirable, as spatial correlation is the main motivation behind CNNs. We show that the loss of this property is not a consequence of the infinite limit, but rather of choosing an independent weight prior. Correlating the weights maintains the correlations in the activations. Varying the amount of correlation interpolates between independent-weight limits and mean-pooling. Empirical evaluation of the infinitely wide network shows that optimal performance is achieved between the extremes, indicating that correlations can be useful.

</p>
</details>

<details><summary><b>Distributed Double Machine Learning with a Serverless Architecture</b>
<a href="https://arxiv.org/abs/2101.04025">arxiv:2101.04025</a>
&#x1F4C8; 7 <br>
<p>Malte S. Kurz</p></summary>
<p>

**Abstract:** This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation \texttt{DoubleML-Serverless} for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs.

</p>
</details>

<details><summary><b>The Slodderwetenschap (Sloppy Science) of Stochastic Parrots -- A Plea for Science to NOT take the Route Advocated by Gebru and Bender</b>
<a href="https://arxiv.org/abs/2101.10098">arxiv:2101.10098</a>
&#x1F4C8; 6 <br>
<p>Michael Lissack</p></summary>
<p>

**Abstract:** This article is a position paper written in reaction to the now-infamous paper titled "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" by Timnit Gebru, Emily Bender, and others who were, as of the date of this writing, still unnamed. I find the ethics of the Parrot Paper lacking, and in that lack, I worry about the direction in which computer science, machine learning, and artificial intelligence are heading. At best, I would describe the argumentation and evidentiary practices embodied in the Parrot Paper as Slodderwetenschap (Dutch for Sloppy Science) -- a word which the academic world last widely used in conjunction with the Diederik Stapel affair in psychology [2]. What is missing in the Parrot Paper are three critical elements: 1) acknowledgment that it is a position paper/advocacy piece rather than research, 2) explicit articulation of the critical presuppositions, and 3) explicit consideration of cost/benefit trade-offs rather than a mere recitation of potential "harms" as if benefits did not matter. To leave out these three elements is not good practice for either science or research.

</p>
</details>

<details><summary><b>CleftNet: Augmented Deep Learning for Synaptic Cleft Detection from Brain Electron Microscopy</b>
<a href="https://arxiv.org/abs/2101.04266">arxiv:2101.04266</a>
&#x1F4C8; 6 <br>
<p>Yi Liu, Shuiwang Ji</p></summary>
<p>

**Abstract:** Detecting synaptic clefts is a crucial step to investigate the biological function of synapses. The volume electron microscopy (EM) allows the identification of synaptic clefts by photoing EM images with high resolution and fine details. Machine learning approaches have been employed to automatically predict synaptic clefts from EM images. In this work, we propose a novel and augmented deep learning model, known as CleftNet, for improving synaptic cleft detection from brain EM images. We first propose two novel network components, known as the feature augmentor and the label augmentor, for augmenting features and labels to improve cleft representations. The feature augmentor can fuse global information from inputs and learn common morphological patterns in clefts, leading to augmented cleft features. In addition, it can generate outputs with varying dimensions, making it flexible to be integrated in any deep network. The proposed label augmentor augments the label of each voxel from a value to a vector, which contains both the segmentation label and boundary label. This allows the network to learn important shape information and to produce more informative cleft representations. Based on the proposed feature augmentor and label augmentor, We build the CleftNet as a U-Net like network. The effectiveness of our methods is evaluated on both online and offline tasks. Our CleftNet currently ranks \#1 on the online task of the CREMI open challenge. In addition, both quantitative and qualitative results in the offline tasks show that our method outperforms the baseline approaches significantly.

</p>
</details>

<details><summary><b>System Design for a Data-driven and Explainable Customer Sentiment Monitor</b>
<a href="https://arxiv.org/abs/2101.04086">arxiv:2101.04086</a>
&#x1F4C8; 6 <br>
<p>An Nguyen, Stefan Foerstel, Thomas Kittler, Andrey Kurzyukov, Leo Schwinn, Dario Zanca, Tobias Hipp, Da Jun Sun, Michael Schrapp, Eva Rothgang, Bjoern Eskofier</p></summary>
<p>

**Abstract:** The most important goal of customer services is to keep the customer satisfied. However, service resources are always limited and must be prioritized. Therefore, it is important to identify customers who potentially become unsatisfied and might lead to escalations. Today this prioritization of customers is often done manually. Data science on IoT data (esp. log data) for machine health monitoring, as well as analytics on enterprise data for customer relationship management (CRM) have mainly been researched and applied independently. In this paper, we present a framework for a data-driven decision support system which combines IoT and enterprise data to model customer sentiment. Such decision support systems can help to prioritize customers and service resources to effectively troubleshoot problems or even avoid them. The framework is applied in a real-world case study with a major medical device manufacturer. This includes a fully automated and interpretable machine learning pipeline designed to meet the requirements defined with domain experts and end users. The overall framework is currently deployed, learns and evaluates predictive models from terabytes of IoT and enterprise data to actively monitor the customer sentiment for a fleet of thousands of high-end medical devices. Furthermore, we provide an anonymized industrial benchmark dataset for the research community.

</p>
</details>

<details><summary><b>Preconditioned training of normalizing flows for variational inference in inverse problems</b>
<a href="https://arxiv.org/abs/2101.03709">arxiv:2101.03709</a>
&#x1F4C8; 6 <br>
<p>Ali Siahkoohi, Gabrio Rizzuti, Mathias Louboutin, Philipp A. Witte, Felix J. Herrmann</p></summary>
<p>

**Abstract:** Obtaining samples from the posterior distribution of inverse problems with expensive forward operators is challenging especially when the unknowns involve the strongly heterogeneous Earth. To meet these challenges, we propose a preconditioning scheme involving a conditional normalizing flow (NF) capable of sampling from a low-fidelity posterior distribution directly. This conditional NF is used to speed up the training of the high-fidelity objective involving minimization of the Kullback-Leibler divergence between the predicted and the desired high-fidelity posterior density for indirect measurements at hand. To minimize costs associated with the forward operator, we initialize the high-fidelity NF with the weights of the pretrained low-fidelity NF, which is trained beforehand on available model and data pairs. Our numerical experiments, including a 2D toy and a seismic compressed sensing example, demonstrate that thanks to the preconditioning considerable speed-ups are achievable compared to training NFs from scratch.

</p>
</details>

<details><summary><b>Solving Common-Payoff Games with Approximate Policy Iteration</b>
<a href="https://arxiv.org/abs/2101.04237">arxiv:2101.04237</a>
&#x1F4C8; 5 <br>
<p>Samuel Sokota, Edward Lockhart, Finbarr Timbers, Elnaz Davoodi, Ryan D'Orazio, Neil Burch, Martin Schmid, Michael Bowling, Marc Lanctot</p></summary>
<p>

**Abstract:** For artificially intelligent learning systems to have widespread applicability in real-world settings, it is important that they be able to operate decentrally. Unfortunately, decentralized control is difficult -- computing even an epsilon-optimal joint policy is a NEXP complete problem. Nevertheless, a recently rediscovered insight -- that a team of agents can coordinate via common knowledge -- has given rise to algorithms capable of finding optimal joint policies in small common-payoff games. The Bayesian action decoder (BAD) leverages this insight and deep reinforcement learning to scale to games as large as two-player Hanabi. However, the approximations it uses to do so prevent it from discovering optimal joint policies even in games small enough to brute force optimal solutions. This work proposes CAPI, a novel algorithm which, like BAD, combines common knowledge with deep reinforcement learning. However, unlike BAD, CAPI prioritizes the propensity to discover optimal joint policies over scalability. While this choice precludes CAPI from scaling to games as large as Hanabi, empirical results demonstrate that, on the games to which CAPI does scale, it is capable of discovering optimal joint policies even when other modern multi-agent reinforcement learning algorithms are unable to do so. Code is available at https://github.com/ssokota/capi .

</p>
</details>

<details><summary><b>PyHealth: A Python Library for Health Predictive Models</b>
<a href="https://arxiv.org/abs/2101.04209">arxiv:2101.04209</a>
&#x1F4C8; 5 <br>
<p>Yue Zhao, Zhi Qiao, Cao Xiao, Lucas Glass, Jimeng Sun</p></summary>
<p>

**Abstract:** Despite the explosion of interest in healthcare AI research, the reproducibility and benchmarking of those research works are often limited due to the lack of standard benchmark datasets and diverse evaluation metrics. To address this reproducibility challenge, we develop PyHealth, an open-source Python toolbox for developing various predictive models on healthcare data.
  PyHealth consists of data preprocessing module, predictive modeling module, and evaluation module. The target users of PyHealth are both computer science researchers and healthcare data scientists. With PyHealth, they can conduct complex machine learning pipelines on healthcare datasets with fewer than ten lines of code. The data preprocessing module enables the transformation of complex healthcare datasets such as longitudinal electronic health records, medical images, continuous signals (e.g., electrocardiogram), and clinical notes into machine learning friendly formats. The predictive modeling module provides more than 30 machine learning models, including established ensemble trees and deep neural network-based approaches, via a unified but extendable API designed for both researchers and practitioners. The evaluation module provides various evaluation strategies (e.g., cross-validation and train-validation-test split) and predictive model metrics.
  With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, and interactive examples are introduced in the library's development. PyHealth can be installed through the Python Package Index (PyPI) or https://github.com/yzhao062/PyHealth .

</p>
</details>

<details><summary><b>Action Priors for Large Action Spaces in Robotics</b>
<a href="https://arxiv.org/abs/2101.04178">arxiv:2101.04178</a>
&#x1F4C8; 5 <br>
<p>Ondrej Biza, Dian Wang, Robert Platt, Jan-Willem van de Meent, Lawson L. S. Wong</p></summary>
<p>

**Abstract:** In robotics, it is often not possible to learn useful policies using pure model-free reinforcement learning without significant reward shaping or curriculum learning. As a consequence, many researchers rely on expert demonstrations to guide learning. However, acquiring expert demonstrations can be expensive. This paper proposes an alternative approach where the solutions of previously solved tasks are used to produce an action prior that can facilitate exploration in future tasks. The action prior is a probability distribution over actions that summarizes the set of policies found solving previous tasks. Our results indicate that this approach can be used to solve robotic manipulation problems that would otherwise be infeasible without expert demonstrations. Source code is available at \url{https://github.com/ondrejba/action_priors}.

</p>
</details>

<details><summary><b>A Brief Survey of Associations Between Meta-Learning and General AI</b>
<a href="https://arxiv.org/abs/2101.04283">arxiv:2101.04283</a>
&#x1F4C8; 4 <br>
<p>Huimin Peng</p></summary>
<p>

**Abstract:** This paper briefly reviews the history of meta-learning and describes its contribution to general AI. Meta-learning improves model generalization capacity and devises general algorithms applicable to both in-distribution and out-of-distribution tasks potentially. General AI replaces task-specific models with general algorithmic systems introducing higher level of automation in solving diverse tasks using AI. We summarize main contributions of meta-learning to the developments in general AI, including memory module, meta-learner, coevolution, curiosity, forgetting and AI-generating algorithm. We present connections between meta-learning and general AI and discuss how meta-learning can be used to formulate general AI algorithms.

</p>
</details>

<details><summary><b>A Convergence Theory Towards Practical Over-parameterized Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2101.04243">arxiv:2101.04243</a>
&#x1F4C8; 4 <br>
<p>Asaf Noy, Yi Xu, Yonathan Aflalo, Lihi Zelnik-Manor, Rong Jin</p></summary>
<p>

**Abstract:** Deep neural networks' remarkable ability to correctly fit training data when optimized by gradient-based algorithms is yet to be fully understood. Recent theoretical results explain the convergence for ReLU networks that are wider than those used in practice by orders of magnitude. In this work, we take a step towards closing the gap between theory and practice by significantly improving the known theoretical bounds on both the network width and the convergence time. We show that convergence to a global minimum is guaranteed for networks with widths quadratic in the sample size and linear in their depth at a time logarithmic in both. Our analysis and convergence bounds are derived via the construction of a surrogate network with fixed activation patterns that can be transformed at any time to an equivalent ReLU network of a reasonable size. This construction can be viewed as a novel technique to accelerate training, while its tight finite-width equivalence to Neural Tangent Kernel (NTK) suggests it can be utilized to study generalization as well.

</p>
</details>

<details><summary><b>Resolution-Based Distillation for Efficient Histology Image Classification</b>
<a href="https://arxiv.org/abs/2101.04170">arxiv:2101.04170</a>
&#x1F4C8; 4 <br>
<p>Joseph DiPalma, Arief A. Suriawinata, Laura J. Tafe, Lorenzo Torresani, Saeed Hassanpour</p></summary>
<p>

**Abstract:** Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classification. The proposed approach is robust when used with images that have reduced input resolution and can be trained effectively with limited labeled data. Pre-trained on the original high-resolution (HR) images, our method uses knowledge distillation (KD) to transfer learned knowledge from a teacher model to a student model trained on the same images at a much lower resolution. To address the lack of large-scale labeled histology image datasets, we perform KD in a self-supervised manner. We evaluate our approach on two histology image datasets associated with celiac disease (CD) and lung adenocarcinoma (LUAD). Our results show that a combination of KD and self-supervision allows the student model to approach, and in some cases, surpass the classification accuracy of the teacher, while being much more efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential to scale further. For the CD data, our model outperforms the HR teacher model, while needing 4 times fewer computations. For the LUAD data, our student model results at 1.25x magnification are within 3% of the teacher model at 10x magnification, with a 64 times computational cost reduction. Moreover, our CD outcomes benefit from performance scaling with the use of more unlabeled data. For 0.625x magnification, using unlabeled data improves accuracy by 4% over the baseline. Thus, our method can improve the feasibility of deep learning solutions for digital pathology with standard computational hardware.

</p>
</details>

<details><summary><b>Automatic Polyp Segmentation using Fully Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2101.04001">arxiv:2101.04001</a>
&#x1F4C8; 4 <br>
<p>Nikhil Kumar Tomar</p></summary>
<p>

**Abstract:** Colorectal cancer is one of fatal cancer worldwide. Colonoscopy is the standard treatment for examination, localization, and removal of colorectal polyps. However, it has been shown that the miss-rate of colorectal polyps during colonoscopy is between 6 to 27%. The use of an automated, accurate, and real-time polyp segmentation during colonoscopy examinations can help the clinicians to eliminate missing lesions and prevent further progression of colorectal cancer. The ``Medico automatic polyp segmentation challenge'' provides an opportunity to study polyp segmentation and build a fast segmentation model. The challenge organizers provide a Kvasir-SEG dataset to train the model. Then it is tested on a separate unseen dataset to validate the efficiency and speed of the segmentation model. The experiments demonstrate that the model trained on the Kvasir-SEG dataset and tested on an unseen dataset achieves a dice coefficient of 0.7801, mIoU of 0.6847, recall of 0.8077, and precision of 0.8126, demonstrating the generalization ability of our model. The model has achieved 80.60 FPS on the unseen dataset with an image resolution of $512 \times 512$.

</p>
</details>

<details><summary><b>Individual Mobility Prediction: An Interpretable Activity-based Hidden Markov Approach</b>
<a href="https://arxiv.org/abs/2101.03996">arxiv:2101.03996</a>
&#x1F4C8; 4 <br>
<p>Baichuan Mo, Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao</p></summary>
<p>

**Abstract:** Individual mobility is driven by demand for activities with diverse spatiotemporal patterns, but existing methods for mobility prediction often overlook the underlying activity patterns. To address this issue, this study develops an activity-based modeling framework for individual mobility prediction. Specifically, an input-output hidden Markov model (IOHMM) framework is proposed to simultaneously predict the (continuous) time and (discrete) location of an individual's next trip using transit smart card data. The prediction task can be transformed into predicting the hidden activity duration and end location. Based on a case study of Hong Kong's metro system, we show that the proposed model can achieve similar prediction performance as the state-of-the-art long short-term memory (LSTM) model. Unlike LSTM, the proposed IOHMM model can also be used to analyze hidden activity patterns, which provides meaningful behavioral interpretation for why an individual makes a certain trip. Therefore, the activity-based prediction framework offers a way to preserve the predictive power of advanced machine learning methods while enhancing our ability to generate insightful behavioral explanations, which is useful for enhancing situational awareness in user-centric transportation applications such as personalized traveler information.

</p>
</details>

<details><summary><b>Cycle Generative Adversarial Networks Algorithm With Style Transfer For Image Generation</b>
<a href="https://arxiv.org/abs/2101.03921">arxiv:2101.03921</a>
&#x1F4C8; 4 <br>
<p>Anugrah Akbar Praramadhan, Guntur Eka Saputra</p></summary>
<p>

**Abstract:** The biggest challenge faced by a Machine Learning Engineer is the lack of data they have, especially for 2-dimensional images. The image is processed to be trained into a Machine Learning model so that it can recognize patterns in the data and provide predictions. This research is intended to create a solution using the Cycle Generative Adversarial Networks (GANs) algorithm in overcoming the problem of lack of data. Then use Style Transfer to be able to generate a new image based on the given style. Based on the results of testing the resulting model has been carried out several improvements, previously the loss value of the photo generator: 3.1267, monet style generator: 3.2026, photo discriminator: 0.6325, and monet style discriminator: 0.6931 to photo generator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956, and monet style discriminator: 0.4940. It is hoped that the research will make the application of this solution useful in the fields of Education, Arts, Information Technology, Medicine, Astronomy, Automotive and other important fields.

</p>
</details>

<details><summary><b>Model Generalization on COVID-19 Fake News Detection</b>
<a href="https://arxiv.org/abs/2101.03841">arxiv:2101.03841</a>
&#x1F4C8; 4 <br>
<p>Yejin Bang, Etsuko Ishii, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung</p></summary>
<p>

**Abstract:** Amid the pandemic COVID-19, the world is facing unprecedented infodemic with the proliferation of both fake and real information. Considering the problematic consequences that the COVID-19 fake-news have brought, the scientific community has put effort to tackle it. To contribute to this fight against the infodemic, we aim to achieve a robust model for the COVID-19 fake-news detection task proposed at CONSTRAINT 2021 (FakeNews-19) by taking two separate approaches: 1) fine-tuning transformers based language models with robust loss functions and 2) removing harmful training instances through influence calculation. We further evaluate the robustness of our models by evaluating on different COVID-19 misinformation test set (Tweets-19) to understand model generalization ability. With the first approach, we achieve 98.13% for weighted F1 score (W-F1) for the shared task, whereas 38.18% W-F1 on the Tweets-19 highest. On the contrary, by performing influence data cleansing, our model with 99% cleansing percentage can achieve 54.33% W-F1 score on Tweets-19 with a trade-off. By evaluating our models on two COVID-19 fake-news test sets, we suggest the importance of model generalization ability in this task to step forward to tackle the COVID-19 fake-news problem in online social media platforms.

</p>
</details>

<details><summary><b>Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain Detection</b>
<a href="https://arxiv.org/abs/2101.03778">arxiv:2101.03778</a>
&#x1F4C8; 4 <br>
<p>Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, Irina Piontkovskaya</p></summary>
<p>

**Abstract:** Real-life applications, heavily relying on machine learning, such as dialog systems, demand out-of-domain detection methods. Intent classification models should be equipped with a mechanism to distinguish seen intents from unseen ones so that the dialog agent is capable of rejecting the latter and avoiding undesired behavior. However, despite increasing attention paid to the task, the best practices for out-of-domain intent detection have not yet been fully established.
  This paper conducts a thorough comparison of out-of-domain intent detection methods. We prioritize the methods, not requiring access to out-of-domain data during training, gathering of which is extremely time- and labor-consuming due to lexical and stylistic variation of user utterances. We evaluate multiple contextual encoders and methods, proven to be efficient, on three standard datasets for intent classification, expanded with out-of-domain utterances. Our main findings show that fine-tuning Transformer-based encoders on in-domain data leads to superior results. Mahalanobis distance, together with utterance representations, derived from Transformer-based encoders, outperforms other methods by a wide margin and establishes new state-of-the-art results for all datasets.
  The broader analysis shows that the reason for success lies in the fact that the fine-tuned Transformer is capable of constructing homogeneous representations of in-domain utterances, revealing geometrical disparity to out of domain utterances. In turn, the Mahalanobis distance captures this disparity easily.

</p>
</details>

<details><summary><b>Cognitive Visual Inspection Service for LCD Manufacturing Industry</b>
<a href="https://arxiv.org/abs/2101.03747">arxiv:2101.03747</a>
&#x1F4C8; 4 <br>
<p>Yuanyuan Ding, Junchi Yan, Guoqiang Hu, Jun Zhu</p></summary>
<p>

**Abstract:** With the rapid growth of display devices, quality inspection via machine vision technology has become increasingly important for flat-panel displays (FPD) industry. This paper discloses a novel visual inspection system for liquid crystal display (LCD), which is currently a dominant type in the FPD industry. The system is based on two cornerstones: robust/high-performance defect recognition model and cognitive visual inspection service architecture. A hybrid application of conventional computer vision technique and the latest deep convolutional neural network (DCNN) leads to an integrated defect detection, classfication and impact evaluation model that can be economically trained with only image-level class annotations to achieve a high inspection accuracy. In addition, the properly trained model is robust to the variation of the image qulity, significantly alleviating the dependency between the model prediction performance and the image aquisition environment. This in turn justifies the decoupling of the defect recognition functions from the front-end device to the back-end serivce, motivating the design and realization of the cognitive visual inspection service architecture. Empirical case study is performed on a large-scale real-world LCD dataset from a manufacturing line with different layers and products, which shows the promising utility of our system, which has been deployed in a real-world LCD manufacturing line from a major player in the world.

</p>
</details>

<details><summary><b>A Transfer Learning-based State of Charge Estimation for Lithium-Ion Battery at Varying Ambient Temperatures</b>
<a href="https://arxiv.org/abs/2101.03704">arxiv:2101.03704</a>
&#x1F4C8; 4 <br>
<p>Yan Qin, Stefan Adams, Chau Yuen</p></summary>
<p>

**Abstract:** Accurate and reliable state of charge (SoC) estimation becomes increasingly important to provide a stable and efficient environment for Lithium-ion batteries (LiBs) powered devices. Most data-driven SoC models are built for a fixed ambient temperature, which neglect the high sensitivity of LiBs to temperature and may cause severe prediction errors. Nevertheless, a systematic evaluation of the impact of temperature on SoC estimation and ways for a prompt adjustment of the estimation model to new temperatures using limited data have been hardly discussed. To solve these challenges, a novel SoC estimation method is proposed by exploiting temporal dynamics of measurements and transferring consistent estimation ability among different temperatures. First, temporal dynamics, which is presented by correlations between the past fluctuation and the future motion, is extracted using canonical variate analysis. Next, two models, including a reference SoC estimation model and an estimation ability monitoring model, are developed with temporal dynamics. The monitoring model provides a path to quantitatively evaluate the influences of temperature on SoC estimation ability. After that, once the inability of the reference SoC estimation model is detected, consistent temporal dynamics between temperatures are selected for transfer learning. Finally, the efficacy of the proposed method is verified through a benchmark. Our proposed method not only reduces prediction errors at fixed temperatures (e.g., reduced by 24.35% at -20°C, 49.82% at 25°C) but also improves prediction accuracies at new temperatures.

</p>
</details>

<details><summary><b>Estimating Galactic Distances From Images Using Self-supervised Representation Learning</b>
<a href="https://arxiv.org/abs/2101.04293">arxiv:2101.04293</a>
&#x1F4C8; 3 <br>
<p>Md Abul Hayat, Peter Harrington, George Stein, Zarija Lukić, Mustafa Mustafa</p></summary>
<p>

**Abstract:** We use a contrastive self-supervised learning framework to estimate distances to galaxies from their photometric images. We incorporate data augmentations from computer vision as well as an application-specific augmentation accounting for galactic dust. We find that the resulting visual representations of galaxy images are semantically useful and allow for fast similarity searches, and can be successfully fine-tuned for the task of redshift estimation. We show that (1) pretraining on a large corpus of unlabeled data followed by fine-tuning on some labels can attain the accuracy of a fully-supervised model which requires 2-4x more labeled data, and (2) that by fine-tuning our self-supervised representations using all available data labels in the Main Galaxy Sample of the Sloan Digital Sky Survey (SDSS), we outperform the state-of-the-art supervised learning method.

</p>
</details>

<details><summary><b>Evaluating Disentanglement of Structured Latent Representations</b>
<a href="https://arxiv.org/abs/2101.04041">arxiv:2101.04041</a>
&#x1F4C8; 3 <br>
<p>Raphaël Dang-Nhu</p></summary>
<p>

**Abstract:** We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of intrinsic and extrinsic object properties. We theoretically show that our framework gives stronger guarantees of selecting a good model than previous disentanglement metrics. Experimentally, we demonstrate that viewing object compositionality as a disentanglement problem addresses several issues with prior visual metrics of object separation. As a core technical component, we present the first representation probing algorithm handling slot permutation invariance.

</p>
</details>

<details><summary><b>Identification of COVID-19 related Fake News via Neural Stacking</b>
<a href="https://arxiv.org/abs/2101.03988">arxiv:2101.03988</a>
&#x1F4C8; 3 <br>
<p>Boshko Koloski, Timen Stepišnik Perdih, Senja Pollak, Blaž Škrlj</p></summary>
<p>

**Abstract:** Identification of Fake News plays a prominent role in the ongoing pandemic, impacting multiple aspects of day-to-day life. In this work we present a solution to the shared task titled COVID19 Fake News Detection in English, scoring the 50th place amongst 168 submissions. The solution was within 1.5% of the best performing solution. The proposed solution employs a heterogeneous representation ensemble, adapted for the classification task via an additional neural classification head comprised of multiple hidden layers. The paper consists of detailed ablation studies further displaying the proposed method's behavior and possible implications. The solution is freely available. \url{https://gitlab.com/boshko.koloski/covid19-fake-news}

</p>
</details>

<details><summary><b>Generalize Ultrasound Image Segmentation via Instant and Plug & Play Style Transfer</b>
<a href="https://arxiv.org/abs/2101.03711">arxiv:2101.03711</a>
&#x1F4C8; 3 <br>
<p>Zhendong Liu, Xiaoqiong Huang, Xin Yang, Rui Gao, Rui Li, Yuanji Zhang, Yankai Huang, Guangquan Zhou, Yi Xiong, Alejandro F Frangi, Dong Ni</p></summary>
<p>

**Abstract:** Deep segmentation models that generalize to images with unknown appearance are important for real-world medical image analysis. Retraining models leads to high latency and complex pipelines, which are impractical in clinical settings. The situation becomes more severe for ultrasound image analysis because of their large appearance shifts. In this paper, we propose a novel method for robust segmentation under unknown appearance shifts. Our contribution is three-fold. First, we advance a one-stage plug-and-play solution by embedding hierarchical style transfer units into a segmentation architecture. Our solution can remove appearance shifts and perform segmentation simultaneously. Second, we adopt Dynamic Instance Normalization to conduct precise and dynamic style transfer in a learnable manner, rather than previously fixed style normalization. Third, our solution is fast and lightweight for routine clinical adoption. Given 400*400 image input, our solution only needs an additional 0.2ms and 1.92M FLOPs to handle appearance shifts compared to the baseline pipeline. Extensive experiments are conducted on a large dataset from three vendors demonstrate our proposed method enhances the robustness of deep segmentation models.

</p>
</details>

<details><summary><b>Clutter Slices Approach for Identification-on-the-fly of Indoor Spaces</b>
<a href="https://arxiv.org/abs/2101.04262">arxiv:2101.04262</a>
&#x1F4C8; 2 <br>
<p>Upinder Kaur, Praveen Abbaraju, Harrison McCarty, Richard M. Voyles</p></summary>
<p>

**Abstract:** Construction spaces are constantly evolving, dynamic environments in need of continuous surveying, inspection, and assessment. Traditional manual inspection of such spaces proves to be an arduous and time-consuming activity. Automation using robotic agents can be an effective solution. Robots, with perception capabilities can autonomously classify and survey indoor construction spaces. In this paper, we present a novel identification-on-the-fly approach for coarse classification of indoor spaces using the unique signature of clutter. Using the context granted by clutter, we recognize common indoor spaces such as corridors, staircases, shared spaces, and restrooms. The proposed clutter slices pipeline achieves a maximum accuracy of 93.6% on the presented clutter slices dataset. This sensor independent approach can be generalized to various domains to equip intelligent autonomous agents in better perceiving their environment.

</p>
</details>

<details><summary><b>Transforming Multi-Conditioned Generation from Meaning Representation</b>
<a href="https://arxiv.org/abs/2101.04257">arxiv:2101.04257</a>
&#x1F4C8; 2 <br>
<p>Joosung Lee</p></summary>
<p>

**Abstract:** In task-oriented conversation systems, natural language generation systems that generate sentences with specific information related to conversation flow are useful. Our study focuses on language generation by considering various information representing the meaning of utterances as multiple conditions of generation. NLG from meaning representations, the conditions for sentence meaning, generally goes through two steps: sentence planning and surface realization. However, we propose a simple one-stage framework to generate utterances directly from MR (Meaning Representation). Our model is based on GPT2 and generates utterances with flat conditions on slot and value pairs, which does not need to determine the structure of the sentence. We evaluate several systems in the E2E dataset with 6 automatic metrics. Our system is a simple method, but it demonstrates comparable performance to previous systems in automated metrics. In addition, using only 10\% of the data set without any other techniques, our model achieves comparable performance, and shows the possibility of performing zero-shot generation and expanding to other datasets.

</p>
</details>

<details><summary><b>General Hannan and Quinn Criterion for Common Time Series</b>
<a href="https://arxiv.org/abs/2101.04210">arxiv:2101.04210</a>
&#x1F4C8; 2 <br>
<p>Kare Kamila</p></summary>
<p>

**Abstract:** This paper aims to study data driven model selection criteria for a large class of time series, which includes ARMA or AR($\infty$) processes, as well as GARCH or ARCH($\infty$), APARCH and many others processes. We tackled the challenging issue of designing adaptive criteria which enjoys the strong consistency property. When the observations are generated from one of the aforementioned models, the new criteria, select the true model almost surely asymptotically. The proposed criteria are based on the minimization of a penalized contrast akin to the Hannan and Quinn's criterion and then involved a term which is known for most classical time series models and for more complex models, this term can be data driven calibrated. Monte-Carlo experiments and an illustrative example on the CAC 40 index are performed to highlight the obtained results.

</p>
</details>

<details><summary><b>Where you live matters: a spatial analysis of COVID-19 mortality</b>
<a href="https://arxiv.org/abs/2101.04199">arxiv:2101.04199</a>
&#x1F4C8; 2 <br>
<p>Behzad Javaheri</p></summary>
<p>

**Abstract:** The COVID-19 pandemic has caused ~ 2 million fatalities. Significant progress has been made in advancing our understanding of the disease process, one of the unanswered questions, however, is the anomaly in the case/mortality ratio with Mexico as a clear example. Herein, this anomaly is explored by spatial analysis and whether mortality varies locally according to local factors. To address this, hexagonal cartogram maps (hexbin) used to spatially map COVID-19 mortality and visualise association with patient-level data on demographics and pre-existing health conditions. This was further interrogated at local Mexico City level by choropleth mapping. Our data show that the use of hexagonal cartograms is a better approach for spatial mapping of COVID-19 data in Mexico as it addresses bias in area size and population. We report sex/age-related spatial relationship with mortality amongst the Mexican states and a trend between health conditions and mortality at the state level. Within Mexico City, there is a clear south, north divide with higher mortality in the northern municipalities. Deceased patients in these northern municipalities have the highest pre-existing health conditions. Taken together, this study provides an improved presentation of COVID-19 mapping in Mexico and demonstrates spatial divergence of the mortality in Mexico.

</p>
</details>

<details><summary><b>On the Practicality of Differential Privacy in Federated Learning by Tuning Iteration Times</b>
<a href="https://arxiv.org/abs/2101.04163">arxiv:2101.04163</a>
&#x1F4C8; 2 <br>
<p>Yao Fu, Yipeng Zhou, Di Wu, Shui Yu, Yonggang Wen, Chao Li</p></summary>
<p>

**Abstract:** In spite that Federated Learning (FL) is well known for its privacy protection when training machine learning models among distributed clients collaboratively, recent studies have pointed out that the naive FL is susceptible to gradient leakage attacks. In the meanwhile, Differential Privacy (DP) emerges as a promising countermeasure to defend against gradient leakage attacks. However, the adoption of DP by clients in FL may significantly jeopardize the model accuracy. It is still an open problem to understand the practicality of DP from a theoretic perspective. In this paper, we make the first attempt to understand the practicality of DP in FL through tuning the number of conducted iterations. Based on the FedAvg algorithm, we formally derive the convergence rate with DP noises in FL. Then, we theoretically derive: 1) the conditions for the DP based FedAvg to converge as the number of global iterations (GI) approaches infinity; 2) the method to set the number of local iterations (LI) to minimize the negative influence of DP noises. By further substituting the Laplace and Gaussian mechanisms into the derived convergence rate respectively, we show that: 3) The DP based FedAvg with the Laplace mechanism cannot converge, but the divergence rate can be effectively prohibited by setting the number of LIs with our method; 4) The learning error of the DP based FedAvg with the Gaussian mechanism can converge to a constant number finally if we use a fixed number of LIs per GI. To verify our theoretical findings, we conduct extensive experiments using two real-world datasets. The results not only validate our analysis results, but also provide useful guidelines on how to optimize model accuracy when incorporating DP into FL

</p>
</details>

<details><summary><b>Evaluation of Deep Learning Models for Hostility Detection in Hindi Text</b>
<a href="https://arxiv.org/abs/2101.04144">arxiv:2101.04144</a>
&#x1F4C8; 2 <br>
<p>Ramchandra Joshi, Rushabh Karnavat, Kaustubh Jirapure, Raviraj Joshi</p></summary>
<p>

**Abstract:** The social media platform is a convenient medium to express personal thoughts and share useful information. It is fast, concise, and has the ability to reach millions. It is an effective place to archive thoughts, share artistic content, receive feedback, promote products, etc. Despite having numerous advantages these platforms have given a boost to hostile posts. Hate speech and derogatory remarks are being posted for personal satisfaction or political gain. The hostile posts can have a bullying effect rendering the entire platform experience hostile. Therefore detection of hostile posts is important to maintain social media hygiene. The problem is more pronounced languages like Hindi which are low in resources. In this work, we present approaches for hostile text detection in the Hindi language. The proposed approaches are evaluated on the Constraint@AAAI 2021 Hindi hostility detection dataset. The dataset consists of hostile and non-hostile texts collected from social media platforms. The hostile posts are further segregated into overlapping classes of fake, offensive, hate, and defamation. We evaluate a host of deep learning approaches based on CNN, LSTM, and BERT for this multi-label classification problem. The pre-trained Hindi fast text word embeddings by IndicNLP and Facebook are used in conjunction with CNN and LSTM models. Two variations of pre-trained multilingual transformer language models mBERT and IndicBERT are used. We show that the performance of BERT based models is best. Moreover, CNN and LSTM models also perform competitively with BERT based models.

</p>
</details>

<details><summary><b>Automating the Compilation of Potential Core-Outcomes for Clinical Trials</b>
<a href="https://arxiv.org/abs/2101.04076">arxiv:2101.04076</a>
&#x1F4C8; 2 <br>
<p>Shwetha Bharadwaj, Melanie Laffin</p></summary>
<p>

**Abstract:** Due to increased access to clinical trial outcomes and analysis, researchers and scientists are able to iterate or improve upon relevant approaches more effectively. However, the metrics and related results of clinical trials typically do not follow any standardization in their reports, making it more difficult for researchers to parse the results of different trials. The objective of this paper is to describe an automated method utilizing natural language processing in order to describe the probable core outcomes of clinical trials, in order to alleviate the issues around disparate clinical trial outcomes. As the nature of this process is domain specific, BioBERT was employed in order to conduct a multi-class entity normalization task. In addition to BioBERT, an unsupervised feature-based approach making use of only the encoder output embedding representations for the outcomes and labels was utilized. Finally, cosine similarity was calculated across the vectors to obtain the semantic similarity. This method was able to both harness the domain-specific context of each of the tokens from the learned embeddings of the BioBERT model as well as a more stable metric of sentence similarity. Some common outcomes identified using the Jaccard similarity in each of the classifications were compiled, and while some are untenable, a pipeline for which this automation process could be conducted was established.

</p>
</details>

<details><summary><b>Learning to Ignore: Fair and Task Independent Representations</b>
<a href="https://arxiv.org/abs/2101.04047">arxiv:2101.04047</a>
&#x1F4C8; 2 <br>
<p>Linda H. Boedi, Helmut Grabner</p></summary>
<p>

**Abstract:** Training fair machine learning models, aiming for their interpretability and solving the problem of domain shift has gained a lot of interest in the last years. There is a vast amount of work addressing these topics, mostly in separation. In this work we show that they can be seen as a common framework of learning invariant representations. The representations should allow to predict the target while at the same time being invariant to sensitive attributes which split the dataset into subgroups. Our approach is based on the simple observation that it is impossible for any learning algorithm to differentiate samples if they have the same feature representation. This is formulated as an additional loss (regularizer) enforcing a common feature representation across subgroups. We apply it to learn fair models and interpret the influence of the sensitive attribute. Furthermore it can be used for domain adaptation, transferring knowledge and learning effectively from very few examples. In all applications it is essential not only to learn to predict the target, but also to learn what to ignore.

</p>
</details>

<details><summary><b>Analysis of skin lesion images with deep learning</b>
<a href="https://arxiv.org/abs/2101.03814">arxiv:2101.03814</a>
&#x1F4C8; 2 <br>
<p>Josef Steppan, Sten Hanke</p></summary>
<p>

**Abstract:** Skin cancer is the most common cancer worldwide, with melanoma being the deadliest form. Dermoscopy is a skin imaging modality that has shown an improvement in the diagnosis of skin cancer compared to visual examination without support. We evaluate the current state of the art in the classification of dermoscopic images based on the ISIC-2019 Challenge for the classification of skin lesions and current literature. Various deep neural network architectures pre-trained on the ImageNet data set are adapted to a combined training data set comprised of publicly available dermoscopic and clinical images of skin lesions using transfer learning and model fine-tuning. The performance and applicability of these models for the detection of eight classes of skin lesions are examined. Real-time data augmentation, which uses random rotation, translation, shear, and zoom within specified bounds is used to increase the number of available training samples. Model predictions are multiplied by inverse class frequencies and normalized to better approximate actual probability distributions. Overall prediction accuracy is further increased by using the arithmetic mean of the predictions of several independently trained models. The best single model has been published as a web service.

</p>
</details>

<details><summary><b>Hierarchical Clustering using Auto-encoded Compact Representation for Time-series Analysis</b>
<a href="https://arxiv.org/abs/2101.03742">arxiv:2101.03742</a>
&#x1F4C8; 2 <br>
<p>Soma Bandyopadhyay, Anish Datta, Arpan Pal</p></summary>
<p>

**Abstract:** Getting a robust time-series clustering with best choice of distance measure and appropriate representation is always a challenge. We propose a novel mechanism to identify the clusters combining learned compact representation of time-series, Auto Encoded Compact Sequence (AECS) and hierarchical clustering approach. Proposed algorithm aims to address the large computing time issue of hierarchical clustering as learned latent representation AECS has a length much less than the original length of time-series and at the same time want to enhance its performance.Our algorithm exploits Recurrent Neural Network (RNN) based under complete Sequence to Sequence(seq2seq) autoencoder and agglomerative hierarchical clustering with a choice of best distance measure to recommend the best clustering. Our scheme selects the best distance measure and corresponding clustering for both univariate and multivariate time-series. We have experimented with real-world time-series from UCR and UCI archive taken from diverse application domains like health, smart-city, manufacturing etc. Experimental results show that proposed method not only produce close to benchmark results but also in some cases outperform the benchmark.

</p>
</details>

<details><summary><b>Trace Ratio Optimization with an Application to Multi-view Learning</b>
<a href="https://arxiv.org/abs/2101.04292">arxiv:2101.04292</a>
&#x1F4C8; 1 <br>
<p>Li Wang, Lei-Hong Zhang, Ren-Cang Li</p></summary>
<p>

**Abstract:** A trace ratio optimization problem over the Stiefel manifold is investigated from the perspectives of both theory and numerical computations. At least three special cases of the problem have arisen from Fisher linear discriminant analysis, canonical correlation analysis, and unbalanced Procrustes problem, respectively. Necessary conditions in the form of nonlinear eigenvalue problem with eigenvector dependency are established and a numerical method based on the self-consistent field (SCF) iteration is designed and proved to be always convergent. As an application to multi-view subspace learning, a new framework and its instantiated concrete models are proposed and demonstrated on real world data sets. Numerical results show that the efficiency of the proposed numerical methods and effectiveness of the new multi-view subspace learning models.

</p>
</details>

<details><summary><b>A Neural Question Answering System for Basic Questions about Subroutines</b>
<a href="https://arxiv.org/abs/2101.03999">arxiv:2101.03999</a>
&#x1F4C8; 1 <br>
<p>Aakash Bansal, Zachary Eberhart, Lingfei Wu, Collin McMillan</p></summary>
<p>

**Abstract:** A question answering (QA) system is a type of conversational AI that generates natural language answers to questions posed by human users. QA systems often form the backbone of interactive dialogue systems, and have been studied extensively for a wide variety of tasks ranging from restaurant recommendations to medical diagnostics. Dramatic progress has been made in recent years, especially from the use of encoder-decoder neural architectures trained with big data input. In this paper, we take initial steps to bringing state-of-the-art neural QA technologies to Software Engineering applications by designing a context-based QA system for basic questions about subroutines. We curate a training dataset of 10.9 million question/context/answer tuples based on rules we extract from recent empirical studies. Then, we train a custom neural QA model with this dataset and evaluate the model in a study with professional programmers. We demonstrate the strengths and weaknesses of the system, and lay the groundwork for its use in eventual dialogue systems for software engineering.

</p>
</details>

<details><summary><b>Load Embeddings for Scalable AC-OPF Learning</b>
<a href="https://arxiv.org/abs/2101.03973">arxiv:2101.03973</a>
&#x1F4C8; 1 <br>
<p>Terrence W. K. Mak, Ferdinando Fioretto, Pascal VanHentenryck</p></summary>
<p>

**Abstract:** AC Optimal Power Flow (AC-OPF) is a fundamental building block in power system optimization. It is often solved repeatedly, especially in regions with large penetration of renewable generation, to avoid violating operational limits. Recent work has shown that deep learning can be effective in providing highly accurate approximations of AC-OPF. However, deep learning approaches may suffer from scalability issues, especially when applied to large realistic grids. This paper addresses these scalability limitations and proposes a load embedding scheme using a 3-step approach. The first step formulates the load embedding problem as a bilevel optimization model that can be solved using a penalty method. The second step learns the encoding optimization to quickly produce load embeddings for new OPF instances. The third step is a deep learning model that uses load embeddings to produce accurate AC-OPF approximations. The approach is evaluated experimentally on large-scale test cases from the NESTA library. The results demonstrate that the proposed approach produces an order of magnitude improvements in training convergence and prediction accuracy.

</p>
</details>

<details><summary><b>First-Order Problem Solving through Neural MCTS based Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2101.04167">arxiv:2101.04167</a>
&#x1F4C8; 0 <br>
<p>Ruiyang Xu, Prashank Kadam, Karl Lieberherr</p></summary>
<p>

**Abstract:** The formal semantics of an interpreted first-order logic (FOL) statement can be given in Tarskian Semantics or a basically equivalent Game Semantics. The latter maps the statement and the interpretation into a two-player semantic game. Many combinatorial problems can be described using interpreted FOL statements and can be mapped into a semantic game. Therefore, learning to play a semantic game perfectly leads to the solution of a specific instance of a combinatorial problem. We adapt the AlphaZero algorithm so that it becomes better at learning to play semantic games that have different characteristics than Go and Chess. We propose a general framework, Persephone, to map the FOL description of a combinatorial problem to a semantic game so that it can be solved through a neural MCTS based reinforcement learning algorithm. Our goal for Persephone is to make it tabula-rasa, mapping a problem stated in interpreted FOL to a solution without human intervention.

</p>
</details>

<details><summary><b>Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning Model Optimization</b>
<a href="https://arxiv.org/abs/2101.04073">arxiv:2101.04073</a>
&#x1F4C8; 0 <br>
<p>Anush Sankaran, Olivier Mastropietro, Ehsan Saboori, Yasser Idris, Davis Sawyer, MohammadHossein AskariHemmat, Ghouthi Boukli Hacene</p></summary>
<p>

**Abstract:** Designing deep learning-based solutions is becoming a race for training deeper models with a greater number of layers. While a large-size deeper model could provide competitive accuracy, it creates a lot of logistical challenges and unreasonable resource requirements during development and deployment. This has been one of the key reasons for deep learning models not being excessively used in various production environments, especially in edge devices. There is an immediate requirement for optimizing and compressing these deep learning models, to enable on-device intelligence. In this research, we introduce a black-box framework, Deeplite Neutrino for production-ready optimization of deep learning models. The framework provides an easy mechanism for the end-users to provide constraints such as a tolerable drop in accuracy or target size of the optimized models, to guide the whole optimization process. The framework is easy to include in an existing production pipeline and is available as a Python Package, supporting PyTorch and Tensorflow libraries. The optimization performance of the framework is shown across multiple benchmark datasets and popular deep learning models. Further, the framework is currently used in production and the results and testimonials from several clients are summarized.

</p>
</details>

<details><summary><b>Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data</b>
<a href="https://arxiv.org/abs/2101.03735">arxiv:2101.03735</a>
&#x1F4C8; 0 <br>
<p>Bo Wang, Wei Xie, Tugce Martagan, Alp Akcay, Bram van Ravenstein</p></summary>
<p>

**Abstract:** In biopharmaceutical manufacturing, fermentation processes play a critical role on productivity and profit. A fermentation process uses living cells with complex biological mechanisms, and this leads to high variability in the process outputs. By building on the biological mechanisms of protein and impurity growth, we introduce a stochastic model to characterize the accumulation of the protein and impurity levels in the fermentation process. However, a common challenge in industry is the availability of only very limited amount of data especially in the development and early stage of production. This adds an additional layer of uncertainty, referred to as model risk, due to the difficulty of estimating the model parameters with limited data. In this paper, we study the harvesting decision for a fermentation process under model risk. In particular, we adopt a Bayesian approach to update the unknown parameters of the growth-rate distributions, and use the resulting posterior distributions to characterize the impact of model risk on fermentation output variability. The harvesting problem is formulated as a Markov decision process model with knowledge states that summarize the posterior distributions and hence incorporate the model risk in decision-making. The resulting model is solved by using a reinforcement learning algorithm based on Bayesian sparse sampling. We provide analytical results on the structure of the optimal policy and its objective function, and explicitly study the impact of model risk on harvesting decisions. Our case studies at MSD Animal Health demonstrate that the proposed model and solution approach improve the harvesting decisions in real life by achieving substantially higher average output from a fermentation batch along with lower batch-to-batch variability.

</p>
</details>


{% endraw %}
Prev: [2021.01.10]({{ '/2021/01/10/2021.01.10.html' | relative_url }})  Next: [2021.01.12]({{ '/2021/01/12/2021.01.12.html' | relative_url }})