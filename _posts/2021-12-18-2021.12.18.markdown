Prev: [2021.12.17]({{ '/2021/12/17/2021.12.17.html' | relative_url }})  Next: [2021.12.19]({{ '/2021/12/19/2021.12.19.html' | relative_url }})
{% raw %}
## Summary for 2021-12-18, created on 2021-12-28


<details><summary><b>Weisfeiler and Leman go Machine Learning: The Story so far</b>
<a href="https://arxiv.org/abs/2112.09992">arxiv:2112.09992</a>
&#x1F4C8; 79 <br>
<p>Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M. Kriege, Martin Grohe, Matthias Fey, Karsten Borgwardt</p></summary>
<p>

**Abstract:** In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph- and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.

</p>
</details>

<details><summary><b>Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks</b>
<a href="https://arxiv.org/abs/2112.10017">arxiv:2112.10017</a>
&#x1F4C8; 9 <br>
<p>Zixuan Ke, Bing Liu, Xingchang Huang</p></summary>
<p>

**Abstract:** Existing research on continual learning of a sequence of tasks focused on dealing with catastrophic forgetting, where the tasks are assumed to be dissimilar and have little shared knowledge. Some work has also been done to transfer previously learned knowledge to the new task when the tasks are similar and have shared knowledge. To the best of our knowledge, no technique has been proposed to learn a sequence of mixed similar and dissimilar tasks that can deal with forgetting and also transfer knowledge forward and backward. This paper proposes such a technique to learn both types of tasks in the same network. For dissimilar tasks, the algorithm focuses on dealing with forgetting, and for similar tasks, the algorithm focuses on selectively transferring the knowledge learned from some similar previous tasks to improve the new task learning. Additionally, the algorithm automatically detects whether a new task is similar to any previous tasks. Empirical evaluation using sequences of mixed tasks demonstrates the effectiveness of the proposed model.

</p>
</details>

<details><summary><b>Continual Learning with Knowledge Transfer for Sentiment Classification</b>
<a href="https://arxiv.org/abs/2112.10021">arxiv:2112.10021</a>
&#x1F4C8; 7 <br>
<p>Zixuan Ke, Bing Liu, Hao Wang, Lei Shu</p></summary>
<p>

**Abstract:** This paper studies continual learning (CL) for sentiment classification (SC). In this setting, the CL system learns a sequence of SC tasks incrementally in a neural network, where each task builds a classifier to classify the sentiment of reviews of a particular product category or domain. Two natural questions are: Can the system transfer the knowledge learned in the past from the previous tasks to the new task to help it learn a better model for the new task? And, can old models for previous tasks be improved in the process as well? This paper proposes a novel technique called KAN to achieve these objectives. KAN can markedly improve the SC accuracy of both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of KAN is demonstrated through extensive experiments.

</p>
</details>

<details><summary><b>Leveraging Transformers for Hate Speech Detection in Conversational Code-Mixed Tweets</b>
<a href="https://arxiv.org/abs/2112.09986">arxiv:2112.09986</a>
&#x1F4C8; 7 <br>
<p>Zaki Mustafa Farooqi, Sreyan Ghosh, Rajiv Ratn Shah</p></summary>
<p>

**Abstract:** In the current era of the internet, where social media platforms are easily accessible for everyone, people often have to deal with threats, identity attacks, hate, and bullying due to their association with a cast, creed, gender, religion, or even acceptance or rejection of a notion. Existing works in hate speech detection primarily focus on individual comment classification as a sequence labeling task and often fail to consider the context of the conversation. The context of a conversation often plays a substantial role when determining the author's intent and sentiment behind the tweet. This paper describes the system proposed by team MIDAS-IIITD for HASOC 2021 subtask 2, one of the first shared tasks focusing on detecting hate speech from Hindi-English code-mixed conversations on Twitter. We approach this problem using neural networks, leveraging the transformer's cross-lingual embeddings and further finetuning them for low-resource hate-speech classification in transliterated Hindi text. Our best performing system, a hard voting ensemble of Indic-BERT, XLM-RoBERTa, and Multilingual BERT, achieved a macro F1 score of 0.7253, placing us first on the overall leaderboard standings.

</p>
</details>

<details><summary><b>Improving Learning-to-Defer Algorithms Through Fine-Tuning</b>
<a href="https://arxiv.org/abs/2112.10768">arxiv:2112.10768</a>
&#x1F4C8; 6 <br>
<p>Naveen Raman, Michael Yee</p></summary>
<p>

**Abstract:** The ubiquity of AI leads to situations where humans and AI work together, creating the need for learning-to-defer algorithms that determine how to partition tasks between AI and humans. We work to improve learning-to-defer algorithms when paired with specific individuals by incorporating two fine-tuning algorithms and testing their efficacy using both synthetic and image datasets. We find that fine-tuning can pick up on simple human skill patterns, but struggles with nuance, and we suggest future work that uses robust semi-supervised to improve learning.

</p>
</details>

<details><summary><b>Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages</b>
<a href="https://arxiv.org/abs/2112.09866">arxiv:2112.09866</a>
&#x1F4C8; 6 <br>
<p>Hariom A. Pandya, Bhavik Ardeshna, Dr. Brijesh S. Bhatt</p></summary>
<p>

**Abstract:** Transformer based architectures have shown notable results on many down streaming tasks including question answering. The availability of data, on the other hand, impedes obtaining legitimate performance for low-resource languages. In this paper, we investigate the applicability of pre-trained multilingual models to improve the performance of question answering in low-resource languages. We tested four combinations of language and task adapters using multilingual transformer architectures on seven languages similar to MLQA dataset. Additionally, we have also proposed zero-shot transfer learning of low-resource question answering using language and task adapters. We observed that stacking the language and the task adapters improves the multilingual transformer models' performance significantly for low-resource languages.

</p>
</details>

<details><summary><b>Deeper Learning with CoLU Activation</b>
<a href="https://arxiv.org/abs/2112.12078">arxiv:2112.12078</a>
&#x1F4C8; 5 <br>
<p>Advait Vagerwal</p></summary>
<p>

**Abstract:** In neural networks, non-linearity is introduced by activation functions. One commonly used activation function is Rectified Linear Unit (ReLU). ReLU has been a popular choice as an activation but has flaws. State-of-the-art functions like Swish and Mish are now gaining attention as a better choice as they combat many flaws presented by other activation functions. CoLU is an activation function similar to Swish and Mish in properties. It is defined as f(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded above, bounded below, non-saturating, and non-monotonic. Based on experiments done with CoLU with different activation functions, it is observed that CoLU usually performs better than other functions on deeper neural networks. While training different neural networks on MNIST on an incrementally increasing number of convolutional layers, CoLU retained the highest accuracy for more layers. On a smaller network with 8 convolutional layers, CoLU had the highest mean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST, CoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU. On ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish, 0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is observed that activation functions may behave better than other activation functions based on different factors including the number of layers, types of layers, number of parameters, learning rate, optimizer, etc. Further research can be done on these factors and activation functions for more optimal activation functions and more knowledge on their behavior.

</p>
</details>

<details><summary><b>Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles</b>
<a href="https://arxiv.org/abs/2112.11217">arxiv:2112.11217</a>
&#x1F4C8; 5 <br>
<p>Xinglong Zhang, Yaoqian Peng, Biao Luo, Wei Pan, Xin Xu, Haibin Xie</p></summary>
<p>

**Abstract:** Recently, barrier function-based safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a model-based safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier-based control policy structure that can guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic learning algorithm is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Safety Gym environment. Furthermore, the approach is applied to the integrated path following and collision avoidance problem for two real-world intelligent vehicles. A differential-drive vehicle and an Ackermann-drive one are used to verify the offline deployment performance and the online learning performance, respectively. Our approach shows an impressive sim-to-real transfer capability and a satisfactory online control performance in the experiment.

</p>
</details>

<details><summary><b>A-ESRGAN: Training Real-World Blind Super-Resolution with Attention U-Net Discriminators</b>
<a href="https://arxiv.org/abs/2112.10046">arxiv:2112.10046</a>
&#x1F4C8; 5 <br>
<p>Zihao Wei, Yidong Huang, Yuang Chen, Chenhao Zheng, Jinnan Gao</p></summary>
<p>

**Abstract:** Blind image super-resolution(SR) is a long-standing task in CV that aims to restore low-resolution images suffering from unknown and complex distortions. Recent work has largely focused on adopting more complicated degradation models to emulate real-world degradations. The resulting models have made breakthroughs in perceptual loss and yield perceptually convincing results. However, the limitation brought by current generative adversarial network structures is still significant: treating pixels equally leads to the ignorance of the image's structural features, and results in performance drawbacks such as twisted lines and background over-sharpening or blurring. In this paper, we present A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net based, multi-scale discriminator that can be seamlessly integrated with other generators. To our knowledge, this is the first work to introduce attention U-Net structure as the discriminator of GAN to solve blind SR problems. And the paper also gives an interpretation for the mechanism behind multi-scale attention U-Net that brings performance breakthrough to the model. Through comparison experiments with prior works, our model presents state-of-the-art level performance on the non-reference natural image quality evaluator metric. And our ablation studies have shown that with our discriminator, the RRDB based generator can leverage the structural features of an image in multiple scales, and consequently yields more perceptually realistic high-resolution images compared to prior works.

</p>
</details>

<details><summary><b>Supervised laser-speckle image sampling of skin tissue to detect very early stage of diabetes by its effects on skin subcellular properties</b>
<a href="https://arxiv.org/abs/2112.10024">arxiv:2112.10024</a>
&#x1F4C8; 5 <br>
<p>Ahmet Orun, Luke Vella Critien, Jennifer Carter, Martin Stacey</p></summary>
<p>

**Abstract:** This paper investigates the effectiveness of an expert system based on K-nearest neighbors algorithm for laser speckle image sampling applied to the early detection of diabetes. With the latest developments in artificial intelligent guided laser speckle imaging technologies, it may be possible to optimise laser parameters, such as wavelength, energy level and image texture measures in association with a suitable AI technique to interact effectively with the subcellular properties of a skin tissue to detect early signs of diabetes. The new approach is potentially more effective than the classical skin glucose level observation because of its optimised combination of laser physics and AI techniques, and additionally, it allows non-expert individuals to perform more frequent skin tissue tests for an early detection of diabetes.

</p>
</details>

<details><summary><b>Cross-Domain Federated Learning in Medical Imaging</b>
<a href="https://arxiv.org/abs/2112.10001">arxiv:2112.10001</a>
&#x1F4C8; 5 <br>
<p>Vishwa S Parekh, Shuhao Lai, Vladimir Braverman, Jeff Leal, Steven Rowe, Jay J Pillai, Michael A Jacobs</p></summary>
<p>

**Abstract:** Federated learning is increasingly being explored in the field of medical imaging to train deep learning models on large scale datasets distributed across different data centers while preserving privacy by avoiding the need to transfer sensitive patient information. In this manuscript, we explore federated learning in a multi-domain, multi-task setting wherein different participating nodes may contain datasets sourced from different domains and are trained to solve different tasks. We evaluated cross-domain federated learning for the tasks of object detection and segmentation across two different experimental settings: multi-modal and multi-organ. The result from our experiments on cross-domain federated learning framework were very encouraging with an overlap similarity of 0.79 for organ localization and 0.65 for lesion segmentation. Our results demonstrate the potential of federated learning in developing multi-domain, multi-task deep learning models without sharing data from different domains.

</p>
</details>

<details><summary><b>3D Instance Segmentation of MVS Buildings</b>
<a href="https://arxiv.org/abs/2112.09902">arxiv:2112.09902</a>
&#x1F4C8; 5 <br>
<p>Yanghui Xu, Jiazhou Chen, Shufang Lu, Ronghua Liang, Liangliang Nan</p></summary>
<p>

**Abstract:** We present a novel framework for instance segmentation of 3D buildings from Multi-view Stereo (MVS) urban scenes. Unlike existing works focusing on semantic segmentation of an urban scene, the emphasis of this work lies in detecting and segmenting 3D building instances even if they are attached and embedded in a large and imprecise 3D surface model. Multi-view RGB images are first enhanced to RGBH images by adding a heightmap and are segmented to obtain all roof instances using a fine-tuned 2D instance segmentation neural network. Roof instance masks from different multi-view images are then clustered into global masks. Our mask clustering accounts for spatial occlusion and overlapping, which can eliminate segmentation ambiguities among multi-view images. Based on these global masks, 3D roof instances are segmented out by mask back-projections and extended to the entire building instances through a Markov random field (MRF) optimization. Quantitative evaluations and ablation studies have shown the effectiveness of all major steps of the method. A dataset for the evaluation of instance segmentation of 3D building models is provided as well. To the best of our knowledge, it is the first dataset for 3D urban buildings on the instance segmentation level.

</p>
</details>

<details><summary><b>Off-Policy Evaluation Using Information Borrowing and Context-Based Switching</b>
<a href="https://arxiv.org/abs/2112.09865">arxiv:2112.09865</a>
&#x1F4C8; 5 <br>
<p>Sutanoy Dasgupta, Yabo Niu, Kishan Panaganti, Dileep Kalathil, Debdeep Pati, Bani Mallick</p></summary>
<p>

**Abstract:** We consider the off-policy evaluation (OPE) problem in contextual bandits, where the goal is to estimate the value of a target policy using the data collected by a logging policy. Most popular approaches to the OPE are variants of the doubly robust (DR) estimator obtained by combining a direct method (DM) estimator and a correction term involving the inverse propensity score (IPS). Existing algorithms primarily focus on strategies to reduce the variance of the DR estimator arising from large IPS. We propose a new approach called the Doubly Robust with Information borrowing and Context-based switching (DR-IC) estimator that focuses on reducing both bias and variance. The DR-IC estimator replaces the standard DM estimator with a parametric reward model that borrows information from the 'closer' contexts through a correlation structure that depends on the IPS. The DR-IC estimator also adaptively interpolates between this modified DM estimator and a modified DR estimator based on a context-specific switching rule. We give provable guarantees on the performance of the DR-IC estimator. We also demonstrate the superior performance of the DR-IC estimator compared to the state-of-the-art OPE algorithms on a number of benchmark problems.

</p>
</details>

<details><summary><b>Controlling the Quality of Distillation in Response-Based Network Compression</b>
<a href="https://arxiv.org/abs/2112.10047">arxiv:2112.10047</a>
&#x1F4C8; 4 <br>
<p>Vibhas Vats, David Crandall</p></summary>
<p>

**Abstract:** The performance of a distillation-based compressed network is governed by the quality of distillation. The reason for the suboptimal distillation of a large network (teacher) to a smaller network (student) is largely attributed to the gap in the learning capacities of given teacher-student pair. While it is hard to distill all the knowledge of a teacher, the quality of distillation can be controlled to a large extent to achieve better performance. Our experiments show that the quality of distillation is largely governed by the quality of teacher's response, which in turn is heavily affected by the presence of similarity information in its response. A well-trained large capacity teacher loses similarity information between classes in the process of learning fine-grained discriminative properties for classification. The absence of similarity information causes the distillation process to be reduced from one example-many class learning to one example-one class learning, thereby throttling the flow of diverse knowledge from the teacher. With the implicit assumption that only the instilled knowledge can be distilled, instead of focusing only on the knowledge distilling process, we scrutinize the knowledge inculcation process. We argue that for a given teacher-student pair, the quality of distillation can be improved by finding the sweet spot between batch size and number of epochs while training the teacher. We discuss the steps to find this sweet spot for better distillation. We also propose the distillation hypothesis to differentiate the behavior of the distillation process between knowledge distillation and regularization effect. We conduct all our experiments on three different datasets.

</p>
</details>

<details><summary><b>3D Structural Analysis of the Optic Nerve Head to Robustly Discriminate Between Papilledema and Optic Disc Drusen</b>
<a href="https://arxiv.org/abs/2112.09970">arxiv:2112.09970</a>
&#x1F4C8; 4 <br>
<p>Michaël J. A. Girard, Satish K. Panda, Tin Aung Tun, Elisabeth A. Wibroe, Raymond P. Najjar, Aung Tin, Alexandre H. Thiéry, Steffen Hamann, Clare Fraser, Dan Milea</p></summary>
<p>

**Abstract:** Purpose: (1) To develop a deep learning algorithm to identify major tissue structures of the optic nerve head (ONH) in 3D optical coherence tomography (OCT) scans; (2) to exploit such information to robustly differentiate among healthy, optic disc drusen (ODD), and papilledema ONHs.
  It was a cross-sectional comparative study with confirmed ODD (105 eyes), papilledema due to high intracranial pressure (51 eyes), and healthy controls (100 eyes). 3D scans of the ONHs were acquired using OCT, then processed to improve deep-tissue visibility. At first, a deep learning algorithm was developed using 984 B-scans (from 130 eyes) in order to identify: major neural/connective tissues, and ODD regions. The performance of our algorithm was assessed using the Dice coefficient (DC). In a 2nd step, a classification algorithm (random forest) was designed using 150 OCT volumes to perform 3-class classifications (1: ODD, 2: papilledema, 3: healthy) strictly from their drusen and prelamina swelling scores (derived from the segmentations). To assess performance, we reported the area under the receiver operating characteristic curves (AUCs) for each class.
  Our segmentation algorithm was able to isolate neural and connective tissues, and ODD regions whenever present. This was confirmed by an average DC of 0.93$\pm$0.03 on the test set, corresponding to good performance. Classification was achieved with high AUCs, i.e. 0.99$\pm$0.01 for the detection of ODD, 0.99 $\pm$ 0.01 for the detection of papilledema, and 0.98$\pm$0.02 for the detection of healthy ONHs.
  Our AI approach accurately discriminated ODD from papilledema, using a single OCT scan. Our classification performance was excellent, with the caveat that validation in a much larger population is warranted. Our approach may have the potential to establish OCT as the mainstay of diagnostic imaging in neuro-ophthalmology.

</p>
</details>

<details><summary><b>Rapid Face Mask Detection and Person Identification Model based on Deep Neural Networks</b>
<a href="https://arxiv.org/abs/2112.09951">arxiv:2112.09951</a>
&#x1F4C8; 4 <br>
<p>Abdullah Ahmad Khan, Mohd. Belal,  GhufranUllah</p></summary>
<p>

**Abstract:** As Covid-19 has been constantly getting mutated and in three or four months a new variant gets introduced to us and it comes with more deadly problems. The things that prevent us from getting Covid is getting vaccinated and wearing a face mask. In this paper, we have implemented a new Face Mask Detection and Person Recognition model named Insight face which is based on SoftMax loss classification algorithm Arc Face loss and names it as RFMPI-DNN(Rapid Face Detection and Peron Identification Model based on Deep Neural Networks) to detect face mask and person identity rapidly as compared to other models available. To compare our new model, we have used previous MobileNet_V2 model and face recognition module for effective comparison on the basis of time. The proposed model implemented in the system has outperformed the model compared in this paper in every aspect

</p>
</details>

<details><summary><b>Syntactic-GCN Bert based Chinese Event Extraction</b>
<a href="https://arxiv.org/abs/2112.09939">arxiv:2112.09939</a>
&#x1F4C8; 4 <br>
<p>Jiangwei Liu, Jingshu Zhang, Xiaohong Huang, Liangyu Min</p></summary>
<p>

**Abstract:** With the rapid development of information technology, online platforms (e.g., news portals and social media) generate enormous web information every moment. Therefore, it is crucial to extract structured representations of events from social streams. Generally, existing event extraction research utilizes pattern matching, machine learning, or deep learning methods to perform event extraction tasks. However, the performance of Chinese event extraction is not as good as English due to the unique characteristics of the Chinese language. In this paper, we propose an integrated framework to perform Chinese event extraction. The proposed approach is a multiple channel input neural framework that integrates semantic features and syntactic features. The semantic features are captured by BERT architecture. The Part of Speech (POS) features and Dependency Parsing (DP) features are captured by profiling embeddings and Graph Convolutional Network (GCN), respectively. We also evaluate our model on a real-world dataset. Experimental results show that the proposed method outperforms the benchmark approaches significantly.

</p>
</details>

<details><summary><b>Does Explainable Machine Learning Uncover the Black Box in Vision Applications?</b>
<a href="https://arxiv.org/abs/2112.09898">arxiv:2112.09898</a>
&#x1F4C8; 4 <br>
<p>Manish Narwaria</p></summary>
<p>

**Abstract:** Machine learning (ML) in general and deep learning (DL) in particular has become an extremely popular tool in several vision applications (like object detection, super resolution, segmentation, object tracking etc.). Almost in parallel, the issue of explainability in ML (i.e. the ability to explain/elaborate the way a trained ML model arrived at its decision) in vision has also received fairly significant attention from various quarters. However, we argue that the current philosophy behind explainable ML suffers from certain limitations, and the resulting explanations may not meaningfully uncover black box ML models. To elaborate our assertion, we first raise a few fundamental questions which have not been adequately discussed in the corresponding literature. We also provide perspectives on how explainablity in ML can benefit by relying on more rigorous principles in the related areas.

</p>
</details>

<details><summary><b>Morpheme Boundary Detection & Grammatical Feature Prediction for Gujarati : Dataset & Model</b>
<a href="https://arxiv.org/abs/2112.09860">arxiv:2112.09860</a>
&#x1F4C8; 4 <br>
<p>Jatayu Baxi, Dr. Brijesh Bhatt</p></summary>
<p>

**Abstract:** Developing Natural Language Processing resources for a low resource language is a challenging but essential task. In this paper, we present a Morphological Analyzer for Gujarati. We have used a Bi-Directional LSTM based approach to perform morpheme boundary detection and grammatical feature tagging. We have created a data set of Gujarati words with lemma and grammatical features. The Bi-LSTM based model of Morph Analyzer discussed in the paper handles the language morphology effectively without the knowledge of any hand-crafted suffix rules. To the best of our knowledge, this is the first dataset and morph analyzer model for the Gujarati language which performs both grammatical feature tagging and morpheme boundary detection tasks.

</p>
</details>

<details><summary><b>Time-Aware Neighbor Sampling for Temporal Graph Networks</b>
<a href="https://arxiv.org/abs/2112.09845">arxiv:2112.09845</a>
&#x1F4C8; 4 <br>
<p>Yiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, Bryan Hooi</p></summary>
<p>

**Abstract:** We present a new neighbor sampling method on temporal graphs. In a temporal graph, predicting different nodes' time-varying properties can require the receptive neighborhood of various temporal scales. In this work, we propose the TNS (Time-aware Neighbor Sampling) method: TNS learns from temporal information to provide an adaptive receptive neighborhood for every node at any time. Learning how to sample neighbors is non-trivial, since the neighbor indices in time order are discrete and not differentiable. To address this challenge, we transform neighbor indices from discrete values to continuous ones by interpolating the neighbors' messages. TNS can be flexibly incorporated into popular temporal graph networks to improve their effectiveness without increasing their time complexity. TNS can be trained in an end-to-end manner. It needs no extra supervision and is automatically and implicitly guided to sample the neighbors that are most beneficial for prediction. Empirical results on multiple standard datasets show that TNS yields significant gains on edge prediction and node classification.

</p>
</details>

<details><summary><b>Enhanced Object Detection in Floor-plan through Super Resolution</b>
<a href="https://arxiv.org/abs/2112.09844">arxiv:2112.09844</a>
&#x1F4C8; 4 <br>
<p>Dev Khare, N S Kamal, Barathi Ganesh HB, V Sowmya, V V Sajith Variyar</p></summary>
<p>

**Abstract:** Building Information Modelling (BIM) software use scalable vector formats to enable flexible designing of floor plans in the industry. Floor plans in the architectural domain can come from many sources that may or may not be in scalable vector format. The conversion of floor plan images to fully annotated vector images is a process that can now be realized by computer vision. Novel datasets in this field have been used to train Convolutional Neural Network (CNN) architectures for object detection. Image enhancement through Super-Resolution (SR) is also an established CNN based network in computer vision that is used for converting low resolution images to high resolution ones. This work focuses on creating a multi-component module that stacks a SR model on a floor plan object detection model. The proposed stacked model shows greater performance than the corresponding vanilla object detection model. For the best case, the the inclusion of SR showed an improvement of 39.47% in object detection over the vanilla network. Data and code are made publicly available at https://github.com/rbg-research/Floor-Plan-Detection.

</p>
</details>

<details><summary><b>Low-resource Learning with Knowledge Graphs: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2112.10006">arxiv:2112.10006</a>
&#x1F4C8; 3 <br>
<p>Jiaoyan Chen, Yuxia Geng, Zhuo Chen, Jeff Z. Pan, Yuan He, Wen Zhang, Ian Horrocks, Huajun Chen</p></summary>
<p>

**Abstract:** Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with no enough resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over $90$ papers about KG-aware research for two major low-resource learning settings -- zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We first introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation-based and the optimization-based. We next presented different applications, including not only KG augmented tasks in Computer Vision and Natural Language Processing (e.g., image classification, text classification and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs.

</p>
</details>

<details><summary><b>Multiple Time Series Fusion Based on LSTM An Application to CAP A Phase Classification Using EEG</b>
<a href="https://arxiv.org/abs/2112.11218">arxiv:2112.11218</a>
&#x1F4C8; 2 <br>
<p>Fábio Mendonça, Sheikh Shanawaz Mostafa, Diogo Freitas, Fernando Morgado-Dias, Antonio G. Ravelo-García</p></summary>
<p>

**Abstract:** Biomedical decision making involves multiple signal processing, either from different sensors or from different channels. In both cases, information fusion plays a significant role. A deep learning based electroencephalogram channels' feature level fusion is carried out in this work for the electroencephalogram cyclic alternating pattern A phase classification. Channel selection, fusion, and classification procedures were optimized by two optimization algorithms, namely, Genetic Algorithm and Particle Swarm Optimization. The developed methodologies were evaluated by fusing the information from multiple electroencephalogram channels for patients with nocturnal frontal lobe epilepsy and patients without any neurological disorder, which was significantly more challenging when compared to other state of the art works. Results showed that both optimization algorithms selected a comparable structure with similar feature level fusion, consisting of three electroencephalogram channels, which is in line with the CAP protocol to ensure multiple channels' arousals for CAP detection. Moreover, the two optimized models reached an area under the receiver operating characteristic curve of 0.82, with average accuracy ranging from 77% to 79%, a result which is in the upper range of the specialist agreement. The proposed approach is still in the upper range of the best state of the art works despite a difficult dataset, and has the advantage of providing a fully automatic analysis without requiring any manual procedure. Ultimately, the models revealed to be noise resistant and resilient to multiple channel loss.

</p>
</details>

<details><summary><b>Learning-based methods to model small body gravity fields for proximity operations: Safety and Robustness</b>
<a href="https://arxiv.org/abs/2112.09998">arxiv:2112.09998</a>
&#x1F4C8; 2 <br>
<p>Daniel Neamati, Yashwanth Kumar Nakka, Soon-Jo Chung</p></summary>
<p>

**Abstract:** Accurate gravity field models are essential for safe proximity operations around small bodies. State-of-the-art techniques use spherical harmonics or high-fidelity polyhedron shape models. Unfortunately, these techniques can become inaccurate near the surface of the small body or have high computational costs, especially for binary or heterogeneous small bodies. New learning-based techniques do not encode a predefined structure and are more versatile. In exchange for versatility, learning-based techniques can be less robust outside the training data domain. In deployment, the spacecraft trajectory is the primary source of dynamics data. Therefore, the training data domain should include spacecraft trajectories to accurately evaluate the learned model's safety and robustness. We have developed a novel method for learning-based gravity models that directly uses the spacecraft's past trajectories. We further introduce a method to evaluate the safety and robustness of learning-based techniques via comparing accuracy within and outside of the training domain. We demonstrate this safety and robustness method for two learning-based frameworks: Gaussian processes and neural networks. Along with the detailed analysis provided, we empirically establish the need for robustness verification of learned gravity models when used for proximity operations.

</p>
</details>

<details><summary><b>Exploiting Expert-guided Symmetry Detection in Markov Decision Processes</b>
<a href="https://arxiv.org/abs/2112.09943">arxiv:2112.09943</a>
&#x1F4C8; 2 <br>
<p>Giorgio Angelotti, Nicolas Drougard, Caroline P. C. Chanel</p></summary>
<p>

**Abstract:** Offline estimation of the dynamical model of a Markov Decision Process (MDP) is a non-trivial task that greatly depends on the data available to the learning phase. Sometimes the dynamics of the model is invariant with respect to some transformations of the current state and action. Recent works showed that an expert-guided pipeline relying on Density Estimation methods as Deep Neural Network based Normalizing Flows effectively detects this structure in deterministic environments, both categorical and continuous-valued. The acquired knowledge can be exploited to augment the original data set, leading eventually to a reduction in the distributional shift between the true and the learnt model. In this work we extend the paradigm to also tackle non deterministic MDPs, in particular 1) we propose a detection threshold in categorical environments based on statistical distances, 2) we introduce a benchmark of the distributional shift in continuous environments based on the Wilcoxon signed-rank statistical test and 3) we show that the former results lead to a performance improvement when solving the learnt MDP and then applying the optimal policy in the real environment.

</p>
</details>

<details><summary><b>DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning</b>
<a href="https://arxiv.org/abs/2112.09933">arxiv:2112.09933</a>
&#x1F4C8; 2 <br>
<p>Yuliang Wei, Haotian Li, Yao Wang, Guodong Xin, Hongri Liu</p></summary>
<p>

**Abstract:** Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, due to the nature of rapid iteration as well as incompleteness of data, KGs are usually huge and there are inevitably missing facts in KGs. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can capture latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, previous studies rarely pay attention to heterogeneous KGs. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for inferring on KGs. Specifically, we study the problem of predicting missing links in heterogeneous KGs that involve entities and relations of various types from the perspective of the degrees of nodes. Experimentally, we demonstrate that our DegreEmbed model outperforms the state-of-the-art methods on real world datasets. Meanwhile, the rules mined by our model are of high quality and interpretability.

</p>
</details>

<details><summary><b>Improving Subgraph Recognition with Variational Graph Information Bottleneck</b>
<a href="https://arxiv.org/abs/2112.09899">arxiv:2112.09899</a>
&#x1F4C8; 2 <br>
<p>Junchi Yu, Jie Cao, Ran He</p></summary>
<p>

**Abstract:** Subgraph recognition aims at discovering a compressed substructure of a graph that is most informative to the graph property. It can be formulated by optimizing Graph Information Bottleneck (GIB) with a mutual information estimator. However, GIB suffers from training instability since the mutual information of graph data is intrinsically difficult to estimate. This paper introduces a noise injection method to compress the information in the subgraphs, which leads to a novel Variational Graph Information Bottleneck (VGIB) framework. VGIB allows a tractable variational approximation to its objective under mild assumptions. Therefore, VGIB enjoys more stable and efficient training process - we find that VGIB converges 10 times faster than GIB with improved performances in practice. Extensive experiments on graph interpretation, explainability of Graph Neural Networks, and graph classification show that VGIB finds better subgraphs than existing methods.

</p>
</details>

<details><summary><b>Towards the Explanation of Graph Neural Networks in Digital Pathology with Information Flows</b>
<a href="https://arxiv.org/abs/2112.09895">arxiv:2112.09895</a>
&#x1F4C8; 2 <br>
<p>Junchi Yu, Tingyang Xu, Ran He</p></summary>
<p>

**Abstract:** As Graph Neural Networks (GNNs) are widely adopted in digital pathology, there is increasing attention to developing explanation models (explainers) of GNNs for improved transparency in clinical decisions.
  Existing explainers discover an explanatory subgraph relevant to the prediction.
  However, such a subgraph is insufficient to reveal all the critical biological substructures for the prediction because the prediction will remain unchanged after removing that subgraph.
  Hence, an explanatory subgraph should be not only necessary for prediction, but also sufficient to uncover the most predictive regions for the explanation.
  Such explanation requires a measurement of information transferred from different input subgraphs to the predictive output, which we define as information flow.
  In this work, we address these key challenges and propose IFEXPLAINER, which generates a necessary and sufficient explanation for GNNs.
  To evaluate the information flow within GNN's prediction, we first propose a novel notion of predictiveness, named $f$-information, which is directional and incorporates the realistic capacity of the GNN model.
  Based on it, IFEXPLAINER generates the explanatory subgraph with maximal information flow to the prediction.
  Meanwhile, it minimizes the information flow from the input to the predictive result after removing the explanation.
  Thus, the produced explanation is necessarily important to the prediction and sufficient to reveal the most crucial substructures.
  We evaluate IFEXPLAINER to interpret GNN's predictions on breast cancer subtyping.
  Experimental results on the BRACS dataset show the superior performance of the proposed method.

</p>
</details>

<details><summary><b>Deep Filtering with DNN, CNN and RNN</b>
<a href="https://arxiv.org/abs/2112.12616">arxiv:2112.12616</a>
&#x1F4C8; 1 <br>
<p>Bin Xie, Qing Zhang</p></summary>
<p>

**Abstract:** This paper is about a deep learning approach for linear and nonlinear filtering. The idea is to train a neural network with Monte Carlo samples generated from a nominal dynamic model. Then the network weights are applied to Monte Carlo samples from an actual dynamic model. A main focus of this paper is on the deep filters with three major neural network architectures (DNN, CNN, RNN). Our deep filter compares favorably to the traditional Kalman filter in linear cases and outperform the extended Kalman filter in nonlinear cases. Then a switching model with jumps is studied to show the adaptiveness and power of our deep filtering. Among the three major NNs, the CNN outperform the others on average. while the RNN does not seem to be suitable for the filtering problem. One advantage of the deep filter is its robustness when the nominal model and actual model differ. The other advantage of deep filtering is real data can be used directly to train the deep neutral network. Therefore, model calibration can be by-passed all together.

</p>
</details>

<details><summary><b>List Autoencoder: Towards Deep Learning Based Reliable Transmission Over Noisy Channels</b>
<a href="https://arxiv.org/abs/2112.11920">arxiv:2112.11920</a>
&#x1F4C8; 1 <br>
<p>Hamid Saber, Homayoon Hatami, Jung Hyun Bae</p></summary>
<p>

**Abstract:** There has been a growing interest in automating the design of channel encoders and decoders in an auto-encoder(AE) framework in recent years for reliable transmission of data over noisy channels. In this paper we present a new framework for designing AEs for this purpose. In particular, we present an AE framework, namely listAE, in which the decoder network outputs a list of decoded message word candidates. A genie is assumed to be available at the output of the decoder and specific loss functions are proposed to optimize the performance of the genie-aided (GA)-listAE. The listAE is a general AE framework and can be used with any network architecture. We propose a specific end-to-end network architecture which decodes the received word on a sequence of component codes with decreasing rates. The listAE based on the proposed architecture, referred to as incremental redundancy listAE (IR-listAE), improves the state-of-the-art AE performance by 1 dB at low block error rates under GA decoding. We then employ cyclic redundancy check (CRC) codes to replace the genie at the decoder, giving CRC-aided (CA)-listAE with negligible performance loss compared to the GA-listAE. The CA-listAE shows meaningful coding gain at the price of a slight decrease in the rate due to appending CRC to the message word.

</p>
</details>

<details><summary><b>Jamming Pattern Recognition over Multi-Channel Networks: A Deep Learning Approach</b>
<a href="https://arxiv.org/abs/2112.11222">arxiv:2112.11222</a>
&#x1F4C8; 1 <br>
<p>Ali Pourranjbar, Georges Kaddoum, Walid Saad</p></summary>
<p>

**Abstract:** With the advent of intelligent jammers, jamming attacks have become a more severe threat to the performance of wireless systems. An intelligent jammer is able to change its policy to minimize the probability of being traced by legitimate nodes. Thus, an anti-jamming mechanism capable of constantly adjusting to the jamming policy is required to combat such a jammer. Remarkably, existing anti-jamming methods are not applicable here because they mainly focus on mitigating jamming attacks with an invariant jamming policy, and they rarely consider an intelligent jammer as an adversary. Therefore, in this paper, to employ a jamming type recognition technique working alongside an anti-jamming technique is proposed. The proposed recognition method employs a recurrent neural network that takes the jammer's occupied channels as inputs and outputs the jammer type. Under this scheme, the real-time jammer policy is first identified, and, then, the most appropriate countermeasure is chosen. Consequently, any changes to the jammer policy can be instantly detected with the proposed recognition technique allowing for a rapid switch to a new anti-jamming method fitted to the new jamming policy. To evaluate the performance of the proposed recognition method, the accuracy of the detection is derived as a function of the jammer policy switching time. Simulation results show the detection accuracy for all the considered users numbers is greater than 70% when the jammer switches its policy every 5 time slots and the accuracy raises to 90% when the jammer policy switching time is 45.

</p>
</details>

<details><summary><b>Curriculum Based Reinforcement Learning of Grid Topology Controllers to Prevent Thermal Cascading</b>
<a href="https://arxiv.org/abs/2112.09996">arxiv:2112.09996</a>
&#x1F4C8; 1 <br>
<p>Amarsagar Reddy Ramapuram Matavalam, Kishan Prudhvi Guddanti, Yang Weng, Venkataramana Ajjarapu</p></summary>
<p>

**Abstract:** This paper describes how domain knowledge of power system operators can be integrated into reinforcement learning (RL) frameworks to effectively learn agents that control the grid's topology to prevent thermal cascading. Typical RL-based topology controllers fail to perform well due to the large search/optimization space. Here, we propose an actor-critic-based agent to address the problem's combinatorial nature and train the agent using the RL environment developed by RTE, the French TSO. To address the challenge of the large optimization space, a curriculum-based approach with reward tuning is incorporated into the training procedure by modifying the environment using network physics for enhanced agent learning. Further, a parallel training approach on multiple scenarios is employed to avoid biasing the agent to a few scenarios and make it robust to the natural variability in grid operations. Without these modifications to the training procedure, the RL agent failed for most test scenarios, illustrating the importance of properly integrating domain knowledge of physical systems for real-world RL learning. The agent was tested by RTE for the 2019 learning to run the power network challenge and was awarded the 2nd place in accuracy and 1st place in speed. The developed code is open-sourced for public use.

</p>
</details>

<details><summary><b>Data-Driven Reachability analysis and Support set Estimation with Christoffel Functions</b>
<a href="https://arxiv.org/abs/2112.09995">arxiv:2112.09995</a>
&#x1F4C8; 1 <br>
<p>Alex Devonport, Forest Yang, Laurent El Ghaoui, Murat Arcak</p></summary>
<p>

**Abstract:** We present algorithms for estimating the forward reachable set of a dynamical system using only a finite collection of independent and identically distributed samples. The produced estimate is the sublevel set of a function called an empirical inverse Christoffel function: empirical inverse Christoffel functions are known to provide good approximations to the support of probability distributions. In addition to reachability analysis, the same approach can be applied to general problems of estimating the support of a random variable, which has applications in data science towards detection of novelties and outliers in data sets. In applications where safety is a concern, having a guarantee of accuracy that holds on finite data sets is critical. In this paper, we prove such bounds for our algorithms under the Probably Approximately Correct (PAC) framework. In addition to applying classical Vapnik-Chervonenkis (VC) dimension bound arguments, we apply the PAC-Bayes theorem by leveraging a formal connection between kernelized empirical inverse Christoffel functions and Gaussian process regression models. The bound based on PAC-Bayes applies to a more general class of Christoffel functions than the VC dimension argument, and achieves greater sample efficiency in experiments.

</p>
</details>

<details><summary><b>The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large Web Corpus</b>
<a href="https://arxiv.org/abs/2112.09924">arxiv:2112.09924</a>
&#x1F4C8; 1 <br>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-tau Yih, Sebastian Riedel</p></summary>
<p>

**Abstract:** In order to address the increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web scale knowledge, lack of structure, inconsistent quality, and noise. To this end, we propose a new setup for evaluating existing KI-NLP tasks in which we generalize the background corpus to a universal web snapshot. We repurpose KILT, a standard KI-NLP benchmark initially developed for Wikipedia, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the Internet. We find that despite potential gaps of coverage, challenges of scale, lack of structure and lower quality, retrieval from Sphere enables a state-of-the-art retrieve-and-read system to match and even outperform Wikipedia-based models on several KILT tasks - even if we aggressively filter content that looks like Wikipedia. We also observe that while a single dense passage index over Wikipedia can outperform a sparse BM25 version, on Sphere this is not yet possible. To facilitate further research into this area, and minimise the community's reliance on proprietary black box search engines, we will share our indices, evaluation metrics and infrastructure.

</p>
</details>

<details><summary><b>Learning to Model the Relationship Between Brain Structural and Functional Connectomes</b>
<a href="https://arxiv.org/abs/2112.09906">arxiv:2112.09906</a>
&#x1F4C8; 1 <br>
<p>Yang Li, Gonzalo Mateos, Zhengwu Zhang</p></summary>
<p>

**Abstract:** Recent advances in neuroimaging along with algorithmic innovations in statistical learning from network data offer a unique pathway to integrate brain structure and function, and thus facilitate revealing some of the brain's organizing principles at the system level. In this direction, we develop a supervised graph representation learning framework to model the relationship between brain structural connectivity (SC) and functional connectivity (FC) via a graph encoder-decoder system, where the SC is used as input to predict empirical FC. A trainable graph convolutional encoder captures direct and indirect interactions between brain regions-of-interest that mimic actual neural communications, as well as to integrate information from both the structural network topology and nodal (i.e., region-specific) attributes. The encoder learns node-level SC embeddings which are combined to generate (whole brain) graph-level representations for reconstructing empirical FC networks. The proposed end-to-end model utilizes a multi-objective loss function to jointly reconstruct FC networks and learn discriminative graph representations of the SC-to-FC mapping for downstream subject (i.e., graph-level) classification. Comprehensive experiments demonstrate that the learnt representations of said relationship capture valuable information from the intrinsic properties of the subject's brain networks and lead to improved accuracy in classifying a large population of heavy drinkers and non-drinkers from the Human Connectome Project. Our work offers new insights on the relationship between brain networks that support the promising prospect of using graph representation learning to discover more about human brain activity and function.

</p>
</details>

<details><summary><b>Deep Learning for Stability Analysis of a Freely Vibrating Sphere at Moderate Reynolds Number</b>
<a href="https://arxiv.org/abs/2112.09858">arxiv:2112.09858</a>
&#x1F4C8; 1 <br>
<p>A. Chizfahm, R. Jaiman</p></summary>
<p>

**Abstract:** In this paper, we present a deep learning-based reduced-order model (DL-ROM) for the stability prediction of unsteady 3D fluid-structure interaction systems. The proposed DL-ROM has the format of a nonlinear state-space model and employs a recurrent neural network with long short-term memory (LSTM). We consider a canonical fluid-structure system of an elastically-mounted sphere coupled with incompressible fluid flow in a state-space format. We develop a nonlinear data-driven coupling for predicting unsteady forces and vortex-induced vibration (VIV) lock-in of the freely vibrating sphere in a transverse direction. We design an input-output relationship as a temporal sequence of force and displacement datasets for a low-dimensional approximation of the fluid-structure system. Based on the prior knowledge of the VIV lock-in process, the input function contains a range of frequencies and amplitudes, which enables an efficient DL-ROM without the need for a massive training dataset for the low-dimensional modeling. Once trained, the network provides a nonlinear mapping of input-output dynamics that can predict the coupled fluid-structure dynamics for a longer horizon via the feedback process. By integrating the LSTM network with the eigensystem realization algorithm (ERA), we construct a data-driven state-space model for the reduced-order stability analysis. We investigate the underlying mechanism and stability characteristics of VIV via an eigenvalue selection process. To understand the frequency lock-in mechanism, we study the eigenvalue trajectories for a range of the reduced oscillation frequencies and the mass ratios. Consistent with the full-order simulations, the frequency lock-in branches are accurately captured by the combined LSTM-ERA procedure. The proposed DL-ROM aligns with the development of physics-based digital twin of engineering systems involving fluid-structure interactions.

</p>
</details>


{% endraw %}
Prev: [2021.12.17]({{ '/2021/12/17/2021.12.17.html' | relative_url }})  Next: [2021.12.19]({{ '/2021/12/19/2021.12.19.html' | relative_url }})