## Summary for 2021-05-02, created on 2021-12-21


<details><summary><b>Learning Visually Guided Latent Actions for Assistive Teleoperation</b>
<a href="https://arxiv.org/abs/2105.00580">arxiv:2105.00580</a>
&#x1F4C8; 9 <br>
<p>Siddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, Dorsa Sadigh</p></summary>
<p>

**Abstract:** It is challenging for humans -- particularly those living with physical disabilities -- to control high-dimensional, dexterous robots. Prior work explores learning embedding functions that map a human's low-dimensional inputs (e.g., via a joystick) to complex, high-dimensional robot actions for assistive teleoperation; however, a central problem is that there are many more high-dimensional actions than available low-dimensional inputs. To extract the correct action and maximally assist their human controller, robots must reason over their context: for example, pressing a joystick down when interacting with a coffee cup indicates a different action than when interacting with knife. In this work, we develop assistive robots that condition their latent embeddings on visual inputs. We explore a spectrum of visual encoders and show that incorporating object detectors pretrained on small amounts of cheap, easy-to-collect structured data enables i) accurately and robustly recognizing the current context and ii) generalizing control embeddings to new objects and tasks. In user studies with a high-dimensional physical robot arm, participants leverage this approach to perform new tasks with unseen objects. Our results indicate that structured visual representations improve few-shot performance and are subjectively preferred by users.

</p>
</details>

<details><summary><b>On Feature Decorrelation in Self-Supervised Learning</b>
<a href="https://arxiv.org/abs/2105.00470">arxiv:2105.00470</a>
&#x1F4C8; 8 <br>
<p>Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, Hang Zhao</p></summary>
<p>

**Abstract:** In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.

</p>
</details>

<details><summary><b>Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer</b>
<a href="https://arxiv.org/abs/2105.00522">arxiv:2105.00522</a>
&#x1F4C8; 7 <br>
<p>Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S. Yu</p></summary>
<p>

**Abstract:** Sequential Recommendation characterizes the evolving patterns by modeling item sequences chronologically. The essential target of it is to capture the item transition correlations. The recent developments of transformer inspire the community to design effective sequence encoders, \textit{e.g.,} SASRec and BERT4Rec. However, we observe that these transformer-based models suffer from the cold-start issue, \textit{i.e.,} performing poorly for short sequences. Therefore, we propose to augment short sequences while still preserving original sequential correlations. We introduce a new framework for \textbf{A}ugmenting \textbf{S}equential \textbf{Re}commendation with \textbf{P}seudo-prior items~(ASReP). We firstly pre-train a transformer with sequences in a reverse direction to predict prior items. Then, we use this transformer to generate fabricated historical items at the beginning of short sequences. Finally, we fine-tune the transformer using these augmented sequences from the time order to predict the next item. Experiments on two real-world datasets verify the effectiveness of ASReP. The code is available on \url{https://github.com/DyGRec/ASReP}.

</p>
</details>

<details><summary><b>TE-ESN: Time Encoding Echo State Network for Prediction Based on Irregularly Sampled Time Series Data</b>
<a href="https://arxiv.org/abs/2105.00412">arxiv:2105.00412</a>
&#x1F4C8; 6 <br>
<p>Chenxi Sun, Shenda Hong, Moxian Song, Yanxiu Zhou, Yongyue Sun, Derun Cai, Hongyan Li</p></summary>
<p>

**Abstract:** Prediction based on Irregularly Sampled Time Series (ISTS) is of wide concern in the real-world applications. For more accurate prediction, the methods had better grasp more data characteristics. Different from ordinary time series, ISTS is characterised with irregular time intervals of intra-series and different sampling rates of inter-series. However, existing methods have suboptimal predictions due to artificially introducing new dependencies in a time series and biasedly learning relations among time series when modeling these two characteristics. In this work, we propose a novel Time Encoding (TE) mechanism. TE can embed the time information as time vectors in the complex domain. It has the the properties of absolute distance and relative distance under different sampling rates, which helps to represent both two irregularities of ISTS. Meanwhile, we create a new model structure named Time Encoding Echo State Network (TE-ESN). It is the first ESNs-based model that can process ISTS data. Besides, TE-ESN can incorporate long short-term memories and series fusion to grasp horizontal and vertical relations. Experiments on one chaos system and three real-world datasets show that TE-ESN performs better than all baselines and has better reservoir property.

</p>
</details>

<details><summary><b>OpTorch: Optimized deep learning architectures for resource limited environments</b>
<a href="https://arxiv.org/abs/2105.00619">arxiv:2105.00619</a>
&#x1F4C8; 4 <br>
<p>Salman Ahmed, Hammad Naveed</p></summary>
<p>

**Abstract:** Deep learning algorithms have made many breakthroughs and have various applications in real life. Computational resources become a bottleneck as the data and complexity of the deep learning pipeline increases. In this paper, we propose optimized deep learning pipelines in multiple aspects of training including time and memory. OpTorch is a machine learning library designed to overcome weaknesses in existing implementations of neural network training. OpTorch provides features to train complex neural networks with limited computational resources. OpTorch achieved the same accuracy as existing libraries on Cifar-10 and Cifar-100 datasets while reducing memory usage to approximately 50%. We also explore the effect of weights on total memory usage in deep learning pipelines. In our experiments, parallel encoding-decoding along with sequential checkpoints results in much improved memory and time usage while keeping the accuracy similar to existing pipelines. OpTorch python package is available at available at https://github.com/cbrl-nuces/optorch

</p>
</details>

<details><summary><b>Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation</b>
<a href="https://arxiv.org/abs/2105.00591">arxiv:2105.00591</a>
&#x1F4C8; 4 <br>
<p>Juliano S. Assine, J. C. S. Santos Filho, Eduardo Valle</p></summary>
<p>

**Abstract:** In the past few years, mobile deep-learning deployment progressed by leaps and bounds, but solutions still struggle to accommodate its severe and fluctuating operational restrictions, which include bandwidth, latency, computation, and energy. In this work, we help to bridge that gap, introducing the first configurable solution for object detection that manages the triple communication-computation-accuracy trade-off with a single set of weights. Our solution shows state-of-the-art results on COCO-2017, adding only a minor penalty on the base EfficientDet-D2 architecture. Our design is robust to the choice of base architecture and compressor and should adapt well for future architectures.

</p>
</details>

<details><summary><b>Robust Sample Weighting to Facilitate Individualized Treatment Rule Learning for a Target Population</b>
<a href="https://arxiv.org/abs/2105.00581">arxiv:2105.00581</a>
&#x1F4C8; 4 <br>
<p>Rui Chen, Jared D. Huling, Guanhua Chen, Menggang Yu</p></summary>
<p>

**Abstract:** Learning individualized treatment rules (ITRs) is an important topic in precision medicine. Current literature mainly focuses on deriving ITRs from a single source population. We consider the observational data setting when the source population differs from a target population of interest. We assume subject covariates are available from both populations, but treatment and outcome data are only available from the source population. Although adjusting for differences between source and target populations can potentially lead to an improved ITR for the target population, it can substantially increase the variability in ITR estimation. To address this dilemma, we develop a weighting framework that aims to tailor an ITR for a given target population and protect against high variability due to superfluous covariate shift adjustments. Our method seeks covariate balance over a nonparametric function class characterized by a reproducing kernel Hilbert space and can improve many ITR learning methods that rely on weights. We show that the proposed method encompasses importance weights and the so-called overlap weights as two extreme cases, allowing for a better bias-variance trade-off in between. Numerical examples demonstrate that the use of our weighting method can greatly improve ITR estimation for the target population compared with other weighting methods.

</p>
</details>

<details><summary><b>Intelligent Conversational Android ERICA Applied to Attentive Listening and Job Interview</b>
<a href="https://arxiv.org/abs/2105.00403">arxiv:2105.00403</a>
&#x1F4C8; 4 <br>
<p>Tatsuya Kawahara, Koji Inoue, Divesh Lala</p></summary>
<p>

**Abstract:** Following the success of spoken dialogue systems (SDS) in smartphone assistants and smart speakers, a number of communicative robots are developed and commercialized. Compared with the conventional SDSs designed as a human-machine interface, interaction with robots is expected to be in a closer manner to talking to a human because of the anthropomorphism and physical presence. The goal or task of dialogue may not be information retrieval, but the conversation itself. In order to realize human-level "long and deep" conversation, we have developed an intelligent conversational android ERICA. We set up several social interaction tasks for ERICA, including attentive listening, job interview, and speed dating. To allow for spontaneous, incremental multiple utterances, a robust turn-taking model is implemented based on TRP (transition-relevance place) prediction, and a variety of backchannels are generated based on time frame-wise prediction instead of IPU-based prediction. We have realized an open-domain attentive listening system with partial repeats and elaborating questions on focus words as well as assessment responses. It has been evaluated with 40 senior people, engaged in conversation of 5-7 minutes without a conversation breakdown. It was also compared against the WOZ setting. We have also realized a job interview system with a set of base questions followed by dynamic generation of elaborating questions. It has also been evaluated with student subjects, showing promising results.

</p>
</details>

<details><summary><b>Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping</b>
<a href="https://arxiv.org/abs/2105.02963">arxiv:2105.02963</a>
&#x1F4C8; 3 <br>
<p>Rahul Ghosh, Praveen Ravirathinam, Xiaowei Jia, Chenxi Lin, Zhenong Jin, Vipin Kumar</p></summary>
<p>

**Abstract:** The availability of massive earth observing satellite data provide huge opportunities for land use and land cover mapping. However, such mapping effort is challenging due to the existence of various land cover classes, noisy data, and the lack of proper labels. Also, each land cover class typically has its own unique temporal pattern and can be identified only during certain periods. In this article, we introduce a novel architecture that incorporates the UNet structure with Bidirectional LSTM and Attention mechanism to jointly exploit the spatial and temporal nature of satellite data and to better identify the unique temporal patterns of each land cover. We evaluate this method for mapping crops in multiple regions over the world. We compare our method with other state-of-the-art methods both quantitatively and qualitatively on two real-world datasets which involve multiple land cover classes. We also visualise the attention weights to study its effectiveness in mitigating noise and identifying discriminative time period.

</p>
</details>

<details><summary><b>BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.00579">arxiv:2105.00579</a>
&#x1F4C8; 3 <br>
<p>Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song</p></summary>
<p>

**Abstract:** Recent research has confirmed the feasibility of backdoor attacks in deep reinforcement learning (RL) systems. However, the existing attacks require the ability to arbitrarily modify an agent's observation, constraining the application scope to simple RL systems such as Atari games. In this paper, we migrate backdoor attacks to more complex RL systems involving multiple agents and explore the possibility of triggering the backdoor without directly manipulating the agent's observation. As a proof of concept, we demonstrate that an adversary agent can trigger the backdoor of the victim agent with its own action in two-player competitive RL systems. We prototype and evaluate BACKDOORL in four competitive environments. The results show that when the backdoor is activated, the winning rate of the victim drops by 17% to 37% compared to when not activated.

</p>
</details>

<details><summary><b>An Examination of Fairness of AI Models for Deepfake Detection</b>
<a href="https://arxiv.org/abs/2105.00558">arxiv:2105.00558</a>
&#x1F4C8; 3 <br>
<p>Loc Trinh, Yan Liu</p></summary>
<p>

**Abstract:** Recent studies have demonstrated that deep learning models can discriminate based on protected classes like race and gender. In this work, we evaluate bias present in deepfake datasets and detection models across protected subgroups. Using facial datasets balanced by race and gender, we examine three popular deepfake detectors and find large disparities in predictive performances across races, with up to 10.7% difference in error rate between subgroups. A closer look reveals that the widely used FaceForensics++ dataset is overwhelmingly composed of Caucasian subjects, with the majority being female Caucasians. Our investigation of the racial distribution of deepfakes reveals that the methods used to create deepfakes as positive training signals tend to produce "irregular" faces - when a person's face is swapped onto another person of a different race or gender. This causes detectors to learn spurious correlations between the foreground faces and fakeness. Moreover, when detectors are trained with the Blended Image (BI) dataset from Face X-Rays, we find that those detectors develop systematic discrimination towards certain racial subgroups, primarily female Asians.

</p>
</details>

<details><summary><b>Synthesized Difference in Differences</b>
<a href="https://arxiv.org/abs/2105.00455">arxiv:2105.00455</a>
&#x1F4C8; 3 <br>
<p>Eric V. Strobl, Thomas A. Lasko</p></summary>
<p>

**Abstract:** We consider estimating the conditional average treatment effect for everyone by eliminating confounding and selection bias. Unfortunately, randomized clinical trials (RCTs) eliminate confounding but impose strict exclusion criteria that prevent sampling of the entire clinical population. Observational datasets are more inclusive but suffer from confounding. We therefore analyze RCT and observational data simultaneously in order to extract the strengths of each. Our solution builds upon Difference in Differences (DD), an algorithm that eliminates confounding from observational data by comparing outcomes before and after treatment administration. DD requires a parallel slopes assumption that may not apply in practice when confounding shifts across time. We instead propose Synthesized Difference in Differences (SDD) that infers the correct (possibly non-parallel) slopes by linearly adjusting a conditional version of DD using additional RCT data. The algorithm achieves state of the art performance across multiple synthetic and real datasets even when the RCT excludes the majority of patients.

</p>
</details>

<details><summary><b>AirMixML: Over-the-Air Data Mixup for Inherently Privacy-Preserving Edge Machine Learning</b>
<a href="https://arxiv.org/abs/2105.00395">arxiv:2105.00395</a>
&#x1F4C8; 3 <br>
<p>Yusuke Koda, Jihong Park, Mehdi Bennis, Praneeth Vepakomma, Ramesh Raskar</p></summary>
<p>

**Abstract:** Wireless channels can be inherently privacy-preserving by distorting the received signals due to channel noise, and superpositioning multiple signals over-the-air. By harnessing these natural distortions and superpositions by wireless channels, we propose a novel privacy-preserving machine learning (ML) framework at the network edge, coined over-the-air mixup ML (AirMixML). In AirMixML, multiple workers transmit analog-modulated signals of their private data samples to an edge server who trains an ML model using the received noisy-and superpositioned samples. AirMixML coincides with model training using mixup data augmentation achieving comparable accuracy to that with raw data samples. From a privacy perspective, AirMixML is a differentially private (DP) mechanism limiting the disclosure of each worker's private sample information at the server, while the worker's transmit power determines the privacy disclosure level. To this end, we develop a fractional channel-inversion power control (PC) method, α-Dirichlet mixup PC (DirMix(α)-PC), wherein for a given global power scaling factor after channel inversion, each worker's local power contribution to the superpositioned signal is controlled by the Dirichlet dispersion ratio α. Mathematically, we derive a closed-form expression clarifying the relationship between the local and global PC factors to guarantee a target DP level. By simulations, we provide DirMix(α)-PC design guidelines to improve accuracy, privacy, and energy-efficiency. Finally, AirMixML with DirMix(α)-PC is shown to achieve reasonable accuracy compared to a privacy-violating baseline with neither superposition nor PC.

</p>
</details>

<details><summary><b>Data-driven Weight Initialization with Sylvester Solvers</b>
<a href="https://arxiv.org/abs/2105.10335">arxiv:2105.10335</a>
&#x1F4C8; 2 <br>
<p>Debasmit Das, Yash Bhalgat, Fatih Porikli</p></summary>
<p>

**Abstract:** In this work, we propose a data-driven scheme to initialize the parameters of a deep neural network. This is in contrast to traditional approaches which randomly initialize parameters by sampling from transformed standard distributions. Such methods do not use the training data to produce a more informed initialization. Our method uses a sequential layer-wise approach where each layer is initialized using its input activations. The initialization is cast as an optimization problem where we minimize a combination of encoding and decoding losses of the input activations, which is further constrained by a user-defined latent code. The optimization problem is then restructured into the well-known Sylvester equation, which has fast and efficient gradient-free solutions. Our data-driven method achieves a boost in performance compared to random initialization methods, both before start of training and after training is over. We show that our proposed method is especially effective in few-shot and fine-tuning settings. We conclude this paper with analyses on time complexity and the effect of different latent codes on the recognition performance.

</p>
</details>

<details><summary><b>Noisy Student learning for cross-institution brain hemorrhage detection</b>
<a href="https://arxiv.org/abs/2105.00582">arxiv:2105.00582</a>
&#x1F4C8; 2 <br>
<p>Emily Lin, Weicheng Kuo, Esther Yuh</p></summary>
<p>

**Abstract:** Computed tomography (CT) is the imaging modality used in the diagnosis of neurological emergencies, including acute stroke and traumatic brain injury. Advances in deep learning have led to models that can detect and segment hemorrhage on head CT. PatchFCN, one such supervised fully convolutional network (FCN), recently demonstrated expert-level detection of intracranial hemorrhage on in-sample data. However, its potential for similar accuracy outside the training domain is hindered by its need for pixel-labeled data from outside institutions. Also recently, a semi-supervised technique, Noisy Student (NS) learning, demonstrated state-of-the-art performance on ImageNet by moving from a fully-supervised to a semi-supervised learning paradigm. We combine the PatchFCN and Noisy Student approaches, extending semi-supervised learning to an intracranial hemorrhage segmentation task. Surprisingly, the NS model performance surpasses that of a fully-supervised oracle model trained with image-level labels on the same data. It also performs comparably to another recently reported supervised model trained on a labeled dataset 600x larger than that used to train the NS model. To our knowledge, we are the first to demonstrate the effectiveness of semi-supervised learning on a head CT detection and segmentation task.

</p>
</details>

<details><summary><b>Adapting CRISP-DM for Idea Mining: A Data Mining Process for Generating Ideas Using a Textual Dataset</b>
<a href="https://arxiv.org/abs/2105.00574">arxiv:2105.00574</a>
&#x1F4C8; 2 <br>
<p>W. Y. Ayele</p></summary>
<p>

**Abstract:** Data mining project managers can benefit from using standard data mining process models. The benefits of using standard process models for data mining, such as the de facto and the most popular, Cross-Industry-Standard-Process model for Data Mining (CRISP-DM) are reduced cost and time. Also, standard models facilitate knowledge transfer, reuse of best practices, and minimize knowledge requirements. On the other hand, to unlock the potential of ever-growing textual data such as publications, patents, social media data, and documents of various forms, digital innovation is increasingly needed. Furthermore, the introduction of cutting-edge machine learning tools and techniques enable the elicitation of ideas. The processing of unstructured textual data to generate new and useful ideas is referred to as idea mining. Existing literature about idea mining merely overlooks the utilization of standard data mining process models. Therefore, the purpose of this paper is to propose a reusable model to generate ideas, CRISP-DM, for Idea Mining (CRISP-IM). The design and development of the CRISP-IM are done following the design science approach. The CRISP-IM facilitates idea generation, through the use of Dynamic Topic Modeling (DTM), unsupervised machine learning, and subsequent statistical analysis on a dataset of scholarly articles. The adapted CRISP-IM can be used to guide the process of identifying trends using scholarly literature datasets or temporally organized patent or any other textual dataset of any domain to elicit ideas. The ex-post evaluation of the CRISP-IM is left for future study.

</p>
</details>

<details><summary><b>Universal scaling laws in the gradient descent training of neural networks</b>
<a href="https://arxiv.org/abs/2105.00507">arxiv:2105.00507</a>
&#x1F4C8; 2 <br>
<p>Maksim Velikanov, Dmitry Yarotsky</p></summary>
<p>

**Abstract:** Current theoretical results on optimization trajectories of neural networks trained by gradient descent typically have the form of rigorous but potentially loose bounds on the loss values. In the present work we take a different approach and show that the learning trajectory can be characterized by an explicit asymptotic at large training times. Specifically, the leading term in the asymptotic expansion of the loss behaves as a power law $L(t) \sim t^{-ξ}$ with exponent $ξ$ expressed only through the data dimension, the smoothness of the activation function, and the class of function being approximated. Our results are based on spectral analysis of the integral operator representing the linearized evolution of a large network trained on the expected loss. Importantly, the techniques we employ do not require specific form of a data distribution, for example Gaussian, thus making our findings sufficiently universal.

</p>
</details>

<details><summary><b>Curious Exploration and Return-based Memory Restoration for Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2105.00499">arxiv:2105.00499</a>
&#x1F4C8; 2 <br>
<p>Saeed Tafazzol, Erfan Fathi, Mahdi Rezaei, Ehsan Asali</p></summary>
<p>

**Abstract:** Reward engineering and designing an incentive reward function are non-trivial tasks to train agents in complex environments. Furthermore, an inaccurate reward function may lead to a biased behaviour which is far from an efficient and optimised behaviour. In this paper, we focus on training a single agent to score goals with binary success/failure reward function in Half Field Offense domain. As the major advantage of this research, the agent has no presumption about the environment which means it only follows the original formulation of reinforcement learning agents. The main challenge of using such a reward function is the high sparsity of positive reward signals. To address this problem, we use a simple prediction-based exploration strategy (called Curious Exploration) along with a Return-based Memory Restoration (RMR) technique which tends to remember more valuable memories. The proposed method can be utilized to train agents in environments with fairly complex state and action spaces. Our experimental results show that many recent solutions including our baseline method fail to learn and perform in complex soccer domain. However, the proposed method can converge easily to the nearly optimal behaviour. The video presenting the performance of our trained agent is available at http://bit.ly/HFO_Binary_Reward.

</p>
</details>

<details><summary><b>Automatic Visual Inspection of Rare Defects: A Framework based on GP-WGAN and Enhanced Faster R-CNN</b>
<a href="https://arxiv.org/abs/2105.00447">arxiv:2105.00447</a>
&#x1F4C8; 2 <br>
<p>Masoud Jalayer, Reza Jalayer, Amin Kaboli, Carlotta Orsenigo, Carlo Vercellis</p></summary>
<p>

**Abstract:** A current trend in industries such as semiconductors and foundry is to shift their visual inspection processes to Automatic Visual Inspection (AVI) systems, to reduce their costs, mistakes, and dependency on human experts. This paper proposes a two-staged fault diagnosis framework for AVI systems. In the first stage, a generation model is designed to synthesize new samples based on real samples. The proposed augmentation algorithm extracts objects from the real samples and blends them randomly, to generate new samples and enhance the performance of the image processor. In the second stage, an improved deep learning architecture based on Faster R-CNN, Feature Pyramid Network (FPN), and a Residual Network is proposed to perform object detection on the enhanced dataset. The performance of the algorithm is validated and evaluated on two multi-class datasets. The experimental results performed over a range of imbalance severities demonstrate the superiority of the proposed framework compared to other solutions.

</p>
</details>

<details><summary><b>Controlling Smart Inverters using Proxies: A Chance-Constrained DNN-based Approach</b>
<a href="https://arxiv.org/abs/2105.00429">arxiv:2105.00429</a>
&#x1F4C8; 2 <br>
<p>Sarthak Gupta, Vassilis Kekatos, Ming Jin</p></summary>
<p>

**Abstract:** Coordinating inverters at scale under uncertainty is the desideratum for integrating renewables in distribution grids. Unless load demands and solar generation are telemetered frequently, controlling inverters given approximate grid conditions or proxies thereof becomes a key specification. Although deep neural networks (DNNs) can learn optimal inverter schedules, guaranteeing feasibility is largely elusive. Rather than training DNNs to imitate already computed optimal power flow (OPF) solutions, this work integrates DNN-based inverter policies into the OPF. The proposed DNNs are trained through two OPF alternatives that confine voltage deviations on the average and as a convex restriction of chance constraints. The trained DNNs can be driven by partial, noisy, or proxy descriptors of the current grid conditions. This is important when OPF has to be solved for an unobservable feeder. DNN weights are trained via back-propagation and upon differentiating the AC power flow equations assuming the network model is known. Otherwise, a gradient-free variant is put forth. The latter is relevant when inverters are controlled by an aggregator having access only to a power flow solver or a digital twin of the feeder. Numerical tests compare the DNN-based inverter control schemes with the optimal inverter setpoints in terms of optimality and feasibility.

</p>
</details>

<details><summary><b>Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity</b>
<a href="https://arxiv.org/abs/2105.00425">arxiv:2105.00425</a>
&#x1F4C8; 2 <br>
<p>Megi Isallari, Islem Rekik</p></summary>
<p>

**Abstract:** Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net.

</p>
</details>

<details><summary><b>A survey on VQA_Datasets and Approaches</b>
<a href="https://arxiv.org/abs/2105.00421">arxiv:2105.00421</a>
&#x1F4C8; 2 <br>
<p>Yeyun Zou, Qiyu Xie</p></summary>
<p>

**Abstract:** Visual question answering (VQA) is a task that combines both the techniques of computer vision and natural language processing. It requires models to answer a text-based question according to the information contained in a visual. In recent years, the research field of VQA has been expanded. Research that focuses on the VQA, examining the reasoning ability and VQA on scientific diagrams, has also been explored more. Meanwhile, more multimodal feature fusion mechanisms have been proposed. This paper will review and analyze existing datasets, metrics, and models proposed for the VQA task.

</p>
</details>

<details><summary><b>Consistency and Monotonicity Regularization for Neural Knowledge Tracing</b>
<a href="https://arxiv.org/abs/2105.00607">arxiv:2105.00607</a>
&#x1F4C8; 1 <br>
<p>Seewoo Lee, Youngduck Choi, Juneyoung Park, Byungsoo Kim, Jinwoo Shin</p></summary>
<p>

**Abstract:** Knowledge Tracing (KT), tracking a human's knowledge acquisition, is a central component in online learning and AI in Education. In this paper, we present a simple, yet effective strategy to improve the generalization ability of KT models: we propose three types of novel data augmentation, coined replacement, insertion, and deletion, along with corresponding regularization losses that impose certain consistency or monotonicity biases on the model's predictions for the original and augmented sequence. Extensive experiments on various KT benchmarks show that our regularization scheme consistently improves the model performances, under 3 widely-used neural networks and 4 public benchmarks, e.g., it yields 6.3% improvement in AUC under the DKT model and the ASSISTmentsChall dataset.

</p>
</details>

<details><summary><b>OCTOPUS: Overcoming Performance andPrivatization Bottlenecks in Distributed Learning</b>
<a href="https://arxiv.org/abs/2105.00602">arxiv:2105.00602</a>
&#x1F4C8; 1 <br>
<p>Shuo Wang, Surya Nepal, Kristen Moore, Marthie Grobler, Carsten Rudolph, Alsharif Abuadbba</p></summary>
<p>

**Abstract:** The diversity and quantity of the data warehousing, gathering data from distributed devices such as mobile phones, can enhance machine learning algorithms' success and robustness. Federated learning enables distributed participants to collaboratively learn a commonly-shared model while holding data locally. However, it is also faced with expensive communication and limitations due to the heterogeneity of distributed data sources and lack of access to global data. In this paper, we investigate a practical distributed learning scenario where multiple downstream tasks (e.g., classifiers) could be learned from dynamically-updated and non-iid distributed data sources, efficiently and providing local privatization. We introduce a new distributed learning scheme to address communication overhead via latent compression, leveraging global data while providing local privatization of local data without additional cost due to encryption or perturbation. This scheme divides the learning into (1) informative feature encoding, extracting and transmitting the latent space compressed representation features of local data at each node to address communication overhead; (2) downstream tasks centralized at the server using the encoded codes gathered from each node to address computing and storage overhead. Besides, a disentanglement strategy is applied to address the privatization of sensitive components of local data. Extensive experiments are conducted on image and speech datasets. The results demonstrate that downstream tasks on the compact latent representations can achieve comparable accuracy to centralized learning with the privatization of local data.

</p>
</details>

<details><summary><b>Hard Encoding of Physics for Learning Spatiotemporal Dynamics</b>
<a href="https://arxiv.org/abs/2105.00557">arxiv:2105.00557</a>
&#x1F4C8; 1 <br>
<p>Chengping Rao, Hao Sun, Yang Liu</p></summary>
<p>

**Abstract:** Modeling nonlinear spatiotemporal dynamical systems has primarily relied on partial differential equations (PDEs). However, the explicit formulation of PDEs for many underexplored processes, such as climate systems, biochemical reaction and epidemiology, remains uncertain or partially unknown, where very limited measurement data is yet available. To tackle this challenge, we propose a novel deep learning architecture that forcibly encodes known physics knowledge to facilitate learning in a data-driven manner. The coercive encoding mechanism of physics, which is fundamentally different from the penalty-based physics-informed learning, ensures the network to rigorously obey given physics. Instead of using nonlinear activation functions, we propose a novel elementwise product operation to achieve the nonlinearity of the model. Numerical experiment demonstrates that the resulting physics-encoded learning paradigm possesses remarkable robustness against data noise/scarcity and generalizability compared with some state-of-the-art models for data-driven modeling.

</p>
</details>

<details><summary><b>Analysis of Machine Learning Approaches to Packing Detection</b>
<a href="https://arxiv.org/abs/2105.00473">arxiv:2105.00473</a>
&#x1F4C8; 1 <br>
<p>Charles-Henry Bertrand Van Ouytsel, Thomas Given-Wilson, Jeremy Minet, Julian Roussieau, Axel Legay</p></summary>
<p>

**Abstract:** Packing is an obfuscation technique widely used by malware to hide the content and behavior of a program. Much prior research has explored how to detect whether a program is packed. This research includes a broad variety of approaches such as entropy analysis, syntactic signatures and more recently machine learning classifiers using various features. However, no robust results have indicated which algorithms perform best, or which features are most significant. This is complicated by considering how to evaluate the results since accuracy, cost, generalization capabilities, and other measures are all reasonable. This work explores eleven different machine learning approaches using 119 features to understand: which features are most significant for packing detection; which algorithms offer the best performance; and which algorithms are most economical.

</p>
</details>

<details><summary><b>DRIVE: Machine Learning to Identify Drivers of Cancer with High-Dimensional Genomic Data & Imputed Labels</b>
<a href="https://arxiv.org/abs/2105.00469">arxiv:2105.00469</a>
&#x1F4C8; 1 <br>
<p>Adnan Akbar, Andrey Solovyev, John W Cassidy, Nirmesh Patel, Harry W Clifford</p></summary>
<p>

**Abstract:** Identifying the mutations that drive cancer growth is key in clinical decision making and precision oncology. As driver mutations confer selective advantage and thus have an increased likelihood of occurrence, frequency-based statistical models are currently favoured. These methods are not suited to rare, low frequency, driver mutations. The alternative approach to address this is through functional-impact scores, however methods using this approach are highly prone to false positives. In this paper, we propose a novel combination method for driver mutation identification, which uses the power of both statistical modelling and functional-impact based methods. Initial results show this approach outperforms the state-of-the-art methods in terms of precision, and provides comparable performance in terms of area under receiver operating characteristic curves (AU-ROC). We believe that data-driven systems based on machine learning, such as these, will become an integral part of precision oncology in the near future.

</p>
</details>

<details><summary><b>Unsupervised Anomaly Detection in MR Images using Multi-Contrast Information</b>
<a href="https://arxiv.org/abs/2105.00463">arxiv:2105.00463</a>
&#x1F4C8; 1 <br>
<p>Byungjai Kim, Kinam Kwon, Changheun Oh, Hyunwook Park</p></summary>
<p>

**Abstract:** Anomaly detection in medical imaging is to distinguish the relevant biomarkers of diseases from those of normal tissues. Deep supervised learning methods have shown potentials in various detection tasks, but its performances would be limited in medical imaging fields where collecting annotated anomaly data is limited and labor-intensive. Therefore, unsupervised anomaly detection can be an effective tool for clinical practices, which uses only unlabeled normal images as training data. In this paper, we developed an unsupervised learning framework for pixel-wise anomaly detection in multi-contrast magnetic resonance imaging (MRI). The framework has two steps of feature generation and density estimation with Gaussian mixture model (GMM). A feature is derived through the learning of contrast-to-contrast translation that effectively captures the normal tissue characteristics in multi-contrast MRI. The feature is collaboratively used with another feature that is the low-dimensional representation of multi-contrast images. In density estimation using GMM, a simple but efficient way is introduced to handle the singularity problem which interrupts the joint learning process. The proposed method outperforms previous anomaly detection approaches. Quantitative and qualitative analyses demonstrate the effectiveness of the proposed method in anomaly detection for multi-contrast MRI.

</p>
</details>

<details><summary><b>Fast Power Control Adaptation via Meta-Learning for Random Edge Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2105.00459">arxiv:2105.00459</a>
&#x1F4C8; 1 <br>
<p>Ivana Nikoloska, Osvaldo Simeone</p></summary>
<p>

**Abstract:** Power control in decentralized wireless networks poses a complex stochastic optimization problem when formulated as the maximization of the average sum rate for arbitrary interference graphs. Recent work has introduced data-driven design methods that leverage graph neural network (GNN) to efficiently parametrize the power control policy mapping channel state information (CSI) to the power vector. The specific GNN architecture, known as random edge GNN (REGNN), defines a non-linear graph convolutional architecture whose spatial weights are tied to the channel coefficients, enabling a direct adaption to channel conditions. This paper studies the higher-level problem of enabling fast adaption of the power control policy to time-varying topologies. To this end, we apply first-order meta-learning on data from multiple topologies with the aim of optimizing for a few-shot adaptation to new network configurations.

</p>
</details>

<details><summary><b>A structured proof of the Kolmogorov superposition theorem</b>
<a href="https://arxiv.org/abs/2105.00408">arxiv:2105.00408</a>
&#x1F4C8; 1 <br>
<p>S. Dzhenzher, A. Skopenkov</p></summary>
<p>

**Abstract:** We present a well-structured detailed exposition of a well-known proof of the following celebrated result solving Hilbert's 13th problem on superpositions. For functions of 2 variables the statement is as follows.
  Kolmogorov Theorem. There are continuous functions $\varphi_1,\ldots,\varphi_5 : [\,0, 1\,]\to [\,0,1\,]$ such that for any continuous function $f: [\,0,1\,]^2\to\mathbb R$ there is a continuous function $h: [\,0,3\,]\to\mathbb R$ such that for any $x,y\in [\,0, 1\,]$ we have $$f(x,y)=\sum\limits_{k=1}^5 h\left(\varphi_k(x)+\sqrt{2}\,\varphi_k(y)\right).$$ The proof is accessible to non-specialists, in particular, to students familiar with only basic properties of continuous functions.

</p>
</details>

<details><summary><b>AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation</b>
<a href="https://arxiv.org/abs/2105.00402">arxiv:2105.00402</a>
&#x1F4C8; 1 <br>
<p>Dinh Viet Sang, Tran Quang Chung, Phan Ngoc Lan, Dao Viet Hang, Dao Van Long, Nguyen Thi Thuy</p></summary>
<p>

**Abstract:** Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective screening tool to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provide valuable insights for improving colon polyp detection. However, precise segmentation is still challenging due to variations of polyps in size, shape, texture, and color. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention gates. The network is capable of effectively combining multi-level features to yield accurate polyp segmentation. Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.

</p>
</details>

<details><summary><b>A LiDAR Assisted Control Module with High Precision in Parking Scenarios for Autonomous Driving Vehicle</b>
<a href="https://arxiv.org/abs/2105.00398">arxiv:2105.00398</a>
&#x1F4C8; 1 <br>
<p>Xin Xu, Yu Dong, Fan Zhu</p></summary>
<p>

**Abstract:** Autonomous driving has been quite promising in recent years. The public has seen Robotaxi delivered by Waymo, Baidu, Cruise, and so on. While autonomous driving vehicles certainly have a bright future, we have to admit that it is still a long way to go for products such as Robotaxi. On the other hand, in less complex scenarios autonomous driving may have the potentiality to reliably outperform humans. For example, humans are good at interactive tasks (while autonomous driving systems usually do not), but we are often incompetent for tasks with strict precision demands. In this paper, we introduce a real-world, industrial scenario of which human drivers are not capable. The task required the ego vehicle to keep a stationary lateral distance (i.e. 3? <= 5 centimeters) with respect to a reference. To address this challenge, we redesigned the control module from Baidu Apollo open-source autonomous driving system. A precise (3? <= 2 centimeters) Error Feedback System was first built to partly replace the localization module. Then we investigated the control module thoroughly and added a real-time calibration algorithm to gain extra precision. We also built a simulation to fine-tune the control parameters. After all those works, the results are encouraging, showing that an end-to-end lateral precision with 3? <= 5 centimeters has been achieved. Further, we show that the results not only outperformed original Apollo modules but also beat specially trained and highly experienced human test drivers.

</p>
</details>

<details><summary><b>CARL-DTN: Context Adaptive Reinforcement Learning based Routing Algorithm in Delay Tolerant Network</b>
<a href="https://arxiv.org/abs/2105.00544">arxiv:2105.00544</a>
&#x1F4C8; 0 <br>
<p>Fuad Yimer Yesuf, M. Prathap</p></summary>
<p>

**Abstract:** The term Delay/Disruption-Tolerant Networks (DTN) invented to describe and cover all types of long-delay, disconnected, intermittently connected networks, where mobility and outages or scheduled contacts may be experienced. This environment is characterized by frequent network partitioning, intermittent connectivity, large or variable delay, asymmetric data rate, and low transmission reliability. There have been routing protocols developed in DTN. However, those routing algorithms are design based upon specific assumptions. The assumption makes existing algorithms suitable for specific environment scenarios. Different routing algorithm uses different relay node selection criteria to select the replication node. Too Frequently forwarding messages can result in excessive packet loss and large buffer and network overhead. On the other hand, less frequent transmission leads to a lower delivery ratio. In DTN there is a trade-off off between delivery ratio and overhead. In this study, we proposed context-adaptive reinforcement learning based routing(CARL-DTN) protocol to determine optimal replicas of the message based on the real-time density. Our routing protocol jointly uses a real-time physical context, social-tie strength, and real-time message context using fuzzy logic in the routing decision. Multi-hop forwarding probability is also considered for the relay node selection by employing Q-Learning algorithm to estimate the encounter probability between nodes and to learn about nodes available in the neighbor by discounting reward. The performance of the proposed protocol is evaluated based on various simulation scenarios. The result shows that the proposed protocol has better performance in terms of message delivery ratio and overhead.

</p>
</details>

<details><summary><b>GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning</b>
<a href="https://arxiv.org/abs/2105.00529">arxiv:2105.00529</a>
&#x1F4C8; 0 <br>
<p>Hanchi Ren, Jingjing Deng, Xianghua Xie</p></summary>
<p>

**Abstract:** Data privacy has become an increasingly important issue in machine learning. Many approaches have been developed to tackle this issue, e.g., cryptography (Homomorphic Encryption, Differential Privacy, etc.) and collaborative training (Secure Multi-Party Computation, Distributed Learning and Federated Learning). These techniques have a particular focus on data encryption or secure local computation. They transfer the intermediate information to the third-party to compute the final result. Gradient exchanging is commonly considered to be a secure way of training a robust model collaboratively in deep learning. However, recent researches have demonstrated that sensitive information can be recovered from the shared gradient. Generative Adversarial Networks (GAN), in particular, have shown to be effective in recovering those information. However, GAN based techniques require additional information, such as class labels which are generally unavailable for privacy persevered learning. In this paper, we show that, in Federated Learning (FL) system, image-based privacy data can be easily recovered in full from the shared gradient only via our proposed Generative Regression Neural Network (GRNN). We formulate the attack to be a regression problem and optimise two branches of the generative model by minimising the distance between gradients. We evaluate our method on several image classification tasks. The results illustrate that our proposed GRNN outperforms state-of-the-art methods with better stability, stronger robustness, and higher accuracy. It also has no convergence requirement to the global FL model. Moreover, we demonstrate information leakage using face re-identification. Some defense strategies are also discussed in this work.

</p>
</details>

<details><summary><b>Altruism Design in Networked Public Goods Games</b>
<a href="https://arxiv.org/abs/2105.00505">arxiv:2105.00505</a>
&#x1F4C8; 0 <br>
<p>Sixie Yu, David Kempe, Yevgeniy Vorobeychik</p></summary>
<p>

**Abstract:** Many collective decision-making settings feature a strategic tension between agents acting out of individual self-interest and promoting a common good. These include wearing face masks during a pandemic, voting, and vaccination. Networked public goods games capture this tension, with networks encoding strategic interdependence among agents. Conventional models of public goods games posit solely individual self-interest as a motivation, even though altruistic motivations have long been known to play a significant role in agents' decisions. We introduce a novel extension of public goods games to account for altruistic motivations by adding a term in the utility function that incorporates the perceived benefits an agent obtains from the welfare of others, mediated by an altruism graph. Most importantly, we view altruism not as immutable, but rather as a lever for promoting the common good. Our central algorithmic question then revolves around the computational complexity of modifying the altruism network to achieve desired public goods game investment profiles. We first show that the problem can be solved using linear programming when a principal can fractionally modify the altruism network. While the problem becomes in general intractable if the principal's actions are all-or-nothing, we exhibit several tractable special cases.

</p>
</details>

<details><summary><b>BI-REC: Guided Data Analysis for Conversational Business Intelligence</b>
<a href="https://arxiv.org/abs/2105.00467">arxiv:2105.00467</a>
&#x1F4C8; 0 <br>
<p>Venkata Vamsikrishna Meduri, Abdul Quamar, Chuan Lei, Vasilis Efthymiou, Fatma Ozcan</p></summary>
<p>

**Abstract:** Conversational interfaces to Business Intelligence (BI) applications enable data analysis using a natural language dialog in small incremental steps. To truly unleash the power of conversational BI to democratize access to data, a system needs to provide effective and continuous support for data analysis. In this paper, we propose BI-REC, a conversational recommendation system for BI applications to help users accomplish their data analysis tasks.
  We define the space of data analysis in terms of BI patterns, augmented with rich semantic information extracted from the OLAP cube definition, and use graph embeddings learned using GraphSAGE to create a compact representation of the analysis state. We propose a two-step approach to explore the search space for useful BI pattern recommendations. In the first step, we train a multi-class classifier using prior query logs to predict the next high-level actions in terms of a BI operation (e.g., {\em Drill-Down} or {\em Roll-up}) and a measure that the user is interested in. In the second step, the high-level actions are further refined into actual BI pattern recommendations using collaborative filtering. This two-step approach allows us to not only divide and conquer the huge search space, but also requires less training data. Our experimental evaluation shows that BI-REC achieves an accuracy of 83% for BI pattern recommendations and up to 2X speedup in latency of prediction compared to a state-of-the-art baseline. Our user study further shows that BI-REC provides recommendations with a precision@3 of 91.90% across several different analysis tasks.

</p>
</details>

<details><summary><b>Multi-Agent Routing and Scheduling Through Coalition Formation</b>
<a href="https://arxiv.org/abs/2105.00451">arxiv:2105.00451</a>
&#x1F4C8; 0 <br>
<p>Luca Capezzuto, Danesh Tarapore, Sarvapali D. Ramchurn</p></summary>
<p>

**Abstract:** In task allocation for real-time domains, such as disaster response, a limited number of agents is deployed across a large area to carry out numerous tasks, each with its prerequisites, profit, time window and workload. To maximize profits while minimizing time penalties, agents need to cooperate by forming, disbanding and reforming coalitions. In this paper, we name this problem Multi-Agent Routing and Scheduling through Coalition formation (MARSC) and show that it generalizes the important Team Orienteering Problem with Time Windows. We propose a binary integer program and an anytime and scalable heuristic to solve it. Using public London Fire Brigade records, we create a dataset with 347588 tasks and a test framework that simulates the mobilization of firefighters. In problems with up to 150 agents and 3000 tasks, our heuristic finds solutions up to 3.25 times better than the Earliest Deadline First approach commonly used in real-time systems. Our results constitute the first large-scale benchmark for the MARSC problem.

</p>
</details>


[Next Page]({{ '/2021/05/01/2021.05.01.html' | relative_url }})
