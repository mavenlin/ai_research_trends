## Summary for 2021-04-01, created on 2021-12-22


<details><summary><b>Unconstrained Scene Generation with Locally Conditioned Radiance Fields</b>
<a href="https://arxiv.org/abs/2104.00670">arxiv:2104.00670</a>
&#x1F4C8; 87 <br>
<p>Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind</p></summary>
<p>

**Abstract:** We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.

</p>
</details>

<details><summary><b>Towards General Purpose Vision Systems</b>
<a href="https://arxiv.org/abs/2104.00743">arxiv:2104.00743</a>
&#x1F4C8; 86 <br>
<p>Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem</p></summary>
<p>

**Abstract:** A special purpose learning system assumes knowledge of admissible tasks at design time. Adapting such a system to unforeseen tasks requires architecture manipulation such as adding an output head for each new task or dataset. In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text. The system supports a wide range of vision tasks such as classification, localization, question answering, captioning, and more. We evaluate the system's ability to learn multiple skills simultaneously, to perform tasks with novel skill-concept combinations, and to learn new skills efficiently and without forgetting.

</p>
</details>

<details><summary><b>Avalanche: an End-to-End Library for Continual Learning</b>
<a href="https://arxiv.org/abs/2104.00405">arxiv:2104.00405</a>
&#x1F4C8; 80 <br>
<p>Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer</p></summary>
<p>

**Abstract:** Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.

</p>
</details>

<details><summary><b>Deep Learning of Conjugate Mappings</b>
<a href="https://arxiv.org/abs/2104.01874">arxiv:2104.01874</a>
&#x1F4C8; 63 <br>
<p>Jason J. Bramburger, Steven L. Brunton, J. Nathan Kutz</p></summary>
<p>

**Abstract:** Despite many of the most common chaotic dynamical systems being continuous in time, it is through discrete time mappings that much of the understanding of chaos is formed. Henri Poincaré first made this connection by tracking consecutive iterations of the continuous flow with a lower-dimensional, transverse subspace. The mapping that iterates the dynamics through consecutive intersections of the flow with the subspace is now referred to as a Poincaré map, and it is the primary method available for interpreting and classifying chaotic dynamics. Unfortunately, in all but the simplest systems, an explicit form for such a mapping remains outstanding. This work proposes a method for obtaining explicit Poincaré mappings by using deep learning to construct an invertible coordinate transformation into a conjugate representation where the dynamics are governed by a relatively simple chaotic mapping. The invertible change of variable is based on an autoencoder, which allows for dimensionality reduction, and has the advantage of classifying chaotic systems using the equivalence relation of topological conjugacies. Indeed, the enforcement of topological conjugacies is the critical neural network regularization for learning the coordinate and dynamics pairing. We provide expository applications of the method to low-dimensional systems such as the Rössler and Lorenz systems, while also demonstrating the utility of the method on infinite-dimensional systems, such as the Kuramoto--Sivashinsky equation.

</p>
</details>

<details><summary><b>LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions</b>
<a href="https://arxiv.org/abs/2104.00820">arxiv:2104.00820</a>
&#x1F4C8; 49 <br>
<p>Oğuz Kaan Yüksel, Enis Simsar, Ezgi Gülperi Er, Pinar Yanardag</p></summary>
<p>

**Abstract:** Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions enable controllable image generation and support a wide range of semantic editing operations, such as zoom or rotation. The discovery of such directions is often done in a supervised or semi-supervised manner and requires manual annotations which limits their use in practice. In comparison, unsupervised discovery allows finding subtle directions that are difficult to detect a priori. In this work, we propose a contrastive learning-based approach to discover semantic directions in the latent space of pre-trained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions comparable with state-of-the-art methods.

</p>
</details>

<details><summary><b>NeRF-VAE: A Geometry Aware 3D Scene Generative Model</b>
<a href="https://arxiv.org/abs/2104.00587">arxiv:2104.00587</a>
&#x1F4C8; 45 <br>
<p>Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soňa Mokrá, Danilo J. Rezende</p></summary>
<p>

**Abstract:** We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.

</p>
</details>

<details><summary><b>Replay in Deep Learning: Current Approaches and Missing Biological Elements</b>
<a href="https://arxiv.org/abs/2104.04132">arxiv:2104.04132</a>
&#x1F4C8; 42 <br>
<p>Tyler L. Hayes, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann, Terrence J. Sejnowski, Christopher Kanan</p></summary>
<p>

**Abstract:** Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.

</p>
</details>

<details><summary><b>Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis</b>
<a href="https://arxiv.org/abs/2104.00677">arxiv:2104.00677</a>
&#x1F4C8; 41 <br>
<p>Ajay Jain, Matthew Tancik, Pieter Abbeel</p></summary>
<p>

**Abstract:** We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360° scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.

</p>
</details>

<details><summary><b>Linear Semantics in Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2104.00487">arxiv:2104.00487</a>
&#x1F4C8; 34 <br>
<p>Jianjin Xu, Changxi Zheng</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) are able to generate high-quality images, but it remains difficult to explicitly specify the semantics of synthesized images. In this work, we aim to better understand the semantic representation of GANs, and thereby enable semantic control in GAN's generation process. Interestingly, we find that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way: a linear transformation of feature maps suffices to extract the generated image semantics. To verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled images. Last but not least, leveraging our findings, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing. Given a trained GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics. We have made the code publicly available.

</p>
</details>

<details><summary><b>SimPoE: Simulated Character Control for 3D Human Pose Estimation</b>
<a href="https://arxiv.org/abs/2104.00683">arxiv:2104.00683</a>
&#x1F4C8; 23 <br>
<p>Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, Jason Saragih</p></summary>
<p>

**Abstract:** Accurate estimation of 3D human motion from monocular video requires modeling both kinematics (body motion without physical forces) and dynamics (motion with physical forces). To demonstrate this, we present SimPoE, a Simulation-based approach for 3D human Pose Estimation, which integrates image-based kinematic inference and physics-based dynamics modeling. SimPoE learns a policy that takes as input the current-frame pose estimate and the next image frame to control a physically-simulated character to output the next-frame pose estimate. The policy contains a learnable kinematic pose refinement unit that uses 2D keypoints to iteratively refine its kinematic pose estimate of the next frame. Based on this refined kinematic pose, the policy learns to compute dynamics-based control (e.g., joint torques) of the character to advance the current-frame pose estimate to the pose estimate of the next frame. This design couples the kinematic pose refinement unit with the dynamics-based control generation unit, which are learned jointly with reinforcement learning to achieve accurate and physically-plausible pose estimation. Furthermore, we propose a meta-control mechanism that dynamically adjusts the character's dynamics parameters based on the character state to attain more accurate pose estimates. Experiments on large-scale motion datasets demonstrate that our approach establishes the new state of the art in pose accuracy while ensuring physical plausibility.

</p>
</details>

<details><summary><b>Partition-Guided GANs</b>
<a href="https://arxiv.org/abs/2104.00816">arxiv:2104.00816</a>
&#x1F4C8; 20 <br>
<p>Mohammadreza Armandpour, Ali Sadeghian, Chunyuan Li, Mingyuan Zhou</p></summary>
<p>

**Abstract:** Despite the success of Generative Adversarial Networks (GANs), their training suffers from several well-known problems, including mode collapse and difficulties learning a disconnected set of manifolds. In this paper, we break down the challenging task of learning complex high dimensional distributions, supporting diverse data samples, to simpler sub-tasks. Our solution relies on designing a partitioner that breaks the space into smaller regions, each having a simpler distribution, and training a different generator for each partition. This is done in an unsupervised manner without requiring any labels.
  We formulate two desired criteria for the space partitioner that aid the training of our mixture of generators: 1) to produce connected partitions and 2) provide a proxy of distance between partitions and data samples, along with a direction for reducing that distance. These criteria are developed to avoid producing samples from places with non-existent data density, and also facilitate training by providing additional direction to the generators. We develop theoretical constraints for a space partitioner to satisfy the above criteria. Guided by our theoretical analysis, we design an effective neural architecture for the space partitioner that empirically assures these conditions. Experimental results on various standard benchmarks show that the proposed unsupervised model outperforms several recent methods.

</p>
</details>

<details><summary><b>Mining Wikidata for Name Resources for African Languages</b>
<a href="https://arxiv.org/abs/2104.00558">arxiv:2104.00558</a>
&#x1F4C8; 20 <br>
<p>Jonne Sälevä, Constantine Lignos</p></summary>
<p>

**Abstract:** This work supports further development of language technology for the languages of Africa by providing a Wikidata-derived resource of name lists corresponding to common entity types (person, location, and organization). While we are not the first to mine Wikidata for name lists, our approach emphasizes scalability and replicability and addresses data quality issues for languages that do not use Latin scripts. We produce lists containing approximately 1.9 million names across 28 African languages. We describe the data, the process used to produce it, and its limitations, and provide the software and data for public use. Finally, we discuss the ethical considerations of producing this resource and others of its kind.

</p>
</details>

<details><summary><b>Keyword Transformer: A Self-Attention Model for Keyword Spotting</b>
<a href="https://arxiv.org/abs/2104.00769">arxiv:2104.00769</a>
&#x1F4C8; 16 <br>
<p>Axel Berg, Mark O'Connor, Miguel Tairum Cruz</p></summary>
<p>

**Abstract:** The Transformer architecture has been successful across many domains, including natural language processing, computer vision and speech recognition. In keyword spotting, self-attention has primarily been used on top of convolutional or recurrent encoders. We investigate a range of ways to adapt the Transformer architecture to keyword spotting and introduce the Keyword Transformer (KWT), a fully self-attentional architecture that exceeds state-of-the-art performance across multiple tasks without any pre-training or additional data. Surprisingly, this simple architecture outperforms more complex models that mix convolutional, recurrent and attentive layers. KWT can be used as a drop-in replacement for these models, setting two new benchmark records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks respectively.

</p>
</details>

<details><summary><b>A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification</b>
<a href="https://arxiv.org/abs/2104.00679">arxiv:2104.00679</a>
&#x1F4C8; 13 <br>
<p>Jong-Chyi Su, Zezhou Cheng, Subhransu Maji</p></summary>
<p>

**Abstract:** We evaluate the effectiveness of semi-supervised learning (SSL) on a realistic benchmark where data exhibits considerable class imbalance and contains images from novel classes. Our benchmark consists of two fine-grained classification datasets obtained by sampling classes from the Aves and Fungi taxonomy. We find that recently proposed SSL methods provide significant benefits, and can effectively use out-of-class data to improve performance when deep networks are trained from scratch. Yet their performance pales in comparison to a transfer learning baseline, an alternative approach for learning from a few examples. Furthermore, in the transfer setting, while existing SSL methods provide improvements, the presence of out-of-class is often detrimental. In this setting, standard fine-tuning followed by distillation-based self-training is the most robust. Our work suggests that semi-supervised learning with experts on realistic datasets may require different strategies than those currently prevalent in the literature.

</p>
</details>

<details><summary><b>Many-to-English Machine Translation Tools, Data, and Pretrained Models</b>
<a href="https://arxiv.org/abs/2104.00290">arxiv:2104.00290</a>
&#x1F4C8; 13 <br>
<p>Thamme Gowda, Zhao Zhang, Chris A Mattmann, Jonathan May</p></summary>
<p>

**Abstract:** While there are more than 7000 languages in the world, most translation research efforts have targeted a few high-resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.

</p>
</details>

<details><summary><b>fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation</b>
<a href="https://arxiv.org/abs/2104.00507">arxiv:2104.00507</a>
&#x1F4C8; 11 <br>
<p>Jakub Wiśniewski, Przemysław Biecek</p></summary>
<p>

**Abstract:** Machine learning decision systems are getting omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are really eager to learn social biases present in historical data that can lead to increasing discrimination. If we want to create models responsibly then we need tools for in-depth validation of models also from the perspective of potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in classification models in an easy and flexible fashion. The fairmodels package offers a model-agnostic approach to bias detection, visualization and mitigation. The implemented set of functions and fairness metrics enables model fairness validation from different perspectives. The package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model. The package is designed not only to examine a single model, but also to facilitate comparisons between multiple models.

</p>
</details>

<details><summary><b>Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study</b>
<a href="https://arxiv.org/abs/2104.00676">arxiv:2104.00676</a>
&#x1F4C8; 10 <br>
<p>Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting Cheng, Marios Savvides</p></summary>
<p>

**Abstract:** This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation. We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness. Project page: http://zhiqiangshen.com/projects/LS_and_KD/index.html.

</p>
</details>

<details><summary><b>How Are Learned Perception-Based Controllers Impacted by the Limits of Robust Control?</b>
<a href="https://arxiv.org/abs/2104.00827">arxiv:2104.00827</a>
&#x1F4C8; 9 <br>
<p>Jingxi Xu, Bruce Lee, Nikolai Matni, Dinesh Jayaraman</p></summary>
<p>

**Abstract:** The difficulty of optimal control problems has classically been characterized in terms of system properties such as minimum eigenvalues of controllability/observability gramians. We revisit these characterizations in the context of the increasing popularity of data-driven techniques like reinforcement learning (RL), and in control settings where input observations are high-dimensional images and transition dynamics are unknown. Specifically, we ask: to what extent are quantifiable control and perceptual difficulty metrics of a task predictive of the performance and sample complexity of data-driven controllers? We modulate two different types of partial observability in a cartpole "stick-balancing" problem -- (i) the height of one visible fixation point on the cartpole, which can be used to tune fundamental limits of performance achievable by any controller, and by (ii) the level of perception noise in the fixation point position inferred from depth or RGB images of the cartpole. In these settings, we empirically study two popular families of controllers: RL and system identification-based $H_\infty$ control, using visually estimated system state. Our results show that the fundamental limits of robust control have corresponding implications for the sample-efficiency and performance of learned perception-based controllers. Visit our project website https://jxu.ai/rl-vs-control-web for more information.

</p>
</details>

<details><summary><b>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</b>
<a href="https://arxiv.org/abs/2104.00355">arxiv:2104.00355</a>
&#x1F4C8; 9 <br>
<p>Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux</p></summary>
<p>

**Abstract:** We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.

</p>
</details>

<details><summary><b>Explore Image Deblurring via Blur Kernel Space</b>
<a href="https://arxiv.org/abs/2104.00317">arxiv:2104.00317</a>
&#x1F4C8; 9 <br>
<p>Phong Tran, Anh Tran, Quynh Phung, Minh Hoai</p></summary>
<p>

**Abstract:** This paper introduces a method to encode the blur operators of an arbitrary dataset of sharp-blur image pairs into a blur kernel space. Assuming the encoded kernel space is close enough to in-the-wild blur operators, we propose an alternating optimization algorithm for blind image deblurring. It approximates an unseen blur operator by a kernel in the encoded space and searches for the corresponding sharp image. Unlike recent deep-learning-based methods, our system can handle unseen blur kernel, while avoiding using complicated handcrafted priors on the blur operator often found in classical methods. Due to the method's design, the encoded kernel space is fully differentiable, thus can be easily adopted in deep neural network models. Moreover, our method can be used for blur synthesis by transferring existing blur operators from a given dataset into a new domain. Finally, we provide experimental results to confirm the effectiveness of the proposed method.

</p>
</details>

<details><summary><b>Collision-Aware Target-Driven Object Grasping in Constrained Environments</b>
<a href="https://arxiv.org/abs/2104.00776">arxiv:2104.00776</a>
&#x1F4C8; 8 <br>
<p>Xibai Lou, Yang Yang, Changhyun Choi</p></summary>
<p>

**Abstract:** Grasping a novel target object in constrained environments (e.g., walls, bins, and shelves) requires intensive reasoning about grasp pose reachability to avoid collisions with the surrounding structures. Typical 6-DoF robotic grasping systems rely on the prior knowledge about the environment and intensive planning computation, which is ungeneralizable and inefficient. In contrast, we propose a novel Collision-Aware Reachability Predictor (CARP) for 6-DoF grasping systems. The CARP learns to estimate the collision-free probabilities for grasp poses and significantly improves grasping in challenging environments. The deep neural networks in our approach are trained fully by self-supervision in simulation. The experiments in both simulation and the real world show that our approach achieves more than 75% grasping rate on novel objects in various surrounding structures. The ablation study demonstrates the effectiveness of the CARP, which improves the 6-DoF grasping rate by 95.7%.

</p>
</details>

<details><summary><b>Multiview Pseudo-Labeling for Semi-supervised Learning from Video</b>
<a href="https://arxiv.org/abs/2104.00682">arxiv:2104.00682</a>
&#x1F4C8; 8 <br>
<p>Bo Xiong, Haoqi Fan, Kristen Grauman, Christoph Feichtenhofer</p></summary>
<p>

**Abstract:** We present a multiview pseudo-labeling approach to video learning, a novel framework that uses complementary views in the form of appearance and motion information for semi-supervised learning in video. The complementary views help obtain more reliable pseudo-labels on unlabeled video, to learn stronger video representations than from purely supervised data. Though our method capitalizes on multiple views, it nonetheless trains a model that is shared across appearance and motion input and thus, by design, incurs no additional computation overhead at inference time. On multiple video recognition datasets, our method substantially outperforms its supervised counterpart, and compares favorably to previous work on standard benchmarks in self-supervised video representation learning.

</p>
</details>

<details><summary><b>Fast DCTTS: Efficient Deep Convolutional Text-to-Speech</b>
<a href="https://arxiv.org/abs/2104.00624">arxiv:2104.00624</a>
&#x1F4C8; 8 <br>
<p>Minsu Kang, Jihyun Lee, Simin Kim, Injung Kim</p></summary>
<p>

**Abstract:** We propose an end-to-end speech synthesizer, Fast DCTTS, that synthesizes speech in real time on a single CPU thread. The proposed model is composed of a carefully-tuned lightweight network designed by applying multiple network reduction and fidelity improvement techniques. In addition, we propose a novel group highway activation that can compromise between computational efficiency and the regularization effect of the gating mechanism. As well, we introduce a new metric called Elastic mel-cepstral distortion (EMCD) to measure the fidelity of the output mel-spectrogram. In experiments, we analyze the effect of the acceleration techniques on speed and speech quality. Compared with the baseline model, the proposed model exhibits improved MOS from 2.62 to 2.74 with only 1.76% computation and 2.75% parameters. The speed on a single CPU thread was improved by 7.45 times, which is fast enough to produce mel-spectrogram in real time without GPU.

</p>
</details>

<details><summary><b>quantum Case-Based Reasoning (qCBR)</b>
<a href="https://arxiv.org/abs/2104.00409">arxiv:2104.00409</a>
&#x1F4C8; 8 <br>
<p>Parfait Atchade-Adelomou, Daniel Casado-Fauli, Elisabet Golobardes-Ribe, Xavier Vilasis-Cardona</p></summary>
<p>

**Abstract:** Case-Based Reasoning (CBR) is an artificial intelligence approach to problem-solving with a good record of success. This article proposes using Quantum Computing to improve some of the key processes of CBR defining so a Quantum Case-Based Reasoning (qCBR) paradigm. The focus is set on designing and implementing a qCBR based on the variational principle that improves its classical counterpart in terms of average accuracy, scalability and tolerance to overlapping. A comparative study of the proposed qCBR with a classic CBR is performed for the case of the Social Workers' Problem as a sample of a combinatorial optimization problem with overlapping. The algorithm's quantum feasibility is modelled with docplex and tested on IBMQ computers, and experimented on the Qibo framework.

</p>
</details>

<details><summary><b>Confidence Calibration for Domain Generalization under Covariate Shift</b>
<a href="https://arxiv.org/abs/2104.00742">arxiv:2104.00742</a>
&#x1F4C8; 7 <br>
<p>Yunye Gong, Xiao Lin, Yi Yao, Thomas G. Dietterich, Ajay Divakaran, Melinda Gervasio</p></summary>
<p>

**Abstract:** Existing calibration algorithms address the problem of covariate shift via unsupervised domain adaptation. However, these methods suffer from the following limitations: 1) they require unlabeled data from the target domain, which may not be available at the stage of calibration in real-world applications and 2) their performance depends heavily on the disparity between the distributions of the source and target domains. To address these two limitations, we present novel calibration solutions via domain generalization. Our core idea is to leverage multiple calibration domains to reduce the effective distribution disparity between the target and calibration domains for improved calibration transfer without needing any data from the target domain. We provide theoretical justification and empirical experimental results to demonstrate the effectiveness of our proposed algorithms. Compared against state-of-the-art calibration methods designed for domain adaptation, we observe a decrease of 8.86 percentage points in expected calibration error or, equivalently, an increase of 35 percentage points in improvement ratio for multi-class classification on the Office-Home dataset.

</p>
</details>

<details><summary><b>BRepNet: A topological message passing system for solid models</b>
<a href="https://arxiv.org/abs/2104.00706">arxiv:2104.00706</a>
&#x1F4C8; 7 <br>
<p>Joseph G. Lambourne, Karl D. D. Willis, Pradeep Kumar Jayaraman, Aditya Sanghi, Peter Meltzer, Hooman Shayani</p></summary>
<p>

**Abstract:** Boundary representation (B-rep) models are the standard way 3D shapes are described in Computer-Aided Design (CAD) applications. They combine lightweight parametric curves and surfaces with topological information which connects the geometric entities to describe manifolds. In this paper we introduce BRepNet, a neural network architecture designed to operate directly on B-rep data structures, avoiding the need to approximate the model as meshes or point clouds. BRepNet defines convolutional kernels with respect to oriented coedges in the data structure. In the neighborhood of each coedge, a small collection of faces, edges and coedges can be identified and patterns in the feature vectors from these entities detected by specific learnable parameters. In addition, to encourage further deep learning research with B-reps, we publish the Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep models annotated with information about the modeling operations which created each face. We demonstrate that BRepNet can segment these models with higher accuracy than methods working on meshes, and point clouds.

</p>
</details>

<details><summary><b>Bayesian Graph Convolutional Network for Traffic Prediction</b>
<a href="https://arxiv.org/abs/2104.00488">arxiv:2104.00488</a>
&#x1F4C8; 7 <br>
<p>Jun Fu, Wei Zhou, Zhibo Chen</p></summary>
<p>

**Abstract:** Recently, adaptive graph convolutional network based traffic prediction methods, learning a latent graph structure from traffic data via various attention-based mechanisms, have achieved impressive performance. However, they are still limited to find a better description of spatial relationships between traffic conditions due to: (1) ignoring the prior of the observed topology of the road network; (2) neglecting the presence of negative spatial relationships; and (3) lacking investigation on uncertainty of the graph structure. In this paper, we propose a Bayesian Graph Convolutional Network (BGCN) framework to alleviate these issues. Under this framework, the graph structure is viewed as a random realization from a parametric generative model, and its posterior is inferred using the observed topology of the road network and traffic data. Specifically, the parametric generative model is comprised of two parts: (1) a constant adjacency matrix which discovers potential spatial relationships from the observed physical connections between roads using a Bayesian approach; (2) a learnable adjacency matrix that learns a global shared spatial correlations from traffic data in an end-to-end fashion and can model negative spatial correlations. The posterior of the graph structure is then approximated by performing Monte Carlo dropout on the parametric graph structure. We verify the effectiveness of our method on five real-world datasets, and the experimental results demonstrate that BGCN attains superior performance compared with state-of-the-art methods.

</p>
</details>

<details><summary><b>Towards Evaluating and Training Verifiably Robust Neural Networks</b>
<a href="https://arxiv.org/abs/2104.00447">arxiv:2104.00447</a>
&#x1F4C8; 7 <br>
<p>Zhaoyang Lyu, Minghao Guo, Tong Wu, Guodong Xu, Kehuan Zhang, Dahua Lin</p></summary>
<p>

**Abstract:** Recent works have shown that interval bound propagation (IBP) can be used to train verifiably robust neural networks. Reseachers observe an intriguing phenomenon on these IBP trained networks: CROWN, a bounding method based on tight linear relaxation, often gives very loose bounds on these networks. We also observe that most neurons become dead during the IBP training process, which could hurt the representation capability of the network. In this paper, we study the relationship between IBP and CROWN, and prove that CROWN is always tighter than IBP when choosing appropriate bounding lines. We further propose a relaxed version of CROWN, linear bound propagation (LBP), that can be used to verify large networks to obtain lower verified errors than IBP. We also design a new activation function, parameterized ramp function (ParamRamp), which has more diversity of neuron status than ReLU. We conduct extensive experiments on MNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve state-of-the-art verified robustness. Code and the appendix are available at https://github.com/ZhaoyangLyu/VerifiablyRobustNN.

</p>
</details>

<details><summary><b>WakaVT: A Sequential Variational Transformer for Waka Generation</b>
<a href="https://arxiv.org/abs/2104.00426">arxiv:2104.00426</a>
&#x1F4C8; 7 <br>
<p>Yuka Takeishi, Mingxuan Niu, Jing Luo, Zhong Jin, Xinyu Yang</p></summary>
<p>

**Abstract:** Poetry generation has long been a challenge for artificial intelligence. In the scope of Japanese poetry generation, many researchers have paid attention to Haiku generation, but few have focused on Waka generation. To further explore the creative potential of natural language generation systems in Japanese poetry creation, we propose a novel Waka generation model, WakaVT, which automatically produces Waka poems given user-specified keywords. Firstly, an additive mask-based approach is presented to satisfy the form constraint. Secondly, the structures of Transformer and variational autoencoder are integrated to enhance the quality of generated content. Specifically, to obtain novelty and diversity, WakaVT employs a sequence of latent variables, which effectively captures word-level variability in Waka data. To improve linguistic quality in terms of fluency, coherence, and meaningfulness, we further propose the fused multilevel self-attention mechanism, which properly models the hierarchical linguistic structure of Waka. To the best of our knowledge, we are the first to investigate Waka generation with models based on Transformer and/or variational autoencoder. Both objective and subjective evaluation results demonstrate that our model outperforms baselines significantly.

</p>
</details>

<details><summary><b>Decentralized and Model-Free Federated Learning: Consensus-Based Distillation in Function Space</b>
<a href="https://arxiv.org/abs/2104.00352">arxiv:2104.00352</a>
&#x1F4C8; 7 <br>
<p>Akihito Taya, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto</p></summary>
<p>

**Abstract:** This paper proposes a fully decentralized federated learning (FL) scheme for Internet of Everything (IoE) devices that are connected via multi-hop networks. Because FL algorithms hardly converge the parameters of machine learning (ML) models, this paper focuses on the convergence of ML models in function spaces. Considering that the representative loss functions of ML tasks e.g, mean squared error (MSE) and Kullback-Leibler (KL) divergence, are convex functionals, algorithms that directly update functions in function spaces could converge to the optimal solution. The key concept of this paper is to tailor a consensus-based optimization algorithm to work in the function space and achieve the global optimum in a distributed manner. This paper first analyzes the convergence of the proposed algorithm in a function space, which is referred to as a meta-algorithm, and shows that the spectral graph theory can be applied to the function space in a manner similar to that of numerical vectors. Then, consensus-based multi-hop federated distillation (CMFD) is developed for a neural network (NN) to implement the meta-algorithm. CMFD leverages knowledge distillation to realize function aggregation among adjacent devices without parameter averaging. An advantage of CMFD is that it works even with different NN models among the distributed learners. Although CMFD does not perfectly reflect the behavior of the meta-algorithm, the discussion of the meta-algorithm's convergence property promotes an intuitive understanding of CMFD, and simulation evaluations show that NN models converge using CMFD for several tasks. The simulation results also show that CMFD achieves higher accuracy than parameter aggregation for weakly connected networks, and CMFD is more stable than parameter aggregation methods.

</p>
</details>

<details><summary><b>Cursed yet Satisfied Agents</b>
<a href="https://arxiv.org/abs/2104.00835">arxiv:2104.00835</a>
&#x1F4C8; 6 <br>
<p>Yiling Chen, Alon Eden, Juntao Wang</p></summary>
<p>

**Abstract:** In real life auctions, a widely observed phenomenon is the winner's curse -- the winner's high bid implies that the winner often over-estimates the value of the good for sale, resulting in an incurred negative utility. The seminal work of Eyster and Rabin [Econometrica'05] introduced a behavioral model aimed to explain this observed anomaly. We term agents who display this bias "cursed agents". We adopt their model in the interdependent value setting, and aim to devise mechanisms that prevent the cursed agents from obtaining negative utility. We design mechanisms that are cursed ex-post IC, that is, incentivize agents to bid their true signal even though they are cursed, while ensuring that the outcome is individually rational -- the price the agents pay is no more than the agents' true value.
  Since the agents might over-estimate the good's value, such mechanisms might require the seller to make positive transfers to the agents to prevent agents from over-paying. For revenue maximization, we give the optimal deterministic and anonymous mechanism. For welfare maximization, we require ex-post budget balance (EPBB), as positive transfers might lead to negative revenue. We propose a masking operation that takes any deterministic mechanism, and imposes that the seller would not make positive transfers, enforcing EPBB. We show that in typical settings, EPBB implies that the mechanism cannot make any positive transfers, implying that applying the masking operation on the fully efficient mechanism results in a socially optimal EPBB mechanism. This further implies that if the valuation function is the maximum of agents' signals, the optimal EPBB mechanism obtains zero welfare. In contrast, we show that for sum-concave valuations, which include weighted-sum valuations and l_p-norms, the welfare optimal EPBB mechanism obtains half of the optimal welfare as the number of agents grows large.

</p>
</details>

<details><summary><b>Multi-rate attention architecture for fast streamable Text-to-speech spectrum modeling</b>
<a href="https://arxiv.org/abs/2104.00705">arxiv:2104.00705</a>
&#x1F4C8; 6 <br>
<p>Qing He, Zhiping Xiu, Thilo Koehler, Jilong Wu</p></summary>
<p>

**Abstract:** Typical high quality text-to-speech (TTS) systems today use a two-stage architecture, with a spectrum model stage that generates spectral frames and a vocoder stage that generates the actual audio. High-quality spectrum models usually incorporate the encoder-decoder architecture with self-attention or bi-directional long short-term (BLSTM) units. While these models can produce high quality speech, they often incur O($L$) increase in both latency and real-time factor (RTF) with respect to input length $L$. In other words, longer inputs leads to longer delay and slower synthesis speed, limiting its use in real-time applications. In this paper, we propose a multi-rate attention architecture that breaks the latency and RTF bottlenecks by computing a compact representation during encoding and recurrently generating the attention vector in a streaming manner during decoding. The proposed architecture achieves high audio quality (MOS of 4.31 compared to groundtruth 4.48), low latency, and low RTF at the same time. Meanwhile, both latency and RTF of the proposed system stay constant regardless of input lengths, making it ideal for real-time applications.

</p>
</details>

<details><summary><b>Residual Model Learning for Microrobot Control</b>
<a href="https://arxiv.org/abs/2104.00631">arxiv:2104.00631</a>
&#x1F4C8; 6 <br>
<p>Joshua Gruenstein, Tao Chen, Neel Doshi, Pulkit Agrawal</p></summary>
<p>

**Abstract:** A majority of microrobots are constructed using compliant materials that are difficult to model analytically, limiting the utility of traditional model-based controllers. Challenges in data collection on microrobots and large errors between simulated models and real robots make current model-based learning and sim-to-real transfer methods difficult to apply. We propose a novel framework residual model learning (RML) that leverages approximate models to substantially reduce the sample complexity associated with learning an accurate robot model. We show that using RML, we can learn a model of the Harvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively collected interaction data. The learned model is accurate enough to be leveraged as "proxy-simulator" for learning walking and turning behaviors using model-free reinforcement learning algorithms. RML provides a general framework for learning from extremely small amounts of interaction data, and our experiments with HAMR clearly demonstrate that RML substantially outperforms existing techniques.

</p>
</details>

<details><summary><b>Domain-Adversarial Training of Self-Attention Based Networks for Land Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery</b>
<a href="https://arxiv.org/abs/2104.00564">arxiv:2104.00564</a>
&#x1F4C8; 6 <br>
<p>Mauro Martini, Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge</p></summary>
<p>

**Abstract:** The increasing availability of large-scale remote sensing labeled data has prompted researchers to develop increasingly precise and accurate data-driven models for land cover and crop classification (LC&CC). Moreover, with the introduction of self-attention and introspection mechanisms, deep learning approaches have shown promising results in processing long temporal sequences in the multi-spectral domain with a contained computational request. Nevertheless, most practical applications cannot rely on labeled data, and in the field, surveys are a time consuming solution that poses strict limitations to the number of collected samples. Moreover, atmospheric conditions and specific geographical region characteristics constitute a relevant domain gap that does not allow direct applicability of a trained model on the available dataset to the area of interest. In this paper, we investigate adversarial training of deep neural networks to bridge the domain discrepancy between distinct geographical zones. In particular, we perform a thorough analysis of domain adaptation applied to challenging multi-spectral, multi-temporal data, accurately highlighting the advantages of adapting state-of-the-art self-attention based models for LC&CC to different target zones where labeled data are not available. Extensive experimentation demonstrated significant performance and generalization gain in applying domain-adversarial training to source and target regions with marked dissimilarities between the distribution of extracted features.

</p>
</details>

<details><summary><b>Storchastic: A Framework for General Stochastic Automatic Differentiation</b>
<a href="https://arxiv.org/abs/2104.00428">arxiv:2104.00428</a>
&#x1F4C8; 6 <br>
<p>Emile van Krieken, Jakub M. Tomczak, Annette ten Teije</p></summary>
<p>

**Abstract:** Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library at https://github.com/HEmile/storchastic.

</p>
</details>

<details><summary><b>Commonsense Spatial Reasoning for Visually Intelligent Agents</b>
<a href="https://arxiv.org/abs/2104.00387">arxiv:2104.00387</a>
&#x1F4C8; 6 <br>
<p>Agnese Chiatti, Gianluca Bardaro, Enrico Motta, Enrico Daga</p></summary>
<p>

**Abstract:** Service robots are expected to reliably make sense of complex, fast-changing environments. From a cognitive standpoint, they need the appropriate reasoning capabilities and background knowledge required to exhibit human-like Visual Intelligence. In particular, our prior work has shown that the ability to reason about spatial relations between objects in the world is a key requirement for the development of Visually Intelligent Agents. In this paper, we present a framework for commonsense spatial reasoning which is tailored to real-world robotic applications. Differently from prior approaches to qualitative spatial reasoning, the proposed framework is robust to variations in the robot's viewpoint and object orientation. The spatial relations in the proposed framework are also mapped to the types of commonsense predicates used to describe typical object configurations in English. In addition, we also show how this formally-defined framework can be implemented in a concrete spatial database.

</p>
</details>

<details><summary><b>Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems</b>
<a href="https://arxiv.org/abs/2104.00358">arxiv:2104.00358</a>
&#x1F4C8; 6 <br>
<p>Daniel Buschek, Lukas Mecke, Florian Lehmann, Hai Dang</p></summary>
<p>

**Abstract:** This position paper examines potential pitfalls on the way towards achieving human-AI co-creation with generative models in a way that is beneficial to the users' interests. In particular, we collected a set of nine potential pitfalls, based on the literature and our own experiences as researchers working at the intersection of HCI and AI. We illustrate each pitfall with examples and suggest ideas for addressing it. Reflecting on all pitfalls, we discuss and conclude with implications for future research directions. With this collection, we hope to contribute to a critical and constructive discussion on the roles of humans and AI in co-creative interactions, with an eye on related assumptions and potential side-effects for creative practices and beyond.

</p>
</details>

<details><summary><b>Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation</b>
<a href="https://arxiv.org/abs/2104.00308">arxiv:2104.00308</a>
&#x1F4C8; 6 <br>
<p>Rongjie Li, Songyang Zhang, Bo Wan, Xuming He</p></summary>
<p>

**Abstract:** Scene graph generation is an important visual understanding task with a broad range of vision applications. Despite recent tremendous progress, it remains challenging due to the intrinsic long-tailed class distribution and large intra-class variation. To address these issues, we introduce a novel confidence-aware bipartite graph neural network with adaptive message propagation mechanism for unbiased scene graph generation. In addition, we propose an efficient bi-level data resampling strategy to alleviate the imbalanced data distribution problem in training our graph network. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including Visual Genome, Open Images V4/V6, demonstrating its effectiveness and generality.

</p>
</details>

<details><summary><b>Perspective, Survey and Trends: Public Driving Datasets and Toolsets for Autonomous Driving Virtual Test</b>
<a href="https://arxiv.org/abs/2104.00273">arxiv:2104.00273</a>
&#x1F4C8; 6 <br>
<p>Pengliang Ji, Li Ruan, Yunzhi Xue, Limin Xiao, Qian Dong</p></summary>
<p>

**Abstract:** Owing to the merits of early safety and reliability guarantee, autonomous driving virtual testing has recently gains increasing attention compared with closed-loop testing in real scenarios. Although the availability and quality of autonomous driving datasets and toolsets are the premise to diagnose the autonomous driving system bottlenecks and improve the system performance, due to the diversity and privacy of the datasets and toolsets, collecting and featuring the perspective and quality of them become not only time-consuming but also increasingly challenging. This paper first proposes a Systematic Literature review approach for Autonomous driving tests (SLA), then presents an overview of existing publicly available datasets and toolsets from 2000 to 2020. Quantitative findings with the scenarios concerned, perspectives and trend inferences and suggestions with 35 automated driving test tool sets and 70 test data sets are also presented. To the best of our knowledge, we are the first to perform such recent empirical survey on both the datasets and toolsets using a SLA based survey approach. Our multifaceted analyses and new findings not only reveal insights that we believe are useful for system designers, practitioners and users, but also can promote more researches on a systematic survey analysis in autonomous driving surveys on dataset and toolsets.

</p>
</details>

<details><summary><b>Visualizing computation in large-scale cellular automata</b>
<a href="https://arxiv.org/abs/2104.01008">arxiv:2104.01008</a>
&#x1F4C8; 5 <br>
<p>Hugo Cisneros, Josef Sivic, Tomas Mikolov</p></summary>
<p>

**Abstract:** Emergent processes in complex systems such as cellular automata can perform computations of increasing complexity, and could possibly lead to artificial evolution. Such a feat would require scaling up current simulation sizes to allow for enough computational capacity. Understanding complex computations happening in cellular automata and other systems capable of emergence poses many challenges, especially in large-scale systems. We propose methods for coarse-graining cellular automata based on frequency analysis of cell states, clustering and autoencoders. These innovative techniques facilitate the discovery of large-scale structure formation and complexity analysis in those systems. They emphasize interesting behaviors in elementary cellular automata while filtering out background patterns. Moreover, our methods reduce large 2D automata to smaller sizes and enable identifying systems that behave interestingly at multiple scales.

</p>
</details>

<details><summary><b>Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping</b>
<a href="https://arxiv.org/abs/2104.00878">arxiv:2104.00878</a>
&#x1F4C8; 5 <br>
<p>Yantian Zha, Siddhant Bhambri, Lin Guan</p></summary>
<p>

**Abstract:** Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work, our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective, there is a close association between attention and affordance. Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks, instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment. Our project website can be accessed at https://sites.google.com/asu.edu/affordance-aware-imitation/project.

</p>
</details>

<details><summary><b>Out of a hundred trials, how many errors does your speaker verifier make?</b>
<a href="https://arxiv.org/abs/2104.00732">arxiv:2104.00732</a>
&#x1F4C8; 5 <br>
<p>Niko Brümmer, Luciana Ferrer, Albert Swart</p></summary>
<p>

**Abstract:** Out of a hundred trials, how many errors does your speaker verifier make? For the user this is an important, practical question, but researchers and vendors typically sidestep it and supply instead the conditional error-rates that are given by the ROC/DET curve. We posit that the user's question is answered by the Bayes error-rate. We present a tutorial to show how to compute the error-rate that results when making Bayes decisions with calibrated likelihood ratios, supplied by the verifier, and an hypothesis prior, supplied by the user. For perfect calibration, the Bayes error-rate is upper bounded by min(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior probabilities of the competing hypotheses. The EER represents the accuracy of the verifier, while min(P,1-P) represents the hardness of the classification problem. We further show how the Bayes error-rate can be computed also for non-perfect calibration and how to generalize from error-rate to expected cost. We offer some criticism of decisions made by direct score thresholding. Finally, we demonstrate by analyzing error-rates of the recently published DCA-PLDA speaker verifier.

</p>
</details>

<details><summary><b>Holdout-Based Fidelity and Privacy Assessment of Mixed-Type Synthetic Data</b>
<a href="https://arxiv.org/abs/2104.00635">arxiv:2104.00635</a>
&#x1F4C8; 5 <br>
<p>Michael Platzer, Thomas Reutterer</p></summary>
<p>

**Abstract:** AI-based data synthesis has seen rapid progress over the last several years, and is increasingly recognized for its promise to enable privacy-respecting high-fidelity data sharing. However, adequately evaluating the quality of generated synthetic datasets is still an open challenge. We introduce and demonstrate a holdout-based empirical assessment framework for quantifying the fidelity as well as the privacy risk of synthetic data solutions for mixed-type tabular data. Measuring fidelity is based on statistical distances of lower-dimensional marginal distributions, which provide a model-free and easy-to-communicate empirical metric for the representativeness of a synthetic dataset. Privacy risk is assessed by calculating the individual-level distances to closest record with respect to the training data. By showing that the synthetic samples are just as close to the training as to the holdout data, we yield strong evidence that the synthesizer indeed learned to generalize patterns and is independent of individual training records. We demonstrate the presented framework for seven distinct synthetic data solutions across four mixed-type datasets and compare these to more traditional statistical disclosure techniques. The results highlight the need to systematically assess the fidelity just as well as the privacy of these emerging class of synthetic data generators.

</p>
</details>

<details><summary><b>Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features</b>
<a href="https://arxiv.org/abs/2104.00629">arxiv:2104.00629</a>
&#x1F4C8; 5 <br>
<p>Florian Pargent, Florian Pfisterer, Janek Thomas, Bernd Bischl</p></summary>
<p>

**Abstract:** Because most machine learning (ML) algorithms are designed for numerical inputs, efficiently encoding categorical variables is a crucial aspect during data analysis. An often encountered problem are high cardinality features, i.e. unordered categorical predictor variables with a high number of levels. We study techniques that yield numeric representations of categorical variables which can then be used in subsequent ML applications. We focus on the impact of those techniques on a subsequent algorithm's predictive performance, and -- if possible -- derive best practices on when to use which technique. We conducted a large-scale benchmark experiment, where we compared different encoding strategies together with five ML algorithms (lasso, random forest, gradient boosting, k-nearest neighbours, support vector machine) using datasets from regression, binary- and multiclass- classification settings. Throughout our study, regularized versions of target encoding (i.e. using target predictions based on the feature levels in the training set as a new numerical feature) consistently provided the best results. Traditional encodings that make unreasonable assumptions to map levels to integers (e.g. integer encoding) or to reduce the number of levels (possibly based on target information, e.g. leaf encoding) before creating binary indicator variables (one-hot or dummy encoding) were not as effective.

</p>
</details>

<details><summary><b>Model Selection's Disparate Impact in Real-World Deep Learning Applications</b>
<a href="https://arxiv.org/abs/2104.00606">arxiv:2104.00606</a>
&#x1F4C8; 5 <br>
<p>Jessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir-Aggrey, Chris De Sa, Michael Littman</p></summary>
<p>

**Abstract:** Algorithmic fairness has emphasized the role of biased data in automated decision outcomes. Recently, there has been a shift in attention to sources of bias that implicate fairness in other stages in the ML pipeline. We contend that one source of such bias, human preferences in model selection, remains under-explored in terms of its role in disparate impact across demographic groups. Using a deep learning model trained on real-world medical imaging data, we verify our claim empirically and argue that choice of metric for model comparison, especially those that do not take variability into account, can significantly bias model selection outcomes.

</p>
</details>

<details><summary><b>Learning with Neural Tangent Kernels in Near Input Sparsity Time</b>
<a href="https://arxiv.org/abs/2104.00415">arxiv:2104.00415</a>
&#x1F4C8; 5 <br>
<p>Amir Zandieh</p></summary>
<p>

**Abstract:** The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide neural nets trained under least squares loss by gradient descent. However, despite its importance, the super-quadratic runtime of kernel methods limits the use of NTK in large-scale learning tasks. To accelerate kernel machines with NTK, we propose a near input sparsity time algorithm that maps the input data to a randomized low-dimensional feature space so that the inner product of the transformed data approximates their NTK evaluation. Our transformation works by sketching the polynomial expansions of arc-cosine kernels. Furthermore, we propose a feature map for approximating the convolutional counterpart of the NTK, which can transform any image using a runtime that is only linear in the number of pixels. We show that in standard large-scale regression and classification tasks a linear regressor trained on our features outperforms trained Neural Nets and Nystrom approximation of NTK kernel.

</p>
</details>

<details><summary><b>Mitigating Media Bias through Neutral Article Generation</b>
<a href="https://arxiv.org/abs/2104.00336">arxiv:2104.00336</a>
&#x1F4C8; 5 <br>
<p>Nayeon Lee, Yejin Bang, Andrea Madotto, Pascale Fung</p></summary>
<p>

**Abstract:** Media bias can lead to increased political polarization, and thus, the need for automatic mitigation methods is growing. Existing mitigation work displays articles from multiple news outlets to provide diverse news coverage, but without neutralizing the bias inherent in each of the displayed articles. Therefore, we propose a new task, a single neutralized article generation out of multiple biased articles, to facilitate more efficient access to balanced and unbiased information. In this paper, we compile a new dataset NeuWS, define an automatic evaluation metric, and provide baselines and multiple analyses to serve as a solid starting point for the proposed task. Lastly, we obtain a human evaluation to demonstrate the alignment between our metric and human judgment.

</p>
</details>

<details><summary><b>Students are the Best Teacher: Exit-Ensemble Distillation with Multi-Exits</b>
<a href="https://arxiv.org/abs/2104.00299">arxiv:2104.00299</a>
&#x1F4C8; 5 <br>
<p>Hojung Lee, Jong-Seok Lee</p></summary>
<p>

**Abstract:** This paper proposes a novel knowledge distillation-based learning method to improve the classification performance of convolutional neural networks (CNNs) without a pre-trained teacher network, called exit-ensemble distillation. Our method exploits the multi-exit architecture that adds auxiliary classifiers (called exits) in the middle of a conventional CNN, through which early inference results can be obtained. The idea of our method is to train the network using the ensemble of the exits as the distillation target, which greatly improves the classification performance of the overall network. Our method suggests a new paradigm of knowledge distillation; unlike the conventional notion of distillation where teachers only teach students, we show that students can also help other students and even the teacher to learn better. Experimental results demonstrate that our method achieves significant improvement of classification performance on various popular CNN architectures (VGG, ResNet, ResNeXt, WideResNet, etc.). Furthermore, the proposed method can expedite the convergence of learning with improved stability. Our code will be available on Github.

</p>
</details>

<details><summary><b>PolyDNN: Polynomial Representation of NN for Communication-less SMPC Inference</b>
<a href="https://arxiv.org/abs/2104.00863">arxiv:2104.00863</a>
&#x1F4C8; 4 <br>
<p>Philip Derbeko, Shlomi Dolev</p></summary>
<p>

**Abstract:** The structure and weights of Deep Neural Networks (DNN) typically encode and contain very valuable information about the dataset that was used to train the network.
  One way to protect this information when DNN is published is to perform an interference of the network using secure multi-party computations (MPC).
  In this paper, we suggest a translation of deep neural networks to polynomials, which are easier to calculate efficiently with MPC techniques.
  We show a way to translate complete networks into a single polynomial and how to calculate the polynomial with an efficient and information-secure MPC algorithm.
  The calculation is done without intermediate communication between the participating parties, which is beneficial in several cases, as explained in the paper.

</p>
</details>

<details><summary><b>A Combined Deep Learning based End-to-End Video Coding Architecture for YUV Color Space</b>
<a href="https://arxiv.org/abs/2104.00807">arxiv:2104.00807</a>
&#x1F4C8; 4 <br>
<p>Ankitesh K. Singh, Hilmi E. Egilmez, Reza Pourreza, Muhammed Coban, Marta Karczewicz, Taco S. Cohen</p></summary>
<p>

**Abstract:** Most of the existing deep learning based end-to-end video coding (DLEC) architectures are designed specifically for RGB color format, yet the video coding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over past few decades, have been designed primarily for YUV 4:2:0 format, where the chrominance (U and V) components are subsampled to achieve superior compression performances considering the human visual system. While a broad number of papers on DLEC compare these two distinct coding schemes in RGB domain, it is ideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair comparison. This paper introduces a new DLEC architecture for video coding to effectively support YUV 4:2:0 and compares its performance against the HEVC standard under a common evaluation framework. The experimental results on YUV 4:2:0 video sequences show that the proposed architecture can outperform HEVC in intra-frame coding, however inter-frame coding is not as efficient on contrary to the RGB coding results reported in recent papers.

</p>
</details>

<details><summary><b>No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks</b>
<a href="https://arxiv.org/abs/2104.00795">arxiv:2104.00795</a>
&#x1F4C8; 4 <br>
<p>Shyamgopal Karthik, Ameya Prabhu, Puneet K. Dokania, Vineet Gandhi</p></summary>
<p>

**Abstract:** There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-$k$ predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.

</p>
</details>

<details><summary><b>Drug Discovery Approaches using Quantum Machine Learning</b>
<a href="https://arxiv.org/abs/2104.00746">arxiv:2104.00746</a>
&#x1F4C8; 4 <br>
<p>Junde Li, Mahabubul Alam, Congzhou M Sha, Jian Wang, Nikolay V. Dokholyan, Swaroop Ghosh</p></summary>
<p>

**Abstract:** Traditional drug discovery pipeline takes several years and cost billions of dollars. Deep generative and predictive models are widely adopted to assist in drug development. Classical machines cannot efficiently produce atypical patterns of quantum computers which might improve the training quality of learning tasks. We propose a suite of quantum machine learning techniques e.g., generative adversarial network (GAN), convolutional neural network (CNN) and variational auto-encoder (VAE) to generate small drug molecules, classify binding pockets in proteins, and generate large drug molecules, respectively.

</p>
</details>

<details><summary><b>HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection</b>
<a href="https://arxiv.org/abs/2104.00639">arxiv:2104.00639</a>
&#x1F4C8; 4 <br>
<p>Rafel Palliser-Sans, Albert Rial-Farràs</p></summary>
<p>

**Abstract:** This paper presents our submission to SemEval-2021 Task 5: Toxic Spans Detection. The purpose of this task is to detect the spans that make a text toxic, which is a complex labour for several reasons. Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually. Following this idea of focusing on both single words and multi-word expressions, we study the impact of using a multi-depth DistilBERT model, which uses embeddings from different layers to estimate the final per-token toxicity. Our quantitative results show that using information from multiple depths boosts the performance of the model. Finally, we also analyze our best model qualitatively.

</p>
</details>

<details><summary><b>Text to Image Generation with Semantic-Spatial Aware GAN</b>
<a href="https://arxiv.org/abs/2104.00567">arxiv:2104.00567</a>
&#x1F4C8; 4 <br>
<p>Kai Hu, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn</p></summary>
<p>

**Abstract:** A text to image generation (T2I) model aims to generate photo-realistic images which are semantically consistent with the text descriptions. Built upon the recent advances in generative adversarial networks (GANs), existing T2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) The condition batch normalization methods are applied on the whole image feature maps equally, ignoring the local semantics; (2) The text encoder is fixed during training, which should be trained with the image generator jointly to learn better text representations for image generation. To address these limitations, we propose a novel framework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion so that the text encoder can exploit better text information. Concretely, we introduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a mask map in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code is available at https://github.com/wtliao/text2image.

</p>
</details>

<details><summary><b>Touch-based Curiosity for Sparse-Reward Tasks</b>
<a href="https://arxiv.org/abs/2104.00442">arxiv:2104.00442</a>
&#x1F4C8; 4 <br>
<p>Sai Rajeswar, Cyril Ibrahim, Nitin Surya, Florian Golemo, David Vazquez, Aaron Courville, Pedro O. Pinheiro</p></summary>
<p>

**Abstract:** Robots in many real-world settings have access to force/torque sensors in their gripper and tactile sensing is often necessary in tasks that involve contact-rich motion. In this work, we leverage surprise from mismatches in touch feedback to guide exploration in hard sparse-reward reinforcement learning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible objects interactions are supposed to "feel" like. We encourage exploration by rewarding interactions where the expectation and the experience don't match. In our proposed method, an initial task-independent exploration phase is followed by an on-task learning phase, in which the original interactions are relabeled with on-task rewards. We test our approach on a range of touch-intensive robot arm tasks (e.g. pushing objects, opening doors), which we also release as part of this work. Across multiple experiments in a simulated setting, we demonstrate that our method is able to learn these difficult tasks through sparse reward and curiosity alone. We compare our cross-modal approach to single-modality (touch- or vision-only) approaches as well as other curiosity-based methods and find that our method performs better and is more sample-efficient.

</p>
</details>

<details><summary><b>Federated Few-Shot Learning with Adversarial Learning</b>
<a href="https://arxiv.org/abs/2104.00365">arxiv:2104.00365</a>
&#x1F4C8; 4 <br>
<p>Chenyou Fan, Jianwei Huang</p></summary>
<p>

**Abstract:** We are interested in developing a unified machine learning model over many mobile devices for practical learning tasks, where each device only has very few training data. This is a commonly encountered situation in mobile computing scenarios, where data is scarce and distributed while the tasks are distinct. In this paper, we propose a federated few-shot learning (FedFSL) framework to learn a few-shot classification model that can classify unseen data classes with only a few labeled samples. With the federated learning strategy, FedFSL can utilize many data sources while keeping data privacy and communication efficiency. There are two technical challenges: 1) directly using the existing federated learning approach may lead to misaligned decision boundaries produced by client models, and 2) constraining the decision boundaries to be similar over clients would overfit to training tasks but not adapt well to unseen tasks. To address these issues, we propose to regularize local updates by minimizing the divergence of client models. We also formulate the training in an adversarial fashion and optimize the client models to produce a discriminative feature space that can better represent unseen data samples. We demonstrate the intuitions and conduct experiments to show our approaches outperform baselines by more than 10% in learning vision tasks and 5% in language tasks.

</p>
</details>

<details><summary><b>Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction</b>
<a href="https://arxiv.org/abs/2104.00312">arxiv:2104.00312</a>
&#x1F4C8; 4 <br>
<p>Luoqiu Li, Xiang Chen, Zhen Bi, Xin Xie, Shumin Deng, Ningyu Zhang, Chuanqi Tan, Mosha Chen, Huajun Chen</p></summary>
<p>

**Abstract:** Recent neural-based relation extraction approaches, though achieving promising improvement on benchmark datasets, have reported their vulnerability towards adversarial attacks. Thus far, efforts mostly focused on generating adversarial samples or defending adversarial attacks, but little is known about the difference between normal and adversarial samples. In this work, we take the first step to leverage the salience-based method to analyze those adversarial samples. We observe that salience tokens have a direct correlation with adversarial perturbations. We further find the adversarial perturbations are either those tokens not existing in the training set or superficial cues associated with relation labels. To some extent, our approach unveils the characters against adversarial samples. We release an open-source testbed, "DiagnoseAdv" in https://github.com/zjunlp/DiagnoseAdv.

</p>
</details>

<details><summary><b>A proof of convergence for stochastic gradient descent in the training of artificial neural networks with ReLU activation for constant target functions</b>
<a href="https://arxiv.org/abs/2104.00277">arxiv:2104.00277</a>
&#x1F4C8; 4 <br>
<p>Arnulf Jentzen, Adrian Riekert</p></summary>
<p>

**Abstract:** In this article we study the stochastic gradient descent (SGD) optimization method in the training of fully-connected feedforward artificial neural networks with ReLU activation. The main result of this work proves that the risk of the SGD process converges to zero if the target function under consideration is constant. In the established convergence result the considered artificial neural networks consist of one input layer, one hidden layer, and one output layer (with $d \in \mathbb{N}$ neurons on the input layer, $H \in \mathbb{N}$ neurons on the hidden layer, and one neuron on the output layer). The learning rates of the SGD process are assumed to be sufficiently small and the input data used in the SGD process to train the artificial neural networks is assumed to be independent and identically distributed.

</p>
</details>

<details><summary><b>Neurological Status Classification Using Convolutional Neural Network</b>
<a href="https://arxiv.org/abs/2104.02058">arxiv:2104.02058</a>
&#x1F4C8; 3 <br>
<p>Mehrad Jaloli, Divya Choudhary, Marzia Cescon</p></summary>
<p>

**Abstract:** In this study we show that a Convolutional Neural Network (CNN) model is able to accuratelydiscriminate between 4 different phases of neurological status in a non-Electroencephalogram(EEG) dataset recorded in an experiment in which subjects are exposed to physical, cognitiveand emotional stress. We demonstrate that the proposed model is able to obtain 99.99% AreaUnder the Curve (AUC) of Receiver Operation characteristic (ROC) and 99.82% classificationaccuracy on the test dataset. Furthermore, for comparison, we show that our models outperformstraditional classification methods such as SVM, and RF. Finally, we show the advantage of CNN models, in comparison to other methods, in robustness to noise by 97.46% accuracy on a noisy dataset.

</p>
</details>

<details><summary><b>Quick Line Outage Identification in Urban Distribution Grids via Smart Meters</b>
<a href="https://arxiv.org/abs/2104.02056">arxiv:2104.02056</a>
&#x1F4C8; 3 <br>
<p>Yizheng Liao, Yang Weng, Chin-woo Tan, Ram Rajagopal</p></summary>
<p>

**Abstract:** The growing integration of distributed energy resources (DERs) in distribution grids raises various reliability issues due to DER's uncertain and complex behaviors. With a large-scale DER penetration in distribution grids, traditional outage detection methods, which rely on customers report and smart meters' last gasp signals, will have poor performance, because the renewable generators and storages and the mesh structure in urban distribution grids can continue supplying power after line outages. To address these challenges, we propose a data-driven outage monitoring approach based on the stochastic time series analysis with a theoretical guarantee. Specifically, we prove via power flow analysis that the dependency of time-series voltage measurements exhibits significant statistical changes after line outages. This makes the theory on optimal change-point detection suitable to identify line outages. However, existing change point detection methods require post-outage voltage distribution, which is unknown in distribution systems. Therefore, we design a maximum likelihood estimator to directly learn the distribution parameters from voltage data. We prove that the estimated parameters-based detection also achieves the optimal performance, making it extremely useful for fast distribution grid outage identifications. Furthermore, since smart meters have been widely installed in distribution grids and advanced infrastructure (e.g., PMU) has not widely been available, our approach only requires voltage magnitude for quick outage identification. Simulation results show highly accurate outage identification in eight distribution grids with 14 configurations with and without DERs using smart meter data.

</p>
</details>

<details><summary><b>Self-supervised Video Representation Learning by Context and Motion Decoupling</b>
<a href="https://arxiv.org/abs/2104.00862">arxiv:2104.00862</a>
&#x1F4C8; 3 <br>
<p>Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, Rong Jin</p></summary>
<p>

**Abstract:** A key challenge in self-supervised video representation learning is how to effectively capture motion information besides context bias. While most existing works implicitly achieve this with video-specific pretext tasks (e.g., predicting clip orders, time arrows, and paces), we develop a method that explicitly decouples motion supervision from context bias through a carefully designed pretext task. Specifically, we take the keyframes and motion vectors in compressed videos (e.g., in H.264 format) as the supervision sources for context and motion, respectively, which can be efficiently extracted at over 500 fps on the CPU. Then we design two pretext tasks that are jointly optimized: a context matching task where a pairwise contrastive loss is cast between video clip and keyframe features; and a motion prediction task where clip features, passed through an encoder-decoder network, are used to estimate motion features in a near future. These two tasks use a shared video backbone and separate MLP heads. Experiments show that our approach improves the quality of the learned video representation over previous works, where we obtain absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Moreover, we find the motion prediction to be a strong regularization for video networks, where using it as an auxiliary task improves the accuracy of action recognition with a margin of 7.4%~13.8%.

</p>
</details>

<details><summary><b>Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation</b>
<a href="https://arxiv.org/abs/2104.00850">arxiv:2104.00850</a>
&#x1F4C8; 3 <br>
<p>Alessandra Lumini, Loris Nanni, Gianluca Maguolo</p></summary>
<p>

**Abstract:** Semantic segmentation has a wide array of applications ranging from medical-image analysis, scene understanding, autonomous driving and robotic navigation. This work deals with medical image segmentation and in particular with accurate polyp detection and segmentation during colonoscopy examinations. Several convolutional neural network architectures have been proposed to effectively deal with this task and with the problem of segmenting objects at different scale input. The basic architecture in image segmentation consists of an encoder and a decoder: the first uses convolutional filters to extract features from the image, the second is responsible for generating the final output. In this work, we compare some variant of the DeepLab architecture obtained by varying the decoder backbone. We compare several decoder architectures, including ResNet, Xception, EfficentNet, MobileNet and we perturb their layers by substituting ReLU activation layers with other functions. The resulting methods are used to create deep ensembles which are shown to be very effective. Our experimental evaluations show that our best ensemble produces good segmentation results by achieving high evaluation scores with a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of 0.818 for the Kvasir-SEG dataset. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.

</p>
</details>

<details><summary><b>Choice-Aware User Engagement Modeling andOptimization on Social Media</b>
<a href="https://arxiv.org/abs/2104.00801">arxiv:2104.00801</a>
&#x1F4C8; 3 <br>
<p>Saketh Reddy Karra, Theja Tulabandhula</p></summary>
<p>

**Abstract:** We address the problem of maximizing user engagement with content (in the form of like, reply, retweet, and retweet with comments)on the Twitter platform. We formulate the engagement forecasting task as a multi-label classification problem that captures choice behavior on an unsupervised clustering of tweet-topics. We propose a neural network architecture that incorporates user engagement history and predicts choice conditional on this context. We study the impact of recommend-ing tweets on engagement outcomes by solving an appropriately defined sweet optimization problem based on the proposed model using a large dataset obtained from Twitter.

</p>
</details>

<details><summary><b>GABO: Graph Augmentations with Bi-level Optimization</b>
<a href="https://arxiv.org/abs/2104.00722">arxiv:2104.00722</a>
&#x1F4C8; 3 <br>
<p>Heejung W. Chung, Avoy Datta, Chris Waites</p></summary>
<p>

**Abstract:** Data augmentation refers to a wide range of techniques for improving model generalization by augmenting training examples. Oftentimes such methods require domain knowledge about the dataset at hand, spawning a plethora of recent literature surrounding automated techniques for data augmentation. In this work we apply one such method, bilevel optimization, to tackle the problem of graph classification on the ogbg-molhiv dataset. Our best performing augmentation achieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which makes it the most effective augmenter for this classifier on the leaderboard. This framework combines a GIN layer augmentation generator with a bias transformation and outperforms the same classifier augmented using the state-of-the-art FLAG augmentation.

</p>
</details>

<details><summary><b>Sampling and Filtering of Neural Machine Translation Distillation Data</b>
<a href="https://arxiv.org/abs/2104.00664">arxiv:2104.00664</a>
&#x1F4C8; 3 <br>
<p>Vilém Zouhar</p></summary>
<p>

**Abstract:** In most of neural machine translation distillation or stealing scenarios, the goal is to preserve the performance of the target model (teacher). The highest-scoring hypothesis of the teacher model is commonly used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be upsampled and poor hypotheses either removed or undersampled.
  This paper explores the importance sampling method landscape (pruning, hypothesis upsampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful upsampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.

</p>
</details>

<details><summary><b>Evidence-based Verification for Real World Information Needs</b>
<a href="https://arxiv.org/abs/2104.00640">arxiv:2104.00640</a>
&#x1F4C8; 3 <br>
<p>James Thorne, Max Glockner, Gisela Vallejo, Andreas Vlachos, Iryna Gurevych</p></summary>
<p>

**Abstract:** Claim verification is the task of predicting the veracity of written statements against evidence. Previous large-scale datasets model the task as classification, ignoring the need to retrieve evidence, or are constructed for research purposes, and may not be representative of real-world needs. In this paper, we introduce a novel claim verification dataset with instances derived from search-engine queries, yielding 10,987 claims annotated with evidence that represent real-world information needs. For each claim, we annotate evidence from full Wikipedia articles with both section and sentence-level granularity. Our annotation allows comparison between two complementary approaches to verification: stance classification, and evidence extraction followed by entailment recognition. In our comprehensive evaluation, we find no significant difference in accuracy between these two approaches. This enables systems to use evidence extraction to summarize a rationale for an end-user while maintaining the accuracy when predicting a claim's veracity. With challenging claims and evidence documents containing hundreds of sentences, our dataset presents interesting challenges that are not captured in previous work -- evidenced through transfer learning experiments. We release code and data to support further research on this task.

</p>
</details>

<details><summary><b>Physics-informed neural networks for the shallow-water equations on the sphere</b>
<a href="https://arxiv.org/abs/2104.00615">arxiv:2104.00615</a>
&#x1F4C8; 3 <br>
<p>Alex Bihlo, Roman O. Popovych</p></summary>
<p>

**Abstract:** We propose the use of physics-informed neural networks for solving the shallow-water equations on the sphere. Physics-informed neural networks are trained to satisfy the differential equations along with the prescribed initial and boundary data, and thus can be seen as an alternative approach to solving differential equations compared to traditional numerical approaches such as finite difference, finite volume or spectral methods. We discuss the training difficulties of physics-informed neural networks for the shallow-water equations on the sphere and propose a simple multi-model approach to tackle test cases of comparatively long time intervals. We illustrate the abilities of the method by solving the most prominent test cases proposed by Williamson et al. [J. Comput. Phys. 102, 211-224, 1992].

</p>
</details>

<details><summary><b>Model Selection for Time Series Forecasting: Empirical Analysis of Different Estimators</b>
<a href="https://arxiv.org/abs/2104.00584">arxiv:2104.00584</a>
&#x1F4C8; 3 <br>
<p>Vitor Cerqueira, Luis Torgo, Carlos Soares</p></summary>
<p>

**Abstract:** Evaluating predictive models is a crucial task in predictive analytics. This process is especially challenging with time series data where the observations show temporal dependencies. Several studies have analysed how different performance estimation methods compare with each other for approximating the true loss incurred by a given forecasting model. However, these studies do not address how the estimators behave for model selection: the ability to select the best solution among a set of alternatives. We address this issue and compare a set of estimation methods for model selection in time series forecasting tasks. We attempt to answer two main questions: (i) how often is the best possible model selected by the estimators; and (ii) what is the performance loss when it does not. We empirically found that the accuracy of the estimators for selecting the best solution is low, and the overall forecasting performance loss associated with the model selection process ranges from 1.2% to 2.3%. We also discovered that some factors, such as the sample size, are important in the relative performance of the estimators.

</p>
</details>

<details><summary><b>SpectralNET: Exploring Spatial-Spectral WaveletCNN for Hyperspectral Image Classification</b>
<a href="https://arxiv.org/abs/2104.00341">arxiv:2104.00341</a>
&#x1F4C8; 3 <br>
<p>Tanmay Chakraborty, Utkarsh Trehan</p></summary>
<p>

**Abstract:** Hyperspectral Image (HSI) classification using Convolutional Neural Networks (CNN) is widely found in the current literature. Approaches vary from using SVMs to 2D CNNs, 3D CNNs, 3D-2D CNNs. Besides 3D-2D CNNs and FuSENet, the other approaches do not consider both the spectral and spatial features together for HSI classification task, thereby resulting in poor performances. 3D CNNs are computationally heavy and are not widely used, while 2D CNNs do not consider multi-resolution processing of images, and only limits itself to the spatial features. Even though 3D-2D CNNs try to model the spectral and spatial features their performance seems limited when applied over multiple dataset. In this article, we propose SpectralNET, a wavelet CNN, which is a variation of 2D CNN for multi-resolution HSI classification. A wavelet CNN uses layers of wavelet transform to bring out spectral features. Computing a wavelet transform is lighter than computing 3D CNN. The spectral features extracted are then connected to the 2D CNN which bring out the spatial features, thereby creating a spatial-spectral feature vector for classification. Overall a better model is achieved that can classify multi-resolution HSI data with high accuracy. Experiments performed with SpectralNET on benchmark dataset, i.e. Indian Pines, University of Pavia, and Salinas Scenes confirm the superiority of proposed SpectralNET with respect to the state-of-the-art methods. The code is publicly available in https://github.com/tanmay-ty/SpectralNET.

</p>
</details>

<details><summary><b>Domain Invariant Adversarial Learning</b>
<a href="https://arxiv.org/abs/2104.00322">arxiv:2104.00322</a>
&#x1F4C8; 3 <br>
<p>Matan Levi, Idan Attias, Aryeh Kontorovich</p></summary>
<p>

**Abstract:** The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities of deep neural networks. Among the variety of techniques introduced to surmount this inherent weakness, adversarial training has emerged as the most effective strategy to achieve robustness. Typically, this is achieved by balancing robust and natural objectives. In this work, we aim to further optimize the trade-off between robust and standard accuracy by enforcing a domain-invariant feature representation. We present a new adversarial training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature representation that is both robust and domain invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial domain. In the case where the source domain consists of natural examples and the target domain is the adversarially perturbed examples, our method learns a feature representation constrained not to discriminate between the natural and adversarial examples, and can therefore achieve a more robust representation. Our experiments indicate that our method improves both robustness and standard accuracy, when compared to other state-of-the-art adversarial training methods.

</p>
</details>

<details><summary><b>Detecting over/under-translation errors for determining adequacy in human translations</b>
<a href="https://arxiv.org/abs/2104.00267">arxiv:2104.00267</a>
&#x1F4C8; 3 <br>
<p>Prabhakar Gupta, Ridha Juneja, Anil Nelakanti, Tamojit Chatterjee</p></summary>
<p>

**Abstract:** We present a novel approach to detecting over and under translations (OT/UT) as part of adequacy error checks in translation evaluation. We do not restrict ourselves to machine translation (MT) outputs and specifically target applications with human generated translation pipeline. The goal of our system is to identify OT/UT errors from human translated video subtitles with high error recall. We achieve this without reference translations by learning a model on synthesized training data. We compare various classification networks that we trained on embeddings from pre-trained language model with our best hybrid network of GRU + CNN achieving 89.3% accuracy on high-quality human-annotated evaluation data in 8 languages.

</p>
</details>

<details><summary><b>UAV-Assisted Communication in Remote Disaster Areas using Imitation Learning</b>
<a href="https://arxiv.org/abs/2105.12823">arxiv:2105.12823</a>
&#x1F4C8; 2 <br>
<p>Alireza Shamsoshoara, Fatemeh Afghah, Erik Blasch, Jonathan Ashdown, Mehdi Bennis</p></summary>
<p>

**Abstract:** The damage to cellular towers during natural and man-made disasters can disturb the communication services for cellular users. One solution to the problem is using unmanned aerial vehicles to augment the desired communication network. The paper demonstrates the design of a UAV-Assisted Imitation Learning (UnVAIL) communication system that relays the cellular users' information to a neighbor base station. Since the user equipment (UEs) are equipped with buffers with limited capacity to hold packets, UnVAIL alternates between different UEs to reduce the chance of buffer overflow, positions itself optimally close to the selected UE to reduce service time, and uncovers a network pathway by acting as a relay node. UnVAIL utilizes Imitation Learning (IL) as a data-driven behavioral cloning approach to accomplish an optimal scheduling solution. Results demonstrate that UnVAIL performs similar to a human expert knowledge-based planning in communication timeliness, position accuracy, and energy consumption with an accuracy of 97.52% when evaluated on a developed simulator to train the UAV.

</p>
</details>

<details><summary><b>An artificial intelligence and Internet of things based automated irrigation system</b>
<a href="https://arxiv.org/abs/2104.04076">arxiv:2104.04076</a>
&#x1F4C8; 2 <br>
<p>Ömer Aydin, Cem Ali Kandemir, Umut Kiraç, Feriştah Dalkiliç</p></summary>
<p>

**Abstract:** It is not hard to see that the need for clean water is growing by considering the decrease of the water sources day by day in the world. Potable fresh water is also used for irrigation, so it should be planned to decrease freshwater wastage. With the development of technology and the availability of cheaper and more effective solutions, the efficiency of irrigation increased and the water loss can be reduced. In particular, Internet of things (IoT) devices has begun to be used in all areas. We can easily and precisely collect temperature, humidity and mineral values from the irrigation field with the IoT devices and sensors. Most of the operations and decisions about irrigation are carried out by people. For people, it is hard to have all the real-time data such as temperature, moisture and mineral levels in the decision-making process and make decisions by considering them. People usually make decisions with their experience. In this study, a wide range of information from the irrigation field was obtained by using IoT devices and sensors. Data collected from IoT devices and sensors sent via communication channels and stored on MongoDB. With the help of Weka software, the data was normalized and the normalized data was used as a learning set. As a result of the examinations, a decision tree (J48) algorithm with the highest accuracy was chosen and an artificial intelligence model was created. Decisions are used to manage operations such as starting, maintaining and stopping the irrigation. The accuracy of the decisions was evaluated and the irrigation system was tested with the results. There are options to manage, view the system remotely and manually and also see the system s decisions with the created mobile application.

</p>
</details>

<details><summary><b>Unsupervised Speech Representation Learning for Behavior Modeling using Triplet Enhanced Contextualized Networks</b>
<a href="https://arxiv.org/abs/2104.03899">arxiv:2104.03899</a>
&#x1F4C8; 2 <br>
<p>Haoqi Li, Brian Baucom, Shrikanth Narayanan, Panayiotis Georgiou</p></summary>
<p>

**Abstract:** Speech encodes a wealth of information related to human behavior and has been used in a variety of automated behavior recognition tasks. However, extracting behavioral information from speech remains challenging including due to inadequate training data resources stemming from the often low occurrence frequencies of specific behavioral patterns. Moreover, supervised behavioral modeling typically relies on domain-specific construct definitions and corresponding manually-annotated data, rendering generalizing across domains challenging. In this paper, we exploit the stationary properties of human behavior within an interaction and present a representation learning method to capture behavioral information from speech in an unsupervised way. We hypothesize that nearby segments of speech share the same behavioral context and hence map onto similar underlying behavioral representations. We present an encoder-decoder based Deep Contextualized Network (DCN) as well as a Triplet-Enhanced DCN (TE-DCN) framework to capture the behavioral context and derive a manifold representation, where speech frames with similar behaviors are closer while frames of different behaviors maintain larger distances. The models are trained on movie audio data and validated on diverse domains including on a couples therapy corpus and other publicly collected data (e.g., stand-up comedy). With encouraging results, our proposed framework shows the feasibility of unsupervised learning within cross-domain behavioral modeling.

</p>
</details>

<details><summary><b>Prediction of Solar Radiation Using Artificial Neural Network</b>
<a href="https://arxiv.org/abs/2104.02573">arxiv:2104.02573</a>
&#x1F4C8; 2 <br>
<p>Shahriar Rahman, Shazzadur Rahman, A K M Bahalul Haque</p></summary>
<p>

**Abstract:** Most solar applications and systems can be reliably used to generate electricity and power in many homes and offices. Recently, there is an increase in many solar required systems that can be found not only in electricity generation but other applications such as solar distillation, water heating, heating of buildings, meteorology and producing solar conversion energy. Prediction of solar radiation is very significant in order to accomplish the previously mentioned objectives. In this paper, the main target is to present an algorithm that can be used to predict an hourly activity of solar radiation. Using a dataset that consists of temperature of air, time, humidity, wind speed, atmospheric pressure, direction of wind and solar radiation data, an Artificial Neural Network (ANN) model is constructed to effectively forecast solar radiation using the available weather forecast data. Two models are created to efficiently create a system capable of interpreting patterns through supervised learning data and predict the correct amount of radiation present in the atmosphere. The results of the two statistical indicators: Mean Absolute Error (MAE) and Mean Squared Error (MSE) are performed and compared with observed and predicted data. These two models were able to generate efficient predictions with sufficient performance accuracy.

</p>
</details>

<details><summary><b>Qubit Routing using Graph Neural Network aided Monte Carlo Tree Search</b>
<a href="https://arxiv.org/abs/2104.01992">arxiv:2104.01992</a>
&#x1F4C8; 2 <br>
<p>Animesh Sinha, Utkarsh Azad, Harjinder Singh</p></summary>
<p>

**Abstract:** Near-term quantum hardware can support two-qubit operations only on the qubits that can interact with each other. Therefore, to execute an arbitrary quantum circuit on the hardware, compilers have to first perform the task of qubit routing, i.e., to transform the quantum circuit either by inserting additional SWAP gates or by reversing existing CNOT gates to satisfy the connectivity constraints of the target topology. We propose a procedure for qubit routing that is architecture agnostic and that outperforms other available routing implementations on various circuit benchmarks. The depth of the transformed quantum circuits is minimised by utilizing the Monte Carlo tree search to perform qubit routing, aided by a Graph neural network that evaluates the value function and action probabilities for each state.

</p>
</details>

<details><summary><b>An NCAP-like Safety Indicator for Self-Driving Cars</b>
<a href="https://arxiv.org/abs/2104.00859">arxiv:2104.00859</a>
&#x1F4C8; 2 <br>
<p>Jimy Cai Huang, Hanna Kurniawati</p></summary>
<p>

**Abstract:** This paper proposes a mechanism to assess the safety of autonomous cars. It assesses the car's safety in scenarios where the car must avoid collision with an adversary. Core to this mechanism is a safety measure, called Safe-Kamikaze Distance (SKD), which computes the average similarity between sets of safe adversary's trajectories and kamikaze trajectories close to the safe trajectories. The kamikaze trajectories are generated based on planning under uncertainty techniques, namely the Partially Observable Markov Decision Processes, to account for the partially observed car policy from the point of view of the adversary. We found that SKD is inversely proportional to the upper bound on the probability that a small deformation changes a collision-free trajectory of the adversary into a colliding one. We perform systematic tests on a scenario where the adversary is a pedestrian crossing a single-lane road in front of the car being assessed --which is, one of the scenarios in the Euro-NCAP's Vulnerable Road User (VRU) tests on Autonomous Emergency Braking. Simulation results on assessing cars with basic controllers and a test on a Machine-Learning controller using a high-fidelity simulator indicates promising results for SKD to measure the safety of autonomous cars. Moreover, the time taken for each simulation test is under 11 seconds, enabling a sufficient statistics to compute SKD from simulation to be generated on a quad-core desktop in less than 25 minutes.

</p>
</details>

<details><summary><b>Streaming Social Event Detection and Evolution Discovery in Heterogeneous Information Networks</b>
<a href="https://arxiv.org/abs/2104.00853">arxiv:2104.00853</a>
&#x1F4C8; 2 <br>
<p>Hao Peng, Jianxin Li, Yangqiu Song, Renyu Yang, Rajiv Ranjan, Philip S. Yu, Lifang He</p></summary>
<p>

**Abstract:** Events are happening in real-world and real-time, which can be planned and organized for occasions, such as social gatherings, festival celebrations, influential meetings or sports activities. Social media platforms generate a lot of real-time text information regarding public events with different topics. However, mining social events is challenging because events typically exhibit heterogeneous texture and metadata are often ambiguous. In this paper, we first design a novel event-based meta-schema to characterize the semantic relatedness of social events and then build an event-based heterogeneous information network (HIN) integrating information from external knowledge base. Second, we propose a novel Pairwise Popularity Graph Convolutional Network, named as PP-GCN, based on weighted meta-path instance similarity and textual semantic representation as inputs, to perform fine-grained social event categorization and learn the optimal weights of meta-paths in different tasks. Third, we propose a streaming social event detection and evolution discovery framework for HINs based on meta-path similarity search, historical information about meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive experiments on real-world streaming social text data are conducted to compare various social event detection and evolution discovery algorithms. Experimental results demonstrate that our proposed framework outperforms other alternative social event detection and evolution discovery techniques.

</p>
</details>

<details><summary><b>Unconstrained Face Recognition using ASURF and Cloud-Forest Classifier optimized with VLAD</b>
<a href="https://arxiv.org/abs/2104.00842">arxiv:2104.00842</a>
&#x1F4C8; 2 <br>
<p>A Vinay, Aviral Joshi, Hardik Mahipal Surana, Harsh Garg, K N BalasubramanyaMurthy, S Natarajan</p></summary>
<p>

**Abstract:** The paper posits a computationally-efficient algorithm for multi-class facial image classification in which images are constrained with translation, rotation, scale, color, illumination and affine distortion. The proposed method is divided into five main building blocks including Haar-Cascade for face detection, Bilateral Filter for image preprocessing to remove unwanted noise, Affine Speeded-Up Robust Features (ASURF) for keypoint detection and description, Vector of Locally Aggregated Descriptors (VLAD) for feature quantization and Cloud Forest for image classification. The proposed method aims at improving the accuracy and the time taken for face recognition systems. The usage of the Cloud Forest algorithm as a classifier on three benchmark datasets, namely the FACES95, FACES96 and ORL facial datasets, showed promising results. The proposed methodology using Cloud Forest algorithm successfully improves the recognition model by 2-12\% when differentiated against other ensemble techniques like the Random Forest classifier depending upon the dataset used.

</p>
</details>

<details><summary><b>Effect of Radiology Report Labeler Quality on Deep Learning Models for Chest X-Ray Interpretation</b>
<a href="https://arxiv.org/abs/2104.00793">arxiv:2104.00793</a>
&#x1F4C8; 2 <br>
<p>Saahil Jain, Akshay Smit, Andrew Y. Ng, Pranav Rajpurkar</p></summary>
<p>

**Abstract:** Although deep learning models for chest X-ray interpretation are commonly trained on labels generated by automatic radiology report labelers, the impact of improvements in report labeling on the performance of chest X-ray classification models has not been systematically investigated. We first compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of extracting accurate chest X-ray image labels from radiology reports, reporting that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers. Next, after training image classification models using labels generated from the different radiology report labelers on one of the largest datasets of chest X-rays, we show that an image classification model trained on labels from the VisualCheXbert labeler outperforms image classification models trained on labels from the CheXpert and CheXbert labelers. Our work suggests that recent improvements in radiology report labeling can translate to the development of higher performing chest X-ray classification models.

</p>
</details>

<details><summary><b>A study on the effects of compression on hyperspectral image classification</b>
<a href="https://arxiv.org/abs/2104.00788">arxiv:2104.00788</a>
&#x1F4C8; 2 <br>
<p>Kiran Mantripragada, Phuong D. Dao, Yuhong He, Faisal Z. Qureshi</p></summary>
<p>

**Abstract:** This paper presents a systematic study the effects of compression on hyperspectral pixel classification task. We use five dimensionality reduction methods -- PCA, KPCA, ICA, AE, and DAE -- to compress 301-dimensional hyperspectral pixels. Compressed pixels are subsequently used to perform pixel-based classifications. Pixel classification accuracies together with compression method, compression rates, and reconstruction errors provide a new lens to study the suitability of a compression method for the task of pixel-based classification. We use three high-resolution hyperspectral image datasets, representing three common landscape units (i.e. urban, transitional suburban, and forests) collected by the Remote Sensing and Spatial Ecosystem Modeling laboratory of the University of Toronto. We found that PCA, KPCA, and ICA post greater signal reconstruction capability; however, when compression rate is more than 90\% those methods showed lower classification scores. AE and DAE methods post better classification accuracy at 95\% compression rate, however decreasing again at 97\%, suggesting a sweet-spot at the 95\% mark. Our results demonstrate that the choice of a compression method with the compression rate are important considerations when designing a hyperspectral image classification pipeline.

</p>
</details>

<details><summary><b>Reservoir-Based Distributed Machine Learning for Edge Operation</b>
<a href="https://arxiv.org/abs/2104.00751">arxiv:2104.00751</a>
&#x1F4C8; 2 <br>
<p>Silvija Kokalj-Filipovic, Paul Toliver, William Johnson, Rob Miller</p></summary>
<p>

**Abstract:** We introduce a novel design for in-situ training of machine learning algorithms built into smart sensors, and illustrate distributed training scenarios using radio frequency (RF) spectrum sensors. Current RF sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent signal classification. We propose a solution using Deepdelay Loop Reservoir Computing (DLR), a processing architecture that supports machine learning algorithms on resource-constrained edge-devices by leveraging delayloop reservoir computing in combination with innovative hardware. DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-ofthe- Art (SoA) neural nets. We demonstrate DLR for two applications: RF Specific Emitter Identification (SEI) and wireless protocol recognition. DLR enables mobile edge platforms to authenticate and then track emitters with fast SEI retraining. Once delay loops separate the data classes, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers such as Ridge Regression (RR), the complexity grows at least quadratically with the input size. DLR with a RR classifier exceeds the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. To authenticate mobile devices across large regions, DLR can be trained in a distributed fashion with very little additional processing and a small communication cost, all while maintaining accuracy. We illustrate how to merge locally trained DLR classifiers in use cases of interest.

</p>
</details>

<details><summary><b>Confidence Adaptive Anytime Pixel-Level Recognition</b>
<a href="https://arxiv.org/abs/2104.00749">arxiv:2104.00749</a>
&#x1F4C8; 2 <br>
<p>Zhuang Liu, Trevor Darrell, Evan Shelhamer</p></summary>
<p>

**Abstract:** Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification. We propose the first unified and end-to-end model approach for anytime pixel-level recognition. A cascade of "exits" is attached to the model to make multiple predictions and direct further computation. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Our full model with redesigned exit architecture and spatial adaptivity enables anytime inference, achieves the same level of final accuracy, and even significantly reduces total computation. We evaluate our approach on semantic segmentation and human pose estimation. On Cityscapes semantic segmentation and MPII human pose estimation, our approach enables anytime inference while also reducing the total FLOPs of its base models by 44.4% and 59.1% without sacrificing accuracy. As a new anytime baseline, we measure the anytime capability of deep equilibrium networks, a recent class of model that is intrinsically iterative, and we show that the accuracy-computation curve of our architecture strictly dominates it.

</p>
</details>

<details><summary><b>Formal Methods for the Informal Engineer: Workshop Recommendations</b>
<a href="https://arxiv.org/abs/2104.00739">arxiv:2104.00739</a>
&#x1F4C8; 2 <br>
<p>Gopal Sarma, James Koppel, Gregory Malecha, Patrick Schultz, Eric Drexler, Ramana Kumar, Cody Roux, Philip Zucker</p></summary>
<p>

**Abstract:** Formal Methods for the Informal Engineer (FMIE) was a workshop held at the Broad Institute of MIT and Harvard in 2021 to explore the potential role of verified software in the biomedical software ecosystem. The motivation for organizing FMIE was the recognition that the life sciences and medicine are undergoing a transition from being passive consumers of software and AI/ML technologies to fundamental drivers of new platforms, including those which will need to be mission and safety-critical. Drawing on conversations leading up to and during the workshop, we make five concrete recommendations to help software leaders organically incorporate tools, techniques, and perspectives from formal methods into their project planning and development trajectories.

</p>
</details>

<details><summary><b>ProcessTransformer: Predictive Business Process Monitoring with Transformer Network</b>
<a href="https://arxiv.org/abs/2104.00721">arxiv:2104.00721</a>
&#x1F4C8; 2 <br>
<p>Zaharah A. Bukhsh, Aaqib Saeed, Remco M. Dijkman</p></summary>
<p>

**Abstract:** Predictive business process monitoring focuses on predicting future characteristics of a running process using event logs. The foresight into process execution promises great potentials for efficient operations, better resource management, and effective customer services. Deep learning-based approaches have been widely adopted in process mining to address the limitations of classical algorithms for solving multiple problems, especially the next event and remaining-time prediction tasks. Nevertheless, designing a deep neural architecture that performs competitively across various tasks is challenging as existing methods fail to capture long-range dependencies in the input sequences and perform poorly for lengthy process traces. In this paper, we propose ProcessTransformer, an approach for learning high-level representations from event logs with an attention-based network. Our model incorporates long-range memory and relies on a self-attention mechanism to establish dependencies between a multitude of event sequences and corresponding outputs. We evaluate the applicability of our technique on nine real event logs. We demonstrate that the transformer-based model outperforms several baselines of prior techniques by obtaining on average above 80% accuracy for the task of predicting the next activity. Our method also perform competitively, compared to baselines, for the tasks of predicting event time and remaining time of a running case

</p>
</details>

<details><summary><b>Deep Multi-Resolution Dictionary Learning for Histopathology Image Analysis</b>
<a href="https://arxiv.org/abs/2104.00669">arxiv:2104.00669</a>
&#x1F4C8; 2 <br>
<p>Nima Hatami, Mohsin Bilal, Nasir Rajpoot</p></summary>
<p>

**Abstract:** The problem of recognizing various types of tissues present in multi-gigapixel histology images is an important fundamental pre-requisite for downstream analysis of the tumor microenvironment in a bottom-up analysis paradigm for computational pathology. In this paper, we propose a deep dictionary learning approach to solve the problem of tissue phenotyping in histology images. We propose deep Multi-Resolution Dictionary Learning (deepMRDL) in order to benefit from deep texture descriptors at multiple different spatial resolutions. We show the efficacy of the proposed approach through extensive experiments on four benchmark histology image datasets from different organs (colorectal cancer, breast cancer and breast lymphnodes) and tasks (namely, cancer grading, tissue phenotyping, tumor detection and tissue type classification). We also show that the proposed framework can employ most off-the-shelf CNNs models to generate effective deep texture descriptors.

</p>
</details>

<details><summary><b>Machine Learning Applications to Kronian Magnetospheric Reconnection Classification</b>
<a href="https://arxiv.org/abs/2104.00496">arxiv:2104.00496</a>
&#x1F4C8; 2 <br>
<p>Tadhg M. Garton, Caitriona M. Jackman, Andy W. Smith, Kiley L. Yeakel, Shane A. Maloney, Jon Vandegriff</p></summary>
<p>

**Abstract:** The products of magnetic reconnection in Saturn's magnetotail are identified in magnetometer observations primarily through characteristic deviations in the north-south component of the magnetic field. These magnetic deflections are caused by travelling plasma structures created during reconnection rapidly passing over the observing spacecraft. Identification of these signatures have long been performed by eye, and more recently through semi-automated methods, however these methods are often limited through a required human verification step. Here, we present a fully automated, supervised learning, feed forward neural network model to identify evidence of reconnection in the Kronian magnetosphere with the three magnetic field components observed by the Cassini spacecraft in Kronocentric radial-theta-phi (KRTP) coordinates as input. This model is constructed from a catalogue of reconnection events which covers three years of observations with a total of 2093 classified events, categorized into plasmoids, travelling compression regions and dipolarizations. This neural network model is capable of rapidly identifying reconnection events in large time-span Cassini datasets, tested against the full year 2010 with a high level of accuracy (87%), true skill score (0.76), and Heidke skill score (0.73). From this model, a full cataloguing and examination of magnetic reconnection events in the Kronian magnetosphere across Cassini's near Saturn lifetime is now possible.

</p>
</details>

<details><summary><b>Towards creativity characterization of generative models via group-based subset scanning</b>
<a href="https://arxiv.org/abs/2104.00479">arxiv:2104.00479</a>
&#x1F4C8; 2 <br>
<p>Celia Cintas, Payel Das, Brian Quanz, Skyler Speakman, Victor Akinwande, Pin-Yu Chen</p></summary>
<p>

**Abstract:** Deep generative models, such as Variational Autoencoders (VAEs), have been employed widely in computational creativity research. However, such models discourage out-of-distribution generation to avoid spurious sample generation, limiting their creativity. Thus, incorporating research on human creativity into generative deep learning techniques presents an opportunity to make their outputs more compelling and human-like. As we see the emergence of generative models directed to creativity research, a need for machine learning-based surrogate metrics to characterize creative output from these models is imperative. We propose group-based subset scanning to quantify, detect, and characterize creative processes by detecting a subset of anomalous node-activations in the hidden layers of generative models. Our experiments on original, typically decoded, and "creatively decoded" (Das et al 2020) image datasets reveal that the proposed subset scores distribution is more useful for detecting creative processes in the activation space rather than the pixel space. Further, we found that creative samples generate larger subsets of anomalies than normal or non-creative samples across datasets. The node activations highlighted during the creative decoding process are different from those responsible for normal sample generation.

</p>
</details>

<details><summary><b>Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features</b>
<a href="https://arxiv.org/abs/2104.00411">arxiv:2104.00411</a>
&#x1F4C8; 2 <br>
<p>Ashkan Khakzar, Yang Zhang, Wejdene Mansour, Yuezhi Cai, Yawei Li, Yucheng Zhang, Seong Tae Kim, Nassir Navab</p></summary>
<p>

**Abstract:** Neural networks have demonstrated remarkable performance in classification and regression tasks on chest X-rays. In order to establish trust in the clinical routine, the networks' prediction mechanism needs to be interpretable. One principal approach to interpretation is feature attribution. Feature attribution methods identify the importance of input features for the output prediction. Building on Information Bottleneck Attribution (IBA) method, for each prediction we identify the chest X-ray regions that have high mutual information with the network's output. Original IBA identifies input regions that have sufficient predictive information. We propose Inverse IBA to identify all informative regions. Thus all predictive cues for pathologies are highlighted on the X-rays, a desirable property for chest X-ray diagnosis. Moreover, we propose Regression IBA for explaining regression models. Using Regression IBA we observe that a model trained on cumulative severity score labels implicitly learns the severity of different X-ray regions. Finally, we propose Multi-layer IBA to generate higher resolution and more detailed attribution/saliency maps. We evaluate our methods using both human-centric (ground-truth-based) interpretability metrics, and human-independent feature importance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is publicly available.

</p>
</details>

<details><summary><b>MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking</b>
<a href="https://arxiv.org/abs/2104.00303">arxiv:2104.00303</a>
&#x1F4C8; 2 <br>
<p>Jennifer Jang, Heinrich Jiang</p></summary>
<p>

**Abstract:** MeanShift is a popular mode-seeking clustering algorithm used in a wide range of applications in machine learning. However, it is known to be prohibitively slow, with quadratic runtime per iteration. We propose MeanShift++, an extremely fast mode-seeking algorithm based on MeanShift that uses a grid-based approach to speed up the mean shift step, replacing the computationally expensive neighbors search with a density-weighted mean of adjacent grid cells. In addition, we show that this grid-based technique for density estimation comes with theoretical guarantees. The runtime is linear in the number of points and exponential in dimension, which makes MeanShift++ ideal on low-dimensional applications such as image segmentation and object tracking. We provide extensive experimental analysis showing that MeanShift++ can be more than 10,000x faster than MeanShift with competitive clustering results on benchmark datasets and nearly identical image segmentations as MeanShift. Finally, we show promising results for object tracking.

</p>
</details>

<details><summary><b>Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors</b>
<a href="https://arxiv.org/abs/2106.16087">arxiv:2106.16087</a>
&#x1F4C8; 1 <br>
<p>Silvija Kokalj-Filipovic, Paul Toliver, William Johnson, Rob Miller</p></summary>
<p>

**Abstract:** Current radio frequency (RF) sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent spectrum monitoring, and sensor data classification in general. We propose a solution via Deep Delay Loop Reservoir Computing (DLR), a processing architecture that supports general machine learning algorithms on compact mobile devices by leveraging delay-loop reservoir computing in combination with innovative electrooptical hardware. With both digital and photonic realizations of our design of the loops, DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-of-the-Art (SoA). The main impact of the reservoir is to project the input data into a higher dimensional space of reservoir state vectors in order to linearly separate the input classes. Once the classes are well separated, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers based on Ridge regression (RR), the complexity grows at least quadratically with the input size. Hence, the hardware reduction required for training on compact devices is in contradiction with the large dimension of state vectors. DLR employs a RR-based classifier to exceed the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. We present DLR architectures composed of multiple smaller loops whose state vectors are linearly combined to create a lower dimensional input into Ridge regression. We demonstrate the advantages of using DLR for two distinct applications: RF Specific Emitter Identification (SEI) for IoT authentication, and wireless protocol recognition for IoT situational awareness.

</p>
</details>

<details><summary><b>Finding Experts in Social Media Data using a Hybrid Approach</b>
<a href="https://arxiv.org/abs/2104.03920">arxiv:2104.03920</a>
&#x1F4C8; 1 <br>
<p>Simon James,  Brady</p></summary>
<p>

**Abstract:** Several approaches to the problem of expert finding have emerged in computer science research. In this work, three of these approaches - content analysis, social graph analysis and the use of Semantic Web technologies are examined. An integrated set of system requirements is then developed that uses all three approaches in one hybrid approach.
  To show the practicality of this hybrid approach, a usable prototype expert finding system called ExpertQuest is developed using a modern functional programming language (Clojure) to query social media data and Linked Data. This system is evaluated and discussed. Finally, a discussion and conclusions are presented which describe the benefits and shortcomings of the hybrid approach and the technologies used in this work.

</p>
</details>

<details><summary><b>Graph Attention Networks for Channel Estimation in RIS-assisted Satellite IoT Communications</b>
<a href="https://arxiv.org/abs/2104.00735">arxiv:2104.00735</a>
&#x1F4C8; 1 <br>
<p>Kürşat Tekbıyık, Güneş Karabulut Kurt, Ali Rıza Ekti, Halim Yanikomeroglu</p></summary>
<p>

**Abstract:** Direct-to-satellite (DtS) communication has gained importance recently to support globally connected Internet of things (IoT) networks. However, relatively long distances of densely deployed satellite networks around the Earth cause a high path loss. In addition, since high complexity operations such as beamforming, tracking and equalization have to be performed in IoT devices partially, both the hardware complexity and the need for high-capacity batteries of IoT devices increase. The reconfigurable intelligent surfaces (RISs) have the potential to increase the energy-efficiency and to perform complex signal processing over the transmission environment instead of IoT devices. But, RISs need the information of the cascaded channel in order to change the phase of the incident signal. This study proposes graph attention networks (GATs) for the challenging channel estimation problem and examines the performance of DtS IoT networks for different RIS configurations under GAT channel estimation.

</p>
</details>

<details><summary><b>Distributed support-vector-machine over dynamic balanced directed networks</b>
<a href="https://arxiv.org/abs/2104.00399">arxiv:2104.00399</a>
&#x1F4C8; 1 <br>
<p>Mohammadreza Doostmohammadian, Alireza Aghasi, Themistoklis Charalambous, Usman A. Khan</p></summary>
<p>

**Abstract:** In this paper, we consider the binary classification problem via distributed Support-Vector-Machines (SVM), where the idea is to train a network of agents, with limited share of data, to cooperatively learn the SVM classifier for the global database. Agents only share processed information regarding the classifier parameters and the gradient of the local loss functions instead of their raw data. In contrast to the existing work, we propose a continuous-time algorithm that incorporates network topology changes in discrete jumps. This hybrid nature allows us to remove chattering that arises because of the discretization of the underlying CT process. We show that the proposed algorithm converges to the SVM classifier over time-varying weight balanced directed graphs by using arguments from the matrix perturbation theory.

</p>
</details>

<details><summary><b>CycleDRUMS: Automatic Drum Arrangement For Bass Lines Using CycleGAN</b>
<a href="https://arxiv.org/abs/2104.00353">arxiv:2104.00353</a>
&#x1F4C8; 1 <br>
<p>Giorgio Barnabò, Giovanni Trappolini, Lorenzo Lastilla, Cesare Campagnano, Angela Fan, Fabio Petroni, Fabrizio Silvestri</p></summary>
<p>

**Abstract:** The two main research threads in computer-based music generation are: the construction of autonomous music-making systems, and the design of computer-based environments to assist musicians. In the symbolic domain, the key problem of automatically arranging a piece music was extensively studied, while relatively fewer systems tackled this challenge in the audio domain. In this contribution, we propose CycleDRUMS, a novel method for generating drums given a bass line. After converting the waveform of the bass into a mel-spectrogram, we are able to automatically generate original drums that follow the beat, sound credible and can be directly mixed with the input bass. We formulated this task as an unpaired image-to-image translation problem, and we addressed it with CycleGAN, a well-established unsupervised style transfer framework, originally designed for treating images. The choice to deploy raw audio and mel-spectrograms enabled us to better represent how humans perceive music, and to potentially draw sounds for new arrangements from the vast collection of music recordings accumulated in the last century. In absence of an objective way of evaluating the output of both generative adversarial networks and music generative systems, we further defined a possible metric for the proposed task, partially based on human (and expert) judgement. Finally, as a comparison, we replicated our results with Pix2Pix, a paired image-to-image translation network, and we showed that our approach outperforms it.

</p>
</details>

<details><summary><b>High-quality Low-dose CT Reconstruction Using Convolutional Neural Networks with Spatial and Channel Squeeze and Excitation</b>
<a href="https://arxiv.org/abs/2104.00325">arxiv:2104.00325</a>
&#x1F4C8; 1 <br>
<p>Jingfeng Lu, Shuo Wang, Ping Li, Dong Ye</p></summary>
<p>

**Abstract:** Low-dose computed tomography (CT) allows the reduction of radiation risk in clinical applications at the expense of image quality, which deteriorates the diagnosis accuracy of radiologists. In this work, we present a High-Quality Imaging network (HQINet) for the CT image reconstruction from Low-dose computed tomography (CT) acquisitions. HQINet was a convolutional encoder-decoder architecture, where the encoder was used to extract spatial and temporal information from three contiguous slices while the decoder was used to recover the spacial information of the middle slice. We provide experimental results on the real projection data from low-dose CT Image and Projection Data (LDCT-and-Projection-data), demonstrating that the proposed approach yielded a notable improvement of the performance in terms of image quality, with a rise of 5.5dB in terms of peak signal-to-noise ratio (PSNR) and 0.29 in terms of mutual information (MI).

</p>
</details>

<details><summary><b>Pinpointing the Memory Behaviors of DNN Training</b>
<a href="https://arxiv.org/abs/2104.00258">arxiv:2104.00258</a>
&#x1F4C8; 1 <br>
<p>Jiansong Li, Xiao Dong, Guangli Li, Peng Zhao, Xueying Wang, Xiaobing Chen, Xianzhi Yu, Yongxin Yang, Zihan Jiang, Wei Cao, Lei Liu, Xiaobing Feng</p></summary>
<p>

**Abstract:** The training of deep neural networks (DNNs) is usually memory-hungry due to the limited device memory capacity of DNN accelerators. Characterizing the memory behaviors of DNN training is critical to optimize the device memory pressures. In this work, we pinpoint the memory behaviors of each device memory block of GPU during training by instrumenting the memory allocators of the runtime system. Our results show that the memory access patterns of device memory blocks are stable and follow an iterative fashion. These observations are useful for the future optimization of memory-efficient training from the perspective of raw memory access patterns.

</p>
</details>

<details><summary><b>A Comparative Analysis of Machine Learning and Grey Models</b>
<a href="https://arxiv.org/abs/2104.00871">arxiv:2104.00871</a>
&#x1F4C8; 0 <br>
<p>Gang He, Khwaja Mutahir Ahmad, Wenxin Yu, Xiaochuan Xu, Jay Kumar</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) has recently shown its capabilities for almost every field of life. Machine Learning, which is a subset of AI, is a `HOT' topic for researchers. Machine Learning outperforms other classical forecasting techniques in almost all-natural applications. It is a crucial part of modern research. As per this statement, Modern Machine Learning algorithms are hungry for big data. Due to the small datasets, the researchers may not prefer to use Machine Learning algorithms. To tackle this issue, the main purpose of this survey is to illustrate, demonstrate related studies for significance of a semi-parametric Machine Learning framework called Grey Machine Learning (GML). This kind of framework is capable of handling large datasets as well as small datasets for time series forecasting likely outcomes. This survey presents a comprehensive overview of the existing semi-parametric machine learning techniques for time series forecasting. In this paper, a primer survey on the GML framework is provided for researchers. To allow an in-depth understanding for the readers, a brief description of Machine Learning, as well as various forms of conventional grey forecasting models are discussed. Moreover, a brief description on the importance of GML framework is presented.

</p>
</details>

<details><summary><b>Constrained non-negative matrix factorization enabling real-time insights of $\textit{in situ}$ and high-throughput experiments</b>
<a href="https://arxiv.org/abs/2104.00864">arxiv:2104.00864</a>
&#x1F4C8; 0 <br>
<p>Phillip M. Maffettone, Aidan C. Daly, Daniel Olds</p></summary>
<p>

**Abstract:** Non-negative Matrix Factorization (NMF) methods offer an appealing unsupervised learning method for real-time analysis of streaming spectral data in time-sensitive data collection, such as $\textit{in situ}$ characterization of materials. However, canonical NMF methods are optimized to reconstruct a full dataset as closely as possible, with no underlying requirement that the reconstruction produces components or weights representative of the true physical processes. In this work, we demonstrate how constraining NMF weights or components, provided as known or assumed priors, can provide significant improvement in revealing true underlying phenomena. We present a PyTorch based method for efficiently applying constrained NMF and demonstrate this on several synthetic examples. When applied to streaming experimentally measured spectral data, an expert researcher-in-the-loop can provide and dynamically adjust the constraints. This set of interactive priors to the NMF model can, for example, contain known or identified independent components, as well as functional expectations about the mixing of components. We demonstrate this application on measured X-ray diffraction and pair distribution function data from $\textit{in situ}$ beamline experiments. Details of the method are described, and general guidance provided to employ constrained NMF in extraction of critical information and insights during $\textit{in situ}$ and high-throughput experiments.

</p>
</details>

<details><summary><b>Dynamic Silos: Increased Modularity in Intra-organizational Communication Networks during the Covid-19 Pandemic</b>
<a href="https://arxiv.org/abs/2104.00641">arxiv:2104.00641</a>
&#x1F4C8; 0 <br>
<p>Tiona Zuzul, Emily Cox Pahnke, Jonathan Larson, Patrick Bourke, Nicholas Caurvina, Neha Parikh Shah, Fereshteh Amini, Youngser Park, Joshua Vogelstein, Jeffrey Weston, Christopher White, Carey E. Priebe</p></summary>
<p>

**Abstract:** Workplace communications around the world were drastically altered by Covid-19 and the resulting work-from-home orders and rise of remote work. We analyze aggregated, anonymized metadata from over 360 billion emails within over 4,000 organizations worldwide to examine changes in network community structures over 24 months. We find that, in 2020, organizations around the world became more siloed than in 2019, evidenced by increased modularity. This shift was concurrent with decreased stability, indicating that organizational siloes had less stable membership. We provide initial insights into the meaning and implications of these network changes -- which we term dynamic silos -- for new models of work.

</p>
</details>

<details><summary><b>NuPS: A Parameter Server for Machine Learning with Non-Uniform Parameter Access</b>
<a href="https://arxiv.org/abs/2104.00501">arxiv:2104.00501</a>
&#x1F4C8; 0 <br>
<p>Alexander Renz-Wieland, Rainer Gemulla, Zoi Kaoudi, Volker Markl</p></summary>
<p>

**Abstract:** Parameter servers (PSs) facilitate the implementation of distributed training for large machine learning tasks. In this paper, we argue that existing PSs are inefficient for tasks that exhibit non-uniform parameter access; their performance may even fall behind that of single node baselines. We identify two major sources of such non-uniform access: skew and sampling. Existing PSs are ill-suited for managing skew because they uniformly apply the same parameter management technique to all parameters. They are inefficient for sampling because the PS is oblivious to the associated randomized accesses and cannot exploit locality. To overcome these performance limitations, we introduce NuPS, a novel PS architecture that (i) integrates multiple management techniques and employs a suitable technique for each parameter and (ii) supports sampling directly via suitable sampling primitives and sampling schemes that allow for a controlled quality--efficiency trade-off. In our experimental study, NuPS outperformed existing PSs by up to one order of magnitude and provided up to linear scalability across multiple machine learning tasks.

</p>
</details>

<details><summary><b>Low-Resource Neural Machine Translation for Southern African Languages</b>
<a href="https://arxiv.org/abs/2104.00366">arxiv:2104.00366</a>
&#x1F4C8; 0 <br>
<p>Evander Nyoni, Bruce A. Bassett</p></summary>
<p>

**Abstract:** Low-resource African languages have not fully benefited from the progress in neural machine translation because of a lack of data. Motivated by this challenge we compare zero-shot learning, transfer learning and multilingual learning on three Bantu languages (Shona, isiXhosa and isiZulu) and English. Our main target is English-to-isiZulu translation for which we have just 30,000 sentence pairs, 28% of the average size of our other corpora. We show the importance of language similarity on the performance of English-to-isiZulu transfer learning based on English-to-isiXhosa and English-to-Shona parent models whose BLEU scores differ by 5.2. We then demonstrate that multilingual learning surpasses both transfer learning and zero-shot learning on our dataset, with BLEU score improvements relative to the baseline English-to-isiZulu model of 9.9, 6.1 and 2.0 respectively. Our best model also improves the previous SOTA BLEU score by more than 10.

</p>
</details>

<details><summary><b>TrajeVAE: Controllable Human Motion Generation from Trajectories</b>
<a href="https://arxiv.org/abs/2104.00351">arxiv:2104.00351</a>
&#x1F4C8; 0 <br>
<p>Kacper Kania, Marek Kowalski, Tomasz Trzciński</p></summary>
<p>

**Abstract:** The creation of plausible and controllable 3D human motion animations is a long-standing problem that requires a manual intervention of skilled artists. Current machine learning approaches can semi-automate the process, however, they are limited in a significant way: they can handle only a single trajectory of the expected motion that precludes fine-grained control over the output. To mitigate that issue, we reformulate the problem of future pose prediction into pose completion in space and time where multiple trajectories are represented as poses with missing joints. We show that such a framework can generalize to other neural networks designed for future pose prediction. Once trained in this framework, a model is capable of predicting sequences from any number of trajectories. We propose a novel transformer-like architecture, TrajeVAE, that builds on this idea and provides a versatile framework for 3D human animation. We demonstrate that TrajeVAE offers better accuracy than the trajectory-based reference approaches and methods that base their predictions on past poses. We also show that it can predict reasonable future poses even if provided only with an initial pose.

</p>
</details>


[Next Page]({{ '/2021/03/31/2021.03.31.html' | relative_url }})
