## Summary for 2021-03-27, created on 2021-12-23


<details><summary><b>SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences</b>
<a href="https://arxiv.org/abs/2103.14898">arxiv:2103.14898</a>
&#x1F4C8; 11 <br>
<p>Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari</p></summary>
<p>

**Abstract:** Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.

</p>
</details>

<details><summary><b>Explaining the Road Not Taken</b>
<a href="https://arxiv.org/abs/2103.14973">arxiv:2103.14973</a>
&#x1F4C8; 10 <br>
<p>Hua Shen, Ting-Hao 'Kenneth' Huang</p></summary>
<p>

**Abstract:** It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank. We found that although users are interested in explanations for the road not taken -- namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart -- most model interpretations cannot answer these questions.

</p>
</details>

<details><summary><b>MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis</b>
<a href="https://arxiv.org/abs/2103.14910">arxiv:2103.14910</a>
&#x1F4C8; 8 <br>
<p>Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee</p></summary>
<p>

**Abstract:** In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE

</p>
</details>

<details><summary><b>HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval</b>
<a href="https://arxiv.org/abs/2103.15049">arxiv:2103.15049</a>
&#x1F4C8; 7 <br>
<p>Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, Zhongyuan Wang</p></summary>
<p>

**Abstract:** Video-Text Retrieval has been a hot research topic with the growth of multimedia data on the internet. Transformer for video-text learning has attracted increasing attention due to its promising performance. However, existing cross-modal transformer approaches typically suffer from two major limitations: 1) Exploitation of the transformer architecture where different layers have different feature characteristics is limited; 2) End-to-end training mechanism limits negative sample interactions in a mini-batch. In this paper, we propose a novel approach named Hierarchical Transformer (HiT) for video-text retrieval. HiT performs Hierarchical Cross-modal Contrastive Matching in both feature-level and semantic-level, achieving multi-view and comprehensive retrieval results. Moreover, inspired by MoCo, we propose Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions on-the-fly, which contributes to the generation of more precise and discriminative representations. Experimental results on the three major Video-Text Retrieval benchmark datasets demonstrate the advantages of our method.

</p>
</details>

<details><summary><b>Embedding Transfer with Label Relaxation for Improved Metric Learning</b>
<a href="https://arxiv.org/abs/2103.14908">arxiv:2103.14908</a>
&#x1F4C8; 6 <br>
<p>Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak</p></summary>
<p>

**Abstract:** This paper presents a novel method for embedding transfer, a task of transferring knowledge of a learned embedding model to another. Our method exploits pairwise similarities between samples in the source embedding space as the knowledge, and transfers them through a loss used for learning target embedding models. To this end, we design a new loss called relaxed contrastive loss, which employs the pairwise similarities as relaxed labels for inter-sample relations. Our loss provides a rich supervisory signal beyond class equivalence, enables more important pairs to contribute more to training, and imposes no restriction on manifolds of target embedding spaces. Experiments on metric learning benchmarks demonstrate that our method largely improves performance, or reduces sizes and output dimensions of target models effectively. We further show that it can be also used to enhance quality of self-supervised representation and performance of classification models. In all the experiments, our method clearly outperforms existing embedding transfer techniques.

</p>
</details>

<details><summary><b>Catalyzing Clinical Diagnostic Pipelines Through Volumetric Medical Image Segmentation Using Deep Neural Networks: Past, Present, & Future</b>
<a href="https://arxiv.org/abs/2103.14969">arxiv:2103.14969</a>
&#x1F4C8; 5 <br>
<p>Teofilo E. Zosa</p></summary>
<p>

**Abstract:** Deep learning has made a remarkable impact in the field of natural image processing over the past decade. Consequently, there is a great deal of interest in replicating this success across unsolved tasks in related domains, such as medical image analysis. Core to medical image analysis is the task of semantic segmentation which enables various clinical workflows. Due to the challenges inherent in manual segmentation, many decades of research have been devoted to discovering extensible, automated, expert-level segmentation techniques. Given the groundbreaking performance demonstrated by recent neural network-based techniques, deep learning seems poised to achieve what classic methods have historically been unable. This paper will briefly overview some of the state-of-the-art (SoTA) neural network-based segmentation algorithms with a particular emphasis on the most recent architectures, comparing and contrasting the contributions and characteristics of each network topology. Using ultrasonography as a motivating example, it will also demonstrate important clinical implications of effective deep learning-based solutions, articulate challenges unique to the modality, and discuss novel approaches developed in response to those challenges, concluding with the proposal of future directions in the field. Given the generally observed ephemerality of the best deep learning approaches (i.e. the extremely quick succession of the SoTA), the main contributions of the paper are its contextualization of modern deep learning architectures with historical background and the elucidation of the current trajectory of volumetric medical image segmentation research.

</p>
</details>

<details><summary><b>Improving prostate whole gland segmentation in t2-weighted MRI with synthetically generated data</b>
<a href="https://arxiv.org/abs/2103.14955">arxiv:2103.14955</a>
&#x1F4C8; 5 <br>
<p>Alvaro Fernandez-Quilez, Steinar Valle Larsen, Morten Goodwin, Thor Ole Gulsurd, Svein Reidar Kjosavik, Ketil Oppedal</p></summary>
<p>

**Abstract:** Whole gland (WG) segmentation of the prostate plays a crucial role in detection, staging and treatment planning of prostate cancer (PCa). Despite promise shown by deep learning (DL) methods, they rely on the availability of a considerable amount of annotated data. Augmentation techniques such as translation and rotation of images present an alternative to increase data availability. Nevertheless, the amount of information provided by the transformed data is limited due to the correlation between the generated data and the original. Based on the recent success of generative adversarial networks (GAN) in producing synthetic images for other domains as well as in the medical domain, we present a pipeline to generate WG segmentation masks and synthesize T2-weighted MRI of the prostate based on a publicly available multi-center dataset. Following, we use the generated data as a form of data augmentation. Results show an improvement in the quality of the WG segmentation when compared to standard augmentation techniques.

</p>
</details>

<details><summary><b>H-GAN: the power of GANs in your Hands</b>
<a href="https://arxiv.org/abs/2103.15017">arxiv:2103.15017</a>
&#x1F4C8; 4 <br>
<p>Sergiu Oprea, Giorgos Karvounas, Pablo Martinez-Gonzalez, Nikolaos Kyriazis, Sergio Orts-Escolano, Iason Oikonomidis, Alberto Garcia-Garcia, Aggeliki Tsoli, Jose Garcia-Rodriguez, Antonis Argyros</p></summary>
<p>

**Abstract:** We present HandGAN (H-GAN), a cycle-consistent adversarial learning approach implementing multi-scale perceptual discriminators. It is designed to translate synthetic images of hands to the real domain. Synthetic hands provide complete ground-truth annotations, yet they are not representative of the target distribution of real-world data. We strive to provide the perfect blend of a realistic hand appearance with synthetic annotations. Relying on image-to-image translation, we improve the appearance of synthetic hands to approximate the statistical distribution underlying a collection of real images of hands. H-GAN tackles not only the cross-domain tone mapping but also structural differences in localized areas such as shading discontinuities. Results are evaluated on a qualitative and quantitative basis improving previous works. Furthermore, we relied on the hand classification task to claim our generated hands are statistically similar to the real domain of hands.

</p>
</details>

<details><summary><b>Graph Unlearning</b>
<a href="https://arxiv.org/abs/2103.14991">arxiv:2103.14991</a>
&#x1F4C8; 4 <br>
<p>Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang</p></summary>
<p>

**Abstract:** The right to be forgotten states that a data subject has the right to erase their data from an entity storing it. In the context of machine learning (ML), it requires the ML model provider to remove the data subject's data from the training set used to build the ML model, a process known as \textit{machine unlearning}. While straightforward and legitimate, retraining the ML model from scratch upon receiving unlearning requests incurs high computational overhead when the training set is large. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility.
  In this paper, we propose GraphEraser, a novel machine unlearning method tailored to graph data. Its contributions include two novel graph partition algorithms, and a learning-based aggregation method. We conduct extensive experiments on five real-world datasets to illustrate the unlearning efficiency and model utility of GraphEraser. We observe that GraphEraser achieves 2.06$\times$ (small dataset) to 35.94$\times$ (large dataset) unlearning time improvement compared to retraining from scratch. On the other hand, GraphEraser achieves up to $62.5\%$ higher F1 score than that of random partitioning. In addition, our proposed learning-based aggregation method achieves up to $112\%$ higher F1 score than that of the majority vote aggregation.

</p>
</details>

<details><summary><b>You Can Do Better! If You Elaborate the Reason When Making Prediction</b>
<a href="https://arxiv.org/abs/2103.14919">arxiv:2103.14919</a>
&#x1F4C8; 4 <br>
<p>Dongfang Li, Jingcong Tao, Qingcai Chen, Baotian Hu</p></summary>
<p>

**Abstract:** Neural predictive models have achieved remarkable performance improvements in various natural language processing tasks. However, most neural predictive models suffer from the lack of explainability of predictions, limiting their practical utility. This paper proposes a neural predictive approach to make a prediction and generate its corresponding explanation simultaneously. It leverages the knowledge entailed in explanations as an additional distillation signal for more efficient learning. We conduct a preliminary study on Chinese medical multiple-choice question answering, English natural language inference, and commonsense question answering tasks. The experimental results show that the proposed approach can generate reasonable explanations for its predictions even with a small-scale training corpus. The proposed method also achieves improved prediction accuracy on three datasets, which indicates that making predictions can benefit from generating the explanation in the decision process.

</p>
</details>

<details><summary><b>Continuous Conditional Generative Adversarial Networks (cGAN) with Generator Regularization</b>
<a href="https://arxiv.org/abs/2103.14884">arxiv:2103.14884</a>
&#x1F4C8; 4 <br>
<p>Yufeng Zheng, Yunkai Zhang, Zeyu Zheng</p></summary>
<p>

**Abstract:** Conditional Generative Adversarial Networks are known to be difficult to train, especially when the conditions are continuous and high-dimensional. To partially alleviate this difficulty, we propose a simple generator regularization term on the GAN generator loss in the form of Lipschitz penalty. Thus, when the generator is fed with neighboring conditions in the continuous space, the regularization term will leverage the neighbor information and push the generator to generate samples that have similar conditional distributions for each neighboring condition. We analyze the effect of the proposed regularization term and demonstrate its robust performance on a range of synthetic and real-world tasks.

</p>
</details>

<details><summary><b>LiBRe: A Practical Bayesian Approach to Adversarial Detection</b>
<a href="https://arxiv.org/abs/2103.14835">arxiv:2103.14835</a>
&#x1F4C8; 4 <br>
<p>Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu</p></summary>
<p>

**Abstract:** Despite their appealing flexibility, deep neural networks (DNNs) are vulnerable against adversarial examples. Various adversarial defense strategies have been proposed to resolve this problem, but they typically demonstrate restricted practicability owing to unsurmountable compromise on universality, effectiveness, or efficiency. In this work, we propose a more practical approach, Lightweight Bayesian Refinement (LiBRe), in the spirit of leveraging Bayesian neural networks (BNNs) for adversarial detection. Empowered by the task and attack agnostic modeling under Bayes principle, LiBRe can endow a variety of pre-trained task-dependent DNNs with the ability of defending heterogeneous adversarial attacks at a low cost. We develop and integrate advanced learning techniques to make LiBRe appropriate for adversarial detection. Concretely, we build the few-layer deep ensemble variational and adopt the pre-training & fine-tuning workflow to boost the effectiveness and efficiency of LiBRe. We further provide a novel insight to realise adversarial detection-oriented uncertainty quantification without inefficiently crafting adversarial examples during training. Extensive empirical studies covering a wide range of scenarios verify the practicability of LiBRe. We also conduct thorough ablation studies to evidence the superiority of our modeling and learning strategies.

</p>
</details>

<details><summary><b>Particle Filter Bridge Interpolation</b>
<a href="https://arxiv.org/abs/2103.14963">arxiv:2103.14963</a>
&#x1F4C8; 3 <br>
<p>Adam Lindhe, Carl Ringqvist, Henrik Hult</p></summary>
<p>

**Abstract:** Auto encoding models have been extensively studied in recent years. They provide an efficient framework for sample generation, as well as for analysing feature learning. Furthermore, they are efficient in performing interpolations between data-points in semantically meaningful ways. In this paper, we build further on a previously introduced method for generating canonical, dimension independent, stochastic interpolations. Here, the distribution of interpolation paths is represented as the distribution of a bridge process constructed from an artificial random data generating process in the latent space, having the prior distribution as its invariant distribution. As a result the stochastic interpolation paths tend to reside in regions of the latent space where the prior has high mass. This is a desirable feature since, generally, such areas produce semantically meaningful samples. In this paper, we extend the bridge process method by introducing a discriminator network that accurately identifies areas of high latent representation density. The discriminator network is incorporated as a change of measure of the underlying bridge process and sampling of interpolation paths is implemented using sequential Monte Carlo. The resulting sampling procedure allows for greater variability in interpolation paths and stronger drift towards areas of high data density.

</p>
</details>

<details><summary><b>OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection</b>
<a href="https://arxiv.org/abs/2103.14953">arxiv:2103.14953</a>
&#x1F4C8; 3 <br>
<p>John Taylor Jewell, Vahid Reza Khazaie, Yalda Mohsenzadeh</p></summary>
<p>

**Abstract:** Novelty detection is the task of recognizing samples that do not belong to the distribution of the target class. During training, the novelty class is absent, preventing the use of traditional classification approaches. Deep autoencoders have been widely used as a base of many unsupervised novelty detection methods. In particular, context autoencoders have been successful in the novelty detection task because of the more effective representations they learn by reconstructing original images from randomly masked images. However, a significant drawback of context autoencoders is that random masking fails to consistently cover important structures of the input image, leading to suboptimal representations - especially for the novelty detection task. In this paper, to optimize input masking, we have designed a framework consisting of two competing networks, a Mask Module and a Reconstructor. The Mask Module is a convolutional autoencoder that learns to generate optimal masks that cover the most important parts of images. Alternatively, the Reconstructor is a convolutional encoder-decoder that aims to reconstruct unperturbed images from masked images. The networks are trained in an adversarial manner in which the Mask Module generates masks that are applied to images given to the Reconstructor. In this way, the Mask Module seeks to maximize the reconstruction error that the Reconstructor is minimizing. When applied to novelty detection, the proposed approach learns semantically richer representations compared to context autoencoders and enhances novelty detection at test time through more optimal masking. Novelty detection experiments on the MNIST and CIFAR-10 image datasets demonstrate the proposed approach's superiority over cutting-edge methods. In a further experiment on the UCSD video dataset for novelty detection, the proposed approach achieves state-of-the-art results.

</p>
</details>

<details><summary><b>SelfGait: A Spatiotemporal Representation Learning Method for Self-supervised Gait Recognition</b>
<a href="https://arxiv.org/abs/2103.14811">arxiv:2103.14811</a>
&#x1F4C8; 3 <br>
<p>Yiqun Liu, Yi Zeng, Jian Pu, Hongming Shan, Peiyang He, Junping Zhang</p></summary>
<p>

**Abstract:** Gait recognition plays a vital role in human identification since gait is a unique biometric feature that can be perceived at a distance. Although existing gait recognition methods can learn gait features from gait sequences in different ways, the performance of gait recognition suffers from insufficient labeled data, especially in some practical scenarios associated with short gait sequences or various clothing styles. It is unpractical to label the numerous gait data. In this work, we propose a self-supervised gait recognition method, termed SelfGait, which takes advantage of the massive, diverse, unlabeled gait data as a pre-training process to improve the representation abilities of spatiotemporal backbones. Specifically, we employ the horizontal pyramid mapping (HPM) and micro-motion template builder (MTB) as our spatiotemporal backbones to capture the multi-scale spatiotemporal representations. Experiments on CASIA-B and OU-MVLP benchmark gait datasets demonstrate the effectiveness of the proposed SelfGait compared with four state-of-the-art gait recognition methods. The source code has been released at https://github.com/EchoItLiu/SelfGait.

</p>
</details>

<details><summary><b>Community Detection in General Hypergraph via Graph Embedding</b>
<a href="https://arxiv.org/abs/2103.15035">arxiv:2103.15035</a>
&#x1F4C8; 2 <br>
<p>Yaoming Zhen, Junhui Wang</p></summary>
<p>

**Abstract:** Conventional network data has largely focused on pairwise interactions between two entities, yet multi-way interactions among multiple entities have been frequently observed in real-life hypergraph networks. In this article, we propose a novel method for detecting community structure in general hypergraph networks, uniform or non-uniform. The proposed method introduces a null vertex to augment a non-uniform hypergraph into a uniform multi-hypergraph, and then embeds the multi-hypergraph in a low-dimensional vector space such that vertices within the same community are close to each other. The resultant optimization task can be efficiently tackled by an alternative updating scheme. The asymptotic consistencies of the proposed method are established in terms of both community detection and hypergraph estimation, which are also supported by numerical experiments on some synthetic and real-life hypergraph networks.

</p>
</details>

<details><summary><b>Thermal transmittance prediction based on the application of artificial neural networks on heat flux method results</b>
<a href="https://arxiv.org/abs/2103.14995">arxiv:2103.14995</a>
&#x1F4C8; 2 <br>
<p>Sanjin Gumbarević, Bojan Milovanović, Mergim Gaši, Marina Bagarić</p></summary>
<p>

**Abstract:** Deep energy renovation of building stock came more into focus in the European Union due to energy efficiency related directives. Many buildings that must undergo deep energy renovation are old and may lack design/renovation documentation, or possible degradation of materials might have occurred in building elements over time. Thermal transmittance (i.e. U-value) is one of the most important parameters for determining the transmission heat losses through building envelope elements. It depends on the thickness and thermal properties of all the materials that form a building element. In-situ U-value can be determined by ISO 9869-1 standard (Heat Flux Method - HFM). Still, measurement duration is one of the reasons why HFM is not widely used in field testing before the renovation design process commences. This paper analyzes the possibility of reducing the measurement time by conducting parallel measurements with one heat-flux sensor. This parallelization could be achieved by applying a specific class of the Artificial Neural Network (ANN) on HFM results to predict unknown heat flux based on collected interior and exterior air temperatures. After the satisfying prediction is achieved, HFM sensor can be relocated to another measuring location. Paper shows a comparison of four ANN cases applied to HFM results for a measurement held on one multi-layer wall - multilayer perceptron with three neurons in one hidden layer, long short-term memory with 100 units, gated recurrent unit with 100 units and combination of 50 long short-term memory units and 50 gated recurrent units. The analysis gave promising results in term of predicting the heat flux rate based on the two input temperatures. Additional analysis on another wall showed possible limitations of the method that serves as a direction for further research on this topic.

</p>
</details>

<details><summary><b>Realistic face animation generation from videos</b>
<a href="https://arxiv.org/abs/2103.14984">arxiv:2103.14984</a>
&#x1F4C8; 2 <br>
<p>Zihao Jian, Minshan Xie</p></summary>
<p>

**Abstract:** 3D face reconstruction and face alignment are two fundamental and highly related topics in computer vision. Recently, some works start to use deep learning models to estimate the 3DMM coefficients to reconstruct 3D face geometry. However, the performance is restricted due to the limitation of the pre-defined face templates. To address this problem, some end-to-end methods, which can completely bypass the calculation of 3DMM coefficients, are proposed and attract much attention. In this report, we introduce and analyse three state-of-the-art methods in 3D face reconstruction and face alignment. Some potential improvement on PRN are proposed to further enhance its accuracy and speed.

</p>
</details>

<details><summary><b>Transmitter Discovery through Radio-Visual Probabilistic Active Sensing</b>
<a href="https://arxiv.org/abs/2103.14965">arxiv:2103.14965</a>
&#x1F4C8; 2 <br>
<p>Luca Varotto, Angelo Cenedese</p></summary>
<p>

**Abstract:** Multi-modal Probabilistic Active Sensing (MMPAS) uses sensor fusion and probabilistic models to control the perception process of robotic sensing platforms. MMPAS is successfully employed in environmental exploration, collaborative mobile robotics, and target tracking, being fostered by the high performance guarantees on autonomous perception. In this context, we propose a bi-Radio-Visual PAS scheme to solve the transmitter discovery problem. Specifically, we firstly exploit the correlation between radio and visual measurements to learn a target detection model in a self-supervised manner. Then, the model is combined with antenna radiation anisotropies into a Bayesian Optimization framework that controls the platform. We show that the proposed algorithm attains an accuracy of 92%, overcoming two other probabilistic active sensing baselines.

</p>
</details>

<details><summary><b>Feature-based Representation for Violin Bridge Admittances</b>
<a href="https://arxiv.org/abs/2103.14895">arxiv:2103.14895</a>
&#x1F4C8; 2 <br>
<p>R. Malvermi, S. Gonzalez, M. Quintavalla, F. Antonacci, A. Sarti, J. A. Torres, R. Corradi</p></summary>
<p>

**Abstract:** Frequency Response Functions (FRFs) are one of the cornerstones of musical acoustic experimental research. They describe the way in which musical instruments vibrate in a wide range of frequencies and are used to predict and understand the acoustic differences between them. In the specific case of stringed musical instruments such as violins, FRFs evaluated at the bridge are known to capture the overall body vibration. These indicators, also called bridge admittances, are widely used in the literature for comparative analyses. However, due to their complex structure they are rather difficult to quantitatively compare and study. In this manuscript we present a way to quantify differences between FRFs, in particular violin bridge admittances, that separates the effects in frequency, amplitude and quality factor of the first resonance peaks characterizing the responses. This approach allows us to define a distance between FRFs and clusterise measurements according to this distance. We use two case studies, one based on Finite Element Analysis and another exploiting measurements on real violins, to prove the effectiveness of such representation. In particular, for simulated bridge admittances the proposed distance is able to highlight the different impact of consecutive simulation `steps' on specific vibrational properties and, for real violins, gives a first insight on similar styles of making, as well as opposite ones.

</p>
</details>

<details><summary><b>Generalization over different cellular automata rules learned by a deep feed-forward neural network</b>
<a href="https://arxiv.org/abs/2103.14886">arxiv:2103.14886</a>
&#x1F4C8; 2 <br>
<p>Marcel Aach, Jens Henrik Goebbert, Jenia Jitsev</p></summary>
<p>

**Abstract:** To test generalization ability of a class of deep neural networks, we randomly generate a large number of different rule sets for 2-D cellular automata (CA), based on John Conway's Game of Life. Using these rules, we compute several trajectories for each CA instance. A deep convolutional encoder-decoder network with short and long range skip connections is trained on various generated CA trajectories to predict the next CA state given its previous states. Results show that the network is able to learn the rules of various, complex cellular automata and generalize to unseen configurations. To some extent, the network shows generalization to rule sets and neighborhood sizes that were not seen during the training at all. Code to reproduce the experiments is publicly available at: https://github.com/SLAMPAI/generalization-cellular-automata

</p>
</details>

<details><summary><b>Human-in-the-loop Handling of Knowledge Drift</b>
<a href="https://arxiv.org/abs/2103.14874">arxiv:2103.14874</a>
&#x1F4C8; 2 <br>
<p>Andrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, Stefano Teso</p></summary>
<p>

**Abstract:** We introduce and study knowledge drift (KD), a complex form of drift that occurs in hierarchical classification. Under KD the vocabulary of concepts, their individual distributions, and the is-a relations between them can all change over time. The main challenge is that, since the ground-truth concept hierarchy is unobserved, it is hard to tell apart different forms of KD. For instance, introducing a new is-a relation between two concepts might be confused with individual changes to those concepts, but it is far from equivalent. Failure to identify the right kind of KD compromises the concept hierarchy used by the classifier, leading to systematic prediction errors. Our key observation is that in many human-in-the-loop applications (like smart personal assistants) the user knows whether and what kind of drift occurred recently. Motivated by this, we introduce TRCKD, a novel approach that combines automated drift detection and adaptation with an interactive stage in which the user is asked to disambiguate between different kinds of KD. In addition, TRCKD implements a simple but effective knowledge-aware adaptation strategy. Our simulations show that often a handful of queries to the user are enough to substantially improve prediction performance on both synthetic and realistic data.

</p>
</details>

<details><summary><b>Co-Imitation Learning without Expert Demonstration</b>
<a href="https://arxiv.org/abs/2103.14823">arxiv:2103.14823</a>
&#x1F4C8; 2 <br>
<p>Kun-Peng Ning, Hu Xu, Kun Zhu, Sheng-Jun Huang</p></summary>
<p>

**Abstract:** Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitation Learning framework, validating that the agents can benefit from each other without external supervision.

</p>
</details>

<details><summary><b>BCNN: Binary Complex Neural Network</b>
<a href="https://arxiv.org/abs/2104.10044">arxiv:2104.10044</a>
&#x1F4C8; 1 <br>
<p>Yanfei Li, Tong Geng, Ang Li, Huimin Yu</p></summary>
<p>

**Abstract:** Binarized neural networks, or BNNs, show great promise in edge-side applications with resource limited hardware, but raise the concerns of reduced accuracy. Motivated by the complex neural networks, in this paper we introduce complex representation into the BNNs and propose Binary complex neural network -- a novel network design that processes binary complex inputs and weights through complex convolution, but still can harvest the extraordinary computation efficiency of BNNs. To ensure fast convergence rate, we propose novel BCNN based batch normalization function and weight initialization function. Experimental results on Cifar10 and ImageNet using state-of-the-art network models (e.g., ResNet, ResNetE and NIN) show that BCNN can achieve better accuracy compared to the original BNN models. BCNN improves BNN by strengthening its learning capability through complex representation and extending its applicability to complex-valued input data. The source code of BCNN will be released on GitHub.

</p>
</details>

<details><summary><b>Transient Information Adaptation of Artificial Intelligence: Towards Sustainable Data Processes in Complex Projects</b>
<a href="https://arxiv.org/abs/2104.04067">arxiv:2104.04067</a>
&#x1F4C8; 1 <br>
<p>Nicholas Dacre, Fredrik Kockum, PK Senyo</p></summary>
<p>

**Abstract:** Large scale projects increasingly operate in complicated settings whilst drawing on an array of complex data-points, which require precise analysis for accurate control and interventions to mitigate possible project failure. Coupled with a growing tendency to rely on new information systems and processes in change projects, 90% of megaprojects globally fail to achieve their planned objectives. Renewed interest in the concept of Artificial Intelligence (AI) against a backdrop of disruptive technological innovations, seeks to enhance project managers cognitive capacity through the project lifecycle and enhance project excellence. However, despite growing interest there remains limited empirical insights on project managers ability to leverage AI for cognitive load enhancement in complex settings. As such this research adopts an exploratory sequential linear mixed methods approach to address unresolved empirical issues on transient adaptations of AI in complex projects, and the impact on cognitive load enhancement. Initial thematic findings from semi-structured interviews with domain experts, suggest that in order to leverage AI technologies and processes for sustainable cognitive load enhancement with complex data over time, project managers require improved knowledge and access to relevant technologies that mediate data processes in complex projects, but equally reflect application across different project phases. These initial findings support further hypothesis testing through a larger quantitative study incorporating structural equation modelling to examine the relationship between artificial intelligence and project managers cognitive load with project data in complex contexts.

</p>
</details>

<details><summary><b>On the Stability of Nonlinear Receding Horizon Control: A Geometric Perspective</b>
<a href="https://arxiv.org/abs/2103.15010">arxiv:2103.15010</a>
&#x1F4C8; 1 <br>
<p>Tyler Westenbroek, Max Simchowitz, Michael I. Jordan, S. Shankar Sastry</p></summary>
<p>

**Abstract:** The widespread adoption of nonlinear Receding Horizon Control (RHC) strategies by industry has led to more than 30 years of intense research efforts to provide stability guarantees for these methods. However, current theoretical guarantees require that each (generally nonconvex) planning problem can be solved to (approximate) global optimality, which is an unrealistic requirement for the derivative-based local optimization methods generally used in practical implementations of RHC. This paper takes the first step towards understanding stability guarantees for nonlinear RHC when the inner planning problem is solved to first-order stationary points, but not necessarily global optima. Special attention is given to feedback linearizable systems, and a mixture of positive and negative results are provided. We establish that, under certain strong conditions, first-order solutions to RHC exponentially stabilize linearizable systems. Crucially, this guarantee requires that state costs applied to the planning problems are in a certain sense `compatible' with the global geometry of the system, and a simple counter-example demonstrates the necessity of this condition. These results highlight the need to rethink the role of global geometry in the context of optimization-based control.

</p>
</details>

<details><summary><b>On the benefits of robust models in modulation recognition</b>
<a href="https://arxiv.org/abs/2103.14977">arxiv:2103.14977</a>
&#x1F4C8; 1 <br>
<p>Javier Maroto, Gérôme Bovet, Pascal Frossard</p></summary>
<p>

**Abstract:** Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the "signal to perturbation ratio" (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems.

</p>
</details>

<details><summary><b>Automatic differentiation for Riemannian optimization on low-rank matrix and tensor-train manifolds</b>
<a href="https://arxiv.org/abs/2103.14974">arxiv:2103.14974</a>
&#x1F4C8; 1 <br>
<p>Alexander Novikov, Maxim Rakhuba, Ivan Oseledets</p></summary>
<p>

**Abstract:** In scientific computing and machine learning applications, matrices and more general multidimensional arrays (tensors) can often be approximated with the help of low-rank decompositions. Since matrices and tensors of fixed rank form smooth Riemannian manifolds, one of the popular tools for finding low-rank approximations is to use Riemannian optimization. Nevertheless, efficient implementation of Riemannian gradients and Hessians, required in Riemannian optimization algorithms, can be a nontrivial task in practice. Moreover, in some cases, analytic formulas are not even available. In this paper, we build upon automatic differentiation and propose a method that, given an implementation of the function to be minimized, efficiently computes Riemannian gradients and matrix-by-vector products between an approximate Riemannian Hessian and a given vector.

</p>
</details>

<details><summary><b>Frequency-specific segregation and integration of human cerebral cortex: an intrinsic functional atlas</b>
<a href="https://arxiv.org/abs/2103.14907">arxiv:2103.14907</a>
&#x1F4C8; 1 <br>
<p>Zhiguo Luo, Ling-Li Zeng, Hui Shen, Dewen Hu</p></summary>
<p>

**Abstract:** The frequency-specific coupling mechanism of the functional human brain networks underpins its complex cognitive and behavioral functions. Nevertheless, it is not well unveiled what are the frequency-specific subdivisions and network topologies of the human brain. In this study, we estimated functional connectivity of the human cerebral cortex using spectral connection, and conducted frequency-specific parcellation using eigen-clustering and gradient-based methods, and then explored their topological structures. 7T fMRI data of 184 subjects in the HCP dataset were used for parcellation and exploring the topological properties of the functional networks, and 3T fMRI data of another 890 subjects were used to confirm the stability of the frequency-specific topologies. Seven to ten functional networks were stably integrated by two to four dissociable hub categories at specific frequencies, and we proposed an intrinsic functional atlas containing 456 parcels according to the parcellations across frequencies. The results revealed that the functional networks contained stable frequency-specific topologies, which may imply more abundant roles of the functional units and more complex interactions among them.

</p>
</details>

<details><summary><b>Self-adaptive Torque Vectoring Controller Using Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2103.14892">arxiv:2103.14892</a>
&#x1F4C8; 1 <br>
<p>Shayan Taherian, Sampo Kuutti, Marco Visca, Saber Fallah</p></summary>
<p>

**Abstract:** Continuous direct yaw moment control systems such as torque-vectoring controller are an essential part for vehicle stabilization. This controller has been extensively researched with the central objective of maintaining the vehicle stability by providing consistent stable cornering response. The ability of careful tuning of the parameters in a torque-vectoring controller can significantly enhance vehicle's performance and stability. However, without any re-tuning of the parameters, especially in extreme driving conditions e.g. low friction surface or high velocity, the vehicle fails to maintain the stability. In this paper, the utility of Reinforcement Learning (RL) based on Deep Deterministic Policy Gradient (DDPG) as a parameter tuning algorithm for torque-vectoring controller is presented. It is shown that, torque-vectoring controller with parameter tuning via reinforcement learning performs well on a range of different driving environment e.g., wide range of friction conditions and different velocities, which highlight the advantages of reinforcement learning as an adaptive algorithm for parameter tuning. Moreover, the robustness of DDPG algorithm are validated under scenarios which are beyond the training environment of the reinforcement learning algorithm. The simulation has been carried out using a four wheels vehicle model with nonlinear tire characteristics. We compare our DDPG based parameter tuning against a genetic algorithm and a conventional trial-and-error tunning of the torque vectoring controller, and the results demonstrated that the reinforcement learning based parameter tuning significantly improves the stability of the vehicle.

</p>
</details>

<details><summary><b>Towards Tool-Support for Interactive-Machine Learning Applications in the Android Ecosystem</b>
<a href="https://arxiv.org/abs/2103.14852">arxiv:2103.14852</a>
&#x1F4C8; 1 <br>
<p>Muhammad Mehran Sunny, Moritz Berghofer, Ilhan Aslan</p></summary>
<p>

**Abstract:** Consumer applications are becoming increasingly smarter and most of them have to run on device ecosystems. Potential benefits are for example enabling cross-device interaction and seamless user experiences. Essential for today's smart solutions with high performance are machine learning models. However, these models are often developed separately by AI engineers for one specific device and do not consider the challenges and potentials associated with a device ecosystem in which their models have to run. We believe that there is a need for tool-support for AI engineers to address the challenges of implementing, testing, and deploying machine learning models for a next generation of smart interactive consumer applications. This paper presents preliminary results of a series of inquiries, including interviews with AI engineers and experiments for an interactive machine learning use case with a Smartwatch and Smartphone. We identified the themes through interviews and hands-on experience working on our use case and proposed features, such as data collection from sensors and easy testing of the resources consumption of running pre-processing code on the target device, which will serve as tool-support for AI engineers.

</p>
</details>

<details><summary><b>Machine Learning Meets Natural Language Processing -- The story so far</b>
<a href="https://arxiv.org/abs/2104.10213">arxiv:2104.10213</a>
&#x1F4C8; 0 <br>
<p>N. -I. Galanis, P. Vafiadis, K. -G. Mirzaev, G. A. Papakostas</p></summary>
<p>

**Abstract:** Natural Language Processing (NLP) has evolved significantly over the last decade. This paper highlights the most important milestones of this period while trying to pinpoint the contribution of each individual model and algorithm to the overall progress. Furthermore, it focuses on issues still remaining to be solved, emphasizing the groundbreaking proposals of Transformers, BERT, and all the similar attention-based models.

</p>
</details>


[Next Page]({{ '/2021/03/26/2021.03.26.html' | relative_url }})
