Prev: [2022.05.19]({{ '/2022/05/19/2022.05.19.html' | relative_url }})  Next: [2022.05.21]({{ '/2022/05/21/2022.05.21.html' | relative_url }})
{% raw %}
## Summary for 2022-05-20, created on 2022-05-27


<details><summary><b>Nothing makes sense in deep learning, except in the light of evolution</b>
<a href="https://arxiv.org/abs/2205.10320">arxiv:2205.10320</a>
&#x1F4C8; 1550 <br>
<p>Artem Kaznatcheev, Konrad Paul Kording</p></summary>
<p>

**Abstract:** Deep Learning (DL) is a surprisingly successful branch of machine learning. The success of DL is usually explained by focusing analysis on a particular recent algorithm and its traits. Instead, we propose that an explanation of the success of DL must look at the population of all algorithms in the field and how they have evolved over time. We argue that cultural evolution is a useful framework to explain the success of DL. In analogy to biology, we use `development' to mean the process converting the pseudocode or text description of an algorithm into a fully trained model. This includes writing the programming code, compiling and running the program, and training the model. If all parts of the process don't align well then the resultant model will be useless (if the code runs at all!). This is a constraint. A core component of evolutionary developmental biology is the concept of deconstraints -- these are modification to the developmental process that avoid complete failure by automatically accommodating changes in other components. We suggest that many important innovations in DL, from neural networks themselves to hyperparameter optimization and AutoGrad, can be seen as developmental deconstraints. These deconstraints can be very helpful to both the particular algorithm in how it handles challenges in implementation and the overall field of DL in how easy it is for new ideas to be generated. We highlight how our perspective can both advance DL and lead to new insights for evolutionary biology.

</p>
</details>

<details><summary><b>Planning with Diffusion for Flexible Behavior Synthesis</b>
<a href="https://arxiv.org/abs/2205.09991">arxiv:2205.09991</a>
&#x1F4C8; 79 <br>
<p>Michael Janner, Yilun Du, Joshua B. Tenenbaum, Sergey Levine</p></summary>
<p>

**Abstract:** Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.

</p>
</details>

<details><summary><b>Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors</b>
<a href="https://arxiv.org/abs/2205.10279">arxiv:2205.10279</a>
&#x1F4C8; 48 <br>
<p>Ravid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, Andrew Gordon Wilson</p></summary>
<p>

**Abstract:** Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.

</p>
</details>

<details><summary><b>Towards Understanding Grokking: An Effective Theory of Representation Learning</b>
<a href="https://arxiv.org/abs/2205.10343">arxiv:2205.10343</a>
&#x1F4C8; 24 <br>
<p>Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, Mike Williams</p></summary>
<p>

**Abstract:** We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.

</p>
</details>

<details><summary><b>B-cos Networks: Alignment is All We Need for Interpretability</b>
<a href="https://arxiv.org/abs/2205.10268">arxiv:2205.10268</a>
&#x1F4C8; 23 <br>
<p>Moritz Böhle, Mario Fritz, Bernt Schiele</p></summary>
<p>

**Abstract:** We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at https://www.github.com/moboehle/B-cos.

</p>
</details>

<details><summary><b>Self-Supervised Depth Estimation with Isometric-Self-Sample-Based Learning</b>
<a href="https://arxiv.org/abs/2205.10006">arxiv:2205.10006</a>
&#x1F4C8; 15 <br>
<p>Geonho Cha, Ho-Deok Jang, Dongyoon Wee</p></summary>
<p>

**Abstract:** Managing the dynamic regions in the photometric loss formulation has been a main issue for handling the self-supervised depth estimation problem. Most previous methods have alleviated this issue by removing the dynamic regions in the photometric loss formulation based on the masks estimated from another module, making it difficult to fully utilize the training images. In this paper, to handle this problem, we propose an isometric self-sample-based learning (ISSL) method to fully utilize the training images in a simple yet effective way. The proposed method provides additional supervision during training using self-generated images that comply with pure static scene assumption. Specifically, the isometric self-sample generator synthesizes self-samples for each training image by applying random rigid transformations on the estimated depth. Thus both the generated self-samples and the corresponding training image always follow the static scene assumption. We show that plugging our ISSL module into several existing models consistently improves the performance by a large margin. In addition, it also boosts the depth accuracy over different types of scene, i.e., outdoor scenes (KITTI and Make3D) and indoor scene (NYUv2), validating its high effectiveness.

</p>
</details>

<details><summary><b>Lossless Acceleration for Seq2seq Generation with Aggressive Decoding</b>
<a href="https://arxiv.org/abs/2205.10350">arxiv:2205.10350</a>
&#x1F4C8; 14 <br>
<p>Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, Furu Wei</p></summary>
<p>

**Abstract:** We study lossless acceleration for seq2seq generation with a novel decoding algorithm -- Aggressive Decoding. Unlike the previous efforts (e.g., non-autoregressive decoding) speeding up seq2seq generation at the cost of quality loss, our approach aims to yield the identical (or better) generation compared with autoregressive decoding but in a significant speedup, achieved by innovative cooperation of aggressive decoding and verification that are both efficient due to parallel computing.
  We propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1) For the seq2seq tasks whose inputs and outputs are highly similar (e.g., Grammatical Error Correction), we propose Input-guided Aggressive Decoding (IAD) that aggressively copies from the input sentence as drafted decoded tokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine Translation), we propose Generalized Aggressive Decoding (GAD) that first employs an additional non-autoregressive decoding model for aggressive decoding and then verifies in parallel in the autoregressive manner.
  We test Aggressive Decoding on the most popular 6-layer Transformer model on GPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a 7x-9x speedup for the Transformer in Grammatical Error Correction and Text Simplification tasks with the identical results as greedy decoding; 2) For GAD, we observe a 3x-5x speedup with the identical or even better quality in two important seq2seq tasks: Machine Translation and Abstractive Summarization. Moreover, Aggressive Decoding can benefit even more from stronger computing devices that are better at parallel computing. Given the lossless quality as well as significant and promising speedup, we believe Aggressive Decoding may potentially evolve into a de facto standard for efficient and lossless seq2seq generation in the near future.

</p>
</details>

<details><summary><b>SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System</b>
<a href="https://arxiv.org/abs/2205.10034">arxiv:2205.10034</a>
&#x1F4C8; 10 <br>
<p>Liang Shen, Zhihua Wu, WeiBao Gong, Hongxiang Hao, Yangfan Bai, HuaChao Wu, Xinxuan Wu, Haoyi Xiong, Dianhai Yu, Yanjun Ma</p></summary>
<p>

**Abstract:** With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a ring of sections to load the model, and executes the computation tasks across the memory sections in a round-robin manner for efficient inference. We carried out extensive experiments to evaluate SE-MoE, where SE-MoE successfully trains a Unified Feature Optimization (UFO) model with a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The comparison against the state-of-the-art shows that SE-MoE outperformed DeepSpeed with 33% higher throughput (tokens per second) in training and 13% higher throughput in inference in general. Particularly, under unbalanced MoE Tasks, e.g., UFO, SE-MoE achieved 64% higher throughput with 18% lower memory footprints. The code of the framework will be released on: https://github.com/PaddlePaddle/Paddle.

</p>
</details>

<details><summary><b>Translating Hanja historical documents to understandable Korean and English</b>
<a href="https://arxiv.org/abs/2205.10019">arxiv:2205.10019</a>
&#x1F4C8; 9 <br>
<p>Juhee Son, Jiho Jin, Haneul Yoo, JinYeong Bak, Kyunghyun Cho, Alice Oh</p></summary>
<p>

**Abstract:** The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon, the 500-year kingdom preceding the modern nation of Korea. The Annals were originally written in an archaic Korean writing system, `Hanja', and translated into Korean from 1968 to 1993. However, this translation was literal and contained many archaic Korean words; thus, a new expert translation effort began in 2012, completing the records of only one king in a decade. Also, expert translators are working on an English translation, of which only one king's records are available because of the high cost and slow progress. Thus, we propose H2KE, the neural machine translation model that translates Hanja historical documents to understandable Korean and English. Based on the multilingual neural machine translation approach, it translates the historical document written in Hanja, using both the full dataset of outdated Korean translation and a small dataset of recently translated Korean and English. We compare our method with two baselines: one is a recent model that simultaneously learns to restore and translate Hanja historical document and the other is the transformer that trained on newly translated corpora only. The results show that our method significantly outperforms the baselines in terms of BLEU score in both modern Korean and English translations. We also conduct a human evaluation that shows that our translation is preferred over the original expert translation.

</p>
</details>

<details><summary><b>ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities</b>
<a href="https://arxiv.org/abs/2205.10312">arxiv:2205.10312</a>
&#x1F4C8; 8 <br>
<p>Yunjun Gao, Xiaoze Liu, Junyang Wu, Tianyi Li, Pengfei Wang, Lu Chen</p></summary>
<p>

**Abstract:** Entity alignment (EA) aims at finding equivalent entities in different knowledge graphs (KGs). Embedding-based approaches have dominated the EA task in recent years. Those methods face problems that come from the geometric properties of embedding vectors, including hubness and isolation. To solve these geometric problems, many normalization approaches have been adopted to EA. However, the increasing scale of KGs renders it is hard for EA models to adopt the normalization processes, thus limiting their usage in real-world applications. To tackle this challenge, we present ClusterEA, a general framework that is capable of scaling up EA models and enhancing their results by leveraging normalization methods on mini-batches with a high entity equivalent rate. ClusterEA contains three components to align entities between large-scale KGs, including stochastic training, ClusterSampler, and SparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic fashion to produce entity embeddings. Based on the embeddings, a novel ClusterSampler strategy is proposed for sampling highly overlapped mini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes local and global similarity and then fuses all similarity matrices to obtain the final similarity matrix. Extensive experiments with real-life datasets on EA benchmarks offer insight into the proposed framework, and suggest that it is capable of outperforming the state-of-the-art scalable EA framework by up to 8 times in terms of Hits@1.

</p>
</details>

<details><summary><b>A General Framework for quantifying Aleatoric and Epistemic uncertainty in Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2205.09968">arxiv:2205.09968</a>
&#x1F4C8; 8 <br>
<p>Sai Munikoti, Deepesh Agarwal, Laya Das, Balasubramaniam Natarajan</p></summary>
<p>

**Abstract:** Graph Neural Networks (GNN) provide a powerful framework that elegantly integrates Graph theory with Machine learning for modeling and analysis of networked data. We consider the problem of quantifying the uncertainty in predictions of GNN stemming from modeling errors and measurement uncertainty. We consider aleatoric uncertainty in the form of probabilistic links and noise in feature vector of nodes, while epistemic uncertainty is incorporated via a probability distribution over the model parameters. We propose a unified approach to treat both sources of uncertainty in a Bayesian framework, where Assumed Density Filtering is used to quantify aleatoric uncertainty and Monte Carlo dropout captures uncertainty in model parameters. Finally, the two sources of uncertainty are aggregated to estimate the total uncertainty in predictions of a GNN. Results in the real-world datasets demonstrate that the Bayesian model performs at par with a frequentist model and provides additional information about predictions uncertainty that are sensitive to uncertainties in the data and model.

</p>
</details>

<details><summary><b>Scaling Laws and Interpretability of Learning from Repeated Data</b>
<a href="https://arxiv.org/abs/2205.10487">arxiv:2205.10487</a>
&#x1F4C8; 7 <br>
<p>Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, Sam McCandlish</p></summary>
<p>

**Abstract:** Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.

</p>
</details>

<details><summary><b>Masterful: A Training Platform for Computer Vision Models</b>
<a href="https://arxiv.org/abs/2205.10469">arxiv:2205.10469</a>
&#x1F4C8; 7 <br>
<p>Samuel Wookey, Yaoshiang Ho, Tom Rikert, Juan David Gil Lopez, Juan Manuel Muñoz Beancur, Santiago Cortes, Ray Tawil, Aaron Sabin, Jack Lynch, Travis Harper, Nikhil Gajendrakumar</p></summary>
<p>

**Abstract:** Masterful is a software platform to train deep learning computer vision models. Data and model architecture are inputs to the platform, and the output is a trained model. The platform's primary goal is to maximize a trained model's accuracy, which it achieves through its regularization and semi-supervised learning implementations. The platform's secondary goal is to minimize the amount of manual experimentation typically required to tune training hyperparameters, which it achieves via multiple metalearning algorithms which are custom built to control the platform's regularization and semi-supervised learning implementations. The platform's tertiary goal is to minimize the computing resources required to train a model, which it achieves via another set of metalearning algorithms which are purpose built to control Tensorflow's optimization implementations. The platform builds on top of Tensorflow's data management, architecture, automatic differentiation, and optimization implementations.

</p>
</details>

<details><summary><b>EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures</b>
<a href="https://arxiv.org/abs/2205.10390">arxiv:2205.10390</a>
&#x1F4C8; 7 <br>
<p>Alex Morehead, Xiao Chen, Tianqi Wu, Jian Liu, Jianlin Cheng</p></summary>
<p>

**Abstract:** Protein complexes are macromolecules essential to the functioning and well-being of all living organisms. As the structure of a protein complex, in particular its region of interaction between multiple protein subunits (i.e., chains), has a notable influence on the biological function of the complex, computational methods that can quickly and effectively be used to refine and assess the quality of a protein complex's 3D structure can directly be used within a drug discovery pipeline to accelerate the development of new therapeutics and improve the efficacy of future vaccines. In this work, we introduce the Equivariant Graph Refiner (EGR), a novel E(3)-equivariant graph neural network (GNN) for multi-task structure refinement and assessment of protein complexes. Our experiments on new, diverse protein complex datasets, all of which we make publicly available in this work, demonstrate the state-of-the-art effectiveness of EGR for atomistic refinement and assessment of protein complexes and outline directions for future work in the field. In doing so, we establish a baseline for future studies in macromolecular refinement and structure analysis.

</p>
</details>

<details><summary><b>Seeking entropy: complex behavior from intrinsic motivation to occupy action-state path space</b>
<a href="https://arxiv.org/abs/2205.10316">arxiv:2205.10316</a>
&#x1F4C8; 7 <br>
<p>Jorge Ramírez-Ruiz, Dmytro Grytskyy, Rubén Moreno-Bote</p></summary>
<p>

**Abstract:** Intrinsic motivation generates behaviors that do not necessarily lead to immediate reward, but help exploration and learning. Here we show that agents having the sole goal of maximizing occupancy of future actions and states, that is, moving and exploring on the long term, are capable of complex behavior without any reference to external rewards. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy with the optimal state-value function, from where we prove uniqueness of the solution of the associated Bellman equation and convergence of our algorithm to the optimal state-value function. Using discrete and continuous state tasks, we show that `dancing', hide-and-seek and a basic form of altruistic behavior naturally result from entropy seeking without external rewards. Intrinsically motivated agents can objectively determine what states constitute rewards, exploiting them to ultimately maximize action-state path entropy.

</p>
</details>

<details><summary><b>The Fairness of Credit Scoring Models</b>
<a href="https://arxiv.org/abs/2205.10200">arxiv:2205.10200</a>
&#x1F4C8; 7 <br>
<p>Christophe Hurlin, Christophe Pérignon, Sébastien Saurin</p></summary>
<p>

**Abstract:** In credit markets, screening algorithms aim to discriminate between good-type and bad-type borrowers. However, when doing so, they also often discriminate between individuals sharing a protected attribute (e.g. gender, age, racial origin) and the rest of the population. In this paper, we show how (1) to test whether there exists a statistically significant difference between protected and unprotected groups, which we call lack of fairness and (2) to identify the variables that cause the lack of fairness. We then use these variables to optimize the fairness-performance trade-off. Our framework provides guidance on how algorithmic fairness can be monitored by lenders, controlled by their regulators, and improved for the benefit of protected groups.

</p>
</details>

<details><summary><b>SALTED: A Framework for SAlient Long-Tail Translation Error Detection</b>
<a href="https://arxiv.org/abs/2205.09988">arxiv:2205.09988</a>
&#x1F4C8; 7 <br>
<p>Vikas Raunak, Matt Post, Arul Menezes</p></summary>
<p>

**Abstract:** Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems in MT. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably in Neural Machine Translation (NMT), greatly undermine the reliability of state-of-the-art MT systems. Consequently, it is important to have visibility into these problems during model development. Towards this direction, we introduce SALTED, a specifications-based framework for behavioral testing of MT models that provides fine-grained views of salient long-tail errors, permitting trustworthy visibility into previously invisible problems. At the core of our approach is the development of high-precision detectors that flag errors (or alternatively, verify output correctness) between a source sentence and a system output. We demonstrate that such detectors could be used not just to identify salient long-tail errors in MT systems, but also for higher-recall filtering of the training data, fixing targeted errors with model fine-tuning in NMT and generating novel data for metamorphic testing to elicit further bugs in models.

</p>
</details>

<details><summary><b>Mapping Emulation for Knowledge Distillation</b>
<a href="https://arxiv.org/abs/2205.10490">arxiv:2205.10490</a>
&#x1F4C8; 6 <br>
<p>Jing Ma, Xiang Xiang, Zihan Zhang, Yuwen Tan, Yiming Wan, Zhigang Zeng, Dacheng Tao</p></summary>
<p>

**Abstract:** This paper formalizes the source-blind knowledge distillation problem that is essential to federated learning. A new geometric perspective is presented to view such a problem as aligning generated distributions between the teacher and student. With its guidance, a new architecture MEKD is proposed to emulate the inverse mapping through generative adversarial training. Unlike mimicking logits and aligning logit distributions, reconstructing the mapping from classifier-logits has a geometric intuition of decreasing empirical distances, and theoretical guarantees using the universal function approximation and optimal mass transportation theories. A new algorithm is also proposed to train the student model that reaches the teacher's performance source-blindly. On various benchmarks, MEKD outperforms existing source-blind KD methods, explainable with ablation studies and visualized results.

</p>
</details>

<details><summary><b>Modernizing Open-Set Speech Language Identification</b>
<a href="https://arxiv.org/abs/2205.10397">arxiv:2205.10397</a>
&#x1F4C8; 6 <br>
<p>Mustafa Eyceoz, Justin Lee, Homayoon Beigi</p></summary>
<p>

**Abstract:** While most modern speech Language Identification methods are closed-set, we want to see if they can be modified and adapted for the open-set problem. When switching to the open-set problem, the solution gains the ability to reject an audio input when it fails to match any of our known language options. We tackle the open-set task by adapting two modern-day state-of-the-art approaches to closed-set language identification: the first using a CRNN with attention and the second using a TDNN. In addition to enhancing our input feature embeddings using MFCCs, log spectral features, and pitch, we will be attempting two approaches to out-of-set language detection: one using thresholds, and the other essentially performing a verification task. We will compare both the performance of the TDNN and the CRNN, as well as our detection approaches.

</p>
</details>

<details><summary><b>Test-time Batch Normalization</b>
<a href="https://arxiv.org/abs/2205.10210">arxiv:2205.10210</a>
&#x1F4C8; 6 <br>
<p>Tao Yang, Shenglong Zhou, Yuwang Wang, Yan Lu, Nanning Zheng</p></summary>
<p>

**Abstract:** Deep neural networks often suffer the data distribution shift between training and testing, and the batch statistics are observed to reflect the shift. In this paper, targeting of alleviating distribution shift in test time, we revisit the batch normalization (BN) in the training process and reveals two key insights benefiting test-time optimization: $(i)$ preserving the same gradient backpropagation form as training, and $(ii)$ using dataset-level statistics for robust optimization and inference. Based on the two insights, we propose a novel test-time BN layer design, GpreBN, which is optimized during testing by minimizing Entropy loss. We verify the effectiveness of our method on two typical settings with distribution shift, i.e., domain generalization and robustness tasks. Our GpreBN significantly improves the test-time performance and achieves the state of the art results.

</p>
</details>

<details><summary><b>A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline</b>
<a href="https://arxiv.org/abs/2205.10199">arxiv:2205.10199</a>
&#x1F4C8; 6 <br>
<p>Zheng Liu, Yaoming Zhuang, Pengrun Jia, Chengdong Wu, Hongli Xu ang Zhanlin Liu</p></summary>
<p>

**Abstract:** For aquaculture resource evaluation and ecological environment monitoring, automatic detection and identification of marine organisms is critical. However, due to the low quality of underwater images and the characteristics of underwater biological, a lack of abundant features may impede traditional hand-designed feature extraction approaches or CNN-based object detection algorithms, particularly in complex underwater environment. Therefore, the goal of this paper is to perform object detection in the underwater environment. This paper proposed a novel method for capturing feature information, which adds the convolutional block attention module (CBAM) to the YOLOv5 backbone. The interference of underwater creature characteristics on object characteristics is decreased, and the output of the backbone network to object information is enhanced. In addition, the self-adaptive global histogram stretching algorithm (SAGHS) is designed to eliminate the degradation problems such as low contrast and color loss caused by underwater environmental information to better restore image quality. Extensive experiments and comprehensive evaluation on the URPC2021 benchmark dataset demonstrate the effectiveness and adaptivity of our methods. Beyond that, this paper conducts an exhaustive analysis of the role of training data on performance.

</p>
</details>

<details><summary><b>The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks</b>
<a href="https://arxiv.org/abs/2205.10144">arxiv:2205.10144</a>
&#x1F4C8; 6 <br>
<p>Lukas S. Huber, Robert Geirhos, Felix A. Wichmann</p></summary>
<p>

**Abstract:** In laboratory object recognition tasks based on undistorted photographs, both adult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike adults', whose object recognition performance is robust against a wide range of image distortions, DNNs trained on standard ImageNet (1.3M images) perform poorly on distorted images. However, the last two years have seen impressive gains in DNN distortion robustness, predominantly achieved through ever-increasing large-scale datasets$\unicode{x2014}$orders of magnitude larger than ImageNet. While this simple brute-force approach is very effective in achieving human-level robustness in DNNs, it raises the question of whether human robustness, too, is simply due to extensive experience with (distorted) visual input during childhood and beyond. Here we investigate this question by comparing the core object recognition performance of 146 children (aged 4$\unicode{x2013}$15) against adults and against DNNs. We find, first, that already 4$\unicode{x2013}$6 year-olds showed remarkable robustness to image distortions and outperform DNNs trained on ImageNet. Second, we estimated the number of $\unicode{x201C}$images$\unicode{x201D}$ children have been exposed to during their lifetime. Compared to various DNNs, children's high robustness requires relatively little data. Third, when recognizing objects children$\unicode{x2014}$like adults but unlike DNNs$\unicode{x2014}$rely heavily on shape but not on texture cues. Together our results suggest that the remarkable robustness to distortions emerges early in the developmental trajectory of human object recognition and is unlikely the result of a mere accumulation of experience with distorted visual input. Even though current DNNs match human performance regarding robustness they seem to rely on different and more data-hungry strategies to do so.

</p>
</details>

<details><summary><b>Semi-self-supervised Automated ICD Coding</b>
<a href="https://arxiv.org/abs/2205.10088">arxiv:2205.10088</a>
&#x1F4C8; 6 <br>
<p>Hlynur D. Hlynsson, Steindór Ellertsson, Jón F. Daðason, Emil L. Sigurdsson, Hrafn Loftsson</p></summary>
<p>

**Abstract:** Clinical Text Notes (CTNs) contain physicians' reasoning process, written in an unstructured free text format, as they examine and interview patients. In recent years, several studies have been published that provide evidence for the utility of machine learning for predicting doctors' diagnoses from CTNs, a task known as ICD coding. Data annotation is time consuming, particularly when a degree of specialization is needed, as is the case for medical data. This paper presents a method of augmenting a sparsely annotated dataset of Icelandic CTNs with a machine-learned imputation in a semi-self-supervised manner. We train a neural network on a small set of annotated CTNs and use it to extract clinical features from a set of un-annotated CTNs. These clinical features consist of answers to about a thousand potential questions that a physician might find the answers to during a consultation of a patient. The features are then used to train a classifier for the diagnosis of certain types of diseases. We report the results of an evaluation of this data augmentation method over three tiers of data availability to the physician. Our data augmentation method shows a significant positive effect which is diminished when clinical features from the examination of the patient and diagnostics are made available. We recommend our method for augmenting scarce datasets for systems that take decisions based on clinical features that do not include examinations or tests.

</p>
</details>

<details><summary><b>The Unreasonable Effectiveness of Deep Evidential Regression</b>
<a href="https://arxiv.org/abs/2205.10060">arxiv:2205.10060</a>
&#x1F4C8; 6 <br>
<p>Nis Meinert, Jakob Gawlikowski, Alexander Lavin</p></summary>
<p>

**Abstract:** There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to propose corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracted from NNs.

</p>
</details>

<details><summary><b>Quantum Kerr Learning</b>
<a href="https://arxiv.org/abs/2205.12004">arxiv:2205.12004</a>
&#x1F4C8; 5 <br>
<p>Junyu Liu, Changchun Zhong, Matthew Otten, Cristian L. Cortes, Chaoyang Ti, Stephen K Gray, Xu Han</p></summary>
<p>

**Abstract:** Quantum machine learning is a rapidly evolving area that could facilitate important applications for quantum computing and significantly impact data science. In our work, we argue that a single Kerr mode might provide some extra quantum enhancements when using quantum kernel methods based on various reasons from complexity theory and physics. Furthermore, we establish an experimental protocol, which we call \emph{quantum Kerr learning} based on circuit QED. A detailed study using the kernel method, neural tangent kernel theory, first-order perturbation theory of the Kerr non-linearity, and non-perturbative numerical simulations, shows quantum enhancements could happen in terms of the convergence time and the generalization error, while explicit protocols are also constructed for higher-dimensional input data.

</p>
</details>

<details><summary><b>DKG: A Descriptive Knowledge Graph for Explaining Relationships between Entities</b>
<a href="https://arxiv.org/abs/2205.10479">arxiv:2205.10479</a>
&#x1F4C8; 5 <br>
<p>Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-mei Hwu</p></summary>
<p>

**Abstract:** In this paper, we propose Descriptive Knowledge Graph (DKG) - an open and interpretable form of modeling relationships between entities. In DKGs, relationships between entities are represented by relation descriptions. For instance, the relationship between entities of machine learning and algorithm can be described as "Machine learning explores the study and construction of algorithms that can learn from and make predictions on data." To construct DKGs, we propose a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and a transformer-based relation description synthesizing model to generate relation descriptions. Experiments demonstrate that our system can extract and generate high-quality relation descriptions for explaining entity relationships.

</p>
</details>

<details><summary><b>DeepStruct: Pretraining of Language Models for Structure Prediction</b>
<a href="https://arxiv.org/abs/2205.10475">arxiv:2205.10475</a>
&#x1F4C8; 5 <br>
<p>Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song</p></summary>
<p>

**Abstract:** We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models on a collection of task-agnostic corpora to generate structures from text. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.

</p>
</details>

<details><summary><b>Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training</b>
<a href="https://arxiv.org/abs/2205.10471">arxiv:2205.10471</a>
&#x1F4C8; 5 <br>
<p>Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, Michael R. Lyu</p></summary>
<p>

**Abstract:** Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.

</p>
</details>

<details><summary><b>Robust Sensible Adversarial Learning of Deep Neural Networks for Image Classification</b>
<a href="https://arxiv.org/abs/2205.10457">arxiv:2205.10457</a>
&#x1F4C8; 5 <br>
<p>Jungeum Kim, Xiao Wang</p></summary>
<p>

**Abstract:** The idea of robustness is central and critical to modern statistical analysis. However, despite the recent advances of deep neural networks (DNNs), many studies have shown that DNNs are vulnerable to adversarial attacks. Making imperceptible changes to an image can cause DNN models to make the wrong classification with high confidence, such as classifying a benign mole as a malignant tumor and a stop sign as a speed limit sign. The trade-off between robustness and standard accuracy is common for DNN models. In this paper, we introduce sensible adversarial learning and demonstrate the synergistic effect between pursuits of standard natural accuracy and robustness. Specifically, we define a sensible adversary which is useful for learning a robust model while keeping high natural accuracy. We theoretically establish that the Bayes classifier is the most robust multi-class classifier with the 0-1 loss under sensible adversarial learning. We propose a novel and efficient algorithm that trains a robust model using implicit loss truncation. We apply sensible adversarial learning for large-scale image classification to a handwritten digital image dataset called MNIST and an object recognition colored image dataset called CIFAR10. We have performed an extensive comparative study to compare our method with other competitive methods. Our experiments empirically demonstrate that our method is not sensitive to its hyperparameter and does not collapse even with a small model capacity while promoting robustness against various attacks and keeping high natural accuracy.

</p>
</details>

<details><summary><b>Down and Across: Introducing Crossword-Solving as a New NLP Benchmark</b>
<a href="https://arxiv.org/abs/2205.10442">arxiv:2205.10442</a>
&#x1F4C8; 5 <br>
<p>Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, Anna Rumshisky</p></summary>
<p>

**Abstract:** Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle. In this work, we introduce solving crossword puzzles as a new natural language understanding task. We release the specification of a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles. These puzzles include a diverse set of clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank, abbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues that depend on the answers to other clues. We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clue-answer pairs. For the question answering task, our baselines include several sequence-to-sequence and retrieval-based generative models. We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle. Finally, we propose an evaluation framework which consists of several complementary performance metrics.

</p>
</details>

<details><summary><b>Towards Better Understanding Attribution Methods</b>
<a href="https://arxiv.org/abs/2205.10435">arxiv:2205.10435</a>
&#x1F4C8; 5 <br>
<p>Sukrut Rao, Moritz Böhle, Bernt Schiele</p></summary>
<p>

**Abstract:** Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For more systematic visualizations, we propose a scheme (AggAtt) to qualitatively evaluate the methods on complete datasets. We use these evaluation schemes to study strengths and shortcomings of some widely used attribution methods. Finally, we propose a post-processing smoothing step that significantly improves the performance of some attribution methods, and discuss its applicability.

</p>
</details>

<details><summary><b>DELMAR: Deep Linear Matrix Approximately Reconstruction to Extract Hierarchical Functional Connectivity in the Human Brain</b>
<a href="https://arxiv.org/abs/2205.10374">arxiv:2205.10374</a>
&#x1F4C8; 5 <br>
<p>Wei Zhang, Yu Bao</p></summary>
<p>

**Abstract:** The Matrix Decomposition techniques have been a vital computational approach to analyzing the hierarchy of functional connectivity in the human brain. However, there are still four shortcomings of these methodologies: 1). Large training samples; 2). Manually tuning hyperparameters; 3). Time-consuming and require extensive computational source; 4). It cannot guarantee convergence to a unique fixed point.
  Therefore, we propose a novel deep matrix factorization technique called Deep Linear Matrix Approximate Reconstruction (DELMAR) to bridge the abovementioned gaps. The advantages of the proposed method are: at first, proposed DELMAR can estimate the important hyperparameters automatically; furthermore, DELMAR employs the matrix backpropagation to reduce the potential accumulative errors; finally, an orthogonal projection is introduced to update all variables of DELMAR rather than directly calculating the inverse matrices.
  The validation experiments of three peer methods and DELMAR using real functional MRI signal of the human brain demonstrates that our proposed method can efficiently identify the spatial feature in fMRI signal even faster and more accurately than other peer methods. Moreover, the theoretical analyses indicate that DELMAR can converge to the unique fixed point and even enable the accurate approximation of original input as DNNs.

</p>
</details>

<details><summary><b>What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment</b>
<a href="https://arxiv.org/abs/2205.10327">arxiv:2205.10327</a>
&#x1F4C8; 5 <br>
<p>Nathan Kallus</p></summary>
<p>

**Abstract:** The fundamental problem of causal inference -- that we never observe counterfactuals -- prevents us from identifying how many might be negatively affected by a proposed intervention. If, in an A/B test, half of users click (or buy, or watch, or renew, etc.), whether exposed to the standard experience A or a new one B, hypothetically it could be because the change affects no one, because the change positively affects half the user population to go from no-click to click while negatively affecting the other half, or something in between. While unknowable, this impact is clearly of material importance to the decision to implement a change or not, whether due to fairness, long-term, systemic, or operational considerations. We therefore derive the tightest-possible (i.e., sharp) bounds on the fraction negatively affected (and other related estimands) given data with only factual observations, whether experimental or observational. Naturally, the more we can stratify individuals by observable covariates, the tighter the sharp bounds. Since these bounds involve unknown functions that must be learned from data, we develop a robust inference algorithm that is efficient almost regardless of how and how fast these functions are learned, remains consistent when some are mislearned, and still gives valid conservative bounds when most are mislearned. Our methodology altogether therefore strongly supports credible conclusions: it avoids spuriously point-identifying this unknowable impact, focusing on the best bounds instead, and it permits exceedingly robust inference on these. We demonstrate our method in simulation studies and in a case study of career counseling for the unemployed.

</p>
</details>

<details><summary><b>LeNSE: Learning To Navigate Subgraph Embeddings for Large-Scale Combinatorial Optimisation</b>
<a href="https://arxiv.org/abs/2205.10106">arxiv:2205.10106</a>
&#x1F4C8; 5 <br>
<p>David Ireland, Giovanni Montana</p></summary>
<p>

**Abstract:** Combinatorial Optimisation problems arise in several application domains and are often formulated in terms of graphs. Many of these problems are NP-hard, but exact solutions are not always needed. Several heuristics have been developed to provide near-optimal solutions; however, they do not typically scale well with the size of the graph. We propose a low-complexity approach for identifying a (possibly much smaller) subgraph of the original graph where the heuristics can be run in reasonable time and with a high likelihood of finding a global near-optimal solution. The core component of our approach is LeNSE, a reinforcement learning algorithm that learns how to navigate the space of possible subgraphs using an Euclidean subgraph embedding as its map. To solve CO problems, LeNSE is provided with a discriminative embedding trained using any existing heuristics using only on a small portion of the original graph. When tested on three problems (vertex cover, max-cut and influence maximisation) using real graphs with up to $10$ million edges, LeNSE identifies small subgraphs yielding solutions comparable to those found by running the heuristics on the entire graph, but at a fraction of the total run time.

</p>
</details>

<details><summary><b>A Case of Exponential Convergence Rates for SVM</b>
<a href="https://arxiv.org/abs/2205.10055">arxiv:2205.10055</a>
&#x1F4C8; 5 <br>
<p>Vivien Cabannes, Stefano Vigogna</p></summary>
<p>

**Abstract:** Classification is often the first problem described in introductory machine learning classes. Generalization guarantees of classification have historically been offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on intractable algorithms, which has led to the theory of surrogate methods in classification. Guarantees offered by surrogate methods are based on calibration inequalities, which have been shown to be highly sub-optimal under some margin conditions, failing short to capture exponential convergence phenomena. Those "super" fast rates are becoming to be well understood for smooth surrogates, but the picture remains blurry for non-smooth losses such as the hinge loss, associated with the renowned support vector machines. In this paper, we present a simple mechanism to obtain fast convergence rates and we investigate its usage for SVM. In particular, we show that SVM can exhibit exponential convergence rates even without assuming the hard Tsybakov margin condition.

</p>
</details>

<details><summary><b>Constructive Interpretability with CoLabel: Corroborative Integration, Complementary Features, and Collaborative Learning</b>
<a href="https://arxiv.org/abs/2205.10011">arxiv:2205.10011</a>
&#x1F4C8; 5 <br>
<p>Abhijit Suprem, Sanjyot Vaidya, Suma Cherkadi, Purva Singh, Joao Eduardo Ferreira, Calton Pu</p></summary>
<p>

**Abstract:** Machine learning models with explainable predictions are increasingly sought after, especially for real-world, mission-critical applications that require bias detection and risk mitigation. Inherent interpretability, where a model is designed from the ground-up for interpretability, provides intuitive insights and transparent explanations on model prediction and performance. In this paper, we present CoLabel, an approach to build interpretable models with explanations rooted in the ground truth. We demonstrate CoLabel in a vehicle feature extraction application in the context of vehicle make-model recognition (VMMR). CoLabel performs VMMR with a composite of interpretable features such as vehicle color, type, and make, all based on interpretable annotations of the ground truth labels. First, CoLabel performs corroborative integration to join multiple datasets that each have a subset of desired annotations of color, type, and make. Then, CoLabel uses decomposable branches to extract complementary features corresponding to desired annotations. Finally, CoLabel fuses them together for final predictions. During feature fusion, CoLabel harmonizes complementary branches so that VMMR features are compatible with each other and can be projected to the same semantic space for classification. With inherent interpretability, CoLabel achieves superior performance to the state-of-the-art black-box models, with accuracy of 0.98, 0.95, and 0.94 on CompCars, Cars196, and BoxCars116K, respectively. CoLabel provides intuitive explanations due to constructive interpretability, and subsequently achieves high accuracy and usability in mission-critical situations.

</p>
</details>

<details><summary><b>Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation</b>
<a href="https://arxiv.org/abs/2205.10481">arxiv:2205.10481</a>
&#x1F4C8; 4 <br>
<p>Guanxing Lu, Yuheng Jia, Junhui Hou</p></summary>
<p>

**Abstract:** In this letter, we propose a novel semi-supervised subspace clustering method, which is able to simultaneously augment the initial supervisory information and construct a discriminative affinity matrix. By representing the limited amount of supervisory information as a pairwise constraint matrix, we observe that the ideal affinity matrix for clustering shares the same low-rank structure as the ideal pairwise constraint matrix. Thus, we stack the two matrices into a 3-D tensor, where a global low-rank constraint is imposed to promote the affinity matrix construction and augment the initial pairwise constraints synchronously. Besides, we use the local geometry structure of input samples to complement the global low-rank prior to achieve better affinity matrix learning. The proposed model is formulated as a Laplacian graph regularized convex low-rank tensor representation problem, which is further solved with an alternative iterative algorithm. In addition, we propose to refine the affinity matrix with the augmented pairwise constraints. Comprehensive experimental results on six commonly-used benchmark datasets demonstrate the superiority of our method over state-of-the-art methods. The code is publicly available at https://github.com/GuanxingLu/Subspace-Clustering.

</p>
</details>

<details><summary><b>A Survey on Physiological Signal Based Emotion Recognition</b>
<a href="https://arxiv.org/abs/2205.10466">arxiv:2205.10466</a>
&#x1F4C8; 4 <br>
<p>Zeeshan Ahmad, Naimul Khan</p></summary>
<p>

**Abstract:** Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.

</p>
</details>

<details><summary><b>PSO-Convolutional Neural Networks with Heterogeneous Learning Rate</b>
<a href="https://arxiv.org/abs/2205.10456">arxiv:2205.10456</a>
&#x1F4C8; 4 <br>
<p>Nguyen Huu Phong, Augusto Santos, Bernardete Ribeiro</p></summary>
<p>

**Abstract:** Convolutional Neural Networks (ConvNets) have been candidly deployed in the scope of computer vision and related fields. Nevertheless, the dynamics of training of these neural networks lie still elusive: it is hard and computationally expensive to train them. A myriad of architectures and training strategies have been proposed to overcome this challenge and address several problems in image processing such as speech, image and action recognition as well as object detection. In this article, we propose a novel Particle Swarm Optimization (PSO) based training for ConvNets. In such framework, the vector of weights of each ConvNet is typically cast as the position of a particle in phase space whereby PSO collaborative dynamics intertwines with Stochastic Gradient Descent (SGD) in order to boost training performance and generalization. Our approach goes as follows: i) [warm-up phase] each ConvNet is trained independently via SGD; ii) [collaborative phase] ConvNets share among themselves their current vector of weights (or particle-position) along with their gradient estimates of the Loss function. Distinct step sizes are coined by distinct ConvNets. By properly blending ConvNets with large (possibly random) step-sizes along with more conservative ones, we propose an algorithm with competitive performance with respect to other PSO-based approaches on Cifar-10 (accuracy of 98.31%). These accuracy levels are obtained by resorting to only four ConvNets -- such results are expected to scale with the number of collaborative ConvNets accordingly. We make our source codes available for download https://github.com/leonlha/PSO-ConvNet-Dynamics.

</p>
</details>

<details><summary><b>Dynamic Ensemble Selection Using Fuzzy Hyperboxes</b>
<a href="https://arxiv.org/abs/2205.10438">arxiv:2205.10438</a>
&#x1F4C8; 4 <br>
<p>Reza Davtalab, Rafael M. O. Cruz, Robert Sabourin</p></summary>
<p>

**Abstract:** Most dynamic ensemble selection (DES) methods utilize the K-Nearest Neighbors (KNN) algorithm to estimate the competence of classifiers in a small region surrounding the query sample. However, KNN is very sensitive to the local distribution of the data. Moreover, it also has a high computational cost as it requires storing the whole data in memory and performing multiple distance calculations during inference. Hence, the dependency on the KNN algorithm ends up limiting the use of DES techniques for large-scale problems. This paper presents a new DES framework based on fuzzy hyperboxes called FH-DES. Each hyperbox can represent a group of samples using only two data points (Min and Max corners). Thus, the hyperbox-based system will have less computational complexity than other dynamic selection methods. In addition, despite the KNN-based approaches, the fuzzy hyperbox is not sensitive to the local data distribution. Therefore, the local distribution of the samples does not affect the system's performance. Furthermore, in this research, for the first time, misclassified samples are used to estimate the competence of the classifiers, which has not been observed in previous fusion approaches. Experimental results demonstrate that the proposed method has high classification accuracy while having a lower complexity when compared with the state-of-the-art dynamic selection methods. The implemented code is available at https://github.com/redavtalab/FH-DES_IJCNN.git.

</p>
</details>

<details><summary><b>Using machine learning on new feature sets extracted from 3D models of broken animal bones to classify fragments according to break agent</b>
<a href="https://arxiv.org/abs/2205.10430">arxiv:2205.10430</a>
&#x1F4C8; 4 <br>
<p>Katrina Yezzi-Woodley, Alexander Terwilliger, Jiafeng Li, Eric Chen, Martha Tappen, Jeff Calder, Peter J. Olver</p></summary>
<p>

**Abstract:** Distinguishing agents of bone modification at paleoanthropological sites is at the root of much of the research directed at understanding early hominin exploitation of large animal resources and the effects those subsistence behaviors had on early hominin evolution. However, current methods, particularly in the area of fracture pattern analysis as a signal of marrow exploitation, have failed to overcome equifinality. Furthermore, researchers debate the replicability and validity of current and emerging methods for analyzing bone modifications. Here we present a new approach to fracture pattern analysis aimed at distinguishing bone fragments resulting from hominin bone breakage and those produced by carnivores. This new method uses 3D models of fragmentary bone to extract a much richer dataset that is more transparent and replicable than feature sets previously used in fracture pattern analysis. Supervised machine learning algorithms are properly used to classify bone fragments according to agent of breakage with average mean accuracy of 77% across tests.

</p>
</details>

<details><summary><b>Current Trends and Approaches in Synonyms Extraction: Potential Adaptation to Arabic</b>
<a href="https://arxiv.org/abs/2205.10412">arxiv:2205.10412</a>
&#x1F4C8; 4 <br>
<p>Eman Naser-Karajah, Nabil Arman, Mustafa Jarrar</p></summary>
<p>

**Abstract:** Extracting synonyms from dictionaries or corpora is gaining special attention as synonyms play an important role in improving NLP application performance. This paper presents a survey of the different approaches and trends used in automatically extracting the synonyms. These approaches can be divided into four main categories. The first approach is to find the Synonyms using a translation graph. The second approach is to discover new transition pairs such as (Arabic-English) (English-France) then (Arabic-France). The third approach is to construct new WordNets by exploring synonymy graphs, and the fourth approach is to find similar words from corpora using Deep Learning methods, such as word embeddings and recently BERT models. The paper also presents a comparative analysis between these approaches and highlights potential adaptation to generate synonyms automatically in the Arabic language as future work.

</p>
</details>

<details><summary><b>A SSIM Guided cGAN Architecture For Clinically Driven Generative Image Synthesis of Multiplexed Spatial Proteomics Channels</b>
<a href="https://arxiv.org/abs/2205.10373">arxiv:2205.10373</a>
&#x1F4C8; 4 <br>
<p>Jillur Rahman Saurav, Mohammad Sadegh Nasr, Paul Koomey, Michael Robben, Manfred Huber, Jon Weidanz, Bríd Ryan, Eytan Ruppin, Peng Jiang, Jacob M. Luber</p></summary>
<p>

**Abstract:** Here we present a structural similarity index measure (SSIM) guided conditional Generative Adversarial Network (cGAN) that generatively performs image-to-image (i2i) synthesis to generate photo-accurate protein channels in multiplexed spatial proteomics images. This approach can be utilized to accurately generate missing spatial proteomics channels that were not included during experimental data collection either at the bench or the clinic. Experimental spatial proteomic data from the Human BioMolecular Atlas Program (HuBMAP) was used to generate spatial representations of missing proteins through a U-Net based image synthesis pipeline. HuBMAP channels were hierarchically clustered by the (SSIM) as a heuristic to obtain the minimal set needed to recapitulate the underlying biology represented by the spatial landscape of proteins. We subsequently prove that our SSIM based architecture allows for scaling of generative image synthesis to slides with up to 100 channels, which is better than current state of the art algorithms which are limited to data with 11 channels. We validate these claims by generating a new experimental spatial proteomics data set from human lung adenocarcinoma tissue sections and show that a model trained on HuBMAP can accurately synthesize channels from our new data set. The ability to recapitulate experimental data from sparsely stained multiplexed histological slides containing spatial proteomic will have tremendous impact on medical diagnostics and drug development, and also raises important questions on the medical ethics of utilizing data produced by generative image synthesis in the clinical setting. The algorithm that we present in this paper will allow researchers and clinicians to save time and costs in proteomics based histological staining while also increasing the amount of data that they can generate through their experiments.

</p>
</details>

<details><summary><b>Nonlinear motion separation via untrained generator networks with disentangled latent space variables and applications to cardiac MRI</b>
<a href="https://arxiv.org/abs/2205.10367">arxiv:2205.10367</a>
&#x1F4C8; 4 <br>
<p> Abdullah, Martin Holler, Karl Kunisch, Malena Sabate Landman</p></summary>
<p>

**Abstract:** In this paper, a nonlinear approach to separate different motion types in video data is proposed. This is particularly relevant in dynamic medical imaging (e.g. PET, MRI), where patient motion poses a significant challenge due to its effects on the image reconstruction as well as for its subsequent interpretation. Here, a new method is proposed where dynamic images are represented as the forward mapping of a sequence of latent variables via a generator neural network. The latent variables are structured so that temporal variations in the data are represented via dynamic latent variables, which are independent of static latent variables characterizing the general structure of the frames. In particular, different kinds of motion are also characterized independently of each other via latent space disentanglement using one-dimensional prior information on all but one of the motion types. This representation allows to freeze any selection of motion types, and to obtain accurate independent representations of other dynamics of interest. Moreover, the proposed algorithm is training-free, i.e., all the network parameters are learned directly from a single video. We illustrate the performance of this method on phantom and real-data MRI examples, where we successfully separate respiratory and cardiac motion.

</p>
</details>

<details><summary><b>Actively Tracking the Optimal Arm in Non-Stationary Environments with Mandatory Probing</b>
<a href="https://arxiv.org/abs/2205.10366">arxiv:2205.10366</a>
&#x1F4C8; 4 <br>
<p>Gourab Ghatak</p></summary>
<p>

**Abstract:** We study a novel multi-armed bandit (MAB) setting which mandates the agent to probe all the arms periodically in a non-stationary environment. In particular, we develop \texttt{TS-GE} that balances the regret guarantees of classical Thompson sampling (TS) with the broadcast probing (BP) of all the arms simultaneously in order to actively detect a change in the reward distributions. Once a system-level change is detected, the changed arm is identified by an optional subroutine called group exploration (GE) which scales as $\log_2(K)$ for a $K-$armed bandit setting. We characterize the probability of missed detection and the probability of false-alarm in terms of the environment parameters. The latency of change-detection is upper bounded by $\sqrt{T}$ while within a period of $\sqrt{T}$, all the arms are probed at least once. We highlight the conditions in which the regret guarantee of \texttt{TS-GE} outperforms that of the state-of-the-art algorithms, in particular, \texttt{ADSWITCH} and \texttt{M-UCB}. Furthermore, unlike the existing bandit algorithms, \texttt{TS-GE} can be deployed for applications such as timely status updates, critical control, and wireless energy transfer, which are essential features of next-generation wireless communication networks. We demonstrate the efficacy of \texttt{TS-GE} by employing it in a n industrial internet-of-things (IIoT) network designed for simultaneous wireless information and power transfer (SWIPT).

</p>
</details>

<details><summary><b>Heterformer: A Transformer Architecture for Node Representation Learning on Heterogeneous Text-Rich Networks</b>
<a href="https://arxiv.org/abs/2205.10282">arxiv:2205.10282</a>
&#x1F4C8; 4 <br>
<p>Bowen Jin, Yu Zhang, Qi Zhu, Jiawei Han</p></summary>
<p>

**Abstract:** We study node representation learning on heterogeneous text-rich networks, where nodes and edges are multi-typed and some types of nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have demonstrated their power in encoding network and text signals, respectively, less focus has been given to delicately coupling these two types of models on heterogeneous text-rich networks. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. In this paper, we propose Heterformer, a Heterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our Heterformer alternately stacks two modules - a graph-attention-based neighbor aggregation module and a transformer-based text and neighbor joint encoding module - to facilitate thorough mutual enhancement between network and text signals. Meanwhile, Heterformer is capable of characterizing network heterogeneity and nodes without text information. Comprehensive experiments on three large-scale datasets from different domains demonstrate the superiority of Heterformer over state-of-the-art baselines in link prediction, transductive/inductive node classification, node clustering, and semantics-based retrieval.

</p>
</details>

<details><summary><b>Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions</b>
<a href="https://arxiv.org/abs/2205.10218">arxiv:2205.10218</a>
&#x1F4C8; 4 <br>
<p>Rui Yang, Jie Wang, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, Feng Wu</p></summary>
<p>

**Abstract:** Generalization across different environments with the same tasks is critical for successful applications of visual reinforcement learning (RL) in real scenarios. However, visual distractions -- which are common in real scenes -- from high-dimensional observations can be hurtful to the learned representations in visual RL, thus degrading the performance of generalization. To tackle this problem, we propose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), to extract the task-relevant information by learning reward sequence distributions (RSDs), as the reward signals are task-relevant in RL and invariant to visual distractions. Specifically, to effectively capture the task-relevant information via RSDs, CRESP introduces an auxiliary task -- that is, predicting the characteristic functions of RSDs -- to learn task-relevant representations, because we can well approximate the high-dimensional distributions by leveraging the corresponding characteristic functions. Experiments demonstrate that CRESP significantly improves the performance of generalization on unseen environments, outperforming several state-of-the-arts on DeepMind Control tasks with different visual distractions.

</p>
</details>

<details><summary><b>How to Guide Adaptive Depth Sampling?</b>
<a href="https://arxiv.org/abs/2205.10202">arxiv:2205.10202</a>
&#x1F4C8; 4 <br>
<p>Ilya Tcenov, Guy Gilboa</p></summary>
<p>

**Abstract:** Recent advances in depth sensing technologies allow fast electronic maneuvering of the laser beam, as opposed to fixed mechanical rotations. This will enable future sensors, in principle, to vary in real-time the sampling pattern. We examine here the abstract problem of whether adapting the sampling pattern for a given frame can reduce the reconstruction error or allow a sparser pattern. We propose a constructive generic method to guide adaptive depth sampling algorithms.
  Given a sampling budget B, a depth predictor P and a desired quality measure M, we propose an Importance Map that highlights important sampling locations. This map is defined for a given frame as the per-pixel expected value of M produced by the predictor P, given a pattern of B random samples. This map can be well estimated in a training phase. We show that a neural network can learn to produce a highly faithful Importance Map, given an RGB image. We then suggest an algorithm to produce a sampling pattern for the scene, which is denser in regions that are harder to reconstruct. The sampling strategy of our modular framework can be adjusted according to hardware limitations, type of depth predictor, and any custom reconstruction error measure that should be minimized. We validate through simulations that our approach outperforms grid and random sampling patterns as well as recent state-of-the-art adaptive algorithms.

</p>
</details>

<details><summary><b>Visual Concepts Tokenization</b>
<a href="https://arxiv.org/abs/2205.10093">arxiv:2205.10093</a>
&#x1F4C8; 4 <br>
<p>Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng</p></summary>
<p>

**Abstract:** Obtaining the human-like perception ability of abstracting visual concepts from concrete pixels has always been a fundamental and important target in machine learning research fields such as disentangled representation learning and scene decomposition. Towards this goal, we propose an unsupervised transformer-based Visual Concepts Tokenization framework, dubbed VCT, to perceive an image into a set of disentangled visual concept tokens, with each concept token responding to one type of independent visual concept. Particularly, to obtain these concept tokens, we only use cross-attention to extract visual information from the image tokens layer by layer without self-attention between concept tokens, preventing information leakage across concept tokens. We further propose a Concept Disentangling Loss to facilitate that different concept tokens represent independent visual concepts. The cross-attention and disentangling loss play the role of induction and mutual exclusion for the concept tokens, respectively. Extensive experiments on several popular datasets verify the effectiveness of VCT on the tasks of disentangled representation learning and scene decomposition. VCT achieves the state of the art results by a large margin.

</p>
</details>

<details><summary><b>On Calibration of Ensemble-Based Credal Predictors</b>
<a href="https://arxiv.org/abs/2205.10082">arxiv:2205.10082</a>
&#x1F4C8; 4 <br>
<p>Thomas Mortier, Viktor Bengs, Eyke Hüllermeier, Stijn Luca, Willem Waegeman</p></summary>
<p>

**Abstract:** In recent years, several classification methods that intend to quantify epistemic uncertainty have been proposed, either by producing predictions in the form of second-order distributions or sets of probability distributions. In this work, we focus on the latter, also called credal predictors, and address the question of how to evaluate them: What does it mean that a credal predictor represents epistemic uncertainty in a faithful manner? To answer this question, we refer to the notion of calibration of probabilistic predictors and extend it to credal predictors. Broadly speaking, we call a credal predictor calibrated if it returns sets that cover the true conditional probability distribution. To verify this property for the important case of ensemble-based credal predictors, we propose a novel nonparametric calibration test that generalizes an existing test for probabilistic predictors to the case of credal predictors. Making use of this test, we empirically show that credal predictors based on deep neural networks are often not well calibrated.

</p>
</details>

<details><summary><b>Understanding and Mitigating the Uncertainty in Zero-Shot Translation</b>
<a href="https://arxiv.org/abs/2205.10068">arxiv:2205.10068</a>
&#x1F4C8; 4 <br>
<p>Wenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng Tu, Michael R. Lyu</p></summary>
<p>

**Abstract:** Zero-shot translation is a promising direction for building a comprehensive multilingual neural machine translation (MNMT) system. However, its quality is still not satisfactory due to off-target issues. In this paper, we aim to understand and alleviate the off-target issues from the perspective of uncertainty in zero-shot translation. By carefully examining the translation output and model confidence, we identify two uncertainties that are responsible for the off-target issues, namely, extrinsic data uncertainty and intrinsic model uncertainty. Based on the observations, we propose two light-weight and complementary approaches to denoise the training data for model training, and mask out the vocabulary of the off-target languages in inference. Extensive experiments on both balanced and unbalanced datasets show that our approaches significantly improve the performance of zero-shot translation over strong MNMT baselines. Qualitative analyses provide insights into where our approaches reduce off-target translations

</p>
</details>

<details><summary><b>Posterior Refinement Improves Sample Efficiency in Bayesian Neural Networks</b>
<a href="https://arxiv.org/abs/2205.10041">arxiv:2205.10041</a>
&#x1F4C8; 4 <br>
<p>Agustinus Kristiadi, Runa Eschenhagen, Philipp Hennig</p></summary>
<p>

**Abstract:** Monte Carlo (MC) integration is the de facto method for approximating the predictive distribution of Bayesian neural networks (BNNs). But, even with many MC samples, Gaussian-based BNNs could still yield bad predictive performance due to the posterior approximation's error. Meanwhile, alternatives to MC integration tend to be more expensive or biased. In this work, we experimentally show that the key to good MC-approximated predictive distributions is the quality of the approximate posterior itself. However, previous methods for obtaining accurate posterior approximations are expensive and non-trivial to implement. We, therefore, propose to refine Gaussian approximate posteriors with normalizing flows. When applied to last-layer BNNs, it yields a simple \emph{post hoc} method for improving pre-existing parametric approximations. We show that the resulting posterior approximation is competitive with even the gold-standard full-batch Hamiltonian Monte Carlo.

</p>
</details>

<details><summary><b>Self-Paced Multi-Agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.10016">arxiv:2205.10016</a>
&#x1F4C8; 4 <br>
<p>Wenshuai Zhao, Joni Pajarinen</p></summary>
<p>

**Abstract:** Curriculum reinforcement learning (CRL) aims to speed up learning of a task by changing gradually the difficulty of the task from easy to hard through control of factors such as initial state or environment dynamics. While automating CRL is well studied in the single-agent setting, in multi-agent reinforcement learning (MARL) an open question is whether control of the number of agents with other factors in a principled manner is beneficial, prior approaches typically relying on hand-crafted heuristics. In addition, how the tasks evolve as the number of agents changes remains understudied, which is critical for scaling to more challenging tasks. We introduce self-paced MARL (SPMARL) that enables optimizing the number of agents with other environment factors in a principled way, and, show that usual assumptions such as that fewer agents make the task always easier are not generally valid. The curriculum induced by SPMARL reveals the evolution of tasks w.r.t. number of agents and experiments show that SPMARL improves the performance when the number of agents sufficiently influences task difficulty.

</p>
</details>

<details><summary><b>SafeNet: Mitigating Data Poisoning Attacks on Private Machine Learning</b>
<a href="https://arxiv.org/abs/2205.09986">arxiv:2205.09986</a>
&#x1F4C8; 4 <br>
<p>Harsh Chaudhari, Matthew Jagielski, Alina Oprea</p></summary>
<p>

**Abstract:** Secure multiparty computation (MPC) has been proposed to allow multiple mutually distrustful data owners to jointly train machine learning (ML) models on their combined data. However, the datasets used for training ML models might be under the control of an adversary mounting a data poisoning attack, and MPC prevents inspecting training sets to detect poisoning. We show that multiple MPC frameworks for private ML training are susceptible to backdoor and targeted poisoning attacks. To mitigate this, we propose SafeNet, a framework for building ensemble models in MPC with formal guarantees of robustness to data poisoning attacks. We extend the security definition of private ML training to account for poisoning and prove that our SafeNet design satisfies the definition. We demonstrate SafeNet's efficiency, accuracy, and resilience to poisoning on several machine learning datasets and models. For instance, SafeNet reduces backdoor attack success from 100% to 0% for a neural network model, while achieving 39x faster training and 36x less communication than the four-party MPC framework of Dalskov et al.

</p>
</details>

<details><summary><b>Neur2SP: Neural Two-Stage Stochastic Programming</b>
<a href="https://arxiv.org/abs/2205.12006">arxiv:2205.12006</a>
&#x1F4C8; 3 <br>
<p>Justin Dumouchelle, Rahul Patel, Elias B. Khalil, Merve Bodur</p></summary>
<p>

**Abstract:** Stochastic programming is a powerful modeling framework for decision-making under uncertainty. In this work, we tackle two-stage stochastic programs (2SPs), the most widely applied and studied class of stochastic programming models. Solving 2SPs exactly requires evaluation of an expected value function that is computationally intractable. Additionally, having a mixed-integer linear program (MIP) or a nonlinear program (NLP) in the second stage further aggravates the problem difficulty. In such cases, solving them can be prohibitively expensive even if specialized algorithms that exploit problem structure are employed. Finding high-quality (first-stage) solutions -- without leveraging problem structure -- can be crucial in such settings. We develop Neur2SP, a new method that approximates the expected value function via a neural network to obtain a surrogate model that can be solved more efficiently than the traditional extensive formulation approach. Moreover, Neur2SP makes no assumptions about the problem structure, in particular about the second-stage problem, and can be implemented using an off-the-shelf solver and open-source libraries. Our extensive computational experiments on benchmark 2SP datasets from four problem classes with different structures (containing MIP and NLP second-stage problems) show the efficiency (time) and efficacy (solution quality) of Neur2SP. Specifically, the proposed method takes less than 1.66 seconds across all problems, achieving high-quality solutions even as the number of scenarios increases, an ideal property that is difficult to have for traditional 2SP solution techniques. Namely, the most generic baseline method typically requires minutes to hours to find solutions of comparable quality.

</p>
</details>

<details><summary><b>Enriched Robust Multi-View Kernel Subspace Clustering</b>
<a href="https://arxiv.org/abs/2205.10495">arxiv:2205.10495</a>
&#x1F4C8; 3 <br>
<p>Mengyuan Zhang, Kai Liu</p></summary>
<p>

**Abstract:** Subspace clustering is to find underlying low-dimensional subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. Most existing methods suffer from two critical issues. First, they usually adopt a two-stage framework and isolate the processes of affinity learning, multi-view information fusion and clustering. Second, they assume the data lies in a linear subspace which may fail in practice as most real-world datasets may have non-linearity structures. To address the above issues, in this paper we propose a novel Enriched Robust Multi-View Kernel Subspace Clustering framework where the consensus affinity matrix is learned from both multi-view data and spectral clustering. Due to the objective and constraints which is difficult to optimize, we propose an iterative optimization method which is easy to implement and can yield closed solution in each step. Extensive experiments have validated the superiority of our method over state-of-the-art clustering methods.

</p>
</details>

<details><summary><b>Nuclear Norm Maximization Based Curiosity-Driven Learning</b>
<a href="https://arxiv.org/abs/2205.10484">arxiv:2205.10484</a>
&#x1F4C8; 3 <br>
<p>Chao Chen, Zijian Gao, Kele Xu, Sen Yang, Yiying Li, Bo Ding, Dawei Feng, Huaimin Wang</p></summary>
<p>

**Abstract:** To handle the sparsity of the extrinsic rewards in reinforcement learning, researchers have proposed intrinsic reward which enables the agent to learn the skills that might come in handy for pursuing the rewards in the future, such as encouraging the agent to visit novel states. However, the intrinsic reward can be noisy due to the undesirable environment's stochasticity and directly applying the noisy value predictions to supervise the policy is detrimental to improve the learning performance and efficiency. Moreover, many previous studies employ $\ell^2$ norm or variance to measure the exploration novelty, which will amplify the noise due to the square operation. In this paper, we address aforementioned challenges by proposing a novel curiosity leveraging the nuclear norm maximization (NNM), which can quantify the novelty of exploring the environment more accurately while providing high-tolerance to the noise and outliers. We conduct extensive experiments across a variety of benchmark environments and the results suggest that NNM can provide state-of-the-art performance compared with previous curiosity methods. On 26 Atari games subset, NNM achieves a human-normalized score of 1.09, which doubles that of competitive intrinsic rewards-based approaches. Our code will be released publicly to enhance the reproducibility.

</p>
</details>

<details><summary><b>A Hybrid Model for Forecasting Short-Term Electricity Demand</b>
<a href="https://arxiv.org/abs/2205.10449">arxiv:2205.10449</a>
&#x1F4C8; 3 <br>
<p>Maria Eleni Athanasopoulou, Justina Deveikyte, Alan Mosca, Ilaria Peri, Alessandro Provetti</p></summary>
<p>

**Abstract:** Currently the UK Electric market is guided by load (demand) forecasts published every thirty minutes by the regulator. A key factor in predicting demand is weather conditions, with forecasts published every hour. We present HYENA: a hybrid predictive model that combines feature engineering (selection of the candidate predictor features), mobile-window predictors and finally LSTM encoder-decoders to achieve higher accuracy with respect to mainstream models from the literature. HYENA decreased MAPE loss by 16\% and RMSE loss by 10\% over the best available benchmark model, thus establishing a new state of the art for the UK electric load (and price) forecasting.

</p>
</details>

<details><summary><b>Predicting Seriousness of Injury in a Traffic Accident: A New Imbalanced Dataset and Benchmark</b>
<a href="https://arxiv.org/abs/2205.10441">arxiv:2205.10441</a>
&#x1F4C8; 3 <br>
<p>Paschalis Lagias, George D. Magoulas, Ylli Prifti, Alessandro Provetti</p></summary>
<p>

**Abstract:** The paper introduces a new dataset to assess the performance of machine learning algorithms in the prediction of the seriousness of injury in a traffic accident. The dataset is created by aggregating publicly available datasets from the UK Department for Transport, which are drastically imbalanced with missing attributes sometimes approaching 50\% of the overall data dimensionality. The paper presents the data analysis pipeline starting from the publicly available data of road traffic accidents and ending with predictors of possible injuries and their degree of severity. It addresses the huge incompleteness of public data with a MissForest model. The paper also introduces two baseline approaches to create injury predictors: a supervised artificial neural network and a reinforcement learning model. The dataset can potentially stimulate diverse aspects of machine learning research on imbalanced datasets and the two approaches can be used as baseline references when researchers test more advanced learning algorithms in this area.

</p>
</details>

<details><summary><b>Learning Dense Reward with Temporal Variant Self-Supervision</b>
<a href="https://arxiv.org/abs/2205.10431">arxiv:2205.10431</a>
&#x1F4C8; 3 <br>
<p>Yuning Wu, Jieliang Luo, Hui Li</p></summary>
<p>

**Abstract:** Rewards play an essential role in reinforcement learning. In contrast to rule-based game environments with well-defined reward functions, complex real-world robotic applications, such as contact-rich manipulation, lack explicit and informative descriptions that can directly be used as a reward. Previous effort has shown that it is possible to algorithmically extract dense rewards directly from multimodal observations. In this paper, we aim to extend this effort by proposing a more efficient and robust way of sampling and learning. In particular, our sampling approach utilizes temporal variance to simulate the fluctuating state and action distribution of a manipulation task. We then proposed a network architecture for self-supervised learning to better incorporate temporal information in latent representations. We tested our approach in two experimental setups, namely joint-assembly and door-opening. Preliminary results show that our approach is effective and efficient in learning dense rewards, and the learned rewards lead to faster convergence than baselines.

</p>
</details>

<details><summary><b>Multilingual Normalization of Temporal Expressions with Masked Language Models</b>
<a href="https://arxiv.org/abs/2205.10399">arxiv:2205.10399</a>
&#x1F4C8; 3 <br>
<p>Lukas Lange, Jannik Strötgen, Heike Adel, Dietrich Klakow</p></summary>
<p>

**Abstract:** The detection and normalization of temporal expressions is an important task and a preprocessing step for many applications. However, prior work on normalization is rule-based, which severely limits the applicability in real-world multilingual settings, due to the costly creation of new rules. We propose a novel neural method for normalizing temporal expressions based on masked language modeling. Our multilingual method outperforms prior rule-based systems in many languages, and in particular, for low-resource languages with performance improvements of up to 35 F1 on average compared to the state of the art.

</p>
</details>

<details><summary><b>A Dynamic Weighted Tabular Method for Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2205.10386">arxiv:2205.10386</a>
&#x1F4C8; 3 <br>
<p>Md Ifraham Iqbal, Md. Saddam Hossain Mukta, Ahmed Rafi Hasan</p></summary>
<p>

**Abstract:** Traditional Machine Learning (ML) models like Support Vector Machine, Random Forest, and Logistic Regression are generally preferred for classification tasks on tabular datasets. Tabular data consists of rows and columns corresponding to instances and features, respectively. Past studies indicate that traditional classifiers often produce unsatisfactory results in complex tabular datasets. Hence, researchers attempt to use the powerful Convolutional Neural Networks (CNN) for tabular datasets. Recent studies propose several techniques like SuperTML, Conditional GAN (CTGAN), and Tabular Convolution (TAC) for applying Convolutional Neural Networks (CNN) on tabular data. These models outperform the traditional classifiers and substantially improve the performance on tabular data. This study introduces a novel technique, namely, Dynamic Weighted Tabular Method (DWTM), that uses feature weights dynamically based on statistical techniques to apply CNNs on tabular datasets. The method assigns weights dynamically to each feature based on their strength of associativity to the class labels. Each data point is converted into images and fed to a CNN model. The features are allocated image canvas space based on their weights. The DWTM is an improvement on the previously mentioned methods as it dynamically implements the entire experimental setting rather than using the static configuration provided in the previous methods. Furthermore, it uses the novel idea of using feature weights to create image canvas space. In this paper, the DWTM is applied to six benchmarked tabular datasets and it achieves outstanding performance (i.e., average accuracy = 95%) on all of them.

</p>
</details>

<details><summary><b>Automatic Generation of Synthetic Colonoscopy Videos for Domain Randomization</b>
<a href="https://arxiv.org/abs/2205.10368">arxiv:2205.10368</a>
&#x1F4C8; 3 <br>
<p>Abhishek Dinkar Jagtap, Mattias Heinrich, Marian Himstedt</p></summary>
<p>

**Abstract:** An increasing number of colonoscopic guidance and assistance systems rely on machine learning algorithms which require a large amount of high-quality training data. In order to ensure high performance, the latter has to resemble a substantial portion of possible configurations. This particularly addresses varying anatomy, mucosa appearance and image sensor characteristics which are likely deteriorated by motion blur and inadequate illumination. The limited amount of readily available training data hampers to account for all of these possible configurations which results in reduced generalization capabilities of machine learning models. We propose an exemplary solution for synthesizing colonoscopy videos with substantial appearance and anatomical variations which enables to learn discriminative domain-randomized representations of the interior colon while mimicking real-world settings.

</p>
</details>

<details><summary><b>Diverse super-resolution with pretrained deep hiererarchical VAEs</b>
<a href="https://arxiv.org/abs/2205.10347">arxiv:2205.10347</a>
&#x1F4C8; 3 <br>
<p>Jean Prost, Antoine Houdard, Nicolas Papadakis, Andrés Almansa</p></summary>
<p>

**Abstract:** Image super-resolution is a one-to-many problem, but most deep-learning based methods only provide one single solution to this problem. In this work, we tackle the problem of diverse super-resolution by reusing VD-VAE, a state-of-the art variational autoencoder (VAE). We find that the hierarchical latent representation learned by VD-VAE naturally separates the image low-frequency information, encoded in the latent groups at the top of the hierarchy, from the image high-frequency details, determined by the latent groups at the bottom of the latent hierarchy. Starting from this observation, we design a super-resolution model exploiting the specific structure of VD-VAE latent space. Specifically, we train an encoder to encode low-resolution images in the subset of VD-VAE latent space encoding the low-frequency information, and we combine this encoder with VD-VAE generative model to sample diverse super-resolved version of a low-resolution input. We demonstrate the ability of our method to generate diverse solutions to the super-resolution problem on face super-resolution with upsampling factors x4, x8, and x16.

</p>
</details>

<details><summary><b>Explanatory machine learning for sequential human teaching</b>
<a href="https://arxiv.org/abs/2205.10250">arxiv:2205.10250</a>
&#x1F4C8; 3 <br>
<p>Lun Ai, Johannes Langer, Stephen H. Muggleton, Ute Schmid</p></summary>
<p>

**Abstract:** The topic of comprehensibility of machine-learned theories has recently drawn increasing attention. Inductive Logic Programming (ILP) uses logic programming to derive logic theories from small data based on abduction and induction techniques. Learned theories are represented in the form of rules as declarative descriptions of obtained knowledge. In earlier work, the authors provided the first evidence of a measurable increase in human comprehension based on machine-learned logic rules for simple classification tasks. In a later study, it was found that the presentation of machine-learned explanations to humans can produce both beneficial and harmful effects in the context of game learning. We continue our investigation of comprehensibility by examining the effects of the ordering of concept presentations on human comprehension. In this work, we examine the explanatory effects of curriculum order and the presence of machine-learned explanations for sequential problem-solving. We show that 1) there exist tasks A and B such that learning A before B has a better human comprehension with respect to learning B before A and 2) there exist tasks A and B such that the presence of explanations when learning A contributes to improved human comprehension when subsequently learning B. We propose a framework for the effects of sequential teaching on comprehension based on an existing definition of comprehensibility and provide evidence for support from data collected in human trials. Empirical results show that sequential teaching of concepts with increasing complexity a) has a beneficial effect on human comprehension and b) leads to human re-discovery of divide-and-conquer problem-solving strategies, and c) studying machine-learned explanations allows adaptations of human problem-solving strategy with better performance.

</p>
</details>

<details><summary><b>EXODUS: Stable and Efficient Training of Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2205.10242">arxiv:2205.10242</a>
&#x1F4C8; 3 <br>
<p>Felix Christian Bauer, Gregor Lenz, Saeid Haghighatshoar, Sadique Sheik</p></summary>
<p>

**Abstract:** Spiking Neural Networks (SNNs) are gaining significant traction in machine learning tasks where energy-efficiency is of utmost importance. Training such networks using the state-of-the-art back-propagation through time (BPTT) is, however, very time-consuming. Previous work by Shrestha and Orchard [2018] employs an efficient GPU-accelerated back-propagation algorithm called SLAYER, which speeds up training considerably. SLAYER, however, does not take into account the neuron reset mechanism while computing the gradients, which we argue to be the source of numerical instability. To counteract this, SLAYER introduces a gradient scale hyperparameter across layers, which needs manual tuning. In this paper, (i) we modify SLAYER and design an algorithm called EXODUS, that accounts for the neuron reset mechanism and applies the Implicit Function Theorem (IFT) to calculate the correct gradients (equivalent to those computed by BPTT), (ii) we eliminate the need for ad-hoc scaling of gradients, thus, reducing the training complexity tremendously, (iii) we demonstrate, via computer simulations, that EXODUS is numerically stable and achieves a comparable or better performance than SLAYER especially in various tasks with SNNs that rely on temporal features. Our code is available at https://github.com/synsense/sinabs-exodus.

</p>
</details>

<details><summary><b>Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization</b>
<a href="https://arxiv.org/abs/2205.10217">arxiv:2205.10217</a>
&#x1F4C8; 3 <br>
<p>Simone Bombari, Mohammad Hossein Amani, Marco Mondelli</p></summary>
<p>

**Abstract:** The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide memorization, optimization and generalization guarantees in deep neural networks. A line of work has studied the NTK spectrum for two-layer and deep networks with at least a layer with $Ω(N)$ neurons, $N$ being the number of training samples. Furthermore, there is increasing evidence suggesting that deep networks with sub-linear layer widths are powerful memorizers and optimizers, as long as the number of parameters exceeds the number of samples. Thus, a natural open question is whether the NTK is well conditioned in such a challenging sub-linear setup. In this paper, we answer this question in the affirmative. Our key technical contribution is a lower bound on the smallest NTK eigenvalue for deep networks with the minimum possible over-parameterization: the number of parameters is roughly $Ω(N)$ and, hence, the number of neurons is as little as $Ω(\sqrt{N})$. To showcase the applicability of our NTK bounds, we provide two results concerning memorization capacity and optimization guarantees for gradient descent training.

</p>
</details>

<details><summary><b>On the Decentralization of Blockchain-enabled Asynchronous Federated Learning</b>
<a href="https://arxiv.org/abs/2205.10201">arxiv:2205.10201</a>
&#x1F4C8; 3 <br>
<p>Francesc Wilhelmi, Elia Guerra, Paolo Dini</p></summary>
<p>

**Abstract:** Federated learning (FL), thanks in part to the emergence of the edge computing paradigm, is expected to enable true real-time applications in production environments. However, its original dependence on a central server for orchestration raises several concerns in terms of security, privacy, and scalability. To solve some of these worries, blockchain technology is expected to bring decentralization, robustness, and enhanced trust to FL. The empowerment of FL through blockchain (also referred to as FLchain), however, has some implications in terms of ledger inconsistencies and age of information (AoI), which are naturally inherited from the blockchain's fully decentralized operation. Such issues stem from the fact that, given the temporary ledger versions in the blockchain, FL devices may use different models for training, and that, given the asynchronicity of the FL operation, stale local updates (computed using outdated models) may be generated. In this paper, we shed light on the implications of the FLchain setting and study the effect that both the AoI and ledger inconsistencies have on the FL performance. To that end, we provide a faithful simulation tool that allows capturing the decentralized and asynchronous nature of the FLchain operation.

</p>
</details>

<details><summary><b>Progressive Class Semantic Matching for Semi-supervised Text Classification</b>
<a href="https://arxiv.org/abs/2205.10189">arxiv:2205.10189</a>
&#x1F4C8; 3 <br>
<p>Hai-Ming Xu, Lingqiao Liu, Ehsan Abbasnejad</p></summary>
<p>

**Abstract:** Semi-supervised learning is a promising way to reduce the annotation cost for text-classification. Combining with pre-trained language models (PLMs), e.g., BERT, recent semi-supervised learning methods achieved impressive performance. In this work, we further investigate the marriage between semi-supervised learning and a pre-trained language model. Unlike existing approaches that utilize PLMs only for model parameter initialization, we explore the inherent topic matching capability inside PLMs for building a more powerful semi-supervised learning approach. Specifically, we propose a joint semi-supervised learning process that can progressively build a standard $K$-way classifier and a matching network for the input text and the Class Semantic Representation (CSR). The CSR will be initialized from the given labeled sentences and progressively updated through the training process. By means of extensive experiments, we show that our method can not only bring remarkable improvement to baselines, but also overall be more stable, and achieves state-of-the-art performance in semi-supervised text classification.

</p>
</details>

<details><summary><b>Towards the Generation of Synthetic Images of Palm Vein Patterns: A Review</b>
<a href="https://arxiv.org/abs/2205.10179">arxiv:2205.10179</a>
&#x1F4C8; 3 <br>
<p>Edwin H. Salazar-Jurado, Ruber Hernández-García, Karina Vilches-Ponce, Ricardo J. Barrientos, Marco Mora, Gaurav Jaswal</p></summary>
<p>

**Abstract:** With the recent success of computer vision and deep learning, remarkable progress has been achieved on automatic personal recognition using vein biometrics. However, collecting large-scale real-world training data for palm vein recognition has turned out to be challenging, mainly due to the noise and irregular variations included at the time of acquisition. Meanwhile, existing palm vein recognition datasets are usually collected under near-infrared light, lacking detailed annotations on attributes (e.g., pose), so the influences of different attributes on vein recognition have been poorly investigated. Therefore, this paper examines the suitability of synthetic vein images generated to compensate for the urgent lack of publicly available large-scale datasets. Firstly, we present an overview of recent research progress on palm vein recognition, from the basic background knowledge to vein anatomical structure, data acquisition, public database, and quality assessment procedures. Then, we focus on the state-of-the-art methods that have allowed the generation of vascular structures for biometric purposes and the modeling of biological networks with their respective application domains. In addition, we review the existing research on the generation of style transfer and biological nature-based synthetic palm vein image algorithms. Afterward, we formalize a general flowchart for the creation of a synthetic database comparing real palm vein images and generated synthetic samples to obtain some understanding into the development of the realistic vein imaging system. Ultimately, we conclude by discussing the challenges, insights, and future perspectives in generating synthetic palm vein images for further works.

</p>
</details>

<details><summary><b>Task Relabelling for Multi-task Transfer using Successor Features</b>
<a href="https://arxiv.org/abs/2205.10175">arxiv:2205.10175</a>
&#x1F4C8; 3 <br>
<p>Martin Balla, Diego Perez-Liebana</p></summary>
<p>

**Abstract:** Deep Reinforcement Learning has been very successful recently with various works on complex domains. Most works are concerned with learning a single policy that solves the target task, but is fixed in the sense that if the environment changes the agent is unable to adapt to it. Successor Features (SFs) proposes a mechanism that allows learning policies that are not tied to any particular reward function. In this work we investigate how SFs may be pre-trained without observing any reward in a custom environment that features resource collection, traps and crafting. After pre-training we expose the SF agents to various target tasks and see how well they can transfer to new tasks. Transferring is done without any further training on the SF agents, instead just by providing a task vector. For training the SFs we propose a task relabelling method which greatly improves the agent's performance.

</p>
</details>

<details><summary><b>Swapping Semantic Contents for Mixing Images</b>
<a href="https://arxiv.org/abs/2205.10158">arxiv:2205.10158</a>
&#x1F4C8; 3 <br>
<p>Rémy Sun, Clément Masson, Gilles Hénaff, Nicolas Thome, Matthieu Cord</p></summary>
<p>

**Abstract:** Deep architecture have proven capable of solving many tasks provided a sufficient amount of labeled data. In fact, the amount of available labeled data has become the principal bottleneck in low label settings such as Semi-Supervised Learning. Mixing Data Augmentations do not typically yield new labeled samples, as indiscriminately mixing contents creates between-class samples. In this work, we introduce the SciMix framework that can learn to generator to embed a semantic style code into image backgrounds, we obtain new mixing scheme for data augmentation. We then demonstrate that SciMix yields novel mixed samples that inherit many characteristics from their non-semantic parents. Afterwards, we verify those samples can be used to improve the performance semi-supervised frameworks like Mean Teacher or Fixmatch, and even fully supervised learning on a small labeled dataset.

</p>
</details>

<details><summary><b>Kernel Normalized Convolutional Networks</b>
<a href="https://arxiv.org/abs/2205.10089">arxiv:2205.10089</a>
&#x1F4C8; 3 <br>
<p>Reza Nasirigerdeh, Reihaneh Torkzadehmahani, Daniel Rueckert, Georgios Kaissis</p></summary>
<p>

**Abstract:** Existing deep convolutional neural network (CNN) architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm significantly improves model performance, but performs poorly with smaller batch sizes. To address this limitation, we propose kernel normalization and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art CNNs such as ResNet and DenseNet while forgoing BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets consistently outperform their batch, group, and layer normalized counterparts in terms of both accuracy and convergence rate while maintaining competitive computational efficiency.

</p>
</details>

<details><summary><b>Unintended memorisation of unique features in neural networks</b>
<a href="https://arxiv.org/abs/2205.10079">arxiv:2205.10079</a>
&#x1F4C8; 3 <br>
<p>John Hartley, Sotirios A. Tsaftaris</p></summary>
<p>

**Abstract:** Neural networks pose a privacy risk due to their propensity to memorise and leak training data. We show that unique features occurring only once in training data are memorised by discriminative multi-layer perceptrons and convolutional neural networks trained on benchmark imaging datasets. We design our method for settings where sensitive training data is not available, for example medical imaging. Our setting knows the unique feature, but not the training data, model weights or the unique feature's label. We develop a score estimating a model's sensitivity to a unique feature by comparing the KL divergences of the model's output distributions given modified out-of-distribution images. We find that typical strategies to prevent overfitting do not prevent unique feature memorisation. And that images containing a unique feature are highly influential, regardless of the influence the images's other features. We also find a significant variation in memorisation with training seed. These results imply that neural networks pose a privacy risk to rarely occurring private information. This risk is more pronounced in healthcare applications since sensitive patient information can be memorised when it remains in training data due to an imperfect data sanitisation process.

</p>
</details>

<details><summary><b>Contrastive Learning with Cross-Modal Knowledge Mining for Multimodal Human Activity Recognition</b>
<a href="https://arxiv.org/abs/2205.10071">arxiv:2205.10071</a>
&#x1F4C8; 3 <br>
<p>Razvan Brinzea, Bulat Khaertdinov, Stylianos Asteriadis</p></summary>
<p>

**Abstract:** Human Activity Recognition is a field of research where input data can take many forms. Each of the possible input modalities describes human behaviour in a different way, and each has its own strengths and weaknesses. We explore the hypothesis that leveraging multiple modalities can lead to better recognition. Since manual annotation of input data is expensive and time-consuming, the emphasis is made on self-supervised methods which can learn useful feature representations without any ground truth labels. We extend a number of recent contrastive self-supervised approaches for the task of Human Activity Recognition, leveraging inertial and skeleton data. Furthermore, we propose a flexible, general-purpose framework for performing multimodal self-supervised learning, named Contrastive Multiview Coding with Cross-Modal Knowledge Mining (CMC-CMKM). This framework exploits modality-specific knowledge in order to mitigate the limitations of typical self-supervised frameworks. The extensive experiments on two widely-used datasets demonstrate that the suggested framework significantly outperforms contrastive unimodal and multimodal baselines on different scenarios, including fully-supervised fine-tuning, activity retrieval and semi-supervised learning. Furthermore, it shows performance competitive even compared to supervised methods.

</p>
</details>

<details><summary><b>MaskGAE: Masked Graph Modeling Meets Graph Autoencoders</b>
<a href="https://arxiv.org/abs/2205.10053">arxiv:2205.10053</a>
&#x1F4C8; 3 <br>
<p>Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, Weiqiang Wang</p></summary>
<p>

**Abstract:** We present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from previous graph autoencoders (GAEs), MaskGAE adopts masked graph modeling (MGM) as a principled pretext task: masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to justify the benefits of this pretext task. Theoretically, we establish the connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a number of benchmark datasets, demonstrating the superiority of MaskGAE over several state-of-the-arts on both link prediction and node classification tasks. Our code is publicly available at \url{https://github.com/EdisonLeeeee/MaskGAE}.

</p>
</details>

<details><summary><b>Exploring Extreme Parameter Compression for Pre-trained Language Models</b>
<a href="https://arxiv.org/abs/2205.10036">arxiv:2205.10036</a>
&#x1F4C8; 3 <br>
<p>Yuxin Ren, Benyou Wang, Lifeng Shang, Xin Jiang, Qun Liu</p></summary>
<p>

**Abstract:** Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g., financial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency during compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves $96.7\%$ performance of BERT-base with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and $2.7 \times$ faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT.

</p>
</details>

<details><summary><b>Trend analysis and forecasting air pollution in Rwanda</b>
<a href="https://arxiv.org/abs/2205.10024">arxiv:2205.10024</a>
&#x1F4C8; 3 <br>
<p>Paterne Gahungu, Jean Remy Kubwimana</p></summary>
<p>

**Abstract:** Air pollution is a major public health problem worldwide although the lack of data is a global issue for most low and middle income countries. Ambient air pollution in the form of fine particulate matter (PM2.5) exceeds the World Health Organization guidelines in Rwanda with a daily average of around 42.6 microgram per meter cube. Monitoring and mitigation strategies require an expensive investment in equipment to collect pollution data. Low-cost sensor technology and machine learning methods have appeared as an alternative solution to get reliable information for decision making. This paper analyzes the trend of air pollution in Rwanda and proposes forecasting models suitable to data collected by a network of low-cost sensors deployed in Rwanda.

</p>
</details>

<details><summary><b>HeadText: Exploring Hands-free Text Entry using Head Gestures by Motion Sensing on a Smart Earpiece</b>
<a href="https://arxiv.org/abs/2205.09978">arxiv:2205.09978</a>
&#x1F4C8; 3 <br>
<p>Songlin Xu, Guanjie Wang, Ziyuan Fang, Guangwei Zhang, Guangzhu Shang, Rongde Lu, Liqun He</p></summary>
<p>

**Abstract:** We present HeadText, a hands-free technique on a smart earpiece for text entry by motion sensing. Users input text utilizing only 7 head gestures for key selection, word selection, word commitment and word cancelling tasks. Head gesture recognition is supported by motion sensing on a smart earpiece to capture head moving signals and machine learning algorithms (K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement). A 10-participant user study proved that HeadText could recognize 7 head gestures at an accuracy of 94.29%. After that, the second user study presented that HeadText could achieve a maximum accuracy of 10.65 WPM and an average accuracy of 9.84 WPM for text entry. Finally, we demonstrate potential applications of HeadText in hands-free scenarios for (a). text entry of people with motor impairments, (b). private text entry, and (c). socially acceptable text entry.

</p>
</details>

<details><summary><b>A New Feature Selection Method for LogNNet and its Application for Diagnosis and Prognosis of COVID-19 Disease Using Routine Blood Values</b>
<a href="https://arxiv.org/abs/2205.09974">arxiv:2205.09974</a>
&#x1F4C8; 3 <br>
<p>Mehmet Tahir Huyut, Andrei Velichko</p></summary>
<p>

**Abstract:** Since February-2020, the world has embarked on an intense struggle with the COVID-19 disease, and health systems have come under a tragic pressure as the disease turned into a pandemic. The aim of this study is to determine the most effective routine-blood-values (RBV) in the diagnosis/prognosis of COVID-19 using new feature selection method for LogNNet reservoir neural network. First dataset in this study consists of a total of 5296-patients with a same number of negative and positive covid test. Second dataset consists of a total of 3899-patients with a diagnosis of COVID-19, who were treated in hospital with severe-infected (203) and mildly-infected (3696). The most important RBVs that affect the diagnosis of the disease from the first dataset were mean-corpuscular-hemoglobin-concentration (MCHC), mean-corpuscular-hemoglobin (MCH) and activated-partial-prothrombin-time (aPTT). The most effective features in the prognosis of the disease were erythrocyte-sedimentation-rate (ESR), neutrophil-count (NEU), C-reactive-protein (CRP). LogNNet-model achieved an accuracy rate of A46 = 99.5% in the diagnosis of the disease with 46 features and A3 = 99.17% with only MCHC, MCH, and aPTT features. Model reached an accuracy rate of A48 = 94.4% in determining the prognosis of the disease with 48 features and A3 = 82.7% with only ESR, NEU, and CRP features. LogNNet model demonstrated a very high disease diagnosis/prognosis of COVID-19 performance without knowing about the symptoms or history of the patients. The model is suitable for devices with low resources (3-14 kB of RAM used on the Arduino microcontroller), and is promising to create mobile health monitoring systems in the Internet of Things. Our method will reduce the negative pressures on the health sector and help doctors understand pathogenesis of COVID-19 through key futures and contribute positively to the treatment processes.

</p>
</details>

<details><summary><b>Action Recognition for American Sign Language</b>
<a href="https://arxiv.org/abs/2205.12261">arxiv:2205.12261</a>
&#x1F4C8; 2 <br>
<p>Nguyen Huu Phong, Bernardete Ribeiro</p></summary>
<p>

**Abstract:** In this research, we present our findings to recognize American Sign Language from series of hand gestures. While most researches in literature focus only on static handshapes, our work target dynamic hand gestures. Since dynamic signs dataset are very few, we collect an initial dataset of 150 videos for 10 signs and an extension of 225 videos for 15 signs. We apply transfer learning models in combination with deep neural networks and background subtraction for videos in different temporal settings. Our primarily results show that we can get an accuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12 frames accordingly.

</p>
</details>

<details><summary><b>Adaptive Fairness-Aware Online Meta-Learning for Changing Environments</b>
<a href="https://arxiv.org/abs/2205.11264">arxiv:2205.11264</a>
&#x1F4C8; 2 <br>
<p>Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen</p></summary>
<p>

**Abstract:** The fairness-aware online learning framework has arisen as a powerful tool for the continual lifelong learning setting. The goal for the learner is to sequentially learn new tasks where they come one after another over time and the learner ensures the statistic parity of the new coming task across different protected sub-populations (e.g. race and gender). A major drawback of existing methods is that they make heavy use of the i.i.d assumption for data and hence provide static regret analysis for the framework. However, low static regret cannot imply a good performance in changing environments where tasks are sampled from heterogeneous distributions. To address the fairness-aware online learning problem in changing environments, in this paper, we first construct a novel regret metric FairSAR by adding long-term fairness constraints onto a strongly adapted loss regret. Furthermore, to determine a good model parameter at each round, we propose a novel adaptive fairness-aware online meta-learning algorithm, namely FairSAOML, which is able to adapt to changing environments in both bias control and model precision. The problem is formulated in the form of a bi-level convex-concave optimization with respect to the model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The theoretic analysis provides sub-linear upper bounds for both loss regret and violation of cumulative fairness constraints. Our experimental evaluation on different real-world datasets with settings of changing environments suggests that the proposed FairSAOML significantly outperforms alternatives based on the best prior online learning approaches.

</p>
</details>

<details><summary><b>Neuro-Symbolic Regex Synthesis Framework via Neural Example Splitting</b>
<a href="https://arxiv.org/abs/2205.11258">arxiv:2205.11258</a>
&#x1F4C8; 2 <br>
<p>Su-Hyeon Kim, Hyunjoon Cheon, Yo-Sub Han, Sang-Ki Ko</p></summary>
<p>

**Abstract:** Due to the practical importance of regular expressions (regexes, for short), there has been a lot of research to automatically generate regexes from positive and negative string examples. We tackle the problem of learning regexes faster from positive and negative strings by relying on a novel approach called `neural example splitting'. Our approach essentially split up each example string into multiple parts using a neural network trained to group similar substrings from positive strings. This helps to learn a regex faster and, thus, more accurately since we now learn from several short-length strings. We propose an effective regex synthesis framework called `SplitRegex' that synthesizes subregexes from `split' positive substrings and produces the final regex by concatenating the synthesized subregexes. For the negative sample, we exploit pre-generated subregexes during the subregex synthesis process and perform the matching against negative strings. Then the final regex becomes consistent with all negative strings. SplitRegex is a divided-and-conquer framework for learning target regexes; split (=divide) positive strings and infer partial regexes for multiple parts, which is much more accurate than the whole string inferring, and concatenate (=conquer) inferred regexes while satisfying negative strings. We empirically demonstrate that the proposed SplitRegex framework substantially improves the previous regex synthesis approaches over four benchmark datasets.

</p>
</details>

<details><summary><b>Making Video Quality Assessment Models Sensitive to Frame Rate Distortions</b>
<a href="https://arxiv.org/abs/2205.10501">arxiv:2205.10501</a>
&#x1F4C8; 2 <br>
<p>Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik</p></summary>
<p>

**Abstract:** We consider the problem of capturing distortions arising from changes in frame rate as part of Video Quality Assessment (VQA). Variable frame rate (VFR) videos have become much more common, and streamed videos commonly range from 30 frames per second (fps) up to 120 fps. VFR-VQA offers unique challenges in terms of distortion types as well as in making non-uniform comparisons of reference and distorted videos having different frame rates. The majority of current VQA models require compared videos to be of the same frame rate, but are unable to adequately account for frame rate artifacts. The recently proposed Generalized Entropic Difference (GREED) VQA model succeeds at this task, using natural video statistics models of entropic differences of temporal band-pass coefficients, delivering superior performance on predicting video quality changes arising from frame rate distortions. Here we propose a simple fusion framework, whereby temporal features from GREED are combined with existing VQA models, towards improving model sensitivity towards frame rate distortions. We find through extensive experiments that this feature fusion significantly boosts model performance on both HFR/VFR datasets as well as fixed frame rate (FFR) VQA databases. Our results suggest that employing efficient temporal representations can result much more robust and accurate VQA models when frame rate variations can occur.

</p>
</details>

<details><summary><b>Diversity vs. Recognizability: Human-like generalization in one-shot generative models</b>
<a href="https://arxiv.org/abs/2205.10370">arxiv:2205.10370</a>
&#x1F4C8; 2 <br>
<p>Victor Boutin, Lakshya Singhal, Xavier Thomas, Thomas Serre</p></summary>
<p>

**Abstract:** Robust generalization to new concepts has long remained a distinctive feature of human intelligence. However, recent progress in deep generative models has now led to neural architectures capable of synthesizing novel instances of unknown visual concepts from a single training example. Yet, a more precise comparison between these models and humans is not possible because existing performance metrics for generative models (i.e., FID, IS, likelihood) are not appropriate for the one-shot generation scenario. Here, we propose a new framework to evaluate one-shot generative models along two axes: sample recognizability vs. diversity (i.e., intra-class variability). Using this framework, we perform a systematic evaluation of representative one-shot generative models on the Omniglot handwritten dataset. We first show that GAN-like and VAE-like models fall on opposite ends of the diversity-recognizability space. Extensive analyses of the effect of key model parameters further revealed that spatial attention and context integration have a linear contribution to the diversity-recognizability trade-off. In contrast, disentanglement transports the model along a parabolic curve that could be used to maximize recognizability. Using the diversity-recognizability framework, we were able to identify models and parameters that closely approximate human data.

</p>
</details>

<details><summary><b>Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)</b>
<a href="https://arxiv.org/abs/2205.10342">arxiv:2205.10342</a>
&#x1F4C8; 2 <br>
<p>Jue Jiang, Neelam Tyagi, Kathryn Tringale, Christopher Crane, Harini Veeraraghavan</p></summary>
<p>

**Abstract:** Vision transformers, with their ability to more efficiently model long-range context, have demonstrated impressive accuracy gains in several computer vision and medical image analysis tasks including segmentation. However, such methods need large labeled datasets for training, which is hard to obtain for medical image analysis. Self-supervised learning (SSL) has demonstrated success in medical image segmentation using convolutional networks. In this work, we developed a \underline{s}elf-distillation learning with \underline{m}asked \underline{i}mage modeling method to perform SSL for vision \underline{t}ransformers (SMIT) applied to 3D multi-organ segmentation from CT and MRI. Our contribution is a dense pixel-wise regression within masked patches called masked image prediction, which we combined with masked patch token distillation as pretext task to pre-train vision transformers. We show our approach is more accurate and requires fewer fine tuning datasets than other pretext tasks. Unlike prior medical image methods, which typically used image sets arising from disease sites and imaging modalities corresponding to the target tasks, we used 3,643 CT scans (602,708 images) arising from head and neck, lung, and kidney cancers as well as COVID-19 for pre-training and applied it to abdominal organs segmentation from MRI pancreatic cancer patients as well as publicly available 13 different abdominal organs segmentation from CT. Our method showed clear accuracy improvement (average DSC of 0.875 from MRI and 0.878 from CT) with reduced requirement for fine-tuning datasets over commonly used pretext tasks. Extensive comparisons against multiple current SSL methods were done. Code will be made available upon acceptance for publication.

</p>
</details>

<details><summary><b>A Review of Safe Reinforcement Learning: Methods, Theory and Applications</b>
<a href="https://arxiv.org/abs/2205.10330">arxiv:2205.10330</a>
&#x1F4C8; 2 <br>
<p>Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, Alois Knoll</p></summary>
<p>

**Abstract:** Reinforcement learning has achieved tremendous success in many complex decision making tasks. When it comes to deploying RL in the real world, safety concerns are usually raised, leading to a growing demand for safe reinforcement learning algorithms, such as in autonomous driving and robotics scenarios. While safety control has a long history, the study of safe RL algorithms is still in the early stages. To establish a good foundation for future research in this thread, in this paper, we provide a review for safe RL from the perspectives of methods, theory and applications. Firstly, we review the progress of safe RL from five dimensions and come up with five problems that are crucial for safe RL being deployed in real-world applications, coined as "2H3W". Secondly, we analyze the theory and algorithm progress from the perspectives of answering the "2H3W" problems. Then, the sample complexity of safe RL methods is reviewed and discussed, followed by an introduction of the applications and benchmarks of safe RL algorithms. Finally, we open the discussion of the challenging problems in safe RL, hoping to inspire more future research on this thread.
  To advance the study of safe RL algorithms, we release a benchmark suite, an open-sourced repository containing the implementations of major safe RL algorithms, along with tutorials at the link: https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git.

</p>
</details>

<details><summary><b>Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction</b>
<a href="https://arxiv.org/abs/2205.10249">arxiv:2205.10249</a>
&#x1F4C8; 2 <br>
<p>Yue Cao, XiaoJiang Zhou, Jiaqi Feng, Peihao Huang, Yao Xiao, Dayao Chen, Sheng Chen</p></summary>
<p>

**Abstract:** Rich user behavior data has been proven to be of great value for Click-Through Rate (CTR) prediction applications, especially in industrial recommender, search, or advertising systems. However, it's non-trivial for real-world systems to make full use of long-term user behaviors due to the strict requirements of online serving time. Most previous works adopt the retrieval-based strategy, where a small number of user behaviors are retrieved first for subsequent attention. However, the retrieval-based methods are sub-optimal and would cause more or less information losses, and it's difficult to balance the effectiveness and efficiency of the retrieval algorithm.
  In this paper, we propose \textbf{SDIM} (\textbf{S}ampling-based \textbf{D}eep \textbf{I}nterest \textbf{M}odeling), a simple yet effective sampling-based end-to-end approach for modeling long-term user behaviors. We sample from multiple hash functions to generate hash signatures of the candidate item and each item in the user behavior sequence, and obtain the user interest by directly gathering behavior items associated with the candidate item with the same hash signature. We show theoretically and experimentally that the proposed method performs on par with standard attention-based models on modeling long-term user behaviors, while being sizable times faster. We also introduce the deployment of SDIM in our system. Specifically, we decouple the behavior sequence hashing, which is the most time-consuming part, from the CTR model by designing a separate module named BSE (behavior Sequence Encoding). BSE is latency-free for the CTR server, enabling us to model extremely long user behaviors. Both offline and online experiments are conducted to demonstrate the effectiveness of SDIM. SDIM now has been deployed online in the search system of Meituan APP.

</p>
</details>

<details><summary><b>Exploring the Trade-off between Plausibility, Change Intensity and Adversarial Power in Counterfactual Explanations using Multi-objective Optimization</b>
<a href="https://arxiv.org/abs/2205.10232">arxiv:2205.10232</a>
&#x1F4C8; 2 <br>
<p>Javier Del Ser, Alejandro Barredo-Arrieta, Natalia Díaz-Rodríguez, Francisco Herrera, Andreas Holzinger</p></summary>
<p>

**Abstract:** There is a broad consensus on the importance of deep learning models in tasks involving complex data. Often, an adequate understanding of these models is required when focusing on the transparency of decisions in human-critical applications. Besides other explainability techniques, trustworthiness can be achieved by using counterfactuals, like the way a human becomes familiar with an unknown process: by understanding the hypothetical circumstances under which the output changes. In this work we argue that automated counterfactual generation should regard several aspects of the produced adversarial instances, not only their adversarial capability. To this end, we present a novel framework for the generation of counterfactual examples which formulates its goal as a multi-objective optimization problem balancing three different objectives: 1) plausibility, i.e., the likeliness of the counterfactual of being possible as per the distribution of the input data; 2) intensity of the changes to the original input; and 3) adversarial power, namely, the variability of the model's output induced by the counterfactual. The framework departs from a target model to be audited and uses a Generative Adversarial Network to model the distribution of input data, together with a multi-objective solver for the discovery of counterfactuals balancing among these objectives. The utility of the framework is showcased over six classification tasks comprising image and three-dimensional data. The experiments verify that the framework unveils counterfactuals that comply with intuition, increasing the trustworthiness of the user, and leading to further insights, such as the detection of bias and data misrepresentation.

</p>
</details>

<details><summary><b>Adversarial Body Shape Search for Legged Robots</b>
<a href="https://arxiv.org/abs/2205.10187">arxiv:2205.10187</a>
&#x1F4C8; 2 <br>
<p>Takaaki Azakami, Hiroshi Kera, Kazuhiko Kawamoto</p></summary>
<p>

**Abstract:** We propose an evolutionary computation method for an adversarial attack on the length and thickness of parts of legged robots by deep reinforcement learning. This attack changes the robot body shape and interferes with walking-we call the attacked body as adversarial body shape. The evolutionary computation method searches adversarial body shape by minimizing the expected cumulative reward earned through walking simulation. To evaluate the effectiveness of the proposed method, we perform experiments with three-legged robots, Walker2d, Ant-v2, and Humanoid-v2 in OpenAI Gym. The experimental results reveal that Walker2d and Ant-v2 are more vulnerable to the attack on the length than the thickness of the body parts, whereas Humanoid-v2 is vulnerable to the attack on both of the length and thickness. We further identify that the adversarial body shapes break left-right symmetry or shift the center of gravity of the legged robots. Finding adversarial body shape can be used to proactively diagnose the vulnerability of legged robot walking.

</p>
</details>

<details><summary><b>Adversarial joint attacks on legged robots</b>
<a href="https://arxiv.org/abs/2205.10098">arxiv:2205.10098</a>
&#x1F4C8; 2 <br>
<p>Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto</p></summary>
<p>

**Abstract:** We address adversarial attacks on the actuators at the joints of legged robots trained by deep reinforcement learning. The vulnerability to the joint attacks can significantly impact the safety and robustness of legged robots. In this study, we demonstrate that the adversarial perturbations to the torque control signals of the actuators can significantly reduce the rewards and cause walking instability in robots. To find the adversarial torque perturbations, we develop black-box adversarial attacks, where, the adversary cannot access the neural networks trained by deep reinforcement learning. The black box attack can be applied to legged robots regardless of the architecture and algorithms of deep reinforcement learning. We employ three search methods for the black-box adversarial attacks: random search, differential evolution, and numerical gradient descent methods. In experiments with the quadruped robot Ant-v2 and the bipedal robot Humanoid-v2, in OpenAI Gym environments, we find that differential evolution can efficiently find the strongest torque perturbations among the three methods. In addition, we realize that the quadruped robot Ant-v2 is vulnerable to the adversarial perturbations, whereas the bipedal robot Humanoid-v2 is robust to the perturbations. Consequently, the joint attacks can be used for proactive diagnosis of robot walking instability.

</p>
</details>

<details><summary><b>A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models</b>
<a href="https://arxiv.org/abs/2205.10083">arxiv:2205.10083</a>
&#x1F4C8; 2 <br>
<p>Ehsan Mokhtarian, Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash</p></summary>
<p>

**Abstract:** We study experiment design for the unique identification of the causal graph of a system where the graph may contain cycles. The presence of cycles in the structure introduces major challenges for experiment design. Unlike the case of acyclic graphs, learning the skeleton of the causal graph from observational distribution may not be possible. Furthermore, intervening on a variable does not necessarily lead to orienting all the edges incident to it. In this paper, we propose an experiment design approach that can learn both cyclic and acyclic graphs and hence, unifies the task of experiment design for both types of graphs. We provide a lower bound on the number of experiments required to guarantee the unique identification of the causal graph in the worst case, showing that the proposed approach is order-optimal in terms of the number of experiments up to an additive logarithmic term. Moreover, we extend our result to the setting where the size of each experiment is bounded by a constant. For this case, we show that our approach is optimal in terms of the size of the largest experiment required for the unique identification of the causal graph in the worst case.

</p>
</details>

<details><summary><b>ExMo: Explainable AI Model using Inverse Frequency Decision Rules</b>
<a href="https://arxiv.org/abs/2205.10045">arxiv:2205.10045</a>
&#x1F4C8; 2 <br>
<p>Pradip Mainali, Ismini Psychoula, Fabien A. P. Petitcolas</p></summary>
<p>

**Abstract:** In this paper, we present a novel method to compute decision rules to build a more accurate interpretable machine learning model, denoted as ExMo. The ExMo interpretable machine learning model consists of a list of IF...THEN... statements with a decision rule in the condition. This way, ExMo naturally provides an explanation for a prediction using the decision rule that was triggered. ExMo uses a new approach to extract decision rules from the training data using term frequency-inverse document frequency (TF-IDF) features. With TF-IDF, decision rules with feature values that are more relevant to each class are extracted. Hence, the decision rules obtained by ExMo can distinguish the positive and negative classes better than the decision rules used in the existing Bayesian Rule List (BRL) algorithm, obtained using the frequent pattern mining approach. The paper also shows that ExMo learns a qualitatively better model than BRL. Furthermore, ExMo demonstrates that the textual explanation can be provided in a human-friendly way so that the explanation can be easily understood by non-expert users. We validate ExMo on several datasets with different sizes to evaluate its efficacy. Experimental validation on a real-world fraud detection application shows that ExMo is 20% more accurate than BRL and that it achieves accuracy similar to those of deep learning models.

</p>
</details>

<details><summary><b>Predicting electrode array impedance after one month from cochlear implantation surgery</b>
<a href="https://arxiv.org/abs/2205.10021">arxiv:2205.10021</a>
&#x1F4C8; 2 <br>
<p>Yousef A. Alohali, Yassin Abdelsamad, Tamer Mesallam, Fida Almuhawas, Abdulrahman Hagr, Mahmoud S. Fayed</p></summary>
<p>

**Abstract:** Sensorineural hearing loss can be treated using Cochlear implantation. After this surgery using the electrode array impedance measurements, we can check the stability of the impedance value and the dynamic range. Deterioration of speech recognition scores could happen because of increased impedance values. Medicines used to do these measures many times during a year after the surgery. Predicting the electrode impedance could help in taking decisions to help the patient get better hearing. In this research we used a dataset of 80 patients of children who did cochlear implantation using MED-EL FLEX28 electrode array of 12 channels. We predicted the electrode impedance on each channel after 1 month from the surgery date. We used different machine learning algorithms like neural networks and decision trees. Our results indicates that the electrode impedance can be predicted, and the best algorithm is different based on the electrode channel. Also, the accuracy level varies between 66% and 100% based on the electrode channel when accepting an error range between 0 and 3 KO. Further research is required to predict the electrode impedance after three months, six months and one year.

</p>
</details>

<details><summary><b>Neural Additive Models for Nowcasting</b>
<a href="https://arxiv.org/abs/2205.10020">arxiv:2205.10020</a>
&#x1F4C8; 2 <br>
<p>Wonkeun Jo, Dongil Kim</p></summary>
<p>

**Abstract:** Deep neural networks (DNNs) are one of the most highlighted methods in machine learning. However, as DNNs are black-box models, they lack explanatory power for their predictions. Recently, neural additive models (NAMs) have been proposed to provide this power while maintaining high prediction performance. In this paper, we propose a novel NAM approach for multivariate nowcasting (NC) problems, which comprise an important focus area of machine learning. For the multivariate time-series data used in NC problems, explanations should be considered for every input value to the variables at distinguishable time steps. By employing generalized additive models, the proposed NAM-NC successfully explains each input value's importance for multiple variables and time steps. Experimental results involving a toy example and two real-world datasets show that the NAM-NC predicts multivariate time-series data as accurately as state-of-the-art neural networks, while also providing the explanatory importance of each input value. We also examine parameter-sharing networks using NAM-NC to decrease their complexity, and NAM-MC's hard-tied feature net extracted explanations with good performance.

</p>
</details>

<details><summary><b>A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection</b>
<a href="https://arxiv.org/abs/2205.10014">arxiv:2205.10014</a>
&#x1F4C8; 2 <br>
<p>Bingzhe Wu, Jintang Li, Junchi Yu, Yatao Bian, Hengtong Zhang, CHaochao Chen, Chengbin Hou, Guoji Fu, Liang Chen, Tingyang Xu, Yu Rong, Xiaolin Zheng, Junzhou Huang, Ran He, Baoyuan Wu, GUangyu Sun, Peng Cui, Zibin Zheng, Zhe Liu, Peilin Zhao</p></summary>
<p>

**Abstract:** Deep graph learning has achieved remarkable progresses in both business and scientific areas ranging from finance and e-commerce, to drug and advanced material discovery. Despite these progresses, how to ensure various deep graph learning algorithms behave in a socially responsible manner and meet regulatory compliance requirements becomes an emerging problem, especially in risk-sensitive domains. Trustworthy graph learning (TwGL) aims to solve the above problems from a technical viewpoint. In contrast to conventional graph learning research which mainly cares about model performance, TwGL considers various reliability and safety aspects of the graph learning framework including but not limited to robustness, explainability, and privacy. In this survey, we provide a comprehensive review of recent leading approaches in the TwGL field from three dimensions, namely, reliability, explainability, and privacy protection. We give a general categorization for existing work and review typical work for each category. To give further insights for TwGL research, we provide a unified view to inspect previous works and build the connection between them. We also point out some important open problems remaining to be solved in the future developments of TwGL.

</p>
</details>

<details><summary><b>The price of ignorance: how much does it cost to forget noise structure in low-rank matrix estimation?</b>
<a href="https://arxiv.org/abs/2205.10009">arxiv:2205.10009</a>
&#x1F4C8; 2 <br>
<p>Jean Barbier, TianQi Hou, Marco Mondelli, Manuel Sáenz</p></summary>
<p>

**Abstract:** We consider the problem of estimating a rank-1 signal corrupted by structured rotationally invariant noise, and address the following question: how well do inference algorithms perform when the noise statistics is unknown and hence Gaussian noise is assumed? While the matched Bayes-optimal setting with unstructured noise is well understood, the analysis of this mismatched problem is only at its premises. In this paper, we make a step towards understanding the effect of the strong source of mismatch which is the noise statistics. Our main technical contribution is the rigorous analysis of a Bayes estimator and of an approximate message passing (AMP) algorithm, both of which incorrectly assume a Gaussian setup. The first result exploits the theory of spherical integrals and of low-rank matrix perturbations; the idea behind the second one is to design and analyze an artificial AMP which, by taking advantage of the flexibility in the denoisers, is able to "correct" the mismatch. Armed with these sharp asymptotic characterizations, we unveil a rich and often unexpected phenomenology. For example, despite AMP is in principle designed to efficiently compute the Bayes estimator, the former is outperformed by the latter in terms of mean-square error. We show that this performance gap is due to an incorrect estimation of the signal norm. In fact, when the SNR is large enough, the overlaps of the AMP and the Bayes estimator coincide, and they even match those of optimal estimators taking into account the structure of the noise.

</p>
</details>

<details><summary><b>Set-based Meta-Interpolation for Few-Task Meta-Learning</b>
<a href="https://arxiv.org/abs/2205.09990">arxiv:2205.09990</a>
&#x1F4C8; 2 <br>
<p>Seanie Lee, Bruno Andreis, Kenji Kawaguchi, Juho Lee, Sung Ju Hwang</p></summary>
<p>

**Abstract:** Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes expressive neural set functions to densify the meta-training task distribution using bilevel optimization. We empirically validate the efficacy of Meta-Interpolation on eight datasets spanning across various domains such as image classification, molecule property prediction, text classification and speech recognition. Experimentally, we show that Meta-Interpolation consistently outperforms all the relevant baselines. Theoretically, we prove that task interpolation with the set function regularizes the meta-learner to improve generalization.

</p>
</details>

<details><summary><b>On Tackling Explanation Redundancy in Decision Trees</b>
<a href="https://arxiv.org/abs/2205.09971">arxiv:2205.09971</a>
&#x1F4C8; 2 <br>
<p>Yacine Izza, Alexey Ignatiev, Joao Marques-Silva</p></summary>
<p>

**Abstract:** Decision trees (DTs) epitomize the ideal of interpretability of machine learning (ML) models. The interpretability of decision trees motivates explainability approaches by so-called intrinsic interpretability, and it is at the core of recent proposals for applying interpretable ML models in high-risk applications. The belief in DT interpretability is justified by the fact that explanations for DT predictions are generally expected to be succinct. Indeed, in the case of DTs, explanations correspond to DT paths. Since decision trees are ideally shallow, and so paths contain far fewer features than the total number of features, explanations in DTs are expected to be succinct, and hence interpretable. This paper offers both theoretical and experimental arguments demonstrating that, as long as interpretability of decision trees equates with succinctness of explanations, then decision trees ought not be deemed interpretable. The paper introduces logically rigorous path explanations and path explanation redundancy, and proves that there exist functions for which decision trees must exhibit paths with arbitrarily large explanation redundancy. The paper also proves that only a very restricted class of functions can be represented with DTs that exhibit no explanation redundancy. In addition, the paper includes experimental results substantiating that path explanation redundancy is observed ubiquitously in decision trees, including those obtained using different tree learning algorithms, but also in a wide range of publicly available decision trees. The paper also proposes polynomial-time algorithms for eliminating path explanation redundancy, which in practice require negligible time to compute. Thus, these algorithms serve to indirectly attain irreducible, and so succinct, explanations for decision trees.

</p>
</details>

<details><summary><b>A Fully Controllable Agent in the Path Planning using Goal-Conditioned Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.09967">arxiv:2205.09967</a>
&#x1F4C8; 2 <br>
<p>GyeongTaek Lee</p></summary>
<p>

**Abstract:** The aim of path planning is to reach the goal from starting point by searching for the route of an agent. In the path planning, the routes may vary depending on the number of variables such that it is important for the agent to reach various goals. Numerous studies, however, have dealt with a single goal that is predefined by the user. In the present study, I propose a novel reinforcement learning framework for a fully controllable agent in the path planning. To do this, I propose a bi-directional memory editing to obtain various bi-directional trajectories of the agent, in which the behavior of the agent and sub-goals are trained on the goal-conditioned RL. As for moving the agent in various directions, I utilize the sub-goals dedicated network, separated from a policy network. Lastly, I present the reward shaping to shorten the number of steps for the agent to reach the goal. In the experimental result, the agent was able to reach the various goals that have never been visited by the agent in the training. We confirmed that the agent could perform difficult missions such as a round trip and the agent used the shorter route with the reward shaping.

</p>
</details>

<details><summary><b>How to Find Actionable Static Analysis Warnings</b>
<a href="https://arxiv.org/abs/2205.10504">arxiv:2205.10504</a>
&#x1F4C8; 1 <br>
<p>Rahul Yedida, Hong Jin Kang, Huy Tu, Xueqi Yang, David Lo, Tim Menzies</p></summary>
<p>

**Abstract:** Automatically generated static code warnings suffer from a large number of false alarms. Hence, developers only take action on a small percent of those warnings. To better predict which static code warnings should not be ignored, we suggest that analysts need to look deeper into their algorithms to find choices that better improve the particulars of their specific problem. Specifically, we show here that effective predictors of such warnings can be created by methods that locally adjust the decision boundary (between actionable warnings and others). These methods yield a new high water-mark for recognizing actionable static code warnings. For eight open-source Java projects (CASSANDRA, JMETER, COMMONS, LUCENE-SOLR, ANT, TOMCAT, DERBY) we achieve perfect test results on 4/8 datasets and, overall, a median AUC (area under the true negatives, true positives curve) of 92\%.

</p>
</details>

<details><summary><b>LSTM-Based Adaptive Vehicle Position Control for Dynamic Wireless Charging</b>
<a href="https://arxiv.org/abs/2205.10491">arxiv:2205.10491</a>
&#x1F4C8; 1 <br>
<p>Lokesh Chandra Das, Dipankar Dasgupta, Myounggyu Won</p></summary>
<p>

**Abstract:** Dynamic wireless charging (DWC) is an emerging technology that allows electric vehicles (EVs) to be wirelessly charged while in motion. It is gaining significant momentum as it can potentially address the range limitation issue for EVs. However, due to significant power loss caused by wireless power transfer, improving charging efficiency remains as a major challenge for DWC systems. This paper presents the first LSTM-based vehicle motion control system for DWC designed to maximize charging efficiency. The dynamics of the electromagnetic field generated by the transmitter coils of a DWC system are modeled based on a multi-layer LSTM. The LSTM model is used to make a prediction of the lateral position where the electromagnetic strength is expected to be maximal and to control the EV motion accordingly to optimize charging efficiency. Simulations were conducted to demonstrate that our LSTM-based approach achieves by up to 162.3% higher charging efficiency compared with state-of-the-art vehicle motion control systems focused on keeping an EV in the center of lane.

</p>
</details>

<details><summary><b>Low-cost Relevance Generation and Evaluation Metrics for Entity Resolution in AI</b>
<a href="https://arxiv.org/abs/2205.10298">arxiv:2205.10298</a>
&#x1F4C8; 1 <br>
<p>Venkat Varada, Mina Ghashami, Jitesh Mehta, Haotian Jiang, Kurtis Voris</p></summary>
<p>

**Abstract:** Entity Resolution (ER) in voice assistants is a prime component during run time that resolves entities in users request to real world entities. ER involves two major functionalities 1. Relevance generation and 2. Ranking. In this paper we propose a low cost relevance generation framework by generating features using customer implicit and explicit feedback signals. The generated relevance datasets can serve as test sets to measure ER performance. We also introduce a set of metrics that accurately measures the performance of ER systems in various dimensions. They provide great interpretability to deep dive and identifying root cause of ER issues, whether the problem is in relevance generation or ranking.

</p>
</details>

<details><summary><b>Self-supervised deep learning MRI reconstruction with Noisier2Noise</b>
<a href="https://arxiv.org/abs/2205.10278">arxiv:2205.10278</a>
&#x1F4C8; 1 <br>
<p>Charles Millard, Mark Chiew</p></summary>
<p>

**Abstract:** In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. Further, we use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. We also use Noisier2Noise to propose a modification of SSDU that we find substantially improves its reconstruction quality and robustness, offering a test set mean-squared-error within 1% of fully supervised training on the fastMRI brain dataset.

</p>
</details>

<details><summary><b>Towards Extremely Fast Bilevel Optimization with Self-governed Convergence Guarantees</b>
<a href="https://arxiv.org/abs/2205.10054">arxiv:2205.10054</a>
&#x1F4C8; 1 <br>
<p>Risheng Liu, Xuan Liu, Wei Yao, Shangzhi Zeng, Jin Zhang</p></summary>
<p>

**Abstract:** Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning and vision fields. The validity of existing works heavily relies on solving a series of approximation subproblems with extraordinarily high accuracy. Unfortunately, to achieve the approximation accuracy requires executing a large quantity of time-consuming iterations and computational burden is naturally caused. This paper is thus devoted to address this critical computational issue. In particular, we propose a single-level formulation to uniformly understand existing explicit and implicit Gradient-based BLOs (GBLOs). This together with our designed counter-example can clearly illustrate the fundamental numerical and theoretical issues of GBLOs and their naive accelerations. By introducing the dual multipliers as a new variable, we then establish Bilevel Alternating Gradient with Dual Correction (BAGDC), a general framework, which significantly accelerates different categories of existing methods by taking specific settings. A striking feature of our convergence result is that, compared to those original unaccelerated GBLO versions, the fast BAGDC admits a unified non-asymptotic convergence theory towards stationarity. A variety of numerical experiments have also been conducted to demonstrate the superiority of the proposed algorithmic framework.

</p>
</details>


{% endraw %}
Prev: [2022.05.19]({{ '/2022/05/19/2022.05.19.html' | relative_url }})  Next: [2022.05.21]({{ '/2022/05/21/2022.05.21.html' | relative_url }})