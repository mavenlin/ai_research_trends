Prev: [2022.07.08]({{ '/2022/07/08/2022.07.08.html' | relative_url }})  Next: [2022.07.10]({{ '/2022/07/10/2022.07.10.html' | relative_url }})
{% raw %}
## Summary for 2022-07-09, created on 2022-07-19


<details><summary><b>An Introduction to Lifelong Supervised Learning</b>
<a href="https://arxiv.org/abs/2207.04354">arxiv:2207.04354</a>
&#x1F4C8; 94 <br>
<p>Shagun Sodhani, Mojtaba Faramarzi, Sanket Vaibhav Mehta, Pranshu Malviya, Mohamed Abdelsalam, Janarthanan Janarthanan, Sarath Chandar</p></summary>
<p>

**Abstract:** This primer is an attempt to provide a detailed summary of the different facets of lifelong learning. We start with Chapter 2 which provides a high-level overview of lifelong learning systems. In this chapter, we discuss prominent scenarios in lifelong learning (Section 2.4), provide 8 Introduction a high-level organization of different lifelong learning approaches (Section 2.5), enumerate the desiderata for an ideal lifelong learning system (Section 2.6), discuss how lifelong learning is related to other learning paradigms (Section 2.7), describe common metrics used to evaluate lifelong learning systems (Section 2.8). This chapter is more useful for readers who are new to lifelong learning and want to get introduced to the field without focusing on specific approaches or benchmarks. The remaining chapters focus on specific aspects (either learning algorithms or benchmarks) and are more useful for readers who are looking for specific approaches or benchmarks. Chapter 3 focuses on regularization-based approaches that do not assume access to any data from previous tasks. Chapter 4 discusses memory-based approaches that typically use a replay buffer or an episodic memory to save subset of data across different tasks. Chapter 5 focuses on different architecture families (and their instantiations) that have been proposed for training lifelong learning systems. Following these different classes of learning algorithms, we discuss the commonly used evaluation benchmarks and metrics for lifelong learning (Chapter 6) and wrap up with a discussion of future challenges and important research directions in Chapter 7.

</p>
</details>

<details><summary><b>Explaining Chest X-ray Pathologies in Natural Language</b>
<a href="https://arxiv.org/abs/2207.04343">arxiv:2207.04343</a>
&#x1F4C8; 66 <br>
<p>Maxime Kayser, Cornelius Emde, Oana-Maria Camburu, Guy Parsons, Bartlomiej Papiez, Thomas Lukasiewicz</p></summary>
<p>

**Abstract:** Most deep learning algorithms lack explanations for their predictions, which limits their deployment in clinical practice. Approaches to improve explainability, especially in medical imaging, have often been shown to convey limited information, be overly reassuring, or lack robustness. In this work, we introduce the task of generating natural language explanations (NLEs) to justify predictions made on medical images. NLEs are human-friendly and comprehensive, and enable the training of intrinsically explainable models. To this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of various thoracic pathologies and chest X-ray findings. We propose a general approach to solve the task and evaluate several architectures on this dataset, including via clinician assessment.

</p>
</details>

<details><summary><b>Improving Diffusion Model Efficiency Through Patching</b>
<a href="https://arxiv.org/abs/2207.04316">arxiv:2207.04316</a>
&#x1F4C8; 40 <br>
<p>Troy Luhman, Eric Luhman</p></summary>
<p>

**Abstract:** Diffusion models are a powerful class of generative models that iteratively denoise samples to produce data. While many works have focused on the number of iterations in this sampling procedure, few have focused on the cost of each iteration. We find that adding a simple ViT-style patching transformation can considerably reduce a diffusion model's sampling time and memory usage. We justify our approach both through an analysis of the diffusion model objective, and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024. We provide implementations in Tensorflow and Pytorch.

</p>
</details>

<details><summary><b>State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections</b>
<a href="https://arxiv.org/abs/2207.04361">arxiv:2207.04361</a>
&#x1F4C8; 39 <br>
<p>Shivesh Khaitan, John M. Dolan</p></summary>
<p>

**Abstract:** Traversing intersections is a challenging problem for autonomous vehicles, especially when the intersections do not have traffic control. Recently deep reinforcement learning has received massive attention due to its success in dealing with autonomous driving tasks. In this work, we address the problem of traversing unsignalized intersections using a novel curriculum for deep reinforcement learning. The proposed curriculum leads to: 1) A faster training process for the reinforcement learning agent, and 2) Better performance compared to an agent trained without curriculum. Our main contribution is two-fold: 1) Presenting a unique curriculum for training deep reinforcement learning agents, and 2) showing the application of the proposed curriculum for the unsignalized intersection traversal task. The framework expects processed observations of the surroundings from the perception system of the autonomous vehicle. We test our method in the CommonRoad motion planning simulator on T-intersections and four-way intersections.

</p>
</details>

<details><summary><b>A Comparative Study of Self-supervised Speech Representation Based Voice Conversion</b>
<a href="https://arxiv.org/abs/2207.04356">arxiv:2207.04356</a>
&#x1F4C8; 7 <br>
<p>Wen-Chin Huang, Shu-Wen Yang, Tomoki Hayashi, Tomoki Toda</p></summary>
<p>

**Abstract:** We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.

</p>
</details>

<details><summary><b>Few-shot training LLMs for project-specific code-summarization</b>
<a href="https://arxiv.org/abs/2207.04237">arxiv:2207.04237</a>
&#x1F4C8; 6 <br>
<p>Toufique Ahmed, Premkumar Devanbu</p></summary>
<p>

**Abstract:** Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.

</p>
</details>

<details><summary><b>Adaptive Graph Spatial-Temporal Transformer Network for Traffic Flow Forecasting</b>
<a href="https://arxiv.org/abs/2207.05064">arxiv:2207.05064</a>
&#x1F4C8; 5 <br>
<p>Aosong Feng, Leandros Tassiulas</p></summary>
<p>

**Abstract:** Traffic flow forecasting on graphs has real-world applications in many fields, such as transportation system and computer networks. Traffic forecasting can be highly challenging due to complex spatial-temporal correlations and non-linear traffic patterns. Existing works mostly model such spatial-temporal dependencies by considering spatial correlations and temporal correlations separately and fail to model the direct spatial-temporal correlations. Inspired by the recent success of transformers in the graph domain, in this paper, we propose to directly model the cross-spatial-temporal correlations on the spatial-temporal graph using local multi-head self-attentions. To reduce the time complexity, we set the attention receptive field to the spatially neighboring nodes, and we also introduce an adaptive graph to capture the hidden spatial-temporal dependencies. Based on these attention mechanisms, we propose a novel Adaptive Graph Spatial-Temporal Transformer Network (ASTTN), which stacks multiple spatial-temporal attention layers to apply self-attention on the input graph, followed by linear layers for predictions. Experimental results on public traffic network datasets, METR-LA PEMS-BAY, PeMSD4, and PeMSD7, demonstrate the superior performance of our model.

</p>
</details>

<details><summary><b>Video Coding Using Learned Latent GAN Compression</b>
<a href="https://arxiv.org/abs/2207.04324">arxiv:2207.04324</a>
&#x1F4C8; 5 <br>
<p>Mustafa Shukor, Bharath Bhushan Damodaran, Xu Yao, Pierre Hellier</p></summary>
<p>

**Abstract:** We propose in this paper a new paradigm for facial video compression. We leverage the generative capacity of GANs such as StyleGAN to represent and compress a video, including intra and inter compression. Each frame is inverted in the latent space of StyleGAN, from which the optimal compression is learned. To do so, a diffeomorphic latent representation is learned using a normalizing flows model, where an entropy model can be optimized for image coding. In addition, we propose a new perceptual loss that is more efficient than other counterparts. Finally, an entropy model for video inter coding with residual is also learned in the previously constructed latent representation. Our method (SGANC) is simple, faster to train, and achieves better results for image and video coding compared to state-of-the-art codecs such as VTM, AV1, and recent deep learning techniques. In particular, it drastically minimizes perceptual distortion at low bit rates.

</p>
</details>

<details><summary><b>Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and Shape with Unstructured Flash Photography</b>
<a href="https://arxiv.org/abs/2207.04236">arxiv:2207.04236</a>
&#x1F4C8; 4 <br>
<p>Inseung Hwang, Daniel S. Jeon, Adolfo Muñoz, Diego Gutierrez, Xin Tong, Min H. Kim</p></summary>
<p>

**Abstract:** Ellipsometry techniques allow to measure polarization information of materials, requiring precise rotations of optical components with different configurations of lights and sensors. This results in cumbersome capture devices, carefully calibrated in lab conditions, and in very long acquisition times, usually in the order of a few days per object. Recent techniques allow to capture polarimetric spatially-varying reflectance information, but limited to a single view, or to cover all view directions, but limited to spherical objects made of a single homogeneous material. We present sparse ellipsometry, a portable polarimetric acquisition method that captures both polarimetric SVBRDF and 3D shape simultaneously. Our handheld device consists of off-the-shelf, fixed optical components. Instead of days, the total acquisition time varies between twenty and thirty minutes per object. We develop a complete polarimetric SVBRDF model that includes diffuse and specular components, as well as single scattering, and devise a novel polarimetric inverse rendering algorithm with data augmentation of specular reflection samples via generative modeling. Our results show a strong agreement with a recent ground-truth dataset of captured polarimetric BRDFs of real-world objects.

</p>
</details>

<details><summary><b>COVID-19 Disease Identification on Chest-CT images using CNN and VGG16</b>
<a href="https://arxiv.org/abs/2207.04212">arxiv:2207.04212</a>
&#x1F4C8; 4 <br>
<p>Briskline Kiruba S, Petchiammal A, D. Murugan</p></summary>
<p>

**Abstract:** A newly identified coronavirus disease called COVID-19 mainly affects the human respiratory system. COVID-19 is an infectious disease caused by a virus originating in Wuhan, China, in December 2019. Early diagnosis is the primary challenge of health care providers. In the earlier stage, medical organizations were dazzled because there were no proper health aids or medicine to detect a COVID-19. A new diagnostic tool RT-PCR (Reverse Transcription Polymerase Chain Reaction), was introduced. It collects swab specimens from the patient's nose or throat, where the COVID-19 virus gathers. This method has some limitations related to accuracy and testing time. Medical experts suggest an alternative approach called CT (Computed Tomography) that can quickly diagnose the infected lung areas and identify the COVID-19 in an earlier stage. Using chest CT images, computer researchers developed several deep learning models identifying the COVID-19 disease. This study presents a Convolutional Neural Network (CNN) and VGG16-based model for automated COVID-19 identification on chest CT images. The experimental results using a public dataset of 14320 CT images showed a classification accuracy of 96.34% and 96.99% for CNN and VGG16, respectively.

</p>
</details>

<details><summary><b>Deep Transformer Model with Pre-Layer Normalization for COVID-19 Growth Prediction</b>
<a href="https://arxiv.org/abs/2207.06356">arxiv:2207.06356</a>
&#x1F4C8; 3 <br>
<p>Rizki Ramadhan Fitra, Novanto Yudistira, Wayan Firdaus Mahmudy</p></summary>
<p>

**Abstract:** Coronavirus disease or COVID-19 is an infectious disease caused by the SARS-CoV-2 virus. The first confirmed case caused by this virus was found at the end of December 2019 in Wuhan City, China. This case then spread throughout the world, including Indonesia. Therefore, the COVID-19 case was designated as a global pandemic by WHO. The growth of COVID-19 cases, especially in Indonesia, can be predicted using several approaches, such as the Deep Neural Network (DNN). One of the DNN models that can be used is Deep Transformer which can predict time series. The model is trained with several test scenarios to get the best model. The evaluation is finding the best hyperparameters. Then, further evaluation was carried out using the best hyperparameters setting of the number of prediction days, the optimizer, the number of features, and comparison with the former models of the Long Short-Term Memory (LSTM) and Recurrent Neural Network (RNN). All evaluations used metric of the Mean Absolute Percentage Error (MAPE). Based on the results of the evaluations, Deep Transformer produces the best results when using the Pre-Layer Normalization and predicting one day ahead with a MAPE value of 18.83. Furthermore, the model trained with the Adamax optimizer obtains the best performance among other tested optimizers. The performance of the Deep Transformer also exceeds other test models, which are LSTM and RNN.

</p>
</details>

<details><summary><b>Faster Privacy Accounting via Evolving Discretization</b>
<a href="https://arxiv.org/abs/2207.04381">arxiv:2207.04381</a>
&#x1F4C8; 3 <br>
<p>Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi</p></summary>
<p>

**Abstract:** We introduce a new algorithm for numerical composition of privacy random variables, useful for computing the accurate differential privacy parameters for composition of mechanisms. Our algorithm achieves a running time and memory usage of $\mathrm{polylog}(k)$ for the task of self-composing a mechanism, from a broad class of mechanisms, $k$ times; this class, e.g., includes the sub-sampled Gaussian mechanism, that appears in the analysis of differentially private stochastic gradient descent. By comparison, recent work by Gopi et al. (NeurIPS 2021) has obtained a running time of $\widetilde{O}(\sqrt{k})$ for the same task. Our approach extends to the case of composing $k$ different mechanisms in the same class, improving upon their running time and memory usage from $\widetilde{O}(k^{1.5})$ to $\widetilde{O}(k)$.

</p>
</details>

<details><summary><b>Unsupervised Joint Image Transfer and Uncertainty Quantification using Patch Invariant Networks</b>
<a href="https://arxiv.org/abs/2207.04325">arxiv:2207.04325</a>
&#x1F4C8; 3 <br>
<p>Christoph Angermann, Markus Haltmeier, Ahsan Raza Siyal</p></summary>
<p>

**Abstract:** Unsupervised image transfer enables intra- and inter-modality transfer for medical applications where a large amount of paired training data is not abundant. To ensure a structure-preserving mapping from the input to the target domain, existing methods for unpaired medical image transfer are commonly based on cycle-consistency, causing additional computation resources and instability due to the learning of an inverse mapping. This paper presents a novel method for uni-directional domain mapping where no paired data is needed throughout the entire training process. A reasonable transfer is ensured by employing the GAN architecture and a novel generator loss based on patch invariance. To be more precise, generator outputs are evaluated and compared on different scales, which brings increased attention to high-frequency details as well as implicit data augmentation. This novel term also gives the opportunity to predict aleatoric uncertainty by modeling an input-dependent scale map for the patch residuals. The proposed method is comprehensively evaluated on three renowned medical databases. Superior accuracy on these datasets compared to four different state-of-the-art methods for unpaired image transfer suggests the great potential of this approach for uncertainty-aware medical image translation. Implementation of the proposed framework is released here: https://github.com/anger-man/unsupervised-image-transfer-and-uq.

</p>
</details>

<details><summary><b>Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises and Challenges</b>
<a href="https://arxiv.org/abs/2207.04295">arxiv:2207.04295</a>
&#x1F4C8; 3 <br>
<p>Guang Yang, Arvind Rao, Christine Fernandez-Maloigne, Vince Calhoun, Gloria Menegaz</p></summary>
<p>

**Abstract:** Artificial intelligence has become pervasive across disciplines and fields, and biomedical image and signal processing is no exception. The growing and widespread interest on the topic has triggered a vast research activity that is reflected in an exponential research effort. Through study of massive and diverse biomedical data, machine and deep learning models have revolutionized various tasks such as modeling, segmentation, registration, classification and synthesis, outperforming traditional techniques. However, the difficulty in translating the results into biologically/clinically interpretable information is preventing their full exploitation in the field. Explainable AI (XAI) attempts to fill this translational gap by providing means to make the models interpretable and providing explanations. Different solutions have been proposed so far and are gaining increasing interest from the community. This paper aims at providing an overview on XAI in biomedical data processing and points to an upcoming Special Issue on Deep Learning in Biomedical Image and Signal Processing of the IEEE Signal Processing Magazine that is going to appear in March 2022.

</p>
</details>

<details><summary><b>A novel evaluation methodology for supervised Feature Ranking algorithms</b>
<a href="https://arxiv.org/abs/2207.04258">arxiv:2207.04258</a>
&#x1F4C8; 3 <br>
<p>Jeroen G. S. Overschie</p></summary>
<p>

**Abstract:** Both in the domains of Feature Selection and Interpretable AI, there exists a desire to `rank' features based on their importance. Such feature importance rankings can then be used to either: (1) reduce the dataset size or (2) interpret the Machine Learning model. In the literature, however, such Feature Rankers are not evaluated in a systematic, consistent way. Many papers have a different way of arguing which feature importance ranker works best. This paper fills this gap, by proposing a new evaluation methodology. By making use of synthetic datasets, feature importance scores can be known beforehand, allowing more systematic evaluation. To facilitate large-scale experimentation using the new methodology, a benchmarking framework was built in Python, called fseval. The framework allows running experiments in parallel and distributed over machines on HPC systems. By integrating with an online platform called Weights and Biases, charts can be interactively explored on a live dashboard. The software was released as open-source software, and is published as a package on the PyPi platform. The research concludes by exploring one such large-scale experiment, to find the strengths and weaknesses of the participating algorithms, on many fronts.

</p>
</details>

<details><summary><b>Batch-efficient EigenDecomposition for Small and Medium Matrices</b>
<a href="https://arxiv.org/abs/2207.04228">arxiv:2207.04228</a>
&#x1F4C8; 3 <br>
<p>Yue Song, Nicu Sebe, Wei Wang</p></summary>
<p>

**Abstract:** EigenDecomposition (ED) is at the heart of many computer vision algorithms and applications. One crucial bottleneck limiting its usage is the expensive computation cost, particularly for a mini-batch of matrices in the deep neural networks. In this paper, we propose a QR-based ED method dedicated to the application scenarios of computer vision. Our proposed method performs the ED entirely by batched matrix/vector multiplication, which processes all the matrices simultaneously and thus fully utilizes the power of GPUs. Our technique is based on the explicit QR iterations by Givens rotation with double Wilkinson shifts. With several acceleration techniques, the time complexity of QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test shows that for small and medium batched matrices (\emph{e.g.,} $dim{<}32$) our method can be much faster than the Pytorch SVD function. Experimental results on visual recognition and image generation demonstrate that our methods also achieve competitive performances.

</p>
</details>

<details><summary><b>SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification</b>
<a href="https://arxiv.org/abs/2207.04224">arxiv:2207.04224</a>
&#x1F4C8; 3 <br>
<p>Xingzhao Jia, Dongye Changlei, Yanjun Peng</p></summary>
<p>

**Abstract:** RGB-D SOD uses depth information to handle challenging scenes and obtain high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection methods overwhelmingly rely on the strategy of directly fusing depth information. Although these methods improve the accuracy of saliency prediction through various cross-modality fusion strategies, misinformation provided by some poor-quality depth images can affect the saliency prediction result. To address this issue, a novel RGB-D salient object detection model (SiaTrans) is proposed in this paper, which allows training on depth image quality classification at the same time as training on SOD. In light of the common information between RGB and depth images on salient objects, SiaTrans uses a Siamese transformer network with shared weight parameters as the encoder and extracts RGB and depth features concatenated on the batch dimension, saving space resources without compromising performance. SiaTrans uses the Class token in the backbone network (T2T-ViT) to classify the quality of depth images without preventing the token sequence from going on with the saliency detection task. Transformer-based cross-modality fusion module (CMF) can effectively fuse RGB and depth information. And in the testing process, CMF can choose to fuse cross-modality information or enhance RGB information according to the quality classification signal of the depth image. The greatest benefit of our designed CMF and decoder is that they maintain the consistency of RGB and RGB-D information decoding: SiaTrans decodes RGB-D or RGB information under the same model parameters according to the classification signal during testing. Comprehensive experiments on nine RGB-D SOD benchmark datasets show that SiaTrans has the best overall performance and the least computation compared with recent state-of-the-art methods.

</p>
</details>

<details><summary><b>Dual-path Attention is All You Need for Audio-Visual Speech Extraction</b>
<a href="https://arxiv.org/abs/2207.04213">arxiv:2207.04213</a>
&#x1F4C8; 3 <br>
<p>Zhongweiyang Xu, Xulin Fan, Mark Hasegawa-Johnson</p></summary>
<p>

**Abstract:** Audio-visual target speech extraction, which aims to extract a certain speaker's speech from the noisy mixture by looking at lip movements, has made significant progress combining time-domain speech separation models and visual feature extractors (CNN). One problem of fusing audio and video information is that they have different time resolutions. Most current research upsamples the visual features along the time dimension so that audio and video features are able to align in time. However, we believe that lip movement should mostly contain long-term, or phone-level information. Based on this assumption, we propose a new way to fuse audio-visual features. We observe that for DPRNN \cite{dprnn}, the interchunk dimension's time resolution could be very close to the time resolution of video frames. Like \cite{sepformer}, the LSTM in DPRNN is replaced by intra-chunk and inter-chunk self-attention, but in the proposed algorithm, inter-chunk attention incorporates the visual features as an additional feature stream. This prevents the upsampling of visual cues, resulting in more efficient audio-visual fusion. The result shows we achieve superior results compared with other time-domain based audio-visual fusion models.

</p>
</details>

<details><summary><b>BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid Counterfactual Training for Robust Content-based Image Retrieval</b>
<a href="https://arxiv.org/abs/2207.04211">arxiv:2207.04211</a>
&#x1F4C8; 3 <br>
<p>Wenqiao Zhang, Jiannan Guo, Mengze Li, Haochen Shi, Shengyu Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang</p></summary>
<p>

**Abstract:** Content-Based Image Retrieval (CIR) aims to search for a target image by concurrently comprehending the composition of an example image and a complementary text, which potentially impacts a wide variety of real-world applications, such as internet search and fashion retrieval. In this scenario, the input image serves as an intuitive context and background for the search, while the corresponding language expressly requests new traits on how specific characteristics of the query image should be modified in order to get the intended target image. This task is challenging since it necessitates learning and understanding the composite image-text representation by incorporating cross-granular semantic updates. In this paper, we tackle this task by a novel \underline{\textbf{B}}ottom-up cr\underline{\textbf{O}}ss-modal \underline{\textbf{S}}emantic compo\underline{\textbf{S}}ition (\textbf{BOSS}) with Hybrid Counterfactual Training framework, which sheds new light on the CIR task by studying it from two previously overlooked perspectives: \emph{implicitly bottom-up composition of visiolinguistic representation} and \emph{explicitly fine-grained correspondence of query-target construction}. On the one hand, we leverage the implicit interaction and composition of cross-modal embeddings from the bottom local characteristics to the top global semantics, preserving and transforming the visual representation conditioned on language semantics in several continuous steps for effective target image search. On the other hand, we devise a hybrid counterfactual training strategy that can reduce the model's ambiguity for similar queries.

</p>
</details>

<details><summary><b>Learning to Separate Voices by Spatial Regions</b>
<a href="https://arxiv.org/abs/2207.04203">arxiv:2207.04203</a>
&#x1F4C8; 3 <br>
<p>Zhongweiyang Xu, Romit Roy Choudhury</p></summary>
<p>

**Abstract:** We consider the problem of audio voice separation for binaural applications, such as earphones and hearing aids. While today's neural networks perform remarkably well (separating $4+$ sources with 2 microphones) they assume a known or fixed maximum number of sources, K. Moreover, today's models are trained in a supervised manner, using training data synthesized from generic sources, environments, and human head shapes.
  This paper intends to relax both these constraints at the expense of a slight alteration in the problem definition. We observe that, when a received mixture contains too many sources, it is still helpful to separate them by region, i.e., isolating signal mixtures from each conical sector around the user's head. This requires learning the fine-grained spatial properties of each region, including the signal distortions imposed by a person's head. We propose a two-stage self-supervised framework in which overheard voices from earphones are pre-processed to extract relatively clean personalized signals, which are then used to train a region-wise separation model. Results show promising performance, underscoring the importance of personalization over a generic supervised approach. (audio samples available at our project website: https://uiuc-earable-computing.github.io/binaural/. We believe this result could help real-world applications in selective hearing, noise cancellation, and audio augmented reality.

</p>
</details>

<details><summary><b>Smart Multi-tenant Federated Learning</b>
<a href="https://arxiv.org/abs/2207.04202">arxiv:2207.04202</a>
&#x1F4C8; 3 <br>
<p>Weiming Zhuang, Yonggang Wen, Shuai Zhang</p></summary>
<p>

**Abstract:** Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous training activities could overload resource-constrained devices. In this work, we propose a smart multi-tenant FL system, MuFL, to effectively coordinate and execute simultaneous training activities. We first formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and introduce a vanilla multi-tenant FL system that trains activities sequentially to form baselines. Then, we propose two approaches to optimize multi-tenant FL: 1) activity consolidation merges training activities into one activity with a multi-task architecture; 2) after training it for rounds, activity splitting divides it into groups by employing affinities among activities such that activities within a group have better synergy. Extensive experiments demonstrate that MuFL outperforms other methods while consuming 40% less energy. We hope this work will inspire the community to further study and optimize multi-tenant FL.

</p>
</details>

<details><summary><b>Connect the Dots: Tighter Discrete Approximations of Privacy Loss Distributions</b>
<a href="https://arxiv.org/abs/2207.04380">arxiv:2207.04380</a>
&#x1F4C8; 2 <br>
<p>Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi</p></summary>
<p>

**Abstract:** The privacy loss distribution (PLD) provides a tight characterization of the privacy loss of a mechanism in the context of differential privacy (DP). Recent work has shown that PLD-based accounting allows for tighter $(\varepsilon, δ)$-DP guarantees for many popular mechanisms compared to other known methods. A key question in PLD-based accounting is how to approximate any (potentially continuous) PLD with a PLD over any specified discrete support.
  We present a novel approach to this problem. Our approach supports both pessimistic estimation, which overestimates the hockey-stick divergence (i.e., $δ$) for any value of $\varepsilon$, and optimistic estimation, which underestimates the hockey-stick divergence. Moreover, we show that our pessimistic estimate is the best possible among all pessimistic estimates. Experimental evaluation shows that our approach can work with much larger discretization intervals while keeping a similar error bound compared to previous approaches and yet give a better approximation than existing methods.

</p>
</details>

<details><summary><b>On Graph Neural Network Fairness in the Presence of Heterophilous Neighborhoods</b>
<a href="https://arxiv.org/abs/2207.04376">arxiv:2207.04376</a>
&#x1F4C8; 2 <br>
<p>Donald Loveland, Jiong Zhu, Mark Heimann, Ben Fish, Michael T. Schaub, Danai Koutra</p></summary>
<p>

**Abstract:** We study the task of node classification for graph neural networks (GNNs) and establish a connection between group fairness, as measured by statistical parity and equal opportunity, and local assortativity, i.e., the tendency of linked nodes to have similar attributes. Such assortativity is often induced by homophily, the tendency for nodes of similar properties to connect. Homophily can be common in social networks where systemic factors have forced individuals into communities which share a sensitive attribute. Through synthetic graphs, we study the interplay between locally occurring homophily and fair predictions, finding that not all node neighborhoods are equal in this respect -- neighborhoods dominated by one category of a sensitive attribute often struggle to obtain fair treatment, especially in the case of diverging local class and sensitive attribute homophily. After determining that a relationship between local homophily and fairness exists, we investigate if the issue of unfairness can be associated to the design of the applied GNN model. We show that by adopting heterophilous GNN designs capable of handling disassortative group labels, group fairness in locally heterophilous neighborhoods can be improved by up to 25% over homophilous designs in real and synthetic datasets.

</p>
</details>

<details><summary><b>Segmentation of Blood Vessels, Optic Disc Localization, Detection of Exudates and Diabetic Retinopathy Diagnosis from Digital Fundus Images</b>
<a href="https://arxiv.org/abs/2207.04345">arxiv:2207.04345</a>
&#x1F4C8; 2 <br>
<p>Soham Basu, Sayantan Mukherjee, Ankit Bhattacharya, Anindya Sen</p></summary>
<p>

**Abstract:** Diabetic Retinopathy (DR) is a complication of long-standing, unchecked diabetes and one of the leading causes of blindness in the world. This paper focuses on improved and robust methods to extract some of the features of DR, viz. Blood Vessels and Exudates. Blood vessels are segmented using multiple morphological and thresholding operations. For the segmentation of exudates, k-means clustering and contour detection on the original images are used. Extensive noise reduction is performed to remove false positives from the vessel segmentation algorithm's results. The localization of Optic Disc using k-means clustering and template matching is also performed. Lastly, this paper presents a Deep Convolutional Neural Network (DCNN) model with 14 Convolutional Layers and 2 Fully Connected Layers, for the automatic, binary diagnosis of DR. The vessel segmentation, optic disc localization and DCNN achieve accuracies of 95.93%, 98.77% and 75.73% respectively. The source code and pre-trained model are available https://github.com/Sohambasu07/DR_2021

</p>
</details>

<details><summary><b>Emerging Patterns in the Continuum Representation of Protein-Lipid Fingerprints</b>
<a href="https://arxiv.org/abs/2207.04333">arxiv:2207.04333</a>
&#x1F4C8; 2 <br>
<p>Konstantia Georgouli, Helgi I Ingólfsson, Fikret Aydin, Mark Heimann, Felice C Lightstone, Peer-Timo Bremer, Harsh Bhatia</p></summary>
<p>

**Abstract:** Capturing intricate biological phenomena often requires multiscale modeling where coarse and inexpensive models are developed using limited components of expensive and high-fidelity models. Here, we consider such a multiscale framework in the context of cancer biology and address the challenge of evaluating the descriptive capabilities of a continuum model developed using 1-dimensional statistics from a molecular dynamics model. Using deep learning, we develop a highly predictive classification model that identifies complex and emergent behavior from the continuum model. With over 99.9% accuracy demonstrated for two simulations, our approach confirms the existence of protein-specific "lipid fingerprints", i.e. spatial rearrangements of lipids in response to proteins of interest. Through this demonstration, our model also provides external validation of the continuum model, affirms the value of such multiscale modeling, and can foster new insights through further analysis of these fingerprints.

</p>
</details>

<details><summary><b>Multi-Model Federated Learning with Provable Guarantees</b>
<a href="https://arxiv.org/abs/2207.04330">arxiv:2207.04330</a>
&#x1F4C8; 2 <br>
<p>Neelkamal Bhuyan, Sharayu Moharir, Gauri Joshi</p></summary>
<p>

**Abstract:** Federated Learning (FL) is a variant of distributed learning where edge devices collaborate to learn a model without sharing their data with the central server or each other. We refer to the process of training multiple independent models simultaneously in a federated setting using a common pool of clients as multi-model FL. In this work, we propose two variants of the popular FedAvg algorithm for multi-model FL, with provable convergence guarantees. We further show that for the same amount of computation, multi-model FL can have better performance than training each model separately. We supplement our theoretical results with experiments in strongly convex, convex, and non-convex settings.

</p>
</details>

<details><summary><b>QKVA grid: Attention in Image Perspective and Stacked DETR</b>
<a href="https://arxiv.org/abs/2207.04313">arxiv:2207.04313</a>
&#x1F4C8; 2 <br>
<p>Wenyuan Sheng</p></summary>
<p>

**Abstract:** We present a new model named Stacked-DETR(SDETR), which inherits the main ideas in canonical DETR. We improve DETR in two directions: simplifying the cost of training and introducing the stacked architecture to enhance the performance. To the former, we focus on the inside of the Attention block and propose the QKVA grid, a new perspective to describe the process of attention. By this, we can step further on how Attention works for image problems and the effect of multi-head. These two ideas contribute the design of single-head encoder-layer. To the latter, SDETR reaches great improvement(+1.1AP, +3.4APs) to DETR. Especially to the performance on small objects, SDETR achieves better results to the optimized Faster R-CNN baseline, which was a shortcoming in DETR. Our changes are based on the code of DETR. Training code and pretrained models are available at https://github.com/shengwenyuan/sdetr.

</p>
</details>

<details><summary><b>TensorIR: An Abstraction for Automatic Tensorized Program Optimization</b>
<a href="https://arxiv.org/abs/2207.04296">arxiv:2207.04296</a>
&#x1F4C8; 2 <br>
<p>Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, Tianqi Chen</p></summary>
<p>

**Abstract:** Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional tensor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.

</p>
</details>

<details><summary><b>Attention and Self-Attention in Random Forests</b>
<a href="https://arxiv.org/abs/2207.04293">arxiv:2207.04293</a>
&#x1F4C8; 2 <br>
<p>Lev V. Utkin, Andrei V. Konstantinov</p></summary>
<p>

**Abstract:** New models of random forests jointly using the attention and self-attention mechanisms are proposed for solving the regression problem. The models can be regarded as extensions of the attention-based random forest whose idea stems from applying a combination of the Nadaraya-Watson kernel regression and the Huber's contamination model to random forests. The self-attention aims to capture dependencies of the tree predictions and to remove noise or anomalous predictions in the random forest. The self-attention module is trained jointly with the attention module for computing weights. It is shown that the training process of attention weights is reduced to solving a single quadratic or linear optimization problem. Three modifications of the general approach are proposed and compared. A specific multi-head self-attention for the random forest is also considered. Heads of the self-attention are obtained by changing its tuning parameters including the kernel parameters and the contamination parameter of models. Numerical experiments with various datasets illustrate the proposed models and show that the supplement of the self-attention improves the model performance for many datasets.

</p>
</details>

<details><summary><b>Fuzzy Clustering by Hyperbolic Smoothing</b>
<a href="https://arxiv.org/abs/2207.04261">arxiv:2207.04261</a>
&#x1F4C8; 2 <br>
<p>David Masis, Esteban Segura, Javier Trejos, Adilson Xavier</p></summary>
<p>

**Abstract:** We propose a novel method for building fuzzy clusters of large data sets, using a smoothing numerical approach. The usual sum-of-squares criterion is relaxed so the search for good fuzzy partitions is made on a continuous space, rather than a combinatorial space as in classical methods \cite{Hartigan}. The smoothing allows a conversion from a strongly non-differentiable problem into differentiable subproblems of optimization without constraints of low dimension, by using a differentiable function of infinite class. For the implementation of the algorithm we used the statistical software $R$ and the results obtained were compared to the traditional fuzzy $C$--means method, proposed by Bezdek.

</p>
</details>

<details><summary><b>A Statistically-Based Approach to Feedforward Neural Network Model Selection</b>
<a href="https://arxiv.org/abs/2207.04248">arxiv:2207.04248</a>
&#x1F4C8; 2 <br>
<p>Andrew McInerney, Kevin Burke</p></summary>
<p>

**Abstract:** Feedforward neural networks (FNNs) can be viewed as non-linear regression models, where covariates enter the model through a combination of weighted summations and non-linear functions. Although these models have some similarities to the models typically used in statistical modelling, the majority of neural network research has been conducted outside of the field of statistics. This has resulted in a lack of statistically-based methodology, and, in particular, there has been little emphasis on model parsimony. Determining the input layer structure is analogous to variable selection, while the structure for the hidden layer relates to model complexity. In practice, neural network model selection is often carried out by comparing models using out-of-sample performance. However, in contrast, the construction of an associated likelihood function opens the door to information-criteria-based variable and architecture selection. A novel model selection method, which performs both input- and hidden-node selection, is proposed using the Bayesian information criterion (BIC) for FNNs. The choice of BIC over out-of-sample performance as the model selection objective function leads to an increased probability of recovering the true model, while parsimoniously achieving favourable out-of-sample performance. Simulation studies are used to evaluate and justify the proposed method, and applications on real data are investigated.

</p>
</details>

<details><summary><b>CEG4N: Counter-Example Guided Neural Network Quantization Refinement</b>
<a href="https://arxiv.org/abs/2207.04231">arxiv:2207.04231</a>
&#x1F4C8; 2 <br>
<p>João Batista P. Matos Jr., Iury Bessa, Edoardo Manino, Xidan Song, Lucas C. Cordeiro</p></summary>
<p>

**Abstract:** Neural networks are essential components of learning-based software systems. However, their high compute, memory, and power requirements make using them in low resources domains challenging. For this reason, neural networks are often quantized before deployment. Existing quantization techniques tend to degrade the network accuracy. We propose Counter-Example Guided Neural Network Quantization Refinement (CEG4N). This technique combines search-based quantization and equivalence verification: the former minimizes the computational requirements, while the latter guarantees that the network's output does not change after quantization. We evaluate CEG4N~on a diverse set of benchmarks, including large and small networks. Our technique successfully quantizes the networks in our evaluation while producing models with up to 72% better accuracy than state-of-the-art techniques.

</p>
</details>

<details><summary><b>Generating Pseudo-labels Adaptively for Few-shot Model-Agnostic Meta-Learning</b>
<a href="https://arxiv.org/abs/2207.04217">arxiv:2207.04217</a>
&#x1F4C8; 2 <br>
<p>Guodong Liu, Tongling Wang, Shuoxi Zhang, Kun He</p></summary>
<p>

**Abstract:** Model-Agnostic Meta-Learning (MAML) is a famous few-shot learning method that has inspired many follow-up efforts, such as ANIL and BOIL. However, as an inductive method, MAML is unable to fully utilize the information of query set, limiting its potential of gaining higher generality. To address this issue, we propose a simple yet effective method that generates psuedo-labels adaptively and could boost the performance of the MAML family. The proposed methods, dubbed Generative Pseudo-label based MAML (GP-MAML), GP-ANIL and GP-BOIL, leverage statistics of the query set to improve the performance on new tasks. Specifically, we adaptively add pseudo labels and pick samples from the query set, then re-train the model using the picked query samples together with the support set. The GP series can also use information from the pseudo query set to re-train the network during the meta-testing. While some transductive methods, such as Transductive Propagation Network (TPN), struggle to achieve this goal.

</p>
</details>

<details><summary><b>SCouT: Synthetic Counterfactuals via Spatiotemporal Transformers for Actionable Healthcare</b>
<a href="https://arxiv.org/abs/2207.04208">arxiv:2207.04208</a>
&#x1F4C8; 2 <br>
<p>Bhishma Dedhia, Roshini Balasubramanian, Niraj K. Jha</p></summary>
<p>

**Abstract:** The Synthetic Control method has pioneered a class of powerful data-driven techniques to estimate the counterfactual reality of a unit from donor units. At its core, the technique involves a linear model fitted on the pre-intervention period that combines donor outcomes to yield the counterfactual. However, linearly combining spatial information at each time instance using time-agnostic weights fails to capture important inter-unit and intra-unit temporal contexts and complex nonlinear dynamics of real data. We instead propose an approach to use local spatiotemporal information before the onset of the intervention as a promising way to estimate the counterfactual sequence. To this end, we suggest a Transformer model that leverages particular positional embeddings, a modified decoder attention mask, and a novel pre-training task to perform spatiotemporal sequence-to-sequence modeling. Our experiments on synthetic data demonstrate the efficacy of our method in the typical small donor pool setting and its robustness against noise. We also generate actionable healthcare insights at the population and patient levels by simulating a state-wide public health policy to evaluate its effectiveness, an in silico trial for asthma medications to support randomized controlled trials, and a medical intervention for patients with Friedreich's ataxia to improve clinical decision-making and promote personalized therapy.

</p>
</details>

<details><summary><b>Learning Structured Representations of Visual Scenes</b>
<a href="https://arxiv.org/abs/2207.04200">arxiv:2207.04200</a>
&#x1F4C8; 2 <br>
<p>Meng-Jiun Chiou</p></summary>
<p>

**Abstract:** As the intermediate-level representations bridging the two levels, structured representations of visual scenes, such as visual relationships between pairwise objects, have been shown to not only benefit compositional models in learning to reason along with the structures but provide higher interpretability for model decisions. Nevertheless, these representations receive much less attention than traditional recognition tasks, leaving numerous open challenges unsolved. In the thesis, we study how machines can describe the content of the individual image or video with visual relationships as the structured representations. Specifically, we explore how structured representations of visual scenes can be effectively constructed and learned in both the static-image and video settings, with improvements resulting from external knowledge incorporation, bias-reducing mechanism, and enhanced representation models. At the end of this thesis, we also discuss some open challenges and limitations to shed light on future directions of structured representation learning for visual scenes.

</p>
</details>

<details><summary><b>Rank-Enhanced Low-Dimensional Convolution Set for Hyperspectral Image Denoising</b>
<a href="https://arxiv.org/abs/2207.04266">arxiv:2207.04266</a>
&#x1F4C8; 1 <br>
<p>Jinhui Hou, Zhiyu Zhu, Hui Liu, Junhui Hou</p></summary>
<p>

**Abstract:** This paper tackles the challenging problem of hyperspectral (HS) image denoising. Unlike existing deep learning-based methods usually adopting complicated network architectures or empirically stacking off-the-shelf modules to pursue performance improvement, we focus on the efficient and effective feature extraction manner for capturing the high-dimensional characteristics of HS images. To be specific, based on the theoretical analysis that increasing the rank of the matrix formed by the unfolded convolutional kernels can promote feature diversity, we propose rank-enhanced low-dimensional convolution set (Re-ConvSet), which separately performs 1-D convolution along the three dimensions of an HS image side-by-side, and then aggregates the resulting spatial-spectral embeddings via a learnable compression layer. Re-ConvSet not only learns the diverse spatial-spectral features of HS images, but also reduces the parameters and complexity of the network. We then incorporate Re-ConvSet into the widely-used U-Net architecture to construct an HS image denoising method. Surprisingly, we observe such a concise framework outperforms the most recent method to a large extent in terms of quantitative metrics, visual results, and efficiency. We believe our work may shed light on deep learning-based HS image processing and analysis.

</p>
</details>

<details><summary><b>Subclasses of Class Function used to Implement Transformations of Statistical Models</b>
<a href="https://arxiv.org/abs/2207.04218">arxiv:2207.04218</a>
&#x1F4C8; 1 <br>
<p>Lloyd Allison</p></summary>
<p>

**Abstract:** A library of software for inductive inference guided by the Minimum Message Length (MML) principle was created previously. It contains various (object-oriented-) classes and subclasses of statistical Model and can be used to infer Models from given data sets in machine learning problems. Here transformations of statistical Models are considered and implemented within the library so as to have desirable properties from the object-oriented programming and mathematical points of view. The subclasses of class Function needed to do such transformations are defined.

</p>
</details>

<details><summary><b>Sequential Manipulation Planning on Scene Graph</b>
<a href="https://arxiv.org/abs/2207.04364">arxiv:2207.04364</a>
&#x1F4C8; 0 <br>
<p>Ziyuan Jiao, Yida Niu, Zeyu Zhang, Song-Chun Zhu, Yixin Zhu, Hangxin Liu</p></summary>
<p>

**Abstract:** We devise a 3D scene graph representation, contact graph+ (cg+), for efficient sequential task planning. Augmented with predicate-like attributes, this contact graph-based representation abstracts scene layouts with succinct geometric information and valid robot-scene interactions. Goal configurations, naturally specified on contact graphs, can be produced by a genetic algorithm with a stochastic optimization method. A task plan is then initialized by computing the Graph Editing Distance (GED) between the initial contact graphs and the goal configurations, which generates graph edit operations corresponding to possible robot actions. We finalize the task plan by imposing constraints to regulate the temporal feasibility of graph edit operations, ensuring valid task and motion correspondences. In a series of simulations and experiments, robots successfully complete complex sequential object rearrangement tasks that are difficult to specify using conventional planning language like Planning Domain Definition Language (PDDL), demonstrating the high feasibility and potential of robot sequential task planning on contact graph.

</p>
</details>

<details><summary><b>Wasserstein Graph Distance based on $L_1$-Approximated Tree Edit Distance between Weisfeiler-Lehman Subtrees</b>
<a href="https://arxiv.org/abs/2207.04216">arxiv:2207.04216</a>
&#x1F4C8; 0 <br>
<p>Zhongxi Fang, Jianming Huang, Xun Su, Hiroyuki Kasai</p></summary>
<p>

**Abstract:** The Weisfeiler-Lehman (WL) test has been widely applied to graph kernels, metrics, and neural networks. However, it considers only the graph consistency, resulting in the weak descriptive power of structural information. Thus, it limits the performance improvement of applied methods. In addition, the similarity and distance between graphs defined by the WL test are in coarse measurements. To the best of our knowledge, this paper clarifies these facts for the first time and defines a metric we call the Wasserstein WL subtree (WWLS) distance. We introduce the WL subtree as the structural information in the neighborhood of nodes and assign it to each node. Then we define a new graph embedding space based on $L_1$-approximated tree edit distance ($L_1$-TED): the $L_1$ norm of the difference between node feature vectors on the space is the $L_1$-TED between these nodes. We further propose a fast algorithm for graph embedding. Finally, we use the Wasserstein distance to reflect the $L_1$-TED to the graph level. The WWLS can capture small changes in structure that are difficult with traditional metrics. We demonstrate its performance in several graph classification and metric validation experiments.

</p>
</details>


{% endraw %}
Prev: [2022.07.08]({{ '/2022/07/08/2022.07.08.html' | relative_url }})  Next: [2022.07.10]({{ '/2022/07/10/2022.07.10.html' | relative_url }})