Prev: [2022.04.16]({{ '/2022/04/16/2022.04.16.html' | relative_url }})  Next: [2022.04.18]({{ '/2022/04/18/2022.04.18.html' | relative_url }})
{% raw %}
## Summary for 2022-04-17, created on 2022-04-27


<details><summary><b>Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport</b>
<a href="https://arxiv.org/abs/2204.08472">arxiv:2204.08472</a>
&#x1F4C8; 45 <br>
<p>Yingtao Tian, Marco Cuturi, David Ha</p></summary>
<p>

**Abstract:** Recent advances in deep learning, such as powerful generative models and joint text-image embeddings, have provided the computational creativity community with new tools, opening new perspectives for artistic pursuits. Text-to-image synthesis approaches that operate by generating images from text cues provide a case in point. These images are generated with a latent vector that is progressively refined to agree with text cues. To do so, patches are sampled within the generated image, and compared with the text prompts in the common text-image embedding space; The latent vector is then updated, using gradient descent, to reduce the mean (average) distance between these patches and text cues. While this approach provides artists with ample freedom to customize the overall appearance of images, through their choice in generative models, the reliance on a simple criterion (mean of distances) often causes mode collapse: The entire image is drawn to the average of all text cues, thereby losing their diversity. To address this issue, we propose using matching techniques found in the optimal transport (OT) literature, resulting in images that are able to reflect faithfully a wide diversity of prompts. We provide numerous illustrations showing that OT avoids some of the pitfalls arising from estimating vectors with mean distances, and demonstrate the capacity of our proposed method to perform better in experiments, qualitatively and quantitatively.

</p>
</details>

<details><summary><b>An Extendable, Efficient and Effective Transformer-based Object Detector</b>
<a href="https://arxiv.org/abs/2204.07962">arxiv:2204.07962</a>
&#x1F4C8; 42 <br>
<p>Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang</p></summary>
<p>

**Abstract:** Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.

</p>
</details>

<details><summary><b>CPFair: Personalized Consumer and Producer Fairness Re-ranking for Recommender Systems</b>
<a href="https://arxiv.org/abs/2204.08085">arxiv:2204.08085</a>
&#x1F4C8; 21 <br>
<p>Mohammadmehdi Naghiaei, Hossein A. Rahmani, Yashar Deldjoo</p></summary>
<p>

**Abstract:** Recently, there has been a rising awareness that when machine learning (ML) algorithms are used to automate choices, they may treat/affect individuals unfairly, with legal, ethical, or economic consequences. Recommender systems are prominent examples of such ML systems that assist users in making high-stakes judgments. A common trend in the previous literature research on fairness in recommender systems is that the majority of works treat user and item fairness concerns separately, ignoring the fact that recommender systems operate in a two-sided marketplace. In this work, we present an optimization-based re-ranking approach that seamlessly integrates fairness constraints from both the consumer and producer-side in a joint objective framework. We demonstrate through large-scale experiments on 8 datasets that our proposed method is capable of improving both consumer and producer fairness without reducing overall recommendation quality, demonstrating the role algorithms may play in minimizing data biases.

</p>
</details>

<details><summary><b>MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration</b>
<a href="https://arxiv.org/abs/2204.08058">arxiv:2204.08058</a>
&#x1F4C8; 10 <br>
<p>Thomas Hayes, Songyang Zhang, Xi Yin, Guan Pang, Sasha Sheng, Harry Yang, Songwei Ge, Qiyuan Hu, Devi Parikh</p></summary>
<p>

**Abstract:** Multimodal video-audio-text understanding and generation can benefit from datasets that are narrow but rich. The narrowness allows bite-sized challenges that the research community can make progress on. The richness ensures we are making progress along the core challenges. To this end, we present a large-scale video-audio-text dataset MUGEN, collected using the open-sourced platform game CoinRun [11]. We made substantial modifications to make the game richer by introducing audio and enabling new interactions. We trained RL agents with different objectives to navigate the game and interact with 13 objects and characters. This allows us to automatically extract a large collection of diverse videos and associated audio. We sample 375K video clips (3.2s each) and collect text descriptions from human annotators. Each video has additional annotations that are extracted automatically from the game engine, such as accurate semantic maps for each frame and templated textual descriptions. Altogether, MUGEN can help progress research in many tasks in multimodal understanding and generation. We benchmark representative approaches on tasks involving video-audio-text retrieval and generation. Our dataset and code are released at: https://mugen-org.github.io/.

</p>
</details>

<details><summary><b>Self-Aware Personalized Federated Learning</b>
<a href="https://arxiv.org/abs/2204.08069">arxiv:2204.08069</a>
&#x1F4C8; 7 <br>
<p>Huili Chen, Jie Ding, Eric Tramel, Shuang Wu, Anit Kumar Sahu, Salman Avestimehr, Tao Zhang</p></summary>
<p>

**Abstract:** In the context of personalized federated learning (FL), the critical challenge is to balance local model improvement and global model tuning when the personal and global objectives may not be exactly aligned. Inspired by Bayesian hierarchical models, we develop a self-aware personalized FL method where each client can automatically balance the training of its local personal model and the global model that implicitly contributes to other clients' training. Such a balance is derived from the inter-client and intra-client uncertainty quantification. A larger inter-client variation implies more personalization is needed. Correspondingly, our method uses uncertainty-driven local training steps and aggregation rule instead of conventional local fine-tuning and sample size-based aggregation. With experimental studies on synthetic data, Amazon Alexa audio data, and public datasets such as MNIST, FEMNIST, CIFAR10, and Sent140, we show that our proposed method can achieve significantly improved personalization performance compared with the existing counterparts.

</p>
</details>

<details><summary><b>Limit theorems of Chatterjee's rank correlation</b>
<a href="https://arxiv.org/abs/2204.08031">arxiv:2204.08031</a>
&#x1F4C8; 7 <br>
<p>Zhexiao Lin, Fang Han</p></summary>
<p>

**Abstract:** Establishing limiting distributions of Chatterjee's rank correlation for a general, possibly non-independent, pair of random variables has been eagerly awaited to many. This paper shows that (a) Chatterjee's rank correlation is asymptotically normal as long as one variable is not a measurable function of the other, and (b) the corresponding asymptotic variance is uniformly bounded by 36. Similar results also hold for Azadkia-Chatterjee's graph-based correlation coefficient, a multivariate analogue of Chatterjee's original proposal. The proof is given by appealing to Hájek representation and Chatterjee's nearest-neighbor CLT.

</p>
</details>

<details><summary><b>U-Net and its variants for Medical Image Segmentation : A short review</b>
<a href="https://arxiv.org/abs/2204.08470">arxiv:2204.08470</a>
&#x1F4C8; 6 <br>
<p>Vinay Ummadi</p></summary>
<p>

**Abstract:** The paper is a short review of medical image segmentation using U-Net and its variants. As we understand going through a medical images is not an easy job for any clinician either radiologist or pathologist. Analysing medical images is the only way to perform non-invasive diagnosis. Segmenting out the regions of interest has significant importance in medical images and is key for diagnosis. This paper also gives a bird eye view of how medical image segmentation has evolved. Also discusses challenge's and success of the deep neural architectures. Following how different hybrid architectures have built upon strong techniques from visual recognition tasks. In the end we will see current challenges and future directions for medical image segmentation(MIS).

</p>
</details>

<details><summary><b>Non-Parallel Text Style Transfer with Self-Parallel Supervision</b>
<a href="https://arxiv.org/abs/2204.08123">arxiv:2204.08123</a>
&#x1F4C8; 6 <br>
<p>Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, Soroush Vosoughi</p></summary>
<p>

**Abstract:** The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.

</p>
</details>

<details><summary><b>A Data-Driven Methodology for Considering Feasibility and Pairwise Likelihood in Deep Learning Based Guitar Tablature Transcription Systems</b>
<a href="https://arxiv.org/abs/2204.08094">arxiv:2204.08094</a>
&#x1F4C8; 6 <br>
<p>Frank Cwitkowitz, Jonathan Driedger, Zhiyao Duan</p></summary>
<p>

**Abstract:** Guitar tablature transcription is an important but understudied problem within the field of music information retrieval. Traditional signal processing approaches offer only limited performance on the task, and there is little acoustic data with transcription labels for training machine learning models. However, guitar transcription labels alone are more widely available in the form of tablature, which is commonly shared among guitarists online. In this work, a collection of symbolic tablature is leveraged to estimate the pairwise likelihood of notes on the guitar. The output layer of a baseline tablature transcription model is reformulated, such that an inhibition loss can be incorporated to discourage the co-activation of unlikely note pairs. This naturally enforces playability constraints for guitar, and yields tablature which is more consistent with the symbolic data used to estimate pairwise likelihoods. With this methodology, we show that symbolic tablature can be used to shape the distribution of a tablature transcription model's predictions, even when little acoustic data is available.

</p>
</details>

<details><summary><b>NICO++: Towards Better Benchmarking for Domain Generalization</b>
<a href="https://arxiv.org/abs/2204.08040">arxiv:2204.08040</a>
&#x1F4C8; 6 <br>
<p>Xingxuan Zhang, Yue He, Renzhe Xu, Han Yu, Zheyan Shen, Peng Cui</p></summary>
<p>

**Abstract:** Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection.

</p>
</details>

<details><summary><b>On Arbitrary Compression for Decentralized Consensus and Stochastic Optimization over Directed Networks</b>
<a href="https://arxiv.org/abs/2204.08160">arxiv:2204.08160</a>
&#x1F4C8; 5 <br>
<p>Mohammad Taha Toghani, César A. Uribe</p></summary>
<p>

**Abstract:** We study the decentralized consensus and stochastic optimization problems with compressed communications over static directed graphs. We propose an iterative gradient-based algorithm that compresses messages according to a desired compression ratio. The proposed method provably reduces the communication overhead on the network at every communication round. Contrary to existing literature, we allow for arbitrary compression ratios in the communicated messages. We show a linear convergence rate for the proposed method on the consensus problem. Moreover, we provide explicit convergence rates for decentralized stochastic optimization problems on smooth functions that are either (i) strongly convex, (ii) convex, or (iii) non-convex. Finally, we provide numerical experiments to illustrate convergence under arbitrary compression ratios and the communication efficiency of our algorithm.

</p>
</details>

<details><summary><b>Continual Hippocampus Segmentation with Transformers</b>
<a href="https://arxiv.org/abs/2204.08043">arxiv:2204.08043</a>
&#x1F4C8; 5 <br>
<p>Amin Ranem, Camila González, Anirban Mukhopadhyay</p></summary>
<p>

**Abstract:** In clinical settings, where acquisition conditions and patient populations change over time, continual learning is key for ensuring the safe use of deep neural networks. Yet most existing work focuses on convolutional architectures and image classification. Instead, radiologists prefer to work with segmentation models that outline specific regions-of-interest, for which Transformer-based architectures are gaining traction. The self-attention mechanism of Transformers could potentially mitigate catastrophic forgetting, opening the way for more robust medical image segmentation. In this work, we explore how recently-proposed Transformer mechanisms for semantic segmentation behave in sequential learning scenarios, and analyse how best to adapt continual learning strategies for this setting. Our evaluation on hippocampus segmentation shows that Transformer mechanisms mitigate catastrophic forgetting for medical image segmentation compared to purely convolutional architectures, and demonstrates that regularising ViT modules should be done with caution.

</p>
</details>

<details><summary><b>Unsupervised Cross-Task Generalization via Retrieval Augmentation</b>
<a href="https://arxiv.org/abs/2204.07937">arxiv:2204.07937</a>
&#x1F4C8; 5 <br>
<p>Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, Xiang Ren</p></summary>
<p>

**Abstract:** Humans can perform unseen tasks by recalling relevant skills that are acquired previously and then generalizing them to the target tasks, even if there is no supervision at all. In this paper, we aim to improve such cross-task generalization ability of massive multi-task language models such as T0 (Sanh et al., 2021) in an unsupervised setting. We propose a retrieval-augmentation method named ReCross that takes a few unlabelled examples as queries to retrieve a small subset of upstream data and uses them to update the multi-task model for better generalization. Our empirical results show that the proposed ReCross consistently outperforms non-retrieval baselines by a significant margin.

</p>
</details>

<details><summary><b>Learning Theory of Mind via Dynamic Traits Attribution</b>
<a href="https://arxiv.org/abs/2204.09047">arxiv:2204.09047</a>
&#x1F4C8; 4 <br>
<p>Dung Nguyen, Phuoc Nguyen, Hung Le, Kien Do, Svetha Venkatesh, Truyen Tran</p></summary>
<p>

**Abstract:** Machine learning of Theory of Mind (ToM) is essential to build social agents that co-live with humans and other agents. This capacity, once acquired, will help machines infer the mental states of others from observed contextual action trajectories, enabling future prediction of goals, intention, actions and successor representations. The underlying mechanism for such a prediction remains unclear, however. Inspired by the observation that humans often infer the character traits of others, then use it to explain behaviour, we propose a new neural ToM architecture that learns to generate a latent trait vector of an actor from the past trajectories. This trait vector then multiplicatively modulates the prediction mechanism via a `fast weights' scheme in the prediction neural network, which reads the current context and predicts the behaviour. We empirically show that the fast weights provide a good inductive bias to model the character traits of agents and hence improves mindreading ability. On the indirect assessment of false-belief understanding, the new ToM model enables more efficient helping behaviours.

</p>
</details>

<details><summary><b>Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning</b>
<a href="https://arxiv.org/abs/2204.08143">arxiv:2204.08143</a>
&#x1F4C8; 4 <br>
<p>Hongzhan Lin, Jing Ma, Liangliang Chen, Zhiwei Yang, Mingfei Cheng, Guang Chen</p></summary>
<p>

**Abstract:** Massive false rumors emerging along with breaking news or trending topics severely hinder the truth. Existing rumor detection approaches achieve promising performance on the yesterday's news, since there is enough corpus collected from the same domain for model training. However, they are poor at detecting rumors about unforeseen events especially those propagated in different languages due to the lack of training data and prior knowledge (i.e., low-resource regimes). In this paper, we propose an adversarial contrastive learning framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. Our model explicitly overcomes the restriction of domain and/or language usage via language alignment and a novel supervised contrastive training paradigm. Moreover, we develop an adversarial augmentation mechanism to further enhance the robustness of low-resource rumor representation. Extensive experiments conducted on two low-resource datasets collected from real-world microblog platforms demonstrate that our framework achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages.

</p>
</details>

<details><summary><b>Trinary Tools for Continuously Valued Binary Classifiers</b>
<a href="https://arxiv.org/abs/2204.08136">arxiv:2204.08136</a>
&#x1F4C8; 4 <br>
<p>Michael Gleicher, Xinyi Yu, Yuheng Chen</p></summary>
<p>

**Abstract:** Classification methods for binary (yes/no) tasks often produce a continuously valued score. Machine learning practitioners must perform model selection, calibration, discretization, performance assessment, tuning, and fairness assessment. Such tasks involve examining classifier results, typically using summary statistics and manual examination of details. In this paper, we provide an interactive visualization approach to support such continuously-valued classifier examination tasks. Our approach addresses the three phases of these tasks: calibration, operating point selection, and examination. We enhance standard views and introduce task-specific views so that they can be integrated into a multi-view coordination (MVC) system. We build on an existing comparison-based approach, extending it to continuous classifiers by treating the continuous values as trinary (positive, unsure, negative) even if the classifier will not ultimately use the 3-way classification. We provide use cases that demonstrate how our approach enables machine learning practitioners to accomplish key tasks.

</p>
</details>

<details><summary><b>Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction</b>
<a href="https://arxiv.org/abs/2204.08107">arxiv:2204.08107</a>
&#x1F4C8; 4 <br>
<p>Nikhil Krishnaswamy, Sadaf Ghaffari</p></summary>
<p>

**Abstract:** In this paper we present a novel method for a naive agent to detect novel objects it encounters in an interaction. We train a reinforcement learning policy on a stacking task given a known object type, and then observe the results of the agent attempting to stack various other objects based on the same trained policy. By extracting embedding vectors from a convolutional neural net trained over the results of the aforementioned stacking play, we can determine the similarity of a given object to known object types, and determine if the given object is likely dissimilar enough to the known types to be considered a novel class of object. We present the results of this method on two datasets gathered using two different policies and demonstrate what information the agent needs to extract from its environment to make these novelty judgments.

</p>
</details>

<details><summary><b>Learning Compositional Representations for Effective Low-Shot Generalization</b>
<a href="https://arxiv.org/abs/2204.08090">arxiv:2204.08090</a>
&#x1F4C8; 4 <br>
<p>Samarth Mishra, Pengkai Zhu, Venkatesh Saligrama</p></summary>
<p>

**Abstract:** We propose Recognition as Part Composition (RPC), an image encoding approach inspired by human cognition. It is based on the cognitive theory that humans recognize complex objects by components, and that they build a small compact vocabulary of concepts to represent each instance with. RPC encodes images by first decomposing them into salient parts, and then encoding each part as a mixture of a small number of prototypes, each representing a certain concept. We find that this type of learning inspired by human cognition can overcome hurdles faced by deep convolutional networks in low-shot generalization tasks, like zero-shot learning, few-shot learning and unsupervised domain adaptation. Furthermore, we find a classifier using an RPC image encoder is fairly robust to adversarial attacks, that deep neural networks are known to be prone to. Given that our image encoding principle is based on human cognition, one would expect the encodings to be interpretable by humans, which we find to be the case via crowd-sourcing experiments. Finally, we propose an application of these interpretable encodings in the form of generating synthetic attribute annotations for evaluating zero-shot learning methods on new datasets.

</p>
</details>

<details><summary><b>Intelligent Explorations of the String Theory Landscape</b>
<a href="https://arxiv.org/abs/2204.08073">arxiv:2204.08073</a>
&#x1F4C8; 4 <br>
<p>Andrei Constantin</p></summary>
<p>

**Abstract:** The goal of identifying the Standard Model of particle physics and its extensions within string theory has been one of the principal driving forces in string phenomenology. Recently, the incorporation of artificial intelligence in string theory and certain theoretical advancements have brought to light unexpected solutions to mathematical hurdles that have so far hindered progress in this direction. In this review we focus on model building efforts in the context of the $E_8\times E_8$ heterotic string compactified on smooth Calabi-Yau threefolds and discuss several areas in which machine learning is expected to make a difference.

</p>
</details>

<details><summary><b>Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN</b>
<a href="https://arxiv.org/abs/2204.07988">arxiv:2204.07988</a>
&#x1F4C8; 4 <br>
<p>Zhichao Liu, Liyue Qian, Wenke Jing, Desen Zhou, Xuming He, Edmond Lou, Rui Zheng</p></summary>
<p>

**Abstract:** Ultrasound spine imaging technique has been applied to the assessment of spine deformity. However, manual measurements of scoliotic angles on ultrasound images are time-consuming and heavily rely on raters experience. The objectives of this study are to construct a fully automatic framework based on Faster R-CNN for detecting vertebral lamina and to measure the fitting spinal curves from the detected lamina pairs. The framework consisted of two closely linked modules: 1) the lamina detector for identifying and locating each lamina pairs on ultrasound coronal images, and 2) the spinal curvature estimator for calculating the scoliotic angles based on the chain of detected lamina. Two hundred ultrasound images obtained from AIS patients were identified and used for the training and evaluation of the proposed method. The experimental results showed the 0.76 AP on the test set, and the Mean Absolute Difference (MAD) between automatic and manual measurement which was within the clinical acceptance error. Meanwhile the correlation between automatic measurement and Cobb angle from radiographs was 0.79. The results revealed that our proposed technique could provide accurate and reliable automatic curvature measurements on ultrasound spine images for spine deformities.

</p>
</details>

<details><summary><b>SDGCCA: Supervised Deep Generalized Canonical Correlation Analysis for Multi-omics Integration</b>
<a href="https://arxiv.org/abs/2204.09045">arxiv:2204.09045</a>
&#x1F4C8; 3 <br>
<p>Jeongyoung Hwang, Sehwan Moon, Hyunju Lee</p></summary>
<p>

**Abstract:** Integration of multi-omics data provides opportunities for revealing biological mechanisms related to certain phenotypes. We propose a novel method of multi-omics integration called supervised deep generalized canonical correlation analysis (SDGCCA) for modeling correlation structures between nonlinear multi-omics manifolds, aiming for improving classification of phenotypes and revealing biomarkers related to phenotypes. SDGCCA addresses the limitations of other canonical correlation analysis (CCA)-based models (e.g., deep CCA, deep generalized CCA) by considering complex/nonlinear cross-data correlations and discriminating phenotype groups. Although there are a few methods for nonlinear CCA projections for discriminant purposes of phenotypes, they only consider two views. On the other hand, SDGCCA is the nonlinear multiview CCA projection method for discrimination. When we applied SDGCCA to prediction of patients of Alzheimer's disease (AD) and discrimination of early- and late-stage cancers, it outperformed other CCA-based methods and other supervised methods. In addition, we demonstrate that SDGCCA can be used for feature selection to identify important multi-omics biomarkers. In the application on AD data, SDGCCA identified clusters of genes in multi-omics data, which are well known to be associated with AD.

</p>
</details>

<details><summary><b>AI for human assessment: What do professional assessors need?</b>
<a href="https://arxiv.org/abs/2204.08471">arxiv:2204.08471</a>
&#x1F4C8; 3 <br>
<p>Riku Arakawa, Hiromu Yakura</p></summary>
<p>

**Abstract:** We present our case study that aims to help professional assessors make decisions in human assessment, in which they conduct interviews with assessees and evaluate their suitability for certain job roles. Our workshop with two industrial assessors revealed that a computational system that can extract nonverbal cues of assesses from interview videos would be beneficial to assessors in terms of supporting their decision making. In response, we developed such a system based on an unsupervised anomaly detection algorithm using multimodal behavioral features such as facial keypoints, pose, head pose, and gaze. Moreover, we enabled the system to output how much each feature contributed to the outlierness of the detected cues with the purpose of enhancing its interpretability. We then conducted a preliminary study to examine the validity of the system's output by using 20 actual assessment interview videos and involving the two assessors. The results suggested the advantages of using unsupervised anomaly detection in an interpretable manner by illustrating the informativeness of its outputs for assessors. Our approach, which builds on top of the idea of separation of observation and interpretation in human-AI teaming, will facilitate human decision making in highly contextual domains, such as human assessment, while keeping their trust in the system.

</p>
</details>

<details><summary><b>A dynamical systems based framework for dimension reduction</b>
<a href="https://arxiv.org/abs/2204.08155">arxiv:2204.08155</a>
&#x1F4C8; 3 <br>
<p>Ryeongkyung Yoon, Braxton Osting</p></summary>
<p>

**Abstract:** We propose a novel framework for learning a low-dimensional representation of data based on nonlinear dynamical systems, which we call dynamical dimension reduction (DDR). In the DDR model, each point is evolved via a nonlinear flow towards a lower-dimensional subspace; the projection onto the subspace gives the low-dimensional embedding. Training the model involves identifying the nonlinear flow and the subspace. Following the equation discovery method, we represent the vector field that defines the flow using a linear combination of dictionary elements, where each element is a pre-specified linear/nonlinear candidate function. A regularization term for the average total kinetic energy is also introduced and motivated by optimal transport theory. We prove that the resulting optimization problem is well-posed and establish several properties of the DDR method. We also show how the DDR method can be trained using a gradient-based optimization method, where the gradients are computed using the adjoint method from optimal control theory. The DDR method is implemented and compared on synthetic and example datasets to other dimension reductions methods, including PCA, t-SNE, and Umap.

</p>
</details>

<details><summary><b>Parallel Network with Channel Attention and Post-Processing for Carotid Arteries Vulnerable Plaque Segmentation in Ultrasound Images</b>
<a href="https://arxiv.org/abs/2204.08127">arxiv:2204.08127</a>
&#x1F4C8; 3 <br>
<p>Yanchao Yuan, Cancheng Li, Lu Xu, Ke Zhang, Yang Hua, Jicong Zhang</p></summary>
<p>

**Abstract:** Carotid arteries vulnerable plaques are a crucial factor in the screening of atherosclerosis by ultrasound technique. However, the plaques are contaminated by various noises such as artifact, speckle noise, and manual segmentation may be time-consuming. This paper proposes an automatic convolutional neural network (CNN) method for plaque segmentation in carotid ultrasound images using a small dataset. First, a parallel network with three independent scale decoders is utilized as our base segmentation network, pyramid dilation convolutions are used to enlarge receptive fields in the three segmentation sub-networks. Subsequently, the three decoders are merged to be rectified in channels by SENet. Thirdly, in test stage, the initially segmented plaque is refined by the max contour morphology post-processing to obtain the final plaque. Moreover, three loss function Dice loss, SSIM loss and cross-entropy loss are compared to segment plaques. Test results show that the proposed method with dice loss function yields a Dice value of 0.820, an IoU of 0.701, Acc of 0.969, and modified Hausdorff distance (MHD) of 1.43 for 30 vulnerable cases of plaques, it outperforms some of the conventional CNN-based methods on these metrics. Additionally, we apply an ablation experiment to show the validity of each proposed module. Our study provides some reference for similar researches and may be useful in actual applications for plaque segmentation of ultrasound carotid arteries.

</p>
</details>

<details><summary><b>HFT-ONLSTM: Hierarchical and Fine-Tuning Multi-label Text Classification</b>
<a href="https://arxiv.org/abs/2204.08115">arxiv:2204.08115</a>
&#x1F4C8; 3 <br>
<p>Pengfei Gao, Jingpeng Zhao, Yinglong Ma, Ahmad Tanvir, Beihong Jin</p></summary>
<p>

**Abstract:** Many important classification problems in the real-world consist of a large number of closely related categories in a hierarchical structure or taxonomy. Hierarchical multi-label text classification (HMTC) with higher accuracy over large sets of closely related categories organized in a hierarchy or taxonomy has become a challenging problem. In this paper, we present a hierarchical and fine-tuning approach based on the Ordered Neural LSTM neural network, abbreviated as HFT-ONLSTM, for more accurate level-by-level HMTC. First, we present a novel approach to learning the joint embeddings based on parent category labels and textual data for accurately capturing the joint features of both category labels and texts. Second, a fine tuning technique is adopted for training parameters such that the text classification results in the upper level should contribute to the classification in the lower one. At last, the comprehensive analysis is made based on extensive experiments in comparison with the state-of-the-art hierarchical and flat multi-label text classification approaches over two benchmark datasets, and the experimental results show that our HFT-ONLSTM approach outperforms these approaches, in particular reducing computational costs while achieving superior performance.

</p>
</details>

<details><summary><b>Attention Mechanism based Cognition-level Scene Understanding</b>
<a href="https://arxiv.org/abs/2204.08027">arxiv:2204.08027</a>
&#x1F4C8; 3 <br>
<p>Xuejiao Tang, Tai Le Quy, Eirini Ntoutsi, Kea Turner, Vasile Palade, Israat Haque, Peng Xu, Chris Brown, Wenbin Zhang</p></summary>
<p>

**Abstract:** Given a question-image input, the Visual Commonsense Reasoning (VCR) model can predict an answer with the corresponding rationale, which requires inference ability from the real world. The VCR task, which calls for exploiting the multi-source information as well as learning different levels of understanding and extensive commonsense knowledge, is a cognition-level scene understanding task. The VCR task has aroused researchers' interest due to its wide range of applications, including visual question answering, automated vehicle systems, and clinical decision support. Previous approaches to solving the VCR task generally rely on pre-training or exploiting memory with long dependency relationship encoded models. However, these approaches suffer from a lack of generalizability and losing information in long sequences. In this paper, we propose a parallel attention-based cognitive VCR network PAVCR, which fuses visual-textual information efficiently and encodes semantic information in parallel to enable the model to capture rich information for cognition-level inference. Extensive experiments show that the proposed model yields significant improvements over existing methods on the benchmark VCR dataset. Moreover, the proposed model provides intuitive interpretation into visual commonsense reasoning.

</p>
</details>

<details><summary><b>WikiOmnia: generative QA corpus on the whole Russian Wikipedia</b>
<a href="https://arxiv.org/abs/2204.08009">arxiv:2204.08009</a>
&#x1F4C8; 3 <br>
<p>Dina Pisarevskaya, Tatiana Shavrina</p></summary>
<p>

**Abstract:** The General QA field has been developing the methodology referencing the Stanford Question answering dataset (SQuAD) as the significant benchmark. However, compiling factual questions is accompanied by time- and labour-consuming annotation, limiting the training data's potential size. We present the WikiOmnia dataset, a new publicly available set of QA-pairs and corresponding Russian Wikipedia article summary sections, composed with a fully automated generative pipeline. The dataset includes every available article from Wikipedia for the Russian language. The WikiOmnia pipeline is available open-source and is also tested for creating SQuAD-formatted QA on other domains, like news texts, fiction, and social media. The resulting dataset includes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs with paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for ruT5-large) and cleaned data with strict automatic verification (over 160,000 QA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with paragraphs for ruT5-large).

</p>
</details>

<details><summary><b>Wound Severity Classification using Deep Neural Network</b>
<a href="https://arxiv.org/abs/2204.07942">arxiv:2204.07942</a>
&#x1F4C8; 3 <br>
<p>D. M. Anisuzzaman, Yash Patel, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</p></summary>
<p>

**Abstract:** The classification of wound severity is a critical step in wound diagnosis. An effective classifier can help wound professionals categorize wound conditions more quickly and affordably, allowing them to choose the best treatment option. This study used wound photos to construct a deep neural network-based wound severity classifier that classified them into one of three classes: green, yellow, or red. The green class denotes wounds still in the early stages of healing and are most likely to recover with adequate care. Wounds in the yellow category require more attention and treatment than those in the green category. Finally, the red class denotes the most severe wounds that require prompt attention and treatment. A dataset containing different types of wound images is designed with the help of wound specialists. Nine deep learning models are used with applying the concept of transfer learning. Several stacked models are also developed by concatenating these transfer learning models. The maximum accuracy achieved on multi-class classification is 68.49%. In addition, we achieved 78.79%, 81.40%, and 77.57% accuracies on green vs. yellow, green vs. red, and yellow vs. red classifications for binary classifications.

</p>
</details>

<details><summary><b>Centralized Adversarial Learning for Robust Deep Hashing</b>
<a href="https://arxiv.org/abs/2204.10779">arxiv:2204.10779</a>
&#x1F4C8; 2 <br>
<p>Xunguang Wang, Xu Yuan, Zheng Zhang, Guangming Lu, Xiaomeng Li</p></summary>
<p>

**Abstract:** Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. Recently, it becomes a hot issue to study adversarial examples which poses a security challenge to deep hashing models. However, there is still a critical bottleneck: how to find a superior and exact semantic representative as the guide to further enhance the adversarial attack and defense in deep hashing based retrieval. We, for the first time, attempt to design an effective adversarial learning with the min-max paradigm to improve the robustness of hashing networks by using the generated adversarial samples. Specifically, we obtain the optimal solution (called center code) through a proved Continuous Hash Center Method (CHCM), which preserves the semantic similarity with positive samples and dissimilarity with negative samples. On one hand, we propose the Deep Hashing Central Attack (DHCA) for efficient attack on hashing retrieval by maximizing the Hamming distance between the hash code of adversarial example and the center code. On the other hand, we present the Deep Hashing Central Adversarial Training (DHCAT) to optimize the hashing networks for defense, by minimizing the Hamming distance to the center code. Extensive experiments on the benchmark datasets verify that our attack method can achieve better performance than the state-of-the-arts, and our defense algorithm can effectively mitigate the effects of adversarial perturbations.

</p>
</details>

<details><summary><b>BSAL: A Framework of Bi-component Structure and Attribute Learning for Link Prediction</b>
<a href="https://arxiv.org/abs/2204.09508">arxiv:2204.09508</a>
&#x1F4C8; 2 <br>
<p>Bisheng Li, Min Zhou, Shengzhong Zhang, Menglin Yang, Defu Lian, Zengfeng Huang</p></summary>
<p>

**Abstract:** Given the ubiquitous existence of graph-structured data, learning the representations of nodes for the downstream tasks ranging from node classification, link prediction to graph classification is of crucial importance. Regarding missing link inference of diverse networks, we revisit the link prediction techniques and identify the importance of both the structural and attribute information. However, the available techniques either heavily count on the network topology which is spurious in practice or cannot integrate graph topology and features properly. To bridge the gap, we propose a bicomponent structural and attribute learning framework (BSAL) that is designed to adaptively leverage information from topology and feature spaces. Specifically, BSAL constructs a semantic topology via the node attributes and then gets the embeddings regarding the semantic view, which provides a flexible and easy-to-implement solution to adaptively incorporate the information carried by the node attributes. Then the semantic embedding together with topology embedding is fused together using an attention mechanism for the final prediction. Extensive experiments show the superior performance of our proposal and it significantly outperforms baselines on diverse research benchmarks.

</p>
</details>

<details><summary><b>Multi-scale Anomaly Detection for Big Time Series of Industrial Sensors</b>
<a href="https://arxiv.org/abs/2204.08159">arxiv:2204.08159</a>
&#x1F4C8; 2 <br>
<p>Quan Ding, Shenghua Liu, Bin Zhou, Huawei Shen, Xueqi Cheng</p></summary>
<p>

**Abstract:** Given a multivariate big time series, can we detect anomalies as soon as they occur? Many existing works detect anomalies by learning how much a time series deviates away from what it should be in the reconstruction framework. However, most models have to cut the big time series into small pieces empirically since optimization algorithms cannot afford such a long series. The question is raised: do such cuts pollute the inherent semantic segments, like incorrect punctuation in sentences? Therefore, we propose a reconstruction-based anomaly detection method, MissGAN, iteratively learning to decode and encode naturally smooth time series in coarse segments, and finding out a finer segment from low-dimensional representations based on HMM. As a result, learning from multi-scale segments, MissGAN can reconstruct a meaningful and robust time series, with the help of adversarial regularization and extra conditional states. MissGAN does not need labels or only needs labels of normal instances, making it widely applicable. Experiments on industrial datasets of real water network sensors show our MissGAN outperforms the baselines with scalability. Besides, we use a case study on the CMU Motion dataset to demonstrate that our model can well distinguish unexpected gestures from a given conditional motion.

</p>
</details>

<details><summary><b>Characterizing and Understanding Distributed GNN Training on GPUs</b>
<a href="https://arxiv.org/abs/2204.08150">arxiv:2204.08150</a>
&#x1F4C8; 2 <br>
<p>Haiyang Lin, Mingyu Yan, Xiaocheng Yang, Mo Zou, Wenming Li, Xiaochun Ye, Dongrui Fan</p></summary>
<p>

**Abstract:** Graph neural network (GNN) has been demonstrated to be a powerful model in many domains for its effectiveness in learning over graphs. To scale GNN training for large graphs, a widely adopted approach is distributed training which accelerates training using multiple computing nodes. Maximizing the performance is essential, but the execution of distributed GNN training remains preliminarily understood. In this work, we provide an in-depth analysis of distributed GNN training on GPUs, revealing several significant observations and providing useful guidelines for both software optimization and hardware optimization.

</p>
</details>

<details><summary><b>PiouCrypt: Decentralized Lattice-based Method for Visual Symmetric Cryptography</b>
<a href="https://arxiv.org/abs/2204.08017">arxiv:2204.08017</a>
&#x1F4C8; 2 <br>
<p>Navid Abapour, Mohsen Ebadpour</p></summary>
<p>

**Abstract:** In recent years, establishing secure visual communications has turned into one of the essential problems for security engineers and researchers. However, only limited novel solutions are provided for image encryption, and limiting the visual cryptography to only limited schemes can bring up negative consequences, especially with emerging quantum computational systems. This paper presents a novel algorithm for establishing secure private visual communication. The proposed method has a layered architecture with several cohesive components, and corresponded with an NP-hard problem, despite its symmetric structure. This two-step technique is not limited to gray-scale pictures, and furthermore, utilizing a lattice structure causes to proposed method has optimal resistance for the post-quantum era, and is relatively secure from the theoretical dimension.

</p>
</details>

<details><summary><b>On Effectively Learning of Knowledge in Continual Pre-training</b>
<a href="https://arxiv.org/abs/2204.07994">arxiv:2204.07994</a>
&#x1F4C8; 2 <br>
<p>Cunxiang Wang, Fuli Luo, Yanyang Li, Runxin Xu, Fei Huang, Yue Zhang</p></summary>
<p>

**Abstract:** Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.

</p>
</details>

<details><summary><b>AFSC: Adaptive Fourier Space Compression for Anomaly Detection</b>
<a href="https://arxiv.org/abs/2204.07963">arxiv:2204.07963</a>
&#x1F4C8; 2 <br>
<p>Haote Xu, Yunlong Zhang, Liyan Sun, Chenxin Li, Yue Huang, Xinghao Ding</p></summary>
<p>

**Abstract:** Anomaly Detection (AD) on medical images enables a model to recognize any type of anomaly pattern without lesion-specific supervised learning. Data augmentation based methods construct pseudo-healthy images by "pasting" fake lesions on real healthy ones, and a network is trained to predict healthy images in a supervised manner. The lesion can be found by difference between the unhealthy input and pseudo-healthy output. However, using only manually designed fake lesions fail to approximate to irregular real lesions, hence limiting the model generalization. We assume by exploring the intrinsic data property within images, we can distinguish previously unseen lesions from healthy regions in an unhealthy image. In this study, we propose an Adaptive Fourier Space Compression (AFSC) module to distill healthy feature for AD. The compression of both magnitude and phase in frequency domain addresses the hyper intensity and diverse position of lesions. Experimental results on the BraTS and MS-SEG datasets demonstrate an AFSC baseline is able to produce promising detection results, and an AFSC module can be effectively embedded into existing AD methods.

</p>
</details>

<details><summary><b>Towards Comprehensive Testing on the Robustness of Cooperative Multi-agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2204.07932">arxiv:2204.07932</a>
&#x1F4C8; 2 <br>
<p>Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, Simin Li</p></summary>
<p>

**Abstract:** While deep neural networks (DNNs) have strengthened the performance of cooperative multi-agent reinforcement learning (c-MARL), the agent policy can be easily perturbed by adversarial examples. Considering the safety critical applications of c-MARL, such as traffic management, power management and unmanned aerial vehicle control, it is crucial to test the robustness of c-MARL algorithm before it was deployed in reality. Existing adversarial attacks for MARL could be used for testing, but is limited to one robustness aspects (e.g., reward, state, action), while c-MARL model could be attacked from any aspect. To overcome the challenge, we propose MARLSafe, the first robustness testing framework for c-MARL algorithms. First, motivated by Markov Decision Process (MDP), MARLSafe consider the robustness of c-MARL algorithms comprehensively from three aspects, namely state robustness, action robustness and reward robustness. Any c-MARL algorithm must simultaneously satisfy these robustness aspects to be considered secure. Second, due to the scarceness of c-MARL attack, we propose c-MARL attacks as robustness testing algorithms from multiple aspects. Experiments on \textit{SMAC} environment reveals that many state-of-the-art c-MARL algorithms are of low robustness in all aspect, pointing out the urgent need to test and enhance robustness of c-MARL algorithms.

</p>
</details>

<details><summary><b>A Novel ASIC Design Flow using Weight-Tunable Binary Neurons as Standard Cells</b>
<a href="https://arxiv.org/abs/2204.08070">arxiv:2204.08070</a>
&#x1F4C8; 1 <br>
<p>Ankit Wagle, Gian Singh, Sunil Khatri, Sarma Vrudhula</p></summary>
<p>

**Abstract:** In this paper, we describe a design of a mixed signal circuit for a binary neuron (a.k.a perceptron, threshold logic gate) and a methodology for automatically embedding such cells in ASICs. The binary neuron, referred to as an FTL (flash threshold logic) uses floating gate or flash transistors whose threshold voltages serve as a proxy for the weights of the neuron. Algorithms for mapping the weights to the flash transistor threshold voltages are presented. The threshold voltages are determined to maximize both the robustness of the cell and its speed. The performance, power, and area of a single FTL cell are shown to be significantly smaller (79.4%), consume less power (61.6%), and operate faster (40.3%) compared to conventional CMOS logic equivalents. Also included are the architecture and the algorithms to program the flash devices of an FTL. The FTL cells are implemented as standard cells, and are designed to allow commercial synthesis and P&R tools to automatically use them in synthesis of ASICs. Substantial reductions in area and power without sacrificing performance are demonstrated on several ASIC benchmarks by the automatic embedding of FTL cells. The paper also demonstrates how FTL cells can be used for fixing timing errors after fabrication.

</p>
</details>

<details><summary><b>Addressing Leakage in Self-Supervised Contextualized Code Retrieval</b>
<a href="https://arxiv.org/abs/2204.11594">arxiv:2204.11594</a>
&#x1F4C8; 0 <br>
<p>Johannes Villmow, Viola Campos, Adrian Ulges, Ulrich Schwanecke</p></summary>
<p>

**Abstract:** We address contextualized code retrieval, the search for code snippets helpful to fill gaps in a partial input program. Our approach facilitates a large-scale self-supervised contrastive training by splitting source code randomly into contexts and targets. To combat leakage between the two, we suggest a novel approach based on mutual identifier masking, dedentation, and the selection of syntax-aligned targets. Our second contribution is a new dataset for direct evaluation of contextualized code retrieval, based on a dataset of manually aligned subpassages of code clones. Our experiments demonstrate that our approach improves retrieval substantially, and yields new state-of-the-art results for code clone and defect detection.

</p>
</details>

<details><summary><b>Residue-Based Natural Language Adversarial Attack Detection</b>
<a href="https://arxiv.org/abs/2204.10192">arxiv:2204.10192</a>
&#x1F4C8; 0 <br>
<p>Vyas Raina, Mark Gales</p></summary>
<p>

**Abstract:** Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text features, without consideration of model embedding spaces. This work examines what differences result when porting these image designed strategies to Natural Language Processing (NLP) tasks - these detectors are found to not port over well. This is expected as NLP systems have a very different form of input: discrete and sequential in nature, rather than the continuous and fixed size inputs for images. As an equivalent model-focused NLP detection approach, this work proposes a simple sentence-embedding "residue" based detector to identify adversarial examples. On many tasks, it out-performs ported image domain detectors and recent state of the art NLP specific detectors.

</p>
</details>

<details><summary><b>Synthetic Distracted Driving (SynDD1) dataset for analyzing distracted behaviors and various gaze zones of a driver</b>
<a href="https://arxiv.org/abs/2204.08096">arxiv:2204.08096</a>
&#x1F4C8; 0 <br>
<p>Mohammed Shaiqur Rahman, Archana Venkatachalapathy, Anuj Sharma, Jiyang Wang, Senem Velipasalar Gursoy, David Anastasiu, Shuo Wang</p></summary>
<p>

**Abstract:** This article presents a synthetic distracted driving (SynDD1) dataset for machine learning models to detect and analyze drivers' various distracted behavior and different gaze zones. We collected the data in a stationary vehicle using three in-vehicle cameras positioned at locations: on the dashboard, near the rearview mirror, and on the top right-side window corner. The dataset contains two activity types: distracted activities, and gaze zones for each participant and each activity type has two sets: without appearance blocks and with appearance blocks such as wearing a hat or sunglasses. The order and duration of each activity for each participant are random. In addition, the dataset contains manual annotations for each activity, having its start and end time annotated. Researchers could use this dataset to evaluate the performance of machine learning algorithms for the classification of various distracting activities and gaze zones of drivers.

</p>
</details>


{% endraw %}
Prev: [2022.04.16]({{ '/2022/04/16/2022.04.16.html' | relative_url }})  Next: [2022.04.18]({{ '/2022/04/18/2022.04.18.html' | relative_url }})