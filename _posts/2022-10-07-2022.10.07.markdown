Prev: [2022.10.06]({{ '/2022/10/06/2022.10.06.html' | relative_url }})  Next: [2022.10.08]({{ '/2022/10/08/2022.10.08.html' | relative_url }})
{% raw %}
## Summary for 2022-10-07, created on 2022-10-14


<details><summary><b>GNM: A General Navigation Model to Drive Any Robot</b>
<a href="https://arxiv.org/abs/2210.03370">arxiv:2210.03370</a>
&#x1F4C8; 594 <br>
<p>Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, Sergey Levine</p></summary>
<p>

**Abstract:** Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out http://sites.google.com/view/drive-any-robot.

</p>
</details>

<details><summary><b>Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding</b>
<a href="https://arxiv.org/abs/2210.03347">arxiv:2210.03347</a>
&#x1F4C8; 62 <br>
<p>Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova</p></summary>
<p>

**Abstract:** Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.

</p>
</details>

<details><summary><b>Supervised and Unsupervised Learning of Audio Representations for Music Understanding</b>
<a href="https://arxiv.org/abs/2210.03799">arxiv:2210.03799</a>
&#x1F4C8; 10 <br>
<p>Matthew C. McCallum, Filip Korzeniowski, Sergio Oramas, Fabien Gouyon, Andreas F. Ehmann</p></summary>
<p>

**Abstract:** In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.
  We show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.
  Within the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning -- and in some cases, supervised learning -- for music understanding.
  We also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.

</p>
</details>

<details><summary><b>Empowering Graph Representation Learning with Test-Time Graph Transformation</b>
<a href="https://arxiv.org/abs/2210.03561">arxiv:2210.03561</a>
&#x1F4C8; 10 <br>
<p>Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, Neil Shah</p></summary>
<p>

**Abstract:** As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTrans which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTrans on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTrans performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings.

</p>
</details>

<details><summary><b>CLAD: A realistic Continual Learning benchmark for Autonomous Driving</b>
<a href="https://arxiv.org/abs/2210.03482">arxiv:2210.03482</a>
&#x1F4C8; 10 <br>
<p>Eli Verwimp, Kuo Yang, Sarah Parisot, Hong Lanqing, Steven McDonagh, Eduardo Pérez-Pellitero, Matthias De Lange, Tinne Tuytelaars</p></summary>
<p>

**Abstract:** In this paper we describe the design and the ideas motivating a new Continual Learning benchmark for Autonomous Driving (CLAD), that focuses on the problems of object classification and object detection. The benchmark utilises SODA10M, a recently released large-scale dataset that concerns autonomous driving related problems. First, we review and discuss existing continual learning benchmarks, how they are related, and show that most are extreme cases of continual learning. To this end, we survey the benchmarks used in continual learning papers at three highly ranked computer vision conferences. Next, we introduce CLAD-C, an online classification benchmark realised through a chronological data stream that poses both class and domain incremental challenges; and CLAD-D, a domain incremental continual object detection benchmark. We examine the inherent difficulties and challenges posed by the benchmark, through a survey of the techniques and methods used by the top-3 participants in a CLAD-challenge workshop at ICCV 2021. We conclude with possible pathways to improve the current continual learning state of the art, and which directions we deem promising for future research.

</p>
</details>

<details><summary><b>Artificial Intelligence and Natural Language Processing and Understanding in Space: Four ESA Case Studies</b>
<a href="https://arxiv.org/abs/2210.03640">arxiv:2210.03640</a>
&#x1F4C8; 9 <br>
<p>José Manuel Gómez-Pérez, Andrés García-Silva, Rosemarie Leone, Mirko Albani, Moritz Fontaine, Charles Poncet, Leopold Summerer, Alessandro Donati, Ilaria Roma, Stefano Scaglioni</p></summary>
<p>

**Abstract:** The European Space Agency is well known as a powerful force for scientific discovery in numerous areas related to Space. The amount and depth of the knowledge produced throughout the different missions carried out by ESA and their contribution to scientific progress is enormous, involving large collections of documents like scientific publications, feasibility studies, technical reports, and quality management procedures, among many others. Through initiatives like the Open Space Innovation Platform, ESA also acts as a hub for new ideas coming from the wider community across different challenges, contributing to a virtuous circle of scientific discovery and innovation. Handling such wealth of information, of which large part is unstructured text, is a colossal task that goes beyond human capabilities, hence requiring automation. In this paper, we present a methodological framework based on artificial intelligence and natural language processing and understanding to automatically extract information from Space documents, generating value from it, and illustrate such framework through several case studies implemented across different functional areas of ESA, including Mission Design, Quality Assurance, Long-Term Data Preservation, and the Open Space Innovation Platform. In doing so, we demonstrate the value of these technologies in several tasks ranging from effortlessly searching and recommending Space information to automatically determining how innovative an idea can be, answering questions about Space, and generating quizzes regarding quality procedures. Each of these accomplishments represents a step forward in the application of increasingly intelligent AI systems in Space, from structuring and facilitating information access to intelligent systems capable to understand and reason with such information.

</p>
</details>

<details><summary><b>A Higher Purpose: Measuring Electricity Access Using High-Resolution Daytime Satellite Imagery</b>
<a href="https://arxiv.org/abs/2210.03909">arxiv:2210.03909</a>
&#x1F4C8; 8 <br>
<p>Zeal Shah, Simone Fobi, Gabriel Cadamuro, Jay Taneja</p></summary>
<p>

**Abstract:** Governments and international organizations the world over are investing towards the goal of achieving universal energy access for improving socio-economic development. However, in developing settings, monitoring electrification efforts is typically inaccurate, infrequent, and expensive. In this work, we develop and present techniques for high-resolution monitoring of electrification progress at scale. Specifically, our 3 unique contributions are: (i) identifying areas with(out) electricity access, (ii) quantifying the extent of electrification in electrified areas (percentage/number of electrified structures), and (iii) differentiating between customer types in electrified regions (estimating the percentage/number of residential/non-residential electrified structures). We combine high-resolution 50 cm daytime satellite images with Convolutional Neural Networks (CNNs) to train a series of classification and regression models. We evaluate our models using unique ground truth datasets on building locations, building types (residential/non-residential), and building electrification status. Our classification models show a 92% accuracy in identifying electrified regions, 85% accuracy in estimating percent of (low/high) electrified buildings within the region, and 69% accuracy in differentiating between (low/high) percentage of electrified residential buildings. Our regressions show $R^2$ scores of 78% and 80% in estimating the number of electrified buildings and number of residential electrified building in images respectively. We also demonstrate the generalizability of our models in never-before-seen regions to assess their potential for consistent and high-resolution measurements of electrification in emerging economies, and conclude by highlighting opportunities for improvement.

</p>
</details>

<details><summary><b>Scene-level Tracking and Reconstruction without Object Priors</b>
<a href="https://arxiv.org/abs/2210.03815">arxiv:2210.03815</a>
&#x1F4C8; 8 <br>
<p>Haonan Chang, Abdeslam Boularias</p></summary>
<p>

**Abstract:** We present the first real-time system capable of tracking and reconstructing, individually, every visible object in a given scene, without any form of prior on the rigidness of the objects, texture existence, or object category. In contrast with previous methods such as Co-Fusion and MaskFusion that first segment the scene into individual objects and then process each object independently, the proposed method dynamically segments the non-rigid scene as part of the tracking and reconstruction process. When new measurements indicate topology change, reconstructed models are updated in real-time to reflect that change. Our proposed system can provide the live geometry and deformation of all visible objects in a novel scene in real-time, which makes it possible to be integrated seamlessly into numerous existing robotics applications that rely on object models for grasping and manipulation. The capabilities of the proposed system are demonstrated in challenging scenes that contain multiple rigid and non-rigid objects.

</p>
</details>

<details><summary><b>How to Enable Uncertainty Estimation in Proximal Policy Optimization</b>
<a href="https://arxiv.org/abs/2210.03649">arxiv:2210.03649</a>
&#x1F4C8; 8 <br>
<p>Eugene Bykovets, Yannick Metz, Mennatallah El-Assady, Daniel A. Keim, Joachim M. Buhmann</p></summary>
<p>

**Abstract:** While deep reinforcement learning (RL) agents have showcased strong results across many domains, a major concern is their inherent opaqueness and the safety of such systems in real-world use cases. To overcome these issues, we need agents that can quantify their uncertainty and detect out-of-distribution (OOD) states. Existing uncertainty estimation techniques, like Monte-Carlo Dropout or Deep Ensembles, have not seen widespread adoption in on-policy deep RL. We posit that this is due to two reasons: concepts like uncertainty and OOD states are not well defined compared to supervised learning, especially for on-policy RL methods. Secondly, available implementations and comparative studies for uncertainty estimation methods in RL have been limited. To overcome the first gap, we propose definitions of uncertainty and OOD for Actor-Critic RL algorithms, namely, proximal policy optimization (PPO), and present possible applicable measures. In particular, we discuss the concepts of value and policy uncertainty. The second point is addressed by implementing different uncertainty estimation methods and comparing them across a number of environments. The OOD detection performance is evaluated via a custom evaluation benchmark of in-distribution (ID) and OOD states for various RL environments. We identify a trade-off between reward and OOD detection performance. To overcome this, we formulate a Pareto optimization problem in which we simultaneously optimize for reward and OOD detection performance. We show experimentally that the recently proposed method of Masksembles strikes a favourable balance among the survey methods, enabling high-quality uncertainty estimation and OOD detection while matching the performance of original RL agents.

</p>
</details>

<details><summary><b>TAN without a burn: Scaling Laws of DP-SGD</b>
<a href="https://arxiv.org/abs/2210.03403">arxiv:2210.03403</a>
&#x1F4C8; 8 <br>
<p>Tom Sander, Pierre Stock, Alexandre Sablayrolles</p></summary>
<p>

**Abstract:** Differentially Private methods for training Deep Neural Networks (DNNs) have progressed recently, in particular with the use of massive batches and aggregated data augmentations for a large number of steps. These techniques require much more compute than their non-private counterparts, shifting the traditional privacy-accuracy trade-off to a privacy-accuracy-compute trade-off and making hyper-parameter search virtually impossible for realistic scenarios. In this work, we decouple privacy analysis and experimental behavior of noisy training to explore the trade-off with minimal computational requirements. We first use the tools of Rényi Differential Privacy (RDP) to show that the privacy budget, when not overcharged, only depends on the total amount of noise (TAN) injected throughout training. We then derive scaling laws for training models with DP-SGD to optimize hyper-parameters with more than a 100 reduction in computational budget. We apply the proposed method on CIFAR-10 and ImageNet and, in particular, strongly improve the state-of-the-art on ImageNet with a +9 points gain in accuracy for a privacy budget epsilon=8.

</p>
</details>

<details><summary><b>An Analysis of the Effects of Decoding Algorithms on Fairness in Open-Ended Language Generation</b>
<a href="https://arxiv.org/abs/2210.03826">arxiv:2210.03826</a>
&#x1F4C8; 7 <br>
<p>Jwala Dhamala, Varun Kumar, Rahul Gupta, Kai-Wei Chang, Aram Galstyan</p></summary>
<p>

**Abstract:** Several prior works have shown that language models (LMs) can generate text containing harmful social biases and stereotypes. While decoding algorithms play a central role in determining properties of LM generated text, their impact on the fairness of the generations has not been studied. We present a systematic analysis of the impact of decoding algorithms on LM fairness, and analyze the trade-off between fairness, diversity and quality. Our experiments with top-$p$, top-$k$ and temperature decoding algorithms, in open-ended language generation, show that fairness across demographic groups changes significantly with change in decoding algorithm's hyper-parameters. Notably, decoding algorithms that output more diverse text also output more texts with negative sentiment and regard. We present several findings and provide recommendations on standardized reporting of decoding details in fairness evaluations and optimization of decoding algorithms for fairness alongside quality and diversity.

</p>
</details>

<details><summary><b>An Investigation into Whitening Loss for Self-supervised Learning</b>
<a href="https://arxiv.org/abs/2210.03586">arxiv:2210.03586</a>
&#x1F4C8; 7 <br>
<p>Xi Weng, Lei Huang, Lei Zhao, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan</p></summary>
<p>

**Abstract:** A desirable objective in self-supervised learning (SSL) is to avoid feature collapse. Whitening loss guarantees collapse avoidance by minimizing the distance between embeddings of positive pairs under the conditioning that the embeddings from different views are whitened. In this paper, we propose a framework with an informative indicator to analyze whitening loss, which provides a clue to demystify several interesting phenomena as well as a pivoting point connecting to other SSL methods. We reveal that batch whitening (BW) based methods do not impose whitening constraints on the embedding, but they only require the embedding to be full-rank. This full-rank constraint is also sufficient to avoid dimensional collapse. Based on our analysis, we propose channel whitening with random group partition (CW-RGP), which exploits the advantages of BW-based methods in preventing collapse and avoids their disadvantages requiring large batch size. Experimental results on ImageNet classification and COCO object detection reveal that the proposed CW-RGP possesses a promising potential for learning good representations. The code is available at https://github.com/winci-ai/CW-RGP.

</p>
</details>

<details><summary><b>Private and Efficient Meta-Learning with Low Rank and Sparse Decomposition</b>
<a href="https://arxiv.org/abs/2210.03505">arxiv:2210.03505</a>
&#x1F4C8; 7 <br>
<p>Soumyabrata Pal, Prateek Varshney, Prateek Jain, Abhradeep Guha Thakurta, Gagan Madan, Gaurav Aggarwal, Pradeep Shenoy, Gaurav Srivastava</p></summary>
<p>

**Abstract:** Meta-learning is critical for a variety of practical ML systems -- like personalized recommendations systems -- that are required to generalize to new tasks despite a small number of task-specific training points. Existing meta-learning techniques use two complementary approaches of either learning a low-dimensional representation of points for all tasks, or task-specific fine-tuning of a global model trained using all the tasks. In this work, we propose a novel meta-learning framework that combines both the techniques to enable handling of a large number of data-starved tasks. Our framework models network weights as a sum of low-rank and sparse matrices. This allows us to capture information from multiple domains together in the low-rank part while still allowing task specific personalization using the sparse part. We instantiate and study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with hard thresholding -- AMHT-LRS -- to learn the low-rank and sparse part effectively and efficiently. For the realizable, Gaussian data setting, we show that AMHT-LRS indeed solves the problem efficiently with nearly optimal samples. We extend AMHT-LRS to ensure that it preserves privacy of each individual user in the dataset, while still ensuring strong generalization with nearly optimal number of samples. Finally, on multiple datasets, we demonstrate that the framework allows personalized models to obtain superior performance in the data-scarce regime.

</p>
</details>

<details><summary><b>ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints</b>
<a href="https://arxiv.org/abs/2210.03895">arxiv:2210.03895</a>
&#x1F4C8; 6 <br>
<p>Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu</p></summary>
<p>

**Abstract:** Recent studies have demonstrated that visual recognition models lack robustness to distribution shift. However, current work mainly considers model robustness to 2D image transformations, leaving viewpoint changes in the 3D world less explored. In general, viewpoint changes are prevalent in various real-world applications (e.g., autonomous driving), making it imperative to evaluate viewpoint robustness. In this paper, we propose a novel method called ViewFool to find adversarial viewpoints that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations. Experiments validate that the common image classifiers are extremely vulnerable to the generated adversarial viewpoints, which also exhibit high cross-model transferability. Based on ViewFool, we introduce ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint robustness of image classifiers. Evaluation results on 40 classifiers with diverse architectures, objective functions, and data augmentations reveal a significant drop in model performance when tested on ImageNet-V, which provides a possibility to leverage ViewFool as an effective data augmentation strategy to improve viewpoint robustness.

</p>
</details>

<details><summary><b>TAME: Task Agnostic Continual Learning using Multiple Experts</b>
<a href="https://arxiv.org/abs/2210.03869">arxiv:2210.03869</a>
&#x1F4C8; 6 <br>
<p>Haoran Zhu, Maryam Majzoubi, Arihant Jain, Anna Choromanska</p></summary>
<p>

**Abstract:** The goal of lifelong learning is to continuously learn from non-stationary distributions, where the non-stationarity is typically imposed by a sequence of distinct tasks. Prior works have mostly considered idealistic settings, where the identity of tasks is known at least at training. In this paper we focus on a fundamentally harder, so-called task-agnostic setting where the task identities are not known and the learning machine needs to infer them from the observations. Our algorithm, which we call TAME (Task-Agnostic continual learning using Multiple Experts), automatically detects the shift in data distributions and switches between task expert networks in an online manner. At training, the strategy for switching between tasks hinges on an extremely simple observation that for each new coming task there occurs a statistically-significant deviation in the value of the loss function that marks the onset of this new task. At inference, the switching between experts is governed by the selector network that forwards the test sample to its relevant expert network. The selector network is trained on a small subset of data drawn uniformly at random. We control the growth of the task expert networks as well as selector network by employing online pruning. Our experimental results show the efficacy of our approach on benchmark continual learning data sets, outperforming the previous task-agnostic methods and even the techniques that admit task identities at both training and testing, while at the same time using a comparable model size.

</p>
</details>

<details><summary><b>Atomized Deep Learning Models</b>
<a href="https://arxiv.org/abs/2210.03728">arxiv:2210.03728</a>
&#x1F4C8; 6 <br>
<p>Yi-Lin Tuan, Zih-Yun Chiu, William Yang Wang</p></summary>
<p>

**Abstract:** Deep learning models often tackle the intra-sample structure, such as the order of words in a sentence and pixels in an image, but have not pay much attention to the inter-sample relationship. In this paper, we show that explicitly modeling the inter-sample structure to be more discretized can potentially help model's expressivity. We propose a novel method, Atom Modeling, that can discretize a continuous latent space by drawing an analogy between a data point and an atom, which is naturally spaced away from other atoms with distances depending on their intra structures. Specifically, we model each data point as an atom composed of electrons, protons, and neutrons and minimize the potential energy caused by the interatomic force among data points. Through experiments with qualitative analysis in our proposed Atom Modeling on synthetic and real datasets, we find that Atom Modeling can improve the performance by maintaining the inter-sample relation and can capture an interpretable intra-sample relation by mapping each component in a data point to electron/proton/neutron.

</p>
</details>

<details><summary><b>How Large Language Models are Transforming Machine-Paraphrased Plagiarism</b>
<a href="https://arxiv.org/abs/2210.03568">arxiv:2210.03568</a>
&#x1F4C8; 6 <br>
<p>Jan Philip Wahle, Terry Ruas, Frederic Kirstein, Bela Gipp</p></summary>
<p>

**Abstract:** The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work. However, the role of large autoregressive transformers in generating machine-paraphrased plagiarism and their detection is still developing in the literature. This work explores T5 and GPT-3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia. We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples. Our results suggest that large models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.). Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3) achieves a 66% F1-score in detecting paraphrases.

</p>
</details>

<details><summary><b>Automatic Chain of Thought Prompting in Large Language Models</b>
<a href="https://arxiv.org/abs/2210.03493">arxiv:2210.03493</a>
&#x1F4C8; 6 <br>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola</p></summary>
<p>

**Abstract:** Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot

</p>
</details>

<details><summary><b>Adversarial network training using higher-order moments in a modified Wasserstein distance</b>
<a href="https://arxiv.org/abs/2210.03354">arxiv:2210.03354</a>
&#x1F4C8; 6 <br>
<p>Oliver Serang</p></summary>
<p>

**Abstract:** Generative-adversarial networks (GANs) have been used to produce data closely resembling example data in a compressed, latent space that is close to sufficient for reconstruction in the original vector space. The Wasserstein metric has been used as an alternative to binary cross-entropy, producing more numerically stable GANs with greater mode covering behavior. Here, a generalization of the Wasserstein distance, using higher-order moments than the mean, is derived. Training a GAN with this higher-order Wasserstein metric is demonstrated to exhibit superior performance, even when adjusted for slightly higher computational cost. This is illustrated generating synthetic antibody sequences.

</p>
</details>

<details><summary><b>The Lifecycle of "Facts": A Survey of Social Bias in Knowledge Graphs</b>
<a href="https://arxiv.org/abs/2210.03353">arxiv:2210.03353</a>
&#x1F4C8; 6 <br>
<p>Angelie Kraft, Ricardo Usbeck</p></summary>
<p>

**Abstract:** Knowledge graphs are increasingly used in a plethora of downstream tasks or in the augmentation of statistical models to improve factuality. However, social biases are engraved in these representations and propagate downstream. We conducted a critical analysis of literature concerning biases at different steps of a knowledge graph lifecycle. We investigated factors introducing bias, as well as the biases that are rendered by knowledge graphs and their embedded versions afterward. Limitations of existing measurement and mitigation strategies are discussed and paths forward are proposed.

</p>
</details>

<details><summary><b>Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding</b>
<a href="https://arxiv.org/abs/2210.03915">arxiv:2210.03915</a>
&#x1F4C8; 5 <br>
<p>Haoming Jiang, Tianyu Cao, Zheng Li, Chen Luo, Xianfeng Tang, Qingyu Yin, Danqing Zhang, Rahul Goutam, Bing Yin</p></summary>
<p>

**Abstract:** E-commerce query understanding is the process of inferring the shopping intent of customers by extracting semantic meaning from their search queries. The recent progress of pre-trained masked language models (MLM) in natural language processing is extremely attractive for developing effective query understanding models. Specifically, MLM learns contextual text embedding via recovering the masked tokens in the sentences. Such a pre-training process relies on the sufficient contextual information. It is, however, less effective for search queries, which are usually short text. When applying masking to short search queries, most contextual information is lost and the intent of the search queries may be changed. To mitigate the above issues for MLM pre-training on search queries, we propose a novel pre-training task specifically designed for short text, called Extended Token Classification (ETC). Instead of masking the input text, our approach extends the input by inserting tokens via a generator network, and trains a discriminator to identify which tokens are inserted in the extended input. We conduct experiments in an E-commerce store to demonstrate the effectiveness of ETC.

</p>
</details>

<details><summary><b>Koopman Neural Forecaster for Time Series with Temporal Distribution Shifts</b>
<a href="https://arxiv.org/abs/2210.03675">arxiv:2210.03675</a>
&#x1F4C8; 5 <br>
<p>Rui Wang, Yihe Dong, Sercan Ö. Arik, Rose Yu</p></summary>
<p>

**Abstract:** Temporal distributional shifts, with underlying dynamics changing over time, frequently occur in real-world time series, and pose a fundamental challenge for deep neural networks (DNNs). In this paper, we propose a novel deep sequence model based on the Koopman theory for time series forecasting: Koopman Neural Forecaster (KNF) that leverages DNNs to learn the linear Koopman space and the coefficients of chosen measurement functions. KNF imposes appropriate inductive biases for improved robustness against distributional shifts, employing both a global operator to learn shared characteristics, and a local operator to capture changing dynamics, as well as a specially-designed feedback loop to continuously update the learnt operators over time for rapidly varying behaviors. To the best of our knowledge, this is the first time that Koopman theory is applied to real-world chaotic time series without known governing laws. We demonstrate that KNF achieves the superior performance compared to the alternatives, on multiple time series datasets that are shown to suffer from distribution shifts.

</p>
</details>

<details><summary><b>Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos</b>
<a href="https://arxiv.org/abs/2210.03659">arxiv:2210.03659</a>
&#x1F4C8; 5 <br>
<p>Boyang Zhang, SuPing Wu, Hu Cao, Kehua Ma, Pan Li, Lei Lin</p></summary>
<p>

**Abstract:** In this paper, we present a spatio-temporal tendency reasoning (STR) network for recovering human body pose and shape from videos. Previous approaches have focused on how to extend 3D human datasets and temporal-based learning to promote accuracy and temporal smoothing. Different from them, our STR aims to learn accurate and natural motion sequences in an unconstrained environment through temporal and spatial tendency and to fully excavate the spatio-temporal features of existing video data. To this end, our STR learns the representation of features in the temporal and spatial dimensions respectively, to concentrate on a more robust representation of spatio-temporal features. More specifically, for efficient temporal modeling, we first propose a temporal tendency reasoning (TTR) module. TTR constructs a time-dimensional hierarchical residual connection representation within a video sequence to effectively reason temporal sequences' tendencies and retain effective dissemination of human information. Meanwhile, for enhancing the spatial representation, we design a spatial tendency enhancing (STE) module to further learns to excite spatially time-frequency domain sensitive features in human motion information representations. Finally, we introduce integration strategies to integrate and refine the spatio-temporal feature representations. Extensive experimental findings on large-scale publically available datasets reveal that our STR remains competitive with the state-of-the-art on three datasets. Our code are available at https://github.com/Changboyang/STR.git.

</p>
</details>

<details><summary><b>A deep learning approach for detection and localization of leaf anomalies</b>
<a href="https://arxiv.org/abs/2210.03558">arxiv:2210.03558</a>
&#x1F4C8; 5 <br>
<p>Davide Calabrò, Massimiliano Lupo Pasini, Nicola Ferro, Simona Perotto</p></summary>
<p>

**Abstract:** The detection and localization of possible diseases in crops are usually automated by resorting to supervised deep learning approaches. In this work, we tackle these goals with unsupervised models, by applying three different types of autoencoders to a specific open-source dataset of healthy and unhealthy pepper and cherry leaf images. CAE, CVAE and VQ-VAE autoencoders are deployed to screen unlabeled images of such a dataset, and compared in terms of image reconstruction, anomaly removal, detection and localization. The vector-quantized variational architecture turns out to be the best performing one with respect to all these targets.

</p>
</details>

<details><summary><b>Quantifying Political Bias in News Articles</b>
<a href="https://arxiv.org/abs/2210.03404">arxiv:2210.03404</a>
&#x1F4C8; 5 <br>
<p>Gizem Gezici</p></summary>
<p>

**Abstract:** Search bias analysis is getting more attention in recent years since search results could affect In this work, we aim to establish an automated model for evaluating ideological bias in online news articles. The dataset is composed of news articles in search results as well as the newspaper articles. The current automated model results show that model capability is not sufficient to be exploited for annotating the documents automatically, thereby computing bias in search results.

</p>
</details>

<details><summary><b>Temporal Feature Alignment in Contrastive Self-Supervised Learning for Human Activity Recognition</b>
<a href="https://arxiv.org/abs/2210.03382">arxiv:2210.03382</a>
&#x1F4C8; 5 <br>
<p>Bulat Khaertdinov, Stylianos Asteriadis</p></summary>
<p>

**Abstract:** Automated Human Activity Recognition has long been a problem of great interest in human-centered and ubiquitous computing. In the last years, a plethora of supervised learning algorithms based on deep neural networks has been suggested to address this problem using various modalities. While every modality has its own limitations, there is one common challenge. Namely, supervised learning requires vast amounts of annotated data which is practically hard to collect. In this paper, we benefit from the self-supervised learning paradigm (SSL) that is typically used to learn deep feature representations from unlabeled data. Moreover, we upgrade a contrastive SSL framework, namely SimCLR, widely used in various applications by introducing a temporal feature alignment procedure for Human Activity Recognition. Specifically, we propose integrating a dynamic time warping (DTW) algorithm in a latent space to force features to be aligned in a temporal dimension. Extensive experiments have been conducted for the unimodal scenario with inertial modality as well as in multimodal settings using inertial and skeleton data. According to the obtained results, the proposed approach has a great potential in learning robust feature representations compared to the recent SSL baselines, and clearly outperforms supervised models in semi-supervised learning. The code for the unimodal case is available via the following link: https://github.com/bulatkh/csshar_tfa.

</p>
</details>

<details><summary><b>UU-Tax at SemEval-2022 Task 3: Improving the generalizability of language models for taxonomy classification through data augmentation</b>
<a href="https://arxiv.org/abs/2210.03378">arxiv:2210.03378</a>
&#x1F4C8; 5 <br>
<p>Injy Sarhan, Pablo Mosteiro, Marco Spruit</p></summary>
<p>

**Abstract:** This paper presents our strategy to address the SemEval-2022 Task 3 PreTENS: Presupposed Taxonomies Evaluating Neural Network Semantics. The goal of the task is to identify if a sentence is deemed acceptable or not, depending on the taxonomic relationship that holds between a noun pair contained in the sentence. For sub-task 1 -- binary classification -- we propose an effective way to enhance the robustness and the generalizability of language models for better classification on this downstream task. We design a two-stage fine-tuning procedure on the ELECTRA language model using data augmentation techniques. Rigorous experiments are carried out using multi-task learning and data-enriched fine-tuning. Experimental results demonstrate that our proposed model, UU-Tax, is indeed able to generalize well for our downstream task. For sub-task 2 -- regression -- we propose a simple classifier that trains on features obtained from Universal Sentence Encoder (USE). In addition to describing the submitted systems, we discuss other experiments that employ pre-trained language models and data augmentation techniques. For both sub-tasks, we perform error analysis to further understand the behaviour of the proposed models. We achieved a global F1_Binary score of 91.25% in sub-task 1 and a rho score of 0.221 in sub-task 2.

</p>
</details>

<details><summary><b>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review</b>
<a href="https://arxiv.org/abs/2210.03829">arxiv:2210.03829</a>
&#x1F4C8; 4 <br>
<p>Seyed Mojtaba Marvasti-Zadeh, Devin Goodsman, Nilanjan Ray, Nadir Erbilgin</p></summary>
<p>

**Abstract:** Bark beetle outbreaks can result in a devastating impact on forest ecosystem processes, biodiversity, forest structure and function, and economies. Accurate and timely detection of bark beetle infestations is crucial to mitigate further damage, develop proactive forest management activities, and minimize economic losses. Incorporating remote sensing (RS) data with machine learning (ML) (or deep learning (DL)) can provide a great alternative to the current approaches that rely on aerial surveys and field surveys, which are impractical over vast geographical regions. This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three key perspectives: bark beetle & host interactions, RS, and ML/DL. We parse recent literature according to bark beetle species & attack phases, host trees, study regions, imagery platforms & sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks & architectures. This review focuses on challenging early detection, discussing current challenges and potential solutions. Our literature survey suggests that the performance of current ML methods is limited (less than 80%) and depends on various factors, including imagery sensors & resolutions, acquisition dates, and employed features & algorithms/networks. A more promising result from DL networks and then the random forest (RF) algorithm highlighted the potential to detect subtle changes in visible, thermal, and short-wave infrared (SWIR) spectral regions.

</p>
</details>

<details><summary><b>Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative</b>
<a href="https://arxiv.org/abs/2210.03801">arxiv:2210.03801</a>
&#x1F4C8; 4 <br>
<p>Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, Zhangyang Wang</p></summary>
<p>

**Abstract:** This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as HyperGCL). We focus on the following question: How to construct contrastive views for hypergraphs via augmentations? We provide the solutions in two folds. First, guided by domain knowledge, we fabricate two schemes to augment hyperedges with higher-order relations encoded, and adopt three vertex augmentation strategies from graph-structured data. Second, in search of more effective views in a data-driven manner, we for the first time propose a hypergraph generative model to generate augmented views, and then an end-to-end differentiable pipeline to jointly learn hypergraph augmentations and model parameters. Our technical innovations are reflected in designing both fabricated and generative augmentations of hypergraphs. The experimental findings include: (i) Among fabricated augmentations in HyperGCL, augmenting hyperedges provides the most numerical gains, implying that higher-order information in structures is usually more downstream-relevant; (ii) Generative augmentations do better in preserving higher-order information to further benefit generalizability; (iii) HyperGCL also boosts robustness and fairness in hypergraph representation learning. Codes are released at https://github.com/weitianxin/HyperGCL.

</p>
</details>

<details><summary><b>Trustworthiness of Laser-Induced Breakdown Spectroscopy Predictions via Simulation-based Synthetic Data Augmentation and Multitask Learning</b>
<a href="https://arxiv.org/abs/2210.03762">arxiv:2210.03762</a>
&#x1F4C8; 4 <br>
<p>Riccardo Finotello, Daniel L'Hermite, Celine Quéré, Benjamin Rouge, Mohamed Tamaazousti, Jean-Baptiste Sirven</p></summary>
<p>

**Abstract:** We consider quantitative analyses of spectral data using laser-induced breakdown spectroscopy. We address the small size of training data available, and the validation of the predictions during inference on unknown data. For the purpose, we build robust calibration models using deep convolutional multitask learning architectures to predict the concentration of the analyte, alongside additional spectral information as auxiliary outputs. These secondary predictions can be used to validate the trustworthiness of the model by taking advantage of the mutual dependencies of the parameters of the multitask neural networks. Due to the experimental lack of training samples, we introduce a simulation-based data augmentation process to synthesise an arbitrary number of spectra, statistically representative of the experimental data. Given the nature of the deep learning model, no dimensionality reduction or data selection processes are required. The procedure is an end-to-end pipeline including the process of synthetic data augmentation, the construction of a suitable robust, homoscedastic, deep learning model, and the validation of its predictions. In the article, we compare the performance of the multitask model with traditional univariate and multivariate analyses, to highlight the separate contributions of each element introduced in the process.

</p>
</details>

<details><summary><b>Class-wise and reduced calibration methods</b>
<a href="https://arxiv.org/abs/2210.03702">arxiv:2210.03702</a>
&#x1F4C8; 4 <br>
<p>Michael Panchenko, Anes Benmerzoug, Miguel de Benito Delgado</p></summary>
<p>

**Abstract:** For many applications of probabilistic classifiers it is important that the predicted confidence vectors reflect true probabilities (one says that the classifier is calibrated). It has been shown that common models fail to satisfy this property, making reliable methods for measuring and improving calibration important tools. Unfortunately, obtaining these is far from trivial for problems with many classes. We propose two techniques that can be used in tandem. First, a reduced calibration method transforms the original problem into a simpler one. We prove for several notions of calibration that solving the reduced problem minimizes the corresponding notion of miscalibration in the full problem, allowing the use of non-parametric recalibration methods that fail in higher dimensions. Second, we propose class-wise calibration methods, based on intuition building on a phenomenon called neural collapse and the observation that most of the accurate classifiers found in practice can be thought of as a union of K different functions which can be recalibrated separately, one for each class. These typically out-perform their non class-wise counterparts, especially for classifiers trained on imbalanced data sets. Applying the two methods together results in class-wise reduced calibration algorithms, which are powerful tools for reducing the prediction and per-class calibration errors. We demonstrate our methods on real and synthetic datasets and release all code as open source at https://github.com/appliedAI-Initiative

</p>
</details>

<details><summary><b>Label Propagation with Weak Supervision</b>
<a href="https://arxiv.org/abs/2210.03594">arxiv:2210.03594</a>
&#x1F4C8; 4 <br>
<p>Rattana Pukdee, Dylan Sam, Maria-Florina Balcan, Pradeep Ravikumar</p></summary>
<p>

**Abstract:** Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.

</p>
</details>

<details><summary><b>Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering</b>
<a href="https://arxiv.org/abs/2210.03427">arxiv:2210.03427</a>
&#x1F4C8; 4 <br>
<p>Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez</p></summary>
<p>

**Abstract:** Quality management and assurance is key for space agencies to guarantee the success of space missions, which are high-risk and extremely costly. In this paper, we present a system to generate quizzes, a common resource to evaluate the effectiveness of training sessions, from documents about quality assurance procedures in the Space domain. Our system leverages state of the art auto-regressive models like T5 and BART to generate questions, and a RoBERTa model to extract answers for such questions, thus verifying their suitability.

</p>
</details>

<details><summary><b>Event Extraction: A Survey</b>
<a href="https://arxiv.org/abs/2210.03419">arxiv:2210.03419</a>
&#x1F4C8; 4 <br>
<p>Viet Dac Lai</p></summary>
<p>

**Abstract:** Extracting the reported events from text is one of the key research themes in natural language processing. This process includes several tasks such as event detection, argument extraction, role labeling. As one of the most important topics in natural language processing and natural language understanding, the applications of event extraction spans across a wide range of domains such as newswire, biomedical domain, history and humanity, and cyber security. This report presents a comprehensive survey for event detection from textual documents. In this report, we provide the task definition, the evaluation method, as well as the benchmark datasets and a taxonomy of methodologies for event extraction. We also present our vision of future research direction in event detection.

</p>
</details>

<details><summary><b>Zero-shot stance detection based on cross-domain feature enhancement by contrastive learning</b>
<a href="https://arxiv.org/abs/2210.03380">arxiv:2210.03380</a>
&#x1F4C8; 4 <br>
<p>Xuechen Zhao, Jiaying Zou, Zhong Zhang, Feng Xie, Bin Zhou, Lei Tian</p></summary>
<p>

**Abstract:** Zero-shot stance detection is challenging because it requires detecting the stance of previously unseen targets in the inference phase. The ability to learn transferable target-invariant features is critical for zero-shot stance detection. In this work, we propose a stance detection approach that can efficiently adapt to unseen targets, the core of which is to capture target-invariant syntactic expression patterns as transferable knowledge. Specifically, we first augment the data by masking the topic words of sentences, and then feed the augmented data to an unsupervised contrastive learning module to capture transferable features. Then, to fit a specific target, we encode the raw texts as target-specific features. Finally, we adopt an attention mechanism, which combines syntactic expression patterns with target-specific features to obtain enhanced features for predicting previously unseen targets. Experiments demonstrate that our model outperforms competitive baselines on four benchmark datasets.

</p>
</details>

<details><summary><b>A Unified Framework for Multi-intent Spoken Language Understanding with prompting</b>
<a href="https://arxiv.org/abs/2210.03337">arxiv:2210.03337</a>
&#x1F4C8; 4 <br>
<p>Feifan Song, Lianzhe Huang, Houfeng Wang</p></summary>
<p>

**Abstract:** Multi-intent Spoken Language Understanding has great potential for widespread implementation. Jointly modeling Intent Detection and Slot Filling in it provides a channel to exploit the correlation between intents and slots. However, current approaches are apt to formulate these two sub-tasks differently, which leads to two issues: 1) It hinders models from effective extraction of shared features. 2) Pretty complicated structures are involved to enhance expression ability while causing damage to the interpretability of frameworks. In this work, we describe a Prompt-based Spoken Language Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into the same form by offering a common pre-trained Seq2Seq model. In detail, ID and SF are completed by concisely filling the utterance into task-specific prompt templates as input, and sharing output formats of key-value pairs sequence. Furthermore, variable intents are predicted first, then naturally embedded into prompts to guide slot-value pairs inference from a semantic perspective. Finally, we are inspired by prevalent multi-task learning to introduce an auxiliary sub-task, which helps to learn relationships among provided labels. Experiment results show that our framework outperforms several state-of-the-art baselines on two public datasets.

</p>
</details>

<details><summary><b>Explainable AI based Glaucoma Detection using Transfer Learning and LIME</b>
<a href="https://arxiv.org/abs/2210.03332">arxiv:2210.03332</a>
&#x1F4C8; 4 <br>
<p>Touhidul Islam Chayan, Anita Islam, Eftykhar Rahman, Md. Tanzim Reza, Tasnim Sakib Apon, MD. Golam Rabiul Alam</p></summary>
<p>

**Abstract:** Glaucoma is the second driving reason for partial or complete blindness among all the visual deficiencies which mainly occurs because of excessive pressure in the eye due to anxiety or depression which damages the optic nerve and creates complications in vision. Traditional glaucoma screening is a time-consuming process that necessitates the medical professionals' constant attention, and even so time to time due to the time constrains and pressure they fail to classify correctly that leads to wrong treatment. Numerous efforts have been made to automate the entire glaucoma classification procedure however, these existing models in general have a black box characteristics that prevents users from understanding the key reasons behind the prediction and thus medical practitioners generally can not rely on these system. In this article after comparing with various pre-trained models, we propose a transfer learning model that is able to classify Glaucoma with 94.71\% accuracy. In addition, we have utilized Local Interpretable Model-Agnostic Explanations(LIME) that introduces explainability in our system. This improvement enables medical professionals obtain important and comprehensive information that aid them in making judgments. It also lessen the opacity and fragility of the traditional deep learning models.

</p>
</details>

<details><summary><b>Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules</b>
<a href="https://arxiv.org/abs/2210.06184">arxiv:2210.06184</a>
&#x1F4C8; 3 <br>
<p>Kazuki Irie, Jürgen Schmidhuber</p></summary>
<p>

**Abstract:** Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step "denoising" of FPA-generated images to enhance their quality. Our code is public.

</p>
</details>

<details><summary><b>Learning the Network of Graphs for Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2210.03907">arxiv:2210.03907</a>
&#x1F4C8; 3 <br>
<p>Yixiang Shan, Jielong Yang, Xing Liu, Yixing Gao, Hechang Chen, Shuzhi Sam Ge</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) have achieved great success in many scenarios with graph-structured data. However, in many real applications, there are three issues when applying GNNs: graphs are unknown, nodes have noisy features, and graphs contain noisy connections. Aiming at solving these problems, we propose a new graph neural network named as GL-GNN. Our model includes multiple sub-modules, each sub-module selects important data features and learn the corresponding key relation graph of data samples when graphs are unknown. GL-GNN further obtains the network of graphs by learning the network of sub-modules. The learned graphs are further fused using an aggregation method over the network of graphs. Our model solves the first issue by simultaneously learning multiple relation graphs of data samples as well as a relation network of graphs, and solves the second and the third issue by selecting important data features as well as important data sample relations. We compare our method with 14 baseline methods on seven datasets when the graph is unknown and 11 baseline methods on two datasets when the graph is known. The results show that our method achieves better accuracies than the baseline methods and is capable of selecting important features and graph edges from the dataset. Our code will be publicly available at \url{https://github.com/Looomo/GL-GNN}.

</p>
</details>

<details><summary><b>Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts</b>
<a href="https://arxiv.org/abs/2210.03885">arxiv:2210.03885</a>
&#x1F4C8; 3 <br>
<p>Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, Jin Tang</p></summary>
<p>

**Abstract:** In this paper, we tackle the problem of domain shift. Most existing methods perform training on multiple source domains using a single model, and the same trained model is used on all unseen target domains. Such solutions are sub-optimal as each target domain exhibits its own speciality, which is not adapted. Furthermore, expecting the single-model training to learn extensive knowledge from the multiple source domains is counterintuitive. The model is more biased toward learning only domain-invariant features and may result in negative knowledge transfer. In this work, we propose a novel framework for unsupervised test-time adaptation, which is formulated as a knowledge distillation process to address domain shift. Specifically, we incorporate Mixture-of-Experts (MoE) as teachers, where each expert is separately trained on different source domains to maximize their speciality. Given a test-time target domain, a small set of unlabeled data is sampled to query the knowledge from MoE. As the source domains are correlated to the target domains, a transformer-based aggregator then combines the domain knowledge by examining the interconnection among them. The output is treated as a supervision signal to adapt a student prediction network toward the target domain. We further employ meta-learning to enforce the aggregator to distill positive knowledge and the student network to achieve fast adaptation. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art and validates the effectiveness of each proposed component. Our code is available at https://github.com/n3il666/Meta-DMoE.

</p>
</details>

<details><summary><b>Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model</b>
<a href="https://arxiv.org/abs/2210.03859">arxiv:2210.03859</a>
&#x1F4C8; 3 <br>
<p>Hua Li, Wenya Luo, Zhidong Bai, Huanchao Zhou, Zhangni Pu</p></summary>
<p>

**Abstract:** In this paper, we propose an improved linear discriminant analysis, called spectrally-corrected and regularized linear discriminant analysis (SCRLDA). This method integrates the design ideas of the sample spectrally-corrected covariance matrix and the regularized discriminant analysis. The SCRLDA method is specially designed for classification problems under the assumption that the covariance matrix follows a spiked model. Through the real and simulated data analysis, it is shown that our proposed classifier outperforms the classical R-LDA and can be as competitive as the KNN, SVM classifiers while requiring lower computational complexity.

</p>
</details>

<details><summary><b>Is margin all you need? An extensive empirical study of active learning on tabular data</b>
<a href="https://arxiv.org/abs/2210.03822">arxiv:2210.03822</a>
&#x1F4C8; 3 <br>
<p>Dara Bahri, Heinrich Jiang, Tal Schuster, Afshin Rostamizadeh</p></summary>
<p>

**Abstract:** Given a labeled training set and a collection of unlabeled data, the goal of active learning (AL) is to identify the best unlabeled points to label. In this comprehensive study, we analyze the performance of a variety of AL algorithms on deep neural networks trained on 69 real-world tabular classification datasets from the OpenML-CC18 benchmark. We consider different data regimes and the effect of self-supervised model pre-training. Surprisingly, we find that the classical margin sampling technique matches or outperforms all others, including current state-of-art, in a wide range of experimental settings. To researchers, we hope to encourage rigorous benchmarking against margin, and to practitioners facing tabular data labeling constraints that hyper-parameter-free margin may often be all they need.

</p>
</details>

<details><summary><b>xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph</b>
<a href="https://arxiv.org/abs/2210.03768">arxiv:2210.03768</a>
&#x1F4C8; 3 <br>
<p>Arif Usta, Akifhan Karakayali, Özgür Ulusoy</p></summary>
<p>

**Abstract:** Translating natural language queries (NLQ) into structured query language (SQL) in interfaces to relational databases is a challenging task that has been widely studied by researchers from both the database and natural language processing communities. Numerous works have been proposed to attack the natural language interfaces to databases (NLIDB) problem either as a conventional pipeline-based or an end-to-end deep-learning-based solution. Nevertheless, regardless of the approach preferred, such solutions exhibit black-box nature, which makes it difficult for potential users targeted by these systems to comprehend the decisions made to produce the translated SQL. To this end, we propose xDBTagger, an explainable hybrid translation pipeline that explains the decisions made along the way to the user both textually and visually. We also evaluate xDBTagger quantitatively in three real-world relational databases. The evaluation results indicate that in addition to being fully interpretable, xDBTagger is effective in terms of accuracy and translates the queries more efficiently compared to other state-of-the-art pipeline-based systems up to 10000 times.

</p>
</details>

<details><summary><b>Visualize Before You Write: Imagination-Guided Open-Ended Text Generation</b>
<a href="https://arxiv.org/abs/2210.03765">arxiv:2210.03765</a>
&#x1F4C8; 3 <br>
<p>Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang</p></summary>
<p>

**Abstract:** Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in few-shot scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration.

</p>
</details>

<details><summary><b>ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction</b>
<a href="https://arxiv.org/abs/2210.03745">arxiv:2210.03745</a>
&#x1F4C8; 3 <br>
<p>Dawid Rymarczyk, Daniel Dobrowolski, Tomasz Danel</p></summary>
<p>

**Abstract:** In this work, we propose the novel Prototypical Graph Regression Self-explainable Trees (ProGReST) model, which combines prototype learning, soft decision trees, and Graph Neural Networks. In contrast to other works, our model can be used to address various challenging tasks, including compound property prediction. In ProGReST, the rationale is obtained along with prediction due to the model's built-in interpretability. Additionally, we introduce a new graph prototype projection to accelerate model training. Finally, we evaluate PRoGReST on a wide range of chemical datasets for molecular property prediction and perform in-depth analysis with chemical experts to evaluate obtained interpretations. Our method achieves competitive results against state-of-the-art methods.

</p>
</details>

<details><summary><b>Knowledge-Grounded Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2210.03729">arxiv:2210.03729</a>
&#x1F4C8; 3 <br>
<p>Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, Michael C. Yip</p></summary>
<p>

**Abstract:** Receiving knowledge, abiding by laws, and being aware of regulations are common behaviors in human society. Bearing in mind that reinforcement learning (RL) algorithms benefit from mimicking humanity, in this work, we propose that an RL agent can act on external guidance in both its learning process and model deployment, making the agent more socially acceptable. We introduce the concept, Knowledge-Grounded RL (KGRL), with a formal definition that an agent learns to follow external guidelines and develop its own policy. Moving towards the goal of KGRL, we propose a novel actor model with an embedding-based attention mechanism that can attend to either a learnable internal policy or external knowledge. The proposed method is orthogonal to training algorithms, and the external knowledge can be flexibly recomposed, rearranged, and reused in both training and inference stages. Through experiments on tasks with discrete and continuous action space, our KGRL agent is shown to be more sample efficient and generalizable, and it has flexibly rearrangeable knowledge embeddings and interpretable behaviors.

</p>
</details>

<details><summary><b>Understanding Practices, Challenges, and Opportunities for User-Driven Algorithm Auditing in Industry Practice</b>
<a href="https://arxiv.org/abs/2210.03709">arxiv:2210.03709</a>
&#x1F4C8; 3 <br>
<p>Wesley Hanwen Deng, Bill Boyuan Guo, Alicia DeVrio, Hong Shen, Motahhare Eslami, Kenneth Holstein</p></summary>
<p>

**Abstract:** Recent years have seen growing interest among both researchers and practitioners in user-driven approaches to algorithm auditing, which directly engage users in detecting problematic behaviors in algorithmic systems. However, we know little about industry practitioners' current practices and challenges around user-driven auditing, nor what opportunities exist for them to better leverage such approaches in practice. To investigate, we conducted a series of interviews and iterative co-design activities with practitioners who employ user-driven auditing approaches in their work. Our findings reveal several challenges practitioners face in appropriately recruiting and incentivizing user auditors, scaffolding user audits, and deriving actionable insights from user-driven audit reports. Furthermore, practitioners shared organizational obstacles to user-driven auditing, surfacing a complex relationship between practitioners and user auditors. Based on these findings, we discuss opportunities for future HCI research to help realize the potential (and mitigate risks) of user-driven auditing in industry practice.

</p>
</details>

<details><summary><b>NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems</b>
<a href="https://arxiv.org/abs/2210.03696">arxiv:2210.03696</a>
&#x1F4C8; 3 <br>
<p>Simin Chen, Cong Liu, Mirazul Haque, Zihe Song, Wei Yang</p></summary>
<p>

**Abstract:** Neural Machine Translation (NMT) systems have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of NMT systems, which is of paramount importance due to often vast translation demands and real-time requirements, has surprisingly received little attention. In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art NMT systems. By analyzing the working mechanism and implementation of 1455 public-accessible NMT systems, we observe a fundamental property in NMT systems that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that NMT systems would have to go through enough iterations to satisfy the pre-configured threshold. We present NMTSloth, which develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level, which sufficiently delays the appearance of EOS and forces these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of NMTSloth, we conduct a systematic evaluation on three public-available NMT systems: Google T5, AllenAI WMT14, and Helsinki-NLP translators. Experimental results show that NMTSloth can increase NMT systems' response latency and energy consumption by 85% to 3153% and 86% to 3052%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by NMTSloth significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).

</p>
</details>

<details><summary><b>CommsVAE: Learning the brain's macroscale communication dynamics using coupled sequential VAEs</b>
<a href="https://arxiv.org/abs/2210.03667">arxiv:2210.03667</a>
&#x1F4C8; 3 <br>
<p>Eloy Geenjaar, Noah Lewis, Amrit Kashyap, Robyn Miller, Vince Calhoun</p></summary>
<p>

**Abstract:** Communication within or between complex systems is commonplace in the natural sciences and fields such as graph neural networks. The brain is a perfect example of such a complex system, where communication between brain regions is constantly being orchestrated. To analyze communication, the brain is often split up into anatomical regions that each perform certain computations. These regions must interact and communicate with each other to perform tasks and support higher-level cognition. On a macroscale, these regions communicate through signal propagation along the cortex and along white matter tracts over longer distances. When and what types of signals are communicated over time is an unsolved problem and is often studied using either functional or structural data. In this paper, we propose a non-linear generative approach to communication from functional data. We address three issues with common connectivity approaches by explicitly modeling the directionality of communication, finding communication at each timestep, and encouraging sparsity. To evaluate our model, we simulate temporal data that has sparse communication between nodes embedded in it and show that our model can uncover the expected communication dynamics. Subsequently, we apply our model to temporal neural data from multiple tasks and show that our approach models communication that is more specific to each task. The specificity of our method means it can have an impact on the understanding of psychiatric disorders, which are believed to be related to highly specific communication between brain regions compared to controls. In sum, we propose a general model for dynamic communication learning on graphs, and show its applicability to a subfield of the natural sciences, with potential widespread scientific impact.

</p>
</details>

<details><summary><b>Understanding the Covariance Structure of Convolutional Filters</b>
<a href="https://arxiv.org/abs/2210.03651">arxiv:2210.03651</a>
&#x1F4C8; 3 <br>
<p>Asher Trockman, Devin Willmott, J. Zico Kolter</p></summary>
<p>

**Abstract:** Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all.

</p>
</details>

<details><summary><b>GraspCaps: Capsule Networks Are All You Need for Grasping Familiar Objects</b>
<a href="https://arxiv.org/abs/2210.03628">arxiv:2210.03628</a>
&#x1F4C8; 3 <br>
<p>Tomas van der Velde, Hamidreza Kasaei</p></summary>
<p>

**Abstract:** As robots become more accessible outside of industrial settings, the need for reliable object grasping and manipulation grows significantly. In such dynamic environments it is expected that the robot is capable of reliably grasping and manipulating novel objects in different situations. In this work we present GraspCaps: a novel architecture based on Capsule Networks for generating per-point grasp configurations for familiar objects. In our work, the activation vector of each capsule in the deepest capsule layer corresponds to one specific class of object. This way, the network is able to extract a rich feature vector of the objects present in the point cloud input, which is then used for generating per-point grasp vectors. This approach should allow the network to learn specific grasping strategies for each of the different object categories. Along with GraspCaps we present a method for generating a large object grasping dataset using simulated annealing. The obtained dataset is then used to train the GraspCaps network. We performed an extensive set of experiments to assess the performance of the proposed approach regarding familiar object recognition accuracy and grasp success rate on challenging real and simulated scenarios.

</p>
</details>

<details><summary><b>Pose Guided Human Image Synthesis with Partially Decoupled GAN</b>
<a href="https://arxiv.org/abs/2210.03627">arxiv:2210.03627</a>
&#x1F4C8; 3 <br>
<p>Jianhan Wu, Jianzong Wang, Shijing Si, Xiaoyang Qu, Jing Xiao</p></summary>
<p>

**Abstract:** Pose Guided Human Image Synthesis (PGHIS) is a challenging task of transforming a human image from the reference pose to a target pose while preserving its style. Most existing methods encode the texture of the whole reference human image into a latent space, and then utilize a decoder to synthesize the image texture of the target pose. However, it is difficult to recover the detailed texture of the whole human image. To alleviate this problem, we propose a method by decoupling the human body into several parts (\eg, hair, face, hands, feet, \etc) and then using each of these parts to guide the synthesis of a realistic image of the person, which preserves the detailed information of the generated images. In addition, we design a multi-head attention-based module for PGHIS. Because most convolutional neural network-based methods have difficulty in modeling long-range dependency due to the convolutional operation, the long-range modeling capability of attention mechanism is more suitable than convolutional neural networks for pose transfer task, especially for sharp pose deformation. Extensive experiments on Market-1501 and DeepFashion datasets reveal that our method almost outperforms other existing state-of-the-art methods in terms of both qualitative and quantitative metrics.

</p>
</details>

<details><summary><b>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</b>
<a href="https://arxiv.org/abs/2210.03625">arxiv:2210.03625</a>
&#x1F4C8; 3 <br>
<p>Andrew Rouditchenko, Yung-Sung Chuang, Nina Shvetsova, Samuel Thomas, Rogerio Feris, Brian Kingsbury, Leonid Karlinsky, David Harwath, Hilde Kuehne, James Glass</p></summary>
<p>

**Abstract:** Multilingual text-video retrieval methods have improved significantly in recent years, but the performance for other languages lags behind English. We propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve multilingual text-video retrieval. Inspired by the fact that English text-video retrieval outperforms other languages, we train a student model using input text in different languages to match the cross-modal predictions from teacher models using input text in English. We propose a cross entropy based objective which forces the distribution over the student's text-video similarity scores to be similar to those of the teacher models. We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages. Our method improves multilingual text-video retrieval performance on Multi-YouCook2 and several other datasets such as Multi-MSRVTT and VATEX. We also conducted an analysis on the effectiveness of different multilingual text models as teachers.

</p>
</details>

<details><summary><b>Machine Learning Meets The Herbrand Universe</b>
<a href="https://arxiv.org/abs/2210.03590">arxiv:2210.03590</a>
&#x1F4C8; 3 <br>
<p>Jelle Piepenbrock, Josef Urban, Konstantin Korovin, Miroslav Olšák, Tom Heskes, Mikolaš Janota</p></summary>
<p>

**Abstract:** The appearance of strong CDCL-based propositional (SAT) solvers has greatly advanced several areas of automated reasoning (AR). One of the directions in AR is thus to apply SAT solvers to expressive formalisms such as first-order logic, for which large corpora of general mathematical problems exist today. This is possible due to Herbrand's theorem, which allows reduction of first-order problems to propositional problems by instantiation. The core challenge is choosing the right instances from the typically infinite Herbrand universe. In this work, we develop the first machine learning system targeting this task, addressing its combinatorial and invariance properties. In particular, we develop a GNN2RNN architecture based on an invariant graph neural network (GNN) that learns from problems and their solutions independently of symbol names (addressing the abundance of skolems), combined with a recurrent neural network (RNN) that proposes for each clause its instantiations. The architecture is then trained on a corpus of mathematical problems and their instantiation-based proofs, and its performance is evaluated in several ways. We show that the trained system achieves high accuracy in predicting the right instances, and that it is capable of solving many problems by educated guessing when combined with a ground solver. To our knowledge, this is the first convincing use of machine learning in synthesizing relevant elements from arbitrary Herbrand universes.

</p>
</details>

<details><summary><b>Learning to Learn and Sample BRDFs</b>
<a href="https://arxiv.org/abs/2210.03510">arxiv:2210.03510</a>
&#x1F4C8; 3 <br>
<p>Chen Liu, Michael Fischer, Tobias Ritschel</p></summary>
<p>

**Abstract:** We propose a method to accelerate the joint process of physically acquiring and learning neural Bi-directional Reflectance Distribution Function (BRDF) models. While BRDF learning alone can be accelerated by meta-learning, acquisition remains slow as it relies on a mechanical process. We show that meta-learning can be extended to optimize the physical sampling pattern, too. After our method has been meta-trained for a set of fully-sampled BRDFs, it is able to quickly train on new BRDFs with up to five orders of magnitude fewer physical acquisition samples at similar quality. Our approach also extends to other linear and non-linear BRDF models, which we show in an extensive evaluation.

</p>
</details>

<details><summary><b>What Do End-Users Really Want? Investigation of Human-Centered XAI for Mobile Health Apps</b>
<a href="https://arxiv.org/abs/2210.03506">arxiv:2210.03506</a>
&#x1F4C8; 3 <br>
<p>Katharina Weitz, Alexander Zellner, Elisabeth André</p></summary>
<p>

**Abstract:** In healthcare, AI systems support clinicians and patients in diagnosis, treatment, and monitoring, but many systems' poor explainability remains challenging for practical application. Overcoming this barrier is the goal of explainable AI (XAI). However, an explanation can be perceived differently and, thus, not solve the black-box problem for everyone. The domain of Human-Centered AI deals with this problem by adapting AI to users. We present a user-centered persona concept to evaluate XAI and use it to investigate end-users preferences for various explanation styles and contents in a mobile health stress monitoring application. The results of our online survey show that users' demographics and personality, as well as the type of explanation, impact explanation preferences, indicating that these are essential features for XAI design. We subsumed the results in three prototypical user personas: power-, casual-, and privacy-oriented users. Our insights bring an interactive, human-centered XAI closer to practical application.

</p>
</details>

<details><summary><b>FastCLIPStyler: Towards fast text-based image style transfer using style representation</b>
<a href="https://arxiv.org/abs/2210.03461">arxiv:2210.03461</a>
&#x1F4C8; 3 <br>
<p>Ananda Padhmanabhan Suresh, Sanjana Jain, Pavit Noinongyao, Ankush Ganguly</p></summary>
<p>

**Abstract:** Artistic style transfer is usually performed between two images, a style image and a content image. Recently, a model named CLIPStyler demonstrated that a natural language description of style could replace the necessity of a reference style image. They achieved this by taking advantage of the CLIP model, which can compute the similarity between a text phrase and an image. In this work, we demonstrate how combining CLIPStyler with a pre-trained, purely vision-based style transfer model can significantly reduce the inference time of CLIPStyler. We call this model FastCLIPStyler. We do a qualitative exploration of the stylised images from both models and argue that our model also has merits in terms of the visual aesthetics of the generated images. Finally, we also point out how FastCLIPStyler can be used to further extend this line of research to create a generalised text-to-style model that does not require optimisation at inference time, which both CLIPStyler and FastCLIPStyler do currently.

</p>
</details>

<details><summary><b>Monitoring MBE substrate deoxidation via RHEED image-sequence analysis by deep learning</b>
<a href="https://arxiv.org/abs/2210.03430">arxiv:2210.03430</a>
&#x1F4C8; 3 <br>
<p>Abdourahman Khaireh-Walieh, Alexandre Arnoult, Sébastien Plissard, Peter R. Wiecha</p></summary>
<p>

**Abstract:** Reflection high-energy electron diffraction (RHEED) is a powerful tool in molecular beam epitaxy (MBE), but RHEED images are often difficult to interpret, requiring experienced operators. We present an approach for automated surveillance of GaAs substrate deoxidation in MBE using deep learning based RHEED image-sequence classification. Our approach consists of an non-supervised auto-encoder (AE) for feature extraction, combined with a supervised convolutional classifier network. We demonstrate that our lightweight network model can accurately identify the exact deoxidation moment. Furthermore we show that the approach is very robust and allows accurate deoxidation detection during months without requiring re-training. The main advantage of the approach is that it can be applied to raw RHEED images without requiring further information such as the rotation angle, temperature, etc.

</p>
</details>

<details><summary><b>SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts</b>
<a href="https://arxiv.org/abs/2210.03422">arxiv:2210.03422</a>
&#x1F4C8; 3 <br>
<p>Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez, José Antonio Martínez-Heras, Alessandro Donati, Ilaria Roma</p></summary>
<p>

**Abstract:** We present SpaceQA, to the best of our knowledge the first open-domain QA system in Space mission design. SpaceQA is part of an initiative by the European Space Agency (ESA) to facilitate the access, sharing and reuse of information about Space mission design within the agency and with the public. We adopt a state-of-the-art architecture consisting of a dense retriever and a neural reader and opt for an approach based on transfer learning rather than fine-tuning due to the lack of domain-specific annotated data. Our evaluation on a test set produced by ESA is largely consistent with the results originally reported by the evaluated retrievers and confirms the need of fine tuning for reading comprehension. As of writing this paper, ESA is piloting SpaceQA internally.

</p>
</details>

<details><summary><b>Pre-trained Adversarial Perturbations</b>
<a href="https://arxiv.org/abs/2210.03372">arxiv:2210.03372</a>
&#x1F4C8; 3 <br>
<p>Yuanhao Ban, Yinpeng Dong</p></summary>
<p>

**Abstract:** Self-supervised pre-training has drawn increasing attention in recent years due to its superior performance on numerous downstream tasks after fine-tuning. However, it is well-known that deep learning models lack the robustness to adversarial examples, which can also invoke security issues to pre-trained models, despite being less explored. In this paper, we delve into the robustness of pre-trained models by introducing Pre-trained Adversarial Perturbations (PAPs), which are universal perturbations crafted for the pre-trained models to maintain the effectiveness when attacking fine-tuned ones without any knowledge of the downstream tasks. To this end, we propose a Low-Level Layer Lifting Attack (L4A) method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained models. Equipped with an enhanced noise augmentation strategy, L4A is effective at generating more transferable PAPs against fine-tuned models. Extensive experiments on typical pre-trained vision models and ten downstream tasks demonstrate that our method improves the attack success rate by a large margin compared with state-of-the-art methods.

</p>
</details>

<details><summary><b>Can Artificial Intelligence Reconstruct Ancient Mosaics?</b>
<a href="https://arxiv.org/abs/2210.06145">arxiv:2210.06145</a>
&#x1F4C8; 2 <br>
<p>Fernando Moral-Andrés, Elena Merino-Gómez, Pedro Reviriego, Fabrizio Lombardi</p></summary>
<p>

**Abstract:** A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, restoration and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward.

</p>
</details>

<details><summary><b>Scaling Directed Controller Synthesis via Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2210.05393">arxiv:2210.05393</a>
&#x1F4C8; 2 <br>
<p>Tomás Delgado, Víctor Braberman, Sebastian Uchitel</p></summary>
<p>

**Abstract:** Directed Controller Synthesis technique finds solutions for the non-blocking property in discrete event systems by exploring a reduced portion of the exponentially big state space, using best-first search. Aiming to minimize the explored states, it is currently guided by a domain-independent handcrafted heuristic, with which it reaches state-of-the-art performance. In this work, we propose a new method for obtaining heuristics based on Reinforcement Learning. The synthesis algorithm is framed as an RL task with an unbounded action space and a modified version of DQN is used. With a simple and general set of features, we show that it is possible to learn heuristics on small versions of a problem in a way that generalizes to the larger instances. Our agents learn from scratch and outperform the existing heuristic overall, in instances unseen during training.

</p>
</details>

<details><summary><b>Constructing Prediction Intervals with Neural Networks: An Empirical Evaluation of Bootstrapping and Conformal Inference Methods</b>
<a href="https://arxiv.org/abs/2210.05354">arxiv:2210.05354</a>
&#x1F4C8; 2 <br>
<p>Alex Contarino, Christine Schubert Kabban, Chancellor Johnstone, Fairul Mohd-Zaid</p></summary>
<p>

**Abstract:** Artificial neural networks (ANNs) are popular tools for accomplishing many machine learning tasks, including predicting continuous outcomes. However, the general lack of confidence measures provided with ANN predictions limit their applicability. Supplementing point predictions with prediction intervals (PIs) is common for other learning algorithms, but the complex structure and training of ANNs renders constructing PIs difficult. This work provides the network design choices and inferential methods for creating better performing PIs with ANNs. A two-step experiment is executed across 11 data sets, including an imaged-based data set. Two distribution-free methods for constructing PIs, bootstrapping and conformal inference, are considered. The results of the first experimental step reveal that the choices inherent to building an ANN affect PI performance. Guidance is provided for optimizing PI performance with respect to each network feature and PI method. In the second step, 20 algorithms for constructing PIs, each using the principles of bootstrapping or conformal inference, are implemented to determine which provides the best performance while maintaining reasonable computational burden. In general, this trade-off is optimized when implementing the cross-conformal method, which maintained interval coverage and efficiency with decreased computational burden.

</p>
</details>

<details><summary><b>Over-the-Air Split Machine Learning in Wireless MIMO Networks</b>
<a href="https://arxiv.org/abs/2210.04742">arxiv:2210.04742</a>
&#x1F4C8; 2 <br>
<p>Yuzhi Yang, Zhaoyang Zhang, Yuqing Tian, Zhaohui Yang, Chongwen Huang, Caijun Zhong, Kai-Kit Wong</p></summary>
<p>

**Abstract:** In split machine learning (ML), different partitions of a neural network (NN) are executed by different computing nodes, requiring a large amount of communication cost. To ease communication burden, over-the-air computation (OAC) can efficiently implement all or part of the computation at the same time of communication. Based on the proposed system, the system implementation over wireless network is introduced and we provide the problem formulation. In particular, we show that the inter-layer connection in a NN of any size can be mathematically decomposed into a set of linear precoding and combining transformations over MIMO channels. Therefore, the precoding matrix at the transmitter and the combining matrix at the receiver of each MIMO link, as well as the channel matrix itself, can jointly serve as a fully connected layer of the NN. The generalization of the proposed scheme to the conventional NNs is also introduced. Finally, we extend the proposed scheme to the widely used convolutional neural networks and demonstrate its effectiveness under both the static and quasi-static memory channel conditions with comprehensive simulations. In such a split ML system, the precoding and combining matrices are regarded as trainable parameters, while MIMO channel matrix is regarded as unknown (implicit) parameters.

</p>
</details>

<details><summary><b>Time Minimization in Hierarchical Federated Learning</b>
<a href="https://arxiv.org/abs/2210.04689">arxiv:2210.04689</a>
&#x1F4C8; 2 <br>
<p>Chang Liu, Terence Jie Chua, Jun Zhao</p></summary>
<p>

**Abstract:** Federated Learning is a modern decentralized machine learning technique where user equipments perform machine learning tasks locally and then upload the model parameters to a central server. In this paper, we consider a 3-layer hierarchical federated learning system which involves model parameter exchanges between the cloud and edge servers, and the edge servers and user equipment. In a hierarchical federated learning model, delay in communication and computation of model parameters has a great impact on achieving a predefined global model accuracy. Therefore, we formulate a joint learning and communication optimization problem to minimize total model parameter communication and computation delay, by optimizing local iteration counts and edge iteration counts. To solve the problem, an iterative algorithm is proposed. After that, a time-minimized UE-to-edge association algorithm is presented where the maximum latency of the system is reduced. Simulation results show that the global model converges faster under optimal edge server and local iteration counts. The hierarchical federated learning latency is minimized with the proposed UE-to-edge association strategy.

</p>
</details>

<details><summary><b>Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets</b>
<a href="https://arxiv.org/abs/2210.04688">arxiv:2210.04688</a>
&#x1F4C8; 2 <br>
<p>Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Arunesh Sinha, Bowen Xu, Xinwen Hou, Guoliang Fan, David Lo</p></summary>
<p>

**Abstract:** A growing body of research works has focused on the Offline Reinforcement Learning (RL) paradigm. Data providers share large pre-collected datasets on which others can train high-quality agents without interacting with the environments. Such an offline RL paradigm has demonstrated effectiveness in many critical tasks, including robot control, autonomous driving, etc. A well-trained agent can be regarded as a software system. However, less attention is paid to investigating the security threats to the offline RL system. In this paper, we focus on a critical security threat: backdoor attacks. Given normal observations, an agent implanted with backdoors takes actions leading to high rewards. However, the same agent takes actions that lead to low rewards if the observations are injected with triggers that can activate the backdoor. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning) and evaluate how different Offline RL algorithms react to this attack. Our experiments conducted on four tasks and four offline RL algorithms expose a disquieting fact: none of the existing offline RL algorithms is immune to such a backdoor attack. More specifically, Baffle modifies $10\%$ of the datasets for four tasks (3 robotic controls and 1 autonomous driving). Agents trained on the poisoned datasets perform well in normal settings. However, when triggers are presented, the agents' performance decreases drastically by $63.6\%$, $57.8\%$, $60.8\%$ and $44.7\%$ in the four tasks on average. The backdoor still persists after fine-tuning poisoned agents on clean datasets. We further show that the inserted backdoor is also hard to be detected by a popular defensive method. This paper calls attention to developing more effective protection for the open-source offline RL dataset.

</p>
</details>

<details><summary><b>Utilizing Explainable AI for improving the Performance of Neural Networks</b>
<a href="https://arxiv.org/abs/2210.04686">arxiv:2210.04686</a>
&#x1F4C8; 2 <br>
<p>Huawei Sun, Lorenzo Servadei, Hao Feng, Michael Stephan, Robert Wille, Avik Santra</p></summary>
<p>

**Abstract:** Nowadays, deep neural networks are widely used in a variety of fields that have a direct impact on society. Although those models typically show outstanding performance, they have been used for a long time as black boxes. To address this, Explainable Artificial Intelligence (XAI) has been developing as a field that aims to improve the transparency of the model and increase their trustworthiness. We propose a retraining pipeline that consistently improves the model predictions starting from XAI and utilizing state-of-the-art techniques. To do that, we use the XAI results, namely SHapley Additive exPlanations (SHAP) values, to give specific training weights to the data samples. This leads to an improved training of the model and, consequently, better performance. In order to benchmark our method, we evaluate it on both real-life and public datasets. First, we perform the method on a radar-based people counting scenario. Afterward, we test it on the CIFAR-10, a public Computer Vision dataset. Experiments using the SHAP-based retraining approach achieve a 4% more accuracy w.r.t. the standard equal weight retraining for people counting tasks. Moreover, on the CIFAR-10, our SHAP-based weighting strategy ends up with a 3% accuracy rate than the training procedure with equal weighted samples.

</p>
</details>

<details><summary><b>CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure</b>
<a href="https://arxiv.org/abs/2210.04633">arxiv:2210.04633</a>
&#x1F4C8; 2 <br>
<p>Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li, Xuesong Lu, Ming Gao</p></summary>
<p>

**Abstract:** Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose a novel probing method CAT-probing to quantitatively interpret how CodePTMs attend code structure. We first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. After that, we define a new metric CAT-score to measure the commonality between the token-level attention scores generated in CodePTMs and the pair-wise distances between corresponding AST nodes. The higher the CAT-score, the stronger the ability of CodePTMs to capture code structure. We conduct extensive experiments to integrate CAT-probing with representative CodePTMs for different programming languages. Experimental results show the effectiveness of CAT-probing in CodePTM interpretation. Our codes and data are publicly available at https://github.com/nchen909/CodeAttention.

</p>
</details>

<details><summary><b>Mutual Theory of Mind for Human-AI Communication</b>
<a href="https://arxiv.org/abs/2210.03842">arxiv:2210.03842</a>
&#x1F4C8; 2 <br>
<p>Qiaosi Wang, Ashok K. Goel</p></summary>
<p>

**Abstract:** From navigation systems to smart assistants, we communicate with various AI on a daily basis. At the core of such human-AI communication, we convey our understanding of the AI's capability to the AI through utterances with different complexities, and the AI conveys its understanding of our needs and goals to us through system outputs. However, this communication process is prone to failures for two reasons: the AI might have the wrong understanding of the user and the user might have the wrong understanding of the AI. To enhance mutual understanding in human-AI communication, we posit the Mutual Theory of Mind (MToM) framework, inspired by our basic human capability of "Theory of Mind." In this paper, we discuss the motivation of the MToM framework and its three key components that continuously shape the mutual understanding during three stages of human-AI communication. We then describe a case study inspired by the MToM framework to demonstrate the power of MToM framework to guide the design and understanding of human-AI communication.

</p>
</details>

<details><summary><b>The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks</b>
<a href="https://arxiv.org/abs/2210.03820">arxiv:2210.03820</a>
&#x1F4C8; 2 <br>
<p>Daniel Kunin, Atsushi Yamamura, Chao Ma, Surya Ganguli</p></summary>
<p>

**Abstract:** In this work, we explore the maximum-margin bias of quasi-homogeneous neural networks trained with gradient flow on an exponential loss and past a point of separability. We introduce the class of quasi-homogeneous models, which is expressive enough to describe nearly all neural networks with homogeneous activations, even those with biases, residual connections, and normalization layers, while structured enough to enable geometric analysis of its gradient dynamics. Using this analysis, we generalize the existing results of maximum-margin bias for homogeneous networks to this richer class of models. We find that gradient flow implicitly favors a subset of the parameters, unlike in the case of a homogeneous model where all parameters are treated equally. We demonstrate through simple examples how this strong favoritism toward minimizing an asymmetric norm can degrade the robustness of quasi-homogeneous models. On the other hand, we conjecture that this norm-minimization discards, when possible, unnecessary higher-order parameters, reducing the model to a sparser parameterization. Lastly, by applying our theorem to sufficiently expressive neural networks with normalization layers, we reveal a universal mechanism behind the empirical phenomenon of Neural Collapse.

</p>
</details>

<details><summary><b>Evaluating the Performance of StyleGAN2-ADA on Medical Images</b>
<a href="https://arxiv.org/abs/2210.03786">arxiv:2210.03786</a>
&#x1F4C8; 2 <br>
<p>McKell Woodland, John Wood, Brian M. Anderson, Suprateek Kundu, Ethan Lin, Eugene Koay, Bruno Odisio, Caroline Chung, Hyunseon Christine Kang, Aradhana M. Venkatesan, Sireesha Yedururi, Brian De, Yuan-Mao Lin, Ankit B. Patel, Kristy K. Brock</p></summary>
<p>

**Abstract:** Although generative adversarial networks (GANs) have shown promise in medical imaging, they have four main limitations that impeded their utility: computational cost, data requirements, reliable evaluation measures, and training complexity. Our work investigates each of these obstacles in a novel application of StyleGAN2-ADA to high-resolution medical imaging datasets. Our dataset is comprised of liver-containing axial slices from non-contrast and contrast-enhanced computed tomography (CT) scans. Additionally, we utilized four public datasets composed of various imaging modalities. We trained a StyleGAN2 network with transfer learning (from the Flickr-Faces-HQ dataset) and data augmentation (horizontal flipping and adaptive discriminator augmentation). The network's generative quality was measured quantitatively with the Fréchet Inception Distance (FID) and qualitatively with a visual Turing test given to seven radiologists and radiation oncologists.
  The StyleGAN2-ADA network achieved a FID of 5.22 ($\pm$ 0.17) on our liver CT dataset. It also set new record FIDs of 10.78, 3.52, 21.17, and 5.39 on the publicly available SLIVER07, ChestX-ray14, ACDC, and Medical Segmentation Decathlon (brain tumors) datasets. In the visual Turing test, the clinicians rated generated images as real 42% of the time, approaching random guessing. Our computational ablation study revealed that transfer learning and data augmentation stabilize training and improve the perceptual quality of the generated images. We observed the FID to be consistent with human perceptual evaluation of medical images. Finally, our work found that StyleGAN2-ADA consistently produces high-quality results without hyperparameter searches or retraining.

</p>
</details>

<details><summary><b>LOCL: Learning Object-Attribute Composition using Localization</b>
<a href="https://arxiv.org/abs/2210.03780">arxiv:2210.03780</a>
&#x1F4C8; 2 <br>
<p>Satish Kumar, ASM Iftekhar, Ekta Prashnani, B. S. Manjunath</p></summary>
<p>

**Abstract:** This paper describes LOCL (Learning Object Attribute Composition using Localization) that generalizes composition zero shot learning to objects in cluttered and more realistic settings. The problem of unseen Object Attribute (OA) associations has been well studied in the field, however, the performance of existing methods is limited in challenging scenes. In this context, our key contribution is a modular approach to localizing objects and attributes of interest in a weakly supervised context that generalizes robustly to unseen configurations. Localization coupled with a composition classifier significantly outperforms state of the art (SOTA) methods, with an improvement of about 12% on currently available challenging datasets. Further, the modularity enables the use of localized feature extractor to be used with existing OA compositional learning methods to improve their overall performance.

</p>
</details>

<details><summary><b>MRI-based classification of IDH mutation and 1p/19q codeletion status of gliomas using a 2.5D hybrid multi-task convolutional neural network</b>
<a href="https://arxiv.org/abs/2210.03779">arxiv:2210.03779</a>
&#x1F4C8; 2 <br>
<p>Satrajit Chakrabarty, Pamela LaMontagne, Joshua Shimony, Daniel S. Marcus, Aristeidis Sotiras</p></summary>
<p>

**Abstract:** Isocitrate dehydrogenase (IDH) mutation and 1p/19q codeletion status are important prognostic markers for glioma. Currently, they are determined using invasive procedures. Our goal was to develop artificial intelligence-based methods to non-invasively determine these molecular alterations from MRI. For this purpose, pre-operative MRI scans of 2648 patients with gliomas (grade II-IV) were collected from Washington University School of Medicine (WUSM; n = 835) and publicly available datasets viz. Brain Tumor Segmentation (BraTS; n = 378), LGG 1p/19q (n = 159), Ivy Glioblastoma Atlas Project (Ivy GAP; n = 41), The Cancer Genome Atlas (TCGA; n = 461), and the Erasmus Glioma Database (EGD; n = 774). A 2.5D hybrid convolutional neural network was proposed to simultaneously localize the tumor and classify its molecular status by leveraging imaging features from MR scans and prior knowledge features from clinical records and tumor location. The models were tested on one internal (TCGA) and two external (WUSM and EGD) test sets. For IDH, the best-performing model achieved areas under the receiver operating characteristic (AUROC) of 0.925, 0.874, 0.933 and areas under the precision-recall curves (AUPRC) of 0.899, 0.702, 0.853 on the internal, WUSM, and EGD test sets, respectively. For 1p/19q, the best model achieved AUROCs of 0.782, 0.754, 0.842, and AUPRCs of 0.588, 0.713, 0.782, on those three data-splits, respectively. The high accuracy of the model on unseen data showcases its generalization capabilities and suggests its potential to perform a 'virtual biopsy' for tailoring treatment planning and overall clinical management of gliomas.

</p>
</details>

<details><summary><b>In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?</b>
<a href="https://arxiv.org/abs/2210.03773">arxiv:2210.03773</a>
&#x1F4C8; 2 <br>
<p>Henry Kvinge, Tegan H. Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, Jesse D. Lew</p></summary>
<p>

**Abstract:** It is often said that a deep learning model is "invariant" to some specific type of transformation. However, what is meant by this statement strongly depends on the context in which it is made. In this paper we explore the nature of invariance and equivariance of deep learning models with the goal of better understanding the ways in which they actually capture these concepts on a formal level. We introduce a family of invariance and equivariance metrics that allows us to quantify these properties in a way that disentangles them from other metrics such as loss or accuracy. We use our metrics to better understand the two most popular methods used to build invariance into networks: data augmentation and equivariant layers. We draw a range of conclusions about invariance and equivariance in deep learning models, ranging from whether initializing a model with pretrained weights has an effect on a trained model's invariance, to the extent to which invariance learned via training can generalize to out-of-distribution data.

</p>
</details>

<details><summary><b>FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings</b>
<a href="https://arxiv.org/abs/2210.03766">arxiv:2210.03766</a>
&#x1F4C8; 2 <br>
<p>Andrew Silva, Pradyumna Tambwekar, Matthew Gombolay</p></summary>
<p>

**Abstract:** Federated learning is a training paradigm that learns from multiple distributed users without aggregating data on a centralized server. Such a paradigm promises the ability to deploy machine-learning at-scale to a diverse population of end-users without first collecting a large, labeled dataset for all possible tasks. As federated learning typically averages learning updates across a decentralized population, there is a growing need for personalization of federated learning systems (i.e conversational agents must be able to personalize to a specific user's preferences). In this work, we propose a new direction for personalization research within federated learning, leveraging both personal embeddings and shared context embeddings. We also present an approach to predict these ``preference'' embeddings, enabling personalization without backpropagation. Compared to state-of-the-art personalization baselines, our approach achieves a 50\% improvement in test-time perplexity using 0.001\% of the memory required by baseline approaches, and achieving greater sample- and compute-efficiency.

</p>
</details>

<details><summary><b>A deep learning approach to solve forward differential problems on graphs</b>
<a href="https://arxiv.org/abs/2210.03746">arxiv:2210.03746</a>
&#x1F4C8; 2 <br>
<p>Yuanyuan Zhao, Massimiliano Lupo Pasini</p></summary>
<p>

**Abstract:** We propose a novel deep learning (DL) approach to solve one-dimensional non-linear elliptic, parabolic, and hyperbolic problems on graphs. A system of physics-informed neural network (PINN) models is used to solve the differential equations, by assigning each PINN model to a specific edge of the graph. Kirkhoff-Neumann (KN) nodal conditions are imposed in a weak form by adding a penalization term to the training loss function. Through the penalization term that imposes the KN conditions, PINN models associated with edges that share a node coordinate with each other to ensure continuity of the solution and of its directional derivatives computed along the respective edges. Using individual PINN models for each edge of the graph allows our approach to fulfill necessary requirements for parallelization by enabling different PINN models to be trained on distributed compute resources. Numerical results show that the system of PINN models accurately approximate the solutions of the differential problems across the entire graph for a broad set of graph topologies.

</p>
</details>

<details><summary><b>Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts</b>
<a href="https://arxiv.org/abs/2210.03690">arxiv:2210.03690</a>
&#x1F4C8; 2 <br>
<p>Nghia T. Le, Fan Bai, Alan Ritter</p></summary>
<p>

**Abstract:** Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in applying in-context learning to resolve anaphora. For example, encoding a single in-context demonstration that consists of: an anaphor, a paragraph-length context, and a list of corresponding antecedents, requires conditioning a language model on a long sequence of tokens, limiting the number of demonstrations per prompt. In this paper, we present MICE (Mixtures of In-Context Experts), which we demonstrate is effective for few-shot anaphora resolution in scientific protocols (Tamari et al., 2021). Given only a handful of training examples, MICE combines the predictions of hundreds of in-context experts, yielding a 30% increase in F1 score over a competitive prompt retrieval baseline. Furthermore, we show MICE can be used to train compact student models without sacrificing performance. As far as we are aware, this is the first work to present experimental results demonstrating the effectiveness of in-context learning on the task of few-shot anaphora resolution in scientific protocols.

</p>
</details>

<details><summary><b>Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors</b>
<a href="https://arxiv.org/abs/2210.03683">arxiv:2210.03683</a>
&#x1F4C8; 2 <br>
<p>Federico Baldassarre, Quentin Debard, Gonzalo Fiz Pontiveros, Tri Kurniawan Wijaya</p></summary>
<p>

**Abstract:** The proliferation of DeepFake technology is a rising challenge in today's society, owing to more powerful and accessible generation methods. To counter this, the research community has developed detectors of ever-increasing accuracy. However, the ability to explain the decisions of such models to users is lacking behind and is considered an accessory in large-scale benchmarks, despite being a crucial requirement for the correct deployment of automated tools for content moderation. We attribute the issue to the reliance on qualitative comparisons and the lack of established metrics. We describe a simple set of metrics to evaluate the visual quality and informativeness of explanations of video DeepFake classifiers from a human-centric perspective. With these metrics, we compare common approaches to improve explanation quality and discuss their effect on both classification and explanation performance on the recent DFDC and DFD datasets.

</p>
</details>

<details><summary><b>Longtonotes: OntoNotes with Longer Coreference Chains</b>
<a href="https://arxiv.org/abs/2210.03650">arxiv:2210.03650</a>
&#x1F4C8; 2 <br>
<p>Kumar Shridhar, Nicholas Monath, Raghuveer Thirukovalluru, Alessandro Stolfo, Manzil Zaheer, Andrew McCallum, Mrinmaya Sachan</p></summary>
<p>

**Abstract:** Ontonotes has served as the most important benchmark for coreference resolution. However, for ease of annotation, several long documents in Ontonotes were split into smaller parts. In this work, we build a corpus of coreference-annotated documents of significantly longer length than what is currently available. We do so by providing an accurate, manually-curated, merging of annotations from documents that were split into multiple parts in the original Ontonotes annotation process. The resulting corpus, which we call LongtoNotes contains documents in multiple genres of the English language with varying lengths, the longest of which are up to 8x the length of documents in Ontonotes, and 2x those in Litbank. We evaluate state-of-the-art neural coreference systems on this new corpus, analyze the relationships between model architectures/hyperparameters and document length on performance and efficiency of the models, and demonstrate areas of improvement in long-document coreference modeling revealed by our new corpus. Our data and code is available at: https://github.com/kumar-shridhar/LongtoNotes.

</p>
</details>

<details><summary><b>Learnware: Small Models Do Big</b>
<a href="https://arxiv.org/abs/2210.03647">arxiv:2210.03647</a>
&#x1F4C8; 2 <br>
<p>Zhi-Hua Zhou, Zhi-Hao Tan</p></summary>
<p>

**Abstract:** There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identified to reuse according to the requirement of future users who know nothing about the model in advance.

</p>
</details>

<details><summary><b>Learning Social Navigation from Demonstrations with Conditional Neural Processes</b>
<a href="https://arxiv.org/abs/2210.03582">arxiv:2210.03582</a>
&#x1F4C8; 2 <br>
<p>Yigit Yildirim, Emre Ugur</p></summary>
<p>

**Abstract:** Sociability is essential for modern robots to increase their acceptability in human environments. Traditional techniques use manually engineered utility functions inspired by observing pedestrian behaviors to achieve social navigation. However, social aspects of navigation are diverse, changing across different types of environments, societies, and population densities, making it unrealistic to use hand-crafted techniques in each domain. This paper presents a data-driven navigation architecture that uses state-of-the-art neural architectures, namely Conditional Neural Processes, to learn global and local controllers of the mobile robot from observations. Additionally, we leverage a state-of-the-art, deep prediction mechanism to detect situations not similar to the trained ones, where reactive controllers step in to ensure safe navigation. Our results demonstrate that the proposed framework can successfully carry out navigation tasks regarding social norms in the data. Further, we showed that our system produces fewer personal-zone violations, causing less discomfort.

</p>
</details>

<details><summary><b>Automated segmentation and morphological characterization of placental histology images based on a single labeled image</b>
<a href="https://arxiv.org/abs/2210.03566">arxiv:2210.03566</a>
&#x1F4C8; 2 <br>
<p>Arash Rabbani, Masoud Babaei, Masoumeh Gharib</p></summary>
<p>

**Abstract:** In this study, a novel method of data augmentation has been presented for the segmentation of placental histological images when the labeled data are scarce. This method generates new realizations of the placenta intervillous morphology while maintaining the general textures and orientations. As a result, a diversified artificial dataset of images is generated that can be used for training deep learning segmentation models. We have observed that on average the presented method of data augmentation led to a 42% decrease in the binary cross-entropy loss of the validation dataset compared to the common approach in the literature. Additionally, the morphology of the intervillous space is studied under the effect of the proposed image reconstruction technique, and the diversity of the artificially generated population is quantified. Due to the high resemblance of the generated images to the real ones, the applications of the proposed method may not be limited to placental histological images, and it is recommended that other types of tissues be investigated in future studies.

</p>
</details>

<details><summary><b>In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile Networks</b>
<a href="https://arxiv.org/abs/2210.03555">arxiv:2210.03555</a>
&#x1F4C8; 2 <br>
<p>Kaibin Huang, Hai Wu, Zhiyan Liu, Xiaojuan Qi</p></summary>
<p>

**Abstract:** The sixth-generation (6G) mobile networks are expected to feature the ubiquitous deployment of machine learning and AI algorithms at the network edge. With rapid advancements in edge AI, the time has come to realize intelligence downloading onto edge devices (e.g., smartphones and sensors). To materialize this version, we propose a novel technology in this article, called in-situ model downloading, that aims to achieve transparent and real-time replacement of on-device AI models by downloading from an AI library in the network. Its distinctive feature is the adaptation of downloading to time-varying situations (e.g., application, location, and time), devices' heterogeneous storage-and-computing capacities, and channel states. A key component of the presented framework is a set of techniques that dynamically compress a downloaded model at the depth-level, parameter-level, or bit-level to support adaptive model downloading. We further propose a virtualized 6G network architecture customized for deploying in-situ model downloading with the key feature of a three-tier (edge, local, and central) AI library. Furthermore, experiments are conducted to quantify 6G connectivity requirements and research opportunities pertaining to the proposed technology are discussed.

</p>
</details>

<details><summary><b>An Empirical Studies on How the Developers Discussed about Pandas Topics</b>
<a href="https://arxiv.org/abs/2210.03519">arxiv:2210.03519</a>
&#x1F4C8; 2 <br>
<p>Sajib Kumar Saha Joy, Farzad Ahmed, Al Hasib Mahamud, Nibir Chandra Mandal</p></summary>
<p>

**Abstract:** Pandas is defined as a software library which is used for data analysis in Python programming language. As pandas is a fast, easy and open source data analysis tool, it is rapidly used in different software engineering projects like software development, machine learning, computer vision, natural language processing, robotics, and others. So a huge interests are shown in software developers regarding pandas and a huge number of discussions are now becoming dominant in online developer forums, like Stack Overflow (SO). Such discussions can help to understand the popularity of pandas library and also can help to understand the importance, prevalence, difficulties of pandas topics. The main aim of this research paper is to find the popularity and difficulty of pandas topics. For this regard, SO posts are collected which are related to pandas topic discussions. Topic modeling are done on the textual contents of the posts. We found 26 topics which we further categorized into 5 board categories. We observed that developers discuss variety of pandas topics in SO related to error and excepting handling, visualization, External support, dataframe, and optimization. In addition, a trend chart is generated according to the discussion of topics in a predefined time series. The finding of this paper can provide a path to help the developers, educators and learners. For example, beginner developers can learn most important topics in pandas which are essential for develop any model. Educators can understand the topics which seem hard to learners and can build different tutorials which can make that pandas topic understandable. From this empirical study it is possible to understand the preferences of developers in pandas topic by processing their SO posts

</p>
</details>

<details><summary><b>Population-Based Reinforcement Learning for Combinatorial Optimization</b>
<a href="https://arxiv.org/abs/2210.03475">arxiv:2210.03475</a>
&#x1F4C8; 2 <br>
<p>Nathan Grinsztajn, Daniel Furelos-Blanco, Thomas D. Barrett</p></summary>
<p>

**Abstract:** Applying reinforcement learning (RL) to combinatorial optimization problems is attractive as it removes the need for expert knowledge or pre-solved instances. However, it is unrealistic to expect an agent to solve these (often NP-)hard problems in a single shot at inference due to their inherent complexity. Thus, leading approaches often implement additional search strategies, from stochastic sampling and beam-search to explicit fine-tuning. In this paper, we argue for the benefits of learning a population of complementary policies, which can be simultaneously rolled out at inference. To this end, we introduce Poppy, a simple theoretically grounded training procedure for populations. Instead of relying on a predefined or hand-crafted notion of diversity, Poppy induces an unsupervised specialization targeted solely at maximizing the performance of the population. We show that Poppy produces a set of complementary policies, and obtains state-of-the-art RL results on three popular NP-hard problems: the traveling salesman (TSP), the capacitated vehicle routing (CVRP), and 0-1 knapsack (KP) problems. On TSP specifically, Poppy outperforms the previous state-of-the-art, dividing the optimality gap by 5 while reducing the inference time by more than an order of magnitude.

</p>
</details>

<details><summary><b>Geomagnetic Survey Interpolation with the Machine Learning Approach</b>
<a href="https://arxiv.org/abs/2210.03379">arxiv:2210.03379</a>
&#x1F4C8; 2 <br>
<p>Igor Aleshin, Kirill Kholodkov, Ivan Malygin, Roman Shevchuk, Roman Sidorov</p></summary>
<p>

**Abstract:** This paper portrays the method of UAV magnetometry survey data interpolation. The method accommodates the fact that this kind of data has a spatial distribution of the samples along a series of straight lines (similar to maritime tacks), which is a prominent characteristic of many kinds of UAV surveys. The interpolation relies on the very basic Nearest Neighbours algorithm, although augmented with a Machine Learning approach. Such an approach enables the error of less than 5 percent by intelligently adjusting the Nearest Neighbour algorithm parameters. The method was pilot tested on geomagnetic data with Borok Geomagnetic Observatory UAV aeromagnetic survey data.

</p>
</details>

<details><summary><b>Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR</b>
<a href="https://arxiv.org/abs/2210.06352">arxiv:2210.06352</a>
&#x1F4C8; 1 <br>
<p>Sami Alabed, Dominik Grewe, Juliana Franco, Bart Chrzaszcz, Tom Natan, Tamara Norman, Norman A. Rink, Dimitrios Vytiniotis, Michael Schaarschmidt</p></summary>
<p>

**Abstract:** Large neural network models are commonly trained through a combination of advanced parallelism strategies in a single program, multiple data (SPMD) paradigm. For example, training large transformer models requires combining data, model, and pipeline partitioning; and optimizer sharding techniques. However, identifying efficient combinations for many model architectures and accelerator systems requires significant manual analysis. In this work, we present an automatic partitioner that identifies these combinations through a goal-oriented search. Our key findings are that a Monte Carlo Tree Search-based partitioner leveraging partition-specific compiler analysis directly into the search and guided goals matches expert-level strategies for various models.

</p>
</details>

<details><summary><b>Indoor Localization with Robust Global Channel Charting: A Time-Distance-Based Approach</b>
<a href="https://arxiv.org/abs/2210.06294">arxiv:2210.06294</a>
&#x1F4C8; 1 <br>
<p>Maximilian Stahlke, George Yammine, Tobias Feigl, Bjoern M. Eskofier, Christopher Mutschler</p></summary>
<p>

**Abstract:** Fingerprinting-based positioning significantly improves the indoor localization performance in non-line-of-sight-dominated areas. However, its deployment and maintenance is cost-intensive as it needs ground-truth reference systems for both the initial training and the adaption to environmental changes. In contrast, channel charting (CC) works without explicit reference information and only requires the spatial correlations of channel state information (CSI). While CC has shown promising results in modelling the geometry of the radio environment, a deeper insight into CC for localization using multi-anchor large-bandwidth measurements is still pending. We contribute a novel distance metric for time-synchronized single-input/single-output CSIs that approaches a linear correlation to the Euclidean distance. This allows to learn the environment's global geometry without annotations. To efficiently optimize the global channel chart we approximate the metric with a Siamese neural network. This enables full CC-assisted fingerprinting and positioning only using a linear transformation from the chart to the real-world coordinates. We compare our approach to the state-of-the-art of CC on two different real-world data sets recorded with a 5G and UWB radio setup. Our approach outperforms others with localization accuracies of 0.69m for the UWB and 1.4m for the 5G setup. We show that CC-assisted fingerprinting enables highly accurate localization and reduces (or eliminates) the need for annotated training data.

</p>
</details>

<details><summary><b>An Energy-Efficient Spiking Neural Network for Finger Velocity Decoding for Implantable Brain-Machine Interface</b>
<a href="https://arxiv.org/abs/2210.06287">arxiv:2210.06287</a>
&#x1F4C8; 1 <br>
<p>Jiawei Liao, Lars Widmer, Xiaying Wang, Alfio Di Mauro, Samuel R. Nason-Tomaszewski, Cynthia A. Chestek, Luca Benini, Taekwang Jang</p></summary>
<p>

**Abstract:** Brain-machine interfaces (BMIs) are promising for motor rehabilitation and mobility augmentation. High-accuracy and low-power algorithms are required to achieve implantable BMI systems. In this paper, we propose a novel spiking neural network (SNN) decoder for implantable BMI regression tasks. The SNN is trained with enhanced spatio-temporal backpropagation to fully leverage its ability in handling temporal problems. The proposed SNN decoder achieves the same level of correlation coefficient as the state-of-the-art ANN decoder in offline finger velocity decoding tasks, while it requires only 6.8% of the computation operations and 9.4% of the memory access.

</p>
</details>

<details><summary><b>iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction and Deep Learning</b>
<a href="https://arxiv.org/abs/2210.05671">arxiv:2210.05671</a>
&#x1F4C8; 1 <br>
<p>Chuhan Xu, Xia Jiang</p></summary>
<p>

**Abstract:** Background: Breast cancer is a multifactorial disease, genetic and environmental factors will affect its incidence probability. Breast cancer metastasis is one of the main cause of breast cancer related deaths reported by the American Cancer Society (ACS). Method: the iMedBot is a web application that we developed using the python Flask web framework and deployed on Amazon Web Services. It contains a frontend and a backend. The backend is supported by a python program we developed using the python Keras and scikit-learn packages, which can be used to learn deep feedforward neural network (DFNN) models. Result: the iMedBot can provide two main services: 1. it can predict 5-, 10-, or 15-year breast cancer metastasis based on a set of clinical information provided by a user. The prediction is done by using a set of DFNN models that were pretrained, and 2. It can train DFNN models for a user using user-provided dataset. The model trained will be evaluated using AUC and both the AUC value and the AUC ROC curve will be provided. Conclusion: The iMedBot web application provides a user-friendly interface for user-agent interaction in conducting personalized prediction and model training. It is an initial attempt to convert results of deep learning research into an online tool that may stir further research interests in this direction. Keywords: Deep learning, Breast Cancer, Web application, Model training.

</p>
</details>

<details><summary><b>Simulating single-photon detector array sensors for depth imaging</b>
<a href="https://arxiv.org/abs/2210.05644">arxiv:2210.05644</a>
&#x1F4C8; 1 <br>
<p>Stirling Scholes, Germán Mora-Martín, Feng Zhu, Istvan Gyongy, Phil Soan, Jonathan Leach</p></summary>
<p>

**Abstract:** Single-Photon Avalanche Detector (SPAD) arrays are a rapidly emerging technology. These multi-pixel sensors have single-photon sensitivities and pico-second temporal resolutions thus they can rapidly generate depth images with millimeter precision. Such sensors are a key enabling technology for future autonomous systems as they provide guidance and situational awareness. However, to fully exploit the capabilities of SPAD array sensors, it is crucial to establish the quality of depth images they are able to generate in a wide range of scenarios. Given a particular optical system and a finite image acquisition time, what is the best-case depth resolution and what are realistic images generated by SPAD arrays? In this work, we establish a robust yet simple numerical procedure that rapidly establishes the fundamental limits to depth imaging with SPAD arrays under real world conditions. Our approach accurately generates realistic depth images in a wide range of scenarios, allowing the performance of an optical depth imaging system to be established without the need for costly and laborious field testing. This procedure has applications in object detection and tracking for autonomous systems and could be easily extended to systems for underwater imaging or for imaging around corners.

</p>
</details>

<details><summary><b>mPSAuth: Privacy-Preserving and Scalable Authentication for Mobile Web Applications</b>
<a href="https://arxiv.org/abs/2210.04777">arxiv:2210.04777</a>
&#x1F4C8; 1 <br>
<p>David Monschein, Oliver P. Waldhorst</p></summary>
<p>

**Abstract:** As nowadays most web application requests originate from mobile devices, authentication of mobile users is essential in terms of security considerations. To this end, recent approaches rely on machine learning techniques to analyze various aspects of user behavior as a basis for authentication decisions. These approaches face two challenges: first, examining behavioral data raises significant privacy concerns, and second, approaches must scale to support a large number of users. Existing approaches do not address these challenges sufficiently. We propose mPSAuth, an approach for continuously tracking various data sources reflecting user behavior (e.g., touchscreen interactions, sensor data) and estimating the likelihood of the current user being legitimate based on machine learning techniques. With mPSAuth, both the authentication protocol and the machine learning models operate on homomorphically encrypted data to ensure the users' privacy. Furthermore, the number of machine learning models used by mPSAuth is independent of the number of users, thus providing adequate scalability. In an extensive evaluation based on real-world data from a mobile application, we illustrate that mPSAuth can provide high accuracy with low encryption and communication overhead, while the effort for the inference is increased to a tolerable extent.

</p>
</details>

<details><summary><b>Signal Detection in MIMO Systems with Hardware Imperfections: Message Passing on Neural Networks</b>
<a href="https://arxiv.org/abs/2210.03911">arxiv:2210.03911</a>
&#x1F4C8; 1 <br>
<p>Dawei Gao, Qinghua Guo, Guisheng Liao, Yonina C. Eldar, Yonghui Li, Yanguang Yu, Branka Vucetic</p></summary>
<p>

**Abstract:** In this paper, we investigate signal detection in multiple-input-multiple-output (MIMO) communication systems with hardware impairments, such as power amplifier nonlinearity and in-phase/quadrature imbalance. To deal with the complex combined effects of hardware imperfections, neural network (NN) techniques, in particular deep neural networks (DNNs), have been studied to directly compensate for the impact of hardware impairments. However, it is difficult to train a DNN with limited pilot signals, hindering its practical applications. In this work, we investigate how to achieve efficient Bayesian signal detection in MIMO systems with hardware imperfections. Characterizing combined hardware imperfections often leads to complicated signal models, making Bayesian signal detection challenging. To address this issue, we first train an NN to "model" the MIMO system with hardware imperfections and then perform Bayesian inference based on the trained NN. Modelling the MIMO system with NN enables the design of NN architectures based on the signal flow of the MIMO system, minimizing the number of NN layers and parameters, which is crucial to achieving efficient training with limited pilot signals. We then represent the trained NN with a factor graph, and design an efficient message passing based Bayesian signal detector, leveraging the unitary approximate message passing (UAMP) algorithm. The implementation of a turbo receiver with the proposed Bayesian detector is also investigated. Extensive simulation results demonstrate that the proposed technique delivers remarkably better performance than state-of-the-art methods.

</p>
</details>

<details><summary><b>Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU</b>
<a href="https://arxiv.org/abs/2210.03900">arxiv:2210.03900</a>
&#x1F4C8; 1 <br>
<p>Hanqiu Chen, Yahya Alhinai, Yihan Jiang, Eunjee Na, Cong Hao</p></summary>
<p>

**Abstract:** Dynamic graph neural network (DGNN) is becoming increasingly popular because of its widespread use in capturing dynamic features in the real world. A variety of dynamic graph neural networks designed from algorithmic perspectives have succeeded in incorporating temporal information into graph processing. Despite the promising algorithmic performance, deploying DGNNs on hardware presents additional challenges due to the model complexity, diversity, and the nature of the time dependency. Meanwhile, the differences between DGNNs and static graph neural networks make hardware-related optimizations for static graph neural networks unsuitable for DGNNs. In this paper, we select eight prevailing DGNNs with different characteristics and profile them on both CPU and GPU. The profiling results are summarized and analyzed, providing in-depth insights into the bottlenecks of DGNNs on hardware and identifying potential optimization opportunities for future DGNN acceleration. Followed by a comprehensive survey, we provide a detailed analysis of DGNN performance bottlenecks on hardware, including temporal data dependency, workload imbalance, data movement, and GPU warm-up. We suggest several optimizations from both software and hardware perspectives. This paper is the first to provide an in-depth analysis of the hardware performance of DGNN Code is available at https://github.com/sharc-lab/DGNN_analysis.

</p>
</details>

<details><summary><b>Self-Supervised Deep Equilibrium Models for Inverse Problems with Theoretical Guarantees</b>
<a href="https://arxiv.org/abs/2210.03837">arxiv:2210.03837</a>
&#x1F4C8; 1 <br>
<p>Weijie Gan, Chunwei Ying, Parna Eshraghi, Tongyao Wang, Cihat Eldeniz, Yuyang Hu, Jiaming Liu, Yasheng Chen, Hongyu An, Ulugbek S. Kamilov</p></summary>
<p>

**Abstract:** Deep equilibrium models (DEQ) have emerged as a powerful alternative to deep unfolding (DU) for image reconstruction. DEQ models-implicit neural networks with effectively infinite number of layers-were shown to achieve state-of-the-art image reconstruction without the memory complexity associated with DU. While the performance of DEQ has been widely investigated, the existing work has primarily focused on the settings where groundtruth data is available for training. We present self-supervised deep equilibrium model (SelfDEQ) as the first self-supervised reconstruction framework for training model-based implicit networks from undersampled and noisy MRI measurements. Our theoretical results show that SelfDEQ can compensate for unbalanced sampling across multiple acquisitions and match the performance of fully supervised DEQ. Our numerical results on in-vivo MRI data show that SelfDEQ leads to state-of-the-art performance using only undersampled and noisy training data.

</p>
</details>

<details><summary><b>Sampling-Based Decomposition Algorithms for Arbitrary Tensor Networks</b>
<a href="https://arxiv.org/abs/2210.03828">arxiv:2210.03828</a>
&#x1F4C8; 1 <br>
<p>Osman Asif Malik, Vivek Bharadwaj, Riley Murray</p></summary>
<p>

**Abstract:** We show how to develop sampling-based alternating least squares (ALS) algorithms for decomposition of tensors into any tensor network (TN) format. Provided the TN format satisfies certain mild assumptions, resulting algorithms will have input sublinear per-iteration cost. Unlike most previous works on sampling-based ALS methods for tensor decomposition, the sampling in our framework is done according to the exact leverage score distribution of the design matrices in the ALS subproblems. We implement and test two tensor decomposition algorithms that use our sampling framework in a feature extraction experiment where we compare them against a number of other decomposition algorithms.

</p>
</details>

<details><summary><b>GENHOP: An Image Generation Method Based on Successive Subspace Learning</b>
<a href="https://arxiv.org/abs/2210.03689">arxiv:2210.03689</a>
&#x1F4C8; 1 <br>
<p>Xuejing Lei, Wei Wang, C. -C. Jay Kuo</p></summary>
<p>

**Abstract:** Being different from deep-learning-based (DL-based) image generation methods, a new image generative model built upon successive subspace learning principle is proposed and named GenHop (an acronym of Generative PixelHop) in this work. GenHop consists of three modules: 1) high-to-low dimension reduction, 2) seed image generation, and 3) low-to-high dimension expansion. In the first module, it builds a sequence of high-to-low dimensional subspaces through a sequence of whitening processes, each of which contains samples of joint-spatial-spectral representation. In the second module, it generates samples in the lowest dimensional subspace. In the third module, it finds a proper high-dimensional sample for a seed image by adding details back via locally linear embedding (LLE) and a sequence of coloring processes. Experiments show that GenHop can generate visually pleasant images whose FID scores are comparable or even better than those of DL-based generative models for MNIST, Fashion-MNIST and CelebA datasets.

</p>
</details>

<details><summary><b>Novice Type Error Diagnosis with Natural Language Models</b>
<a href="https://arxiv.org/abs/2210.03682">arxiv:2210.03682</a>
&#x1F4C8; 1 <br>
<p>Chuqin Geng, Haolin Ye, Yixuan Li, Tianyu Han, Brigitte Pientka, Xujie Si</p></summary>
<p>

**Abstract:** Strong static type systems help programmers eliminate many errors without much burden of supplying type annotations. However, this flexibility makes it highly non-trivial to diagnose ill-typed programs, especially for novice programmers. Compared to classic constraint solving and optimization-based approaches, the data-driven approach has shown great promise in identifying the root causes of type errors with higher accuracy. Instead of relying on hand-engineered features, this work explores natural language models for type error localization, which can be trained in an end-to-end fashion without requiring any features. We demonstrate that, for novice type error diagnosis, the language model-based approach significantly outperforms the previous state-of-the-art data-driven approach. Specifically, our model could predict type errors correctly 62% of the time, outperforming the state-of-the-art Nate's data-driven model by 11%, in a more rigorous accuracy metric. Furthermore, we also apply structural probes to explain the performance difference between different language models.

</p>
</details>

<details><summary><b>Do We Need Explainable AI in Companies? Investigation of Challenges, Expectations, and Chances from Employees' Perspective</b>
<a href="https://arxiv.org/abs/2210.03527">arxiv:2210.03527</a>
&#x1F4C8; 1 <br>
<p>Katharina Weitz, Chi Tai Dang, Elisabeth André</p></summary>
<p>

**Abstract:** By using AI, companies want to improve their business success and innovation chances. However, in doing so, they (companies and their employees) are faced with new requirements. In particular, legal regulations call for transparency and comprehensibility of AI systems. The field of XAI deals with these issues. Currently, the results are mostly obtained in lab studies, while the transfer to real-world applications is lacking. This includes considering employees' needs and attributes, which may differ from end-users in the lab. Therefore, this project report paper provides initial insights into employees' specific needs and attitudes towards (X)AI. For this, the results of a project's online survey are reported that investigate two employees' perspectives (i.e., company level and employee level) on (X)AI to create a holistic view of challenges, risks, and needs of employees. Our findings suggest that AI and XAI are well-known terms perceived as important for employees. This is a first step for XAI to be a potential driver to foster the successful usage of AI by providing transparent and comprehensible insights into AI technologies. To benefit from (X)AI technologies, supportive employees on the management level are valuable catalysts. This work contributes to the ongoing demand for XAI research to develop human-centered and domain-specific XAI designs.

</p>
</details>

<details><summary><b>Multi-objective and multi-fidelity Bayesian optimization of laser-plasma acceleration</b>
<a href="https://arxiv.org/abs/2210.03484">arxiv:2210.03484</a>
&#x1F4C8; 1 <br>
<p>Faran Irshad, Stefan Karsch, Andreas Döpp</p></summary>
<p>

**Abstract:** Beam parameter optimization in accelerators involves multiple, sometimes competing objectives. Condensing these multiple objectives into a single objective unavoidably results in bias towards particular outcomes that do not necessarily represent the best possible outcome for the operator in terms of parameter optimization. A more versatile approach is multi-objective optimization, which establishes the trade-off curve or Pareto front between objectives. Here we present first results on multi-objective Bayesian optimization of a simulated laser-plasma accelerator. We find that multi-objective optimization is equal or even superior in performance to its single-objective counterparts, and that it is more resilient to different statistical descriptions of objectives.
  As a second major result of our paper, we significantly reduce the computational costs of the optimization by choosing the resolution and box size of the simulations dynamically. This is relevant since even with the use of Bayesian statistics, performing such optimizations on a multi-dimensional search space may require hundreds or thousands of simulations. Our algorithm translates information gained from fast, low-resolution runs with lower fidelity to high-resolution data, thus requiring fewer actual simulations at highest computational cost.
  The techniques demonstrated in this paper can be translated to many different use cases, both computational and experimental.

</p>
</details>

<details><summary><b>Flexible Alignment Super-Resolution Network for Multi-Contrast MRI</b>
<a href="https://arxiv.org/abs/2210.03460">arxiv:2210.03460</a>
&#x1F4C8; 1 <br>
<p>Yiming Liu, Mengxi Zhang, Weiqin Zhang, Bo Hou, Dan Liu, Heqing Lian, Bo Jiang</p></summary>
<p>

**Abstract:** Magnetic resonance images play an essential role in clinical diagnosis by acquiring the structural information of biological tissue. However, during acquiring magnetic resonance images, patients have to endure physical and psychological discomfort, including irritating noise and acute anxiety. To make the patient feel cozier, technically, it will reduce the retention time that patients stay in the strong magnetic field at the expense of image quality. Therefore, Super-Resolution plays a crucial role in preprocessing the low-resolution images for more precise medical analysis. In this paper, we propose the Flexible Alignment Super-Resolution Network (FASR-Net) for multi-contrast magnetic resonance images Super-Resolution. The core of multi-contrast SR is to match the patches of low-resolution and reference images. However, the inappropriate foreground scale and patch size of multi-contrast MRI sometimes lead to the mismatch of patches. To tackle this problem, the Flexible Alignment module is proposed to endow receptive fields with flexibility. Flexible Alignment module contains two parts: (1) The Single-Multi Pyramid Alignmet module serves for low-resolution and reference image with different scale. (2) The Multi-Multi Pyramid Alignment module serves for low-resolution and reference image with the same scale. Extensive experiments on the IXI and FastMRI datasets demonstrate that the FASR-Net outperforms the existing state-of-the-art approaches. In addition, by comparing the reconstructed images with the counterparts obtained by the existing algorithms, our method could retain more textural details by leveraging multi-contrast images.

</p>
</details>

<details><summary><b>Research on Self-adaptive Online Vehicle Velocity Prediction Strategy Considering Traffic Information Fusion</b>
<a href="https://arxiv.org/abs/2210.03402">arxiv:2210.03402</a>
&#x1F4C8; 1 <br>
<p>Ziyan Zhang, Junhao Shen, Dongwei Yao, Feng Wu</p></summary>
<p>

**Abstract:** In order to increase the prediction accuracy of the online vehicle velocity prediction (VVP) strategy, a self-adaptive velocity prediction algorithm fused with traffic information was presented for the multiple scenarios. Initially, traffic scenarios were established inside the co-simulation environment. In addition, the algorithm of a general regressive neural network (GRNN) paired with datasets of the ego-vehicle, the front vehicle, and traffic lights was used in traffic scenarios, which increasingly improved the prediction accuracy. To ameliorate the robustness of the algorithm, then the strategy was optimized by particle swarm optimization (PSO) and k-fold cross-validation to find the optimal parameters of the neural network in real-time, which constructed a self-adaptive online PSO-GRNN VVP strategy with multi-information fusion to adapt with different operating situations. The self-adaptive online PSO-GRNN VVP strategy was then deployed to a variety of simulated scenarios to test its efficacy under various operating situations. Finally, the simulation results reveal that in urban and highway scenarios, the prediction accuracy is separately increased by 27.8% and 54.5% when compared to the traditional GRNN VVP strategy with fixed parameters utilizing only the historical ego-vehicle velocity dataset.

</p>
</details>

<details><summary><b>The $(1+(λ,λ))$ Global SEMO Algorithm</b>
<a href="https://arxiv.org/abs/2210.03618">arxiv:2210.03618</a>
&#x1F4C8; 0 <br>
<p>Benjamin Doerr, Omar El Hadri, Adrien Pinard</p></summary>
<p>

**Abstract:** The $(1+(λ,λ))$ genetic algorithm is a recently proposed single-objective evolutionary algorithm with several interesting properties. We show that its main working principle, mutation with a high rate and crossover as repair mechanism, can be transported also to multi-objective evolutionary computation. We define the $(1+(λ,λ))$ global SEMO algorithm, a variant of the classic global SEMO algorithm, and prove that it optimizes the OneMinMax benchmark asymptotically faster than the global SEMO. Following the single-objective example, we design a one-fifth rule inspired dynamic parameter setting (to the best of our knowledge for the first time in discrete multi-objective optimization) and prove that it further improves the runtime to $O(n^2)$, whereas the best runtime guarantee for the global SEMO is only $O(n^2 \log n)$.

</p>
</details>

<details><summary><b>1st ICLR International Workshop on Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data (PAIR^2Struct)</b>
<a href="https://arxiv.org/abs/2210.03612">arxiv:2210.03612</a>
&#x1F4C8; 0 <br>
<p>Hao Wang, Wanyu Lin, Hao He, Di Wang, Chengzhi Mao, Muhan Zhang</p></summary>
<p>

**Abstract:** Recent years have seen advances on principles and guidance relating to accountable and ethical use of artificial intelligence (AI) spring up around the globe. Specifically, Data Privacy, Accountability, Interpretability, Robustness, and Reasoning have been broadly recognized as fundamental principles of using machine learning (ML) technologies on decision-critical and/or privacy-sensitive applications. On the other hand, in tremendous real-world applications, data itself can be well represented as various structured formalisms, such as graph-structured data (e.g., networks), grid-structured data (e.g., images), sequential data (e.g., text), etc. By exploiting the inherently structured knowledge, one can design plausible approaches to identify and use more relevant variables to make reliable decisions, thereby facilitating real-world deployments.

</p>
</details>


{% endraw %}
Prev: [2022.10.06]({{ '/2022/10/06/2022.10.06.html' | relative_url }})  Next: [2022.10.08]({{ '/2022/10/08/2022.10.08.html' | relative_url }})