Prev: [2022.04.29]({{ '/2022/04/29/2022.04.29.html' | relative_url }})  Next: [2022.05.01]({{ '/2022/05/01/2022.05.01.html' | relative_url }})
{% raw %}
## Summary for 2022-04-30, created on 2022-05-07


<details><summary><b>Learning to Get Up</b>
<a href="https://arxiv.org/abs/2205.00307">arxiv:2205.00307</a>
&#x1F4C8; 34300 <br>
<p>Tianxin Tao, Matthew Wilson, Ruiyu Gou, Michiel van de Panne</p></summary>
<p>

**Abstract:** Getting up from an arbitrary fallen state is a basic human skill. Existing methods for learning this skill often generate highly dynamic and erratic get-up motions, which do not resemble human get-up strategies, or are based on tracking recorded human get-up motions. In this paper, we present a staged approach using reinforcement learning, without recourse to motion capture data. The method first takes advantage of a strong character model, which facilitates the discovery of solution modes. A second stage then learns to adapt the control policy to work with progressively weaker versions of the character. Finally, a third stage learns control policies that can reproduce the weaker get-up motions at much slower speeds. We show that across multiple runs, the method can discover a diverse variety of get-up strategies, and execute them at a variety of speeds. The results usually produce policies that use a final stand-up strategy that is common to the recovery motions seen from all initial states. However, we also find policies for which different strategies are seen for prone and supine initial fallen states. The learned get-up control strategies often have significant static stability, i.e., they can be paused at a variety of points during the get-up motion. We further test our method on novel constrained scenarios, such as having a leg and an arm in a cast.

</p>
</details>

<details><summary><b>Visual Spatial Reasoning</b>
<a href="https://arxiv.org/abs/2205.00363">arxiv:2205.00363</a>
&#x1F4C8; 588 <br>
<p>Fangyu Liu, Guy Emerson, Nigel Collier</p></summary>
<p>

**Abstract:** Spatial relations are fundamental to human cognition and are the most basic knowledge for us to understand and communicate about our physical surroundings. In this paper, we ask the critical question: Are current vision-and-language models (VLMs) able to correctly understand spatial relations? To answer this question, we propose Visual Spatial Reasoning (VSR), a novel benchmark task with human labelled dataset for investigating VLMs' capabilities in recognising 65 types of spatial relationships (e.g., under, in front of, facing etc.) in natural text-image pairs. Specifically, given a caption and an image, the model needs to perform binary classification and decide if the caption accurately describes the spatial relationships of two objects presented in the image. While being seemingly simple and straightforward, the task shows a large gap between human and model performance (human ceiling on the VSR task is above 95% and models only achieve around 70%). With fine-grained categorisation and control on both concepts and relations, our VSR benchmark enables us to perform interesting probing analysis to pinpoint VLMs' failure cases and the reasons behind. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations that concern orientations of objects. Also, VLMs have poor zero-shot generalisation toward unseen concepts. The dataset and code are released at github.com/cambridgeltl/visual-spatial-reasoning.

</p>
</details>

<details><summary><b>Foundational Models for Continual Learning: An Empirical Study of Latent Replay</b>
<a href="https://arxiv.org/abs/2205.00329">arxiv:2205.00329</a>
&#x1F4C8; 44 <br>
<p>Oleksiy Ostapenko, Timothee Lesort, Pau Rodríguez, Md Rifat Arefin, Arthur Douillard, Irina Rish, Laurent Charlin</p></summary>
<p>

**Abstract:** Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efficacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efficacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classifier at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efficacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efficient playground for further CL research. The codebase is available under https://github.com/oleksost/latent_CL.

</p>
</details>

<details><summary><b>LayoutBERT: Masked Language Layout Model for Object Insertion</b>
<a href="https://arxiv.org/abs/2205.00347">arxiv:2205.00347</a>
&#x1F4C8; 27 <br>
<p>Kerem Turgutlu, Sanat Sharma, Jayant Kumar</p></summary>
<p>

**Abstract:** Image compositing is one of the most fundamental steps in creative workflows. It involves taking objects/parts of several images to create a new image, called a composite. Currently, this process is done manually by creating accurate masks of objects to be inserted and carefully blending them with the target scene or images, usually with the help of tools such as Photoshop or GIMP. While there have been several works on automatic selection of objects for creating masks, the problem of object placement within an image with the correct position, scale, and harmony remains a difficult problem with limited exploration. Automatic object insertion in images or designs is a difficult problem as it requires understanding of the scene geometry and the color harmony between objects. We propose LayoutBERT for the object insertion task. It uses a novel self-supervised masked language model objective and bidirectional multi-head self-attention. It outperforms previous layout-based likelihood models and shows favorable properties in terms of model capacity. We demonstrate the effectiveness of our approach for object insertion in the image compositing setting and other settings like documents and design templates. We further demonstrate the usefulness of the learned representations for layout-based retrieval tasks. We provide both qualitative and quantitative evaluations on datasets from diverse domains like COCO, PublayNet, and two new datasets which we call Image Layouts and Template Layouts. Image Layouts which consists of 5.8 million images with layout annotations is the largest image layout dataset to our knowledge. We also share ablation study results on the effect of dataset size, model size and class sample size for this task.

</p>
</details>

<details><summary><b>Software Testing for Machine Learning</b>
<a href="https://arxiv.org/abs/2205.00210">arxiv:2205.00210</a>
&#x1F4C8; 8 <br>
<p>Dusica Marijan, Arnaud Gotlieb</p></summary>
<p>

**Abstract:** Machine learning has become prevalent across a wide variety of applications. Unfortunately, machine learning has also shown to be susceptible to deception, leading to errors, and even fatal failures. This circumstance calls into question the widespread use of machine learning, especially in safety-critical applications, unless we are able to assure its correctness and trustworthiness properties. Software verification and testing are established technique for assuring such properties, for example by detecting errors. However, software testing challenges for machine learning are vast and profuse - yet critical to address. This summary talk discusses the current state-of-the-art of software testing for machine learning. More specifically, it discusses six key challenge areas for software testing of machine learning systems, examines current approaches to these challenges and highlights their limitations. The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning.

</p>
</details>

<details><summary><b>Leveraging Emotion-specific Features to Improve Transformer Performance for Emotion Classification</b>
<a href="https://arxiv.org/abs/2205.00283">arxiv:2205.00283</a>
&#x1F4C8; 6 <br>
<p>Shaily Desai, Atharva Kshirsagar, Aditi Sidnerlikar, Nikhil Khodake, Manisha Marathe</p></summary>
<p>

**Abstract:** This paper describes the approach to the Emotion Classification shared task held at WASSA 2022 by team PVGs AI Club. This Track 2 sub-task focuses on building models which can predict a multi-class emotion label based on essays from news articles where a person, group or another entity is affected. Baseline transformer models have been demonstrating good results on sequence classification tasks, and we aim to improve this performance with the help of ensembling techniques, and by leveraging two variations of emotion-specific representations. We observe better results than our baseline models and achieve an accuracy of 0.619 and a macro F1 score of 0.520 on the emotion classification task.

</p>
</details>

<details><summary><b>Opponent Modeling in Negotiation Dialogues by Related Data Adaptation</b>
<a href="https://arxiv.org/abs/2205.00344">arxiv:2205.00344</a>
&#x1F4C8; 5 <br>
<p>Kushal Chawla, Gale M. Lucas, Jonathan May, Jonathan Gratch</p></summary>
<p>

**Abstract:** Opponent modeling is the task of inferring another party's mental state within the context of social interactions. In a multi-issue negotiation, it involves inferring the relative importance that the opponent assigns to each issue under discussion, which is crucial for finding high-value deals. A practical model for this task needs to infer these priorities of the opponent on the fly based on partial dialogues as input, without needing additional annotations for training. In this work, we propose a ranker for identifying these priorities from negotiation dialogues. The model takes in a partial dialogue as input and predicts the priority order of the opponent. We further devise ways to adapt related data sources for this task to provide more explicit supervision for incorporating the opponent's preferences and offers, as a proxy to relying on granular utterance-level annotations. We show the utility of our proposed approach through extensive experiments based on two dialogue datasets. We find that the proposed data adaptations lead to strong performance in zero-shot and few-shot scenarios. Moreover, they allow the model to perform better than baselines while accessing fewer utterances from the opponent. We release our code to support future work in this direction.

</p>
</details>

<details><summary><b>Understanding the Generalization Performance of Spectral Clustering Algorithms</b>
<a href="https://arxiv.org/abs/2205.00281">arxiv:2205.00281</a>
&#x1F4C8; 4 <br>
<p>Shaojie Li, Sheng Ouyang, Yong Liu</p></summary>
<p>

**Abstract:** The theoretical analysis of spectral clustering mainly focuses on consistency, while there is relatively little research on its generalization performance. In this paper, we study the excess risk bounds of the popular spectral clustering algorithms: \emph{relaxed} RatioCut and \emph{relaxed} NCut. Firstly, we show that their excess risk bounds between the empirical continuous optimal solution and the population-level continuous optimal solution have a $\mathcal{O}(1/\sqrt{n})$ convergence rate, where $n$ is the sample size. Secondly, we show the fundamental quantity in influencing the excess risk between the empirical discrete optimal solution and the population-level discrete optimal solution. At the empirical level, algorithms can be designed to reduce this quantity. Based on our theoretical analysis, we propose two novel algorithms that can not only penalize this quantity, but also cluster the out-of-sample data without re-eigendecomposition on the overall sample. Experiments verify the effectiveness of the proposed algorithms.

</p>
</details>

<details><summary><b>A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction</b>
<a href="https://arxiv.org/abs/2205.00241">arxiv:2205.00241</a>
&#x1F4C8; 4 <br>
<p>Runxin Xu, Peiyi Wang, Tianyu Liu, Shuang Zeng, Baobao Chang, Zhifang Sui</p></summary>
<p>

**Abstract:** Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.

</p>
</details>

<details><summary><b>Explainable Artificial Intelligence for Bayesian Neural Networks: Towards trustworthy predictions of ocean dynamics</b>
<a href="https://arxiv.org/abs/2205.00202">arxiv:2205.00202</a>
&#x1F4C8; 4 <br>
<p>Mariana C. A. Clare, Maike Sonnewald, Redouane Lguensat, Julie Deshayes, Venkatramani Balaji</p></summary>
<p>

**Abstract:** The trustworthiness of neural networks is often challenged because they lack the ability to express uncertainty and explain their skill. This can be problematic given the increasing use of neural networks in high stakes decision-making such as in climate change applications. We address both issues by successfully implementing a Bayesian Neural Network (BNN), where parameters are distributions rather than deterministic, and applying novel implementations of explainable AI (XAI) techniques. The uncertainty analysis from the BNN provides a comprehensive overview of the prediction more suited to practitioners' needs than predictions from a classical neural network. Using a BNN means we can calculate the entropy (i.e. uncertainty) of the predictions and determine if the probability of an outcome is statistically significant. To enhance trustworthiness, we also spatially apply the two XAI techniques of Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanation (SHAP) values. These XAI methods reveal the extent to which the BNN is suitable and/or trustworthy. Using two techniques gives a more holistic view of BNN skill and its uncertainty, as LRP considers neural network parameters, whereas SHAP considers changes to outputs. We verify these techniques using comparison with intuition from physical theory. The differences in explanation identify potential areas where new physical theory guided studies are needed.

</p>
</details>

<details><summary><b>Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF</b>
<a href="https://arxiv.org/abs/2205.00377">arxiv:2205.00377</a>
&#x1F4C8; 3 <br>
<p>Haoming Guo, Tianyi Huang, Huixuan Huang, Mingyue Fan, Gerald Friedland</p></summary>
<p>

**Abstract:** The sharing of fake news and conspiracy theories on social media has wide-spread negative effects. By designing and applying different machine learning models, researchers have made progress in detecting fake news from text. However, existing research places a heavy emphasis on general, common-sense fake news, while in reality fake news often involves rapidly changing topics and domain-specific vocabulary. In this paper, we present our methods and results for three fake news detection tasks at MediaEval benchmark 2021 that specifically involve COVID-19 related topics. We experiment with a group of text-based models including Support Vector Machines, Random Forest, BERT, and RoBERTa. We find that a pre-trained transformer yields the best validation results, but a randomly initialized transformer with smart design can also be trained to reach accuracies close to that of the pre-trained transformer.

</p>
</details>

<details><summary><b>Orthogonal Statistical Learning with Self-Concordant Loss</b>
<a href="https://arxiv.org/abs/2205.00350">arxiv:2205.00350</a>
&#x1F4C8; 3 <br>
<p>Lang Liu, Carlos Cinelli, Zaid Harchaoui</p></summary>
<p>

**Abstract:** Orthogonal statistical learning and double machine learning have emerged as general frameworks for two-stage statistical prediction in the presence of a nuisance component. We establish non-asymptotic bounds on the excess risk of orthogonal statistical learning methods with a loss function satisfying a self-concordance property. Our bounds improve upon existing bounds by a dimension factor while lifting the assumption of strong convexity. We illustrate the results with examples from multiple treatment effect estimation and generalized partially linear modeling.

</p>
</details>

<details><summary><b>Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening</b>
<a href="https://arxiv.org/abs/2205.00225">arxiv:2205.00225</a>
&#x1F4C8; 3 <br>
<p>Li Duan, Gerardo Argon-Camarasa</p></summary>
<p>

**Abstract:** Robotic deformable-object manipulation is a challenge in the robotic industry because deformable objects have complicated and various object states. Predicting those object states and updating manipulation planning are time-consuming and computationally expensive. In this paper, we propose an effective robotic manipulation approach for recognising 'known configurations' of garments with a 'Known Configuration neural Network' (KCNet) and choosing pre-designed manipulation plans based on the recognised known configurations. Our robotic manipulation plan features a four-action strategy: finding two critical grasping points, stretching the garments, and lifting down the garments. We demonstrate that our approach only needs 98 seconds on average to flatten garments of five categories.

</p>
</details>

<details><summary><b>Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods</b>
<a href="https://arxiv.org/abs/2205.00189">arxiv:2205.00189</a>
&#x1F4C8; 3 <br>
<p>Takane Ueno, Yuto Sawa, Yeongdae Kim, Jacqueline Urakami, Hiroki Oura, Katie Seaborn</p></summary>
<p>

**Abstract:** Trust has emerged as a key factor in people's interactions with AI-infused systems. Yet, little is known about what models of trust have been used and for what systems: robots, virtual characters, smart vehicles, decision aids, or others. Moreover, there is yet no known standard approach to measuring trust in AI. This scoping review maps out the state of affairs on trust in human-AI interaction (HAII) from the perspectives of models, measures, and methods. Findings suggest that trust is an important and multi-faceted topic of study within HAII contexts. However, most work is under-theorized and under-reported, generally not using established trust models and missing details about methods, especially Wizard of Oz. We offer several targets for systematic review work as well as a research agenda for combining the strengths and addressing the weaknesses of the current literature.

</p>
</details>

<details><summary><b>Processing Network Controls via Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.02119">arxiv:2205.02119</a>
&#x1F4C8; 2 <br>
<p>Mark Gluzman</p></summary>
<p>

**Abstract:** Novel advanced policy gradient (APG) algorithms, such as proximal policy optimization (PPO), trust region policy optimization, and their variations, have become the dominant reinforcement learning (RL) algorithms because of their ease of implementation and good practical performance. This dissertation is concerned with theoretical justification and practical application of the APG algorithms for solving processing network control optimization problems. Processing network control problems are typically formulated as Markov decision process (MDP) or semi-Markov decision process (SMDP) problems that have several unconventional for RL features: infinite state spaces, unbounded costs, long-run average cost objectives. Policy improvement bounds play a crucial role in the theoretical justification of the APG algorithms. In this thesis we refine existing bounds for MDPs with finite state spaces and prove novel policy improvement bounds for classes of MDPs and SMDPs used to model processing network operations. We consider two examples of processing network control problems and customize the PPO algorithm to solve them. First, we consider parallel-server and multiclass queueing networks controls. Second, we consider the drivers repositioning problem in a ride-hailing service system. For both examples the PPO algorithm with auxiliary modifications consistently generates control policies that outperform state-of-art heuristics.

</p>
</details>

<details><summary><b>Optimizing One-pixel Black-box Adversarial Attacks</b>
<a href="https://arxiv.org/abs/2205.02116">arxiv:2205.02116</a>
&#x1F4C8; 2 <br>
<p>Tianxun Zhou, Shubhankar Agrawal, Prateek Manocha</p></summary>
<p>

**Abstract:** The output of Deep Neural Networks (DNN) can be altered by a small perturbation of the input in a black box setting by making multiple calls to the DNN. However, the high computation and time required makes the existing approaches unusable. This work seeks to improve the One-pixel (few-pixel) black-box adversarial attacks to reduce the number of calls to the network under attack. The One-pixel attack uses a non-gradient optimization algorithm to find pixel-level perturbations under the constraint of a fixed number of pixels, which causes the network to predict the wrong label for a given image. We show through experimental results how the choice of the optimization algorithm and initial positions to search can reduce function calls and increase attack success significantly, making the attack more practical in real-world settings.

</p>
</details>

<details><summary><b>A Simple Duality Proof for Wasserstein Distributionally Robust Optimization</b>
<a href="https://arxiv.org/abs/2205.00362">arxiv:2205.00362</a>
&#x1F4C8; 2 <br>
<p>Luhao Zhang, Jincheng Yang, Rui Gao</p></summary>
<p>

**Abstract:** We present a short and elementary proof of the duality for Wasserstein distributionally robust optimization, which holds for any arbitrary Kantorovich transport distance, any arbitrary measurable loss function, and any arbitrary nominal probability distribution, as long as certain interchangeability principle holds.

</p>
</details>

<details><summary><b>Engineering flexible machine learning systems by traversing functionally invariant paths in weight space</b>
<a href="https://arxiv.org/abs/2205.00334">arxiv:2205.00334</a>
&#x1F4C8; 2 <br>
<p>Guruprasad Raghavan, Matt Thomson</p></summary>
<p>

**Abstract:** Deep neural networks achieve human-like performance on a variety of perceptual and decision making tasks. However, deep networks perform poorly when confronted with changing tasks or goals, and broadly fail to match the flexibility and robustness of human intelligence. Here, we develop a mathematical and algorithmic framework that enables continual training of deep neural networks on a broad range of objectives by defining path connected sets of neural networks that achieve equivalent functional performance on a given machine learning task while modulating network weights to achieve high-performance on a secondary objective. We view the weight space of a neural network as a curved Riemannian manifold and move a neural network along a functionally invariant path in weight space while searching for networks that satisfy a secondary objective. We introduce a path-sampling algorithm that trains networks with millions of weight parameters to learn a series of image classification tasks without performance loss. The algorithm generalizes to accommodate a range of secondary objectives including weight-pruning and weight diversification and exhibits state of the art performance on network compression and adversarial robustness benchmarks. Broadly, we demonstrate how the intrinsic geometry of machine learning problems can be harnessed to construct flexible and robust neural networks.

</p>
</details>

<details><summary><b>"And Then There Were None": Cracking White-box DNN Watermarks via Invariant Neuron Transforms</b>
<a href="https://arxiv.org/abs/2205.00199">arxiv:2205.00199</a>
&#x1F4C8; 2 <br>
<p>Yifan Yan, Xudong Pan, Yining Wang, Mi Zhang, Min Yang</p></summary>
<p>

**Abstract:** Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry.
  In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On $9$ state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact.

</p>
</details>

<details><summary><b>Fair Feature Subset Selection using Multiobjective Genetic Algorithm</b>
<a href="https://arxiv.org/abs/2205.01512">arxiv:2205.01512</a>
&#x1F4C8; 1 <br>
<p>Ayaz Ur Rehman, Anas Nadeem, Muhammad Zubair Malik</p></summary>
<p>

**Abstract:** The feature subset selection problem aims at selecting the relevant subset of features to improve the performance of a Machine Learning (ML) algorithm on training data. Some features in data can be inherently noisy, costly to compute, improperly scaled, or correlated to other features, and they can adversely affect the accuracy, cost, and complexity of the induced algorithm. The goal of traditional feature selection approaches has been to remove such irrelevant features. In recent years ML is making a noticeable impact on the decision-making processes of our everyday lives. We want to ensure that these decisions do not reflect biased behavior towards certain groups or individuals based on protected attributes such as age, sex, or race. In this paper, we present a feature subset selection approach that improves both fairness and accuracy objectives and computes Pareto-optimal solutions using the NSGA-II algorithm. We use statistical disparity as a fairness metric and F1-Score as a metric for model performance. Our experiments on the most commonly used fairness benchmark datasets with three different machine learning algorithms show that using the evolutionary algorithm we can effectively explore the trade-off between fairness and accuracy.

</p>
</details>

<details><summary><b>Abnormal-aware Multi-person Evaluation System with Improved Fuzzy Weighting</b>
<a href="https://arxiv.org/abs/2205.00388">arxiv:2205.00388</a>
&#x1F4C8; 1 <br>
<p>Shutong Ni</p></summary>
<p>

**Abstract:** There exists a phenomenon that subjectivity highly lies in the daily evaluation process. Our research primarily concentrates on a multi-person evaluation system with anomaly detection to minimize the possible inaccuracy that subjective assessment brings. We choose the two-stage screening method, which consists of rough screening and score-weighted Kendall-$τ$ Distance to winnow out abnormal data, coupled with hypothesis testing to narrow global discrepancy. Then we use Fuzzy Synthetic Evaluation Method(FSE) to determine the significance of scores given by reviewers as well as their reliability, culminating in a more impartial weight for each reviewer in the final conclusion. The results demonstrate a clear and comprehensive ranking instead of unilateral scores, and we get to have an efficiency in filtering out abnormal data as well as a reasonably objective weight determination mechanism. We can sense that through our study, people will have a chance of modifying a multi-person evaluation system to attain both equity and a relatively superior competitive atmosphere.

</p>
</details>

<details><summary><b>Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition</b>
<a href="https://arxiv.org/abs/2205.00380">arxiv:2205.00380</a>
&#x1F4C8; 1 <br>
<p>Jinsheng Wei, Wei Peng, Guanming Lu, Yante Li, Jingjie Yan, Guoying Zhao</p></summary>
<p>

**Abstract:** Micro-expression recognition (MER) is valuable because the involuntary nature of micro-expressions (MEs) can reveal genuine emotions. Most works recognize MEs by taking RGB videos or images as input. In fact, the activated facial regions in ME images are very small and the subtle motion can be easily submerged in the unrelated information. Facial landmarks are a low-dimensional and compact modality, which leads to much lower computational cost and can potentially concentrate more on ME-related features. However, the discriminability of landmarks for MER is not clear. Thus, this paper explores the contribution of facial landmarks and constructs a new framework to efficiently recognize MEs with sole facial landmark information. Specially, we design a separate structure module to separately aggregate the spatial and temporal information in the geometric movement graph based on facial landmarks, and a Geometric Two-Stream Graph Network is constructed to aggregate the low-order geometric information and high-order semantic information of facial landmarks. Furthermore, two core components are proposed to enhance features. Specifically, a semantic adjacency matrix can automatically model the relationship between nodes even long-distance nodes in a self-learning fashion; and an Adaptive Action Unit loss is introduced to guide the learning process such that the learned features are forced to have a synchronized pattern with facial action units. Notably, this work tackles MER only utilizing geometric features, processed based on a graph model, which provides a new idea with much higher efficiency to promote MER. The experimental results demonstrate that the proposed method can achieve competitive or even superior performance with a significantly reduced computational cost, and facial landmarks can significantly contribute to MER and are worth further study for efficient ME analysis.

</p>
</details>

<details><summary><b>Combined Learning of Neural Network Weights for Privacy in Collaborative Tasks</b>
<a href="https://arxiv.org/abs/2205.00361">arxiv:2205.00361</a>
&#x1F4C8; 1 <br>
<p>Aline R. Ioste, Alan M. Durham, Marcelo Finger</p></summary>
<p>

**Abstract:** We introduce CoLN, Combined Learning of Neural network weights, a novel method to securely combine Machine Learning models over sensitive data with no sharing of data. With CoLN, local hosts use the same Neural Network architecture and base parameters to train a model using only locally available data. Locally trained models are then submitted to a combining agent, which produces a combined model. The new model's parameters can be sent back to hosts, and can then be used as initial parameters for a new training iteration. CoLN is capable of combining several distributed neural networks of the same kind but is not restricted to any single neural architecture. In this paper we detail the combination algorithm and present experiments with feed-forward, convolutional, and recurrent Neural Network architectures, showing that the CoLN combined model approximates the performance of a hypothetical ideal centralized model, trained using the combination of the local datasets. CoLN can contribute to secure collaborative research, as required in the medical area, where privacy issues preclude data sharing, but where the limitations of local data demand information derived from larger datasets.

</p>
</details>

<details><summary><b>End-to-End Signal Classification in Signed Cumulative Distribution Transform Space</b>
<a href="https://arxiv.org/abs/2205.00348">arxiv:2205.00348</a>
&#x1F4C8; 1 <br>
<p>Abu Hasnat Mohammad Rubaiyat, Shiying Li, Xuwang Yin, Mohammad Shifat E Rabbi, Yan Zhuang, Gustavo K. Rohde</p></summary>
<p>

**Abstract:** This paper presents a new end-to-end signal classification method using the signed cumulative distribution transform (SCDT). We adopt a transport-based generative model to define the classification problem. We then make use of mathematical properties of the SCDT to render the problem easier in transform domain, and solve for the class of an unknown sample using a nearest local subspace (NLS) search algorithm in SCDT domain. Experiments show that the proposed method provides high accuracy classification results while being data efficient, robust to out-of-distribution samples, and competitive in terms of computational complexity with respect to the deep learning end-to-end classification methods. The implementation of the proposed method in Python language is integrated as a part of the software package PyTransKit (https://github.com/rohdelab/PyTransKit).

</p>
</details>

<details><summary><b>Artificial Intelligence and Medicine: A literature review</b>
<a href="https://arxiv.org/abs/2205.00322">arxiv:2205.00322</a>
&#x1F4C8; 1 <br>
<p>Chottiwatt Jittprasong</p></summary>
<p>

**Abstract:** In practically every industry today, artificial intelligence is one of the most effective ways for machines to assist humans. Since its inception, a large number of researchers throughout the globe have been pioneering the application of artificial intelligence in medicine. Although artificial intelligence may seem to be a 21st-century concept, Alan Turing pioneered the first foundation concept in the 1940s. Artificial intelligence in medicine has a huge variety of applications that researchers are continually exploring. The tremendous increase in computer and human resources has hastened progress in the 21st century, and it will continue to do so for many years to come. This review of the literature will highlight the emerging field of artificial intelligence in medicine and its current level of development.

</p>
</details>

<details><summary><b>Neural Network Optimal Feedback Control with Guaranteed Local Stability</b>
<a href="https://arxiv.org/abs/2205.00394">arxiv:2205.00394</a>
&#x1F4C8; 0 <br>
<p>Tenavi Nakamura-Zimmerer, Qi Gong, Wei Kang</p></summary>
<p>

**Abstract:** Recent research shows that deep learning can be an effective tool for designing optimal feedback controllers for high-dimensional nonlinear dynamic systems. But the behavior of these neural network (NN) controllers is still not well understood. In particular, some NNs with high test accuracy can fail to even locally stabilize the dynamic system. To address this challenge we propose several novel NN architectures, which we show guarantee local stability while retaining the semi-global approximation capacity to learn the optimal feedback policy. The proposed architectures are compared against standard NN feedback controllers through numerical simulations of two high-dimensional nonlinear optimal control problems (OCPs): stabilization of an unstable Burgers-type partial differential equation (PDE), and altitude and course tracking for a six degree-of-freedom (6DoF) unmanned aerial vehicle (UAV). The simulations demonstrate that standard NNs can fail to stabilize the dynamics even when trained well, while the proposed architectures are always at least locally stable. Moreover, the proposed controllers are found to be near-optimal in testing.

</p>
</details>

<details><summary><b>FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings</b>
<a href="https://arxiv.org/abs/2205.00313">arxiv:2205.00313</a>
&#x1F4C8; 0 <br>
<p>Cheng-Te Li, Cheng Hsu, Yang Zhang</p></summary>
<p>

**Abstract:** Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This paper aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users' and items' attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness.

</p>
</details>

<details><summary><b>TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2205.00293">arxiv:2205.00293</a>
&#x1F4C8; 0 <br>
<p>Konstantin Sozykin, Andrei Chertkov, Roman Schutski, Anh-Huy Phan, Andrzej Cichocki, Ivan Oseledets</p></summary>
<p>

**Abstract:** We present a novel procedure for optimization based on the combination of efficient quantized tensor train representation and a generalized maximum matrix volume principle. We demonstrate the applicability of the new Tensor Train Optimizer (TTOpt) method for various tasks, ranging from minimization of multidimensional functions to reinforcement learning. Our algorithm compares favorably to popular evolutionary-based methods and outperforms them by the number of function evaluations or execution time, often by a significant margin.

</p>
</details>

<details><summary><b>Learning Effective SDEs from Brownian Dynamics Simulations of Colloidal Particles</b>
<a href="https://arxiv.org/abs/2205.00286">arxiv:2205.00286</a>
&#x1F4C8; 0 <br>
<p>Nikolaos Evangelou, Felix Dietrich, Juan M. Bello-Rivas, Alex Yeh, Rachel Stein, Michael A. Bevan, Ioannis G. Kevekidis</p></summary>
<p>

**Abstract:** We construct a reduced, data-driven, parameter dependent effective Stochastic Differential Equation (eSDE) for electric-field mediated colloidal crystallization using data obtained from Brownian Dynamics Simulations. We use Diffusion Maps (a manifold learning algorithm) to identify a set of useful latent observables. In this latent space we identify an eSDE using a deep learning architecture inspired by numerical stochastic integrators and compare it with the traditional Kramers-Moyal expansion estimation. We show that the obtained variables and the learned dynamics accurately encode the physics of the Brownian Dynamic Simulations. We further illustrate that our reduced model captures the dynamics of corresponding experimental data. Our dimension reduction/reduced model identification approach can be easily ported to a broad class of particle systems dynamics experiments/models.

</p>
</details>

<details><summary><b>Deep Learning-Enabled Semantic Communication Systems with Task-Unaware Transmitter and Dynamic Data</b>
<a href="https://arxiv.org/abs/2205.00271">arxiv:2205.00271</a>
&#x1F4C8; 0 <br>
<p>Hongwei Zhang, Shuo Shao, Meixia Tao, Xiaoyan Bi, Khaled B. Letaief</p></summary>
<p>

**Abstract:** Existing deep learning-enabled semantic communication systems often rely on shared background knowledge between the transmitter and receiver that includes empirical data and their associated semantic information. In practice, the semantic information is defined by the pragmatic task of the receiver and cannot be known to the transmitter. The actual observable data at the transmitter can also have non-identical distribution with the empirical data in the shared background knowledge library. To address these practical issues, this paper proposes a new neural network-based semantic communication system for image transmission, where the task is unaware at the transmitter and the data environment is dynamic. The system consists of two main parts, namely the semantic extraction (SE) network and the data adaptation (DA) network. The SE network learns how to extract the semantic information using a receiver-leading training process. By using domain adaptation technique from transfer learning, the DA network learns how to convert the data observed into a similar form of the empirical data that the SE network can process without re-training. Numerical experiments show that the proposed method can be adaptive to observable datasets while keeping high performance in terms of both data recovery and task execution. The codes are available on https://github.com/SJTU-mxtao/Semantic-Communication-Systems.

</p>
</details>

<details><summary><b>PGD: A Large-scale Professional Go Dataset for Data-driven Analytics</b>
<a href="https://arxiv.org/abs/2205.00254">arxiv:2205.00254</a>
&#x1F4C8; 0 <br>
<p>Yifan Gao</p></summary>
<p>

**Abstract:** Lee Sedol is on a winning streak--does this legend rise again after the competition with AlphaGo? Ke Jie is invincible in the world championship--can he still win the title this time? Go is one of the most popular board games in East Asia, with a stable professional sports system that has lasted for decades in China, Japan, and Korea. There are mature data-driven analysis technologies for many sports, such as soccer, basketball, and esports. However, developing such technology for Go remains nontrivial and challenging due to the lack of datasets, meta-information, and in-game statistics. This paper creates the Professional Go Dataset (PGD), containing 98,043 games played by 2,148 professional players from 1950 to 2021. After manual cleaning and labeling, we provide detailed meta-information for each player, game, and tournament. Moreover, the dataset includes analysis results for each move in the match evaluated by advanced AlphaZero-based AI. To establish a benchmark for PGD, we further analyze the data and extract meaningful in-game features based on prior knowledge related to Go that can indicate the game status. With the help of complete meta-information and constructed in-game features, our results prediction system achieves an accuracy of 75.30%, much higher than several state-of-the-art approaches (64%-65%). As far as we know, PGD is the first dataset for data-driven analytics in Go and even in board games. Beyond this promising result, we provide more examples of tasks that benefit from our dataset. The ultimate goal of this paper is to bridge this ancient game and the modern data science community. It will advance research on Go-related analytics to enhance the fan experience, help players improve their ability, and facilitate other promising aspects. The dataset will be made publicly available.

</p>
</details>

<details><summary><b>An Initial Look at Self-Reprogramming Artificial Intelligence</b>
<a href="https://arxiv.org/abs/2205.00167">arxiv:2205.00167</a>
&#x1F4C8; 0 <br>
<p>Alex Sheng</p></summary>
<p>

**Abstract:** Rapid progress in deep learning research has greatly extended the capabilities of artificial intelligence technology. Conventional AI models are constrained to explicit human-designed algorithms, although a growing body of work in meta-learning, neural architecture search, and related approaches have explored algorithms that self-modify to some extent. In this paper, we develop and experimentally validate the first fully self-reprogramming AI system. Applying AI-based computer code generation to AI itself, we implement an algorithm with the ability to continuously modify and rewrite its own neural network source code.

</p>
</details>


{% endraw %}
Prev: [2022.04.29]({{ '/2022/04/29/2022.04.29.html' | relative_url }})  Next: [2022.05.01]({{ '/2022/05/01/2022.05.01.html' | relative_url }})