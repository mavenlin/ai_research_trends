Prev: [2022.03.16]({{ '/2022/03/16/2022.03.16.html' | relative_url }})  Next: [2022.03.18]({{ '/2022/03/18/2022.03.18.html' | relative_url }})
{% raw %}
## Summary for 2022-03-17, created on 2022-03-27


<details><summary><b>Monotonic Differentiable Sorting Networks</b>
<a href="https://arxiv.org/abs/2203.09630">arxiv:2203.09630</a>
&#x1F4C8; 116000 <br>
<p>Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen</p></summary>
<p>

**Abstract:** Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current differentiable sorting methods is that they are non-monotonic. To address this issue, we propose a novel relaxation of conditional swap operations that guarantees monotonicity in differentiable sorting networks. We introduce a family of sigmoid functions and prove that they produce differentiable sorting networks that are monotonic. Monotonicity ensures that the gradients always have the correct sign, which is an advantage in gradient-based optimization. We demonstrate that monotonic differentiable sorting networks improve upon previous differentiable sorting methods.

</p>
</details>

<details><summary><b>Nearest Neighbor Classifier with Margin Penalty for Active Learning</b>
<a href="https://arxiv.org/abs/2203.09174">arxiv:2203.09174</a>
&#x1F4C8; 103 <br>
<p>Yuan Cao, Zhiqiao Gao, Jie Hu, Mingchuan Yang</p></summary>
<p>

**Abstract:** As deep learning becomes the mainstream in the field of natural language processing, the need for suitable active learning method are becoming unprecedented urgent. Active Learning (AL) methods based on nearest neighbor classifier are proposed and demonstrated superior results. However, existing nearest neighbor classifier are not suitable for classifying mutual exclusive classes because inter-class discrepancy cannot be assured by nearest neighbor classifiers. As a result, informative samples in the margin area can not be discovered and AL performance are damaged. To this end, we propose a novel Nearest neighbor Classifier with Margin penalty for Active Learning(NCMAL). Firstly, mandatory margin penalty are added between classes, therefore both inter-class discrepancy and intra-class compactness are both assured. Secondly, a novel sample selection strategy are proposed to discover informative samples within the margin area. To demonstrate the effectiveness of the methods, we conduct extensive experiments on for datasets with other state-of-the-art methods. The experimental results demonstrate that our method achieves better results with fewer annotated samples than all baseline methods.

</p>
</details>

<details><summary><b>One-Shot Adaptation of GAN in Just One CLIP</b>
<a href="https://arxiv.org/abs/2203.09301">arxiv:2203.09301</a>
&#x1F4C8; 92 <br>
<p>Gihyun Kwon, Jong Chul Ye</p></summary>
<p>

**Abstract:** There are many recent research efforts to fine-tune a pre-trained generator with a few target images to generate images of a novel domain. Unfortunately, these methods often suffer from overfitting or under-fitting when fine-tuned with a single target image. To address this, here we present a novel single-shot GAN adaptation method through unified CLIP space manipulations. Specifically, our model employs a two-step training strategy: reference image search in the source generator using a CLIP-guided latent optimization, followed by generator fine-tuning with a novel loss function that imposes CLIP space consistency between the source and adapted generators. To further improve the adapted model to produce spatially consistent samples with respect to the source generator, we also propose contrastive regularization for patchwise relationships in the CLIP space. Experimental results show that our model generates diverse outputs with the target texture and outperforms the baseline models both qualitatively and quantitatively. Furthermore, we show that our CLIP space manipulation strategy allows more effective attribute editing.

</p>
</details>

<details><summary><b>Do Deep Networks Transfer Invariances Across Classes?</b>
<a href="https://arxiv.org/abs/2203.09739">arxiv:2203.09739</a>
&#x1F4C8; 90 <br>
<p>Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn</p></summary>
<p>

**Abstract:** To generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have "class-agnostic" nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many real-world datasets are heavily class imbalanced and contain only a few examples for most of the classes. We therefore pose the question: how well do neural networks transfer class-agnostic invariances learned from the large classes to the small ones? Through careful experimentation, we observe that invariance to class-agnostic transformations is still heavily dependent on class size, with the networks being much less invariant on smaller classes. This result holds even when using data balancing techniques, and suggests poor invariance transfer across classes. Our results provide one explanation for why classifiers generalize poorly on unbalanced and long-tailed distributions. Based on this analysis, we show how a generative approach for learning the nuisance transformations can help transfer invariances across classes and improve performance on a set of imbalanced image classification benchmarks. Source code for our experiments is available at https://github.com/AllanYangZhou/generative-invariance-transfer.

</p>
</details>

<details><summary><b>RoMe: A Robust Metric for Evaluating Natural Language Generation</b>
<a href="https://arxiv.org/abs/2203.09183">arxiv:2203.09183</a>
&#x1F4C8; 48 <br>
<p>Md Rashad Al Hasan Rony, Liubov Kovriguina, Debanjan Chaudhuri, Ricardo Usbeck, Jens Lehmann</p></summary>
<p>

**Abstract:** Evaluating Natural Language Generation (NLG) systems is a challenging task. Firstly, the metric should ensure that the generated hypothesis reflects the reference's semantics. Secondly, it should consider the grammatical quality of the generated sentence. Thirdly, it should be robust enough to handle various surface forms of the generated sentence. Thus, an effective evaluation metric has to be multifaceted. In this paper, we propose an automatic evaluation metric incorporating several core aspects of natural language understanding (language competence, syntactic and semantic variation). Our proposed metric, RoMe, is trained on language features such as semantic similarity combined with tree edit distance and grammatical acceptability, using a self-supervised neural network to assess the overall quality of the generated sentence. Moreover, we perform an extensive robustness analysis of the state-of-the-art methods and RoMe. Empirical results suggest that RoMe has a stronger correlation to human judgment over state-of-the-art metrics in evaluating system-generated sentences across several NLG tasks.

</p>
</details>

<details><summary><b>Transframer: Arbitrary Frame Prediction with Generative Models</b>
<a href="https://arxiv.org/abs/2203.09494">arxiv:2203.09494</a>
&#x1F4C8; 39 <br>
<p>Charlie Nash, Jo√£o Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, Peter Battaglia</p></summary>
<p>

**Abstract:** We present a general-purpose framework for image modelling and vision tasks based on probabilistic frame prediction. Our approach unifies a broad range of tasks, from image segmentation, to novel view synthesis and video interpolation. We pair this framework with an architecture we term Transframer, which uses U-Net and Transformer components to condition on annotated context frames, and outputs sequences of sparse, compressed image features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is competitive with the strongest models on few-shot view synthesis, and can generate coherent 30 second videos from a single image without any explicit geometric information. A single generalist Transframer simultaneously produces promising results on 8 tasks, including semantic segmentation, image classification and optical flow prediction with no task-specific architectural components, demonstrating that multi-task computer vision can be tackled using probabilistic image models. Our approach can in principle be applied to a wide range of applications that require learning the conditional structure of annotated image-formatted data.

</p>
</details>

<details><summary><b>AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation</b>
<a href="https://arxiv.org/abs/2203.09516">arxiv:2203.09516</a>
&#x1F4C8; 20 <br>
<p>Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, Shubham Tulsiani</p></summary>
<p>

**Abstract:** Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the effectiveness of the proposed method using both quantitative and qualitative evaluation and show that the proposed method outperforms the specialized state-of-the-art methods trained for individual tasks. The project page with code and video visualizations can be found at https://yccyenchicheng.github.io/AutoSDF/.

</p>
</details>

<details><summary><b>Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations</b>
<a href="https://arxiv.org/abs/2203.09697">arxiv:2203.09697</a>
&#x1F4C8; 14 <br>
<p>Anuroop Sriram, Abhishek Das, Brandon M. Wood, Siddharth Goyal, C. Lawrence Zitnick</p></summary>
<p>

**Abstract:** Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric for the S2EF task and 2) 21% on the AFbT metric for the IS2RS task, establishing new state-of-the-art results.

</p>
</details>

<details><summary><b>A Learning Framework for Bandwidth-Efficient Distributed Inference in Wireless IoT</b>
<a href="https://arxiv.org/abs/2203.09631">arxiv:2203.09631</a>
&#x1F4C8; 10 <br>
<p>Mostafa Hussien, Kim Khoa Nguyen, Mohamed Cheriet</p></summary>
<p>

**Abstract:** In wireless Internet of things (IoT), the sensors usually have limited bandwidth and power resources. Therefore, in a distributed setup, each sensor should compress and quantize the sensed observations before transmitting them to a fusion center (FC) where a global decision is inferred. Most of the existing compression techniques and entropy quantizers consider only the reconstruction fidelity as a metric, which means they decouple the compression from the sensing goal. In this work, we argue that data compression mechanisms and entropy quantizers should be co-designed with the sensing goal, specifically for machine-consumed data. To this end, we propose a novel deep learning-based framework for compressing and quantizing the observations of correlated sensors. Instead of maximizing the reconstruction fidelity, our objective is to compress the sensor observations in a way that maximizes the accuracy of the inferred decision (i.e., sensing goal) at the FC. Unlike prior work, we do not impose any assumptions about the observations distribution which emphasizes the wide applicability of our framework. We also propose a novel loss function that keeps the model focused on learning complementary features at each sensor. The results show the superior performance of our framework compared to other benchmark models.

</p>
</details>

<details><summary><b>Triangle and Four Cycle Counting with Predictions in Graph Streams</b>
<a href="https://arxiv.org/abs/2203.09572">arxiv:2203.09572</a>
&#x1F4C8; 10 <br>
<p>Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner, David P. Woodruff, Michael Zhang</p></summary>
<p>

**Abstract:** We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, (Hsu 2018) and (Jiang 2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior "classical" algorithms that did not use oracles. In this paper, we explore the power of a "heavy edge" oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on "classical" streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms.

</p>
</details>

<details><summary><b>Finding Structural Knowledge in Multimodal-BERT</b>
<a href="https://arxiv.org/abs/2203.09306">arxiv:2203.09306</a>
&#x1F4C8; 10 <br>
<p>Victor Milewski, Miryam de Lhoneux, Marie-Francine Moens</p></summary>
<p>

**Abstract:** In this work, we investigate the knowledge learned in the embeddings of multimodal-BERT models. More specifically, we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data. To reach that goal, we first make the inherent structure of language and visuals explicit by a dependency parse of the sentences that describe the image and by the dependencies between the object regions in the image, respectively. We call this explicit visual structure the \textit{scene tree}, that is based on the dependency tree of the language description. Extensive probing experiments show that the multimodal-BERT models do not encode these scene trees.Code available at \url{https://github.com/VSJMilewski/multimodal-probes}.

</p>
</details>

<details><summary><b>On Multi-Domain Long-Tailed Recognition, Generalization and Beyond</b>
<a href="https://arxiv.org/abs/2203.09513">arxiv:2203.09513</a>
&#x1F4C8; 9 <br>
<p>Yuzhe Yang, Hao Wang, Dina Katabi</p></summary>
<p>

**Abstract:** Real-world data often exhibit imbalanced label distributions. Existing studies on data imbalance focus on single-domain settings, i.e., samples are from the same data distribution. However, natural data can originate from distinct domains, where a minority class in one domain could have abundant instances from other domains. We formalize the task of Multi-Domain Long-Tailed Recognition (MDLT), which learns from multi-domain imbalanced data, addresses label imbalance, domain shift, and divergent label distributions across domains, and generalizes to all domain-class pairs. We first develop the domain-class transferability graph, and show that such transferability governs the success of learning in MDLT. We then propose BoDA, a theoretically grounded learning strategy that tracks the upper bound of transferability statistics, and ensures balanced alignment and calibration across imbalanced domain-class distributions. We curate five MDLT benchmarks based on widely-used multi-domain datasets, and compare BoDA to twenty algorithms that span different learning strategies. Extensive and rigorous experiments verify the superior performance of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on Domain Generalization benchmarks, improving generalization to unseen domains. Code and data are available at https://github.com/YyzHarry/multi-domain-imbalance.

</p>
</details>

<details><summary><b>DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection</b>
<a href="https://arxiv.org/abs/2203.09510">arxiv:2203.09510</a>
&#x1F4C8; 9 <br>
<p>Jinhyung Park, Chenfeng Xu, Yiyang Zhou, Masayoshi Tomizuka, Wei Zhan</p></summary>
<p>

**Abstract:** While numerous 3D detection works leverage the complementary relationship between RGB images and point clouds, developments in the broader framework of semi-supervised object recognition remain uninfluenced by multi-modal fusion. Current methods develop independent pipelines for 2D and 3D semi-supervised learning despite the availability of paired image and point cloud frames. Observing that the distinct characteristics of each sensor cause them to be biased towards detecting different objects, we propose DetMatch, a flexible framework for joint semi-supervised learning on 2D and 3D modalities. By identifying objects detected in both sensors, our pipeline generates a cleaner, more robust set of pseudo-labels that both demonstrates stronger performance and stymies single-modality error propagation. Further, we leverage the richer semantics of RGB images to rectify incorrect 3D class predictions and improve localization of 3D boxes. Evaluating on the challenging KITTI and Waymo datasets, we improve upon strong semi-supervised learning methods and observe higher quality pseudo-labels. Code will be released at https://github.com/Divadi/DetMatch

</p>
</details>

<details><summary><b>Continual Learning Based on OOD Detection and Task Masking</b>
<a href="https://arxiv.org/abs/2203.09450">arxiv:2203.09450</a>
&#x1F4C8; 9 <br>
<p>Gyuhak Kim, Sepideh Esmaeilpour, Changnan Xiao, Bing Liu</p></summary>
<p>

**Abstract:** Existing continual learning techniques focus on either task incremental learning (TIL) or class incremental learning (CIL) problem, but not both. CIL and TIL differ mainly in that the task-id is provided for each test sample during testing for TIL, but not provided for CIL. Continual learning methods intended for one problem have limitations on the other problem. This paper proposes a novel unified approach based on out-of-distribution (OOD) detection and task masking, called CLOM, to solve both problems. The key novelty is that each task is trained as an OOD detection model rather than a traditional supervised learning model, and a task mask is trained to protect each task to prevent forgetting. Our evaluation shows that CLOM outperforms existing state-of-the-art baselines by large margins. The average TIL/CIL accuracy of CLOM over six experiments is 87.6/67.9% while that of the best baselines is only 82.4/55.0%.

</p>
</details>

<details><summary><b>Euler State Networks</b>
<a href="https://arxiv.org/abs/2203.09382">arxiv:2203.09382</a>
&#x1F4C8; 9 <br>
<p>Claudio Gallicchio</p></summary>
<p>

**Abstract:** Inspired by the numerical solution of ordinary differential equations, in this paper we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The introduced approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.
  Our mathematical analysis shows that the resulting model is biased towards unitary effective spectral radius and zero local Lyapunov exponents, intrinsically operating at the edge of stability. Experiments on synthetic tasks indicate the marked superiority of the proposed approach, compared to standard RC models, in tasks requiring long-term memorization skills. Furthermore, results on real-world time series classification benchmarks point out that EuSN is capable of matching (or even surpassing) the level of accuracy of trainable Recurrent Neural Networks, while allowing up to 100-fold savings in computation time and energy consumption.

</p>
</details>

<details><summary><b>EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training</b>
<a href="https://arxiv.org/abs/2203.09313">arxiv:2203.09313</a>
&#x1F4C8; 9 <br>
<p>Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan Zhu, Jie Tang, Minlie Huang</p></summary>
<p>

**Abstract:** Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and make our models and code publicly available. To our knowledge, EVA2.0 is the largest open-source Chinese dialogue model. Automatic and human evaluations show that our model significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future directions.

</p>
</details>

<details><summary><b>Symmetry-Based Representations for Artificial and Biological General Intelligence</b>
<a href="https://arxiv.org/abs/2203.09250">arxiv:2203.09250</a>
&#x1F4C8; 9 <br>
<p>Irina Higgins, S√©bastien Racani√®re, Danilo Rezende</p></summary>
<p>

**Abstract:** Biological intelligence is remarkable in its ability to produce complex behaviour in many diverse situations through data efficient, generalisable and transferable skill acquisition. It is believed that learning "good" sensory representations is important for enabling this, however there is little agreement as to what a good representation should look like. In this review article we are going to argue that symmetry transformations are a fundamental principle that can guide our search for what makes a good representation. The idea that there exist transformations (symmetries) that affect some aspects of the system but not others, and their relationship to conserved quantities has become central in modern physics, resulting in a more unified theoretical framework and even ability to predict the existence of new particles. Recently, symmetries have started to gain prominence in machine learning too, resulting in more data efficient and generalisable algorithms that can mimic some of the complex behaviours produced by biological intelligence. Finally, first demonstrations of the importance of symmetry transformations for representation learning in the brain are starting to arise in neuroscience. Taken together, the overwhelming positive effect that symmetries bring to these disciplines suggest that they may be an important general framework that determines the structure of the universe, constrains the nature of natural tasks and consequently shapes both biological and artificial intelligence.

</p>
</details>

<details><summary><b>Generative Principal Component Analysis</b>
<a href="https://arxiv.org/abs/2203.09693">arxiv:2203.09693</a>
&#x1F4C8; 8 <br>
<p>Zhaoqiang Liu, Jiulong Liu, Subhroshekhar Ghosh, Jun Han, Jonathan Scarlett</p></summary>
<p>

**Abstract:** In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\sqrt{\frac{k\log L}{m}}$, where $m$ is the number of samples. We also provide a near-matching algorithm-independent lower bound. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis.

</p>
</details>

<details><summary><b>The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents</b>
<a href="https://arxiv.org/abs/2203.09498">arxiv:2203.09498</a>
&#x1F4C8; 8 <br>
<p>Patrick M. Pilarski, Andrew Butcher, Elnaz Davoodi, Michael Bradley Johanson, Dylan J. A. Brenneis, Adam S. R. Parker, Leslie Acker, Matthew M. Botvinick, Joseph Modayil, Adam White</p></summary>
<p>

**Abstract:** Learned communication between agents is a powerful tool when approaching decision-making problems that are hard to overcome by any single agent in isolation. However, continual coordination and communication learning between machine agents or human-machine partnerships remains a challenging open problem. As a stepping stone toward solving the continual communication learning problem, in this paper we contribute a multi-faceted study into what we term Pavlovian signalling -- a process by which learned, temporally extended predictions made by one agent inform decision-making by another agent with different perceptual access to their shared environment. We seek to establish how different temporal processes and representational choices impact Pavlovian signalling between learning agents. To do so, we introduce a partially observable decision-making domain we call the Frost Hollow. In this domain a prediction learning agent and a reinforcement learning agent are coupled into a two-part decision-making system that seeks to acquire sparse reward while avoiding time-conditional hazards. We evaluate two domain variations: 1) machine prediction and control learning in a linear walk, and 2) a prediction learning machine interacting with a human participant in a virtual reality environment. Our results showcase the speed of learning for Pavlovian signalling, the impact that different temporal representations do (and do not) have on agent-agent coordination, and how temporal aliasing impacts agent-agent and human-agent interactions differently. As a main contribution, we establish Pavlovian signalling as a natural bridge between fixed signalling paradigms and fully adaptive communication learning. Our results therefore point to an actionable, constructivist path towards continual communication learning between reinforcement learning agents, with potential impact in a range of real-world settings.

</p>
</details>

<details><summary><b>STICC: A multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity</b>
<a href="https://arxiv.org/abs/2203.09611">arxiv:2203.09611</a>
&#x1F4C8; 7 <br>
<p>Yuhao Kang, Kunlin Wu, Song Gao, Ignavier Ng, Jinmeng Rao, Shan Ye, Fan Zhang, Teng Fei</p></summary>
<p>

**Abstract:** Spatial clustering has been widely used for spatial data mining and knowledge discovery. An ideal multivariate spatial clustering should consider both spatial contiguity and aspatial attributes. Existing spatial clustering approaches may face challenges for discovering repeated geographic patterns with spatial contiguity maintained. In this paper, we propose a Spatial Toeplitz Inverse Covariance-Based Clustering (STICC) method that considers both attributes and spatial relationships of geographic objects for multivariate spatial clustering. A subregion is created for each geographic object serving as the basic unit when performing clustering. A Markov random field is then constructed to characterize the attribute dependencies of subregions. Using a spatial consistency strategy, nearby objects are encouraged to belong to the same cluster. To test the performance of the proposed STICC algorithm, we apply it in two use cases. The comparison results with several baseline methods show that the STICC outperforms others significantly in terms of adjusted rand index and macro-F1 score. Join count statistics is also calculated and shows that the spatial contiguity is well preserved by STICC. Such a spatial clustering method may benefit various applications in the fields of geography, remote sensing, transportation, and urban planning, etc.

</p>
</details>

<details><summary><b>Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation</b>
<a href="https://arxiv.org/abs/2203.09553">arxiv:2203.09553</a>
&#x1F4C8; 7 <br>
<p>Kai Zhang, Yu Wang, Hongyi Wang, Lifu Huang, Carl Yang, Lichao Sun</p></summary>
<p>

**Abstract:** Federated Learning (FL) on knowledge graphs (KGs) has yet to be as well studied as other domains, such as computer vision and natural language processing. A recent study FedE first proposes an FL framework that shares entity embeddings of KGs across all clients. However, compared with model sharing in vanilla FL, entity embedding sharing from FedE would incur severe privacy leakage. Specifically, the known entity embedding can be used to infer whether a specific relation between two entities exists in a private client. In this paper, we first develop a novel attack that aims to recover the original data based on embedding information, which is further used to evaluate the vulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm with privacy-preserving Relation embedding aggregation (FedR) to tackle the privacy issue in FedE. Compared to entity embedding sharing, relation embedding sharing policy can significantly reduce the communication cost due to its smaller size of queries. We conduct extensive experiments to evaluate FedR with five different embedding learning models and three benchmark KG datasets. Compared to FedE, FedR achieves similar utility and significant (nearly 2X) improvements in both privacy and efficiency on link prediction task.

</p>
</details>

<details><summary><b>Knowledge Graph Embedding Methods for Entity Alignment: An Experimental Review</b>
<a href="https://arxiv.org/abs/2203.09280">arxiv:2203.09280</a>
&#x1F4C8; 7 <br>
<p>Nikolaos Fanourakis, Vasilis Efthymiou, Dimitris Kotzinos, Vassilis Christophides</p></summary>
<p>

**Abstract:** In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods' effectiveness and efficiency.

</p>
</details>

<details><summary><b>Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed</b>
<a href="https://arxiv.org/abs/2203.09179">arxiv:2203.09179</a>
&#x1F4C8; 7 <br>
<p>Toni Karvonen, Chris J. Oates</p></summary>
<p>

**Abstract:** Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed. That is, when the predictions of the regression model are continuous (or insensitive to small perturbations) in the training data. This article presents a rigorous proof that the maximum likelihood estimator fails to be well-posed in Hellinger distance in a scenario where the data are noiseless. The failure case occurs for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is informally well-known, these theoretical results appear to be the first of their kind, and suggest that well-posedness may need to be assessed post-hoc, on a case-by-case basis, when maximum likelihood estimation is used to train a Gaussian process model.

</p>
</details>

<details><summary><b>How Many Data Samples is an Additional Instruction Worth?</b>
<a href="https://arxiv.org/abs/2203.09161">arxiv:2203.09161</a>
&#x1F4C8; 7 <br>
<p>Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, Chitta Baral</p></summary>
<p>

**Abstract:** Recently introduced instruction-paradigm empowers non-expert users to leverage NLP resources by defining a new task in natural language. Instruction-tuned models have significantly outperformed multitask learning models (without instruction); however they are far from state of the art task specific models. Conventional approaches to improve model performance via creating large datasets with lots of task instances or architectural/training changes in model may not be feasible for non-expert users. However, they can write alternate instructions to represent an instruction task. Is Instruction-augumentation helpful? We augment a subset of tasks in the expanded version of NATURAL INSTRUCTIONS with additional instructions and find that these significantly improve model performance (up to 35%), especially in the low-data regime. Our results indicate that an additional instruction can be equivalent to ~200 data samples on average across tasks.

</p>
</details>

<details><summary><b>Low-degree learning and the metric entropy of polynomials</b>
<a href="https://arxiv.org/abs/2203.09659">arxiv:2203.09659</a>
&#x1F4C8; 6 <br>
<p>Alexandros Eskenazis, Paata Ivanisvili, Lauritz Streck</p></summary>
<p>

**Abstract:** Let $\mathscr{F}_{n,d}$ be the class of all functions $f:\{-1,1\}^n\to[-1,1]$ on the $n$-dimensional discrete hypercube of degree at most $d$. In the first part of this paper, we prove that any (deterministic or randomized) algorithm which learns $\mathscr{F}_{n,d}$ with $L_2$-accuracy $\varepsilon$ requires at least $Œ©((1-\sqrt{\varepsilon})2^d\log n)$ queries for large enough $n$, thus establishing the sharpness as $n\to\infty$ of a recent upper bound of Eskenazis and Ivanisvili (2021). To do this, we show that the $L_2$-packing numbers $\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon)$ of the concept class $\mathscr{F}_{n,d}$ satisfy the two-sided estimate $$c(1-\varepsilon)2^d\log n \leq \log \mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon) \leq \frac{2^{Cd}\log n}{\varepsilon^4}$$ for large enough $n$, where $c, C>0$ are universal constants. In the second part of the paper, we present a logarithmic upper bound for the randomized query complexity of classes of bounded approximate polynomials whose Fourier spectra are concentrated on few subsets. As an application, we prove new estimates for the number of random queries required to learn approximate juntas of a given degree, functions with rapidly decaying Fourier tails and constant depth circuits of given size. Finally, we obtain bounds for the number of queries required to learn the polynomial class $\mathscr{F}_{n,d}$ without error in the query and random example models.

</p>
</details>

<details><summary><b>Image Super-Resolution With Deep Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2203.09445">arxiv:2203.09445</a>
&#x1F4C8; 6 <br>
<p>Darius Chira, Ilian Haralampiev, Ole Winther, Andrea Dittadi, Valentin Li√©vin</p></summary>
<p>

**Abstract:** Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. Models based on Variational Autoencoders (VAEs) have often been criticized for their feeble generative performance, but with new advancements such as VDVAE (very deep VAE), there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon image super-resolution using transfer learning on pretrained VDVAEs. Through qualitative and quantitative evaluations, we show that the proposed model is competitive with other state-of-the-art methods.

</p>
</details>

<details><summary><b>POSTER: Diagnosis of COVID-19 through Transfer Learning Techniques on CT Scans: A Comparison of Deep Learning Models</b>
<a href="https://arxiv.org/abs/2203.09348">arxiv:2203.09348</a>
&#x1F4C8; 6 <br>
<p>Aeyan Ashraf, Asad Malik, Zahid Khan</p></summary>
<p>

**Abstract:** The novel coronavirus disease (COVID-19) constitutes a public health emergency globally. It is a deadly disease which has infected more than 230 million people worldwide. Therefore, early and unswerving detection of COVID-19 is necessary. Evidence of this virus is most commonly being tested by RT-PCR test. This test is not 100% reliable as it is known to give false positives and false negatives. Other methods like X-Ray images or CT scans show the detailed imaging of lungs and have been proven more reliable. This paper compares different deep learning models used to detect COVID-19 through transfer learning technique on CT scan dataset. VGG-16 outperforms all the other models achieving an accuracy of 85.33% on the dataset.

</p>
</details>

<details><summary><b>On the Spectral Bias of Convolutional Neural Tangent and Gaussian Process Kernels</b>
<a href="https://arxiv.org/abs/2203.09255">arxiv:2203.09255</a>
&#x1F4C8; 6 <br>
<p>Amnon Geifman, Meirav Galun, David Jacobs, Ronen Basri</p></summary>
<p>

**Abstract:** We study the properties of various over-parametrized convolutional neural architectures through their respective Gaussian process and neural tangent kernels. We prove that, with normalized multi-channel input and ReLU activation, the eigenfunctions of these kernels with the uniform measure are formed by products of spherical harmonics, defined over the channels of the different pixels. We next use hierarchical factorizable kernels to bound their respective eigenvalues. We show that the eigenvalues decay polynomially, quantify the rate of decay, and derive measures that reflect the composition of hierarchical features in these networks. Our results provide concrete quantitative characterization of over-parameterized convolutional network architectures.

</p>
</details>

<details><summary><b>Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning</b>
<a href="https://arxiv.org/abs/2203.09249">arxiv:2203.09249</a>
&#x1F4C8; 6 <br>
<p>Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, Ling-Yu Duan</p></summary>
<p>

**Abstract:** Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data heterogeneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most existing approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model aggregation. Instead, we propose a data-free knowledge distillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggregation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Besides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowledge, which implicitly mitigates the distribution discrepancy across clients. Extensive experiments show that our FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and SCAFFOLD.

</p>
</details>

<details><summary><b>Active Visuo-Haptic Object Shape Completion</b>
<a href="https://arxiv.org/abs/2203.09149">arxiv:2203.09149</a>
&#x1F4C8; 6 <br>
<p>Lukas Rustler, Jens Lundell, Jan Kristof Behrens, Ville Kyrki, Matej Hoffmann</p></summary>
<p>

**Abstract:** Recent advancements in object shape completion have enabled impressive object reconstructions using only visual input. However, due to self-occlusion, the reconstructions have high uncertainty in the occluded object parts, which negatively impacts the performance of downstream robotic tasks such as grasping. In this work, we propose an active visuo-haptic shape completion method called Act-VH that actively computes where to touch the objects based on the reconstruction uncertainty. Act-VH reconstructs objects from point clouds and calculates the reconstruction uncertainty using IGR, a recent state-of-the-art implicit surface deep neural network. We experimentally evaluate the reconstruction accuracy of Act-VH against five baselines in simulation and in the real world. We also propose a new simulation environment for this purpose. The results show that Act-VH outperforms all baselines and that an uncertainty-driven haptic exploration policy leads to higher reconstruction accuracy than a random policy and a policy driven by Gaussian Process Implicit Surfaces. As a final experiment, we evaluate Act-VH and the best reconstruction baseline on grasping 10 novel objects. The results show that Act-VH reaches a significantly higher grasp success rate than the baseline on all objects. Together, this work opens up the door for using active visuo-haptic shape completion in more complex cluttered scenes.

</p>
</details>

<details><summary><b>Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning</b>
<a href="https://arxiv.org/abs/2203.09137">arxiv:2203.09137</a>
&#x1F4C8; 6 <br>
<p>Haoxiang Wang, Yite Wang, Ruoyu Sun, Bo Li</p></summary>
<p>

**Abstract:** Model-agnostic meta-learning (MAML) and its variants have become popular approaches for few-shot learning. However, due to the non-convexity of deep neural nets (DNNs) and the bi-level formulation of MAML, the theoretical properties of MAML with DNNs remain largely unknown. In this paper, we first prove that MAML with over-parameterized DNNs is guaranteed to converge to global optima at a linear rate. Our convergence analysis indicates that MAML with over-parameterized DNNs is equivalent to kernel regression with a novel class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then, we propose MetaNTK-NAS, a new training-free neural architecture search (NAS) method for few-shot learning that uses MetaNTK to rank and select architectures. Empirically, we compare our MetaNTK-NAS with previous NAS methods on two popular few-shot learning benchmarks, miniImageNet, and tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or better than the state-of-the-art NAS method designed for few-shot learning while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS makes itself more practical for many real-world tasks.

</p>
</details>

<details><summary><b>Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input</b>
<a href="https://arxiv.org/abs/2203.09123">arxiv:2203.09123</a>
&#x1F4C8; 6 <br>
<p>Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Hee-Seon Kim, Changick Kim</p></summary>
<p>

**Abstract:** The transferability of adversarial examples allows the deception on black-box models, and transfer-based targeted attacks have attracted a lot of interest due to their practical applicability. To maximize the transfer success rate, adversarial examples should avoid overfitting to the source model, and image augmentation is one of the primary approaches for this. However, prior works utilize simple image transformations such as resizing, which limits input diversity. To tackle this limitation, we propose the object-based diverse input (ODI) method that draws an adversarial image on a 3D object and induces the rendered image to be classified as the target class. Our motivation comes from the humans' superior perception of an image printed on a 3D object. If the image is clear enough, humans can recognize the image content in a variety of viewing conditions. Likewise, if an adversarial example looks like the target class to the model, the model should also classify the rendered image of the 3D object as the target class. The ODI method effectively diversifies the input by leveraging an ensemble of multiple source objects and randomizing viewing conditions. In our experimental results on the ImageNet-Compatible dataset, this method boosts the average targeted attack success rate from 28.3% to 47.0% compared to the state-of-the-art methods. We also demonstrate the applicability of the ODI method to adversarial examples on the face verification task and its superior performance improvement. Our code is available at https://github.com/dreamflake/ODI.

</p>
</details>

<details><summary><b>Label conditioned segmentation</b>
<a href="https://arxiv.org/abs/2203.10091">arxiv:2203.10091</a>
&#x1F4C8; 5 <br>
<p>Tianyu Ma, Benjamin C. Lee, Mert R. Sabuncu</p></summary>
<p>

**Abstract:** Semantic segmentation is an important task in computer vision that is often tackled with convolutional neural networks (CNNs). A CNN learns to produce pixel-level predictions through training on pairs of images and their corresponding ground-truth segmentation labels. For segmentation tasks with multiple classes, the standard approach is to use a network that computes a multi-channel probabilistic segmentation map, with each channel representing one class. In applications where the image grid size (e.g., when it is a 3D volume) and/or the number of labels is relatively large, the standard (baseline) approach can become prohibitively expensive for our computational resources. In this paper, we propose a simple yet effective method to address this challenge. In our approach, the segmentation network produces a single-channel output, while being conditioned on a single class label, which determines the output class of the network. Our method, called label conditioned segmentation (LCS), can be used to segment images with a very large number of classes, which might be infeasible for the baseline approach. We also demonstrate in the experiments that label conditioning can improve the accuracy of a given backbone architecture, likely, thanks to its parameter efficiency. Finally, as we show in our results, an LCS model can produce previously unseen fine-grained labels during inference time, when only coarse labels were available during training. We provide all of our code here: https://github.com/tym002/Label-conditioned-segmentation

</p>
</details>

<details><summary><b>Deterministic Bridge Regression for Compressive Classification</b>
<a href="https://arxiv.org/abs/2203.09721">arxiv:2203.09721</a>
&#x1F4C8; 5 <br>
<p>Kar-Ann Toh, Giuseppe Molteni, Zhiping Lin</p></summary>
<p>

**Abstract:** Pattern classification with compact representation is an important component in machine intelligence. In this work, an analytic bridge solution is proposed for compressive classification. The proposal has been based upon solving a penalized error formulation utilizing an approximated $\ell_p$-norm. The solution comes in a primal form for over-determined systems and in a dual form for under-determined systems. While the primal form is suitable for problems of low dimension with large data samples, the dual form is suitable for problems of high dimension but with a small number of data samples. The solution has also been extended for problems with multiple classification outputs. Numerical studies based on simulated and real-world data validated the effectiveness of the proposed solution.

</p>
</details>

<details><summary><b>Modeling Intensification for Sign Language Generation: A Computational Approach</b>
<a href="https://arxiv.org/abs/2203.09679">arxiv:2203.09679</a>
&#x1F4C8; 5 <br>
<p>Mert ƒ∞nan, Yang Zhong, Sabit Hassan, Lorna Quandt, Malihe Alikhani</p></summary>
<p>

**Abstract:** End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present different strategies grounded in linguistics of sign language that inform how intensity modifiers can be represented in gloss annotations. To employ our strategies, we first annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset, with different levels of intensification. We then use a supervised intensity tagger to extend the annotated dataset and obtain labels for the remaining portion of it. This enhanced dataset is then used to train state-of-the-art transformer models for sign language generation. We find that our efforts in intensification modeling yield better results when evaluated with automatic metrics. Human evaluation also indicates a higher preference of the videos generated using our model.

</p>
</details>

<details><summary><b>A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory</b>
<a href="https://arxiv.org/abs/2203.09674">arxiv:2203.09674</a>
&#x1F4C8; 5 <br>
<p>Devin A. Rippner, Pranav Raja, J. Mason Earles, Alexander Buchko, Mina Momayyezi, Fiona Duong, Dilworth Parkinson, Elizabeth Forrestel, Ken Shackel, Andrew J. McElrone</p></summary>
<p>

**Abstract:** X-ray micro-computed tomography (X-ray microCT) has enabled the characterization of the properties and processes that take place in plants and soils at the micron scale. Despite the widespread use of this advanced technique, major limitations in both hardware and software limit the speed and accuracy of image processing and data analysis. Recent advances in machine learning, specifically the application of convolutional neural networks to image analysis, have enabled rapid and accurate segmentation of image data. Yet, challenges remain in applying convolutional neural networks to the analysis of environmentally and agriculturally relevant images. Specifically, there is a disconnect between the computer scientists and engineers, who build these AI/ML tools, and the potential end users in agricultural research, who may be unsure of how to apply these tools in their work. Additionally, the computing resources required for training and applying deep learning models are unique, more common to computer gaming systems or graphics design work, than to traditional computational systems. To navigate these challenges, we developed a modular workflow for applying convolutional neural networks to X-ray microCT images, using low-cost resources in Googles Colaboratory web application. Here we present the results of the workflow, illustrating how parameters can be optimized to achieve best results using example scans from walnut leaves, almond flower buds, and a soil aggregate. We expect that this framework will accelerate the adoption and use of emerging deep learning techniques within the plant and soil sciences.

</p>
</details>

<details><summary><b>Inventing Relational State and Action Abstractions for Effective and Efficient Bilevel Planning</b>
<a href="https://arxiv.org/abs/2203.09634">arxiv:2203.09634</a>
&#x1F4C8; 5 <br>
<p>Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas Lozano-Perez, Leslie Pack Kaelbling, Joshua Tenenbaum</p></summary>
<p>

**Abstract:** Effective and efficient planning in continuous state and action spaces is fundamentally hard, even when the transition model is deterministic and known. One way to alleviate this challenge is to perform bilevel planning with abstractions, where a high-level search for abstract plans is used to guide planning in the original transition space. In this paper, we develop a novel framework for learning state and action abstractions that are explicitly optimized for both effective (successful) and efficient (fast) bilevel planning. Given demonstrations of tasks in an environment, our data-efficient approach learns relational, neuro-symbolic abstractions that generalize over object identities and numbers. The symbolic components resemble the STRIPS predicates and operators found in AI planning, and the neural components refine the abstractions into actions that can be executed in the environment. Experimentally, we show across four robotic planning environments that our learned abstractions are able to quickly solve held-out tasks of longer horizons than were seen in the demonstrations, and can even outperform the efficiency of abstractions that we manually specified. We also find that as the planner configuration varies, the learned abstractions adapt accordingly, indicating that our abstraction learning method is both "task-aware" and "planner-aware." Code: https://tinyurl.com/predicators-release

</p>
</details>

<details><summary><b>Learning Distributionally Robust Models at Scale via Composite Optimization</b>
<a href="https://arxiv.org/abs/2203.09607">arxiv:2203.09607</a>
&#x1F4C8; 5 <br>
<p>Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Amin Karbasi</p></summary>
<p>

**Abstract:** To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples -- which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods. We also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets.

</p>
</details>

<details><summary><b>Delta Distillation for Efficient Video Processing</b>
<a href="https://arxiv.org/abs/2203.09594">arxiv:2203.09594</a>
&#x1F4C8; 5 <br>
<p>Amirhossein Habibian, Haitam Ben Yahia, Davide Abati, Efstratios Gavves, Fatih Porikli</p></summary>
<p>

**Abstract:** This paper aims to accelerate video stream processing, such as object detection and semantic segmentation, by leveraging the temporal redundancies that exist between video frames. Instead of propagating and warping features using motion alignment, such as optical flow, we propose a novel knowledge distillation schema coined as Delta Distillation. In our proposal, the student learns the variations in the teacher's intermediate features over time. We demonstrate that these temporal variations can be effectively distilled due to the temporal redundancies within video frames. During inference, both teacher and student cooperate for providing predictions: the former by providing initial representations extracted only on the key-frame, and the latter by iteratively estimating and applying deltas for the successive frames. Moreover, we consider various design choices to learn optimal student architectures including an end-to-end learnable architecture search. By extensive experiments on a wide range of architectures, including the most efficient ones, we demonstrate that delta distillation sets a new state of the art in terms of accuracy vs. efficiency trade-off for semantic segmentation and object detection in videos. Finally, we show that, as a by-product, delta distillation improves the temporal consistency of the teacher model.

</p>
</details>

<details><summary><b>Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations</b>
<a href="https://arxiv.org/abs/2203.09590">arxiv:2203.09590</a>
&#x1F4C8; 5 <br>
<p>Zhen Han, Ruotong Liao, Beiyan Liu, Yao Zhang, Zifeng Ding, Heinz K√∂ppl, Hinrich Sch√ºtze, Volker Tresp</p></summary>
<p>

**Abstract:** With the emerging research effort to integrate structured and unstructured knowledge, many approaches incorporate factual knowledge into pre-trained language models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP tasks. However, (1) they only consider static factual knowledge, but knowledge graphs (KGs) also contain temporal facts or events indicating evolutionary relationships among entities at different timestamps. (2) PLMs cannot be directly applied to many KG tasks, such as temporal KG completion.
  In this paper, we focus on \textbf{e}nhancing temporal knowledge embeddings with \textbf{co}ntextualized \textbf{la}nguage representations (ECOLA). We align structured knowledge contained in temporal knowledge graphs with their textual descriptions extracted from news articles and propose a novel knowledge-text prediction task to inject the abundant information from descriptions into temporal knowledge embeddings. ECOLA jointly optimizes the knowledge-text prediction objective and the temporal knowledge embeddings, which can simultaneously take full advantage of textual and knowledge information. For training ECOLA, we introduce three temporal KG datasets with aligned textual descriptions. Experimental results on the temporal knowledge graph completion task show that ECOLA outperforms state-of-the-art temporal KG models by a large margin. The proposed datasets can serve as new temporal KG benchmarks and facilitate future research on structured and unstructured knowledge integration.

</p>
</details>

<details><summary><b>Towards Data-Efficient Detection Transformers</b>
<a href="https://arxiv.org/abs/2203.09507">arxiv:2203.09507</a>
&#x1F4C8; 5 <br>
<p>Wen Wang, Jing Zhang, Yang Cao, Yongliang Shen, Dacheng Tao</p></summary>
<p>

**Abstract:** Detection Transformers have achieved competitive performance on the sample-rich COCO dataset. However, we show most of them suffer from significant performance drops on small-size datasets, like Cityscapes. In other words, the detection transformers are generally data-hungry. To tackle this problem, we empirically analyze the factors that affect data efficiency, through a step-by-step transition from a data-efficient RCNN variant to the representative DETR. The empirical results suggest that sparse feature sampling from local image areas holds the key. Based on this observation, we alleviate the data-hungry issue of existing detection transformers by simply alternating how key and value sequences are constructed in the cross-attention layer, with minimum modifications to the original models. Besides, we introduce a simple yet effective label augmentation method to provide richer supervision and improve data efficiency. Experiments show that our method can be readily applied to different detection transformers and improve their performance on both small-size and sample-rich datasets. Code will be made publicly available at \url{https://github.com/encounter1997/DE-DETRs}.

</p>
</details>

<details><summary><b>Gaussian initializations help deep variational quantum circuits escape from the barren plateau</b>
<a href="https://arxiv.org/abs/2203.09376">arxiv:2203.09376</a>
&#x1F4C8; 5 <br>
<p>Kaining Zhang, Min-Hsiu Hsieh, Liu Liu, Dacheng Tao</p></summary>
<p>

**Abstract:** Variational quantum circuits have been widely employed in quantum simulation and quantum machine learning in recent years. However, quantum circuits with random structures have poor trainability due to the exponentially vanishing gradient with respect to the circuit depth and the qubit number. This result leads to a general belief that deep quantum circuits will not be feasible for practical tasks. In this work, we propose an initialization strategy with theoretical guarantees for the vanishing gradient problem in general deep circuits. Specifically, we prove that under proper Gaussian initialized parameters, the norm of the gradient decays at most polynomially when the qubit number and the circuit depth increase. Our theoretical results hold for both the local and the global observable cases, where the latter was believed to have vanishing gradients even for shallow circuits. Experimental results verify our theoretical findings in the quantum simulation and quantum chemistry.

</p>
</details>

<details><summary><b>Prediction of speech intelligibility with DNN-based performance measures</b>
<a href="https://arxiv.org/abs/2203.09148">arxiv:2203.09148</a>
&#x1F4C8; 5 <br>
<p>Angel Mario Castro Martinez, Constantin Spille, Jana Ro√übach, Birger Kollmeier, Bernd T. Meyer</p></summary>
<p>

**Abstract:** This paper presents a speech intelligibility model based on automatic speech recognition (ASR), combining phoneme probabilities from deep neural networks (DNN) and a performance measure that estimates the word error rate from these probabilities. This model does not require the clean speech reference nor the word labels during testing as the ASR decoding step, which finds the most likely sequence of words given phoneme posterior probabilities, is omitted. The model is evaluated via the root-mean-squared error between the predicted and observed speech reception thresholds from eight normal-hearing listeners. The recognition task consists of identifying noisy words from a German matrix sentence test. The speech material was mixed with eight noise maskers covering different modulation types, from speech-shaped stationary noise to a single-talker masker. The prediction performance is compared to five established models and an ASR-model using word labels. Two combinations of features and networks were tested. Both include temporal information either at the feature level (amplitude modulation filterbanks and a feed-forward network) or captured by the architecture (mel-spectrograms and a time-delay deep neural network, TDNN). The TDNN model is on par with the DNN while reducing the number of parameters by a factor of 37; this optimization allows parallel streams on dedicated hearing aid hardware as a forward-pass can be computed within the 10ms of each frame. The proposed model performs almost as well as the label-based model and produces more accurate predictions than the baseline models.

</p>
</details>

<details><summary><b>Contrastive Learning with Positive-Negative Frame Mask for Music Representation</b>
<a href="https://arxiv.org/abs/2203.09129">arxiv:2203.09129</a>
&#x1F4C8; 5 <br>
<p>Dong Yao, Zhou Zhao, Shengyu Zhang, Jieming Zhu, Yudong Zhu, Rui Zhang, Xiuqiang He</p></summary>
<p>

**Abstract:** Self-supervised learning, especially contrastive learning, has made an outstanding contribution to the development of many deep learning research fields. Recently, researchers in the acoustic signal processing field noticed its success and leveraged contrastive learning for better music representation. Typically, existing approaches maximize the similarity between two distorted audio segments sampled from the same music. In other words, they ensure a semantic agreement at the music level. However, those coarse-grained methods neglect some inessential or noisy elements at the frame level, which may be detrimental to the model to learn the effective representation of music. Towards this end, this paper proposes a novel Positive-nEgative frame mask for Music Representation based on the contrastive learning framework, abbreviated as PEMR. Concretely, PEMR incorporates a Positive-Negative Mask Generation module, which leverages transformer blocks to generate frame masks on the Log-Mel spectrogram. We can generate self-augmented negative and positive samples by masking important components or inessential components, respectively. We devise a novel contrastive learning objective to accommodate both self-augmented positives/negatives sampled from the same music. We conduct experiments on four public datasets. The experimental results of two music-related downstream tasks, music classification, and cover song identification, demonstrate the generalization ability and transferability of music representation learned by PEMR.

</p>
</details>

<details><summary><b>Knowledge Graph-Enabled Text-Based Automatic Personality Prediction</b>
<a href="https://arxiv.org/abs/2203.09103">arxiv:2203.09103</a>
&#x1F4C8; 5 <br>
<p>Majid Ramezani, Mohammad-Reza Feizi-Derakhshi, Mohammad-Ali Balafar</p></summary>
<p>

**Abstract:** How people think, feel, and behave, primarily is a representation of their personality characteristics. By being conscious of personality characteristics of individuals whom we are dealing with or decided to deal with, one can competently ameliorate the relationship, regardless of its type. With the rise of Internet-based communication infrastructures (social networks, forums, etc.), a considerable amount of human communications take place there. The most prominent tool in such communications, is the language in written and spoken form that adroitly encodes all those essential personality characteristics of individuals. Text-based Automatic Personality Prediction (APP) is the automated forecasting of the personality of individuals based on the generated/exchanged text contents. This paper presents a novel knowledge graph-enabled approach to text-based APP that relies on the Big Five personality traits. To this end, given a text a knowledge graph which is a set of interlinked descriptions of concepts, was built through matching the input text's concepts with DBpedia knowledge base entries. Then, due to achieving more powerful representation the graph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon, and MRC psycholinguistic database information. Afterwards, the knowledge graph which is now a knowledgeable alternative for the input text was embedded to yield an embedding matrix. Finally, to perform personality predictions the resulting embedding matrix was fed to four suggested deep learning models independently, which are based on convolutional neural network (CNN), simple recurrent neural network (RNN), long short term memory (LSTM) and bidirectional long short term memory (BiLSTM). The results indicated a considerable improvements in prediction accuracies in all of the suggested classifiers.

</p>
</details>

<details><summary><b>GAM(L)A: An econometric model for interpretable Machine Learning</b>
<a href="https://arxiv.org/abs/2203.11691">arxiv:2203.11691</a>
&#x1F4C8; 4 <br>
<p>Emmanuel Flachaire, Gilles Hacheme, Sullivan Hu√©, S√©bastien Laurent</p></summary>
<p>

**Abstract:** Despite their high predictive performance, random forest and gradient boosting are often considered as black boxes or uninterpretable models which has raised concerns from practitioners and regulators. As an alternative, we propose in this paper to use partial linear models that are inherently interpretable. Specifically, this article introduces GAM-lasso (GAMLA) and GAM-autometrics (GAMA), denoted as GAM(L)A in short. GAM(L)A combines parametric and non-parametric functions to accurately capture linearities and non-linearities prevailing between dependent and explanatory variables, and a variable selection procedure to control for overfitting issues. Estimation relies on a two-step procedure building upon the double residual method. We illustrate the predictive performance and interpretability of GAM(L)A on a regression and a classification problem. The results show that GAM(L)A outperforms parametric models augmented by quadratic, cubic and interaction effects. Moreover, the results also suggest that the performance of GAM(L)A is not significantly different from that of random forest and gradient boosting.

</p>
</details>

<details><summary><b>PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning</b>
<a href="https://arxiv.org/abs/2203.09735">arxiv:2203.09735</a>
&#x1F4C8; 4 <br>
<p>Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, Chao Zhang</p></summary>
<p>

**Abstract:** Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning -- the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and then discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with fully supervised models. Our Implementation is available at \url{https://github.com/rz-zhang/PRBoost}.

</p>
</details>

<details><summary><b>DeepLSS: breaking parameter degeneracies in large scale structure with deep learning analysis of combined probes</b>
<a href="https://arxiv.org/abs/2203.09616">arxiv:2203.09616</a>
&#x1F4C8; 4 <br>
<p>Tomasz Kacprzak, Janis Fluri</p></summary>
<p>

**Abstract:** In classical cosmological analysis of large scale structure surveys with 2-pt functions, the parameter measurement precision is limited by several key degeneracies within the cosmology and astrophysics sectors. For cosmic shear, clustering amplitude $œÉ_8$ and matter density $Œ©_m$ roughly follow the $S_8=œÉ_8(Œ©_m/0.3)^{0.5}$ relation. In turn, $S_8$ is highly correlated with the intrinsic galaxy alignment amplitude $A_{\rm{IA}}$. For galaxy clustering, the bias $b_g$ is degenerate with both $œÉ_8$ and $Œ©_m$, as well as the stochasticity $r_g$. Moreover, the redshift evolution of IA and bias can cause further parameter confusion. A tomographic 2-pt probe combination can partially lift these degeneracies. In this work we demonstrate that a deep learning analysis of combined probes of weak gravitational lensing and galaxy clustering, which we call DeepLSS, can effectively break these degeneracies and yield significantly more precise constraints on $œÉ_8$, $Œ©_m$, $A_{\rm{IA}}$, $b_g$, $r_g$, and IA redshift evolution parameter $Œ∑_{\rm{IA}}$. The most significant gains are in the IA sector: the precision of $A_{\rm{IA}}$ is increased by approximately 8x and is almost perfectly decorrelated from $S_8$. Galaxy bias $b_g$ is improved by 1.5x, stochasticity $r_g$ by 3x, and the redshift evolution $Œ∑_{\rm{IA}}$ and $Œ∑_b$ by 1.6x. Breaking these degeneracies leads to a significant gain in constraining power for $œÉ_8$ and $Œ©_m$, with the figure of merit improved by 15x. We give an intuitive explanation for the origin of this information gain using sensitivity maps. These results indicate that the fully numerical, map-based forward modeling approach to cosmological inference with machine learning may play an important role in upcoming LSS surveys. We discuss perspectives and challenges in its practical deployment for a full survey analysis.

</p>
</details>

<details><summary><b>Video-based Formative and Summative Assessment of Surgical Tasks using Deep Learning</b>
<a href="https://arxiv.org/abs/2203.09589">arxiv:2203.09589</a>
&#x1F4C8; 4 <br>
<p>Erim Yanik, Uwe Kruger, Xavier Intes, Rahul Rahul, Suvranu De</p></summary>
<p>

**Abstract:** To ensure satisfactory clinical outcomes, surgical skill assessment must be objective, time-efficient, and preferentially automated - none of which is currently achievable. Video-based assessment (VBA) is being deployed in intraoperative and simulation settings to evaluate technical skill execution. However, VBA remains manually- and time-intensive and prone to subjective interpretation and poor inter-rater reliability. Herein, we propose a deep learning (DL) model that can automatically and objectively provide a high-stakes summative assessment of surgical skill execution based on video feeds and low-stakes formative assessment to guide surgical skill acquisition. Formative assessment is generated using heatmaps of visual features that correlate with surgical performance. Hence, the DL model paves the way to the quantitative and reproducible evaluation of surgical tasks from videos with the potential for broad dissemination in surgical training, certification, and credentialing.

</p>
</details>

<details><summary><b>SepTr: Separable Transformer for Audio Spectrogram Processing</b>
<a href="https://arxiv.org/abs/2203.09581">arxiv:2203.09581</a>
&#x1F4C8; 4 <br>
<p>Nicolae-Catalin Ristea, Radu Tudor Ionescu, Fahad Shahbaz Khan</p></summary>
<p>

**Abstract:** Following the successful application of vision transformers in multiple computer vision tasks, these models have drawn the attention of the signal processing community. This is because signals are often represented as spectrograms (e.g. through Discrete Fourier Transform) which can be directly provided as input to vision transformers. However, naively applying transformers to spectrograms is suboptimal. Since the axes represent distinct dimensions, i.e. frequency and time, we argue that a better approach is to separate the attention dedicated to each axis. To this end, we propose the Separable Transformer (SepTr), an architecture that employs two transformer blocks in a sequential manner, the first attending to tokens within the same frequency bin, and the second attending to tokens within the same time interval. We conduct experiments on three benchmark data sets, showing that our separable architecture outperforms conventional vision transformers and other state-of-the-art methods. Unlike standard transformers, SepTr linearly scales the number of trainable parameters with the input size, thus having a lower memory footprint. Our code is available as open source at https://github.com/ristea/septr.

</p>
</details>

<details><summary><b>Outcome Assumptions and Duality Theory for Balancing Weights</b>
<a href="https://arxiv.org/abs/2203.09557">arxiv:2203.09557</a>
&#x1F4C8; 4 <br>
<p>David Bruns-Smith, Avi Feller</p></summary>
<p>

**Abstract:** We study balancing weight estimators, which reweight outcomes from a source population to estimate missing outcomes in a target population. These estimators minimize the worst-case error by making an assumption about the outcome model. In this paper, we show that this outcome assumption has two immediate implications. First, we can replace the minimax optimization problem for balancing weights with a simple convex loss over the assumed outcome function class. Second, we can replace the commonly-made overlap assumption with a more appropriate quantitative measure, the minimum worst-case bias. Finally, we show conditions under which the weights remain robust when our assumptions on the outcomes are wrong.

</p>
</details>

<details><summary><b>SemTUI: a Framework for the Interactive Semantic Enrichment of Tabular Data</b>
<a href="https://arxiv.org/abs/2203.09521">arxiv:2203.09521</a>
&#x1F4C8; 4 <br>
<p>Marco Ripamonti, Flavio De Paoli, Matteo Palmonari</p></summary>
<p>

**Abstract:** The large availability of datasets fosters the use of \acrshort{ml} and \acrshort{ai} technologies to gather insights, study trends, and predict unseen behaviours out of the world of data. Today, gathering and integrating data from different sources is mainly a manual activity that requires the knowledge of expert users at an high cost in terms of both time and money. It is, therefore, necessary to make the process of gathering and linking data from many different sources affordable to make datasets ready to perform the desired analysis. In this work, we propose the development of a comprehensive framework, named SemTUI, to make the enrichment process flexible, complete, and effective through the use of semantics. The approach is to promote fast integration of external services to perform enrichment tasks such as reconciliation and extension; and to provide users with a graphical interface to support additional tasks, such as refinement to correct ambiguous results provided by automatic enrichment algorithms. A task-driven user evaluation proved SemTUI to be understandable, usable, and capable of achieving table enrichment with little effort and time with user tests that involved people with different skills and experiences.

</p>
</details>

<details><summary><b>Stability and Risk Bounds of Iterative Hard Thresholding</b>
<a href="https://arxiv.org/abs/2203.09413">arxiv:2203.09413</a>
&#x1F4C8; 4 <br>
<p>Xiao-Tong Yuan, Ping Li</p></summary>
<p>

**Abstract:** In this paper, we analyze the generalization performance of the Iterative Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The parameter estimation and sparsity recovery consistency of IHT has long been known in compressed sensing. From the perspective of statistical learning, another fundamental question is how well the IHT estimation would predict on unseen data. This paper makes progress towards answering this open question by introducing a novel sparse generalization theory for IHT under the notion of algorithmic stability. Our theory reveals that: 1) under natural conditions on the empirical risk function over $n$ samples of dimension $p$, IHT with sparsity level $k$ enjoys an $\mathcal{\tilde O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk; 2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be established by imposing an additional iteration stability condition on a hypothetical IHT procedure invoked to the population risk; and 3) a fast rate of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be derived for strongly convex risk function under proper strong-signal conditions. The results have been substantialized to sparse linear regression and sparse logistic regression models to demonstrate the applicability of our theory. Preliminary numerical evidence is provided to confirm our theoretical predictions.

</p>
</details>

<details><summary><b>A Framework and Benchmark for Deep Batch Active Learning for Regression</b>
<a href="https://arxiv.org/abs/2203.09410">arxiv:2203.09410</a>
&#x1F4C8; 4 <br>
<p>David Holzm√ºller, Viktor Zaverkin, Johannes K√§stner, Ingo Steinwart</p></summary>
<p>

**Abstract:** We study the performance of different pool-based Batch Mode Deep Active Learning (BMDAL) methods for regression on tabular data, focusing on methods that do not require to modify the network architecture and training. Our contributions are three-fold: First, we present a framework for constructing BMDAL methods out of kernels, kernel transformations and selection methods, showing that many of the most popular BMDAL methods fit into our framework. Second, we propose new components, leading to a new BMDAL method. Third, we introduce an open-source benchmark with 15 large tabular data sets, which we use to compare different BMDAL methods. Our benchmark results show that a combination of our novel components yields new state-of-the-art results in terms of RMSE and is computationally efficient. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.

</p>
</details>

<details><summary><b>CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation</b>
<a href="https://arxiv.org/abs/2203.09343">arxiv:2203.09343</a>
&#x1F4C8; 4 <br>
<p>Renhao Wang, Hang Zhao, Yang Gao</p></summary>
<p>

**Abstract:** Many recent approaches in contrastive learning have worked to close the gap between pretraining on iconic images like ImageNet and pretraining on complex scenes like COCO. This gap exists largely because commonly used random crop augmentations obtain semantically inconsistent content in crowded scene images of diverse objects. Previous works use preprocessing pipelines to localize salient objects for improved cropping, but an end-to-end solution is still elusive. In this work, we propose a framework which accomplishes this goal via joint learning of representations and segmentation. We leverage segmentation masks to train a model with a mask-dependent contrastive loss, and use the partially trained model to bootstrap better masks. By iterating between these two components, we ground the contrastive updates in segmentation information, and simultaneously improve segmentation throughout pretraining. Experiments show our representations transfer robustly to downstream tasks in classification, detection and segmentation.

</p>
</details>

<details><summary><b>Localizing Visual Sounds the Easy Way</b>
<a href="https://arxiv.org/abs/2203.09324">arxiv:2203.09324</a>
&#x1F4C8; 4 <br>
<p>Shentong Mo, Pedro Morgado</p></summary>
<p>

**Abstract:** Unsupervised audio-visual source localization aims at localizing visible sound sources in a video without relying on ground-truth localization for training. Previous works often seek high audio-visual similarities for likely positive (sounding) regions and low similarities for likely negative regions. However, accurately distinguishing between sounding and non-sounding regions is challenging without manual annotations. In this work, we propose a simple yet effective approach for Easy Visual Sound Localization, namely EZ-VSL, without relying on the construction of positive and/or negative regions during training. Instead, we align audio and visual spaces by seeking audio-visual representations that are aligned in, at least, one location of the associated image, while not matching other images, at any location. We also introduce a novel object guided localization scheme at inference time for improved precision. Our simple and effective framework achieves state-of-the-art performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to 83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is available at https://github.com/stoneMo/EZ-VSL.

</p>
</details>

<details><summary><b>Ranking of Communities in Multiplex Spatiotemporal Models of Brain Dynamics</b>
<a href="https://arxiv.org/abs/2203.09281">arxiv:2203.09281</a>
&#x1F4C8; 4 <br>
<p>James B. Wilsenach, Catherine E. Warnaby, Charlotte M. Deane, Gesine D. Reinert</p></summary>
<p>

**Abstract:** As a relatively new field, network neuroscience has tended to focus on aggregate behaviours of the brain averaged over many successive experiments or over long recordings in order to construct robust brain models. These models are limited in their ability to explain dynamic state changes in the brain which occurs spontaneously as a result of normal brain function. Hidden Markov Models (HMMs) trained on neuroimaging time series data have since arisen as a method to produce dynamical models that are easy to train but can be difficult to fully parametrise or analyse. We propose an interpretation of these neural HMMs as multiplex brain state graph models we term Hidden Markov Graph Models (HMGMs). This interpretation allows for dynamic brain activity to be analysed using the full repertoire of network analysis techniques. Furthermore, we propose a general method for selecting HMM hyperparameters in the absence of external data, based on the principle of maximum entropy, and use this to select the number of layers in the multiplex model. We produce a new tool for determining important communities of brain regions using a spatiotemporal random walk-based procedure that takes advantage of the underlying Markov structure of the model. Our analysis of real multi-subject fMRI data provides new results that corroborate the modular processing hypothesis of the brain at rest as well as contributing new evidence of functional overlap between and within dynamic brain state communities. Our analysis pipeline provides a way to characterise dynamic network activity of the brain under novel behaviours or conditions.

</p>
</details>

<details><summary><b>Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images</b>
<a href="https://arxiv.org/abs/2203.09207">arxiv:2203.09207</a>
&#x1F4C8; 4 <br>
<p>Fuxin Fan, Ludwig Ritschl, Marcel Beister, Ramyar Biniazan, Bj√∂rn Kreher, Tristan M. Gottschalk, Steffen Kappler, Andreas Maier</p></summary>
<p>

**Abstract:** In several image acquisition and processing steps of X-ray radiography, knowledge of the existence of metal implants and their exact position is highly beneficial (e.g. dose regulation, image contrast adjustment). Another application which would benefit from an accurate metal segmentation is cone beam computed tomography (CBCT) which is based on 2D X-ray projections. Due to the high attenuation of metals, severe artifacts occur in the 3D X-ray acquisitions. The metal segmentation in CBCT projections usually serves as a prerequisite for metal artifact avoidance and reduction algorithms. Since the generation of high quality clinical training is a constant challenge, this study proposes to generate simulated X-ray images based on CT data sets combined with self-designed computer aided design (CAD) implants and make use of convolutional neural network (CNN) and vision transformer (ViT) for metal segmentation. Model test is performed on accurately labeled X-ray test datasets obtained from specimen scans. The CNN encoder-based network like U-Net has limited performance on cadaver test data with an average dice score below 0.30, while the metal segmentation transformer with dual decoder (MST-DD) shows high robustness and generalization on the segmentation task, with an average dice score of 0.90. Our study indicates that the CAD model-based data generation has high flexibility and could be a way to overcome the problem of shortage in clinical data sampling and labelling. Furthermore, the MST-DD approach generates a more reliable neural network in case of training on simulated data.

</p>
</details>

<details><summary><b>An Interactive Explanatory AI System for Industrial Quality Control</b>
<a href="https://arxiv.org/abs/2203.09181">arxiv:2203.09181</a>
&#x1F4C8; 4 <br>
<p>Dennis M√ºller, Michael M√§rz, Stephan Scheele, Ute Schmid</p></summary>
<p>

**Abstract:** Machine learning based image classification algorithms, such as deep neural network approaches, will be increasingly employed in critical settings such as quality control in industry, where transparency and comprehensibility of decisions are crucial. Therefore, we aim to extend the defect detection task towards an interactive human-in-the-loop approach that allows us to integrate rich background knowledge and the inference of complex relationships going beyond traditional purely data-driven approaches. We propose an approach for an interactive support system for classifications in an industrial quality control setting that combines the advantages of both (explainable) knowledge-driven and data-driven machine learning methods, in particular inductive logic programming and convolutional neural networks, with human expertise and control. The resulting system can assist domain experts with decisions, provide transparent explanations for results, and integrate feedback from users; thus reducing workload for humans while both respecting their expertise and without removing their agency or accountability.

</p>
</details>

<details><summary><b>On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks</b>
<a href="https://arxiv.org/abs/2203.09168">arxiv:2203.09168</a>
&#x1F4C8; 4 <br>
<p>Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, Georg Martius</p></summary>
<p>

**Abstract:** Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify potential hazards associated with the use of log-likelihood in conjunction with gradient-based optimizers. First, we present a synthetic example illustrating how this approach can lead to very poor but stable parameter estimates. Second, we identify the culprit to be the log-likelihood loss, along with certain conditions that exacerbate the issue. Third, we present an alternative formulation, termed $Œ≤$-NLL, in which each data point's contribution to the loss is weighted by the $Œ≤$-exponentiated variance estimate. We show that using an appropriate $Œ≤$ largely mitigates the issue in our illustrative example. Fourth, we evaluate this approach on a range of domains and tasks and show that it achieves considerable improvements and performs more robustly concerning hyperparameters, both in predictive RMSE and log-likelihood criteria.

</p>
</details>

<details><summary><b>Are Vision Transformers Robust to Spurious Correlations?</b>
<a href="https://arxiv.org/abs/2203.09125">arxiv:2203.09125</a>
&#x1F4C8; 4 <br>
<p>Soumya Suvra Ghosal, Yifei Ming, Yixuan Li</p></summary>
<p>

**Abstract:** Deep neural networks may be susceptible to learning spurious correlations that hold on average but not in atypical test samples. As with the recent emergence of vision transformer (ViT) models, it remains underexplored how spurious correlations are manifested in such architectures. In this paper, we systematically investigate the robustness of vision transformers to spurious correlations on three challenging benchmark datasets and compare their performance with popular CNNs. Our study reveals that when pre-trained on a sufficiently large dataset, ViT models are more robust to spurious correlations than CNNs. Key to their success is the ability to generalize better from the examples where spurious correlations do not hold. Further, we perform extensive ablations and experiments to understand the role of the self-attention mechanism in providing robustness under spuriously correlated environments. We hope that our work will inspire future research on further understanding the robustness of ViT models.

</p>
</details>

<details><summary><b>Causal Robot Communication Inspired by Observational Learning Insights</b>
<a href="https://arxiv.org/abs/2203.09114">arxiv:2203.09114</a>
&#x1F4C8; 4 <br>
<p>Zhao Han, Boyoung Kim, Holly A. Yanco, Tom Williams</p></summary>
<p>

**Abstract:** Autonomous robots must communicate about their decisions to gain trust and acceptance. When doing so, robots must determine which actions are causal, i.e., which directly give rise to the desired outcome, so that these actions can be included in explanations. In behavior learning in psychology, this sort of reasoning during an action sequence has been studied extensively in the context of imitation learning. And yet, these techniques and empirical insights are rarely applied to human-robot interaction (HRI). In this work, we discuss the relevance of behavior learning insights for robot intent communication, and present the first application of these insights for a robot to efficiently communicate its intent by selectively explaining the causal actions in an action sequence.

</p>
</details>

<details><summary><b>A Novel Exploration of Diffusion Process based on Multi-types Galton-Watson Forests</b>
<a href="https://arxiv.org/abs/2203.11816">arxiv:2203.11816</a>
&#x1F4C8; 3 <br>
<p>Yanjiao Zhu, Qilin Li, Wanquan Liu, Chuancun Yin, Zhenlong Gao</p></summary>
<p>

**Abstract:** Diffusion is a commonly used technique for spreading information from point to point on a graph. The rationale behind diffusion is not clear. And the multi-types Galton-Watson forest is a random model of population growth without space or any other resource constraints. In this paper, we use the degenerated multi-types Galton-Watson forest (MGWF) to interpret the diffusion process and establish an equivalent relationship between them. With the two-phase setting of the MGWF, one can interpret the diffusion process and the Google PageRank system explicitly. It also improves the convergence behaviour of the iterative diffusion process and Google PageRank system. We validate the proposal by experiment while providing new research directions.

</p>
</details>

<details><summary><b>Revealing Reliable Signatures by Learning Top-Rank Pairs</b>
<a href="https://arxiv.org/abs/2203.09927">arxiv:2203.09927</a>
&#x1F4C8; 3 <br>
<p>Xiaotong Ji, Yan Zheng, Daiki Suehiro, Seiichi Uchida</p></summary>
<p>

**Abstract:** Signature verification, as a crucial practical documentation analysis task, has been continuously studied by researchers in machine learning and pattern recognition fields. In specific scenarios like confirming financial documents and legal instruments, ensuring the absolute reliability of signatures is of top priority. In this work, we proposed a new method to learn "top-rank pairs" for writer-independent offline signature verification tasks. By this scheme, it is possible to maximize the number of absolutely reliable signatures. More precisely, our method to learn top-rank pairs aims at pushing positive samples beyond negative samples, after pairing each of them with a genuine reference signature. In the experiment, BHSig-B and BHSig-H datasets are used for evaluation, on which the proposed model achieves overwhelming better pos@top (the ratio of absolute top positive samples to all of the positive samples) while showing encouraging performance on both Area Under the Curve (AUC) and accuracy.

</p>
</details>

<details><summary><b>DEFORM: A Practical, Universal Deep Beamforming System</b>
<a href="https://arxiv.org/abs/2203.09727">arxiv:2203.09727</a>
&#x1F4C8; 3 <br>
<p>Hai N. Nguyen, Guevara Noubir</p></summary>
<p>

**Abstract:** We introduce, design, and evaluate a set of universal receiver beamforming techniques. Our approach and system DEFORM, a Deep Learning (DL) based RX beamforming achieves significant gain for multi antenna RF receivers while being agnostic to the transmitted signal features (e.g., modulation or bandwidth). It is well known that combining coherent RF signals from multiple antennas results in a beamforming gain proportional to the number of receiving elements. However in practice, this approach heavily relies on explicit channel estimation techniques, which are link specific and require significant communication overhead to be transmitted to the receiver. DEFORM addresses this challenge by leveraging Convolutional Neural Network to estimate the channel characteristics in particular the relative phase to antenna elements. It is specifically designed to address the unique features of wireless signals complex samples, such as the ambiguous $2œÄ$ phase discontinuity and the high sensitivity of the link Bit Error Rate. The channel prediction is subsequently used in the Maximum Ratio Combining algorithm to achieve an optimal combination of the received signals. While being trained on a fixed, basic RF settings, we show that DEFORM DL model is universal, achieving up to 3 dB of SNR gain for a two antenna receiver in extensive experiments demonstrating various settings of modulations, bandwidths, and channels. The universality of DEFORM is demonstrated through joint beamforming relaying of LoRa (Chirp Spread Spectrum modulation) and ZigBee signals, achieving significant improvements to Packet Loss/Delivery Rates relatively to conventional Amplify and Forward (LoRa PLR reduced by 23 times and ZigBee PDR increased by 8 times).

</p>
</details>

<details><summary><b>Rethinking the optimization process for self-supervised model-driven MRI reconstruction</b>
<a href="https://arxiv.org/abs/2203.09724">arxiv:2203.09724</a>
&#x1F4C8; 3 <br>
<p>Weijian Huang, Cheng Li, Wenxin Fan, Yongjin Zhou, Qiegen Liu, Hairong Zheng, Shanshan Wang</p></summary>
<p>

**Abstract:** Recovering high-quality images from undersampled measurements is critical for accelerated MRI reconstruction. Recently, various supervised deep learning-based MRI reconstruction methods have been developed. Despite the achieved promising performances, these methods require fully sampled reference data, the acquisition of which is resource-intensive and time-consuming. Self-supervised learning has emerged as a promising solution to alleviate the reliance on fully sampled datasets. However, existing self-supervised methods suffer from reconstruction errors due to the insufficient constraint enforced on the non-sampled data points and the error accumulation happened alongside the iterative image reconstruction process for model-driven deep learning reconstrutions. To address these challenges, we propose K2Calibrate, a K-space adaptation strategy for self-supervised model-driven MR reconstruction optimization. By iteratively calibrating the learned measurements, K2Calibrate can reduce the network's reconstruction deterioration caused by statistically dependent noise. Extensive experiments have been conducted on the open-source dataset FastMRI, and K2Calibrate achieves better results than five state-of-the-art methods. The proposed K2Calibrate is plug-and-play and can be easily integrated with different model-driven deep learning reconstruction methods.

</p>
</details>

<details><summary><b>M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization</b>
<a href="https://arxiv.org/abs/2203.09707">arxiv:2203.09707</a>
&#x1F4C8; 3 <br>
<p>Yuexiu Gao, Chen Lyu</p></summary>
<p>

**Abstract:** Source code summarization aims to generate natural language descriptions of code snippets. Many existing studies learn the syntactic and semantic knowledge of code snippets from their token sequences and Abstract Syntax Trees (ASTs). They use the learned code representations as input to code summarization models, which can accordingly generate summaries describing source code. Traditional models traverse ASTs as sequences or split ASTs into paths as input. However, the former loses the structural properties of ASTs, and the latter destroys the overall structure of ASTs. Therefore, comprehensively capturing the structural features of ASTs in learning code representations for source code summarization remains a challenging problem to be solved. In this paper, we propose M2TS, a Multi-scale Multi-modal approach based on Transformer for source code Summarization. M2TS uses a multi-scale AST feature extraction method, which can extract the structures of ASTs more completely and accurately at multiple local and global levels. To complement missing semantic information in ASTs, we also obtain code token features, and further combine them with the extracted AST features using a cross modality fusion method that not only fuses the syntactic and contextual semantic information of source code, but also highlights the key features of each modality. We conduct experiments on two Java and one Python datasets, and the experimental results demonstrate that M2TS outperforms current state-of-the-art methods. We release our code at https://github.com/TranSMS/M2TS.

</p>
</details>

<details><summary><b>Federated Learning for Privacy Preservation in Smart Healthcare Systems: A Comprehensive Survey</b>
<a href="https://arxiv.org/abs/2203.09702">arxiv:2203.09702</a>
&#x1F4C8; 3 <br>
<p>Mansoor Ali, Faisal Naeem, Muhammad Tariq, Geroges Kaddoum</p></summary>
<p>

**Abstract:** Recent advances in electronic devices and communication infrastructure have revolutionized the traditional healthcare system into a smart healthcare system by using IoMT devices. However, due to the centralized training approach of artificial intelligence (AI), the use of mobile and wearable IoMT devices raises privacy concerns with respect to the information that has been communicated between hospitals and end users. The information conveyed by the IoMT devices is highly confidential and can be exposed to adversaries. In this regard, federated learning (FL), a distributive AI paradigm has opened up new opportunities for privacy-preservation in IoMT without accessing the confidential data of the participants. Further, FL provides privacy to end users as only gradients are shared during training. For these specific properties of FL, in this paper we present privacy related issues in IoMT. Afterwards, we present the role of FL in IoMT networks for privacy preservation and introduce some advanced FL architectures incorporating deep reinforcement learning (DRL), digital twin, and generative adversarial networks (GANs) for detecting privacy threats. Subsequently, we present some practical opportunities of FL in smart healthcare systems. At the end, we conclude this survey by providing open research challenges for FL that can be used in future smart healthcare systems

</p>
</details>

<details><summary><b>Fast Bayesian Coresets via Subsampling and Quasi-Newton Refinement</b>
<a href="https://arxiv.org/abs/2203.09675">arxiv:2203.09675</a>
&#x1F4C8; 3 <br>
<p>Cian Naik, Judith Rousseau, Trevor Campbell</p></summary>
<p>

**Abstract:** Bayesian coresets approximate a posterior distribution by building a small weighted subset of the data points. Any inference procedure that is too computationally expensive to be run on the full posterior can instead be run inexpensively on the coreset, with results that approximate those on the full data. However, current approaches are limited by either a significant run-time or the need for the user to specify a low-cost approximation to the full posterior. We propose a Bayesian coreset construction algorithm that first selects a uniformly random subset of data, and then optimizes the weights using a novel quasi-Newton method. Our algorithm is simple to implement, does not require the user to specify a low-cost posterior approximation, and is the first to come with a general high-probability bound on the KL divergence of the output coreset posterior. Experiments demonstrate that the method provides orders of magnitude improvement in construction time against the state-of-the-art black-box method. Moreover, it provides significant improvements in coreset quality against alternatives with comparable construction times, with far less storage cost and user input required.

</p>
</details>

<details><summary><b>Meta Reinforcement Learning for Adaptive Control: An Offline Approach</b>
<a href="https://arxiv.org/abs/2203.09661">arxiv:2203.09661</a>
&#x1F4C8; 3 <br>
<p>Daniel G. McClement, Nathan P. Lawrence, Johan U. Backstrom, Philip D. Loewen, Michael G. Forbes, R. Bhushan Gopaluni</p></summary>
<p>

**Abstract:** Meta-learning is a branch of machine learning which trains neural network models to synthesize a wide variety of data in order to rapidly solve new problems. In process control, many systems have similar and well-understood dynamics, which suggests it is feasible to create a generalizable controller through meta-learning. In this work, we formulate a meta reinforcement learning (meta-RL) control strategy that takes advantage of known, offline information for training, such as the system gain or time constant, yet efficiently controls novel systems in a completely model-free fashion. Our meta-RL agent has a recurrent structure that accumulates "context" for its current dynamics through a hidden state variable. This end-to-end architecture enables the agent to automatically adapt to changes in the process dynamics. Moreover, the same agent can be deployed on systems with previously unseen nonlinearities and timescales. In tests reported here, the meta-RL agent was trained entirely offline, yet produced excellent results in novel settings. A key design element is the ability to leverage model-based information offline during training, while maintaining a model-free policy structure for interacting with novel environments. To illustrate the approach, we take the actions proposed by the meta-RL agent to be changes to gains of a proportional-integral controller, resulting in a generalized, adaptive, closed-loop tuning strategy. Meta-learning is a promising approach for constructing sample-efficient intelligent controllers.

</p>
</details>

<details><summary><b>Unified Line and Paragraph Detection by Graph Convolutional Networks</b>
<a href="https://arxiv.org/abs/2203.09638">arxiv:2203.09638</a>
&#x1F4C8; 3 <br>
<p>Shuang Liu, Renshen Wang, Michalis Raptis, Yasuhisa Fujii</p></summary>
<p>

**Abstract:** We formulate the task of detecting lines and paragraphs in a document into a unified two-level clustering problem. Given a set of text detection boxes that roughly correspond to words, a text line is a cluster of boxes and a paragraph is a cluster of lines. These clusters form a two-level tree that represents a major part of the layout of a document. We use a graph convolutional network to predict the relations between text detection boxes and then build both levels of clusters from these predictions. Experimentally, we demonstrate that the unified approach can be highly efficient while still achieving state-of-the-art quality for detecting paragraphs in public benchmarks and real-world images.

</p>
</details>

<details><summary><b>Design of Compressed Sensing Systems via Density-Evolution Framework for Structure Recovery in Graphical Models</b>
<a href="https://arxiv.org/abs/2203.09636">arxiv:2203.09636</a>
&#x1F4C8; 3 <br>
<p>Muralikrishnna G. Sethuraman, Hang Zhang, Faramarz Fekri</p></summary>
<p>

**Abstract:** It has been shown that the task of learning the structure of Bayesian networks (BN) from observational data is an NP-Hard problem. Although there have been attempts made to tackle this problem, these solutions assume direct access to the observational data which may not be practical in certain applications. In this paper, we explore the feasibility of recovering the structure of Gaussian Bayesian Network (GBN) from compressed (low dimensional and indirect) measurements. We propose a novel density-evolution based framework for optimizing compressed linear measurement systems that would, by design, allow for more accurate retrieval of the covariance matrix and thereby the graph structure. In particular, under the assumption that both the covariance matrix and the graph are sparse, we show that the structure of GBN can indeed be recovered from resulting compressed measurements. The numerical simulations show that our sensing systems outperform the state of the art with respect to Maximum absolute error (MAE) and have comparable performance with respect to precision and recall, without any need for ad-hoc parameter tuning.

</p>
</details>

<details><summary><b>On the Importance of Data Size in Probing Fine-tuned Models</b>
<a href="https://arxiv.org/abs/2203.09627">arxiv:2203.09627</a>
&#x1F4C8; 3 <br>
<p>Houman Mehrafarin, Sara Rajaee, Mohammad Taher Pilehvar</p></summary>
<p>

**Abstract:** Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model's linguistic knowledge.

</p>
</details>

<details><summary><b>DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection</b>
<a href="https://arxiv.org/abs/2203.09598">arxiv:2203.09598</a>
&#x1F4C8; 3 <br>
<p>Nic Jedema, Thuy Vu, Manish Gupta, Alessandro Moschitti</p></summary>
<p>

**Abstract:** While transformers demonstrate impressive performance on many knowledge intensive (KI) tasks, their ability to serve as implicit knowledge bases (KBs) remains limited, as shown on several slot-filling, question-answering (QA), fact verification, and entity-linking tasks. In this paper, we implement an efficient, data-programming technique that enriches training data with KB-derived context and improves transformer utilization of encoded knowledge when fine-tuning for a particular QA task, namely answer sentence selection (AS2). Our method outperforms state of the art transformer approach on WikiQA and TrecQA, two widely studied AS2 benchmarks, increasing by 2.0% p@1, 1.3% MAP, 1.1% MRR, and 4.4% p@1, 0.9% MAP, 2.4% MRR, respectively. To demonstrate our improvements in an industry setting, we additionally evaluate our approach on a proprietary dataset of Alexa QA pairs, and show increase of 2.3% F1 and 2.0% MAP. We additionally find that these improvements remain even when KB context is omitted at inference time, allowing for the use of our models within existing transformer workflows without additional latency or deployment costs.

</p>
</details>

<details><summary><b>Prioritized Variable-length Test Cases Generation for Finite State Machines</b>
<a href="https://arxiv.org/abs/2203.09596">arxiv:2203.09596</a>
&#x1F4C8; 3 <br>
<p>Vaclav Rechtberger, Miroslav Bures, Bestoun S. Ahmed, Youcef Belkhier, Jiri Nema, Hynek Schvach</p></summary>
<p>

**Abstract:** Model-based Testing (MBT) is an effective approach for testing when parts of a system-under-test have the characteristics of a finite state machine (FSM). Despite various strategies in the literature on this topic, little work exists to handle special testing situations. More specifically, when concurrently: (1) the test paths can start and end only in defined states of the FSM, (2) a prioritization mechanism that requires only defined states and transitions of the FSM to be visited by test cases is required, and (3) the test paths must be in a given length range, not necessarily of explicit uniform length. This paper presents a test generation strategy that satisfies all these requirements. A concurrent combination of these requirements is highly practical for real industrial testing. Six variants of possible algorithms to implement this strategy are described. Using a mixture of 180 problem instances from real automotive and defense projects and artificially generated FSMs, all variants are compared with a baseline strategy based on an established N-switch coverage concept modification. Various properties of the generated test paths and their potential to activate fictional defects defined in FSMs are evaluated. The presented strategy outperforms the baseline in most problem configurations. Out of the six analyzed variants, three give the best results even though a universal best performer is hard to identify. Depending on the application of the FSM, the strategy and evaluation presented in this paper are applicable both in testing functional and non-functional software requirements.

</p>
</details>

<details><summary><b>Surface Defect Detection and Evaluation for Marine Vessels using Multi-Stage Deep Learning</b>
<a href="https://arxiv.org/abs/2203.09580">arxiv:2203.09580</a>
&#x1F4C8; 3 <br>
<p>Li Yu, Kareem Metwaly, James Z. Wang, Vishal Monga</p></summary>
<p>

**Abstract:** Detecting and evaluating surface coating defects is important for marine vessel maintenance. Currently, the assessment is carried out manually by qualified inspectors using international standards and their own experience. Automating the processes is highly challenging because of the high level of variation in vessel type, paint surface, coatings, lighting condition, weather condition, paint colors, areas of the vessel, and time in service. We present a novel deep learning-based pipeline to detect and evaluate the percentage of corrosion, fouling, and delamination on the vessel surface from normal photographs. We propose a multi-stage image processing framework, including ship section segmentation, defect segmentation, and defect classification, to automatically recognize different types of defects and measure the coverage percentage on the ship surface. Experimental results demonstrate that our proposed pipeline can objectively perform a similar assessment as a qualified inspector.

</p>
</details>

<details><summary><b>On the expressive power of message-passing neural networks as global feature map transformers</b>
<a href="https://arxiv.org/abs/2203.09555">arxiv:2203.09555</a>
&#x1F4C8; 3 <br>
<p>Floris Geerts, Jasper Steegmans, Jan Van den Bussche</p></summary>
<p>

**Abstract:** We investigate the power of message-passing neural networks (MPNNs) in their capacity to transform the numerical features stored in the nodes of their input graphs. Our focus is on global expressive power, uniformly over all input graphs, or over graphs of bounded degree with features from a bounded domain. Accordingly, we introduce the notion of a global feature map transformer (GFMT). As a yardstick for expressiveness, we use a basic language for GFMTs, which we call MPLang. Every MPNN can be expressed in MPLang, and our results clarify to which extent the converse inclusion holds. We consider exact versus approximate expressiveness; the use of arbitrary activation functions; and the case where only the ReLU activation function is allowed.

</p>
</details>

<details><summary><b>Learning of Structurally Unambiguous Probabilistic Grammars</b>
<a href="https://arxiv.org/abs/2203.09441">arxiv:2203.09441</a>
&#x1F4C8; 3 <br>
<p>Dana Fisman, Dolav Nitay, Michal Ziv-Ukelson</p></summary>
<p>

**Abstract:** The problem of identifying a probabilistic context free grammar has two aspects: the first is determining the grammar's topology (the rules of the grammar) and the second is estimating probabilistic weights for each rule. Given the hardness results for learning context-free grammars in general, and probabilistic grammars in particular, most of the literature has concentrated on the second problem. In this work we address the first problem. We restrict attention to structurally unambiguous weighted context-free grammars (SUWCFG) and provide a query learning algorithm for \structurally unambiguous probabilistic context-free grammars (SUPCFG). We show that SUWCFG can be represented using \emph{co-linear multiplicity tree automata} (CMTA), and provide a polynomial learning algorithm that learns CMTAs. We show that the learned CMTA can be converted into a probabilistic grammar, thus providing a complete algorithm for learning a structurally unambiguous probabilistic context free grammar (both the grammar topology and the probabilistic weights) using structured membership queries and structured equivalence queries. A summarized version of this work was published at AAAI 21.

</p>
</details>

<details><summary><b>An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival</b>
<a href="https://arxiv.org/abs/2203.09438">arxiv:2203.09438</a>
&#x1F4C8; 3 <br>
<p>S√∂ren Schleibaum, J√∂rg P. M√ºller, Monika Sester</p></summary>
<p>

**Abstract:** To compare alternative taxi schedules and to compute them, as well as to provide insights into an upcoming taxi trip to drivers and passengers, the duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To reach a high prediction precision, machine learning models for ETA are state of the art. One yet unexploited option to further increase prediction precision is to combine multiple ETA models into an ensemble. While an increase of prediction precision is likely, the main drawback is that the predictions made by such an ensemble become less transparent due to the sophisticated ensemble architecture. One option to remedy this drawback is to apply eXplainable Artificial Intelligence (XAI). The contribution of this paper is three-fold. First, we combine multiple machine learning models from our previous work for ETA into a two-level ensemble model - a stacked ensemble model - which on its own is novel; therefore, we can outperform previous state-of-the-art static route-free ETA approaches. Second, we apply existing XAI methods to explain the first- and second-level models of the ensemble. Third, we propose three joining methods for combining the first-level explanations with the second-level ones. Those joining methods enable us to explain stacked ensembles for regression tasks. An experimental evaluation shows that the ETA models correctly learned the importance of those input features driving the prediction.

</p>
</details>

<details><summary><b>PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer</b>
<a href="https://arxiv.org/abs/2203.09293">arxiv:2203.09293</a>
&#x1F4C8; 3 <br>
<p>Lina Achaji, Thierno Barry, Thibault Fouqueray, Julien Moreau, Francois Aioun, Francois Charpillet</p></summary>
<p>

**Abstract:** Nowadays, our mobility systems are evolving into the era of intelligent vehicles that aim to improve road safety. Due to their vulnerability, pedestrians are the users who will benefit the most from these developments. However, predicting their trajectory is one of the most challenging concerns. Indeed, accurate prediction requires a good understanding of multi-agent interactions that can be complex. Learning the underlying spatial and temporal patterns caused by these interactions is even more of a competitive and open problem that many researchers are tackling. In this paper, we introduce a model called PRediction Transformer (PReTR) that extracts features from the multi-agent scenes by employing a factorized spatio-temporal attention module. It shows less computational needs than previously studied models with empirically better results. Besides, previous works in motion prediction suffer from the exposure bias problem caused by generating future sequences conditioned on model prediction samples rather than ground-truth samples. In order to go beyond the proposed solutions, we leverage encoder-decoder Transformer networks for parallel decoding a set of learned object queries. This non-autoregressive solution avoids the need for iterative conditioning and arguably decreases training and testing computational time. We evaluate our model on the ETH/UCY datasets, a publicly available benchmark for pedestrian trajectory prediction. Finally, we justify our usage of the parallel decoding technique by showing that the trajectory prediction task can be better solved as a non-autoregressive task.

</p>
</details>

<details><summary><b>Transfer learning for cross-modal demand prediction of bike-share and public transit</b>
<a href="https://arxiv.org/abs/2203.09279">arxiv:2203.09279</a>
&#x1F4C8; 3 <br>
<p>Mingzhuang Hua, Francisco Camara Pereira, Yu Jiang, Xuewu Chen</p></summary>
<p>

**Abstract:** The urban transportation system is a combination of multiple transport modes, and the interdependencies across those modes exist. This means that the travel demand across different travel modes could be correlated as one mode may receive demand from or create demand for another mode, not to mention natural correlations between different demand time series due to general demand flow patterns across the network. It is expectable that cross-modal ripple effects become more prevalent, with Mobility as a Service. Therefore, by propagating demand data across modes, a better demand prediction could be obtained. To this end, this study explores various machine learning models and transfer learning strategies for cross-modal demand prediction. The trip data of bike-share, metro, and taxi are processed as the station-level passenger flows, and then the proposed prediction method is tested in the large-scale case studies of Nanjing and Chicago. The results suggest that prediction models with transfer learning perform better than unimodal prediction models. Furthermore, stacked Long Short-Term Memory model performs particularly well in cross-modal demand prediction. These results verify our combined method's forecasting improvement over existing benchmarks and demonstrate the good transferability for cross-modal demand prediction in multiple cities.

</p>
</details>

<details><summary><b>Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series</b>
<a href="https://arxiv.org/abs/2203.09270">arxiv:2203.09270</a>
&#x1F4C8; 3 <br>
<p>Kristoffer Wickstr√∏m, Michael Kampffmeyer, Karl √òyvind Mikalsen, Robert Jenssen</p></summary>
<p>

**Abstract:** The lack of labeled data is a key challenge for learning useful representation from time series data. However, an unsupervised representation framework that is capable of producing high quality representations could be of great value. It is key to enabling transfer learning, which is especially beneficial for medical applications, where there is an abundance of data but labeling is costly and time consuming. We propose an unsupervised contrastive learning framework that is motivated from the perspective of label smoothing. The proposed approach uses a novel contrastive loss that naturally exploits a data augmentation scheme in which new samples are generated by mixing two data samples with a mixing component. The task in the proposed framework is to predict the mixing component, which is utilized as soft targets in the loss function. Experiments demonstrate the framework's superior performance compared to other representation learning approaches on both univariate and multivariate time series and illustrate its benefits for transfer learning for clinical time series.

</p>
</details>

<details><summary><b>Explainability in Graph Neural Networks: An Experimental Survey</b>
<a href="https://arxiv.org/abs/2203.09258">arxiv:2203.09258</a>
&#x1F4C8; 3 <br>
<p>Peibo Li, Yixing Yang, Maurice Pagnucco, Yang Song</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) have been extensively developed for graph representation learning in various application domains. However, similar to all other neural networks models, GNNs suffer from the black-box problem as people cannot understand the mechanism underlying them. To solve this problem, several GNN explainability methods have been proposed to explain the decisions made by GNNs. In this survey, we give an overview of the state-of-the-art GNN explainability methods and how they are evaluated. Furthermore, we propose a new evaluation metric and conduct thorough experiments to compare GNN explainability methods on real world datasets. We also suggest future directions for GNN explainability.

</p>
</details>

<details><summary><b>Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs</b>
<a href="https://arxiv.org/abs/2203.09251">arxiv:2203.09251</a>
&#x1F4C8; 3 <br>
<p>Andrea Tirinzoni, Aymen Al-Marjani, Emilie Kaufmann</p></summary>
<p>

**Abstract:** In probably approximately correct (PAC) reinforcement learning (RL), an agent is required to identify an $Œµ$-optimal policy with probability $1-Œ¥$. While minimax optimal algorithms exist for this problem, its instance-dependent complexity remains elusive in episodic Markov decision processes (MDPs). In this paper, we propose the first (nearly) matching upper and lower bounds on the sample complexity of PAC RL in deterministic episodic MDPs with finite state and action spaces. In particular, our bounds feature a new notion of sub-optimality gap for state-action pairs that we call the deterministic return gap. While our instance-dependent lower bound is written as a linear program, our algorithms are very simple and do not require solving such an optimization problem during learning. Their design and analyses employ novel ideas, including graph-theoretical concepts such as minimum flows and maximum cuts, which we believe to shed new light on this problem.

</p>
</details>

<details><summary><b>On the Properties of Adversarially-Trained CNNs</b>
<a href="https://arxiv.org/abs/2203.09243">arxiv:2203.09243</a>
&#x1F4C8; 3 <br>
<p>Mattia Carletti, Matteo Terzi, Gian Antonio Susto</p></summary>
<p>

**Abstract:** Adversarial Training has proved to be an effective training paradigm to enforce robustness against adversarial examples in modern neural network architectures. Despite many efforts, explanations of the foundational principles underpinning the effectiveness of Adversarial Training are limited and far from being widely accepted by the Deep Learning community. In this paper, we describe surprising properties of adversarially-trained models, shedding light on mechanisms through which robustness against adversarial attacks is implemented. Moreover, we highlight limitations and failure modes affecting these models that were not discussed by prior works. We conduct extensive analyses on a wide range of architectures and datasets, performing a deep comparison between robust and natural models.

</p>
</details>

<details><summary><b>SoK: Differential Privacy on Graph-Structured Data</b>
<a href="https://arxiv.org/abs/2203.09205">arxiv:2203.09205</a>
&#x1F4C8; 3 <br>
<p>Tamara T. Mueller, Dmitrii Usynin, Johannes C. Paetzold, Daniel Rueckert, Georgios Kaissis</p></summary>
<p>

**Abstract:** In this work, we study the applications of differential privacy (DP) in the context of graph-structured data. We discuss the formulations of DP applicable to the publication of graphs and their associated statistics as well as machine learning on graph-based data, including graph neural networks (GNNs). The formulation of DP in the context of graph-structured data is difficult, as individual data points are interconnected (often non-linearly or sparsely). This connectivity complicates the computation of individual privacy loss in differentially private learning. The problem is exacerbated by an absence of a single, well-established formulation of DP in graph settings. This issue extends to the domain of GNNs, rendering private machine learning on graph-structured data a challenging task. A lack of prior systematisation work motivated us to study graph-based learning from a privacy perspective. In this work, we systematise different formulations of DP on graphs, discuss challenges and promising applications, including the GNN domain. We compare and separate works into graph analysis tasks and graph learning tasks with GNNs. Finally, we conclude our work with a discussion of open questions and potential directions for further research in this area.

</p>
</details>

<details><summary><b>A Novel End-To-End Network for Reconstruction of Non-Regularly Sampled Image Data Using Locally Fully Connected Layers</b>
<a href="https://arxiv.org/abs/2203.09180">arxiv:2203.09180</a>
&#x1F4C8; 3 <br>
<p>Simon Grosche, Fabian Brand, Andr√© Kaup</p></summary>
<p>

**Abstract:** Quarter sampling and three-quarter sampling are novel sensor concepts that enable the acquisition of higher resolution images without increasing the number of pixels. This is achieved by non-regularly covering parts of each pixel of a low-resolution sensor such that only one quadrant or three quadrants of the sensor area of each pixel is sensitive to light. Combining a properly designed mask and a high-quality reconstruction algorithm, a higher image quality can be achieved than using a low-resolution sensor and subsequent upsampling. For the latter case, the image quality can be further enhanced using super resolution algorithms such as the very deep super resolution network (VDSR). In this paper, we propose a novel end-to-end neural network to reconstruct high resolution images from non-regularly sampled sensor data. The network is a concatenation of a locally fully connected reconstruction network (LFCR) and a standard VDSR network. Altogether, using a three-quarter sampling sensor with our novel neural network layout, the image quality in terms of PSNR for the Urban100 dataset can be increased by 2.96 dB compared to the state-of-the-art approach. Compared to a low-resolution sensor with VDSR, a gain of 1.11 dB is achieved.

</p>
</details>

<details><summary><b>Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding</b>
<a href="https://arxiv.org/abs/2203.09175">arxiv:2203.09175</a>
&#x1F4C8; 3 <br>
<p>Joachim Nyborg, Charlotte Pelletier, Ira Assent</p></summary>
<p>

**Abstract:** Large-scale crop type classification is a task at the core of remote sensing efforts with applications of both economic and ecological importance. Current state-of-the-art deep learning methods are based on self-attention and use satellite image time series (SITS) to discriminate crop types based on their unique growth patterns. However, existing methods generalize poorly to regions not seen during training mainly due to not being robust to temporal shifts of the growing season caused by variations in climate. To this end, we propose Thermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike previous positional encoding based on calendar time (e.g. day-of-year), TPE is based on thermal time, which is obtained by accumulating daily average temperatures over the growing season. Since crop growth is directly related to thermal time, but not calendar time, TPE addresses the temporal shifts between different regions to improve generalization. We propose multiple TPE strategies, including learnable methods, to further improve results compared to the common fixed positional encodings. We demonstrate our approach on a crop classification task across four different European regions, where we obtain state-of-the-art generalization results.

</p>
</details>

<details><summary><b>Modeling Dual Read/Write Paths for Simultaneous Machine Translation</b>
<a href="https://arxiv.org/abs/2203.09163">arxiv:2203.09163</a>
&#x1F4C8; 3 <br>
<p>Shaolei Zhang, Yang Feng</p></summary>
<p>

**Abstract:** Simultaneous machine translation (SiMT) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path. Although the read/write path is essential to SiMT performance, no direct supervision is given to the path in the existing methods. In this paper, we propose a method of dual-path SiMT which introduces duality constraints to direct the read/write path. According to duality constraints, the read/write path in source-to-target and target-to-source SiMT models can be mapped to each other. As a result, the two SiMT models can be optimized jointly by forcing their read/write paths to satisfy the mapping. Experiments on En-Vi and De-En tasks show that our method can outperform strong baselines under all latency.

</p>
</details>

<details><summary><b>Optimal Rejection Function Meets Character Recognition Tasks</b>
<a href="https://arxiv.org/abs/2203.09151">arxiv:2203.09151</a>
&#x1F4C8; 3 <br>
<p>Xiaotong Ji, Yuchen Zheng, Daiki Suehiro, Seiichi Uchida</p></summary>
<p>

**Abstract:** In this paper, we propose an optimal rejection method for rejecting ambiguous samples by a rejection function. This rejection function is trained together with a classification function under the framework of Learning-with-Rejection (LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic but has a strong background from a machine learning theory, and (2) the rejection function can be trained on an arbitrary feature space which is different from the feature space for classification. The latter suggests we can choose a feature space that is more suitable for rejection. Although the past research on LwR focused only on its theoretical aspect, we propose to utilize LwR for practical pattern classification tasks. Moreover, we propose to use features from different CNN layers for classification and rejection. Our extensive experiments of notMNIST classification and character/non-character classification demonstrate that the proposed method achieves better performance than traditional rejection strategies.

</p>
</details>

<details><summary><b>POLARIS: A Geographic Pre-trained Model and its Applications in Baidu Maps</b>
<a href="https://arxiv.org/abs/2203.09127">arxiv:2203.09127</a>
&#x1F4C8; 3 <br>
<p>Jizhou Huang, Haifeng Wang, Yibo Sun, Yunsheng Shi, Zhengjie Huang, An Zhuo, Shikun Feng</p></summary>
<p>

**Abstract:** Pre-trained models (PTMs) have become a fundamental backbone for downstream tasks in natural language processing and computer vision. Despite initial gains that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps, a clear performance plateau over time was observed. One of the main reasons for this plateau is the lack of readily available geographic knowledge in generic PTMs. To address this problem, in this paper, we present POLARIS, which is a geographic pre-trained model designed and developed for improving the geo-related tasks at Baidu Maps. POLARIS is elaborately designed to learn a universal representation of geography-language by pre-training on large-scale data generated from a heterogeneous graph that contains abundant geographic knowledge. Extensive quantitative and qualitative experiments conducted on large-scale real-world datasets demonstrate the superiority and effectiveness of POLARIS. POLARIS has already been deployed in production at Baidu Maps since April 2021, which significantly benefits the performance of a wide range of downstream tasks. This demonstrates that POLARIS can serve as a fundamental backbone for geo-related tasks.

</p>
</details>

<details><summary><b>MotionAug: Augmentation with Physical Correction for Human Motion Prediction</b>
<a href="https://arxiv.org/abs/2203.09116">arxiv:2203.09116</a>
&#x1F4C8; 3 <br>
<p>Takahiro Maeda, Norimichi Ukita</p></summary>
<p>

**Abstract:** This paper presents a motion data augmentation scheme incorporating motion synthesis encouraging diversity and motion correction imposing physical plausibility. This motion synthesis consists of our modified Variational AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed sampling-near-samples method generates various valid motions even with insufficient training motion data. Our IK-based motion synthesis method allows us to generate a variety of motions semi-automatically. Since these two schemes generate unrealistic artifacts in the synthesized motions, our motion correction rectifies them. This motion correction scheme consists of imitation learning with physics simulation and subsequent motion debiasing. For this imitation learning, we propose the PD-residual force that significantly accelerates the training process. Furthermore, our motion debiasing successfully offsets the motion bias induced by imitation learning to maximize the effect of augmentation. As a result, our method outperforms previous noise-based motion augmentation methods by a large margin on both Recurrent Neural Network-based and Graph Convolutional Network-based human motion prediction models. The code is available at {\rm \url{https://github.com/meaten/MotionAug}}.

</p>
</details>

<details><summary><b>TMS: A Temporal Multi-scale Backbone Design for Speaker Embedding</b>
<a href="https://arxiv.org/abs/2203.09098">arxiv:2203.09098</a>
&#x1F4C8; 3 <br>
<p>Ruiteng Zhang, Jianguo Wei, Xugang Lu, Wenhuan Lu, Di Jin, Junhai Xu, Lin Zhang, Yantao Ji, Jianwu Dang</p></summary>
<p>

**Abstract:** Speaker embedding is an important front-end module to explore discriminative speaker features for many speech applications where speaker information is needed. Current SOTA backbone networks for speaker embedding are designed to aggregate multi-scale features from an utterance with multi-branch network architectures for speaker representation. However, naively adding many branches of multi-scale features with the simple fully convolutional operation could not efficiently improve the performance due to the rapid increase of model parameters and computational complexity. Therefore, in the most current state-of-the-art network architectures, only a few branches corresponding to a limited number of temporal scales could be designed for speaker embeddings. To address this problem, in this paper, we propose an effective temporal multi-scale (TMS) model where multi-scale branches could be efficiently designed in a speaker embedding network almost without increasing computational costs. The new model is based on the conventional TDNN, where the network architecture is smartly separated into two modeling operators: a channel-modeling operator and a temporal multi-branch modeling operator. Adding temporal multi-scale in the temporal multi-branch operator needs only a little bit increase of the number of parameters, and thus save more computational budget for adding more branches with large temporal scales. Moreover, in the inference stage, we further developed a systemic re-parameterization method to convert the TMS-based model into a single-path-based topology in order to increase inference speed. We investigated the performance of the new TMS method for automatic speaker verification (ASV) on in-domain and out-of-domain conditions. Results show that the TMS-based model obtained a significant increase in the performance over the SOTA ASV models, meanwhile, had a faster inference speed.

</p>
</details>

<details><summary><b>Decouple-and-Sample: Protecting sensitive information in task agnostic data release</b>
<a href="https://arxiv.org/abs/2203.13204">arxiv:2203.13204</a>
&#x1F4C8; 2 <br>
<p>Abhishek Singh, Ethan Garza, Ayush Chopra, Praneeth Vepakomma, Vivek Sharma, Ramesh Raskar</p></summary>
<p>

**Abstract:** We propose sanitizer, a framework for secure and task-agnostic data release. While releasing datasets continues to make a big impact in various applications of computer vision, its impact is mostly realized when data sharing is not inhibited by privacy concerns. We alleviate these concerns by sanitizing datasets in a two-stage process. First, we introduce a global decoupling stage for decomposing raw data into sensitive and non-sensitive latent representations. Secondly, we design a local sampling stage to synthetically generate sensitive information with differential privacy and merge it with non-sensitive latent features to create a useful representation while preserving the privacy. This newly formed latent information is a task-agnostic representation of the original dataset with anonymized sensitive information. While most algorithms sanitize data in a task-dependent manner, a few task-agnostic sanitization techniques sanitize data by censoring sensitive information. In this work, we show that a better privacy-utility trade-off is achieved if sensitive information can be synthesized privately. We validate the effectiveness of the sanitizer by outperforming state-of-the-art baselines on the existing benchmark tasks and demonstrating tasks that are not possible using existing techniques.

</p>
</details>

<details><summary><b>Towards an AI-Driven Universal Anti-Jamming Solution with Convolutional Interference Cancellation Network</b>
<a href="https://arxiv.org/abs/2203.09717">arxiv:2203.09717</a>
&#x1F4C8; 2 <br>
<p>Hai N. Nguyen, Guevara Noubir</p></summary>
<p>

**Abstract:** Wireless links are increasingly used to deliver critical services, while intentional interference (jamming) remains a very serious threat to such services. In this paper, we are concerned with the design and evaluation of a universal anti-jamming building block, that is agnostic to the specifics of the communication link and can therefore be combined with existing technologies. We believe that such a block should not require explicit probes, sounding, training sequences, channel estimation, or even the cooperation of the transmitter. To meet these requirements, we propose an approach that relies on advances in Machine Learning, and the promises of neural accelerators and software defined radios. We identify and address multiple challenges, resulting in a convolutional neural network architecture and models for a multi-antenna system to infer the existence of interference, the number of interfering emissions and their respective phases. This information is continuously fed into an algorithm that cancels the interfering signal. We develop a two-antenna prototype system and evaluate our jamming cancellation approach in various environment settings and modulation schemes using Software Defined Radio platforms. We demonstrate that the receiving node equipped with our approach can detect a jammer with over 99% of accuracy and achieve a Bit Error Rate (BER) as low as $10^{-6}$ even when the jammer power is nearly two orders of magnitude (18 dB) higher than the legitimate signal, and without requiring modifications to the link modulation. In non-adversarial settings, our approach can have other advantages such as detecting and mitigating collisions.

</p>
</details>

<details><summary><b>Overview of Test Coverage Criteria for Test Case Generation from Finite State Machines Modelled as Directed Graphs</b>
<a href="https://arxiv.org/abs/2203.09604">arxiv:2203.09604</a>
&#x1F4C8; 2 <br>
<p>Vaclav Rechtberger, Miroslav Bures, Bestoun S. Ahmed</p></summary>
<p>

**Abstract:** Test Coverage criteria are an essential concept for test engineers when generating the test cases from a System Under Test model. They are routinely used in test case generation for user interfaces, middleware, and back-end system parts for software, electronics, or Internet of Things (IoT) systems. Test Coverage criteria define the number of actions or combinations by which a system is tested, informally determining a potential "strength" of a test set. As no previous study summarized all commonly used test coverage criteria for Finite State Machines and comprehensively discussed them regarding their subsumption, equivalence, or non-comparability, this paper provides this overview. In this study, 14 most common test coverage criteria and seven of their synonyms for Finite State Machines defined via a directed graph are summarized and compared. The results give researchers and industry testing engineers a helpful overview when setting a software-based or IoT system test strategy.

</p>
</details>

<details><summary><b>GAC: A Deep Reinforcement Learning Model Toward User Incentivization in Unknown Social Networks</b>
<a href="https://arxiv.org/abs/2203.09578">arxiv:2203.09578</a>
&#x1F4C8; 2 <br>
<p>Shiqing Wu, Weihua Li, Quan Bai</p></summary>
<p>

**Abstract:** In recent years, providing incentives to human users for attracting their attention and engagement has been widely adopted in many applications. To effectively incentivize users, most incentive mechanisms determine incentive values based on users' individual attributes, such as preferences. These approaches could be ineffective when such information is unavailable. Meanwhile, due to the budget limitation, the number of users who can be incentivized is also restricted. In this light, we intend to utilize social influence among users to maximize the incentivization. By directly incentivizing influential users in the social network, their followers and friends could be indirectly incentivized with fewer incentives or no incentive. However, it is difficult to identify influential users beforehand in the social network, as the influence strength between each pair of users is typically unknown. In this work, we propose an end-to-end reinforcement learning-based framework, named Geometric Actor-Critic (GAC), to discover effective incentive allocation policies under limited budgets. More specifically, the proposed approach can extract information from a high-level network representation for learning effective incentive allocation policies. The proposed GAC only requires the topology of the social network and does not rely on any prior information about users' attributes. We use three real-world social network datasets to evaluate the performance of the proposed GAC. The experimental results demonstrate the effectiveness of the proposed approach.

</p>
</details>

<details><summary><b>Using the Order of Tomographic Slices as a Prior for Neural Networks Pre-Training</b>
<a href="https://arxiv.org/abs/2203.09372">arxiv:2203.09372</a>
&#x1F4C8; 2 <br>
<p>Yaroslav Zharov, Alexey Ershov, Tilo Baumbach, Vincent Heuveline</p></summary>
<p>

**Abstract:** The technical advances in Computed Tomography (CT) allow to obtain immense amounts of 3D data. For such datasets it is very costly and time-consuming to obtain the accurate 3D segmentation markup to train neural networks. The annotation is typically done for a limited number of 2D slices, followed by an interpolation. In this work, we propose a pre-training method SortingLoss. It performs pre-training on slices instead of volumes, so that a model could be fine-tuned on a sparse set of slices, without the interpolation step. Unlike general methods (e.g. SimCLR or Barlow Twins), the task specific methods (e.g. Transferable Visual Words) trade broad applicability for quality benefits by imposing stronger assumptions on the input data. We propose a relatively mild assumption -- if we take several slices along some axis of a volume, structure of the sample presented on those slices, should give a strong clue to reconstruct the correct order of those slices along the axis. Many biomedical datasets fulfill this requirement due to the specific anatomy of a sample and pre-defined alignment of the imaging setup. We examine the proposed method on two datasets: medical CT of lungs affected by COVID-19 disease, and high-resolution synchrotron-based full-body CT of model organisms (Medaka fish). We show that the proposed method performs on par with SimCLR, while working 2x faster and requiring 1.5x less memory. In addition, we present the benefits in terms of practical scenarios, especially the applicability to the pre-training of large models and the ability to localize samples within volumes in an unsupervised setup.

</p>
</details>

<details><summary><b>Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models</b>
<a href="https://arxiv.org/abs/2203.09354">arxiv:2203.09354</a>
&#x1F4C8; 2 <br>
<p>Nathan Vaska, Kevin Leahy, Victoria Helus</p></summary>
<p>

**Abstract:** Increasing the semantic understanding and contextual awareness of machine learning models is important for improving robustness and reducing susceptibility to data shifts. In this work, we leverage contextual awareness for the anomaly detection problem. Although graphed-based anomaly detection has been widely studied, context-dependent anomaly detection is an open problem and without much current research. We develop a general framework for converting a context-dependent anomaly detection problem to a link prediction problem, allowing well-established techniques from this domain to be applied. We implement a system based on our framework that utilizes knowledge graph embedding models and demonstrates the ability to detect outliers using context provided by a semantic knowledge base. We show that our method can detect context-dependent anomalies with a high degree of accuracy and show that current object detectors can detect enough classes to provide the needed context for good performance within our example domain.

</p>
</details>

<details><summary><b>Dimensionality Reduction and Wasserstein Stability for Kernel Regression</b>
<a href="https://arxiv.org/abs/2203.09347">arxiv:2203.09347</a>
&#x1F4C8; 2 <br>
<p>Stephan Eckstein, Armin Iske, Mathias Trabs</p></summary>
<p>

**Abstract:** In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable. More specifically we combine principal component analysis (PCA) with kernel regression. In order to analyze the resulting regression errors, a novel stability result of kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit a kernel function. We combine the stability result with known estimates from the literature on both principal component analysis and kernel regression to obtain convergence rates for the two-step procedure.

</p>
</details>

<details><summary><b>Depth-aware Neural Style Transfer using Instance Normalization</b>
<a href="https://arxiv.org/abs/2203.09242">arxiv:2203.09242</a>
&#x1F4C8; 2 <br>
<p>Eleftherios Ioannou, Steve Maddock</p></summary>
<p>

**Abstract:** Neural Style Transfer (NST) is concerned with the artistic stylization of visual media. It can be described as the process of transferring the style of an artistic image onto an ordinary photograph. Recently, a number of studies have considered the enhancement of the depth-preserving capabilities of the NST algorithms to address the undesired effects that occur when the input content images include numerous objects at various depths. Our approach uses a deep residual convolutional network with instance normalization layers that utilizes an advanced depth prediction network to integrate depth preservation as an additional loss function to content and style. We demonstrate results that are effective in retaining the depth and global structure of content images. Three different evaluation processes show that our system is capable of preserving the structure of the stylized results while exhibiting style-capture capabilities and aesthetic qualities comparable or superior to state-of-the-art methods.

</p>
</details>

<details><summary><b>HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR</b>
<a href="https://arxiv.org/abs/2203.09215">arxiv:2203.09215</a>
&#x1F4C8; 2 <br>
<p>Yudi Dai, Yitai Lin, Chenglu Wen, Siqi Shen, Lan Xu, Jingyi Yu, Yuexin Ma, Cheng Wang</p></summary>
<p>

**Abstract:** We propose Human-centered 4D Scene Capture (HSC4D) to accurately and efficiently create a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, and rich interactions between humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is space-free without any external devices' constraints and map-free without pre-built maps. Considering that IMUs can capture human poses but always drift for long-period use, while LiDAR is stable for global localization but rough for local positions and orientations, HSC4D makes both sensors complement each other by a joint optimization and achieves promising results for long-term capture. Relationships between humans and environments are also explored to make their interaction more realistic. To facilitate many down-stream tasks, like AR, VR, robots, autonomous driving, etc., we propose a dataset containing three large scenes (1k-5k $m^2$) with accurate dynamic human motions and locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.) and challenging human activities (exercising, walking up/down stairs, climbing, etc.) demonstrate the effectiveness and the generalization ability of HSC4D. The dataset and code is available at https://github.com/climbingdaily/HSC4D.

</p>
</details>

<details><summary><b>Novel Consistency Check For Fast Recursive Reconstruction Of Non-Regularly Sampled Video Data</b>
<a href="https://arxiv.org/abs/2203.09200">arxiv:2203.09200</a>
&#x1F4C8; 2 <br>
<p>Simon Grosche, J√ºrgen Seiler, Andr√© Kaup</p></summary>
<p>

**Abstract:** Quarter sampling is a novel sensor design that allows for an acquisition of higher resolution images without increasing the number of pixels. When being used for video data, one out of four pixels is measured in each frame. Effectively, this leads to a non-regular spatio-temporal sub-sampling. Compared to purely spatial or temporal sub-sampling, this allows for an increased reconstruction quality, as aliasing artifacts can be reduced. For the fast reconstruction of such sensor data with a fixed mask, recursive variant of frequency selective reconstruction (FSR) was proposed. Here, pixels measured in previous frames are projected into the current frame to support its reconstruction. In doing so, the motion between the frames is computed using template matching. Since some of the motion vectors may be erroneous, it is important to perform a proper consistency checking. In this paper, we propose faster consistency checking methods as well as a novel recursive FSR that uses the projected pixels different than in literature and can handle dynamic masks. Altogether, we are able to significantly increase the reconstruction quality by + 1.01 dB compared to the state-of-the-art recursive reconstruction method using a fixed mask. Compared to a single frame reconstruction, an average gain of about + 1.52 dB is achieved for dynamic masks. At the same time, the computational complexity of the consistency checks is reduced by a factor of 13 compared to the literature algorithm.

</p>
</details>

<details><summary><b>Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal Monte Carlo Samplers</b>
<a href="https://arxiv.org/abs/2203.09142">arxiv:2203.09142</a>
&#x1F4C8; 2 <br>
<p>Gersende Fort, Barbara Pascal, Patrice Abry, Nelly Pustelnik</p></summary>
<p>

**Abstract:** Monitoring the Covid19 pandemic constitutes a critical societal stake that received considerable research efforts. The intensity of the pandemic on a given territory is efficiently measured by the reproduction number, quantifying the rate of growth of daily new infections. Recently, estimates for the time evolution of the reproduction number were produced using an inverse problem formulation with a nonsmooth functional minimization. While it was designed to be robust to the limited quality of the Covid19 data (outliers, missing counts), the procedure lacks the ability to output credibility interval based estimates. This remains a severe limitation for practical use in actual pandemic monitoring by epidemiologists that the present work aims to overcome by use of Monte Carlo sampling. After interpretation of the functional into a Bayesian framework, several sampling schemes are tailored to adjust the nonsmooth nature of the resulting posterior distribution. The originality of the devised algorithms stems from combining a Langevin Monte Carlo sampling scheme with Proximal operators. Performance of the new algorithms in producing relevant credibility intervals for the reproduction number estimates and denoised counts are compared. Assessment is conducted on real daily new infection counts made available by the Johns Hopkins University. The interest of the devised monitoring tools are illustrated on Covid19 data from several different countries.

</p>
</details>

<details><summary><b>Short Text Topic Modeling: Application to tweets about Bitcoin</b>
<a href="https://arxiv.org/abs/2203.11152">arxiv:2203.11152</a>
&#x1F4C8; 1 <br>
<p>Hugo Schnoering</p></summary>
<p>

**Abstract:** Understanding the semantic of a collection of texts is a challenging task. Topic models are probabilistic models that aims at extracting "topics" from a corpus of documents. This task is particularly difficult when the corpus is composed of short texts, such as posts on social networks. Following several previous research papers, we explore in this paper a set of collected tweets about bitcoin. In this work, we train three topic models and evaluate their output with several scores. We also propose a concrete application of the extracted topics.

</p>
</details>

<details><summary><b>Developing a Successful Bomberman Agent</b>
<a href="https://arxiv.org/abs/2203.09608">arxiv:2203.09608</a>
&#x1F4C8; 1 <br>
<p>Dominik Kowalczyk, Jakub Kowalski, Hubert Obrzut, Micha≈Ç Maras, Szymon Kosakowski, Rados≈Çaw Miernik</p></summary>
<p>

**Abstract:** In this paper, we study AI approaches to successfully play a 2-4 players, full information, Bomberman variant published on the CodinGame platform. We compare the behavior of three search algorithms: Monte Carlo Tree Search, Rolling Horizon Evolution, and Beam Search. We present various enhancements leading to improve the agents' strength that concern search, opponent prediction, game state evaluation, and game engine encoding. Our top agent variant is based on a Beam Search with low-level bit-based state representation and evaluation function heavy relying on pruning unpromising states based on simulation-based estimation of survival. It reached the top one position among the 2,300 AI agents submitted on the CodinGame arena.

</p>
</details>

<details><summary><b>Strategic Maneuver and Disruption with Reinforcement Learning Approaches for Multi-Agent Coordination</b>
<a href="https://arxiv.org/abs/2203.09565">arxiv:2203.09565</a>
&#x1F4C8; 1 <br>
<p>Derrik E. Asher, Anjon Basak, Rolando Fernandez, Piyush K. Sharma, Erin G. Zaroukian, Christopher D. Hsu, Michael R. Dorothy, Thomas Mahre, Gerardo Galindo, Luke Frerichs, John Rogers, John Fossaceca</p></summary>
<p>

**Abstract:** Reinforcement learning (RL) approaches can illuminate emergent behaviors that facilitate coordination across teams of agents as part of a multi-agent system (MAS), which can provide windows of opportunity in various military tasks. Technologically advancing adversaries pose substantial risks to a friendly nation's interests and resources. Superior resources alone are not enough to defeat adversaries in modern complex environments because adversaries create standoff in multiple domains against predictable military doctrine-based maneuvers. Therefore, as part of a defense strategy, friendly forces must use strategic maneuvers and disruption to gain superiority in complex multi-faceted domains such as multi-domain operations (MDO). One promising avenue for implementing strategic maneuver and disruption to gain superiority over adversaries is through coordination of MAS in future military operations. In this paper, we present overviews of prominent works in the RL domain with their strengths and weaknesses for overcoming the challenges associated with performing autonomous strategic maneuver and disruption in military contexts.

</p>
</details>

<details><summary><b>A Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusion Problems</b>
<a href="https://arxiv.org/abs/2203.09436">arxiv:2203.09436</a>
&#x1F4C8; 1 <br>
<p>Xufeng Cai, Chaobing Song, Crist√≥bal Guzm√°n, Jelena Diakonikolas</p></summary>
<p>

**Abstract:** We study stochastic monotone inclusion problems, which widely appear in machine learning applications, including robust regression and adversarial learning. We propose novel variants of stochastic Halpern iteration with recursive variance reduction. In the cocoercive -- and more generally Lipschitz-monotone -- setup, our algorithm attains $Œµ$ norm of the operator with $\mathcal{O}(\frac{1}{Œµ^3})$ stochastic operator evaluations, which significantly improves over state of the art $\mathcal{O}(\frac{1}{Œµ^4})$ stochastic operator evaluations required for existing monotone inclusion solvers applied to the same problem classes. We further show how to couple one of the proposed variants of stochastic Halpern iteration with a scheduled restart scheme to solve stochastic monotone inclusion problems with ${\mathcal{O}}(\frac{\log(1/Œµ)}{Œµ^2})$ stochastic operator evaluations under additional sharpness or strong monotonicity assumptions. Finally, we argue via reductions between different problem classes that our stochastic oracle complexity bounds are tight up to logarithmic factors in terms of their $Œµ$-dependence.

</p>
</details>

<details><summary><b>Error estimates for physics informed neural networks approximating the Navier-Stokes equations</b>
<a href="https://arxiv.org/abs/2203.09346">arxiv:2203.09346</a>
&#x1F4C8; 1 <br>
<p>Tim De Ryck, Ameya D. Jagtap, Siddhartha Mishra</p></summary>
<p>

**Abstract:** We prove rigorous bounds on the errors resulting from the approximation of the incompressible Navier-Stokes equations with (extended) physics informed neural networks. We show that the underlying PDE residual can be made arbitrarily small for tanh neural networks with two hidden layers. Moreover, the total error can be estimated in terms of the training error, network size and number of quadrature points. The theory is illustrated with numerical experiments.

</p>
</details>

<details><summary><b>CodeReviewer: Pre-Training for Automating Code Review Activities</b>
<a href="https://arxiv.org/abs/2203.09095">arxiv:2203.09095</a>
&#x1F4C8; 1 <br>
<p>Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan</p></summary>
<p>

**Abstract:** Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review senario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews.

</p>
</details>

<details><summary><b>Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI</b>
<a href="https://arxiv.org/abs/2203.09268">arxiv:2203.09268</a>
&#x1F4C8; 0 <br>
<p>Stefano B. Blumberg, Hongxiang Lin, Francesco Grussu, Yukun Zhou, Matteo Figini, Daniel C. Alexander</p></summary>
<p>

**Abstract:** We present PROSUB: PROgressive SUBsampling, a deep learning based, automated methodology that subsamples an oversampled data set (e.g. multi-channeled 3D images) with minimal loss of information. We build upon a recent dual-network approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI measurement sampling-reconstruction challenge, but suffers from deep learning training instability, by subsampling with a hard decision boundary. PROSUB uses the paradigm of recursive feature elimination (RFE) and progressively subsamples measurements during deep learning training, improving optimization stability. PROSUB also integrates a neural architecture search (NAS) paradigm, allowing the network architecture hyperparameters to respond to the subsampling process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge, producing large improvements >18% MSE on the MUDI challenge sub-tasks and qualitative improvements on downstream processes useful for clinical applications. We also show the benefits of incorporating NAS and analyze the effect of PROSUB's components. As our method generalizes to other problems beyond MRI measurement selection-reconstruction, our code is https://github.com/sbb-gh/PROSUB

</p>
</details>


{% endraw %}
Prev: [2022.03.16]({{ '/2022/03/16/2022.03.16.html' | relative_url }})  Next: [2022.03.18]({{ '/2022/03/18/2022.03.18.html' | relative_url }})