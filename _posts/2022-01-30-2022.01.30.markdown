Prev: [2022.01.29]({{ '/2022/01/29/2022.01.29.html' | relative_url }})  Next: [2022.01.31]({{ '/2022/01/31/2022.01.31.html' | relative_url }})
{% raw %}
## Summary for 2022-01-30, created on 2022-02-09


<details><summary><b>Cryptocurrency Valuation: An Explainable AI Approach</b>
<a href="https://arxiv.org/abs/2201.12893">arxiv:2201.12893</a>
&#x1F4C8; 44 <br>
<p>Yulin Liu, Luyao Zhang</p></summary>
<p>

**Abstract:** Currently, there are no convincing proxies for the fundamentals of cryptocurrency assets. We propose a new market-to-fundamental ratio, the price-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We then proxy various fundamental-to-market ratios by Bitcoin historical data and find they have little predictive power for short-term bitcoin returns. However, PU ratio effectively predicts long-term bitcoin returns. We verify PU ratio valuation by unsupervised and supervised machine learning. The valuation method informs investment returns and predicts bull markets effectively. Finally, we present an automated trading strategy advised by the PU ratio that outperforms the conventional buy-and-hold and market-timing strategies. We distribute the trading algorithms as open-source software via Python Package Index for future research.

</p>
</details>

<details><summary><b>Deep Non-Crossing Quantiles through the Partial Derivative</b>
<a href="https://arxiv.org/abs/2201.12848">arxiv:2201.12848</a>
&#x1F4C8; 44 <br>
<p>Axel Brando, Joan Gimeno, Jose A. Rodríguez-Serrano, Jordi Vitrià</p></summary>
<p>

**Abstract:** Quantile Regression (QR) provides a way to approximate a single conditional quantile. To have a more informative description of the conditional distribution, QR can be merged with deep learning techniques to simultaneously estimate multiple quantiles. However, the minimisation of the QR-loss function does not guarantee non-crossing quantiles, which affects the validity of such predictions and introduces a critical issue in certain scenarios. In this article, we propose a generic deep learning algorithm for predicting an arbitrary number of quantiles that ensures the quantile monotonicity constraint up to the machine precision and maintains its modelling performance with respect to alternative models. The presented method is evaluated over several real-world datasets obtaining state-of-the-art results as well as showing that it scales to large-size data sets.

</p>
</details>

<details><summary><b>COIN++: Data Agnostic Neural Compression</b>
<a href="https://arxiv.org/abs/2201.12904">arxiv:2201.12904</a>
&#x1F4C8; 21 <br>
<p>Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Goliński, Yee Whye Teh, Arnaud Doucet</p></summary>
<p>

**Abstract:** Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the effectiveness of our method by compressing various data modalities, from images to medical and climate data.

</p>
</details>

<details><summary><b>Plug-In Inversion: Model-Agnostic Inversion for Vision with Data Augmentations</b>
<a href="https://arxiv.org/abs/2201.12961">arxiv:2201.12961</a>
&#x1F4C8; 9 <br>
<p>Amin Ghiasi, Hamid Kazemi, Steven Reich, Chen Zhu, Micah Goldblum, Tom Goldstein</p></summary>
<p>

**Abstract:** Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images. In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning. Under our proposed augmentation-based scheme, the same set of augmentation hyper-parameters can be used for inverting a wide range of image classification models, regardless of input dimensions or the architecture. We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works.

</p>
</details>

<details><summary><b>N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting</b>
<a href="https://arxiv.org/abs/2201.12886">arxiv:2201.12886</a>
&#x1F4C8; 8 <br>
<p>Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler, Artur Dubrawski</p></summary>
<p>

**Abstract:** Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting long-horizon forecasting are the volatility of the predictions and their computational complexity. In this paper, we introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, selectively emphasizing components with different frequencies and scales, while decomposing the input signal and synthesizing the forecast. We conduct an extensive empirical evaluation demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon forecasting methods. On an array of multivariate forecasting tasks, the proposed method provides an average accuracy improvement of 25% over the latest Transformer architectures while reducing the computation time by an order of magnitude. Our code is available at https://github.com/cchallu/n-hits.

</p>
</details>

<details><summary><b>Similarity and Generalization: From Noise to Corruption</b>
<a href="https://arxiv.org/abs/2201.12803">arxiv:2201.12803</a>
&#x1F4C8; 5 <br>
<p>Nayara Fonseca, Veronica Guidetti</p></summary>
<p>

**Abstract:** Contrastive learning aims to extract distinctive features from data by finding an embedding representation where similar samples are close to each other, and different ones are far apart. We study generalization in contrastive learning, focusing on its simplest representative: Siamese Neural Networks (SNNs). We show that Double Descent also appears in SNNs and is exacerbated by noise. We point out that SNNs can be affected by two distinct sources of noise: Pair Label Noise (PLN) and Single Label Noise (SLN). The effect of SLN is asymmetric, but it preserves similarity relations, while PLN is symmetric but breaks transitivity. We show that the dataset topology crucially affects generalization. While sparse datasets show the same performances under SLN and PLN for an equal amount of noise, SLN outperforms PLN in the overparametrized region in dense datasets. Indeed, in this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete overfitting cannot be achieved. We call this phenomenon Density-Induced Break of Similarity (DIBS). We also probe the equivalence between online optimization and offline generalization for similarity tasks. We observe that an online/offline correspondence in similarity learning can be affected by both the network architecture and label noise.

</p>
</details>

<details><summary><b>No-Regret Learning in Time-Varying Zero-Sum Games</b>
<a href="https://arxiv.org/abs/2201.12736">arxiv:2201.12736</a>
&#x1F4C8; 5 <br>
<p>Mengxiao Zhang, Peng Zhao, Haipeng Luo, Zhi-Hua Zhou</p></summary>
<p>

**Abstract:** Learning from repeated play in a fixed two-player zero-sum game is a classic problem in game theory and online learning. We consider a variant of this problem where the game payoff matrix changes over time, possibly in an adversarial manner. We first present three performance measures to guide the algorithmic design for this problem: 1) the well-studied individual regret, 2) an extension of duality gap, and 3) a new measure called dynamic Nash Equilibrium regret, which quantifies the cumulative difference between the player's payoff and the minimax game value. Next, we develop a single parameter-free algorithm that simultaneously enjoys favorable guarantees under all these three performance measures. These guarantees are adaptive to different non-stationarity measures of the payoff matrices and, importantly, recover the best known results when the payoff matrix is fixed. Our algorithm is based on a two-layer structure with a meta-algorithm learning over a group of black-box base-learners satisfying a certain property, along with several novel ingredients specifically designed for the time-varying game setting. Empirical results further validate the effectiveness of our algorithm.

</p>
</details>

<details><summary><b>Rotting infinitely many-armed bandits</b>
<a href="https://arxiv.org/abs/2201.12975">arxiv:2201.12975</a>
&#x1F4C8; 4 <br>
<p>Jung-hun Kim, Milan Vojnovic, Se-Young Yun</p></summary>
<p>

**Abstract:** We consider the infinitely many-armed bandit problem with rotting rewards, where the mean reward of an arm decreases at each pull of the arm according to an arbitrary trend with maximum rotting rate $\varrho=o(1)$. We show that this learning problem has an $Ω(\max\{\varrho^{1/3}T,\sqrt{T}\})$ worst-case regret lower bound where $T$ is the horizon time. We show that a matching upper bound $\tilde{O}(\max\{\varrho^{1/3}T,\sqrt{T}\})$, up to a poly-logarithmic factor, can be achieved by an algorithm that uses a UCB index for each arm and a threshold value to decide whether to continue pulling an arm or remove the arm from further consideration, when the algorithm knows the value of the maximum rotting rate $\varrho$. We also show that an $\tilde{O}(\max\{\varrho^{1/3}T,T^{3/4}\})$ regret upper bound can be achieved by an algorithm that does not know the value of $\varrho$, by using an adaptive UCB index along with an adaptive threshold value.

</p>
</details>

<details><summary><b>Fair Wrapping for Black-box Predictions</b>
<a href="https://arxiv.org/abs/2201.12947">arxiv:2201.12947</a>
&#x1F4C8; 4 <br>
<p>Alexander Soen, Ibrahim Alabdulmohsin, Sanmi Koyejo, Yishay Mansour, Nyalleng Moorosi, Richard Nock, Ke Sun, Lexing Xie</p></summary>
<p>

**Abstract:** We introduce a new family of techniques to post-process ("wrap") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimisation can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an α-tree, which modifies the prediction. We provide two generic boosting algorithms to learn α-trees. We show that our modification has appealing properties in terms of composition ofα-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value at risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets.

</p>
</details>

<details><summary><b>Scaling Gaussian Process Optimization by Evaluating a Few Unique Candidates Multiple Times</b>
<a href="https://arxiv.org/abs/2201.12909">arxiv:2201.12909</a>
&#x1F4C8; 4 <br>
<p>Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, Lorenzo Rosasco</p></summary>
<p>

**Abstract:** Computing a Gaussian process (GP) posterior has a computational cost cubical in the number of historical points. A reformulation of the same GP posterior highlights that this complexity mainly depends on how many \emph{unique} historical points are considered. This can have important implication in active learning settings, where the set of historical points is constructed sequentially by the learner. We show that sequential black-box optimization based on GPs (GP-Opt) can be made efficient by sticking to a candidate solution for multiple evaluation steps and switch only when necessary. Limiting the number of switches also limits the number of unique points in the history of the GP. Thus, the efficient GP reformulation can be used to exactly and cheaply compute the posteriors required to run the GP-Opt algorithms. This approach is especially useful in real-world applications of GP-Opt with high switch costs (e.g. switching chemicals in wet labs, data/model loading in hyperparameter optimization). As examples of this meta-approach, we modify two well-established GP-Opt algorithms, GP-UCB and GP-EI, to switch candidates as infrequently as possible adapting rules from batched GP-Opt. These versions preserve all the theoretical no-regret guarantees while improving practical aspects of the algorithms such as runtime, memory complexity, and the ability of batching candidates and evaluating them in parallel.

</p>
</details>

<details><summary><b>5G enabled Mobile Edge Computing security for Autonomous Vehicles</b>
<a href="https://arxiv.org/abs/2202.00005">arxiv:2202.00005</a>
&#x1F4C8; 3 <br>
<p>Daryll Ralph D'Costa, Dr. Robert Abbas</p></summary>
<p>

**Abstract:** The world is moving into a new era with the deployment of 5G communication infrastructure. Many new developments are deployed centred around this technology. One such advancement is 5G Vehicle to Everything communication. This technology can be used for applications such as driverless delivery of goods, immediate response to emergencies and improving traffic efficiency. The concept of Intelligent Transport Systems (ITS) is built around this system which is completely autonomous. This paper studies the Distributed Denial of Service (DDoS) attack carried out over a 5G network and analyses security attacks, particularly the DDoS attack. The aim is to implement a machine learning model capable of classifying different types of DDoS attacks and predicting the quality of 5G latency. The initial steps of implementation involved the synthetic addition of 5G parameters into the dataset. Subsequently, the data was label encoded, and minority classes were oversampled to match the other classes. Finally, the data was split as training and testing, and machine learning models were applied. Although the paper resulted in a model that predicted DDoS attacks, the dataset acquired significantly lacked 5G related information. Furthermore, the 5G classification model needed more modification. The research was based on largely quantitative research methods in a simulated environment. Hence, the biggest limitation of this research has been the lack of resources for data collection and sole reliance on online data sets. Ideally, a Vehicle to Everything (V2X) project would greatly benefit from an autonomous 5G enabled vehicle connected to a mobile edge cloud. However, this project was conducted solely online on a single PC which further limits the outcomes. Although the model underperformed, this paper can be used as a framework for future research in Intelligent Transport System development.

</p>
</details>

<details><summary><b>GenMod: A generative modeling approach for spectral representation of PDEs with random inputs</b>
<a href="https://arxiv.org/abs/2201.12973">arxiv:2201.12973</a>
&#x1F4C8; 3 <br>
<p>Jacqueline Wentz, Alireza Doostan</p></summary>
<p>

**Abstract:** We propose a method for quantifying uncertainty in high-dimensional PDE systems with random parameters, where the number of solution evaluations is small. Parametric PDE solutions are often approximated using a spectral decomposition based on polynomial chaos expansions. For the class of systems we consider (i.e., high dimensional with limited solution evaluations) the coefficients are given by an underdetermined linear system in a regression formulation. This implies additional assumptions, such as sparsity of the coefficient vector, are needed to approximate the solution. Here, we present an approach where we assume the coefficients are close to the range of a generative model that maps from a low to a high dimensional space of coefficients. Our approach is inspired be recent work examining how generative models can be used for compressed sensing in systems with random Gaussian measurement matrices. Using results from PDE theory on coefficient decay rates, we construct an explicit generative model that predicts the polynomial chaos coefficient magnitudes. The algorithm we developed to find the coefficients, which we call GenMod, is composed of two main steps. First, we predict the coefficient signs using Orthogonal Matching Pursuit. Then, we assume the coefficients are within a sparse deviation from the range of a sign-adjusted generative model. This allows us to find the coefficients by solving a nonconvex optimization problem, over the input space of the generative model and the space of sparse vectors. We obtain theoretical recovery results for a Lipschitz continuous generative model and for a more specific generative model, based on coefficient decay rate bounds. We examine three high-dimensional problems and show that, for all three examples, the generative model approach outperforms sparsity promoting methods at small sample sizes.

</p>
</details>

<details><summary><b>Compositionality as Lexical Symmetry</b>
<a href="https://arxiv.org/abs/2201.12926">arxiv:2201.12926</a>
&#x1F4C8; 3 <br>
<p>Ekin Akyürek, Jacob Andreas</p></summary>
<p>

**Abstract:** Standard deep network models lack the inductive biases needed to generalize compositionally in tasks like semantic parsing, translation, and question answering. A large body of work in natural language processing seeks to overcome this limitation with new model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general framework for compositional modeling that instead formulates compositionality as a constraint on data distributions. We prove that for any task factorizable into a lexicon and a composition function, there exists a family of data transformation functions that are guaranteed to produce new, well-formed examples when applied to training data. We further show that it is possible to identify these data transformations even when the composition function is unknown (e.g. when we do not know how to write or infer a symbolic grammar). Using these transformation functions to perform data augmentation for ordinary RNN and transformer sequence models, we obtain state-of-the-art results on the CLEVR-CoGenT visual question answering dataset, and results comparable to specialized model architectures on the COGS semantic parsing dataset.

</p>
</details>

<details><summary><b>Provable Domain Generalization via Invariant-Feature Subspace Recovery</b>
<a href="https://arxiv.org/abs/2201.12919">arxiv:2201.12919</a>
&#x1F4C8; 3 <br>
<p>Haoxiang Wang, Haozhe Si, Bo Li, Han Zhao</p></summary>
<p>

**Abstract:** Domain generalization asks for models trained on a set of training environments to perform well on unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) has been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this paper, we propose to achieve domain generalization with Invariant-feature Subspace Recovery (ISR). Our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with $d_s+1$ training environments under the data model of Rosenfeld et al. (2021). Our second algorithm, ISR-Cov, further reduces the required number of training environments to $O(1)$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Empirically, our ISRs can obtain superior performance compared with IRM on synthetic benchmarks. In addition, on three real-world image and text datasets, we show that ISR-Mean can be used as a simple yet effective post-processing method to increase the worst-case accuracy of trained models against spurious correlations and group shifts.

</p>
</details>

<details><summary><b>Interpretable AI-based Large-scale 3D Pathloss Prediction Model for enabling Emerging Self-Driving Networks</b>
<a href="https://arxiv.org/abs/2201.12899">arxiv:2201.12899</a>
&#x1F4C8; 3 <br>
<p>Usama Masood, Hasan Farooq, Ali Imran, Adnan Abu-Dayya</p></summary>
<p>

**Abstract:** In modern wireless communication systems, radio propagation modeling to estimate pathloss has always been a fundamental task in system design and optimization. The state-of-the-art empirical propagation models are based on measurements in specific environments and limited in their ability to capture idiosyncrasies of various propagation environments. To cope with this problem, ray-tracing based solutions are used in commercial planning tools, but they tend to be extremely time-consuming and expensive. We propose a Machine Learning (ML)-based model that leverages novel key predictors for estimating pathloss. By quantitatively evaluating the ability of various ML algorithms in terms of predictive, generalization and computational performance, our results show that Light Gradient Boosting Machine (LightGBM) algorithm overall outperforms others, even with sparse training data, by providing a 65% increase in prediction accuracy as compared to empirical models and 13x decrease in prediction time as compared to ray-tracing. To address the interpretability challenge that thwarts the adoption of most ML-based models, we perform extensive secondary analysis using SHapley Additive exPlanations (SHAP) method, yielding many practically useful insights that can be leveraged for intelligently tuning the network configuration, selective enrichment of training data in real networks and for building lighter ML-based propagation model to enable low-latency use-cases.

</p>
</details>

<details><summary><b>Hyperbolic Neural Networks for Molecular Generation</b>
<a href="https://arxiv.org/abs/2201.12825">arxiv:2201.12825</a>
&#x1F4C8; 3 <br>
<p>Eric Qu, Dongmian Zou</p></summary>
<p>

**Abstract:** With the recent advance of deep learning, neural networks have been extensively used for the task of molecular generation. Many deep generators extract atomic relations from molecular graphs and ignore hierarchical information at both atom and molecule levels. In order to extract such hierarchical information, we propose a novel hyperbolic generative model. Our model contains three parts: first, a fully hyperbolic junction-tree encoder-decoder that embeds the hierarchical information of the molecules in the latent hyperbolic space; second, a latent generative adversarial network for generating the latent embeddings; third, a molecular generator that inherits the decoders from the first part and the latent generator from the second part. We evaluate our model on the ZINC dataset using the MOSES benchmarking platform and achieve competitive results, especially in metrics about structural similarity.

</p>
</details>

<details><summary><b>Improving End-to-End Contextual Speech Recognition with Fine-grained Contextual Knowledge Selection</b>
<a href="https://arxiv.org/abs/2201.12806">arxiv:2201.12806</a>
&#x1F4C8; 3 <br>
<p>Minglun Han, Linhao Dong, Zhenlin Liang, Meng Cai, Shiyu Zhou, Zejun Ma, Bo Xu</p></summary>
<p>

**Abstract:** Nowadays, most methods in end-to-end contextual speech recognition bias the recognition process towards contextual knowledge. Since all-neural contextual biasing methods rely on phrase-level contextual modeling and attention-based relevance modeling, they may encounter confusion between similar context-specific phrases, which hurts predictions at the token level. In this work, we focus on mitigating confusion problems with fine-grained contextual knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge to reduce the uncertainty of token predictions. Specifically, we first apply phrase selection to narrow the range of phrase candidates, and then conduct token attention on the tokens in the selected phrase candidates. Moreover, we re-normalize the attention weights of most relevant phrases in inference to obtain more focused phrase-level contextual representations, and inject position information to better discriminate phrases or tokens. On LibriSpeech and an in-house 160,000-hour dataset, we explore the proposed methods based on a controllable all-neural biasing method, collaborative decoding (ColDec). The proposed methods provide at most 6.1% relative word error rate reduction on LibriSpeech and 16.4% relative character error rate reduction on the in-house dataset over ColDec.

</p>
</details>

<details><summary><b>TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2201.12785">arxiv:2201.12785</a>
&#x1F4C8; 3 <br>
<p>Jiangyun Li, Wenxuan Wang, Chen Chen, Tianxiang Zhang, Sen Zha, Hong Yu, Jing Wang</p></summary>
<p>

**Abstract:** Transformer, benefiting from global (long-range) information modeling using self-attention mechanism, has been successful in natural language processing and computer vision recently. Convolutional Neural Networks, capable of capturing local features, are unable to model explicit long-distance dependencies from global feature space. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we exploit Transformer in 3D CNN for 3D medical image volumetric segmentation and propose a novel network named TransBTSV2 based on the encoder-decoder structure. Different from our original TransBTS, the proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but focuses on general medical image segmentation, providing a strong and efficient 3D baseline for volumetric segmentation of medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of medical images without any pre-training. With the proposed insight to redesign the internal structure of Transformer and the introduced Deformable Bottleneck Module, a highly efficient architecture is achieved with superior performance. Extensive experimental results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results as compared to the state-of-the-art methods for the segmentation of brain tumor, liver tumor as well as kidney tumor. Code is available at https://github.com/Wenxuan-1119/TransBTS.

</p>
</details>

<details><summary><b>Implicit Regularization Towards Rank Minimization in ReLU Networks</b>
<a href="https://arxiv.org/abs/2201.12760">arxiv:2201.12760</a>
&#x1F4C8; 3 <br>
<p>Nadav Timor, Gal Vardi, Ohad Shamir</p></summary>
<p>

**Abstract:** We study the conjectured relationship between the implicit regularization in neural networks, trained with gradient-based methods, and rank minimization of their weight matrices. Previously, it was proved that for linear networks (of depth 2 and vector-valued outputs), gradient flow (GF) w.r.t. the square loss acts as a rank minimization heuristic. However, understanding to what extent this generalizes to nonlinear networks is an open problem. In this paper, we focus on nonlinear ReLU networks, providing several new positive and negative results. On the negative side, we prove (and demonstrate empirically) that, unlike the linear case, GF on ReLU networks may no longer tend to minimize ranks, in a rather strong sense (even approximately, for "most" datasets of size 2). On the positive side, we reveal that ReLU networks of sufficient depth are provably biased towards low-rank solutions in several reasonable settings.

</p>
</details>

<details><summary><b>Approximate Bayesian Computation Based on Maxima Weighted Isolation Kernel Mapping</b>
<a href="https://arxiv.org/abs/2201.12745">arxiv:2201.12745</a>
&#x1F4C8; 3 <br>
<p>Iurii S. Nagornov</p></summary>
<p>

**Abstract:** Motivation: The branching processes model yields unevenly stochastically distributed data that consists of sparse and dense regions. The work tries to solve the problem of a precise evaluation of a parameter for this type of model. The application of the branching processes model to cancer cell evolution has many difficulties like high dimensionality and the rare appearance of a result of interest. Moreover, we would like to solve the ambitious task of obtaining the coefficients of the model reflecting the relationship of driver genes mutations and cancer hallmarks on the basis of personal data of variant allele frequencies. Results: The Approximate Bayesian computation method based on the Isolation kernel is designed. The method includes a transformation row data to a Hilbert space (mapping) and measures the similarity between simulation points and maxima weighted Isolation kernel mapping related to the observation point. Also, we designed a heuristic algorithm to find parameter estimation without gradient calculation and dimension-independent. The advantage of the proposed machine learning method is shown for multidimensional test data as well as for an example of cancer cell evolution.

</p>
</details>

<details><summary><b>FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting</b>
<a href="https://arxiv.org/abs/2201.12740">arxiv:2201.12740</a>
&#x1F4C8; 3 <br>
<p>Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, Rong Jin</p></summary>
<p>

**Abstract:** Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\%$ and $22.6\%$ for multivariate and univariate time series, respectively. the code will be released soon.

</p>
</details>

<details><summary><b>Heterogeneous Federated Learning via Grouped Sequential-to-Parallel Training</b>
<a href="https://arxiv.org/abs/2201.12976">arxiv:2201.12976</a>
&#x1F4C8; 2 <br>
<p>Shenglai Zeng, Zonghang Li, Hongfang Yu, Yihong He, Zenglin Xu, Dusit Niyato, Han Yu</p></summary>
<p>

**Abstract:** Federated learning (FL) is a rapidly growing privacy-preserving collaborative machine learning paradigm. In practical FL applications, local data from each data silo reflect local usage patterns. Therefore, there exists heterogeneity of data distributions among data owners (a.k.a. FL clients). If not handled properly, this can lead to model performance degradation. This challenge has inspired the research field of heterogeneous federated learning, which currently remains open. In this paper, we propose a data heterogeneity-robust FL approach, FedGSP, to address this challenge by leveraging on a novel concept of dynamic Sequential-to-Parallel (STP) collaborative training. FedGSP assigns FL clients to homogeneous groups to minimize the overall distribution divergence among groups, and increases the degree of parallelism by reassigning more groups in each round. It is also incorporated with a novel Inter-Cluster Grouping (ICG) algorithm to assist in group assignment, which uses the centroid equivalence theorem to simplify the NP-hard grouping problem to make it solvable. Extensive experiments have been conducted on the non-i.i.d. FEMNIST dataset. The results show that FedGSP improves the accuracy by 3.7% on average compared with seven state-of-the-art approaches, and reduces the training time and communication overhead by more than 90%.

</p>
</details>

<details><summary><b>Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection</b>
<a href="https://arxiv.org/abs/2201.12910">arxiv:2201.12910</a>
&#x1F4C8; 2 <br>
<p>Tomojit Ghosh, Michael Kirby</p></summary>
<p>

**Abstract:** We develop a sparse optimization problem for the determination of the total set of features that discriminate two or more classes. This is a sparse implementation of the centroid-encoder for nonlinear data reduction and visualization called Sparse Centroid-Encoder (SCE). We also provide a feature selection framework that first ranks each feature by its occurrence, and the optimal number of features is chosen using a validation set. The algorithm is applied to a wide variety of data sets including, single-cell biological data, high dimensional infectious disease data, hyperspectral data, image data, and speech data. We compared our method to various state-of-the-art feature selection techniques, including two neural network-based models (DFS, and LassoNet), Sparse SVM, and Random Forest. We empirically showed that SCE features produced better classification accuracy on the unseen test data, often with fewer features.

</p>
</details>

<details><summary><b>Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers</b>
<a href="https://arxiv.org/abs/2201.12896">arxiv:2201.12896</a>
&#x1F4C8; 2 <br>
<p>Rui P. Cardoso, Emma Hart, David Burth Kurka, Jeremy V. Pitt</p></summary>
<p>

**Abstract:** Using Neuroevolution combined with Novelty Search to promote behavioural diversity is capable of constructing high-performing ensembles for classification. However, using gradient descent to train evolved architectures during the search can be computationally prohibitive. Here we propose a method to overcome this limitation by using a surrogate model which estimates the behavioural distance between two neural network architectures required to calculate the sparseness term in Novelty Search. We demonstrate a speedup of 10 times over previous work and significantly improve on previous reported results on three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and SVHN. This results from the expanded architecture search space facilitated by using a surrogate. Our method represents an improved paradigm for implementing horizontal scaling of learning algorithms by making an explicit search for diversity considerably more tractable for the same bounded resources.

</p>
</details>

<details><summary><b>Learning Collective Action under Risk Diversity</b>
<a href="https://arxiv.org/abs/2201.12891">arxiv:2201.12891</a>
&#x1F4C8; 2 <br>
<p>Ramona Merhej, Fernando P. Santos, Francisco S. Melo, Mohamed Chetouani, Francisco C. Santos</p></summary>
<p>

**Abstract:** Collective risk dilemmas (CRDs) are a class of n-player games that represent societal challenges where groups need to coordinate to avoid the risk of a disastrous outcome. Multi-agent systems incurring such dilemmas face difficulties achieving cooperation and often converge to sub-optimal, risk-dominant solutions where everyone defects. In this paper we investigate the consequences of risk diversity in groups of agents learning to play CRDs. We find that risk diversity places new challenges to cooperation that are not observed in homogeneous groups. We show that increasing risk diversity significantly reduces overall cooperation and hinders collective target achievement. It leads to asymmetrical changes in agents' policies -- i.e. the increase in contributions from individuals at high risk is unable to compensate for the decrease in contributions from individuals at low risk -- which overall reduces the total contributions in a population. When comparing RL behaviors to rational individualistic and social behaviors, we find that RL populations converge to fairer contributions among agents. Our results highlight the need for aligning risk perceptions among agents or develop new learning techniques that explicitly account for risk diversity.

</p>
</details>

<details><summary><b>Fast Monte-Carlo Approximation of the Attention Mechanism</b>
<a href="https://arxiv.org/abs/2201.12854">arxiv:2201.12854</a>
&#x1F4C8; 2 <br>
<p>Hyunjun Kim, JeongGil Ko</p></summary>
<p>

**Abstract:** We introduce Monte-Carlo Attention (MCA), a randomized approximation method for reducing the computational cost of self-attention mechanisms in Transformer architectures. MCA exploits the fact that the importance of each token in an input sequence varies with respect to their attention scores; thus, some degree of error can be tolerable when encoding tokens with low attention. Using approximate matrix multiplication, MCA applies different error bounds to encode input tokens such that those with low attention scores are computed with relaxed precision, whereas errors of salient elements are minimized. MCA can operate in parallel with other attention optimization schemes and does not require model modification. We study the theoretical error bounds and demonstrate that MCA reduces attention complexity (in FLOPS) for various Transformer models by up to 11$\times$ in GLUE benchmarks without compromising model accuracy.

</p>
</details>

<details><summary><b>A Safety-Critical Decision Making and Control Framework Combining Machine Learning and Rule-based Algorithms</b>
<a href="https://arxiv.org/abs/2201.12819">arxiv:2201.12819</a>
&#x1F4C8; 2 <br>
<p>Andrei Aksjonov, Ville Kyrki</p></summary>
<p>

**Abstract:** While artificial-intelligence-based methods suffer from lack of transparency, rule-based methods dominate in safety-critical systems. Yet, the latter cannot compete with the first ones in robustness to multiple requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, to benefit from both methods they must be joined in a single system. This paper proposes a decision making and control framework, which profits from advantages of both the rule- and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. A rule-based switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized every time, when the Learned one does not meet the safety constraint, and also directly participates in the safe Learned controller training. Decision making and control in autonomous driving is chosen as the system case study, where an autonomous vehicle learns a multi-task policy to safely cross an unprotected intersection. Multiple requirements (i.e., safety, efficiency, and comfort) are set for vehicle operation. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environment is successfully demonstrated.

</p>
</details>

<details><summary><b>Contrastive Learning from Demonstrations</b>
<a href="https://arxiv.org/abs/2201.12813">arxiv:2201.12813</a>
&#x1F4C8; 2 <br>
<p>André Correia, Luís A. Alexandre</p></summary>
<p>

**Abstract:** This paper presents a framework for learning visual representations from unlabeled video demonstrations captured from multiple viewpoints. We show that these representations are applicable for imitating several robotic tasks, including pick and place. We optimize a recently proposed self-supervised learning algorithm by applying contrastive learning to enhance task-relevant information while suppressing irrelevant information in the feature embeddings. We validate the proposed method on the publicly available Multi-View Pouring and a custom Pick and Place data sets and compare it with the TCN triplet baseline. We evaluate the learned representations using three metrics: viewpoint alignment, stage classification and reinforcement learning, and in all cases the results improve when compared to state-of-the-art approaches, with the added benefit of reduced number of training iterations.

</p>
</details>

<details><summary><b>A Brief Overview of Physics-inspired Metaheuristic Optimization Techniques</b>
<a href="https://arxiv.org/abs/2201.12810">arxiv:2201.12810</a>
&#x1F4C8; 2 <br>
<p>Soumitri Chattopadhyay, Aritra Marik, Rishav Pramanik</p></summary>
<p>

**Abstract:** Metaheuristic algorithms are methods devised to efficiently solve computationally challenging optimization problems. Researchers have taken inspiration from various natural and physical processes alike to formulate meta-heuristics that have successfully provided near-optimal or optimal solutions to several engineering tasks. This chapter focuses on meta-heuristic algorithms modelled upon non-linear physical phenomena having a concrete optimization paradigm, having shown formidable exploration and exploitation abilities for such optimization problems. Specifically, this chapter focuses on several popular physics-based metaheuristics as well as describing the underlying unique physical processes associated with each algorithm.

</p>
</details>

<details><summary><b>Graph Self-Attention for learning graph representation with Transformer</b>
<a href="https://arxiv.org/abs/2201.12787">arxiv:2201.12787</a>
&#x1F4C8; 2 <br>
<p>Wonpyo Park, Woonggi Chang, Donggeon Lee, Juntae Kim</p></summary>
<p>

**Abstract:** We propose a novel Graph Self-Attention module to enable Transformer models to learn graph representation. We aim to incorporate graph information, on the attention map and hidden representations of Transformer. To this end, we propose context-aware attention which considers the interactions between query, key and graph information. Moreover, we propose graph-embedded value to encode the graph information on the hidden representation. Our extensive experiments and ablation studies validate that our method successfully encodes graph representation on Transformer architecture. Finally, our method achieves state-of-the-art performance on multiple benchmarks of graph representation learning, such as graph classification on images and molecules to graph regression on quantum chemistry.

</p>
</details>

<details><summary><b>Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent Decision-Making in Mixed Traffic Environments</b>
<a href="https://arxiv.org/abs/2201.12776">arxiv:2201.12776</a>
&#x1F4C8; 2 <br>
<p>Qi Liu, Zirui Li, Xueyuan Li, Jingda Wu, Shihua Yuan</p></summary>
<p>

**Abstract:** An efficient and reliable multi-agent decision-making system is highly demanded for the safe and efficient operation of connected autonomous vehicles in intelligent transportation systems. Current researches mainly focus on the Deep Reinforcement Learning (DRL) methods. However, utilizing DRL methods in interactive traffic scenarios is hard to represent the mutual effects between different vehicles and model the dynamic traffic environments due to the lack of interactive information in the representation of the environments, which results in low accuracy of cooperative decisions generation. To tackle these difficulties, this research proposes a framework to enable different Graph Reinforcement Learning (GRL) methods for decision-making, and compares their performance in interactive driving scenarios. GRL methods combinate the Graph Neural Network (GNN) and DRL to achieve the better decisions generation in interactive scenarios of autonomous vehicles, where the features of interactive scenarios are extracted by the GNN, and cooperative behaviors are generated by DRL framework. Several GRL approaches are summarized and implemented in the proposed framework. To evaluate the performance of the proposed GRL methods, an interactive driving scenarios on highway with two ramps is constructed, and simulated experiment in the SUMO platform is carried out to evaluate the performance of different GRL approaches. Finally, results are analyzed in multiple perspectives and dimensions to compare the characteristic of different GRL approaches in intelligent transportation scenarios. Results show that the implementation of GNN can well represents the interaction between vehicles, and the combination of GNN and DRL is able to improve the performance of the generation of lane-change behaviors. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL.

</p>
</details>

<details><summary><b>MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds</b>
<a href="https://arxiv.org/abs/2201.12769">arxiv:2201.12769</a>
&#x1F4C8; 2 <br>
<p>Chuanyu Luo, Xiaohan Li, Nuo Cheng, Han Li, Shengguang Lei, Pu Li</p></summary>
<p>

**Abstract:** Semantic segmentation of 3D point cloud is an essential task for autonomous driving environment perception. The pipeline of most pointwise point cloud semantic segmentation methods includes points sampling, neighbor searching, feature aggregation, and classification. Neighbor searching method like K-nearest neighbors algorithm, KNN, has been widely applied. However, the complexity of KNN is always a bottleneck of efficiency. In this paper, we propose an end-to-end neural architecture, Multiple View Pointwise Net, MVP-Net, to efficiently and directly infer large-scale outdoor point cloud without KNN or any complex pre/postprocessing. Instead, assumption-based sorting and multi-rotation of point cloud methods are introduced to point feature aggregation and receptive field expanding. Numerical experiments show that the proposed MVP-Net is 11 times faster than the most efficient pointwise semantic segmentation method RandLA-Net and achieves the same accuracy on the large-scale benchmark SemanticKITTI dataset.

</p>
</details>

<details><summary><b>Bayesian Optimization For Multi-Objective Mixed-Variable Problems</b>
<a href="https://arxiv.org/abs/2201.12767">arxiv:2201.12767</a>
&#x1F4C8; 2 <br>
<p>Haris Moazam Sheikh, Philip S. Marcus</p></summary>
<p>

**Abstract:** Optimizing multiple, non-preferential objectives for mixed-variable, expensive black-box problems is important in many areas of engineering and science. The expensive, noisy black-box nature of these problems makes them ideal candidates for Bayesian optimization (BO). Mixed-variable and multi-objective problems, however, are a challenge due to the BO's underlying smooth Gaussian process surrogate model. Current multi-objective BO algorithms cannot deal with mixed-variable problems. We present MixMOBO, the first mixed variable multi-objective Bayesian optimization framework for such problems. Using a genetic algorithm to sample the surrogate surface, optimal Pareto-fronts for multi-objective, mixed-variable design spaces can be found efficiently while ensuring diverse solutions. The method is sufficiently flexible to incorporate many different kernels and acquisition functions, including those that were developed for mixed-variable or multi-objective problems by other authors. We also present HedgeMO, a modified Hedge strategy that uses a portfolio of acquisition functions in multi-objective problems. We present a new acquisition function SMC. We show that MixMOBO performs well against other mixed-variable algorithms on synthetic problems. We apply MixMOBO to the real-world design of an architected material and show that our optimal design, which was experimentally fabricated and validated, has a normalized strain energy density $10^4$ times greater than existing structures.

</p>
</details>

<details><summary><b>AutoSNN: Towards Energy-Efficient Spiking Neural Networks</b>
<a href="https://arxiv.org/abs/2201.12738">arxiv:2201.12738</a>
&#x1F4C8; 2 <br>
<p>Byunggook Na, Jisoo Mok, Seongsik Park, Dongjin Lee, Hyeokjun Choe, Sungroh Yoon</p></summary>
<p>

**Abstract:** Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN architectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets.

</p>
</details>

<details><summary><b>TPC: Transformation-Specific Smoothing for Point Cloud Models</b>
<a href="https://arxiv.org/abs/2201.12733">arxiv:2201.12733</a>
&#x1F4C8; 2 <br>
<p>Wenda Chu, Linyi Li, Bo Li</p></summary>
<p>

**Abstract:** Point cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable against adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 3D transformations show that TPC significantly outperforms the state of the art. For example, our framework boosts the certified accuracy against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$ to 83.8$\%$.

</p>
</details>

<details><summary><b>Empirical complexity of comparator-based nearest neighbor descent</b>
<a href="https://arxiv.org/abs/2202.00517">arxiv:2202.00517</a>
&#x1F4C8; 1 <br>
<p>Jacob D. Baron, R. W. R. Darling</p></summary>
<p>

**Abstract:** A Java parallel streams implementation of the $K$-nearest neighbor descent algorithm is presented using a natural statistical termination criterion. Input data consist of a set $S$ of $n$ objects of type V, and a Function<V, Comparator<V>>, which enables any $x \in S$ to decide which of $y, z \in S\setminus\{x\}$ is more similar to $x$. Experiments with the Kullback-Leibler divergence Comparator support the prediction that the number of rounds of $K$-nearest neighbor updates need not exceed twice the diameter of the undirected version of a random regular out-degree $K$ digraph on $n$ vertices. Overall complexity was $O(n K^2 \log_K(n))$ in the class of examples studied. When objects are sampled uniformly from a $d$-dimensional simplex, accuracy of the $K$-nearest neighbor approximation is high up to $d = 20$, but declines in higher dimensions, as theory would predict.

</p>
</details>

<details><summary><b>Inverse design of photonic devices with strict foundry fabrication constraints</b>
<a href="https://arxiv.org/abs/2201.12965">arxiv:2201.12965</a>
&#x1F4C8; 1 <br>
<p>Martin F. Schubert, Alfred K. C. Cheung, Ian A. D. Williamson, Aleksandra Spyra, David H. Alexander</p></summary>
<p>

**Abstract:** We introduce a new method for inverse design of nanophotonic devices which guarantees that designs satisfy strict length scale constraints -- including minimum width and spacing constraints required by commercial semiconductor foundries. The method adopts several concepts from machine learning to transform the problem of topology optimization with strict length scale constraints to an unconstrained stochastic gradient optimization problem. Specifically, we introduce a conditional generator for feasible designs and adopt a straight-through estimator for backpropagation of gradients to a latent design. We demonstrate the performance and reliability of our method by designing several common integrated photonic components.

</p>
</details>

<details><summary><b>Generalized Bayesian Upper Confidence Bound with Approximate Inference for Bandit Problems</b>
<a href="https://arxiv.org/abs/2201.12955">arxiv:2201.12955</a>
&#x1F4C8; 1 <br>
<p>Ziyi Huang, Henry Lam, Amirhossein Meisami, Haofeng Zhang</p></summary>
<p>

**Abstract:** Bayesian bandit algorithms with approximate inference have been widely used in practice with superior performance. Yet, few studies regarding the fundamental understanding of their performances are available. In this paper, we propose a Bayesian bandit algorithm, which we call Generalized Bayesian Upper Confidence Bound (GBUCB), for bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that in Bernoulli multi-armed bandit, GBUCB can achieve $O(\sqrt{T}(\log T)^c)$ frequentist regret if the inference error measured by symmetrized Kullback-Leibler divergence is controllable. This analysis relies on a novel sensitivity analysis for quantile shifts with respect to inference errors. To our best knowledge, our work provides the first theoretical regret bound that is better than $o(T)$ in the setting of approximate inference. Our experimental evaluations on multiple approximate inference settings corroborate our theory, showing that our GBUCB is consistently superior to BUCB and Thompson sampling.

</p>
</details>

<details><summary><b>OpTopNET: A Learning Optimal Topology Synthesizer for Ad-hoc Robot Networks</b>
<a href="https://arxiv.org/abs/2201.12900">arxiv:2201.12900</a>
&#x1F4C8; 1 <br>
<p>Matin Macktoobian, Zhan Shu, Qing Zhao</p></summary>
<p>

**Abstract:** In this paper, we synthesize a machine-learning stacked ensemble model a vector of which predicts the optimal topology of a robot network. This problem is technically a multi-task classification problem. However, we divide it into a class of multi-class classification problems that can be more efficiently solved. For this purpose, we first compose an algorithm to create ground-truth topologies associated with various configurations of a robot network. This algorithm incorporates a complex collection of nonlinear optimality criteria that our learning model successfully manages to learn. Then, we propose a stacked ensemble model whose output is the topology prediction for the particular robot associated with it. Each stacked ensemble instance constitutes three low-level estimators whose outputs will be aggregated by a high-level boosting blender. The results of the simulations, applying our model to a network of 10 robots, represents over %80 accuracy in the prediction of optimal topologies corresponding to various configurations of this complex optimal topology learning problem.

</p>
</details>

<details><summary><b>Neural-PIM: Efficient Processing-In-Memory with Neural Approximation of Peripherals</b>
<a href="https://arxiv.org/abs/2201.12861">arxiv:2201.12861</a>
&#x1F4C8; 1 <br>
<p>Weidong Cao, Yilong Zhao, Adith Boloor, Yinhe Han, Xuan Zhang, Li Jiang</p></summary>
<p>

**Abstract:** Processing-in-memory (PIM) architectures have demonstrated great potential in accelerating numerous deep learning tasks. Particularly, resistive random-access memory (RRAM) devices provide a promising hardware substrate to build PIM accelerators due to their abilities to realize efficient in-situ vector-matrix multiplications (VMMs). However, existing PIM accelerators suffer from frequent and energy-intensive analog-to-digital (A/D) conversions, severely limiting their performance. This paper presents a new PIM architecture to efficiently accelerate deep learning tasks by minimizing the required A/D conversions with analog accumulation and neural approximated peripheral circuits. We first characterize the different dataflows employed by existing PIM accelerators, based on which a new dataflow is proposed to remarkably reduce the required A/D conversions for VMMs by extending shift and add (S+A) operations into the analog domain before the final quantizations. We then leverage a neural approximation method to design both analog accumulation circuits (S+A) and quantization circuits (ADCs) with RRAM crossbar arrays in a highly-efficient manner. Finally, we apply them to build an RRAM-based PIM accelerator (i.e., \textbf{Neural-PIM}) upon the proposed analog dataflow and evaluate its system-level performance. Evaluations on different benchmarks demonstrate that Neural-PIM can improve energy efficiency by 5.36x (1.73x) and speed up throughput by 3.43x (1.59x) without losing accuracy, compared to the state-of-the-art RRAM-based PIM accelerators, i.e., ISAAC (CASCADE).

</p>
</details>

<details><summary><b>Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning</b>
<a href="https://arxiv.org/abs/2201.12835">arxiv:2201.12835</a>
&#x1F4C8; 1 <br>
<p>Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim</p></summary>
<p>

**Abstract:** Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias). Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.

</p>
</details>

<details><summary><b>ClassSPLOM -- A Scatterplot Matrix to Visualize Separation of Multiclass Multidimensional Data</b>
<a href="https://arxiv.org/abs/2201.12822">arxiv:2201.12822</a>
&#x1F4C8; 1 <br>
<p>Michael Aupetit, Ahmed Ali</p></summary>
<p>

**Abstract:** In multiclass classification of multidimensional data, the user wants to build a model of the classes to predict the label of unseen data. The model is trained on the data and tested on unseen data with known labels to evaluate its quality. The results are visualized as a confusion matrix which shows how many data labels have been predicted correctly or confused with other classes. The multidimensional nature of the data prevents the direct visualization of the classes so we design ClassSPLOM to give more perceptual insights about the classification results. It uses the Scatterplot Matrix (SPLOM) metaphor to visualize a Linear Discriminant Analysis projection of the data for each pair of classes and a set of Receiving Operating Curves to evaluate their trustworthiness. We illustrate ClassSPLOM on a use case in Arabic dialects identification.

</p>
</details>

<details><summary><b>Practical Noise Simulation for RGB Images</b>
<a href="https://arxiv.org/abs/2201.12773">arxiv:2201.12773</a>
&#x1F4C8; 1 <br>
<p>Saeed Ranjbar Alvar, Ivan V. Bajić</p></summary>
<p>

**Abstract:** This document describes a noise generator that simulates realistic noise found in smartphone cameras. The generator simulates Poissonian-Gaussian noise whose parameters have been estimated on the Smartphone Image Denoising Dataset (SIDD). The generator is available online, and is currently being used in compressed-domain denoising exploration experiments in JPEG AI.

</p>
</details>

<details><summary><b>Do We Need to Penalize Variance of Losses for Learning with Label Noise?</b>
<a href="https://arxiv.org/abs/2201.12739">arxiv:2201.12739</a>
&#x1F4C8; 1 <br>
<p>Yexiong Lin, Yu Yao, Yuxuan Du, Jun Yu, Bo Han, Mingming Gong, Tongliang Liu</p></summary>
<p>

**Abstract:** Algorithms which minimize the averaged loss have been widely designed for dealing with noisy labels. Intuitively, when there is a finite training sample, penalizing the variance of losses will improve the stability and generalization of the algorithms. Interestingly, we found that the variance should be increased for the problem of learning with noisy labels. Specifically, increasing the variance will boost the memorization effects and reduce the harmfulness of incorrect labels. By exploiting the label noise transition matrix, regularizers can be easily designed to reduce the variance of losses and be plugged in many existing algorithms. Empirically, the proposed method by increasing the variance of losses significantly improves the generalization ability of baselines on both synthetic and real-world datasets.

</p>
</details>

<details><summary><b>A least squares support vector regression for anisotropic diffusion filtering</b>
<a href="https://arxiv.org/abs/2202.00595">arxiv:2202.00595</a>
&#x1F4C8; 0 <br>
<p>Arsham Gholamzadeh Khoee, Kimia Mohammadi Mohammadi, Mostafa Jani, Kourosh Parand</p></summary>
<p>

**Abstract:** Anisotropic diffusion filtering for signal smoothing as a low-pass filter has the advantage of the edge-preserving, i.e., it does not affect the edges that contain more critical data than the other parts of the signal. In this paper, we present a numerical algorithm based on least squares support vector regression by using Legendre orthogonal kernel with the discretization of the nonlinear diffusion problem in time by the Crank-Nicolson method. This method transforms the signal smoothing process into solving an optimization problem that can be solved by efficient numerical algorithms. In the final analysis, we have reported some numerical experiments to show the effectiveness of the proposed machine learning based approach for signal smoothing.

</p>
</details>


{% endraw %}
Prev: [2022.01.29]({{ '/2022/01/29/2022.01.29.html' | relative_url }})  Next: [2022.01.31]({{ '/2022/01/31/2022.01.31.html' | relative_url }})