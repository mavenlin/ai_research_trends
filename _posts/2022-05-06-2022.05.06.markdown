Prev: [2022.05.05]({{ '/2022/05/05/2022.05.05.html' | relative_url }})  Next: [2022.05.07]({{ '/2022/05/07/2022.05.07.html' | relative_url }})
{% raw %}
## Summary for 2022-05-06, created on 2022-05-13


<details><summary><b>Convex Analysis at Infinity: An Introduction to Astral Space</b>
<a href="https://arxiv.org/abs/2205.03260">arxiv:2205.03260</a>
&#x1F4C8; 47 <br>
<p>Miroslav Dud√≠k, Ziwei Ji, Robert E. Schapire, Matus Telgarsky</p></summary>
<p>

**Abstract:** Not all convex functions on $\mathbb{R}^n$ have finite minimizers; some can only be minimized by a sequence as it heads to infinity. In this work, we aim to develop a theory for understanding such minimizers at infinity. We study astral space, a compact extension of $\mathbb{R}^n$ to which such points at infinity have been added. Astral space is constructed to be as small as possible while still ensuring that all linear functions can be continuously extended to the new space. Although astral space includes all of $\mathbb{R}^n$, it is not a vector space, nor even a metric space. However, it is sufficiently well-structured to allow useful and meaningful extensions of concepts of convexity, conjugacy, and subdifferentials. We develop these concepts and analyze various properties of convex functions on astral space, including the detailed structure of their minimizers, exact characterizations of continuity, and convergence of descent algorithms.

</p>
</details>

<details><summary><b>CLIP-CLOP: CLIP-Guided Collage and Photomontage</b>
<a href="https://arxiv.org/abs/2205.03146">arxiv:2205.03146</a>
&#x1F4C8; 42 <br>
<p>Piotr Mirowski, Dylan Banarse, Mateusz Malinowski, Simon Osindero, Chrisantha Fernando</p></summary>
<p>

**Abstract:** The unabated mystique of large-scale neural networks, such as the CLIP dual image-and-text encoder, popularized automatically generated art. Increasingly more sophisticated generators enhanced the artworks' realism and visual appearance, and creative prompt engineering enabled stylistic expression. Guided by an artist-in-the-loop ideal, we design a gradient-based generator to produce collages. It requires the human artist to curate libraries of image patches and to describe (with prompts) the whole image composition, with the option to manually adjust the patches' positions during generation, thereby allowing humans to reclaim some control of the process and achieve greater creative freedom. We explore the aesthetic potentials of high-resolution collages, and provide an open-source Google Colab as an artistic tool.

</p>
</details>

<details><summary><b>How to Spend Your Robot Time: Bridging Kickstarting and Offline Reinforcement Learning for Vision-based Robotic Manipulation</b>
<a href="https://arxiv.org/abs/2205.03353">arxiv:2205.03353</a>
&#x1F4C8; 23 <br>
<p>Alex X. Lee, Coline Devin, Jost Tobias Springenberg, Yuxiang Zhou, Thomas Lampe, Abbas Abdolmaleki, Konstantinos Bousmalis</p></summary>
<p>

**Abstract:** Reinforcement learning (RL) has been shown to be effective at learning control from experience. However, RL typically requires a large amount of online interaction with the environment. This limits its applicability to real-world settings, such as in robotics, where such interaction is expensive. In this work we investigate ways to minimize online interactions in a target task, by reusing a suboptimal policy we might have access to, for example from training on related prior tasks, or in simulation. To this end, we develop two RL algorithms that can speed up training by using not only the action distributions of teacher policies, but also data collected by such policies on the task at hand. We conduct a thorough experimental study of how to use suboptimal teachers on a challenging robotic manipulation benchmark on vision-based stacking with diverse objects. We compare our methods to offline, online, offline-to-online, and kickstarting RL algorithms. By doing so, we find that training on data from both the teacher and student, enables the best performance for limited data budgets. We examine how to best allocate a limited data budget -- on the target task -- between the teacher and the student policy, and report experiments using varying budgets, two teachers with different degrees of suboptimality, and five stacking tasks that require a diverse set of behaviors. Our analysis, both in simulation and in the real world, shows that our approach is the best across data budgets, while standard offline RL from teacher rollouts is surprisingly effective when enough data is given.

</p>
</details>

<details><summary><b>Factory: Fast Contact for Robotic Assembly</b>
<a href="https://arxiv.org/abs/2205.03532">arxiv:2205.03532</a>
&#x1F4C8; 22 <br>
<p>Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Moravanszky, Gavriel State, Michelle Lu, Ankur Handa, Dieter Fox</p></summary>
<p>

**Abstract:** Robotic assembly is one of the oldest and most challenging applications of robotics. In other areas of robotics, such as perception and grasping, simulation has rapidly accelerated research progress, particularly when combined with modern deep learning. However, accurately, efficiently, and robustly simulating the range of contact-rich interactions in assembly remains a longstanding challenge. In this work, we present Factory, a set of physics simulation methods and robot learning tools for such applications. We achieve real-time or faster simulation of a wide range of contact-rich scenes, including simultaneous simulation of 1000 nut-and-bolt interactions. We provide $60$ carefully-designed part models, 3 robotic assembly environments, and 7 robot controllers for training and testing virtual robots. Finally, we train and evaluate proof-of-concept reinforcement learning policies for nut-and-bolt assembly. We aim for Factory to open the doors to using simulation for robotic assembly, as well as many other contact-rich applications in robotics. Please see https://sites.google.com/nvidia.com/factory for supplementary content, including videos.

</p>
</details>

<details><summary><b>Global Multi-modal 2D/3D Registration via Local Descriptors Learning</b>
<a href="https://arxiv.org/abs/2205.03439">arxiv:2205.03439</a>
&#x1F4C8; 20 <br>
<p>Viktoria Markova, Matteo Ronchetti, Wolfgang Wein, Oliver Zettinig, Raphael Prevost</p></summary>
<p>

**Abstract:** Multi-modal registration is a required step for many image-guided procedures, especially ultrasound-guided interventions that require anatomical context. While a number of such registration algorithms are already available, they all require a good initialization to succeed due to the challenging appearance of ultrasound images and the arbitrary coordinate system they are acquired in. In this paper, we present a novel approach to solve the problem of registration of an ultrasound sweep to a pre-operative image. We learn dense keypoint descriptors from which we then estimate the registration. We show that our method overcomes the challenges inherent to registration tasks with freehand ultrasound sweeps, namely, the multi-modality and multidimensionality of the data in addition to lack of precise ground truth and low amounts of training examples. We derive a registration method that is fast, generic, fully automatic, does not require any initialization and can naturally generate visualizations aiding interpretability and explainability. Our approach is evaluated on a clinical dataset of paired MR volumes and ultrasound sequences.

</p>
</details>

<details><summary><b>Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition</b>
<a href="https://arxiv.org/abs/2205.03433">arxiv:2205.03433</a>
&#x1F4C8; 14 <br>
<p>Yuan Gong, Jin Yu, James Glass</p></summary>
<p>

**Abstract:** Recognizing human non-speech vocalizations is an important task and has broad applications such as automatic sound transcription and health condition monitoring. However, existing datasets have a relatively small number of vocal sound samples or noisy labels. As a consequence, state-of-the-art audio event classification models may not perform well in detecting human vocal sounds. To support research on building robust and accurate vocal sound recognition, we have created a VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Experiments show that the vocal sound recognition performance of a model can be significantly improved by 41.9% by adding VocalSound dataset to an existing dataset as training material. In addition, different from previous datasets, the VocalSound dataset contains meta information such as speaker age, gender, native language, country, and health condition.

</p>
</details>

<details><summary><b>What Makes A Good Fisherman? Linear Regression under Self-Selection Bias</b>
<a href="https://arxiv.org/abs/2205.03246">arxiv:2205.03246</a>
&#x1F4C8; 13 <br>
<p>Yeshwanth Cherapanamjeri, Constantinos Daskalakis, Andrew Ilyas, Manolis Zampetakis</p></summary>
<p>

**Abstract:** In the classical setting of self-selection, the goal is to learn $k$ models, simultaneously from observations $(x^{(i)}, y^{(i)})$ where $y^{(i)}$ is the output of one of $k$ underlying models on input $x^{(i)}$. In contrast to mixture models, where we observe the output of a randomly selected model, here the observed model depends on the outputs themselves, and is determined by some known selection criterion. For example, we might observe the highest output, the smallest output, or the median output of the $k$ models. In known-index self-selection, the identity of the observed model output is observable; in unknown-index self-selection, it is not. Self-selection has a long history in Econometrics and applications in various theoretical and applied fields, including treatment effect estimation, imitation learning, learning from strategically reported data, and learning from markets at disequilibrium.
  In this work, we present the first computationally and statistically efficient estimation algorithms for the most standard setting of this problem where the models are linear. In the known-index case, we require poly$(1/\varepsilon, k, d)$ sample and time complexity to estimate all model parameters to accuracy $\varepsilon$ in $d$ dimensions, and can accommodate quite general selection criteria. In the more challenging unknown-index case, even the identifiability of the linear models (from infinitely many samples) was not known. We show three results in this case for the commonly studied $\max$ self-selection criterion: (1) we show that the linear models are indeed identifiable, (2) for general $k$ we provide an algorithm with poly$(d) \exp(\text{poly}(k))$ sample and time complexity to estimate the regression parameters up to error $1/\text{poly}(k)$, and (3) for $k = 2$ we provide an algorithm for any error $\varepsilon$ and poly$(d, 1/\varepsilon)$ sample and time complexity.

</p>
</details>

<details><summary><b>Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees</b>
<a href="https://arxiv.org/abs/2205.03546">arxiv:2205.03546</a>
&#x1F4C8; 10 <br>
<p>Binghui Wang, Youqi Li, Pan Zhou</p></summary>
<p>

**Abstract:** Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph-based tasks such as node classification and graph classification. However, many recent works have demonstrated that an attacker can mislead GNN models by slightly perturbing the graph structure. Existing attacks to GNNs are either under the less practical threat model where the attacker is assumed to access the GNN model parameters, or under the practical black-box threat model but consider perturbing node features that are shown to be not enough effective. In this paper, we aim to bridge this gap and consider black-box attacks to GNNs with structure perturbation as well as with theoretical guarantees. We propose to address this challenge through bandit techniques. Specifically, we formulate our attack as an online optimization with bandit feedback. This original problem is essentially NP-hard due to the fact that perturbing the graph structure is a binary optimization problem. We then propose an online attack based on bandit optimization which is proven to be {sublinear} to the query number $T$, i.e., $\mathcal{O}(\sqrt{N}T^{3/4})$ where $N$ is the number of nodes in the graph. Finally, we evaluate our proposed attack by conducting experiments over multiple datasets and GNN models. The experimental results on various citation graphs and image graphs show that our attack is both effective and efficient. Source code is available at~\url{https://github.com/Metaoblivion/Bandit_GNN_Attack}

</p>
</details>

<details><summary><b>Dynamically writing coupled memories using a reinforcement learning agent, meeting physical bounds</b>
<a href="https://arxiv.org/abs/2205.03471">arxiv:2205.03471</a>
&#x1F4C8; 10 <br>
<p>Th√©o Jules, Laura Michel, Ad√®le Douin, Fr√©d√©ric Lechenault</p></summary>
<p>

**Abstract:** Traditional memory writing operations proceed one bit at a time, where e.g. an individual magnetic domain is force-flipped by a localized external field. One way to increase material storage capacity would be to write several bits at a time in the bulk of the material. However, the manipulation of bits is commonly done through quasi-static operations. While simple to model, this method is known to reduce memory capacity. In this paper, we demonstrate how a reinforcement learning agent can exploit the dynamical response of a simple multi-bit mechanical system to restore its memory to full capacity. To do so, we introduce a model framework consisting of a chain of bi-stable springs, which is manipulated on one end by the external action of the agent. We show that the agent manages to learn how to reach all available states for three springs, even though some states are not reachable through adiabatic manipulation, and that both the training speed and convergence within physical parameter space are improved using transfer learning techniques. Interestingly, the agent also points to an optimal design of the system in terms of writing time. In fact, it appears to learn how to take advantage of the underlying physics: the control time exhibits a non-monotonic dependence on the internal dissipation, reaching a minimum at a cross-over shown to verify a mechanically motivated scaling relation.

</p>
</details>

<details><summary><b>Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising</b>
<a href="https://arxiv.org/abs/2205.03519">arxiv:2205.03519</a>
&#x1F4C8; 9 <br>
<p>Peizhou Huang, Chaoyi Zhang, Xiaoliang Zhang, Xiaojuan Li, Liang Dong, Leslie Ying</p></summary>
<p>

**Abstract:** Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable unsupervised learning for MR image reconstruction by combining an unsupervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of unsupervised learning by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality.

</p>
</details>

<details><summary><b>Forget Less, Count Better: A Domain-Incremental Self-Distillation Learning Benchmark for Lifelong Crowd Counting</b>
<a href="https://arxiv.org/abs/2205.03307">arxiv:2205.03307</a>
&#x1F4C8; 9 <br>
<p>Jiaqi Gao, Jingqi Li, Hongming Shan, Yanyun Qu, James Z. Wang, Junping Zhang</p></summary>
<p>

**Abstract:** Crowd Counting has important applications in public safety and pandemic control. A robust and practical crowd counting system has to be capable of continuously learning with the new-coming domain data in real-world scenarios instead of fitting one domain only. Off-the-shelf methods have some drawbacks to handle multiple domains. 1) The models will achieve limited performance (even drop dramatically) among old domains after training images from new domains due to the discrepancies of intrinsic data distributions from various domains, which is called catastrophic forgetting. 2) The well-trained model in a specific domain achieves imperfect performance among other unseen domains because of the domain shift. 3) It leads to linearly-increased storage overhead either mixing all the data for training or simply training dozens of separate models for different domains when new ones are available. To overcome these issues, we investigate a new task of crowd counting under the incremental domains training setting, namely, Lifelong Crowd Counting. It aims at alleviating the catastrophic forgetting and improving the generalization ability using a single model updated by the incremental domains. To be more specific, we propose a self-distillation learning framework as a benchmark~(Forget Less, Count Better, FLCB) for lifelong crowd counting, which helps the model sustainably leverage previous meaningful knowledge for better crowd counting to mitigate the forgetting when the new data arrive. Meanwhile, a new quantitative metric, normalized backward transfer~(nBwT), is developed to evaluate the forgetting degree of the model in the lifelong learning process. Extensive experimental results demonstrate the superiority of our proposed benchmark in achieving a low catastrophic forgetting degree and strong generalization ability.

</p>
</details>

<details><summary><b>Quantification of Robotic Surgeries with Vision-Based Deep Learning</b>
<a href="https://arxiv.org/abs/2205.03028">arxiv:2205.03028</a>
&#x1F4C8; 9 <br>
<p>Dani Kiyasseh, Runzhuo Ma, Taseen F. Haque, Jessica Nguyen, Christian Wagner, Animashree Anandkumar, Andrew J. Hung</p></summary>
<p>

**Abstract:** Surgery is a high-stakes domain where surgeons must navigate critical anatomical structures and actively avoid potential complications while achieving the main task at hand. Such surgical activity has been shown to affect long-term patient outcomes. To better understand this relationship, whose mechanics remain unknown for the majority of surgical procedures, we hypothesize that the core elements of surgery must first be quantified in a reliable, objective, and scalable manner. We believe this is a prerequisite for the provision of surgical feedback and modulation of surgeon performance in pursuit of improved patient outcomes. To holistically quantify surgeries, we propose a unified deep learning framework, entitled Roboformer, which operates exclusively on videos recorded during surgery to independently achieve multiple tasks: surgical phase recognition (the what of surgery), gesture classification and skills assessment (the how of surgery). We validated our framework on four video-based datasets of two commonly-encountered types of steps (dissection and suturing) within minimally-invasive robotic surgeries. We demonstrated that our framework can generalize well to unseen videos, surgeons, medical centres, and surgical procedures. We also found that our framework, which naturally lends itself to explainable findings, identified relevant information when achieving a particular task. These findings are likely to instill surgeons with more confidence in our framework's behaviour, increasing the likelihood of clinical adoption, and thus paving the way for more targeted surgical feedback.

</p>
</details>

<details><summary><b>Quantifying Synthesis and Fusion and their Impact on Machine Translation</b>
<a href="https://arxiv.org/abs/2205.03369">arxiv:2205.03369</a>
&#x1F4C8; 8 <br>
<p>Arturo Oncevay, Duygu Ataman, Niels van Berkel, Barry Haddow, Alexandra Birch, Johannes Bjerva</p></summary>
<p>

**Abstract:** Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017)'s approach to classify morphology using two indices: synthesis (e.g. analytic to polysynthetic) and fusion (agglutinative to fusional). For computing synthesis, we test unsupervised and supervised morphological segmentation methods for English, German and Turkish, whereas for fusion, we propose a semi-automatic method using Spanish as a case study. Then, we analyse the relationship between machine translation quality and the degree of synthesis and fusion at word (nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment level (previous language pairs plus English-German in both directions). We complement the word-level analysis with human evaluation, and overall, we observe a consistent impact of both indexes on machine translation quality.

</p>
</details>

<details><summary><b>Transferring Chemical and Energetic Knowledge Between Molecular Systems with Machine Learning</b>
<a href="https://arxiv.org/abs/2205.03339">arxiv:2205.03339</a>
&#x1F4C8; 8 <br>
<p>Sajjad Heydari, Stefano Raniolo, Lorenzo Livi, Vittorio Limongelli</p></summary>
<p>

**Abstract:** Predicting structural and energetic properties of a molecular system is one of the fundamental tasks in molecular simulations, and it has use cases in chemistry, biology, and medicine. In the past decade, the advent of machine learning algorithms has impacted on molecular simulations for various tasks, including property prediction of atomistic systems. In this paper, we propose a novel methodology for transferring knowledge obtained from simple molecular systems to a more complex one, possessing a significantly larger number of atoms and degrees of freedom. In particular, we focus on the classification of high and low free-energy states. Our approach relies on utilizing (i) a novel hypergraph representation of molecules, encoding all relevant information for characterizing the potential energy of a conformation, and (ii) novel message passing and pooling layers for processing and making predictions on such hypergraph-structured data. Despite the complexity of the problem, our results show a remarkable AUC of 0.92 for transfer learning from tri-alanine to the deca-alanine system. Moreover, we show that the very same transfer learning approach can be used to group, in an unsupervised way, various secondary structures of deca-alanine in clusters having similar free-energy values. Our study represents a proof of concept that reliable transfer learning models for molecular systems can be designed paving the way to unexplored routes in prediction of structural and energetic properties of biologically relevant systems.

</p>
</details>

<details><summary><b>Incremental Data-Uploading for Full-Quantum Classification</b>
<a href="https://arxiv.org/abs/2205.03057">arxiv:2205.03057</a>
&#x1F4C8; 8 <br>
<p>Maniraman Periyasamy, Nico Meyer, Christian Ufrecht, Daniel D. Scherer, Axel Plinge, Christopher Mutschler</p></summary>
<p>

**Abstract:** The data representation in a machine-learning model strongly influences its performance. This becomes even more important for quantum machine learning models implemented on noisy intermediate scale quantum (NISQ) devices. Encoding high dimensional data into a quantum circuit for a NISQ device without any loss of information is not trivial and brings a lot of challenges. While simple encoding schemes (like single qubit rotational gates to encode high dimensional data) often lead to information loss within the circuit, complex encoding schemes with entanglement and data re-uploading lead to an increase in the encoding gate count. This is not well-suited for NISQ devices. This work proposes 'incremental data-uploading', a novel encoding pattern for high dimensional data that tackles these challenges. We spread the encoding gates for the feature vector of a given data point throughout the quantum circuit with parameterized gates in between them. This encoding pattern results in a better representation of data in the quantum circuit with a minimal pre-processing requirement. We show the efficiency of our encoding pattern on a classification task using the MNIST and Fashion-MNIST datasets, and compare different encoding methods via classification accuracy and the effective dimension of the model.

</p>
</details>

<details><summary><b>Inferring electrochemical performance and parameters of Li-ion batteries based on deep operator networks</b>
<a href="https://arxiv.org/abs/2205.03508">arxiv:2205.03508</a>
&#x1F4C8; 7 <br>
<p>Qiang Zheng, Xiaoguang Yin, Dongxiao Zhang</p></summary>
<p>

**Abstract:** The Li-ion battery is a complex physicochemical system that generally takes applied current as input and terminal voltage as output. The mappings from current to voltage can be described by several kinds of models, such as accurate but inefficient physics-based models, and efficient but sometimes inaccurate equivalent circuit and black-box models. To realize accuracy and efficiency simultaneously in battery modeling, we propose to build a data-driven surrogate for a battery system while incorporating the underlying physics as constraints. In this work, we innovatively treat the functional mapping from current curve to terminal voltage as a composite of operators, which is approximated by the powerful deep operator network (DeepONet). Its learning capability is firstly verified through a predictive test for Li-ion concentration at two electrodes. In this experiment, the physics-informed DeepONet is found to be more robust than the purely data-driven DeepONet, especially in temporal extrapolation scenarios. A composite surrogate is then constructed for mapping current curve and solid diffusivity to terminal voltage with three operator networks, in which two parallel physics-informed DeepONets are firstly used to predict Li-ion concentration at two electrodes, and then based on their surface values, a DeepONet is built to give terminal voltage predictions. Since the surrogate is differentiable anywhere, it is endowed with the ability to learn from data directly, which was validated by using terminal voltage measurements to estimate input parameters. The proposed surrogate built upon operator networks possesses great potential to be applied in on-board scenarios, such as battery management system, since it integrates efficiency and accuracy by incorporating underlying physics, and also leaves an interface for model refinement through a totally differentiable model structure.

</p>
</details>

<details><summary><b>Far from Asymptopia</b>
<a href="https://arxiv.org/abs/2205.03343">arxiv:2205.03343</a>
&#x1F4C8; 7 <br>
<p>Michael C. Abbott, Benjamin B. Machta</p></summary>
<p>

**Abstract:** Inference from limited data requires a notion of measure on parameter space, which is most explicit in the Bayesian framework as a prior distribution. Jeffreys prior is the best-known uninformative choice, the invariant volume element from information geometry, but we demonstrate here that this leads to enormous bias in typical high-dimensional models. This is because models found in science typically have an effective dimensionality of accessible behaviours much smaller than the number of microscopic parameters. Any measure which treats all of these parameters equally is far from uniform when projected onto the sub-space of relevant parameters, due to variations in the local co-volume of irrelevant directions. We present results on a principled choice of measure which avoids this issue, and leads to unbiased posteriors, by focusing on relevant parameters. This optimal prior depends on the quantity of data to be gathered, and approaches Jeffreys prior in the asymptotic limit. But for typical models this limit cannot be justified without an impossibly large increase in the quantity of data, exponential in the number of microscopic parameters.

</p>
</details>

<details><summary><b>Rethinking Fairness: An Interdisciplinary Survey of Critiques of Hegemonic ML Fairness Approaches</b>
<a href="https://arxiv.org/abs/2205.04460">arxiv:2205.04460</a>
&#x1F4C8; 6 <br>
<p>Lindsay Weinberg</p></summary>
<p>

**Abstract:** This survey article assesses and compares existing critiques of current fairness-enhancing technical interventions into machine learning (ML) that draw from a range of non-computing disciplines, including philosophy, feminist studies, critical race and ethnic studies, legal studies, anthropology, and science and technology studies. It bridges epistemic divides in order to offer an interdisciplinary understanding of the possibilities and limits of hegemonic computational approaches to ML fairness for producing just outcomes for society's most marginalized. The article is organized according to nine major themes of critique wherein these different fields intersect: 1) how "fairness" in AI fairness research gets defined; 2) how problems for AI systems to address get formulated; 3) the impacts of abstraction on how AI tools function and its propensity to lead to technological solutionism; 4) how racial classification operates within AI fairness research; 5) the use of AI fairness measures to avoid regulation and engage in ethics washing; 6) an absence of participatory design and democratic deliberation in AI fairness considerations; 7) data collection practices that entrench "bias," are non-consensual, and lack transparency; 8) the predatory inclusion of marginalized groups into AI systems; and 9) a lack of engagement with AI's long-term social and ethical outcomes. Drawing from these critiques, the article concludes by imagining future ML fairness research directions that actively disrupt entrenched power dynamics and structural injustices in society.

</p>
</details>

<details><summary><b>Optimizing Terrain Mapping and Landing Site Detection for Autonomous UAVs</b>
<a href="https://arxiv.org/abs/2205.03522">arxiv:2205.03522</a>
&#x1F4C8; 6 <br>
<p>Pedro F. Proen√ßa, Jeff Delaune, Roland Brockers</p></summary>
<p>

**Abstract:** The next generation of Mars rotorcrafts requires on-board autonomous hazard avoidance landing. To this end, this work proposes a system that performs continuous multi-resolution height map reconstruction and safe landing spot detection. Structure-from-Motion measurements are aggregated in a pyramid structure using a novel Optimal Mixture of Gaussians formulation that provides a comprehensive uncertainty model. Our multiresolution pyramid is built more efficiently and accurately than past work by decoupling pyramid filling from the measurement updates of different resolutions. To detect the safest landing location, after an optimized hazard segmentation, we use a mean shift algorithm on multiple distance transform peaks to account for terrain roughness and uncertainty. The benefits of our contributions are evaluated on real and synthetic flight data.

</p>
</details>

<details><summary><b>Norm-Scaling for Out-of-Distribution Detection</b>
<a href="https://arxiv.org/abs/2205.03493">arxiv:2205.03493</a>
&#x1F4C8; 6 <br>
<p>Deepak Ravikumar, Kaushik Roy</p></summary>
<p>

**Abstract:** Out-of-Distribution (OoD) inputs are examples that do not belong to the true underlying distribution of the dataset. Research has shown that deep neural nets make confident mispredictions on OoD inputs. Therefore, it is critical to identify OoD inputs for safe and reliable deployment of deep neural nets. Often a threshold is applied on a similarity score to detect OoD inputs. One such similarity is angular similarity which is the dot product of latent representation with the mean class representation. Angular similarity encodes uncertainty, for example, if the angular similarity is less, it is less certain that the input belongs to that class. However, we observe that, different classes have different distributions of angular similarity. Therefore, applying a single threshold for all classes is not ideal since the same similarity score represents different uncertainties for different classes. In this paper, we propose norm-scaling which normalizes the logits separately for each class. This ensures that a single value consistently represents similar uncertainty for various classes. We show that norm-scaling, when used with maximum softmax probability detector, achieves 9.78% improvement in AUROC, 5.99% improvement in AUPR and 33.19% reduction in FPR95 metrics over previous state-of-the-art methods.

</p>
</details>

<details><summary><b>VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution</b>
<a href="https://arxiv.org/abs/2205.03409">arxiv:2205.03409</a>
&#x1F4C8; 6 <br>
<p>Liangbin Xie. Xintao Wang, Honglun Zhang, Chao Dong, Ying Shan</p></summary>
<p>

**Abstract:** Most of the existing video face super-resolution (VFSR) methods are trained and evaluated on VoxCeleb1, which is designed specifically for speaker identification and the frames in this dataset are of low quality. As a consequence, the VFSR models trained on this dataset can not output visual-pleasing results. In this paper, we develop an automatic and scalable pipeline to collect a high-quality video face dataset (VFHQ), which contains over $16,000$ high-fidelity clips of diverse interview scenarios. To verify the necessity of VFHQ, we further conduct experiments and demonstrate that VFSR models trained on our VFHQ dataset can generate results with sharper edges and finer textures than those trained on VoxCeleb1. In addition, we show that the temporal information plays a pivotal role in eliminating video consistency issues as well as further improving visual performance. Based on VFHQ, by analyzing the benchmarking study of several state-of-the-art algorithms under bicubic and blind settings. See our project page: https://liangbinxie.github.io/projects/vfhq

</p>
</details>

<details><summary><b>Benchmarking Econometric and Machine Learning Methodologies in Nowcasting</b>
<a href="https://arxiv.org/abs/2205.03318">arxiv:2205.03318</a>
&#x1F4C8; 6 <br>
<p>Daniel Hopp</p></summary>
<p>

**Abstract:** Nowcasting can play a key role in giving policymakers timelier insight to data published with a significant time lag, such as final GDP figures. Currently, there are a plethora of methodologies and approaches for practitioners to choose from. However, there lacks a comprehensive comparison of these disparate approaches in terms of predictive performance and characteristics. This paper addresses that deficiency by examining the performance of 12 different methodologies in nowcasting US quarterly GDP growth, including all the methods most commonly employed in nowcasting, as well as some of the most popular traditional machine learning approaches. Performance was assessed on three different tumultuous periods in US economic history: the early 1980s recession, the 2008 financial crisis, and the COVID crisis. The two best performing methodologies in the analysis were long short-term memory artificial neural networks (LSTM) and Bayesian vector autoregression (BVAR). To facilitate further application and testing of each of the examined methodologies, an open-source repository containing boilerplate code that can be applied to different datasets is published alongside the paper, available at: github.com/dhopp1/nowcasting_benchmark.

</p>
</details>

<details><summary><b>Collective Relevance Labeling for Passage Retrieval</b>
<a href="https://arxiv.org/abs/2205.03273">arxiv:2205.03273</a>
&#x1F4C8; 6 <br>
<p>Jihyuk Kim, Minsoo Kim, Seung-won Hwang</p></summary>
<p>

**Abstract:** Deep learning for Information Retrieval (IR) requires a large amount of high-quality query-document relevance labels, but such labels are inherently sparse. Label smoothing redistributes some observed probability mass over unobserved instances, often uniformly, uninformed of the true distribution. In contrast, we propose knowledge distillation for informed labeling, without incurring high computation overheads at evaluation time. Our contribution is designing a simple but efficient teacher model which utilizes collective knowledge, to outperform state-of-the-arts distilled from a more complex teacher model. Specifically, we train up to x8 faster than the state-of-the-art teacher, while distilling the rankings better. Our code is publicly available at https://github.com/jihyukkim-nlp/CollectiveKD

</p>
</details>

<details><summary><b>Scalable computation of prediction intervals for neural networks via matrix sketching</b>
<a href="https://arxiv.org/abs/2205.03194">arxiv:2205.03194</a>
&#x1F4C8; 6 <br>
<p>Alexander Fishkov, Maxim Panov</p></summary>
<p>

**Abstract:** Accounting for the uncertainty in the predictions of modern neural networks is a challenging and important task in many domains. Existing algorithms for uncertainty estimation require modifying the model architecture and training procedure (e.g., Bayesian neural networks) or dramatically increase the computational cost of predictions such as approaches based on ensembling. This work proposes a new algorithm that can be applied to a given trained neural network and produces approximate prediction intervals. The method is based on the classical delta method in statistics but achieves computational efficiency by using matrix sketching to approximate the Jacobian matrix. The resulting algorithm is competitive with state-of-the-art approaches for constructing predictive intervals on various regression datasets from the UCI repository.

</p>
</details>

<details><summary><b>LPGNet: Link Private Graph Networks for Node Classification</b>
<a href="https://arxiv.org/abs/2205.03105">arxiv:2205.03105</a>
&#x1F4C8; 6 <br>
<p>Aashish Kolluri, Teodora Baluta, Bryan Hooi, Prateek Saxena</p></summary>
<p>

**Abstract:** Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than "trivially" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.

</p>
</details>

<details><summary><b>Anomaly Detection in Intra-Vehicle Networks</b>
<a href="https://arxiv.org/abs/2205.03537">arxiv:2205.03537</a>
&#x1F4C8; 5 <br>
<p>Ajeet Kumar Dwivedi</p></summary>
<p>

**Abstract:** The progression of innovation and technology and ease of inter-connectivity among networks has allowed us to evolve towards one of the promising areas, the Internet of Vehicles. Nowadays, modern vehicles are connected to a range of networks, including intra-vehicle networks and external networks. However, a primary challenge in the automotive industry is to make the vehicle safe and reliable; particularly with the loopholes in the existing traditional protocols, cyber-attacks on the vehicle network are rising drastically. Practically every vehicle uses the universal Controller Area Network (CAN) bus protocol for the communication between electronic control units to transmit key vehicle functionality and messages related to driver safety. The CAN bus system, although its critical significance, lacks the key feature of any protocol authentication and authorization. Resulting in compromises of CAN bus security leads to serious issues to both car and driver safety. This paper discusses the security issues of the CAN bus protocol and proposes an Intrusion Detection System (IDS) that detects known attacks on in-vehicle networks. Multiple Artificial Intelligence (AI) algorithms are employed to provide recognition of known potential cyber-attacks based on messages, timestamps, and data packets traveling through the CAN. The main objective of this paper is to accurately detect cyberattacks by considering time-series features and attack frequency. The majority of the evaluated AI algorithms, when considering attack frequency, correctly identify known attacks with remarkable accuracy of more than 99%. However, these models achieve approximately 92% to 97% accuracy when timestamps are not taken into account. Long Short Term Memory (LSTM), Xgboost, and SVC have proved to the well-performing classifiers.

</p>
</details>

<details><summary><b>Investigating and Explaining the Frequency Bias in Image Classification</b>
<a href="https://arxiv.org/abs/2205.03154">arxiv:2205.03154</a>
&#x1F4C8; 5 <br>
<p>ZhiYu Lin, YiFei Gao, JiTao Sang</p></summary>
<p>

**Abstract:** CNNs exhibit many behaviors different from humans, one of which is the capability of employing high-frequency components. This paper discusses the frequency bias phenomenon in image classification tasks: the high-frequency components are actually much less exploited than the low- and mid-frequency components. We first investigate the frequency bias phenomenon by presenting two observations on feature discrimination and learning priority. Furthermore, we hypothesize that (i) the spectral density, (ii) class consistency directly affect the frequency bias. Specifically, our investigations verify that the spectral density of datasets mainly affects the learning priority, while the class consistency mainly affects the feature discrimination.

</p>
</details>

<details><summary><b>Controlled Dropout for Uncertainty Estimation</b>
<a href="https://arxiv.org/abs/2205.03109">arxiv:2205.03109</a>
&#x1F4C8; 5 <br>
<p>Mehedi Hasan, Abbas Khosravi, Ibrahim Hossain, Ashikur Rahman, Saeid Nahavandi</p></summary>
<p>

**Abstract:** Uncertainty quantification in a neural network is one of the most discussed topics for safety-critical applications. Though Neural Networks (NNs) have achieved state-of-the-art performance for many applications, they still provide unreliable point predictions, which lack information about uncertainty estimates. Among various methods to enable neural networks to estimate uncertainty, Monte Carlo (MC) dropout has gained much popularity in a short period due to its simplicity. In this study, we present a new version of the traditional dropout layer where we are able to fix the number of dropout configurations. As such, each layer can take and apply the new dropout layer in the MC method to quantify the uncertainty associated with NN predictions. We conduct experiments on both toy and realistic datasets and compare the results with the MC method using the traditional dropout layer. Performance analysis utilizing uncertainty evaluation metrics corroborates that our dropout layer offers better performance in most cases.

</p>
</details>

<details><summary><b>Sound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation</b>
<a href="https://arxiv.org/abs/2205.03043">arxiv:2205.03043</a>
&#x1F4C8; 5 <br>
<p>Zui Chen, Yansen Jing, Shengcheng Yuan, Yifei Xu, Jian Wu, Hang Zhao</p></summary>
<p>

**Abstract:** Synthesizer is a type of electronic musical instrument that is now widely used in modern music production and sound design. Each parameters configuration of a synthesizer produces a unique timbre and can be viewed as a unique instrument. The problem of estimating a set of parameters configuration that best restore a sound timbre is an important yet complicated problem, i.e.: the synthesizer parameters estimation problem. We proposed a multi-modal deep-learning-based pipeline Sound2Synth, together with a network structure Prime-Dilated Convolution (PDC) specially designed to solve this problem. Our method achieved not only SOTA but also the first real-world applicable results on Dexed synthesizer, a popular FM synthesizer.

</p>
</details>

<details><summary><b>A Fingerprint Detection Method by Fingerprint Ridge Orientation Check</b>
<a href="https://arxiv.org/abs/2205.03019">arxiv:2205.03019</a>
&#x1F4C8; 5 <br>
<p>Kim JuSong, Ri IlYong</p></summary>
<p>

**Abstract:** Fingerprints are popular among the biometric based systems due to ease of acquisition, uniqueness and availability. Nowadays it is used in smart phone security, digital payment and digital locker. Fingerprint recognition technology has been studied for a long time, and its recognition rate has recently risen to a high level. In particular, with the introduction of Deep Neural Network technologies, the recognition rate that could not be reached before was reached. In this paper, we propose a fingerprint detection algorithm used in a fingerprint recognition system.

</p>
</details>

<details><summary><b>DULA and DEBA: Differentiable Ergonomic Risk Models for Postural Assessment and Optimization in Ergonomically Intelligent pHRI</b>
<a href="https://arxiv.org/abs/2205.03491">arxiv:2205.03491</a>
&#x1F4C8; 4 <br>
<p>Amir Yazdani, Roya Sabbagh Novin, Andrew Merryweather, Tucker Hermans</p></summary>
<p>

**Abstract:** Ergonomics and human comfort are essential concerns in physical human-robot interaction applications. Defining an accurate and easy-to-use ergonomic assessment model stands as an important step in providing feedback for postural correction to improve operator health and comfort. Common practical methods in the area suffer from inaccurate ergonomics models in performing postural optimization. In order to retain assessment quality, while improving computational considerations, we propose a novel framework for postural assessment and optimization for ergonomically intelligent physical human-robot interaction. We introduce DULA and DEBA, differentiable and continuous ergonomics models learned to replicate the popular and scientifically validated RULA and REBA assessments with more than 99% accuracy. We show that DULA and DEBA provide assessment comparable to RULA and REBA while providing computational benefits when being used in postural optimization. We evaluate our framework through human and simulation experiments. We highlight DULA and DEBA's strength in a demonstration of postural optimization for a simulated pHRI task.

</p>
</details>

<details><summary><b>Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment</b>
<a href="https://arxiv.org/abs/2205.03432">arxiv:2205.03432</a>
&#x1F4C8; 4 <br>
<p>Yuan Gong, Ziyi Chen, Iek-Heng Chu, Peng Chang, James Glass</p></summary>
<p>

**Abstract:** Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech.

</p>
</details>

<details><summary><b>Alternating Good-for-MDP Automata</b>
<a href="https://arxiv.org/abs/2205.03243">arxiv:2205.03243</a>
&#x1F4C8; 4 <br>
<p>Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak</p></summary>
<p>

**Abstract:** When omega-regular objectives were first proposed in model-free reinforcement learning (RL) for controlling MDPs, deterministic Rabin automata were used in an attempt to provide a direct translation from their transitions to scalar values. While these translations failed, it has turned out that it is possible to repair them by using good-for-MDPs (GFM) B√ºchi automata instead. These are nondeterministic B√ºchi automata with a restricted type of nondeterminism, albeit not as restricted as in good-for-games automata. Indeed, deterministic Rabin automata have a pretty straightforward translation to such GFM automata, which is bi-linear in the number of states and pairs. Interestingly, the same cannot be said for deterministic Streett automata: a translation to nondeterministic Rabin or B√ºchi automata comes at an exponential cost, even without requiring the target automaton to be good-for-MDPs. Do we have to pay more than that to obtain a good-for-MDP automaton? The surprising answer is that we have to pay significantly less when we instead expand the good-for-MDP property to alternating automata: like the nondeterministic GFM automata obtained from deterministic Rabin automata, the alternating good-for-MDP automata we produce from deterministic Streett automata are bi-linear in the the size of the deterministic automaton and its index, and can therefore be exponentially more succinct than minimal nondeterministic B√ºchi automata.

</p>
</details>

<details><summary><b>A Logic-based Tractable Approximation of Probability</b>
<a href="https://arxiv.org/abs/2205.03198">arxiv:2205.03198</a>
&#x1F4C8; 4 <br>
<p>Paolo Baldi, Hykel Hosni</p></summary>
<p>

**Abstract:** We provide a logical framework in which a resource-bounded agent can be seen to perform approximations of probabilistic reasoning. Our main results read as follows. First we identify the conditions under which propositional probability functions can be approximated by a hierarchy of depth-bounded Belief functions. Second we show that under rather palatable restrictions, our approximations of probability lead to uncertain reasoning which, under the usual assumptions in the field, qualifies as tractable.

</p>
</details>

<details><summary><b>Arabic Fake News Detection Based on Deep Contextualized Embedding Models</b>
<a href="https://arxiv.org/abs/2205.03114">arxiv:2205.03114</a>
&#x1F4C8; 4 <br>
<p>Ali Bou Nassif, Ashraf Elnagar, Omar Elgendy, Yaman Afadar</p></summary>
<p>

**Abstract:** Social media is becoming a source of news for many people due to its ease and freedom of use. As a result, fake news has been spreading quickly and easily regardless of its credibility, especially in the last decade. Fake news publishers take advantage of critical situations such as the Covid-19 pandemic and the American presidential elections to affect societies negatively. Fake news can seriously impact society in many fields including politics, finance, sports, etc. Many studies have been conducted to help detect fake news in English, but research conducted on fake news detection in the Arabic language is scarce. Our contribution is twofold: first, we have constructed a large and diverse Arabic fake news dataset. Second, we have developed and evaluated transformer-based classifiers to identify fake news while utilizing eight state-of-the-art Arabic contextualized embedding models. The majority of these models had not been previously used for Arabic fake news detection. We conduct a thorough analysis of the state-of-the-art Arabic contextualized embedding models as well as comparison with similar fake news detection systems. Experimental results confirm that these state-of-the-art models are robust, with accuracy exceeding 98%.

</p>
</details>

<details><summary><b>Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances</b>
<a href="https://arxiv.org/abs/2205.03112">arxiv:2205.03112</a>
&#x1F4C8; 4 <br>
<p>Wongyu Kim, Youbin Ahn, Donghyun Kim, Kyong-Ho Lee</p></summary>
<p>

**Abstract:** Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning. Feature transitions between utterances occur naturally. However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level. To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of utterance that needs attention. Also, we introduce a response generation strategy to help focus on emotion and keywords related to appropriate features when generating responses. Experimental results show that our approach outperforms baselines and especially, achieves significant improvements on multi-turn dialogues.

</p>
</details>

<details><summary><b>Crop Type Identification for Smallholding Farms: Analyzing Spatial, Temporal and Spectral Resolutions in Satellite Imagery</b>
<a href="https://arxiv.org/abs/2205.03104">arxiv:2205.03104</a>
&#x1F4C8; 4 <br>
<p>Depanshu Sani, Sandeep Mahato, Parichya Sirohi, Saket Anand, Gaurav Arora, Charu Chandra Devshali, T. Jayaraman</p></summary>
<p>

**Abstract:** The integration of the modern Machine Learning (ML) models into remote sensing and agriculture has expanded the scope of the application of satellite images in the agriculture domain. In this paper, we present how the accuracy of crop type identification improves as we move from medium-spatiotemporal-resolution (MSTR) to high-spatiotemporal-resolution (HSTR) satellite images. We further demonstrate that high spectral resolution in satellite imagery can improve prediction performance for low spatial and temporal resolutions (LSTR) images. The F1-score is increased by 7% when using multispectral data of MSTR images as compared to the best results obtained from HSTR images. Similarly, when crop season based time series of multispectral data is used we observe an increase of 1.2% in the F1-score. The outcome motivates further advancements in the field of synthetic band generation.

</p>
</details>

<details><summary><b>KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering</b>
<a href="https://arxiv.org/abs/2205.03071">arxiv:2205.03071</a>
&#x1F4C8; 4 <br>
<p>Jianing Wang, Chengyu Wang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Jun Huang, Ming Gao</p></summary>
<p>

**Abstract:** Extractive Question Answering (EQA) is one of the most important tasks in Machine Reading Comprehension (MRC), which can be solved by fine-tuning the span selecting heads of Pre-trained Language Models (PLMs). However, most existing approaches for MRC may perform poorly in the few-shot learning scenario. To solve this issue, we propose a novel framework named Knowledge Enhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to PLMs, we introduce a seminal paradigm for EQA that transform the task into a non-autoregressive Masked Language Modeling (MLM) generation problem. Simultaneously, rich semantics from the external knowledge base (KB) and the passage context are support for enhancing the representations of the query. In addition, to boost the performance of PLMs, we jointly train the model by the MLM and contrastive learning objectives. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in few-shot settings by a large margin.

</p>
</details>

<details><summary><b>RCMNet: A deep learning model assists CAR-T therapy for leukemia</b>
<a href="https://arxiv.org/abs/2205.04230">arxiv:2205.04230</a>
&#x1F4C8; 3 <br>
<p>Ruitao Zhang, Xueying Han, Ijaz Gul, Shiyao Zhai, Ying Liu, Yongbing Zhang, Yuhan Dong, Lan Ma, Dongmei Yu, Jin Zhou, Peiwu Qin</p></summary>
<p>

**Abstract:** Acute leukemia is a type of blood cancer with a high mortality rate. Current therapeutic methods include bone marrow transplantation, supportive therapy, and chemotherapy. Although a satisfactory remission of the disease can be achieved, the risk of recurrence is still high. Therefore, novel treatments are demanding. Chimeric antigen receptor-T (CAR-T) therapy has emerged as a promising approach to treat and cure acute leukemia. To harness the therapeutic potential of CAR-T cell therapy for blood diseases, reliable cell morphological identification is crucial. Nevertheless, the identification of CAR-T cells is a big challenge posed by their phenotypic similarity with other blood cells. To address this substantial clinical challenge, herein we first construct a CAR-T dataset with 500 original microscopy images after staining. Following that, we create a novel integrated model called RCMNet (ResNet18 with CBAM and MHSA) that combines the convolutional neural network (CNN) and Transformer. The model shows 99.63% top-1 accuracy on the public dataset. Compared with previous reports, our model obtains satisfactory results for image classification. Although testing on the CAR-T cells dataset, a decent performance is observed, which is attributed to the limited size of the dataset. Transfer learning is adapted for RCMNet and a maximum of 83.36% accuracy has been achieved, which is higher than other SOTA models. The study evaluates the effectiveness of RCMNet on a big public dataset and translates it to a clinical dataset for diagnostic applications.

</p>
</details>

<details><summary><b>Deep Learning-enabled Detection and Classification of Bacterial Colonies using a Thin Film Transistor (TFT) Image Sensor</b>
<a href="https://arxiv.org/abs/2205.03549">arxiv:2205.03549</a>
&#x1F4C8; 3 <br>
<p>Yuzhu Li, Tairan Liu, Hatice Ceylan Koydemir, Hongda Wang, Keelan O'Riordan, Bijie Bai, Yuta Haga, Junji Kobashi, Hitoshi Tanaka, Takaya Tamaru, Kazunori Yamaguchi, Aydogan Ozcan</p></summary>
<p>

**Abstract:** Early detection and identification of pathogenic bacteria such as Escherichia coli (E. coli) is an essential task for public health. The conventional culture-based methods for bacterial colony detection usually take >24 hours to get the final read-out. Here, we demonstrate a bacterial colony-forming-unit (CFU) detection system exploiting a thin-film-transistor (TFT)-based image sensor array that saves ~12 hours compared to the Environmental Protection Agency (EPA)-approved methods. To demonstrate the efficacy of this CFU detection system, a lensfree imaging modality was built using the TFT image sensor with a sample field-of-view of ~10 cm^2. Time-lapse images of bacterial colonies cultured on chromogenic agar plates were automatically collected at 5-minute intervals. Two deep neural networks were used to detect and count the growing colonies and identify their species. When blindly tested with 265 colonies of E. coli and other coliform bacteria (i.e., Citrobacter and Klebsiella pneumoniae), our system reached an average CFU detection rate of 97.3% at 9 hours of incubation and an average recovery rate of 91.6% at ~12 hours. This TFT-based sensor can be applied to various microbiological detection methods. Due to the large scalability, ultra-large field-of-view, and low cost of the TFT-based image sensors, this platform can be integrated with each agar plate to be tested and disposed of after the automated CFU count. The imaging field-of-view of this platform can be cost-effectively increased to >100 cm^2 to provide a massive throughput for CFU detection using, e.g., roll-to-roll manufacturing of TFTs as used in the flexible display industry.

</p>
</details>

<details><summary><b>Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information</b>
<a href="https://arxiv.org/abs/2205.03534">arxiv:2205.03534</a>
&#x1F4C8; 3 <br>
<p>Zhipeng Zhang, Xinglin Hou, Kai Niu, Zhongzhen Huang, Tiezheng Ge, Yuning Jiang, Qi Wu, Peng Wang</p></summary>
<p>

**Abstract:** Recently, online shopping has gradually become a common way of shopping for people all over the world. Wonderful merchandise advertisements often attract more people to buy. These advertisements properly integrate multimodal multi-structured information of commodities, such as visual spatial information and fine-grained structure information. However, traditional multimodal text generation focuses on the conventional description of what existed and happened, which does not match the requirement of advertisement copywriting in the real world. Because advertisement copywriting has a vivid language style and higher requirements of faithfulness. Unfortunately, there is a lack of reusable evaluation frameworks and a scarcity of datasets. Therefore, we present a dataset, E-MMAD (e-commercial multimodal multi-structured advertisement copywriting), which requires, and supports much more detailed information in text generation. Noticeably, it is one of the largest video captioning datasets in this field. Accordingly, we propose a baseline method and faithfulness evaluation metric on the strength of structured information reasoning to solve the demand in reality on this dataset. It surpasses the previous methods by a large margin on all metrics. The dataset and method are coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.

</p>
</details>

<details><summary><b>Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction</b>
<a href="https://arxiv.org/abs/2205.03521">arxiv:2205.03521</a>
&#x1F4C8; 3 <br>
<p>Xiang Chen, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen</p></summary>
<p>

**Abstract:** Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multi-scaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance. Code is available in https://github.com/zjunlp/HVPNeT.

</p>
</details>

<details><summary><b>Fine-grained Intent Classification in the Legal Domain</b>
<a href="https://arxiv.org/abs/2205.03509">arxiv:2205.03509</a>
&#x1F4C8; 3 <br>
<p>Ankan Mullick, Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, R Raghav</p></summary>
<p>

**Abstract:** A law practitioner has to go through a lot of long legal case proceedings. To understand the motivation behind the actions of different parties/individuals in a legal case, it is essential that the parts of the document that express an intent corresponding to the case be clearly understood. In this paper, we introduce a dataset of 93 legal documents, belonging to the case categories of either Murder, Land Dispute, Robbery, or Corruption, where phrases expressing intent same as the category of the document are annotated. Also, we annotate fine-grained intents for each such phrase to enable a deeper understanding of the case for a reader. Finally, we analyze the performance of several transformer-based models in automating the process of extracting intent phrases (both at a coarse and a fine-grained level), and classifying a document into one of the possible 4 categories, and observe that, our dataset is challenging, especially in the case of fine-grained intent classification.

</p>
</details>

<details><summary><b>Clustered Graph Matching for Label Recovery and Graph Classification</b>
<a href="https://arxiv.org/abs/2205.03486">arxiv:2205.03486</a>
&#x1F4C8; 3 <br>
<p>Zhirui Li, Jesus Arroyo, Konstantinos Pantazis, Vince Lyzinski</p></summary>
<p>

**Abstract:** Given a collection of vertex-aligned networks and an additional label-shuffled network, we propose procedures for leveraging the signal in the vertex-aligned collection to recover the labels of the shuffled network. We consider matching the shuffled network to averages of the networks in the vertex-aligned collection at different levels of granularity. We demonstrate both in theory and practice that if the graphs come from different network classes, then clustering the networks into classes followed by matching the new graph to cluster-averages can yield higher fidelity matching performance than matching to the global average graph. Moreover, by minimizing the graph matching objective function with respect to each cluster average, this approach simultaneously classifies and recovers the vertex labels for the shuffled graph.

</p>
</details>

<details><summary><b>A High-Resolution Chest CT-Scan Image Dataset for COVID-19 Diagnosis and Differentiation</b>
<a href="https://arxiv.org/abs/2205.03408">arxiv:2205.03408</a>
&#x1F4C8; 3 <br>
<p>Iraj Abedi, Mahsa Vali, Bentolhoda Otroshi Shahreza, Hamidreza Bolhasani</p></summary>
<p>

**Abstract:** During the COVID-19 pandemic, computed tomography (CT) is a good way to diagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a form of computed tomography that uses advanced methods to improve image resolution. Publicly accessible COVID-19 CT image datasets are very difficult to come by due to privacy concerns, which impedes the study and development of AI-powered COVID-19 diagnostic algorithms based on CT images. To address this problem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution chest CT Scan image dataset that includes not only COVID-19 cases of Ground Glass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT images of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which includes slice-level, and patient-level labels, has the potential to aid COVID-19 research, especially for diagnosis and differentiation using artificial intelligence algorithms, machine learning and deep learning methods. This dataset is accessible through web at: http://databiox.com and includes 181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy Paving, Air Space Consolidation and Negative.
  Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging, Chest Image.

</p>
</details>

<details><summary><b>UAV-aided RF Mapping for Sensing and Connectivity in Wireless Networks</b>
<a href="https://arxiv.org/abs/2205.03335">arxiv:2205.03335</a>
&#x1F4C8; 3 <br>
<p>David Gesbert, Omid Esrafilian, Junting Chen, Rajeev Gangula, Urbashi Mitra</p></summary>
<p>

**Abstract:** The use of unmanned aerial vehicles (UAV) as flying radio access network (RAN) nodes offers a promising complement to traditional fixed terrestrial deployments. More recently yet still in the context of wireless networks, drones have also been envisioned for use as radio frequency (RF) sensing and localization devices. In both cases, the advantage of using UAVs lies in their ability to navigate themselves freely in 3D and in a timely manner to locations of space where the obtained network throughput or sensing performance is optimal. In practice, the selection of a proper location or trajectory for the UAV very much depends on local terrain features, including the position of surrounding radio obstacles. Hence, the robot must be able to map the features of its radio environment as it performs its data communication or sensing services. The challenges related to this task, referred here as radio mapping, are discussed in this paper. Its promises related to efficient trajectory design for autonomous radio-aware UAVs are highlighted, along with algorithm solutions. The advantages induced by radio-mapping in terms of connectivity, sensing, and localization performance are illustrated.

</p>
</details>

<details><summary><b>Vehicle management in a modular production context using Deep Q-Learning</b>
<a href="https://arxiv.org/abs/2205.03294">arxiv:2205.03294</a>
&#x1F4C8; 3 <br>
<p>Lucain Pouget, Timo Hasenbichler, Jakob Auer, Klaus Lichtenegger, Andreas Windisch</p></summary>
<p>

**Abstract:** We investigate the feasibility of deploying Deep-Q based deep reinforcement learning agents to job-shop scheduling problems in the context of modular production facilities, using discrete event simulations for the environment. These environments are comprised of a source and sink for the parts to be processed, as well as (several) workstations. The agents are trained to schedule automated guided vehicles to transport the parts back and forth between those stations in an optimal fashion. Starting from a very simplistic setup, we increase the complexity of the environment and compare the agents' performances with well established heuristic approaches, such as first-in-first-out based agents, cost tables and a nearest-neighbor approach. We furthermore seek particular configurations of the environments in which the heuristic approaches struggle, to investigate to what degree the Deep-Q agents are affected by these challenges. We find that Deep-Q based agents show comparable performance as the heuristic baselines. Furthermore, our findings suggest that the DRL agents exhibit an increased robustness to noise, as compared to the conventional approaches. Overall, we find that DRL agents constitute a valuable approach for this type of scheduling problems.

</p>
</details>

<details><summary><b>Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning</b>
<a href="https://arxiv.org/abs/2205.03212">arxiv:2205.03212</a>
&#x1F4C8; 3 <br>
<p>Khushdeep Singh Mann, Abhishek Tomy, Anshul Paigwar, Alessandro Renzaglia, Christian Laugier</p></summary>
<p>

**Abstract:** Reliably predicting future occupancy of highly dynamic urban environments is an important precursor for safe autonomous navigation. Common challenges in the prediction include forecasting the relative position of other vehicles, modelling the dynamics of vehicles subjected to different traffic conditions, and vanishing surrounding objects. To tackle these challenges, we propose a spatio-temporal prediction network pipeline that takes the past information from the environment and semantic labels separately for generating future occupancy predictions. Compared to the current SOTA, our approach predicts occupancy for a longer horizon of 3 seconds and in a relatively complex environment from the nuScenes dataset. Our experimental results demonstrate the ability of spatio-temporal networks to understand scene dynamics without the need for HD-Maps and explicit modeling dynamic objects. We publicly release our occupancy grid dataset based on nuScenes to support further research.

</p>
</details>

<details><summary><b>Federated Channel Learning for Intelligent Reflecting Surfaces With Fewer Pilot Signals</b>
<a href="https://arxiv.org/abs/2205.03196">arxiv:2205.03196</a>
&#x1F4C8; 3 <br>
<p>Ahmet M. Elbir, Sinem Coleri, Kumar Vijay Mishra</p></summary>
<p>

**Abstract:** Channel estimation is a critical task in intelligent reflecting surface (IRS)-assisted wireless systems due to the uncertainties imposed by environment dynamics and rapid changes in the IRS configuration. To deal with these uncertainties, deep learning (DL) approaches have been proposed. Previous works consider centralized learning (CL) approach for model training, which entails the collection of the whole training dataset from the users at the base station (BS), hence introducing huge transmission overhead for data collection. To address this challenge, this paper proposes a federated learning (FL) framework to jointly estimate both direct and cascaded channels in IRS-assisted wireless systems. We design a single convolutional neural network trained on the local datasets of the users without sending them to the BS. We show that the proposed FL-based channel estimation approach requires approximately 60% fewer pilot signals and it exhibits 12 times lower transmission overhead than CL, while maintaining satisfactory performance close to CL. In addition, it provides lower estimation error than the state-of-the-art DL-based schemes.

</p>
</details>

<details><summary><b>Geodesics, Non-linearities and the Archive of Novelty Search</b>
<a href="https://arxiv.org/abs/2205.03162">arxiv:2205.03162</a>
&#x1F4C8; 3 <br>
<p>Achkan Salehi, Alexandre Coninx, Stephane Doncieux</p></summary>
<p>

**Abstract:** The Novelty Search (NS) algorithm was proposed more than a decade ago. However, the mechanisms behind its empirical success are still not well formalized/understood. This short note focuses on the effects of the archive on exploration. Experimental evidence from a few application domains suggests that archive-based NS performs in general better than when Novelty is solely computed with respect to the population. An argument that is often encountered in the literature is that the archive prevents exploration from backtracking or cycling, i.e. from revisiting previously encountered areas in the behavior space. We argue that this is not a complete or accurate explanation as backtracking - beside often being desirable - can actually be enabled by the archive. Through low-dimensional/analytical examples, we show that a key effect of the archive is that it counterbalances the exploration biases that result, among other factors, from the use of inadequate behavior metrics and the non-linearities of the behavior mapping. Our observations seem to hint that attributing a more active role to the archive in sampling can be beneficial.

</p>
</details>

<details><summary><b>Fake News Detection with Heterogeneous Transformer</b>
<a href="https://arxiv.org/abs/2205.03100">arxiv:2205.03100</a>
&#x1F4C8; 3 <br>
<p>Tianle Li, Yushi Sun, Shang-ling Hsu, Yanjia Li, Raymond Chi-Wing Wong</p></summary>
<p>

**Abstract:** The dissemination of fake news on social networks has drawn public need for effective and efficient fake news detection methods. Generally, fake news on social networks is multi-modal and has various connections with other entities such as users and posts. The heterogeneity in both news content and the relationship with other entities in social networks brings challenges to designing a model that comprehensively captures the local multi-modal semantics of entities in social networks and the global structural representation of the propagation patterns, so as to classify fake news effectively and accurately. In this paper, we propose a novel Transformer-based model: HetTransformer to solve the fake news detection problem on social networks, which utilises the encoder-decoder structure of Transformer to capture the structural information of news propagation patterns. We first capture the local heterogeneous semantics of news, post, and user entities in social networks. Then, we apply Transformer to capture the global structural representation of the propagation patterns in social networks for fake news detection. Experiments on three real-world datasets demonstrate that our model is able to outperform the state-of-the-art baselines in fake news detection.

</p>
</details>

<details><summary><b>Mixed-UNet: Refined Class Activation Mapping for Weakly-Supervised Semantic Segmentation with Multi-scale Inference</b>
<a href="https://arxiv.org/abs/2205.04227">arxiv:2205.04227</a>
&#x1F4C8; 2 <br>
<p>Yang Liu, Ersi Zhang, Lulu Xu, Chufan Xiao, Xiaoyun Zhong, Lijin Lian, Fang Li, Bin Jiang, Yuhan Dong, Lan Ma, Qiming Huang, Ming Xu, Yongbing Zhang, Dongmei Yu, Chenggang Yan, Peiwu Qin</p></summary>
<p>

**Abstract:** Deep learning techniques have shown great potential in medical image processing, particularly through accurate and reliable image segmentation on magnetic resonance imaging (MRI) scans or computed tomography (CT) scans, which allow the localization and diagnosis of lesions. However, training these segmentation models requires a large number of manually annotated pixel-level labels, which are time-consuming and labor-intensive, in contrast to image-level labels that are easier to obtain. It is imperative to resolve this problem through weakly-supervised semantic segmentation models using image-level labels as supervision since it can significantly reduce human annotation efforts. Most of the advanced solutions exploit class activation mapping (CAM). However, the original CAMs rarely capture the precise boundaries of lesions. In this study, we propose the strategy of multi-scale inference to refine CAMs by reducing the detail loss in single-scale reasoning. For segmentation, we develop a novel model named Mixed-UNet, which has two parallel branches in the decoding phase. The results can be obtained after fusing the extracted features from two branches. We evaluate the designed Mixed-UNet against several prevalent deep learning-based segmentation approaches on our dataset collected from the local hospital and public datasets. The validation results demonstrate that our model surpasses available methods under the same supervision level in the segmentation of various lesions from brain imaging.

</p>
</details>

<details><summary><b>Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution</b>
<a href="https://arxiv.org/abs/2205.03524">arxiv:2205.03524</a>
&#x1F4C8; 2 <br>
<p>Xiaoqian Xu, Pengxu Wei, Weikai Chen, Mingzhi Mao, Liang Lin, Guanbin Li</p></summary>
<p>

**Abstract:** Due to the sophisticated imaging process, an identical scene captured by different cameras could exhibit distinct imaging patterns, introducing distinct proficiency among the super-resolution (SR) models trained on images from different devices. In this paper, we investigate a novel and practical task coded cross-device SR, which strives to adapt a real-world SR model trained on the paired images captured by one camera to low-resolution (LR) images captured by arbitrary target devices. The proposed task is highly challenging due to the absence of paired data from various imaging devices. To address this issue, we propose an unsupervised domain adaptation mechanism for real-world SR, named Dual ADversarial Adaptation (DADA), which only requires LR images in the target domain with available real paired data from a source camera. DADA employs the Domain-Invariant Attention (DIA) module to establish the basis of target model training even without HR supervision. Furthermore, the dual framework of DADA facilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for two LR input images from two domains, and an Intra-domain Adversarial Adaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA together improve the model transferability from the source domain to the target. We empirically conduct experiments under six Real to Real adaptation settings among three different cameras, and achieve superior performance compared with existing state-of-the-art approaches. We also evaluate the proposed DADA to address the adaptation to the video camera, which presents a promising research topic to promote the wide applications of real-world super-resolution. Our source code is publicly available at https://github.com/lonelyhope/DADA.git.

</p>
</details>

<details><summary><b>PARAFAC2$\times$N: Coupled Decomposition of Multi-modal Data with Drift in N Modes</b>
<a href="https://arxiv.org/abs/2205.03501">arxiv:2205.03501</a>
&#x1F4C8; 2 <br>
<p>Michael D. Sorochan Armstrong, Jesper L√∏ve Hinrich, A. Paulina de la Mata, James J. Harynuk</p></summary>
<p>

**Abstract:** Reliable analysis of comprehensive two-dimensional gas chromatography - time-of-flight mass spectrometry (GC$\times$GC-TOFMS) data is considered to be a major bottleneck for its widespread application. For multiple samples, GC$\times$GC-TOFMS data for specific chromatographic regions manifests as a 4th order tensor of I mass spectral acquisitions, J mass channels, K modulations, and L samples. Chromatographic drift is common along both the first-dimension (modulations), and along the second-dimension (mass spectral acquisitions), while drift along the mass channel and sample dimensions is for all practical purposes nonexistent. A number of solutions to handling GC$\times$GC-TOFMS data have been proposed: these involve reshaping the data to make it amenable to either 2nd order decomposition techniques based on Multivariate Curve Resolution (MCR), or 3rd order decomposition techniques such as Parallel Factor Analysis 2 (PARAFAC2). PARAFAC2 has been utilised to model chromatographic drift along one mode, which has enabled its use for robust decomposition of multiple GC-MS experiments. Although extensible, it is not straightforward to implement a PARAFAC2 model that accounts for drift along multiple modes. In this submission, we demonstrate a new approach and a general theory for modelling data with drift along multiple modes, for applications in multidimensional chromatography with multivariate detection.

</p>
</details>

<details><summary><b>Psychologically-Inspired Music Recommendation System</b>
<a href="https://arxiv.org/abs/2205.03459">arxiv:2205.03459</a>
&#x1F4C8; 2 <br>
<p>Danila Rozhevskii, Jie Zhu, Boyuan Zhao</p></summary>
<p>

**Abstract:** In the last few years, automated recommendation systems have been a major focus in the music field, where companies such as Spotify, Amazon, and Apple are competing in the ability to generate the most personalized music suggestions for their users. One of the challenges developers still fail to tackle is taking into account the psychological and emotional aspects of the music. Our goal is to find a way to integrate users' personal traits and their current emotional state into a single music recommendation system with both collaborative and content-based filtering. We seek to relate the personality and the current emotional state of the listener to the audio features in order to build an emotion-aware MRS. We compare the results both quantitatively and qualitatively to the output of the traditional MRS based on the Spotify API data to understand if our advancements make a significant impact on the quality of music recommendations.

</p>
</details>

<details><summary><b>Structure Learning in Graphical Models from Indirect Observations</b>
<a href="https://arxiv.org/abs/2205.03454">arxiv:2205.03454</a>
&#x1F4C8; 2 <br>
<p>Hang Zhang, Afshin Abdi, Faramarz Fekri</p></summary>
<p>

**Abstract:** This paper considers learning of the graphical structure of a $p$-dimensional random vector $X \in R^p$ using both parametric and non-parametric methods. Unlike the previous works which observe $x$ directly, we consider the indirect observation scenario in which samples $y$ are collected via a sensing matrix $A \in R^{d\times p}$, and corrupted with some additive noise $w$, i.e, $Y = AX + W$. For the parametric method, we assume $X$ to be Gaussian, i.e., $x\in R^p\sim N(Œº, Œ£)$ and $Œ£\in R^{p\times p}$. For the first time, we show that the correct graphical structure can be correctly recovered under the indefinite sensing system ($d < p$) using insufficient samples ($n < p$). In particular, we show that for the exact recovery, we require dimension $d = Œ©(p^{0.8})$ and sample number $n = Œ©(p^{0.8}\log^3 p)$. For the nonparametric method, we assume a nonparanormal distribution for $X$ rather than Gaussian. Under mild conditions, we show that our graph-structure estimator can obtain the correct structure. We derive the minimum sample number $n$ and dimension $d$ as $n\gtrsim (deg)^4 \log^4 n$ and $d \gtrsim p + (deg\cdot\log(d-p))^{Œ≤/4}$, respectively, where deg is the maximum Markov blanket in the graphical model and $Œ≤> 0$ is some fixed positive constant. Additionally, we obtain a non-asymptotic uniform bound on the estimation error of the CDF of $X$ from indirect observations with inexact knowledge of the noise distribution. To the best of our knowledge, this bound is derived for the first time and may serve as an independent interest. Numerical experiments on both real-world and synthetic data are provided confirm the theoretical results.

</p>
</details>

<details><summary><b>Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching</b>
<a href="https://arxiv.org/abs/2205.03447">arxiv:2205.03447</a>
&#x1F4C8; 2 <br>
<p>Yuan He, Jiaoyan Chen, Hang Dong, Ernesto Jim√©nez-Ruiz, Ali Hadian, Ian Horrocks</p></summary>
<p>

**Abstract:** Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation results for OM systems of different types to demonstrate the usage of these resources, all of which are publicly available

</p>
</details>

<details><summary><b>The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations</b>
<a href="https://arxiv.org/abs/2205.03295">arxiv:2205.03295</a>
&#x1F4C8; 2 <br>
<p>Aparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, Frank Rudzicz, Marzyeh Ghassemi</p></summary>
<p>

**Abstract:** Machine learning models in safety-critical settings like healthcare are often blackboxes: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.

</p>
</details>

<details><summary><b>Perseus: A Simple High-Order Regularization Method for Variational Inequalities</b>
<a href="https://arxiv.org/abs/2205.03202">arxiv:2205.03202</a>
&#x1F4C8; 2 <br>
<p>Tianyi Lin, Michael. I. Jordan</p></summary>
<p>

**Abstract:** This paper settles an open and challenging question pertaining to the design of simple high-order regularization methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $x^\star \in \mathcal{X}$ such that $\langle F(x), x - x^\star\rangle \geq 0$ for all $x \in \mathcal{X}$ and we consider the setting where $F: \mathbb{R}^d \mapsto \mathbb{R}^d$ is smooth with up to $(p-1)^{th}$-order derivatives. For the case of $p = 2$,~\citet{Nesterov-2006-Constrained} extended the cubic regularized Newton's method to VIs with a global rate of $O(Œµ^{-1})$. \citet{Monteiro-2012-Iteration} proposed another second-order method which achieved an improved rate of $O(Œµ^{-2/3}\log(1/Œµ))$, but this method required a nontrivial binary search procedure as an inner loop. High-order methods based on similar binary search procedures have been further developed and shown to achieve a rate of $O(Œµ^{-2/(p+1)}\log(1/Œµ))$. However, such search procedure can be computationally prohibitive in practice and the problem of finding a simple high-order regularization methods remains as an open and challenging question in optimization theory. We propose a $p^{th}$-order method which does \textit{not} require any binary search scheme and is guaranteed to converge to a weak solution with a global rate of $O(Œµ^{-2/(p+1)})$. A version with restarting attains a global linear and local superlinear convergence rate for smooth and strongly monotone VIs. Further, our method achieves a global rate of $O(Œµ^{-2/p})$ for solving smooth and non-monotone VIs satisfying the Minty condition; moreover, the restarted version again attains a global linear and local superlinear convergence rate if the strong Minty condition holds.

</p>
</details>

<details><summary><b>On boundary conditions parametrized by analytic functions</b>
<a href="https://arxiv.org/abs/2205.03185">arxiv:2205.03185</a>
&#x1F4C8; 2 <br>
<p>Markus Lange-Hegermann, Daniel Robertz</p></summary>
<p>

**Abstract:** Computer algebra can answer various questions about partial differential equations using symbolic algorithms. However, the inclusion of data into equations is rare in computer algebra. Therefore, recently, computer algebra models have been combined with Gaussian processes, a regression model in machine learning, to describe the behavior of certain differential equations under data. While it was possible to describe polynomial boundary conditions in this context, we extend these models to analytic boundary conditions. Additionally, we describe the necessary algorithms for Gr√∂bner and Janet bases of Weyl algebras with certain analytic coefficients. Using these algorithms, we provide examples of divergence-free flow in domains bounded by analytic functions and adapted to observations.

</p>
</details>

<details><summary><b>Probabilistic learning constrained by realizations using a weak formulation of Fourier transform of probability measures</b>
<a href="https://arxiv.org/abs/2205.03078">arxiv:2205.03078</a>
&#x1F4C8; 2 <br>
<p>Christian Soize</p></summary>
<p>

**Abstract:** This paper deals with the taking into account a given set of realizations as constraints in the Kullback-Leibler minimum principle, which is used as a probabilistic learning algorithm. This permits the effective integration of data into predictive models. We consider the probabilistic learning of a random vector that is made up of either a quantity of interest (unsupervised case) or the couple of the quantity of interest and a control parameter (supervised case). A training set of independent realizations of this random vector is assumed to be given and to be generated with a prior probability measure that is unknown. A target set of realizations of the QoI is available for the two considered cases. The framework is the one of non-Gaussian problems in high dimension. A functional approach is developed on the basis of a weak formulation of the Fourier transform of probability measures (characteristic functions). The construction makes it possible to take into account the target set of realizations of the QoI in the Kullback-Leibler minimum principle. The proposed approach allows for estimating the posterior probability measure of the QoI (unsupervised case) or of the posterior joint probability measure of the QoI with the control parameter (supervised case). The existence and the uniqueness of the posterior probability measure is analyzed for the two cases. The numerical aspects are detailed in order to facilitate the implementation of the proposed method. The presented application in high dimension demonstrates the efficiency and the robustness of the proposed algorithm.

</p>
</details>

<details><summary><b>Low-rank Tensor Learning with Nonconvex Overlapped Nuclear Norm Regularization</b>
<a href="https://arxiv.org/abs/2205.03059">arxiv:2205.03059</a>
&#x1F4C8; 2 <br>
<p>Quanming Yao, Yaqing Wang, Bo Han, James Kwok</p></summary>
<p>

**Abstract:** Nonconvex regularization has been popularly used in low-rank matrix learning. However, extending it for low-rank tensor learning is still computationally expensive. To address this problem, we develop an efficient solver for use with a nonconvex extension of the overlapped nuclear norm regularizer. Based on the proximal average algorithm, the proposed algorithm can avoid expensive tensor folding/unfolding operations. A special "sparse plus low-rank" structure is maintained throughout the iterations, and allows fast computation of the individual proximal steps. Empirical convergence is further improved with the use of adaptive momentum. We provide convergence guarantees to critical points on smooth losses and also on objectives satisfying the Kurdyka-≈Åojasiewicz condition. While the optimization problem is nonconvex and nonsmooth, we show that its critical points still have good statistical performance on the tensor completion problem. Experiments on various synthetic and real-world data sets show that the proposed algorithm is efficient in both time and space and more accurate than the existing state-of-the-art.

</p>
</details>

<details><summary><b>Concepts and Algorithms for Agent-based Decentralized and Integrated Scheduling of Production and Auxiliary Processes</b>
<a href="https://arxiv.org/abs/2205.04461">arxiv:2205.04461</a>
&#x1F4C8; 1 <br>
<p>Felix Gehlhoff, Alexander Fay</p></summary>
<p>

**Abstract:** The trend to individualized products and shorter product life cycles has driven many companies to rethink their focus on traditional mass production. New technologies and solution concepts like Industry 4.0 foster the advent of decentralization of production control and distribution of information. A promising technology for realizing such scenarios are Multi-agent systems (MAS). This contribution analyses the requirements for an agent-based decentralized and integrated scheduling approach. Part of the requirements is to develop a linearly scaling communication architecture, as the communication between the agents is a major driver of the scheduling execution time. The approach schedules production and auxiliary operations (transportation, buffering and required shared resource operations such as tools) in an integrated manner so that the interdependencies between them are reflected in the solution. Part of the logistics requirements are concerning constraints for large workpiece handling such as buffer scarcity. However, the approach aims at providing a more general solution that is also applicable to large system sizes that, for example, can be found in production networks where multiple companies engage in a joint production effort. In addition, it is applicable for different kinds of factory organization (flow shop, job shop and open shop). The approach is explained using an example based on industrial requirements. Different experiments have been conducted to evaluate the scheduling execution time. The results of the experiments show the approach's linear scaling behavior. Also, an analysis of the concurrent negotiation ability is provided.

</p>
</details>

<details><summary><b>Journaling Data for Daily PHQ-2 Depression Prediction and Forecasting</b>
<a href="https://arxiv.org/abs/2205.03391">arxiv:2205.03391</a>
&#x1F4C8; 1 <br>
<p>Alexander Kathan, Andreas Triantafyllopoulos, Xiangheng He, Manuel Milling, Tianhao Yan, Srividya Tirunellai Rajamani, Ludwig K√ºster, Mathias Harrer, Elena Heber, Inga Grossmann, David D. Ebert, Bj√∂rn W. Schuller</p></summary>
<p>

**Abstract:** Digital health applications are becoming increasingly important for assessing and monitoring the wellbeing of people suffering from mental health conditions like depression. A common target of said applications is to predict the results of self-assessed Patient-Health-Questionnaires (PHQ), indicating current symptom severity of depressive individuals. In this work, we explore the potential of using actively-collected data to predict and forecast daily PHQ-2 scores on a newly-collected longitudinal dataset. We obtain a best MAE of 1.417 for daily prediction of PHQ-2 scores, which specifically in the used dataset have a range of 0 to 12, using leave-one-subject-out cross-validation, as well as a best MAE of 1.914 for forecasting PHQ-2 scores using data from up to the last 7 days. This illustrates the additive value that can be obtained by incorporating actively-collected data in a depression monitoring application.

</p>
</details>

<details><summary><b>Efficient Minimax Optimal Estimators For Multivariate Convex Regression</b>
<a href="https://arxiv.org/abs/2205.03368">arxiv:2205.03368</a>
&#x1F4C8; 1 <br>
<p>Gil Kur, Eli Putterman</p></summary>
<p>

**Abstract:** We study the computational aspects of the task of multivariate convex regression in dimension $d \geq 5$. We present the first computationally efficient minimax optimal (up to logarithmic factors) estimators for the tasks of (i) $L$-Lipschitz convex regression (ii) $Œì$-bounded convex regression under polytopal support. The proof of the correctness of these estimators uses a variety of tools from different disciplines, among them empirical process theory, stochastic geometry, and potential theory. This work is the first to show the existence of efficient minimax optimal estimators for non-Donsker classes that their corresponding Least Squares Estimators are provably minimax sub-optimal; a result of independent interest.

</p>
</details>

<details><summary><b>UAV-aided Wireless Node Localization Using Hybrid Radio Channel Models</b>
<a href="https://arxiv.org/abs/2205.03327">arxiv:2205.03327</a>
&#x1F4C8; 1 <br>
<p>Omid Esrafilian, Rajeev Gangula, David Gesbert</p></summary>
<p>

**Abstract:** This paper considers the problem of ground user localization based on received signal strength (RSS) measurements obtained by an unmanned aerial vehicle (UAV). We treat UAV-user link channel model parameters and antenna radiation pattern of the UAV as unknowns that need to be estimated. A hybrid channel model is proposed that consists of a traditional path loss model combined with a neural network approximating the UAV antenna gain function. With this model and a set of offline RSS measurements, the unknown parameters are estimated. We then employ the particle swarm optimization (PSO) technique which utilizes the learned hybrid channel model along with a 3D map of the environment to accurately localize the ground users. The performance of the developed algorithm is evaluated through simulations and also real-world experiments.

</p>
</details>

<details><summary><b>HumanAL: Calibrating Human Matching Beyond a Single Task</b>
<a href="https://arxiv.org/abs/2205.03209">arxiv:2205.03209</a>
&#x1F4C8; 1 <br>
<p>Roee Shraga</p></summary>
<p>

**Abstract:** This work offers a novel view on the use of human input as labels, acknowledging that humans may err. We build a behavioral profile for human annotators which is used as a feature representation of the provided input. We show that by utilizing black-box machine learning, we can take into account human behavior and calibrate their input to improve the labeling quality. To support our claims and provide a proof-of-concept, we experiment with three different matching tasks, namely, schema matching, entity matching and text matching. Our empirical evaluation suggests that the method can improve the quality of gathered labels in multiple settings including cross-domain (across different matching tasks).

</p>
</details>

<details><summary><b>Imperceptible Backdoor Attack: From Input Space to Feature Representation</b>
<a href="https://arxiv.org/abs/2205.03190">arxiv:2205.03190</a>
&#x1F4C8; 1 <br>
<p>Nan Zhong, Zhenxing Qian, Xinpeng Zhang</p></summary>
<p>

**Abstract:** Backdoor attacks are rapidly emerging threats to deep neural networks (DNNs). In the backdoor attack scenario, attackers usually implant the backdoor into the target model by manipulating the training dataset or training process. Then, the compromised model behaves normally for benign input yet makes mistakes when the pre-defined trigger appears. In this paper, we analyze the drawbacks of existing attack approaches and propose a novel imperceptible backdoor attack. We treat the trigger pattern as a special kind of noise following a multinomial distribution. A U-net-based network is employed to generate concrete parameters of multinomial distribution for each benign input. This elaborated trigger ensures that our approach is invisible to both humans and statistical detection. Besides the design of the trigger, we also consider the robustness of our approach against model diagnose-based defences. We force the feature representation of malicious input stamped with the trigger to be entangled with the benign one. We demonstrate the effectiveness and robustness against multiple state-of-the-art defences through extensive datasets and networks. Our trigger only modifies less than 1\% pixels of a benign image while the modification magnitude is 1. Our source code is available at https://github.com/Ekko-zn/IJCAI2022-Backdoor.

</p>
</details>

<details><summary><b>Fast Rate Generalization Error Bounds: Variations on a Theme</b>
<a href="https://arxiv.org/abs/2205.03131">arxiv:2205.03131</a>
&#x1F4C8; 1 <br>
<p>Xuetong Wu, Jonathan H. Manton, Uwe Aickelin, Jingge Zhu</p></summary>
<p>

**Abstract:** A recent line of works, initiated by \cite{russo2016controlling} and \cite{xu2017information}, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of O(\sqrt{Œª/{n}}) where Œªis some information-theoretic quantities such as the mutual information between the data sample and the learned hypothesis. However, such a learning rate is typically considered to be "slow", compared to a "fast rate" of O(1/n) in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate (O(1/n)) result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the key conditions needed for the fast rate generalization error, which we call the (Œ∑,c)-central condition. Under this condition, we give information-theoretic bounds on the generalization error and excess risk, with a convergence rate of O(Œª/{n}) for specific learning algorithms such as empirical risk minimization. Finally, analytical examples are given to show the effectiveness of the bounds.

</p>
</details>

<details><summary><b>PTFlash: A deep learning framework for isothermal two-phase equilibrium calculations</b>
<a href="https://arxiv.org/abs/2205.03090">arxiv:2205.03090</a>
&#x1F4C8; 1 <br>
<p>Jingang Qu, Thibault Faney, Jean-Charles de Hemptinne, Soleiman Yousef, Patrick Gallinari</p></summary>
<p>

**Abstract:** Phase equilibrium calculations are an essential part of numerical simulations of multi-component multi-phase flow in porous media, accounting for the largest share of the computational time. In this work, we introduce a GPUenabled, fast, and parallel framework, PTFlash, that vectorizes algorithms required for isothermal two-phase flash calculations using PyTorch, and can facilitate a wide range of downstream applications. In addition, to further accelerate PTFlash, we design two task-specific neural networks, one for predicting the stability of given mixtures and the other for providing estimates of the distribution coefficients, which are trained offline and help shorten computation time by sidestepping stability analysis and reducing the number of iterations to reach convergence. The evaluation of PTFlash was conducted on three case studies involving hydrocarbons, CO$_2$ and N$_2$ , for which the phase equilibrium was tested over a large range of temperature, pressure and composition conditions, using the Soave-Redlich-Kwong (SRK) equation of state. We compare PTFlash with an in-house thermodynamic library, Carnot, written in C++ and performing flash calculations one by one on CPU. Results show speed-ups on large scale calculations up to two order of magnitudes, while maintaining perfect precision with the reference solution provided by Carnot.

</p>
</details>

<details><summary><b>Differentially Private Generalized Linear Models Revisited</b>
<a href="https://arxiv.org/abs/2205.03014">arxiv:2205.03014</a>
&#x1F4C8; 1 <br>
<p>Raman Arora, Raef Bassily, Crist√≥bal Guzm√°n, Michael Menart, Enayat Ullah</p></summary>
<p>

**Abstract:** We study the problem of $(Œµ,Œ¥)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(nŒµ)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{nŒµ}\right\}\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\Vert w^\ast\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\tildeŒ©\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(nŒµ)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{nŒµ}\right\}}\right)$. We also revisit the previously studied case of Lipschitz losses [SSTT20]. For this case, we close the gap in the existing work and show that the optimal rate is (up to log factors) $Œò\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^*\Vert}{\sqrt{nŒµ}},\frac{\sqrt{\text{rank}}\Vert w^*\Vert}{nŒµ}\right\}\right)$, where $\text{rank}$ is the rank of the design matrix. This improves over existing work in the high privacy regime. Finally, our algorithms involve a private model selection approach that we develop to enable attaining the stated rates without a-priori knowledge of $\Vert w^*\Vert$.

</p>
</details>

<details><summary><b>Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning</b>
<a href="https://arxiv.org/abs/2205.03398">arxiv:2205.03398</a>
&#x1F4C8; 0 <br>
<p>Ulrike Kuhl, Andr√© Artelt, Barbara Hammer</p></summary>
<p>

**Abstract:** To foster usefulness and accountability of machine learning (ML), it is essential to explain a model's decisions in addition to evaluating its performance. Accordingly, the field of explainable artificial intelligence (XAI) has resurfaced as a topic of active research, offering approaches to address the "how" and "why" of automated decision-making. Within this domain, counterfactual explanations (CFEs) have gained considerable traction as a psychologically grounded approach to generate post-hoc explanations. To do so, CFEs highlight what changes to a model's input would have changed its prediction in a particular way. However, despite the introduction of numerous CFE approaches, their usability has yet to be thoroughly validated at the human level. Thus, to advance the field of XAI, we introduce the Alien Zoo, an engaging, web-based and game-inspired experimental framework. The Alien Zoo provides the means to evaluate usability of CFEs for gaining new knowledge from an automated system, targeting novice users in a domain-general context. As a proof of concept, we demonstrate the practical efficacy and feasibility of this approach in a user study. Our results suggest that users benefit from receiving CFEs compared to no explanation, both in terms of objective performance in the proposed iterative learning task, and subjective usability. With this work, we aim to equip research groups and practitioners with the means to easily run controlled and well-powered user studies to complement their otherwise often more technology-oriented work. Thus, in the interest of reproducible research, we provide the entire code, together with the underlying models and user data.

</p>
</details>

<details><summary><b>Designing Robust Biotechnological Processes Regarding Variabilities using Multi-Objective Optimization Applied to a Biopharmaceutical Seed Train Design</b>
<a href="https://arxiv.org/abs/2205.03261">arxiv:2205.03261</a>
&#x1F4C8; 0 <br>
<p>Tanja Hern√°ndez Rodr√≠guez, Anton Sekulic, Markus Lange-Hegermann, Bj√∂rn Frahm</p></summary>
<p>

**Abstract:** Development and optimization of biopharmaceutical production processes with cell cultures is cost- and time-consuming and often performed rather empirically. Efficient optimization of multiple-objectives like process time, viable cell density, number of operating steps & cultivation scales, required medium, amount of product as well as product quality depicts a promising approach. This contribution presents a workflow which couples uncertainty-based upstream simulation and Bayes optimization using Gaussian processes. Its application is demonstrated in a simulation case study for a relevant industrial task in process development, the design of a robust cell culture expansion process (seed train), meaning that despite uncertainties and variabilities concerning cell growth, low variations of viable cell density during the seed train are obtained. Compared to a non-optimized reference seed train, the optimized process showed much lower deviation rates regarding viable cell densities (<~10% instead of 41.7%) using 5 or 4 shake flask scales and seed train duration could be reduced by 56 h from 576 h to 520 h. Overall, it is shown that applying Bayes optimization allows for optimization of a multi-objective optimization function with several optimizable input variables and under a considerable amount of constraints with a low computational effort. This approach provides the potential to be used in form of a decision tool, e.g. for the choice of an optimal and robust seed train design or for further optimization tasks within process development.

</p>
</details>

<details><summary><b>Towards QD-suite: developing a set of benchmarks for Quality-Diversity algorithms</b>
<a href="https://arxiv.org/abs/2205.03207">arxiv:2205.03207</a>
&#x1F4C8; 0 <br>
<p>Achkan Salehi, Stephane Doncieux</p></summary>
<p>

**Abstract:** While the field of Quality-Diversity (QD) has grown into a distinct branch of stochastic optimization, a few problems, in particular locomotion and navigation tasks, have become de facto standards. Are such benchmarks sufficient? Are they representative of the key challenges faced by QD algorithms? Do they provide the ability to focus on one particular challenge by properly disentangling it from others? Do they have much predictive power in terms of scalability and generalization? Existing benchmarks are not standardized, and there is currently no MNIST equivalent for QD. Inspired by recent works on Reinforcement Learning benchmarks, we argue that the identification of challenges faced by QD methods and the development of targeted, challenging, scalable but affordable benchmarks is an important step. As an initial effort, we identify three problems that are challenging in sparse reward settings, and propose associated benchmarks: (1) Behavior metric bias, which can result from the use of metrics that do not match the structure of the behavior space. (2) Behavioral Plateaus, with varying characteristics, such that escaping them would require adaptive QD algorithms and (3) Evolvability Traps, where small variations in genotype result in large behavioral changes. The environments that we propose satisfy the properties listed above.

</p>
</details>


{% endraw %}
Prev: [2022.05.05]({{ '/2022/05/05/2022.05.05.html' | relative_url }})  Next: [2022.05.07]({{ '/2022/05/07/2022.05.07.html' | relative_url }})