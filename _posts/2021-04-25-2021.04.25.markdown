## Summary for 2021-04-25, created on 2021-12-22


<details><summary><b>Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos</b>
<a href="https://arxiv.org/abs/2104.12276">arxiv:2104.12276</a>
&#x1F4C8; 16 <br>
<p>Yuming Du, Yang Xiao, Vincent Lepetit</p></summary>
<p>

**Abstract:** The ability to localize and segment objects from unseen classes would open the door to new applications, such as autonomous object learning in active vision. Nonetheless, improving the performance on unseen classes requires additional training data, while manually annotating the objects of the unseen classes can be labor-extensive and expensive. In this paper, we explore the use of unlabeled video sequences to automatically generate training data for objects of unseen classes. It is in principle possible to apply existing video segmentation methods to unlabeled videos and automatically obtain object masks, which can then be used as a training set even for classes with no manual labels available. However, our experiments show that these methods do not perform well enough for this purpose. We therefore introduce a Bayesian method that is specifically designed to automatically create such a training set: Our method starts from a set of object proposals and relies on (non-realistic) analysis-by-synthesis to select the correct ones by performing an efficient optimization over all the frames simultaneously. Through extensive experiments, we show that our method can generate a high-quality training set which significantly boosts the performance of segmenting objects of unseen classes. We thus believe that our method could open the door for open-world instance segmentation using abundant Internet videos.

</p>
</details>

<details><summary><b>dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching</b>
<a href="https://arxiv.org/abs/2104.12297">arxiv:2104.12297</a>
&#x1F4C8; 9 <br>
<p>Zhengyu Huang, Yichen Peng, Tomohiro Hibino, Chunqi Zhao, Haoran Xie, Tsukasa Fukusato, Kazunori Miyata</p></summary>
<p>

**Abstract:** In this paper, we propose dualFace, a portrait drawing interface to assist users with different levels of drawing skills to complete recognizable and authentic face sketches. dualFace consists of two-stage drawing assistance to provide global and local visual guidance: global guidance, which helps users draw contour lines of portraits (i.e., geometric structure), and local guidance, which helps users draws details of facial parts (which conform to user-drawn contour lines), inspired by traditional artist workflows in portrait drawing. In the stage of global guidance, the user draws several contour lines, and dualFace then searches several relevant images from an internal database and displays the suggested face contour lines over the background of the canvas. In the stage of local guidance, we synthesize detailed portrait images with a deep generative model from user-drawn contour lines, but use the synthesized results as detailed drawing guidance. We conducted a user study to verify the effectiveness of dualFace, and we confirmed that dualFace significantly helps achieve a detailed portrait sketch. see http://www.jaist.ac.jp/~xie/dualface.html

</p>
</details>

<details><summary><b>DC3: A learning method for optimization with hard constraints</b>
<a href="https://arxiv.org/abs/2104.12225">arxiv:2104.12225</a>
&#x1F4C8; 6 <br>
<p>Priya L. Donti, David Rolnick, J. Zico Kolter</p></summary>
<p>

**Abstract:** Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap "approximate solvers." Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.

</p>
</details>

<details><summary><b>Learning Latent Graph Dynamics for Deformable Object Manipulation</b>
<a href="https://arxiv.org/abs/2104.12149">arxiv:2104.12149</a>
&#x1F4C8; 6 <br>
<p>Xiao Ma, David Hsu, Wee Sun Lee</p></summary>
<p>

**Abstract:** Manipulating deformable objects, such as cloth and ropes, is a long-standing challenge in robotics: their large number of degrees of freedom (DoFs) and complex non-linear dynamics make motion planning extremely difficult. This work aims to learn latent Graph dynamics for DefOrmable Object Manipulation (G-DOOM). To tackle the challenge of many DoFs and complex dynamics, G-DOOM approximates a deformable object as a sparse set of interacting keypoints and learns a graph neural network that captures abstractly the geometry and interaction dynamics of the keypoints. Further, to tackle the perceptual challenge, specifically, object self-occlusion, G-DOOM adds a recurrent neural network to track the keypoints over time and condition their interactions on the history. We then train the resulting recurrent graph dynamics model through contrastive learning in a high-fidelity simulator. For manipulation planning, G-DOOM explicitly reasons about the learned dynamics model through model-predictive control applied at each of the keypoints. We evaluate G-DOOM on a set of challenging cloth and rope manipulation tasks and show that G-DOOM outperforms a state-of-the-art method. Further, although trained entirely on simulation data, G-DOOM transfers directly to a real robot for both cloth and rope manipulation in our experiments.

</p>
</details>

<details><summary><b>Model-based metrics: Sample-efficient estimates of predictive model subpopulation performance</b>
<a href="https://arxiv.org/abs/2104.12231">arxiv:2104.12231</a>
&#x1F4C8; 5 <br>
<p>Andrew C. Miller, Leon A. Gatys, Joseph Futoma, Emily B. Fox</p></summary>
<p>

**Abstract:** Machine learning models $-$ now commonly developed to screen, diagnose, or predict health conditions $-$ are evaluated with a variety of performance metrics. An important first step in assessing the practical utility of a model is to evaluate its average performance over an entire population of interest. In many settings, it is also critical that the model makes good predictions within predefined subpopulations. For instance, showing that a model is fair or equitable requires evaluating the model's performance in different demographic subgroups. However, subpopulation performance metrics are typically computed using only data from that subgroup, resulting in higher variance estimates for smaller groups. We devise a procedure to measure subpopulation performance that can be more sample-efficient than the typical subsample estimates. We propose using an evaluation model $-$ a model that describes the conditional distribution of the predictive model score $-$ to form model-based metric (MBM) estimates. Our procedure incorporates model checking and validation, and we propose a computationally efficient approximation of the traditional nonparametric bootstrap to form confidence intervals. We evaluate MBMs on two main tasks: a semi-synthetic setting where ground truth metrics are available and a real-world hospital readmission prediction task. We find that MBMs consistently produce more accurate and lower variance estimates of model performance for small subpopulations.

</p>
</details>

<details><summary><b>Transformers to Fight the COVID-19 Infodemic</b>
<a href="https://arxiv.org/abs/2104.12201">arxiv:2104.12201</a>
&#x1F4C8; 5 <br>
<p>Lasitha Uyangodage, Tharindu Ranasinghe, Hansi Hettiarachchi</p></summary>
<p>

**Abstract:** The massive spread of false information on social media has become a global risk especially in a global pandemic situation like COVID-19. False information detection has thus become a surging research topic in recent months. NLP4IF-2021 shared task on fighting the COVID-19 infodemic has been organised to strengthen the research in false information detection where the participants are asked to predict seven different binary labels regarding false information in a tweet. The shared task has been organised in three languages; Arabic, Bulgarian and English. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves a 0.707 mean F1 score in Arabic, 0.578 mean F1 score in Bulgarian and 0.864 mean F1 score in English ranking 4th place in all the languages.

</p>
</details>

<details><summary><b>An Adaptive Learning based Generative Adversarial Network for One-To-One Voice Conversion</b>
<a href="https://arxiv.org/abs/2104.12159">arxiv:2104.12159</a>
&#x1F4C8; 5 <br>
<p>Sandipan Dhar, Nanda Dulal Jana, Swagatam Das</p></summary>
<p>

**Abstract:** Voice Conversion (VC) emerged as a significant domain of research in the field of speech synthesis in recent years due to its emerging application in voice-assisting technology, automated movie dubbing, and speech-to-singing conversion to name a few. VC basically deals with the conversion of vocal style of one speaker to another speaker while keeping the linguistic contents unchanged. VC task is performed through a three-stage pipeline consisting of speech analysis, speech feature mapping, and speech reconstruction. Nowadays the Generative Adversarial Network (GAN) models are widely in use for speech feature mapping from source to target speaker. In this paper, we propose an adaptive learning-based GAN model called ALGAN-VC for an efficient one-to-one VC of speakers. Our ALGAN-VC framework consists of some approaches to improve the speech quality and voice similarity between source and target speakers. The model incorporates a Dense Residual Network (DRN) like architecture to the generator network for efficient speech feature learning, for source to target speech feature conversion. We also integrate an adaptive learning mechanism to compute the loss function for the proposed model. Moreover, we use a boosted learning rate approach to enhance the learning capability of the proposed model. The model is trained by using both forward and inverse mapping simultaneously for a one-to-one VC. The proposed model is tested on Voice Conversion Challenge (VCC) 2016, 2018, and 2020 datasets as well as on our self-prepared speech dataset, which has been recorded in Indian regional languages and in English. A subjective and objective evaluation of the generated speech samples indicated that the proposed model elegantly performed the voice conversion task by achieving high speaker similarity and adequate speech quality.

</p>
</details>

<details><summary><b>SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</b>
<a href="https://arxiv.org/abs/2105.09284">arxiv:2105.09284</a>
&#x1F4C8; 4 <br>
<p>Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino</p></summary>
<p>

**Abstract:** We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image. It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.

</p>
</details>

<details><summary><b>Explore BiLSTM-CRF-Based Models for Open Relation Extraction</b>
<a href="https://arxiv.org/abs/2104.12333">arxiv:2104.12333</a>
&#x1F4C8; 4 <br>
<p>Tao Ni, Qing Wang, Gabriela Ferraro</p></summary>
<p>

**Abstract:** Extracting multiple relations from text sentences is still a challenge for current Open Relation Extraction (Open RE) tasks. In this paper, we develop several Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural network and different contextualized word embedding methods. We also propose a new tagging scheme to solve overlapping problems and enhance models' performance. From the evaluation results and comparisons between models, we select the best combination of tagging scheme, word embedder, and BiLSTM-CRF network to achieve an Open RE model with a remarkable extracting ability on multiple-relation sentences.

</p>
</details>

<details><summary><b>Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps</b>
<a href="https://arxiv.org/abs/2104.12294">arxiv:2104.12294</a>
&#x1F4C8; 4 <br>
<p>Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza Mohammadi</p></summary>
<p>

**Abstract:** One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increases the number of models' weights. ResNet and next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution and also not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification Challenge, MIT Indoors Scenes, and a part of the ImageNet dataset. We investigated the implementation of our architecture on several models of Inception, ResNet and DensNet families. Applying our architecture has revealed a significant effect on increasing convergence speed and accuracy. Our Experiments on images with 224x224 resolution increased the Top-1 accuracy between 2% to 8% on different datasets and models. Running our models on 512x512 resolution images of the MIT Indoors Scenes dataset showed a notable result of improving the Top-1 accuracy within 3% to 26%. We will also demonstrate the GAP layer's disadvantage when the input images are large and the number of classes is not few. In this circumstance, our proposed architecture can do a great help in enhancing classification results. The code is shared at https://github.com/mr7495/image-classification-spatial.

</p>
</details>

<details><summary><b>StegaPos: Preventing Crops and Splices with Imperceptible Positional Encodings</b>
<a href="https://arxiv.org/abs/2104.12290">arxiv:2104.12290</a>
&#x1F4C8; 4 <br>
<p>Gokhan Egri, Todd Zickler</p></summary>
<p>

**Abstract:** We present a model for differentiating between images that are authentic copies of ones published by photographers, and images that have been manipulated by cropping, splicing or downsampling after publication. The model comprises an encoder that resides with the photographer and a matching decoder that is available to observers. The encoder learns to embed imperceptible positional signatures into image values prior to publication. The decoder learns to use these steganographic positional ("stegapos") signatures to determine, for each small image patch, the 2D positional coordinates that were held by the patch in its originally-published image. Crop, splice and downsample edits become detectable by the inconsistencies they cause in the hidden positional signatures. We find that training the encoder and decoder together produces a model that imperceptibly encodes position, and that enables superior performance on established benchmarks for splice detection and high accuracy on a new benchmark for crop detection.

</p>
</details>

<details><summary><b>Breiman's two cultures: You don't have to choose sides</b>
<a href="https://arxiv.org/abs/2104.12219">arxiv:2104.12219</a>
&#x1F4C8; 4 <br>
<p>Andrew C. Miller, Nicholas J. Foti, Emily B. Fox</p></summary>
<p>

**Abstract:** Breiman's classic paper casts data analysis as a choice between two cultures: data modelers and algorithmic modelers. Stated broadly, data modelers use simple, interpretable models with well-understood theoretical properties to analyze data. Algorithmic modelers prioritize predictive accuracy and use more flexible function approximations to analyze data. This dichotomy overlooks a third set of models $-$ mechanistic models derived from scientific theories (e.g., ODE/SDE simulators). Mechanistic models encode application-specific scientific knowledge about the data. And while these categories represent extreme points in model space, modern computational and algorithmic tools enable us to interpolate between these points, producing flexible, interpretable, and scientifically-informed hybrids that can enjoy accurate and robust predictions, and resolve issues with data analysis that Breiman describes, such as the Rashomon effect and Occam's dilemma. Challenges still remain in finding an appropriate point in model space, with many choices on how to compose model components and the degree to which each component informs inferences.

</p>
</details>

<details><summary><b>Generative Adversarial Network: Some Analytical Perspectives</b>
<a href="https://arxiv.org/abs/2104.12210">arxiv:2104.12210</a>
&#x1F4C8; 4 <br>
<p>Haoyang Cao, Xin Guo</p></summary>
<p>

**Abstract:** Ever since its debut, generative adversarial networks (GANs) have attracted tremendous amount of attention. Over the past years, different variations of GANs models have been developed and tailored to different applications in practice. Meanwhile, some issues regarding the performance and training of GANs have been noticed and investigated from various theoretical perspectives. This subchapter will start from an introduction of GANs from an analytical perspective, then move on to the training of GANs via SDE approximations and finally discuss some applications of GANs in computing high dimensional MFGs as well as tackling mathematical finance problems.

</p>
</details>

<details><summary><b>Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes</b>
<a href="https://arxiv.org/abs/2104.14435">arxiv:2104.14435</a>
&#x1F4C8; 3 <br>
<p>Changshun Wu, Yliès Falcone, Saddek Bensalem</p></summary>
<p>

**Abstract:** Classification neural networks fail to detect inputs that do not fall inside the classes they have been trained for. Runtime monitoring techniques on the neuron activation pattern can be used to detect such inputs. We present an approach for monitoring classification systems via data abstraction. Data abstraction relies on the notion of box with a resolution. Box-based abstraction consists in representing a set of values by its minimal and maximal values in each dimension. We augment boxes with a notion of resolution and define their clustering coverage, which is intuitively a quantitative metric that indicates the abstraction quality. This allows studying the effect of different clustering parameters on the constructed boxes and estimating an interval of sub-optimal parameters. Moreover, we automatically construct monitors that leverage both the correct and incorrect behaviors of a system. This allows checking the size of the monitor abstractions and analyzing the separability of the network. Monitors are obtained by combining the sub-monitors of each class of the system placed at some selected layers. Our experiments demonstrate the effectiveness of our clustering coverage estimation and show how to assess the effectiveness and precision of monitors according to the selected clustering parameter and monitored layers.

</p>
</details>

<details><summary><b>ODDObjects: A Framework for Multiclass Unsupervised Anomaly Detection on Masked Objects</b>
<a href="https://arxiv.org/abs/2104.12300">arxiv:2104.12300</a>
&#x1F4C8; 3 <br>
<p>Ricky Ma</p></summary>
<p>

**Abstract:** This paper presents a novel framework for unsupervised anomaly detection on masked objects called ODDObjects, which stands for Out-of-Distribution Detection on Objects. ODDObjects is designed to detect anomalies of various categories using unsupervised autoencoders trained on COCO-style datasets. The method utilizes autoencoder-based image reconstruction, where high reconstruction error indicates the possibility of an anomaly. The framework extends previous work on anomaly detection with autoencoders, comparing state-of-the-art models trained on object recognition datasets. Various model architectures were compared, and experimental results show that memory-augmented deep convolutional autoencoders perform the best at detecting out-of-distribution objects.

</p>
</details>

<details><summary><b>Demystification of Few-shot and One-shot Learning</b>
<a href="https://arxiv.org/abs/2104.12174">arxiv:2104.12174</a>
&#x1F4C8; 3 <br>
<p>Ivan Y. Tyukin, Alexander N. Gorban, Muhammad H. Alkhudaydi, Qinghua Zhou</p></summary>
<p>

**Abstract:** Few-shot and one-shot learning have been the subject of active and intensive research in recent years, with mounting evidence pointing to successful implementation and exploitation of few-shot learning algorithms in practice. Classical statistical learning theories do not fully explain why few- or one-shot learning is at all possible since traditional generalisation bounds normally require large training and testing samples to be meaningful. This sharply contrasts with numerous examples of successful one- and few-shot learning systems and applications.
  In this work we present mathematical foundations for a theory of one-shot and few-shot learning and reveal conditions specifying when such learning schemes are likely to succeed. Our theory is based on intrinsic properties of high-dimensional spaces. We show that if the ambient or latent decision space of a learning machine is sufficiently high-dimensional than a large class of objects in this space can indeed be easily learned from few examples provided that certain data non-concentration conditions are met.

</p>
</details>

<details><summary><b>Scalable End-to-End RF Classification: A Case Study on Undersized Dataset Regularization by Convolutional-MST</b>
<a href="https://arxiv.org/abs/2104.12103">arxiv:2104.12103</a>
&#x1F4C8; 3 <br>
<p>Khalid Youssef, Greg Schuette, Yubin Cai, Daisong Zhang, Yikun Huang, Yahya Rahmat-Samii, Louis-S. Bouchard</p></summary>
<p>

**Abstract:** Unlike areas such as computer vision and speech recognition where convolutional and recurrent neural networks-based approaches have proven effective to the nature of the respective areas of application, deep learning (DL) still lacks a general approach suitable for the unique nature and challenges of RF systems such as radar, signals intelligence, electronic warfare, and communications. Existing approaches face problems in robustness, consistency, efficiency, repeatability and scalability. One of the main challenges in RF sensing such as radar target identification is the difficulty and cost of obtaining data. Hundreds to thousands of samples per class are typically used when training for classifying signals into 2 to 12 classes with reported accuracy ranging from 87% to 99%, where accuracy generally decreases with more classes added. In this paper, we present a new DL approach based on multistage training and demonstrate it on RF sensing signal classification. We consistently achieve over 99% accuracy for up to 17 diverse classes using only 11 samples per class for training, yielding up to 35% improvement in accuracy over standard DL approaches.

</p>
</details>

<details><summary><b>Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining</b>
<a href="https://arxiv.org/abs/2104.12100">arxiv:2104.12100</a>
&#x1F4C8; 3 <br>
<p>Xiang Chen, Yufeng Huang, Lei Xu</p></summary>
<p>

**Abstract:** Rain streaks bring serious blurring and visual quality degradation, which often vary in size, direction and density. Current CNN-based methods achieve encouraging performance, while are limited to depict rain characteristics and recover image details in the poor visibility environment. To address these issues, we present a Multi-scale Hourglass Hierarchical Fusion Network (MH2F-Net) in end-to-end manner, to exactly captures rain streak features with multi-scale extraction, hierarchical distillation and information aggregation. For better extracting the features, a novel Multi-scale Hourglass Extraction Block (MHEB) is proposed to get local and global features across different scales through down- and up-sample process. Besides, a Hierarchical Attentive Distillation Block (HADB) then employs the dual attention feature responses to adaptively recalibrate the hierarchical features and eliminate the redundant ones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy to progressively discriminate feature learning and aggregate different features instead of directly concatenating or adding. Extensive experiments on both synthetic and real rainy datasets demonstrate the effectiveness of the designed MH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our source code will be available on the GitHub: https://github.com/cxtalk/MH2F-Net.

</p>
</details>

<details><summary><b>Regression on Deep Visual Features using Artificial Neural Networks (ANNs) to Predict Hydraulic Blockage at Culverts</b>
<a href="https://arxiv.org/abs/2105.03233">arxiv:2105.03233</a>
&#x1F4C8; 2 <br>
<p>Umair Iqbal, Johan Barthelemy, Wanqing Li, Pascal Perez</p></summary>
<p>

**Abstract:** Cross drainage hydraulic structures (i.e., culverts, bridges) in urban landscapes are prone to getting blocked by transported debris which often results in causing the flash floods. In context of Australia, Wollongong City Council (WCC) blockage conduit policy is the only formal guideline to consider blockage in design process. However, many argue that this policy is based on the post floods visual inspections and hence can not be considered accurate representation of hydraulic blockage. As a result of this on-going debate, visual blockage and hydraulic blockage are considered two distinct terms with no established quantifiable relation among both. This paper attempts to relate both terms by proposing the use of deep visual features for prediction of hydraulic blockage at a given culvert. An end-to-end machine learning pipeline is propounded which takes an image of culvert as input, extract visual features using deep learning models, pre-process the visual features and feed into regression model to predict the corresponding hydraulic blockage. Dataset (i.e., Hydrology-Lab Dataset (HD), Visual Hydrology-Lab Dataset (VHD)) used in this research was collected from in-lab experiments carried out using scaled physical models of culverts where multiple blockage scenarios were replicated at scale. Performance of regression models was assessed using standard evaluation metrics. Furthermore, performance of overall machine learning pipeline was assessed in terms of processing times for relative comparison of models and hardware requirement analysis. From the results ANN used with MobileNet extracted visual features achieved the best regression performance with $R^{2}$ score of 0.7855. Positive value of $R^{2}$ score indicated the presence of correlation between visual features and hydraulic blockage and suggested that both can be interrelated with each other.

</p>
</details>

<details><summary><b>Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study</b>
<a href="https://arxiv.org/abs/2104.14506">arxiv:2104.14506</a>
&#x1F4C8; 2 <br>
<p>Qinghao Ye, Jun Xia, Guang Yang</p></summary>
<p>

**Abstract:** Artificial Intelligence (AI) has made leapfrogs in development across all the industrial sectors especially when deep learning has been introduced. Deep learning helps to learn the behaviour of an entity through methods of recognising and interpreting patterns. Despite its limitless potential, the mystery is how deep learning algorithms make a decision in the first place. Explainable AI (XAI) is the key to unlocking AI and the black-box for deep learning. XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand. The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model. Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19. AI can support rapid evaluation of CT scans to differentiate COVID-19 findings from other lung diseases. However, how these AI tools or deep learning algorithms reach such a decision and which are the most influential features derived from these neural networks with typically deep layers are not clear. The aim of this study is to propose and develop XAI strategies for COVID-19 classification models with an investigation of comparison. The results demonstrate promising quantification and qualitative visualisations that can further enhance the clinician's understanding and decision making with more granular information from the results given by the learned XAI models.

</p>
</details>

<details><summary><b>Algorithms for ridge estimation with convergence guarantees</b>
<a href="https://arxiv.org/abs/2104.12314">arxiv:2104.12314</a>
&#x1F4C8; 2 <br>
<p>Wanli Qiao, Wolfgang Polonik</p></summary>
<p>

**Abstract:** The extraction of filamentary structure from a point cloud is discussed. The filaments are modeled as ridge lines or higher dimensional ridges of an underlying density. We propose two novel algorithms, and provide theoretical guarantees for their convergences. We consider the new algorithms as alternatives to the Subspace Constraint Mean Shift (SCMS) algorithm that do not suffer from a shortcoming of the SCMS that is also revealed in this paper.

</p>
</details>

<details><summary><b>Stochastic Recurrent Neural Network for Multistep Time Series Forecasting</b>
<a href="https://arxiv.org/abs/2104.12311">arxiv:2104.12311</a>
&#x1F4C8; 2 <br>
<p>Zexuan Yin, Paolo Barucca</p></summary>
<p>

**Abstract:** Time series forecasting based on deep architectures has been gaining popularity in recent years due to their ability to model complex non-linear temporal dynamics. The recurrent neural network is one such model capable of handling variable-length input and output. In this paper, we leverage recent advances in deep generative models and the concept of state space models to propose a stochastic adaptation of the recurrent neural network for multistep-ahead time series forecasting, which is trained with stochastic gradient variational Bayes. In our model design, the transition function of the recurrent neural network, which determines the evolution of the hidden states, is stochastic rather than deterministic as in a regular recurrent neural network; this is achieved by incorporating a latent random variable into the transition process which captures the stochasticity of the temporal dynamics. Our model preserves the architectural workings of a recurrent neural network for which all relevant information is encapsulated in its hidden states, and this flexibility allows our model to be easily integrated into any deep architecture for sequential modelling. We test our model on a wide range of datasets from finance to healthcare; results show that the stochastic recurrent neural network consistently outperforms its deterministic counterpart.

</p>
</details>

<details><summary><b>A Bi-Encoder LSTM Model For Learning Unstructured Dialogs</b>
<a href="https://arxiv.org/abs/2104.12269">arxiv:2104.12269</a>
&#x1F4C8; 2 <br>
<p>Diwanshu Shekhar, Pooran S. Negi, Mohammad Mahoor</p></summary>
<p>

**Abstract:** Creating a data-driven model that is trained on a large dataset of unstructured dialogs is a crucial step in developing Retrieval-based Chatbot systems. This paper presents a Long Short Term Memory (LSTM) based architecture that learns unstructured multi-turn dialogs and provides results on the task of selecting the best response from a collection of given responses. Ubuntu Dialog Corpus Version 2 was used as the corpus for training. We show that our model achieves 0.8%, 1.0% and 0.3% higher accuracy for Recall@1, Recall@2 and Recall@5 respectively than the benchmark model. We also show results on experiments performed by using several similarity functions, model hyper-parameters and word embeddings on the proposed architecture

</p>
</details>

<details><summary><b>The 5th AI City Challenge</b>
<a href="https://arxiv.org/abs/2104.12233">arxiv:2104.12233</a>
&#x1F4C8; 2 <br>
<p>Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Xiaodong Yang, Yue Yao, Liang Zheng, Pranamesh Chakraborty, Christian E. Lopez, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff</p></summary>
<p>

**Abstract:** The AI City Challenge was created with two goals in mind: (1) pushing the boundaries of research and development in intelligent video analysis for smarter cities use cases, and (2) assessing tasks where the level of performance is enough to cause real-world adoption. Transportation is a segment ripe for such adoption. The fifth AI City Challenge attracted 305 participating teams across 38 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in five challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation being conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. Track 5 was a new track addressing vehicle retrieval using natural language descriptions. The evaluation system shows a general leader board of all submitted results, and a public leader board of results limited to the contest participation rules, where teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data is limited. Results show the promise of AI in Smarter Transportation. State-of-the-art performance for some tasks shows that these technologies are ready for adoption in real-world systems.

</p>
</details>

<details><summary><b>Sampling Permutations for Shapley Value Estimation</b>
<a href="https://arxiv.org/abs/2104.12199">arxiv:2104.12199</a>
&#x1F4C8; 2 <br>
<p>Rory Mitchell, Joshua Cooper, Eibe Frank, Geoffrey Holmes</p></summary>
<p>

**Abstract:** Game-theoretic attribution techniques based on Shapley values are used extensively to interpret black-box machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi Monte Carlo methods are not well defined on the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel to obtain explicit convergence rates of $O(1/n)$, improving on $O(1/\sqrt{n})$ for plain Monte Carlo. The RKHS perspective also leads to quasi Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere $\mathbb{S}^{d-2}$ and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations.

</p>
</details>

<details><summary><b>Seeking Quality Diversity in Evolutionary Co-design of Morphology and Control of Soft Tensegrity Modular Robots</b>
<a href="https://arxiv.org/abs/2104.12175">arxiv:2104.12175</a>
&#x1F4C8; 2 <br>
<p>Enrico Zardini, Davide Zappetti, Davide Zambrano, Giovanni Iacca, Dario Floreano</p></summary>
<p>

**Abstract:** Designing optimal soft modular robots is difficult, due to non-trivial interactions between morphology and controller. Evolutionary algorithms (EAs), combined with physical simulators, represent a valid tool to overcome this issue. In this work, we investigate algorithmic solutions to improve the Quality Diversity of co-evolved designs of Tensegrity Soft Modular Robots (TSMRs) for two robotic tasks, namely goal reaching and squeezing trough a narrow passage. To this aim, we use three different EAs, i.e., MAP-Elites and two custom algorithms: one based on Viability Evolution (ViE) and NEAT (ViE-NEAT), the other named Double Map MAP-Elites (DM-ME) and devised to seek diversity while co-evolving robot morphologies and neural network (NN)-based controllers. In detail, DM-ME extends MAP-Elites in that it uses two distinct feature maps, referring to morphologies and controllers respectively, and integrates a mechanism to automatically define the NN-related feature descriptor. Considering the fitness, in the goal-reaching task ViE-NEAT outperforms MAP-Elites and results equivalent to DM-ME. Instead, when considering diversity in terms of "illumination" of the feature space, DM-ME outperforms the other two algorithms on both tasks, providing a richer pool of possible robotic designs, whereas ViE-NEAT shows comparable performance to MAP-Elites on goal reaching, although it does not exploit any map.

</p>
</details>

<details><summary><b>Learning to Address Intra-segment Misclassification in Retinal Imaging</b>
<a href="https://arxiv.org/abs/2104.12138">arxiv:2104.12138</a>
&#x1F4C8; 2 <br>
<p>Yukun Zhou, Moucheng Xu, Yipeng Hu, Hongxiang Lin, Joseph Jacob, Pearse A. Keane, Daniel C. Alexander</p></summary>
<p>

**Abstract:** Accurate multi-class segmentation is a long-standing challenge in medical imaging, especially in scenarios where classes share strong similarity. Segmenting retinal blood vessels in retinal photographs is one such scenario, in which arteries and veins need to be identified and differentiated from each other and from the background. Intra-segment misclassification, i.e. veins classified as arteries or vice versa, frequently occurs when arteries and veins intersect, whereas in binary retinal vessel segmentation, error rates are much lower. We thus propose a new approach that decomposes multi-class segmentation into multiple binary, followed by a binary-to-multi-class fusion network. The network merges representations of artery, vein, and multi-class feature maps, each of which are supervised by expert vessel annotation in adversarial training. A skip-connection based merging process explicitly maintains class-specific gradients to avoid gradient vanishing in deep layers, to favor the discriminative features. The results show that, our model respectively improves F1-score by 4.4\%, 5.1\%, and 4.2\% compared with three state-of-the-art deep learning based methods on DRIVE-AV, LES-AV, and HRF-AV data sets. Code: https://github.com/rmaphoh/Learning-AVSegmentation

</p>
</details>

<details><summary><b>FedSup: A Communication-Efficient Federated Learning Fatigue Driving Behaviors Supervision Framework</b>
<a href="https://arxiv.org/abs/2104.12086">arxiv:2104.12086</a>
&#x1F4C8; 2 <br>
<p>Chen Zhao, Zhipeng Gao, Qian Wang, Kaile Xiao, Zijia Mo, M. Jamal Deen</p></summary>
<p>

**Abstract:** With the proliferation of edge smart devices and the Internet of Vehicles (IoV) technologies, intelligent fatigue detection has become one of the most-used methods in our daily driving. To improve the performance of the detection model, a series of techniques have been developed. However, existing work still leaves much to be desired, such as privacy disclosure and communication cost. To address these issues, we propose FedSup, a client-edge-cloud framework for privacy and efficient fatigue detection. Inspired by the federated learning technique, FedSup intelligently utilizes the collaboration between client, edge, and cloud server to realizing dynamic model optimization while protecting edge data privacy. Moreover, to reduce the unnecessary system communication overhead, we further propose a Bayesian convolutional neural network (BCNN) approximation strategy on the clients and an uncertainty weighted aggregation algorithm on the cloud to enhance the central model training efficiency. Extensive experiments demonstrate that the FedSup framework is suitable for IoV scenarios and outperforms other mainstream methods.

</p>
</details>

<details><summary><b>How Well Self-Supervised Pre-Training Performs with Streaming Data?</b>
<a href="https://arxiv.org/abs/2104.12081">arxiv:2104.12081</a>
&#x1F4C8; 2 <br>
<p>Dapeng Hu, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Alfred Shen, Jiashi Feng</p></summary>
<p>

**Abstract:** The common self-supervised pre-training practice requires collecting massive unlabeled data together and then trains a representation model, dubbed \textbf{joint training}. However, in real-world scenarios where data are collected in a streaming fashion, the joint training scheme is usually storage-heavy and time-consuming. A more efficient alternative is to train a model continually with streaming data, dubbed \textbf{sequential training}. Nevertheless, it is unclear how well sequential self-supervised pre-training performs with streaming data. In this paper, we conduct thorough experiments to investigate self-supervised pre-training with streaming data. Specifically, we evaluate the transfer performance of sequential self-supervised pre-training with four different data sequences on three different downstream tasks and make comparisons with joint self-supervised pre-training. Surprisingly, we find sequential self-supervised learning exhibits almost the same performance as the joint training when the distribution shifts within streaming data are mild. Even for data sequences with large distribution shifts, sequential self-supervised training with simple techniques, e.g., parameter regularization or data replay, still performs comparably to joint training. Based on our findings, we recommend using sequential self-supervised training as a \textbf{more efficient yet performance-competitive} representation learning practice for real-world applications.

</p>
</details>

<details><summary><b>Parallel Scale-wise Attention Network for Effective Scene Text Recognition</b>
<a href="https://arxiv.org/abs/2104.12076">arxiv:2104.12076</a>
&#x1F4C8; 2 <br>
<p>Usman Sajid, Michael Chow, Jin Zhang, Taejoon Kim, Guanghui Wang</p></summary>
<p>

**Abstract:** The paper proposes a new text recognition network for scene-text images. Many state-of-the-art methods employ the attention mechanism either in the text encoder or decoder for the text alignment. Although the encoder-based attention yields promising results, these schemes inherit noticeable limitations. They perform the feature extraction (FE) and visual attention (VA) sequentially, which bounds the attention mechanism to rely only on the FE final single-scale output. Moreover, the utilization of the attention process is limited by only applying it directly to the single scale feature-maps. To address these issues, we propose a new multi-scale and encoder-based attention network for text recognition that performs the multi-scale FE and VA in parallel. The multi-scale channels also undergo regular fusion with each other to develop the coordinated knowledge together. Quantitative evaluation and robustness analysis on the standard benchmarks demonstrate that the proposed network outperforms the state-of-the-art in most cases.

</p>
</details>

<details><summary><b>Random Embeddings and Linear Regression can Predict Protein Function</b>
<a href="https://arxiv.org/abs/2104.14661">arxiv:2104.14661</a>
&#x1F4C8; 1 <br>
<p>Tianyu Lu, Alex X. Lu, Alan M. Moses</p></summary>
<p>

**Abstract:** Large self-supervised models pretrained on millions of protein sequences have recently gained popularity in generating embeddings of protein sequences for protein function prediction. However, the absence of random baselines makes it difficult to conclude whether pretraining has learned useful information for protein function prediction. Here we show that one-hot encoding and random embeddings, both of which do not require any pretraining, are strong baselines for protein function prediction across 14 diverse sequence-to-function tasks.

</p>
</details>

<details><summary><b>Spatially Coherent Clustering Based on Orthogonal Nonnegative Matrix Factorization</b>
<a href="https://arxiv.org/abs/2104.12289">arxiv:2104.12289</a>
&#x1F4C8; 1 <br>
<p>Pascal Fernsel</p></summary>
<p>

**Abstract:** Classical approaches in cluster analysis are typically based on a feature space analysis. However, many applications lead to datasets with additional spatial information and a ground truth with spatially coherent classes, which will not necessarily be reconstructed well by standard clustering methods. Motivated by applications in hyperspectral imaging, we introduce in this work clustering models based on orthogonal nonnegative matrix factorization, which include an additional total variation (TV) regularization procedure on the cluster membership matrix to enforce the needed spatial coherence in the clusters. We propose several approaches with different optimization techniques, where the TV regularization is either performed as a subsequent postprocessing step or included into the clustering algorithm. Finally, we provide a numerical evaluation of all proposed methods on a hyperspectral dataset obtained from a matrix-assisted laser desorption/ionisation imaging measurement, which leads to significantly better clustering results compared to classical clustering models.

</p>
</details>

<details><summary><b>Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data</b>
<a href="https://arxiv.org/abs/2104.12284">arxiv:2104.12284</a>
&#x1F4C8; 1 <br>
<p>Lucas Cassiel Jacaruso</p></summary>
<p>

**Abstract:** Deep learning methods have shown suitability for time series classification in the health and medical domain, with promising results for electrocardiogram data classification. Successful identification of myocardial infarction holds life saving potential and any meaningful improvement upon deep learning models in this area is of great interest. Conventionally, data augmentation methods are applied universally to the training set when data are limited in order to ameliorate data resolution or sample size. In the method proposed in this study, data augmentation was not applied in the context of data scarcity. Instead, samples that yield low confidence predictions were selectively augmented in order to bolster the model's sensitivity to features or patterns less strongly associated with a given class. This approach was tested for improving the performance of a Fully Convolutional Network. The proposed approach achieved 90 percent accuracy for classifying myocardial infarction as opposed to 82 percent accuracy for the baseline, a marked improvement. Further, the accuracy of the proposed approach was optimal near a defined upper threshold for qualifying low confidence samples and decreased as this threshold was raised to include higher confidence samples. This suggests exclusively selecting lower confidence samples for data augmentation comes with distinct benefits for electrocardiogram data classification with Fully Convolutional Networks.

</p>
</details>

<details><summary><b>Potential Idiomatic Expression (PIE)-English: Corpus for Classes of Idioms</b>
<a href="https://arxiv.org/abs/2105.03280">arxiv:2105.03280</a>
&#x1F4C8; 0 <br>
<p>Tosin P. Adewumi, Saleha Javed, Roshanak Vadoodi, Aparajita Tripathy, Konstantina Nikolaidou, Foteini Liwicki, Marcus Liwicki</p></summary>
<p>

**Abstract:** We present a fairly large, Potential Idiomatic Expression (PIE) dataset for Natural Language Processing (NLP) in English. The challenges with NLP systems with regards to tasks such as Machine Translation (MT), word sense disambiguation (WSD) and information retrieval make it imperative to have a labelled idioms dataset with classes such as it is in this work. To the best of the authors' knowledge, this is the first idioms corpus with classes of idioms beyond the literal and the general idioms classification. In particular, the following classes are labelled in the dataset: metaphor, simile, euphemism, parallelism, personification, oxymoron, paradox, hyperbole, irony and literal. Many past efforts have been limited in the corpus size and classes of samples but this dataset contains over 20,100 samples with almost 1,200 cases of idioms (with their meanings) from 10 classes (or senses). The corpus may also be extended by researchers to meet specific needs. The corpus has part of speech (PoS) tagging from the NLTK library. Classification experiments performed on the corpus to obtain a baseline and comparison among three common models, including the BERT model, give good results. We also make publicly available the corpus and the relevant codes for working with it for NLP tasks.

</p>
</details>

<details><summary><b>Performance and Energy-Aware Bi-objective Tasks Scheduling for Cloud Data Centers</b>
<a href="https://arxiv.org/abs/2105.00843">arxiv:2105.00843</a>
&#x1F4C8; 0 <br>
<p>Huned Materwala, Leila Ismail</p></summary>
<p>

**Abstract:** Cloud computing enables remote execution of users tasks. The pervasive adoption of cloud computing in smart cities services and applications requires timely execution of tasks adhering to Quality of Services (QoS).
  However, the increasing use of computing servers exacerbates the issues of high energy consumption, operating costs, and environmental pollution. Maximizing the performance and minimizing the energy in a cloud data center is challenging. In this paper, we propose a performance and energy optimization bi-objective algorithm to tradeoff the contradicting performance and energy objectives. An evolutionary algorithm-based multi-objective optimization is for the first time proposed using system performance counters. The performance of the proposed model is evaluated using a realistic cloud dataset in a cloud computing environment. Our experimental results achieve higher performance and lower energy consumption compared to a state of the art algorithm.

</p>
</details>

<details><summary><b>Revisiting the dynamics of Bose-Einstein condensates in a double well by deep learning with a hybrid network</b>
<a href="https://arxiv.org/abs/2104.14657">arxiv:2104.14657</a>
&#x1F4C8; 0 <br>
<p>Shurui Li, Jianqin Xu, Jing Qian, Weiping Zhang</p></summary>
<p>

**Abstract:** Deep learning, accounting for the use of an elaborate neural network, has recently been developed as an efficient and powerful tool to solve diverse problems in physics and other sciences. In the present work, we propose a novel learning method based on a hybrid network integrating two different kinds of neural networks: Long Short-Term Memory(LSTM) and Deep Residual Network(ResNet), in order to overcome the difficulty met in numerically simulating strongly-oscillating dynamical evolutions of physical systems. By taking the dynamics of Bose-Einstein condensates in a double-well potential as an example, we show that our new method makes a high efficient pre-learning and a high-fidelity prediction about the whole dynamics. This benefits from the advantage of the combination of the LSTM and the ResNet and is impossibly achieved by a single network in the case of direct learning. Our method can be applied for simulating complex cooperative dynamics in a system with fast multiple-frequency oscillations with the aid of auxiliary spectrum analysis.

</p>
</details>

<details><summary><b>A STDP-based Encoding Algorithm for Associative and Composite Data</b>
<a href="https://arxiv.org/abs/2104.12249">arxiv:2104.12249</a>
&#x1F4C8; 0 <br>
<p>Hong-Gyu Yoon, Pilwon Kim</p></summary>
<p>

**Abstract:** Spike-timing-dependent plasticity(STDP) is a biological process of synaptic modification caused by the difference of firing order and timing between neurons. One of the neurodynamical roles of STDP is to form a macroscopic geometrical structure in the neuronal state space in response to a periodic input. This work proposes a practical memory model based on STDP that can store and retrieve high-dimensional associative data. The model combines STDP dynamics with an encoding scheme for distributed representations and can handle multiple composite data in a continuous manner. In the auto-associative memory task where a group of images is continuously streamed to the model, the images are successfully retrieved from an oscillating neural state whenever a proper cue is given. In the second task that deals with semantic memories embedded from sentences, the results show that words can recall multiple sentences simultaneously or one exclusively, depending on their grammatical relations.

</p>
</details>


[Next Page]({{ '/2021/04/24/2021.04.24.html' | relative_url }})
