Prev: [2022.03.04]({{ '/2022/03/04/2022.03.04.html' | relative_url }})  Next: [2022.03.06]({{ '/2022/03/06/2022.03.06.html' | relative_url }})
{% raw %}
## Summary for 2022-03-05, created on 2022-03-15


<details><summary><b>Towards Efficient and Scalable Sharpness-Aware Minimization</b>
<a href="https://arxiv.org/abs/2203.02714">arxiv:2203.02714</a>
&#x1F4C8; 91 <br>
<p>Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, Yang You</p></summary>
<p>

**Abstract:** Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of the loss landscape and generalization, has demonstrated significant performance boosts on training large-scale models such as vision transformers. However, the update rule of SAM requires two sequential (non-parallelizable) gradient computations at each step, which can double the computational overhead. In this paper, we propose a novel algorithm LookSAM - that only periodically calculates the inner gradient ascent, to significantly reduce the additional training cost of SAM. The empirical results illustrate that LookSAM achieves similar accuracy gains to SAM while being tremendously faster - it enjoys comparable computational complexity with first-order optimizers such as SGD or Adam. To further evaluate the performance and scalability of LookSAM, we incorporate a layer-wise modification and perform experiments in the large-batch training scenario, which is more prone to converge to sharp local minima. We are the first to successfully scale up the batch size when training Vision Transformers (ViTs). With a 64k batch size, we are able to train ViTs from scratch in minutes while maintaining competitive performance.

</p>
</details>

<details><summary><b>Leveraging Pre-trained BERT for Audio Captioning</b>
<a href="https://arxiv.org/abs/2203.02838">arxiv:2203.02838</a>
&#x1F4C8; 6 <br>
<p>Xubo Liu, Xinhao Mei, Qiushi Huang, Jianyuan Sun, Jinzheng Zhao, Haohe Liu, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang</p></summary>
<p>

**Abstract:** Audio captioning aims at using natural language to describe the content of an audio clip. Existing audio captioning systems are generally based on an encoder-decoder architecture, in which acoustic information is extracted by an audio encoder and then a language decoder is used to generate the captions. Training an audio captioning system often encounters the problem of data scarcity. Transferring knowledge from pre-trained audio models such as Pre-trained Audio Neural Networks (PANNs) have recently emerged as a useful method to mitigate this issue. However, there is less attention on exploiting pre-trained language models for the decoder, compared with the encoder. BERT is a pre-trained language model that has been extensively used in Natural Language Processing (NLP) tasks. Nevertheless, the potential of BERT as the language decoder for audio captioning has not been investigated. In this study, we demonstrate the efficacy of the pre-trained BERT model for audio captioning. Specifically, we apply PANNs as the encoder and initialize the decoder from the public pre-trained BERT models. We conduct an empirical study on the use of these BERT models for the decoder in the audio captioning model. Our models achieve competitive results with the existing audio captioning methods on the AudioCaps dataset.

</p>
</details>

<details><summary><b>Towards Robust Part-aware Instance Segmentation for Industrial Bin Picking</b>
<a href="https://arxiv.org/abs/2203.02767">arxiv:2203.02767</a>
&#x1F4C8; 6 <br>
<p>Yidan Feng, Biqi Yang, Xianzhi Li, Chi-Wing Fu, Rui Cao, Kai Chen, Qi Dou, Mingqiang Wei, Yun-Hui Liu, Pheng-Ann Heng</p></summary>
<p>

**Abstract:** Industrial bin picking is a challenging task that requires accurate and robust segmentation of individual object instances. Particularly, industrial objects can have irregular shapes, that is, thin and concave, whereas in bin-picking scenarios, objects are often closely packed with strong occlusion. To address these challenges, we formulate a novel part-aware instance segmentation pipeline. The key idea is to decompose industrial objects into correlated approximate convex parts and enhance the object-level segmentation with part-level segmentation. We design a part-aware network to predict part masks and part-to-part offsets, followed by a part aggregation module to assemble the recognized parts into instances. To guide the network learning, we also propose an automatic label decoupling scheme to generate ground-truth part-level labels from instance-level labels. Finally, we contribute the first instance segmentation dataset, which contains a variety of industrial objects that are thin and have non-trivial shapes. Extensive experimental results on various industrial objects demonstrate that our method can achieve the best segmentation results compared with the state-of-the-art approaches.

</p>
</details>

<details><summary><b>DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN</b>
<a href="https://arxiv.org/abs/2203.02762">arxiv:2203.02762</a>
&#x1F4C8; 5 <br>
<p>Wanchao Su, Hui Ye, Shu-Yu Chen, Lin Gao, Hongbo Fu</p></summary>
<p>

**Abstract:** The research topic of sketch-to-portrait generation has witnessed a boost of progress with deep learning techniques. The recently proposed StyleGAN architectures achieve state-of-the-art generation ability but the original StyleGAN is not friendly for sketch-based creation due to its unconditional generation nature. To address this issue, we propose a direct conditioning strategy to better preserve the spatial information under the StyleGAN framework. Specifically, we introduce Spatially Conditioned StyleGAN (SC-StyleGAN for short), which explicitly injects spatial constraints to the original StyleGAN generation process. We explore two input modalities, sketches and semantic maps, which together allow users to express desired generation results more precisely and easily. Based on SC-StyleGAN, we present DrawingInStyles, a novel drawing interface for non-professional users to easily produce high-quality, photo-realistic face images with precise control, either from scratch or editing existing ones. Qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.

</p>
</details>

<details><summary><b>Just Rank: Rethinking Evaluation with Word and Sentence Similarities</b>
<a href="https://arxiv.org/abs/2203.02679">arxiv:2203.02679</a>
&#x1F4C8; 4 <br>
<p>Bin Wang, C. -C. Jay Kuo, Haizhou Li</p></summary>
<p>

**Abstract:** Word and sentence embeddings are useful feature representations in natural language processing. However, intrinsic evaluation for embeddings lags far behind, and there has been no significant update since the past decade. Word and sentence similarity tasks have become the de facto evaluation method. It leads models to overfit to such evaluations, negatively impacting embedding models' development. This paper first points out the problems using semantic similarity as the gold standard for word and sentence embedding evaluations. Further, we propose a new intrinsic evaluation method called EvalRank, which shows a much stronger correlation with downstream tasks. Extensive experiments are conducted based on 60+ models and popular datasets to certify our judgments. Finally, the practical evaluation toolkit is released for future benchmarking purposes.

</p>
</details>

<details><summary><b>KPF-AE-LSTM: A Deep Probabilistic Model for Net-Load Forecasting in High Solar Scenarios</b>
<a href="https://arxiv.org/abs/2203.04401">arxiv:2203.04401</a>
&#x1F4C8; 3 <br>
<p>Deepthi Sen, Indrasis Chakraborty, Soumya Kundu, Andrew P. Reiman, Ian Beil, Andy Eiden</p></summary>
<p>

**Abstract:** With the expected rise in behind-the-meter solar penetration within the distribution networks, there is a need to develop time-series forecasting methods that can reliably predict the net-load, accurately quantifying its uncertainty and variability. This paper presents a deep learning method to generate probabilistic forecasts of day-ahead net-load at 15-min resolution, at various solar penetration levels. Our proposed deep-learning based architecture utilizes the dimensional reduction, from a higher-dimensional input to a lower-dimensional latent space, via a convolutional Autoencoder (AE). The extracted features from AE are then utilized to generate probability distributions across the latent space, by passing the features through a kernel-embedded Perron-Frobenius (kPF) operator. Finally, long short-term memory (LSTM) layers are used to synthesize time-series probability distributions of the forecasted net-load, from the latent space distributions. The models are shown to deliver superior forecast performance (as per several metrics), as well as maintain superior training efficiency, in comparison to existing benchmark models. Detailed analysis is carried out to evaluate the model performance across various solar penetration levels (up to 50\%), prediction horizons (e.g., 15\,min and 24\,hr ahead), and aggregation level of houses, as well as its robustness against missing measurements.

</p>
</details>

<details><summary><b>Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement</b>
<a href="https://arxiv.org/abs/2203.03622">arxiv:2203.03622</a>
&#x1F4C8; 3 <br>
<p>Ujjwal Upadhyay, Mukul Ranjan, Satish Golla, Swetha Tanamala, Preetham Sreenivas, Sasank Chilamkurthy, Jeyaraj Pandian, Jason Tarpley</p></summary>
<p>

**Abstract:** A stroke occurs when an artery in the brain ruptures and bleeds or when the blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's tissues due to the rupture or obstruction resulting in tissue death. The Middle cerebral artery (MCA) is the largest cerebral artery and the most commonly damaged vessel in stroke. The quick onset of a focused neurological deficit caused by interruption of blood flow in the territory supplied by the MCA is known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is used to estimate the extent of early ischemic changes in patients with MCA stroke. This study proposes a deep learning-based method to score the CT scan for ASPECTS. Our work has three highlights. First, we propose a novel method for medical image segmentation for stroke detection. Second, we show the effectiveness of AI solution for fully-automated ASPECT scoring with reduced diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72 for the infarcts segmentation. Lastly, we show that our model's performance is inline with inter-reader variability between radiologists.

</p>
</details>

<details><summary><b>Hybrid Deep Learning Model using SPCAGAN Augmentation for Insider Threat Analysis</b>
<a href="https://arxiv.org/abs/2203.02855">arxiv:2203.02855</a>
&#x1F4C8; 3 <br>
<p>R G Gayathri, Atul Sajjanhar, Yong Xiang</p></summary>
<p>

**Abstract:** Cyberattacks from within an organization's trusted entities are known as insider threats. Anomaly detection using deep learning requires comprehensive data, but insider threat data is not readily available due to confidentiality concerns of organizations. Therefore, there arises demand to generate synthetic data to explore enhanced approaches for threat analysis. We propose a linear manifold learning-based generative adversarial network, SPCAGAN, that takes input from heterogeneous data sources and adds a novel loss function to train the generator to produce high-quality data that closely resembles the original data distribution. Furthermore, we introduce a deep learning-based hybrid model for insider threat analysis. We provide extensive experiments for data synthesis, anomaly detection, adversarial robustness, and synthetic data quality analysis using benchmark datasets. In this context, empirical comparisons show that GAN-based oversampling is competitive with numerous typical oversampling regimes. For synthetic data generation, our SPCAGAN model overcame the problem of mode collapse and converged faster than previous GAN models. Results demonstrate that our proposed approach has a lower error, is more accurate, and generates substantially superior synthetic insider threat data than previous models.

</p>
</details>

<details><summary><b>Tabula: Efficiently Computing Nonlinear Activation Functions for Secure Neural Network Inference</b>
<a href="https://arxiv.org/abs/2203.02833">arxiv:2203.02833</a>
&#x1F4C8; 3 <br>
<p>Maximilian Lam, Michael Mitzenmacher, Vijay Janapa Reddi, Gu-Yeon Wei, David Brooks</p></summary>
<p>

**Abstract:** Multiparty computation approaches to secure neural network inference traditionally rely on garbled circuits for securely executing nonlinear activation functions. However, garbled circuits require excessive communication between server and client, impose significant storage overheads, and incur large runtime penalties. To eliminate these costs, we propose an alternative to garbled circuits: Tabula, an algorithm based on secure lookup tables. Tabula leverages neural networks' ability to be quantized and employs a secure lookup table approach to efficiently, securely, and accurately compute neural network nonlinear activation functions. Compared to garbled circuits with quantized inputs, when computing individual nonlinear functions, our experiments show Tabula uses between $35 \times$-$70 \times$ less communication, is over $100\times$ faster, and uses a comparable amount of storage. This leads to significant performance gains over garbled circuits with quantized inputs during secure inference on neural networks: Tabula reduces overall communication by up to $9 \times$ and achieves a speedup of up to $50 \times$, while imposing comparable storage costs.

</p>
</details>

<details><summary><b>Distributional Hardness Against Preconditioned Lasso via Erasure-Robust Designs</b>
<a href="https://arxiv.org/abs/2203.02824">arxiv:2203.02824</a>
&#x1F4C8; 3 <br>
<p>Jonathan A. Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi</p></summary>
<p>

**Abstract:** Sparse linear regression with ill-conditioned Gaussian random designs is widely believed to exhibit a statistical/computational gap, but there is surprisingly little formal evidence for this belief, even in the form of examples that are hard for restricted classes of algorithms. Recent work has shown that, for certain covariance matrices, the broad class of Preconditioned Lasso programs provably cannot succeed on polylogarithmically sparse signals with a sublinear number of samples. However, this lower bound only shows that for every preconditioner, there exists at least one signal that it fails to recover successfully. This leaves open the possibility that, for example, trying multiple different preconditioners solves every sparse linear regression problem.
  In this work, we prove a stronger lower bound that overcomes this issue. For an appropriate covariance matrix, we construct a single signal distribution on which any invertibly-preconditioned Lasso program fails with high probability, unless it receives a linear number of samples.
  Surprisingly, at the heart of our lower bound is a new positive result in compressed sensing. We show that standard sparse random designs are with high probability robust to adversarial measurement erasures, in the sense that if $b$ measurements are erased, then all but $O(b)$ of the coordinates of the signal are still information-theoretically identifiable. To our knowledge, this is the first time that partial recoverability of arbitrary sparse signals under erasures has been studied in compressed sensing.

</p>
</details>

<details><summary><b>Bathymetry Inversion using a Deep-Learning-Based Surrogate for Shallow Water Equations Solvers</b>
<a href="https://arxiv.org/abs/2203.02821">arxiv:2203.02821</a>
&#x1F4C8; 3 <br>
<p>Xiaofeng Liu, Yalan Song, Chaopeng Shen</p></summary>
<p>

**Abstract:** River bathymetry is critical for many aspects of water resources management. We propose and demonstrate a bathymetry inversion method using a deep-learning-based surrogate for shallow water equations solvers. The surrogate uses the convolutional autoencoder with a shared-encoder, separate-decoder architecture. It encodes the input bathymetry and decodes to separate outputs for flow-field variables. A gradient-based optimizer is used to perform bathymetry inversion with the trained surrogate. Two physically-based constraints on both bed elevation value and slope have to be added as inversion loss regularizations to obtain usable inversion results. Using the "L-curve" criterion, a heuristic approach was proposed to determine the regularization parameters. Both the surrogate model and the inversion algorithm show good performance. We found the bathymetry inversion process has two distinctive stages, which resembles the sculptural process of initial broad-brush calving and final detailing. The inversion loss due to flow prediction error reaches its minimum in the first stage and remains almost constant afterward. The bed elevation value and slope regularizations play the dominant role in the second stage in selecting the most probable solution. We also found the surrogate architecture (whether with both velocity and water surface elevation or velocity only as outputs) does not show significant impact on inversion result.

</p>
</details>

<details><summary><b>Fuzzy Forests For Feature Selection in High-Dimensional Survey Data: An Application to the 2020 U.S. Presidential Election</b>
<a href="https://arxiv.org/abs/2203.02818">arxiv:2203.02818</a>
&#x1F4C8; 3 <br>
<p>Sreemanti Dey, R. Michael Alvarez</p></summary>
<p>

**Abstract:** An increasingly common methodological issue in the field of social science is high-dimensional and highly correlated datasets that are unamenable to the traditional deductive framework of study. Analysis of candidate choice in the 2020 Presidential Election is one area in which this issue presents itself: in order to test the many theories explaining the outcome of the election, it is necessary to use data such as the 2020 Cooperative Election Study Common Content, with hundreds of highly correlated features. We present the Fuzzy Forests algorithm, a variant of the popular Random Forests ensemble method, as an efficient way to reduce the feature space in such cases with minimal bias, while also maintaining predictive performance on par with common algorithms like Random Forests and logit. Using Fuzzy Forests, we isolate the top correlates of candidate choice and find that partisan polarization was the strongest factor driving the 2020 presidential election.

</p>
</details>

<details><summary><b>The Impact of Differential Privacy on Group Disparity Mitigation</b>
<a href="https://arxiv.org/abs/2203.02745">arxiv:2203.02745</a>
&#x1F4C8; 3 <br>
<p>Victor Petrén Bach Hansen, Atula Tejaswi Neerkaje, Ramit Sawhney, Lucie Flek, Anders Søgaard</p></summary>
<p>

**Abstract:** The performance cost of differential privacy has, for some applications, been shown to be higher for minority groups; fairness, conversely, has been shown to disproportionally compromise the privacy of members of such groups. Most work in this area has been restricted to computer vision and risk assessment. In this paper, we evaluate the impact of differential privacy on fairness across four tasks, focusing on how attempts to mitigate privacy violations and between-group performance differences interact: Does privacy inhibit attempts to ensure fairness? To this end, we train $(\varepsilon,δ)$-differentially private models with empirical risk minimization and group distributionally robust training objectives. Consistent with previous findings, we find that differential privacy increases between-group performance differences in the baseline setting; but more interestingly, differential privacy reduces between-group performance differences in the robust setting. We explain this by reinterpreting differential privacy as regularization.

</p>
</details>

<details><summary><b>A Novel Dual Dense Connection Network for Video Super-resolution</b>
<a href="https://arxiv.org/abs/2203.02723">arxiv:2203.02723</a>
&#x1F4C8; 3 <br>
<p>Guofang Li, Yonggui Zhu</p></summary>
<p>

**Abstract:** Video super-resolution (VSR) refers to the reconstruction of high-resolution (HR) video from the corresponding low-resolution (LR) video. Recently, VSR has received increasing attention. In this paper, we propose a novel dual dense connection network that can generate high-quality super-resolution (SR) results. The input frames are creatively divided into reference frame, pre-temporal group and post-temporal group, representing information in different time periods. This grouping method provides accurate information of different time periods without causing time information disorder. Meanwhile, we produce a new loss function, which is beneficial to enhance the convergence ability of the model. Experiments show that our model is superior to other advanced models in Vid4 datasets and SPMCS-11 datasets.

</p>
</details>

<details><summary><b>Fusion-Correction Network for Single-Exposure Correction and Multi-Exposure Fusion</b>
<a href="https://arxiv.org/abs/2203.03624">arxiv:2203.03624</a>
&#x1F4C8; 2 <br>
<p>Jin Liang, Anran Zhang, Jun Xu, Hui Li, Xiantong Zhen</p></summary>
<p>

**Abstract:** The photographs captured by digital cameras usually suffer from over-exposure or under-exposure problems. The Single-Exposure Correction (SEC) and Multi-Exposure Fusion (MEF) are two widely studied image processing tasks for image exposure enhancement. However, current SEC and MEF methods ignore the internal correlation between SEC and MEF, and are proposed under distinct frameworks. What's more, most MEF methods usually fail at processing a sequence containing only under-exposed or over-exposed images. To alleviate these problems, in this paper, we develop an integrated framework to simultaneously tackle the SEC and MEF tasks. Built upon the Laplacian Pyramid (LP) decomposition, we propose a novel Fusion-Correction Network (FCNet) to fuse and correct an image sequence sequentially in a multi-level scheme. In each LP level, the image sequence is feed into a Fusion block and a Correction block for consecutive image fusion and exposure correction. The corrected image is upsampled and re-composed with the high-frequency detail components in next-level, producing the base sequence for the next-level blocks. Experiments on the benchmark dataset demonstrate that our FCNet is effective on both the SEC and MEF tasks.

</p>
</details>

<details><summary><b>Diffusion Maps : Using the Semigroup Property for Parameter Tuning</b>
<a href="https://arxiv.org/abs/2203.02867">arxiv:2203.02867</a>
&#x1F4C8; 2 <br>
<p>Shan Shan, Ingrid Daubechies</p></summary>
<p>

**Abstract:** Diffusion maps (DM) constitute a classic dimension reduction technique, for data lying on or close to a (relatively) low-dimensional manifold embedded in a much larger dimensional space. The DM procedure consists in constructing a spectral parametrization for the manifold from simulated random walks or diffusion paths on the data set. However, DM is hard to tune in practice. In particular, the task to set a diffusion time t when constructing the diffusion kernel matrix is critical. We address this problem by using the semigroup property of the diffusion operator. We propose a semigroup criterion for picking t. Experiments show that this principled approach is effective and robust.

</p>
</details>

<details><summary><b>The Proof is in the Pudding: Using Automated Theorem Proving to Generate Cooking Recipes</b>
<a href="https://arxiv.org/abs/2203.02683">arxiv:2203.02683</a>
&#x1F4C8; 2 <br>
<p>Louis Mahon, Carl Vogel</p></summary>
<p>

**Abstract:** This paper presents FASTFOOD, a rule-based Natural Language Generation Program for cooking recipes. Recipes are generated by using an Automated Theorem Proving procedure to select the ingredients and instructions, with ingredients corresponding to axioms and instructions to implications. FASTFOOD also contains a temporal optimization module which can rearrange the recipe to make it more time-efficient for the user, e.g. the recipe specifies to chop the vegetables while the rice is boiling. The system is described in detail, using a framework which divides Natural Language Generation into 4 phases: content production, content selection, content organisation and content realisation. A comparison is then made with similar existing systems and techniques.

</p>
</details>

<details><summary><b>Multi-channel deep convolutional neural networks for multi-classifying thyroid disease</b>
<a href="https://arxiv.org/abs/2203.03627">arxiv:2203.03627</a>
&#x1F4C8; 1 <br>
<p>Xinyu Zhang, Vincent CS. Lee, Jia Rong, James C. Lee, Jiangning Song, Feng Liu</p></summary>
<p>

**Abstract:** Thyroid disease instances have been continuously increasing since the 1990s, and thyroid cancer has become the most rapidly rising disease among all the malignancies in recent years. Most existing studies focused on applying deep convolutional neural networks for detecting thyroid cancer. Despite their satisfactory performance on binary classification tasks, limited studies have explored multi-class classification of thyroid disease types; much less is known of the diagnosis of co-existence situation for different types of thyroid diseases. Therefore, this study proposed a novel multi-channel convolutional neural network (CNN) architecture to address the multi-class classification task of thyroid disease. The multi-channel CNN merits from computed tomography to drive a comprehensive diagnostic decision for the overall thyroid gland, emphasizing the disease co-existence circumstance. Moreover, this study also examined alternative strategies to enhance the diagnostic accuracy of CNN models through concatenation of different scales of feature maps. Benchmarking experiments demonstrate the improved performance of the proposed multi-channel CNN architecture compared with the standard single-channel CNN architecture. More specifically, the multi-channel CNN achieved an accuracy of 0.909, precision of 0.944, recall of 0.896, specificity of 0.994, and F1 of 0.917, in contrast to the single-channel CNN, which obtained 0.902, 0.892, 0.909, 0.993, 0.898, respectively. In addition, the proposed model was evaluated in different gender groups; it reached a diagnostic accuracy of 0.908 for the female group and 0.901 for the male group. Collectively, the results highlight that the proposed multi-channel CNN has excellent generalization and has the potential to be deployed to provide computational decision support in clinical settings.

</p>
</details>

<details><summary><b>Coordinate Translator for Learning Deformable Medical Image Registration</b>
<a href="https://arxiv.org/abs/2203.03626">arxiv:2203.03626</a>
&#x1F4C8; 1 <br>
<p>Yihao Liu, Lianrui Zuo, Shuo Han, Jerry L. Prince, Aaron Carass</p></summary>
<p>

**Abstract:** The majority of deep learning (DL) based deformable image registration methods use convolutional neural networks (CNNs) to estimate displacement fields from pairs of moving and fixed images. This, however, requires the convolutional kernels in the CNN to not only extract intensity features from the inputs but also understand image coordinate systems. We argue that the latter task is challenging for traditional CNNs, limiting their performance in registration tasks. To tackle this problem, we first introduce Coordinate Translator (CoTr), a differentiable module that identifies matched features between the fixed and moving image and outputs their coordinate correspondences without the need for training. It unloads the burden of understanding image coordinate systems for CNNs, allowing them to focus on feature extraction. We then propose a novel deformable registration network, im2grid, that uses multiple CoTr's with the hierarchical features extracted from a CNN encoder and outputs a deformation field in a coarse-to-fine fashion. We compared im2grid with the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic resonance image registration. Our experiments show that im2grid outperforms these methods both qualitatively and quantitatively.

</p>
</details>

<details><summary><b>Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction</b>
<a href="https://arxiv.org/abs/2203.03623">arxiv:2203.03623</a>
&#x1F4C8; 1 <br>
<p>Yutong Xie, Quanzheng Li</p></summary>
<p>

**Abstract:** We propose a novel and unified method, measurement-conditioned denoising diffusion probabilistic model (MC-DDPM), for under-sampled medical image reconstruction based on DDPM. Different from previous works, MC-DDPM is defined in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on under-sampling mask. We apply this method to accelerate MRI reconstruction and the experimental results show excellent performance, outperforming full supervision baseline and the state-of-the-art score-based reconstruction method. Due to its generative nature, MC-DDPM can also quantify the uncertainty of reconstruction. Our code is available on github.

</p>
</details>

<details><summary><b>Compartmental Models for COVID-19 and Control via Policy Interventions</b>
<a href="https://arxiv.org/abs/2203.02860">arxiv:2203.02860</a>
&#x1F4C8; 1 <br>
<p>Swapneel Mehta, Noah Kasmanoff</p></summary>
<p>

**Abstract:** We demonstrate an approach to replicate and forecast the spread of the SARS-CoV-2 (COVID-19) pandemic using the toolkit of probabilistic programming languages (PPLs). Our goal is to study the impact of various modeling assumptions and motivate policy interventions enacted to limit the spread of infectious diseases. Using existing compartmental models we show how to use inference in PPLs to obtain posterior estimates for disease parameters. We improve popular existing models to reflect practical considerations such as the under-reporting of the true number of COVID-19 cases and motivate the need to model policy interventions for real-world data. We design an SEI3RD model as a reusable template and demonstrate its flexibility in comparison to other models. We also provide a greedy algorithm that selects the optimal series of policy interventions that are likely to control the infected population subject to provided constraints. We work within a simple, modular, and reproducible framework to enable immediate cross-domain access to the state-of-the-art in probabilistic inference with emphasis on policy interventions. We are not epidemiologists; the sole aim of this study is to serve as an exposition of methods, not to directly infer the real-world impact of policy-making for COVID-19.

</p>
</details>

<details><summary><b>Graph Neural Network Potential for Magnetic Materials</b>
<a href="https://arxiv.org/abs/2203.02853">arxiv:2203.02853</a>
&#x1F4C8; 1 <br>
<p>Hongyu Yu, Yang Zhong, Changsong Xu, Xingao Gong, Hongjun Xiang</p></summary>
<p>

**Abstract:** Machine Learning (ML) interatomic potential has shown its great power in condensed matter physics. However, ML interatomic potential for a magnetic system including both structural degrees of freedom and magnetic moments has not been well developed yet. A spin-dependent ML interatomic potential approach based on the crystal graph neural network (GNN) has been developed for any magnetic system. It consists of the Heisenberg edge graph neural network (HEGNN) and spin-distance edge graph neural network (SEGNN). The network captures the Heisenberg coefficient variation between different structures and the fine spin-lattice coupling of high order and multi-body interaction with high accuracy. In the tests, this method perfectly fitted a high-order spin Hamiltonian and two complex spin-lattice Hamiltonian and captured the fine spin-lattice coupling in BiFeO3. In addition, a disturbed structure of BiFeO3 with strain was successfully optimized with the trained potential. Our work has expanded the powerful ML GNN potentials to magnetic systems, which paves a new way for large-scale dynamic simulations on spin-lattice coupled systems.

</p>
</details>

<details><summary><b>Recursive Reasoning Graph for Multi-Agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2203.02844">arxiv:2203.02844</a>
&#x1F4C8; 1 <br>
<p>Xiaobai Ma, David Isele, Jayesh K. Gupta, Kikuo Fujimura, Mykel J. Kochenderfer</p></summary>
<p>

**Abstract:** Multi-agent reinforcement learning (MARL) provides an efficient way for simultaneously learning policies for multiple agents interacting with each other. However, in scenarios requiring complex interactions, existing algorithms can suffer from an inability to accurately anticipate the influence of self-actions on other agents. Incorporating an ability to reason about other agents' potential responses can allow an agent to formulate more effective strategies. This paper adopts a recursive reasoning model in a centralized-training-decentralized-execution framework to help learning agents better cooperate with or compete against others. The proposed algorithm, referred to as the Recursive Reasoning Graph (R2G), shows state-of-the-art performance on multiple multi-agent particle and robotics games.

</p>
</details>

<details><summary><b>Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer</b>
<a href="https://arxiv.org/abs/2203.02794">arxiv:2203.02794</a>
&#x1F4C8; 1 <br>
<p>Yawei Li, Xin Wu, Ping Yang, Guoqian Jiang, Yuan Luo</p></summary>
<p>

**Abstract:** The recent development of imaging and sequencing technologies enables systematic advances in the clinical study of lung cancer. Meanwhile, the human mind is limited in effectively handling and fully utilizing the accumulation of such enormous amounts of data. Machine learning-based approaches play a critical role in integrating and analyzing these large and complex datasets, which have extensively characterized lung cancer through the use of different perspectives from these accrued data. In this article, we provide an overview of machine learning-based approaches that strengthen the varying aspects of lung cancer diagnosis and therapy, including early detection, auxiliary diagnosis, prognosis prediction and immunotherapy practice. Moreover, we highlight the challenges and opportunities for future applications of machine learning in lung cancer.

</p>
</details>

<details><summary><b>Rib Suppression in Digital Chest Tomosynthesis</b>
<a href="https://arxiv.org/abs/2203.02772">arxiv:2203.02772</a>
&#x1F4C8; 1 <br>
<p>Yihua Sun, Qingsong Yao, Yuanyuan Lyu, Jianji Wang, Yi Xiao, Hongen Liao, S. Kevin Zhou</p></summary>
<p>

**Abstract:** Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D images of a human chest for pulmonary disease screening, with 2D X-ray projections taken within an extremely limited range of angles. However, under the limited angle scenario, DCT contains strong artifacts caused by the presence of ribs, jamming the imaging quality of the lung area. Recently, great progress has been achieved for rib suppression in a single X-ray image, to reveal a clearer lung texture. We firstly extend the rib suppression problem to the 3D case at the software level. We propose a $\textbf{T}$omosynthesis $\textbf{RI}$b Su$\textbf{P}$pression and $\textbf{L}$ung $\textbf{E}$nhancement $\textbf{Net}$work (TRIPLE-Net) to model the 3D rib component and provide a rib-free DCT. TRIPLE-Net takes the advantages from both 2D and 3D domains, which model the ribs in DCT with the exact FBP procedure and 3D depth information, respectively. The experiments on simulated datasets and clinical data have shown the effectiveness of TRIPLE-Net to preserve lung details as well as improve the imaging quality of pulmonary diseases. Finally, an expert user study confirms our findings.

</p>
</details>

<details><summary><b>Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance</b>
<a href="https://arxiv.org/abs/2203.02770">arxiv:2203.02770</a>
&#x1F4C8; 1 <br>
<p>Shiwei Liu, Yuesong Tian, Tianlong Chen, Li Shen</p></summary>
<p>

**Abstract:** Generative adversarial networks (GANs) have received an upsurging interest since being proposed due to the high quality of the generated data. While achieving increasingly impressive results, the resource demands associated with the large model size hinders the usage of GANs in resource-limited scenarios. For inference, the existing model compression techniques can reduce the model complexity with comparable performance. However, the training efficiency of GANs has less been explored due to the fragile training process of GANs. In this paper, we, for the first time, explore the possibility of directly training sparse GAN from scratch without involving any dense or pre-training steps. Even more unconventionally, our proposed method enables directly training sparse unbalanced GANs with an extremely sparse generator from scratch. Instead of training full GANs, we start with sparse GANs and dynamically explore the parameter space spanned over the generator throughout training. Such a sparse-to-sparse training procedure enhances the capacity of the highly sparse generator progressively while sticking to a fixed small parameter budget with appealing training and inference efficiency gains. Extensive experiments with modern GAN architectures validate the effectiveness of our method. Our sparsified GANs, trained from scratch in one single run, are able to outperform the ones learned by expensive iterative pruning and re-training. Perhaps most importantly, we find instead of inheriting parameters from expensive pre-trained GANs, directly training sparse GANs from scratch can be a much more efficient solution. For example, only training with a 80% sparse generator and a 70% sparse discriminator, our method can achieve even better performance than the dense BigGAN.

</p>
</details>

<details><summary><b>Flurry: a Fast Framework for Reproducible Multi-layered Provenance Graph Representation Learning</b>
<a href="https://arxiv.org/abs/2203.02744">arxiv:2203.02744</a>
&#x1F4C8; 1 <br>
<p>Maya Kapoor, Joshua Melton, Michael Ridenhour, Mahalavanya Sriram, Thomas Moyer, Siddharth Krishnan</p></summary>
<p>

**Abstract:** Complex heterogeneous dynamic networks like knowledge graphs are powerful constructs that can be used in modeling data provenance from computer systems. From a security perspective, these attributed graphs enable causality analysis and tracing for analyzing a myriad of cyberattacks. However, there is a paucity in systematic development of pipelines that transform system executions and provenance into usable graph representations for machine learning tasks. This lack of instrumentation severely inhibits scientific advancement in provenance graph machine learning by hindering reproducibility and limiting the availability of data that are critical for techniques like graph neural networks. To fulfill this need, we present Flurry, an end-to-end data pipeline which simulates cyberattacks, captures provenance data from these attacks at multiple system and application layers, converts audit logs from these attacks into data provenance graphs, and incorporates this data with a framework for training deep neural models that supports preconfigured or custom-designed models for analysis in real-world resilient systems. We showcase this pipeline by processing data from multiple system attacks and performing anomaly detection via graph classification using current benchmark graph representational learning frameworks. Flurry provides a fast, customizable, extensible, and transparent solution for providing this much needed data to cybersecurity professionals.

</p>
</details>

<details><summary><b>MaxDropoutV2: An Improved Method to Drop out Neurons in Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2203.02740">arxiv:2203.02740</a>
&#x1F4C8; 1 <br>
<p>Claudio Filipi Goncalves do Santos, Mateus Roder, Leandro A. Passos, João P. Papa</p></summary>
<p>

**Abstract:** In the last decade, exponential data growth supplied the machine learning-based algorithms' capacity and enabled their usage in daily life activities. Additionally, such an improvement is partially explained due to the advent of deep learning techniques, i.e., stacks of simple architectures that end up in more complex models. Although both factors produce outstanding results, they also pose drawbacks regarding the learning process since training complex models denotes an expensive task and results are prone to overfit the training data. A supervised regularization technique called MaxDropout was recently proposed to tackle the latter, providing several improvements concerning traditional regularization approaches. In this paper, we present its improved version called MaxDropoutV2. Results considering two public datasets show that the model performs faster than the standard version and, in most cases, provides more accurate results.

</p>
</details>

<details><summary><b>High-resolution Coastline Extraction in SAR Images via MISP-GGD Superpixel Segmentation</b>
<a href="https://arxiv.org/abs/2203.02708">arxiv:2203.02708</a>
&#x1F4C8; 1 <br>
<p>Odysseas Pappas, Nantheera Anantrasirichai, Byron Adams, Alin Achim</p></summary>
<p>

**Abstract:** High accuracy coastline/shoreline extraction from SAR imagery is a crucial step in a number of maritime and coastal monitoring applications. We present a method based on image segmentation using the Generalised Gamma Mixture Model superpixel algorithm (MISP-GGD). MISP-GGD produces superpixels adhering with great accuracy to object edges in the image, such as the coastline. Unsupervised clustering of the generated superpixels according to textural and radiometric features allows for generation of a land/water mask from which a highly accurate coastline can be extracted. We present results of our proposed method on a number of SAR images of varying characteristics.

</p>
</details>

<details><summary><b>ECMG: Exemplar-based Commit Message Generation</b>
<a href="https://arxiv.org/abs/2203.02700">arxiv:2203.02700</a>
&#x1F4C8; 1 <br>
<p>Ensheng Shia, Yanlin Wangb, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun</p></summary>
<p>

**Abstract:** Commit messages concisely describe the content of code diffs (i.e., code changes) and the intent behind them. Recently, many approaches have been proposed to generate commit messages automatically. The information retrieval-based methods reuse the commit messages of similar code diffs, while the neural-based methods learn the semantic connection between code diffs and commit messages. However, the reused commit messages might not accurately describe the content/intent of code diffs and neural-based methods tend to generate high-frequent and repetitive tokens in the corpus. In this paper, we combine the advantages of the two technical routes and propose a novel exemplar-based neural commit message generation model, which treats the similar commit message as an exemplar and leverages it to guide the neural network model to generate an accurate commit message. We perform extensive experiments and the results confirm the effectiveness of our model.

</p>
</details>

<details><summary><b>IDmUNet: A new image decomposition induced network for sparse feature segmentation</b>
<a href="https://arxiv.org/abs/2203.02690">arxiv:2203.02690</a>
&#x1F4C8; 1 <br>
<p>Yumeng Ren, Yiming Gao, Chunlin Wu, Xue-cheng Tai</p></summary>
<p>

**Abstract:** UNet and its variants are among the most popular methods for medical image segmentation. Despite their successes in task generality, most of them consider little mathematical modeling behind specific applications. In this paper, we focus on the sparse feature segmentation task and make a task-oriented network design, in which the target objects are sparsely distributed and the background is hard to be mathematically modeled. We start from an image decomposition model with sparsity regularization, and propose a deep unfolding network, namely IDNet, based on an iterative solver, scaled alternating direction method of multipliers (scaled-ADMM). The IDNet splits raw inputs into double feature layers. Then a new task-oriented segmentation network is constructed, dubbed as IDmUNet, based on the proposed IDNets and a mini-UNet. Because of the sparsity prior and deep unfolding method in the structure design, this IDmUNet combines the advantages of mathematical modeling and data-driven approaches. Firstly, our approach has mathematical interpretability and can achieve favorable performance with far fewer learnable parameters. Secondly, our IDmUNet is robust in a simple end-to-end training with explainable behaviors. In the experiments of retinal vessel segmentation (RVS), IDmUNet produces the state-of-the-art results with only 0.07m parameters, whereas SA-UNet, one of the latest variants of UNet, contains 0.54m and the original UNet 31.04m. Moreover, the training procedure of our network converges faster without overfitting phenomenon. This decomposition-based network construction strategy can be generalized to other problems with mathematically clear targets and complicated unclear backgrounds.

</p>
</details>

<details><summary><b>Fully Decentralized, Scalable Gaussian Processes for Multi-Agent Federated Learning</b>
<a href="https://arxiv.org/abs/2203.02865">arxiv:2203.02865</a>
&#x1F4C8; 0 <br>
<p>George P. Kontoudis, Daniel J. Stilwell</p></summary>
<p>

**Abstract:** In this paper, we propose decentralized and scalable algorithms for Gaussian process (GP) training and prediction in multi-agent systems. To decentralize the implementation of GP training optimization algorithms, we employ the alternating direction method of multipliers (ADMM). A closed-form solution of the decentralized proximal ADMM is provided for the case of GP hyper-parameter training with maximum likelihood estimation. Multiple aggregation techniques for GP prediction are decentralized with the use of iterative and consensus methods. In addition, we propose a covariance-based nearest neighbor selection strategy that enables a subset of agents to perform predictions. The efficacy of the proposed methods is illustrated with numerical experiments on synthetic and real data.

</p>
</details>

<details><summary><b>Algorithmic Regularization in Model-free Overparametrized Asymmetric Matrix Factorization</b>
<a href="https://arxiv.org/abs/2203.02839">arxiv:2203.02839</a>
&#x1F4C8; 0 <br>
<p>Liwei Jiang, Yudong Chen, Lijun Ding</p></summary>
<p>

**Abstract:** We study the asymmetric matrix factorization problem under a natural nonconvex formulation with arbitrary overparamatrization. We consider the model-free setting with no further assumption on the rank or singular values of the observed matrix, where the global optima provably overfit. We show that vanilla gradient descent with small random initialization and early stopping produces the best low-rank approximation of the observed matrix, without any additional regularization. We provide a sharp analysis on relationship between the iteration complexity, initialization size, stepsize and final error. In particular, our complexity bound is almost dimension-free and depends logarithmically on the final error, and our results have lenient requirements on the stepsize and initialization. Our bounds improve upon existing work and show good agreement with numerical experiments.

</p>
</details>

<details><summary><b>Better Approximation Guarantees for the NSGA-II by Using the Current Crowding Distance</b>
<a href="https://arxiv.org/abs/2203.02693">arxiv:2203.02693</a>
&#x1F4C8; 0 <br>
<p>Weijie Zheng, Benjamin Doerr</p></summary>
<p>

**Abstract:** A recent runtime analysis (Zheng, Liu, Doerr (2022)) has shown that a variant of the NSGA-II algorithm can efficiently compute the full Pareto front of the OneMinMax problem when the population size is by a constant factor larger than the Pareto front, but that this is not possible when the population size is only equal to the Pareto front size. In this work, we analyze how well the NSGA-II approximates the Pareto front when it cannot compute the whole front. We observe experimentally and by mathematical means that already when the population size is half the Pareto front size, relatively large gaps in the Pareto front remain. The reason for this phenomenon is that the NSGA-II in the selection stage computes the crowding distance once and then repeatedly removes individuals with smallest crowding distance without updating the crowding distance after each removal. We propose an efficient way to implement the NSGA-II using the momentary crowding distance. In our experiments, this algorithm approximates the Pareto front much better than the previous version. We also prove that the gaps in the Pareto front are at most a constant factor larger than the theoretical minimum.

</p>
</details>

<details><summary><b>Koopman operator for time-dependent reliability analysis</b>
<a href="https://arxiv.org/abs/2203.02658">arxiv:2203.02658</a>
&#x1F4C8; 0 <br>
<p>Navaneeth N., Souvik Chakraborty</p></summary>
<p>

**Abstract:** Time-dependent structural reliability analysis of nonlinear dynamical systems is non-trivial; subsequently, scope of most of the structural reliability analysis methods is limited to time-independent reliability analysis only. In this work, we propose a Koopman operator based approach for time-dependent reliability analysis of nonlinear dynamical systems. Since the Koopman representations can transform any nonlinear dynamical system into a linear dynamical system, the time evolution of dynamical systems can be obtained by Koopman operators seamlessly regardless of the nonlinear or chaotic behavior. Despite the fact that the Koopman theory has been in vogue a long time back, identifying intrinsic coordinates is a challenging task; to address this, we propose an end-to-end deep learning architecture that learns the Koopman observables and then use it for time marching the dynamical response. Unlike purely data-driven approaches, the proposed approach is robust even in the presence of uncertainties; this renders the proposed approach suitable for time-dependent reliability analysis. We propose two architectures; one suitable for time-dependent reliability analysis when the system is subjected to random initial condition and the other suitable when the underlying system have uncertainties in system parameters. The proposed approach is robust and generalizes to unseen environment (out-of-distribution prediction). Efficacy of the proposed approached is illustrated using three numerical examples. Results obtained indicate supremacy of the proposed approach as compared to purely data-driven auto-regressive neural network and long-short term memory network.

</p>
</details>


{% endraw %}
Prev: [2022.03.04]({{ '/2022/03/04/2022.03.04.html' | relative_url }})  Next: [2022.03.06]({{ '/2022/03/06/2022.03.06.html' | relative_url }})