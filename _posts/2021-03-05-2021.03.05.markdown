## Summary for 2021-03-05, created on 2021-12-22


<details><summary><b>Off-Belief Learning</b>
<a href="https://arxiv.org/abs/2103.04000">arxiv:2103.04000</a>
&#x1F4C8; 230 <br>
<p>Hengyuan Hu, Adam Lerer, Brandon Cui, David Wu, Luis Pineda, Noam Brown, Jakob Foerster</p></summary>
<p>

**Abstract:** The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents' actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy $π_1$ that is optimized assuming past actions were taken by a given, fixed policy ($π_0$), but assuming that future actions will be taken by $π_1$. When $π_0$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents' behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI & ZSC problem Hanabi.

</p>
</details>

<details><summary><b>Generating Images with Sparse Representations</b>
<a href="https://arxiv.org/abs/2103.03841">arxiv:2103.03841</a>
&#x1F4C8; 80 <br>
<p>Charlie Nash, Jacob Menick, Sander Dieleman, Peter W. Battaglia</p></summary>
<p>

**Abstract:** The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models.

</p>
</details>

<details><summary><b>Slow-Fast Auditory Streams For Audio Recognition</b>
<a href="https://arxiv.org/abs/2103.03516">arxiv:2103.03516</a>
&#x1F4C8; 74 <br>
<p>Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen</p></summary>
<p>

**Abstract:** We propose a two-stream convolutional network for audio recognition, that operates on time-frequency spectrogram inputs. Following similar success in visual recognition, we learn Slow-Fast auditory streams with separable convolutions and multi-level lateral connections. The Slow pathway has high channel capacity while the Fast pathway operates at a fine-grained temporal resolution. We showcase the importance of our two-stream proposal on two diverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve state-of-the-art results on both.

</p>
</details>

<details><summary><b>Rissanen Data Analysis: Examining Dataset Characteristics via Description Length</b>
<a href="https://arxiv.org/abs/2103.03872">arxiv:2103.03872</a>
&#x1F4C8; 66 <br>
<p>Ethan Perez, Douwe Kiela, Kyunghyun Cho</p></summary>
<p>

**Abstract:** We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.

</p>
</details>

<details><summary><b>Measuring Mathematical Problem Solving With the MATH Dataset</b>
<a href="https://arxiv.org/abs/2103.03874">arxiv:2103.03874</a>
&#x1F4C8; 43 <br>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt</p></summary>
<p>

**Abstract:** Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.

</p>
</details>

<details><summary><b>Unbalanced minibatch Optimal Transport; applications to Domain Adaptation</b>
<a href="https://arxiv.org/abs/2103.03606">arxiv:2103.03606</a>
&#x1F4C8; 22 <br>
<p>Kilian Fatras, Thibault Séjourné, Nicolas Courty, Rémi Flamary</p></summary>
<p>

**Abstract:** Optimal transport distances have found many applications in machine learning for their capacity to compare non-parametric probability distributions. Yet their algorithmic complexity generally prevents their direct use on large scale datasets. Among the possible strategies to alleviate this issue, practitioners can rely on computing estimates of these distances over subsets of data, {\em i.e.} minibatches. While computationally appealing, we highlight in this paper some limits of this strategy, arguing it can lead to undesirable smoothing effects. As an alternative, we suggest that the same minibatch strategy coupled with unbalanced optimal transport can yield more robust behavior. We discuss the associated theoretical properties, such as unbiased estimators, existence of gradients and concentration bounds. Our experimental study shows that in challenging problems associated to domain adaptation, the use of unbalanced optimal transport leads to significantly better results, competing with or surpassing recent baselines.

</p>
</details>

<details><summary><b>LOHO: Latent Optimization of Hairstyles via Orthogonalization</b>
<a href="https://arxiv.org/abs/2103.03891">arxiv:2103.03891</a>
&#x1F4C8; 18 <br>
<p>Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi</p></summary>
<p>

**Abstract:** Hairstyle transfer is challenging due to hair structure differences in the source and target hair. Therefore, we propose Latent Optimization of Hairstyles via Orthogonalization (LOHO), an optimization-based approach using GAN inversion to infill missing hair structure details in latent space during hairstyle transfer. Our approach decomposes hair into three attributes: perceptual structure, appearance, and style, and includes tailored losses to model each of these attributes independently. Furthermore, we propose two-stage optimization and gradient orthogonalization to enable disentangled latent space optimization of our hair attributes. Using LOHO for latent space manipulation, users can synthesize novel photorealistic images by manipulating hair attributes either individually or jointly, transferring the desired attributes from reference hairstyles. LOHO achieves a superior FID compared with the current state-of-the-art (SOTA) for hairstyle transfer. Additionally, LOHO preserves the subject's identity comparably well according to PSNR and SSIM when compared to SOTA image embedding pipelines. Code is available at https://github.com/dukebw/LOHO.

</p>
</details>

<details><summary><b>Graph Convolutional Embeddings for Recommender Systems</b>
<a href="https://arxiv.org/abs/2103.03587">arxiv:2103.03587</a>
&#x1F4C8; 9 <br>
<p>Paula Gómez Duran, Alexandros Karatzoglou, Jordi Vitrià, Xin Xin, Ioannis Arapakis</p></summary>
<p>

**Abstract:** Modern recommender systems (RS) work by processing a number of signals that can be inferred from large sets of user-item interaction data. The main signal to analyze stems from the raw matrix that represents interactions. However, we can increase the performance of RS by considering other kinds of signals like the context of interactions, which could be, for example, the time or date of the interaction, the user location, or sequential data corresponding to the historical interactions of the user with the system. These complex, context-based interaction signals are characterized by a rich relational structure that can be represented by a multi-partite graph. Graph Convolutional Networks (GCNs) have been used successfully in collaborative filtering with simple user-item interaction data. In this work, we generalize the use of GCNs for N-partite graphs by considering N multiple context dimensions and propose a simple way for their seamless integration in modern deep learning RS architectures. More specifically, we define a graph convolutional embedding layer for N-partite graphs that processes user-item-context interactions, and constructs node embeddings by leveraging their relational structure. Experiments on several datasets from recommender systems to drug re-purposing show the benefits of the introduced GCN embedding layer by measuring the performance of different context-enriched tasks.

</p>
</details>

<details><summary><b>Over-the-Air Statistical Estimation</b>
<a href="https://arxiv.org/abs/2103.04014">arxiv:2103.04014</a>
&#x1F4C8; 8 <br>
<p>Chuan-Zheng Lee, Leighton Pate Barnes, Ayfer Ozgur</p></summary>
<p>

**Abstract:** We study schemes and lower bounds for distributed minimax statistical estimation over a Gaussian multiple-access channel (MAC) under squared error loss, in a framework combining statistical estimation and wireless communication. First, we develop "analog" joint estimation-communication schemes that exploit the superposition property of the Gaussian MAC and we characterize their risk in terms of the number of nodes and dimension of the parameter space. Then, we derive information-theoretic lower bounds on the minimax risk of any estimation scheme restricted to communicate the samples over a given number of uses of the channel and show that the risk achieved by our proposed schemes is within a logarithmic factor of these lower bounds. We compare both achievability and lower bound results to previous "digital" lower bounds, where nodes transmit errorless bits at the Shannon capacity of the MAC, showing that estimation schemes that leverage the physical layer offer a drastic reduction in estimation error over digital schemes relying on a physical-layer abstraction.

</p>
</details>

<details><summary><b>Disambiguating Affective Stimulus Associations for Robot Perception and Dialogue</b>
<a href="https://arxiv.org/abs/2103.03940">arxiv:2103.03940</a>
&#x1F4C8; 7 <br>
<p>Henrique Siqueira, Alexander Sutherland, Pablo Barros, Mattias Kerzel, Sven Magg, Stefan Wermter</p></summary>
<p>

**Abstract:** Effectively recognising and applying emotions to interactions is a highly desirable trait for social robots. Implicitly understanding how subjects experience different kinds of actions and objects in the world is crucial for natural HRI interactions, with the possibility to perform positive actions and avoid negative actions. In this paper, we utilize the NICO robot's appearance and capabilities to give the NICO the ability to model a coherent affective association between a perceived auditory stimulus and a temporally asynchronous emotion expression. This is done by combining evaluations of emotional valence from vision and language. NICO uses this information to make decisions about when to extend conversations in order to accrue more affective information if the representation of the association is not coherent. Our primary contribution is providing a NICO robot with the ability to learn the affective associations between a perceived auditory stimulus and an emotional expression. NICO is able to do this for both individual subjects and specific stimuli, with the aid of an emotion-driven dialogue system that rectifies emotional expression incoherences. The robot is then able to use this information to determine a subject's enjoyment of perceived auditory stimuli in a real HRI scenario.

</p>
</details>

<details><summary><b>Golem: An algorithm for robust experiment and process optimization</b>
<a href="https://arxiv.org/abs/2103.03716">arxiv:2103.03716</a>
&#x1F4C8; 7 <br>
<p>Matteo Aldeghi, Florian Häse, Riley J. Hickman, Isaac Tamblyn, Alán Aspuru-Guzik</p></summary>
<p>

**Abstract:** Numerous challenges in science and engineering can be framed as optimization tasks, including the maximization of reaction yields, the optimization of molecular and materials properties, and the fine-tuning of automated hardware protocols. Design of experiment and optimization algorithms are often adopted to solve these tasks efficiently. Increasingly, these experiment planning strategies are coupled with automated hardware to enable autonomous experimental platforms. The vast majority of the strategies used, however, do not consider robustness against the variability of experiment and process conditions. In fact, it is generally assumed that these parameters are exact and reproducible. Yet some experiments may have considerable noise associated with some of their conditions, and process parameters optimized under precise control may be applied in the future under variable operating conditions. In either scenario, the optimal solutions found might not be robust against input variability, affecting the reproducibility of results and returning suboptimal performance in practice. Here, we introduce Golem, an algorithm that is agnostic to the choice of experiment planning strategy and that enables robust experiment and process optimization. Golem identifies optimal solutions that are robust to input uncertainty, thus ensuring the reproducible performance of optimized experimental protocols and processes. It can be used to analyze the robustness of past experiments, or to guide experiment planning algorithms toward robust solutions on the fly. We assess the performance and domain of applicability of Golem through extensive benchmark studies and demonstrate its practical relevance by optimizing an analytical chemistry protocol under the presence of significant noise in its experimental conditions.

</p>
</details>

<details><summary><b>Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach</b>
<a href="https://arxiv.org/abs/2103.03501">arxiv:2103.03501</a>
&#x1F4C8; 7 <br>
<p>Giang Truong, Huu Le, David Suter, Erchuan Zhang, Syed Zulqarnain Gilani</p></summary>
<p>

**Abstract:** Robust model fitting is a core algorithm in a large number of computer vision applications. Solving this problem efficiently for datasets highly contaminated with outliers is, however, still challenging due to the underlying computational complexity. Recent literature has focused on learning-based algorithms. However, most approaches are supervised which require a large amount of labelled training data. In this paper, we introduce a novel unsupervised learning framework that learns to directly solve robust model fitting. Unlike other methods, our work is agnostic to the underlying input features, and can be easily generalized to a wide variety of LP-type problems with quasi-convex residuals. We empirically show that our method outperforms existing unsupervised learning approaches, and achieves competitive results compared to traditional methods on several important computer vision problems.

</p>
</details>

<details><summary><b>Selective Replay Enhances Learning in Online Continual Analogical Reasoning</b>
<a href="https://arxiv.org/abs/2103.03987">arxiv:2103.03987</a>
&#x1F4C8; 6 <br>
<p>Tyler L. Hayes, Christopher Kanan</p></summary>
<p>

**Abstract:** In continual learning, a system learns from non-stationary data streams or batches without catastrophic forgetting. While this problem has been heavily studied in supervised image classification and reinforcement learning, continual learning in neural networks designed for abstract reasoning has not yet been studied. Here, we study continual learning of analogical reasoning. Analogical reasoning tests such as Raven's Progressive Matrices (RPMs) are commonly used to measure non-verbal abstract reasoning in humans, and recently offline neural networks for the RPM problem have been proposed. In this paper, we establish experimental baselines, protocols, and forward and backward transfer metrics to evaluate continual learners on RPMs. We employ experience replay to mitigate catastrophic forgetting. Prior work using replay for image classification tasks has found that selectively choosing the samples to replay offers little, if any, benefit over random selection. In contrast, we find that selective replay can significantly outperform random selection for the RPM task.

</p>
</details>

<details><summary><b>Causal Analysis of Agent Behavior for AI Safety</b>
<a href="https://arxiv.org/abs/2103.03938">arxiv:2103.03938</a>
&#x1F4C8; 6 <br>
<p>Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus Kunesch, Shane Legg, Pedro A. Ortega</p></summary>
<p>

**Abstract:** As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.

</p>
</details>

<details><summary><b>MalBERT: Using Transformers for Cybersecurity and Malicious Software Detection</b>
<a href="https://arxiv.org/abs/2103.03806">arxiv:2103.03806</a>
&#x1F4C8; 6 <br>
<p>Abir Rahali, Moulay A. Akhloufi</p></summary>
<p>

**Abstract:** In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to find automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the field of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers' architecture to automatically detect malicious software. We propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection.

</p>
</details>

<details><summary><b>GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra</b>
<a href="https://arxiv.org/abs/2103.03653">arxiv:2103.03653</a>
&#x1F4C8; 6 <br>
<p>Maciej Besta, Zur Vonarburg-Shmaria, Yannick Schaffner, Leonardo Schwarz, Grzegorz Kwasniewski, Lukas Gianinazzi, Jakub Beranek, Kacper Janda, Tobias Holenstein, Sebastian Leisinger, Peter Tatkowski, Esref Ozdemir, Adrian Balla, Marcin Copik, Philipp Lindenberger, Pavel Kalvoda, Marek Konieczny, Onur Mutlu, Torsten Hoefler</p></summary>
<p>

**Abstract:** We propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by up to >2x), maximal clique listing (by up to >9x), k-clique listing (by 1.1x), and subgraph isomorphism (by up to 2.5x), also obtaining better theoretical performance bounds.

</p>
</details>

<details><summary><b>Physics-aware deep neural networks for surrogate modeling of turbulent natural convection</b>
<a href="https://arxiv.org/abs/2103.03565">arxiv:2103.03565</a>
&#x1F4C8; 6 <br>
<p>Didier Lucor, Atul Agrawal, Anne Sergent</p></summary>
<p>

**Abstract:** Recent works have explored the potential of machine learning as data-driven turbulence closures for RANS and LES techniques. Beyond these advances, the high expressivity and agility of physics-informed neural networks (PINNs) make them promising candidates for full fluid flow PDE modeling. An important question is whether this new paradigm, exempt from the traditional notion of discretization of the underlying operators very much connected to the flow scales resolution, is capable of sustaining high levels of turbulence characterized by multi-scale features? We investigate the use of PINNs surrogate modeling for turbulent Rayleigh-B{é}nard (RB) convection flows in rough and smooth rectangular cavities, mainly relying on DNS temperature data from the fluid bulk. We carefully quantify the computational requirements under which the formulation is capable of accurately recovering the flow hidden quantities. We then propose a new padding technique to distribute some of the scattered coordinates-at which PDE residuals are minimized-around the region of labeled data acquisition. We show how it comes to play as a regularization close to the training boundaries which are zones of poor accuracy for standard PINNs and results in a noticeable global accuracy improvement at iso-budget. Finally, we propose for the first time to relax the incompressibility condition in such a way that it drastically benefits the optimization search and results in a much improved convergence of the composite loss function. The RB results obtained at high Rayleigh number Ra = 2 $\bullet$ 10 9 are particularly impressive: the predictive accuracy of the surrogate over the entire half a billion DNS coordinates yields errors for all flow variables ranging between [0.3% -- 4%] in the relative L 2 norm, with a training relying only on 1.6% of the DNS data points.

</p>
</details>

<details><summary><b>Nishimori meets Bethe: a spectral method for node classification in sparse weighted graphs</b>
<a href="https://arxiv.org/abs/2103.03561">arxiv:2103.03561</a>
&#x1F4C8; 6 <br>
<p>Lorenzo Dall'Amico, Romain Couillet, Nicolas Tremblay</p></summary>
<p>

**Abstract:** This article unveils a new relation between the Nishimori temperature parametrizing a distribution P and the Bethe free energy on random Erdos-Renyi graphs with edge weights distributed according to P. Estimating the Nishimori temperature being a task of major importance in Bayesian inference problems, as a practical corollary of this new relation, a numerical method is proposed to accurately estimate the Nishimori temperature from the eigenvalues of the Bethe Hessian matrix of the weighted graph. The algorithm, in turn, is used to propose a new spectral method for node classification in weighted (possibly sparse) graphs. The superiority of the method over competing state-of-the-art approaches is demonstrated both through theoretical arguments and real-world data experiments.

</p>
</details>

<details><summary><b>ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ Neural Network Processing in Phase Change RAM</b>
<a href="https://arxiv.org/abs/2103.03953">arxiv:2103.03953</a>
&#x1F4C8; 5 <br>
<p>Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad Salehi</p></summary>
<p>

**Abstract:** Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in real-world applications related to machine learning and Artificial Intelligence (AI), several hardware accelerator de-signs for ANNs have been proposed recently. In this paper, we present a novel processing-in-memory (PIM) engine called ODIN that employs hybrid binary-stochastic bit-parallel arithmetic in-side phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration of all essential ANN functions such as multiply-accumulate (MAC), nonlinear activation, and pooling. We mapped four ANN benchmark applications on ODIN to compare its performance with a conventional processor-centric design and a crossbar-based in-situ ANN accelerator from prior work. The results of our analysis for the considered ANN topologies indicate that our ODIN accelerator can be at least 5.8x faster and 23.2x more energy-efficient, and up to 90.8x faster and 1554x more energy-efficient, compared to the crossbar-based in-situ ANN accelerator from prior work.

</p>
</details>

<details><summary><b>OperA: Attention-Regularized Transformers for Surgical Phase Recognition</b>
<a href="https://arxiv.org/abs/2103.03873">arxiv:2103.03873</a>
&#x1F4C8; 5 <br>
<p>Tobias Czempiel, Magdalini Paschali, Daniel Ostler, Seong Tae Kim, Benjamin Busam, Nassir Navab</p></summary>
<p>

**Abstract:** In this paper we introduce OperA, a transformer-based model that accurately predicts surgical phases from long video sequences. A novel attention regularization loss encourages the model to focus on high-quality frames during training. Moreover, the attention weights are utilized to identify characteristic high attention frames for each surgical phase, which could further be used for surgery summarization. OperA is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos, outperforming various state-of-the-art temporal refinement approaches.

</p>
</details>

<details><summary><b>Distributed Dynamic Map Fusion via Federated Learning for Intelligent Networked Vehicles</b>
<a href="https://arxiv.org/abs/2103.03786">arxiv:2103.03786</a>
&#x1F4C8; 5 <br>
<p>Zijian Zhang, Shuai Wang, Yuncong Hong, Liangkai Zhou, Qi Hao</p></summary>
<p>

**Abstract:** The technology of dynamic map fusion among networked vehicles has been developed to enlarge sensing ranges and improve sensing accuracies for individual vehicles. This paper proposes a federated learning (FL) based dynamic map fusion framework to achieve high map quality despite unknown numbers of objects in fields of view (FoVs), various sensing and model uncertainties, and missing data labels for online learning. The novelty of this work is threefold: (1) developing a three-stage fusion scheme to predict the number of objects effectively and to fuse multiple local maps with fidelity scores; (2) developing an FL algorithm which fine-tunes feature models (i.e., representation learning networks for feature extraction) distributively by aggregating model parameters; (3) developing a knowledge distillation method to generate FL training labels when data labels are unavailable. The proposed framework is implemented in the Car Learning to Act (CARLA) simulation platform. Extensive experimental results are provided to verify the superior performance and robustness of the developed map fusion and FL schemes.

</p>
</details>

<details><summary><b>Foundations of Population-Based SHM, Part IV: The Geometry of Spaces of Structures and their Feature Spaces</b>
<a href="https://arxiv.org/abs/2103.03655">arxiv:2103.03655</a>
&#x1F4C8; 5 <br>
<p>George Tsialiamanis, Charilaos Mylonas, Eleni Chatzi, Nikolaos Dervilis, David J. Wagg, Keith Worden</p></summary>
<p>

**Abstract:** One of the requirements of the population-based approach to Structural Health Monitoring (SHM) proposed in the earlier papers in this sequence, is that structures be represented by points in an abstract space. Furthermore, these spaces should be metric spaces in a loose sense; i.e. there should be some measure of distance applicable to pairs of points; similar structures should then be close in the metric. However, this geometrical construction is not enough for the framing of problems in data-based SHM, as it leaves undefined the notion of feature spaces. Interpreting the feature values on a structure-by-structure basis as a type of field over the space of structures, it seems sensible to borrow an idea from modern theoretical physics, and define feature assignments as sections in a vector bundle over the structure space. With this idea in place, one can interpret the effect of environmental and operational variations as gauge degrees of freedom, as in modern gauge field theories. This paper will discuss the various geometrical structures required for an abstract theory of feature spaces in SHM, and will draw analogies with how these structures have shown their power in modern physics. In the second part of the paper, the problem of determining the normal condition cross section of a feature bundle is addressed. The solution is provided by the application of Graph Neural Networks (GNN), a versatile non-Euclidean machine learning algorithm which is not restricted to inputs and outputs from vector spaces. In particular, the algorithm is well suited to operating directly on the sort of graph structures which are an important part of the proposed framework for PBSHM. The solution of the normal section problem is demonstrated for a heterogeneous population of truss structures for which the feature of interest is the first natural frequency.

</p>
</details>

<details><summary><b>Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning</b>
<a href="https://arxiv.org/abs/2103.03635">arxiv:2103.03635</a>
&#x1F4C8; 5 <br>
<p>Michel Denuit, Arthur Charpentier, Julien Trufin</p></summary>
<p>

**Abstract:** Boosting techniques and neural networks are particularly effective machine learning methods for insurance pricing. Often in practice, there are nevertheless endless debates about the choice of the right loss function to be used to train the machine learning model, as well as about the appropriate metric to assess the performances of competing models. Also, the sum of fitted values can depart from the observed totals to a large extent and this often confuses actuarial analysts. The lack of balance inherent to training models by minimizing deviance outside the familiar GLM with canonical link setting has been empirically documented in Wüthrich (2019, 2020) who attributes it to the early stopping rule in gradient descent methods for model fitting. The present paper aims to further study this phenomenon when learning proceeds by minimizing Tweedie deviance. It is shown that minimizing deviance involves a trade-off between the integral of weighted differences of lower partial moments and the bias measured on a specific scale. Autocalibration is then proposed as a remedy. This new method to correct for bias adds an extra local GLM step to the analysis. Theoretically, it is shown that it implements the autocalibration concept in pure premium calculation and ensures that balance also holds on a local scale, not only at portfolio level as with existing bias-correction techniques. The convex order appears to be the natural tool to compare competing models, putting a new light on the diagnostic graphs and associated metrics proposed by Denuit et al. (2019).

</p>
</details>

<details><summary><b>WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings</b>
<a href="https://arxiv.org/abs/2103.03598">arxiv:2103.03598</a>
&#x1F4C8; 5 <br>
<p>Bhavya Ghai, Md Naimul Hoque, Klaus Mueller</p></summary>
<p>

**Abstract:** Intersectional bias is a bias caused by an overlap of multiple social factors like gender, sexuality, race, disability, religion, etc. A recent study has shown that word embedding models can be laden with biases against intersectional groups like African American females, etc. The first step towards tackling such intersectional biases is to identify them. However, discovering biases against different intersectional groups remains a challenging task. In this work, we present WordBias, an interactive visual tool designed to explore biases against intersectional groups encoded in static word embeddings. Given a pretrained static word embedding, WordBias computes the association of each word along different groups based on race, age, etc. and then visualizes them using a novel interactive interface. Using a case study, we demonstrate how WordBias can help uncover biases against intersectional groups like Black Muslim Males, Poor Females, etc. encoded in word embedding. In addition, we also evaluate our tool using qualitative feedback from expert interviews. The source code for this tool can be publicly accessed for reproducibility at github.com/bhavyaghai/WordBias.

</p>
</details>

<details><summary><b>Can Pretext-Based Self-Supervised Learning Be Boosted by Downstream Data? A Theoretical Analysis</b>
<a href="https://arxiv.org/abs/2103.03568">arxiv:2103.03568</a>
&#x1F4C8; 5 <br>
<p>Jiaye Teng, Weiran Huang, Haowei He</p></summary>
<p>

**Abstract:** Pretext-based self-supervised learning learns the semantic representation via a handcrafted pretext task over unlabeled data and then uses the learned representation for downstream tasks, which effectively reduces the sample complexity of downstream tasks under Conditional Independence (CI) condition. However, the downstream sample complexity gets much worse if the CI condition does not hold. One interesting question is whether we can make the CI condition hold by using downstream data to refine the unlabeled data to boost self-supervised learning. At first glance, one might think that seeing downstream data in advance would always boost the downstream performance. However, we show that it is not intuitively true and point out that in some cases, it hurts the final performance instead. In particular, we prove both model-free and model-dependent lower bounds of the number of downstream samples used for data refinement. Moreover, we conduct several experiments on both synthetic and real-world datasets to verify our theoretical results.

</p>
</details>

<details><summary><b>Deep Generative Pattern-Set Mixture Models for Nonignorable Missingness</b>
<a href="https://arxiv.org/abs/2103.03532">arxiv:2103.03532</a>
&#x1F4C8; 5 <br>
<p>Sahra Ghalebikesabi, Rob Cornish, Luke J. Kelly, Chris Holmes</p></summary>
<p>

**Abstract:** We propose a variational autoencoder architecture to model both ignorable and nonignorable missing data using pattern-set mixtures as proposed by Little (1993). Our model explicitly learns to cluster the missing data into missingness pattern sets based on the observed data and missingness masks. Underpinning our approach is the assumption that the data distribution under missingness is probabilistically semi-supervised by samples from the observed data distribution. Our setup trades off the characteristics of ignorable and nonignorable missingness and can thus be applied to data of both types. We evaluate our method on a wide range of data sets with different types of missingness and achieve state-of-the-art imputation performance. Our model outperforms many common imputation algorithms, especially when the amount of missing data is high and the missingness mechanism is nonignorable.

</p>
</details>

<details><summary><b>Entangled q-Convolutional Neural Nets</b>
<a href="https://arxiv.org/abs/2103.11785">arxiv:2103.11785</a>
&#x1F4C8; 4 <br>
<p>Vassilis Anagiannis, Miranda C. N. Cheng</p></summary>
<p>

**Abstract:** We introduce a machine learning model, the q-CNN model, sharing key features with convolutional neural networks and admitting a tensor network description. As examples, we apply q-CNN to the MNIST and Fashion MNIST classification tasks. We explain how the network associates a quantum state to each classification label, and study the entanglement structure of these network states. In both our experiments on the MNIST and Fashion-MNIST datasets, we observe a distinct increase in both the left/right as well as the up/down bipartition entanglement entropy during training as the network learns the fine features of the data. More generally, we observe a universal negative correlation between the value of the entanglement entropy and the value of the cost function, suggesting that the network needs to learn the entanglement structure in order the perform the task accurately. This supports the possibility of exploiting the entanglement structure as a guide to design the machine learning algorithm suitable for given tasks.

</p>
</details>

<details><summary><b>Deep reinforcement learning in medical imaging: A literature review</b>
<a href="https://arxiv.org/abs/2103.05115">arxiv:2103.05115</a>
&#x1F4C8; 4 <br>
<p>S. Kevin Zhou, Hoang Ngan Le, Khoa Luu, Hien V. Nguyen, Nicholas Ayache</p></summary>
<p>

**Abstract:** Deep reinforcement learning (DRL) augments the reinforcement learning framework, which learns a sequence of actions that maximizes the expected reward, with the representative power of deep neural networks. Recent works have demonstrated the great potential of DRL in medicine and healthcare. This paper presents a literature review of DRL in medical imaging. We start with a comprehensive tutorial of DRL, including the latest model-free and model-based algorithms. We then cover existing DRL applications for medical imaging, which are roughly divided into three main categories: (I) parametric medical image analysis tasks including landmark detection, object/lesion detection, registration, and view plane localization; (ii) solving optimization tasks including hyperparameter tuning, selecting augmentation strategies, and neural architecture search; and (iii) miscellaneous applications including surgical gesture segmentation, personalized mobile health intervention, and computational model personalization. The paper concludes with discussions of future perspectives.

</p>
</details>

<details><summary><b>Fine-Grained Complexity and Algorithms for the Schulze Voting Method</b>
<a href="https://arxiv.org/abs/2103.03959">arxiv:2103.03959</a>
&#x1F4C8; 4 <br>
<p>Krzysztof Sornat, Virginia Vassilevska Williams, Yinzhan Xu</p></summary>
<p>

**Abstract:** We study computational aspects of a well-known single-winner voting rule called the Schulze method [Schulze, 2003] which is used broadly in practice. In this method the voters give (weak) ordinal preference ballots which are used to define the weighted majority graph (WMG) of direct comparisons between pairs of candidates. The choice of the winner comes from indirect comparisons in the graph, and more specifically from considering directed paths instead of direct comparisons between candidates.
  When the input is the WMG, to our knowledge, the fastest algorithm for computing all winners in the Schulze method uses a folklore reduction to the All-Pairs Bottleneck Paths problem and runs in $O(m^{2.69})$ time, where $m$ is the number of candidates. It is an interesting open question whether this can be improved. Our first result is a combinatorial algorithm with a nearly quadratic running time for computing all winners. This running time is essentially optimal. If the input to the Schulze winners problem is not the WMG but the preference profile, then constructing the WMG is a bottleneck that increases the running time significantly; in the special case when there are $m$ candidates and $n=O(m)$ voters, the running time is $O(m^{2.69})$, or $O(m^{2.5})$ if there is a nearly-linear time algorithm for multiplying dense square matrices. To address this bottleneck, we prove a formal equivalence between the well-studied Dominance Product problem and the problem of computing the WMG. We prove a similar connection between the so called Dominating Pairs problem and the problem of finding a winner in the Schulze method.
  Our paper is the first to bring fine-grained complexity into the field of computational social choice. Using it we can identify voting protocols that are unlikely to be practical for large numbers of candidates and/or voters, as their complexity is likely, say at least cubic.

</p>
</details>

<details><summary><b>Contrastive Disentanglement in Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2103.03636">arxiv:2103.03636</a>
&#x1F4C8; 4 <br>
<p>Lili Pan, Peijun Tang, Zhiyong Chen, Zenglin Xu</p></summary>
<p>

**Abstract:** Disentanglement is defined as the problem of learninga representation that can separate the distinct, informativefactors of variations of data. Learning such a representa-tion may be critical for developing explainable and human-controllable Deep Generative Models (DGMs) in artificialintelligence. However, disentanglement in GANs is not a triv-ial task, as the absence of sample likelihood and posteriorinference for latent variables seems to prohibit the forwardstep. Inspired by contrastive learning (CL), this paper, froma new perspective, proposes contrastive disentanglement ingenerative adversarial networks (CD-GAN). It aims at dis-entangling the factors of inter-class variation of visual datathrough contrasting image features, since the same factorvalues produce images in the same class. More importantly,we probe a novel way to make use of limited amount ofsupervision to the largest extent, to promote inter-class dis-entanglement performance. Extensive experimental resultson many well-known datasets demonstrate the efficacy ofCD-GAN for disentangling inter-class variation.

</p>
</details>

<details><summary><b>Use of Transfer Learning and Wavelet Transform for Breast Cancer Detection</b>
<a href="https://arxiv.org/abs/2103.03602">arxiv:2103.03602</a>
&#x1F4C8; 4 <br>
<p>Ahmed Rasheed, Muhammad Shahzad Younis, Junaid Qadir, Muhammad Bilal</p></summary>
<p>

**Abstract:** Breast cancer is one of the most common cause of deaths among women. Mammography is a widely used imaging modality that can be used for cancer detection in its early stages. Deep learning is widely used for the detection of cancerous masses in the images obtained via mammography. The need to improve accuracy remains constant due to the sensitive nature of the datasets so we introduce segmentation and wavelet transform to enhance the important features in the image scans. Our proposed system aids the radiologist in the screening phase of cancer detection by using a combination of segmentation and wavelet transforms as pre-processing augmentation that leads to transfer learning in neural networks. The proposed system with these pre-processing techniques significantly increases the accuracy of detection on Mini-MIAS.

</p>
</details>

<details><summary><b>NPT-Loss: A Metric Loss with Implicit Mining for Face Recognition</b>
<a href="https://arxiv.org/abs/2103.03503">arxiv:2103.03503</a>
&#x1F4C8; 4 <br>
<p>Syed Safwan Khalid, Muhammad Awais, Chi-Ho Chan, Zhenhua Feng, Ammarah Farooq, Ali Akbari, Josef Kittler</p></summary>
<p>

**Abstract:** Face recognition (FR) using deep convolutional neural networks (DCNNs) has seen remarkable success in recent years. One key ingredient of DCNN-based FR is the appropriate design of a loss function that ensures discrimination between various identities. The state-of-the-art (SOTA) solutions utilise normalised Softmax loss with additive and/or multiplicative margins. Despite being popular, these Softmax+margin based losses are not theoretically motivated and the effectiveness of a margin is justified only intuitively. In this work, we utilise an alternative framework that offers a more direct mechanism of achieving discrimination among the features of various identities. We propose a novel loss that is equivalent to a triplet loss with proxies and an implicit mechanism of hard-negative mining. We give theoretical justification that minimising the proposed loss ensures a minimum separability between all identities. The proposed loss is simple to implement and does not require heavy hyper-parameter tuning as in the SOTA solutions. We give empirical evidence that despite its simplicity, the proposed loss consistently achieves SOTA performance in various benchmarks for both high-resolution and low-resolution FR tasks.

</p>
</details>

<details><summary><b>NeRD: Neural Representation of Distribution for Medical Image Segmentation</b>
<a href="https://arxiv.org/abs/2103.04020">arxiv:2103.04020</a>
&#x1F4C8; 3 <br>
<p>Hang Zhang, Rongguang Wang, Jinwei Zhang, Chao Li, Gufeng Yang, Pascal Spincemaille, Thanh Nguyen, Yi Wang</p></summary>
<p>

**Abstract:** We introduce Neural Representation of Distribution (NeRD) technique, a module for convolutional neural networks (CNNs) that can estimate the feature distribution by optimizing an underlying function mapping image coordinates to the feature distribution. Using NeRD, we propose an end-to-end deep learning model for medical image segmentation that can compensate the negative impact of feature distribution shifting issue caused by commonly used network operations such as padding and pooling. An implicit function is used to represent the parameter space of the feature distribution by querying the image coordinate. With NeRD, the impact of issues such as over-segmenting and missing have been reduced, and experimental results on the challenging white matter lesion segmentation and left atrial segmentation verify the effectiveness of the proposed method. The code is available via https://github.com/tinymilky/NeRD.

</p>
</details>

<details><summary><b>Indoor Future Person Localization from an Egocentric Wearable Camera</b>
<a href="https://arxiv.org/abs/2103.04019">arxiv:2103.04019</a>
&#x1F4C8; 3 <br>
<p>Jianing Qiu, Frank P. -W. Lo, Xiao Gu, Yingnan Sun, Shuo Jiang, Benny Lo</p></summary>
<p>

**Abstract:** Accurate prediction of future person location and movement trajectory from an egocentric wearable camera can benefit a wide range of applications, such as assisting visually impaired people in navigation, and the development of mobility assistance for people with disability. In this work, a new egocentric dataset was constructed using a wearable camera, with 8,250 short clips of a targeted person either walking 1) toward, 2) away, or 3) across the camera wearer in indoor environments, or 4) staying still in the scene, and 13,817 person bounding boxes were manually labelled. Apart from the bounding boxes, the dataset also contains the estimated pose of the targeted person as well as the IMU signal of the wearable camera at each time point. An LSTM-based encoder-decoder framework was designed to predict the future location and movement trajectory of the targeted person in this egocentric setting. Extensive experiments have been conducted on the new dataset, and have shown that the proposed method is able to reliably and better predict future person location and trajectory in egocentric videos captured by the wearable camera compared to three baselines.

</p>
</details>

<details><summary><b>Fibrosis-Net: A Tailored Deep Convolutional Neural Network Design for Prediction of Pulmonary Fibrosis Progression from Chest CT Images</b>
<a href="https://arxiv.org/abs/2103.04008">arxiv:2103.04008</a>
&#x1F4C8; 3 <br>
<p>Alexander Wong, Jack Lu, Adam Dorfman, Paul McInnis, Mahmoud Famouri, Daniel Manary, James Ren Hou Lee, Michael Lynch</p></summary>
<p>

**Abstract:** Pulmonary fibrosis is a devastating chronic lung disease that causes irreparable lung tissue scarring and damage, resulting in progressive loss in lung capacity and has no known cure. A critical step in the treatment and management of pulmonary fibrosis is the assessment of lung function decline, with computed tomography (CT) imaging being a particularly effective method for determining the extent of lung damage caused by pulmonary fibrosis. Motivated by this, we introduce Fibrosis-Net, a deep convolutional neural network design tailored for the prediction of pulmonary fibrosis progression from chest CT images. More specifically, machine-driven design exploration was leveraged to determine a strong architectural design for CT lung analysis, upon which we build a customized network design tailored for predicting forced vital capacity (FVC) based on a patient's CT scan, initial spirometry measurement, and clinical metadata. Finally, we leverage an explainability-driven performance validation strategy to study the decision-making behaviour of Fibrosis-Net as to verify that predictions are based on relevant visual indicators in CT images. Experiments using a patient cohort from the OSIC Pulmonary Fibrosis Progression Challenge showed that the proposed Fibrosis-Net is able to achieve a significantly higher modified Laplace Log Likelihood score than the winning solutions on the challenge. Furthermore, explainability-driven performance validation demonstrated that the proposed Fibrosis-Net exhibits correct decision-making behaviour by leveraging clinically-relevant visual indicators in CT images when making predictions on pulmonary fibrosis progress. While Fibrosis-Net is not yet a production-ready clinical assessment solution, we hope that its release in open source manner will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon it.

</p>
</details>

<details><summary><b>Memory-efficient Learning for High-Dimensional MRI Reconstruction</b>
<a href="https://arxiv.org/abs/2103.04003">arxiv:2103.04003</a>
&#x1F4C8; 3 <br>
<p>Ke Wang, Michael Kellman, Christopher M. Sandino, Kevin Zhang, Shreyas S. Vasanawala, Jonathan I. Tamir, Stella X. Yu, Michael Lustig</p></summary>
<p>

**Abstract:** Deep learning (DL) based unrolled reconstructions have shown state-of-the-art performance for under-sampled magnetic resonance imaging (MRI). Similar to compressed sensing, DL can leverage high-dimensional data (e.g. 3D, 2D+time, 3D+time) to further improve performance. However, network size and depth are currently limited by the GPU memory required for backpropagation. Here we use a memory-efficient learning (MEL) framework which favorably trades off storage with a manageable increase in computation during training. Using MEL with multi-dimensional data, we demonstrate improved image reconstruction performance for in-vivo 3D MRI and 2D+time cardiac cine MRI. MEL uses far less GPU memory while marginally increasing the training time, which enables new applications of DL to high-dimensional MRI.

</p>
</details>

<details><summary><b>Loss Estimators Improve Model Generalization</b>
<a href="https://arxiv.org/abs/2103.03788">arxiv:2103.03788</a>
&#x1F4C8; 3 <br>
<p>Vivek Narayanaswamy, Jayaraman J. Thiagarajan, Deepta Rajan, Andreas Spanias</p></summary>
<p>

**Abstract:** With increased interest in adopting AI methods for clinical diagnosis, a vital step towards safe deployment of such tools is to ensure that the models not only produce accurate predictions but also do not generalize to data regimes where the training data provide no meaningful evidence. Existing approaches for ensuring the distribution of model predictions to be similar to that of the true distribution rely on explicit uncertainty estimators that are inherently hard to calibrate. In this paper, we propose to train a loss estimator alongside the predictive model, using a contrastive training objective, to directly estimate the prediction uncertainties. Interestingly, we find that, in addition to producing well-calibrated uncertainties, this approach improves the generalization behavior of the predictor. Using a dermatology use-case, we show the impact of loss estimators on model generalization, in terms of both its fidelity on in-distribution data and its ability to detect out of distribution samples or new classes unseen during training.

</p>
</details>

<details><summary><b>FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation</b>
<a href="https://arxiv.org/abs/2103.03705">arxiv:2103.03705</a>
&#x1F4C8; 3 <br>
<p>Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, Shadi Albarqouni</p></summary>
<p>

**Abstract:** In recent years, data-driven machine learning (ML) methods have revolutionized the computer vision community by providing novel efficient solutions to many unsolved (medical) image analysis problems. However, due to the increasing privacy concerns and data fragmentation on many different sites, existing medical data are not fully utilized, thus limiting the potential of ML. Federated learning (FL) enables multiple parties to collaboratively train a ML model without exchanging local data. However, data heterogeneity (non-IID) among the distributed clients is yet a challenge. To this end, we propose a novel federated method, denoted Federated Disentanglement (FedDis), to disentangle the parameter space into shape and appearance, and only share the shape parameter with the clients. FedDis is based on the assumption that the anatomical structure in brain MRI images is similar across multiple institutions, and sharing the shape knowledge would be beneficial in anomaly detection. In this paper, we leverage healthy brain scans of 623 subjects from multiple sites with real data (OASIS, ADNI) in a privacy-preserving fashion to learn a model of normal anatomy, that allows to segment abnormal structures. We demonstrate a superior performance of FedDis on real pathological databases containing 109 subjects; two publicly available MS Lesions (MSLUB, MSISBI), and an in-house database with MS and Glioblastoma (MSI and GBI). FedDis achieved an average dice performance of 0.38, outperforming the state-of-the-art (SOTA) auto-encoder by 42% and the SOTA federated method by 11%. Further, we illustrate that FedDis learns a shape embedding that is orthogonal to the appearance and consistent under different intensity augmentations.

</p>
</details>

<details><summary><b>Don't Forget to Sign the Gradients!</b>
<a href="https://arxiv.org/abs/2103.03701">arxiv:2103.03701</a>
&#x1F4C8; 3 <br>
<p>Omid Aramoon, Pin-Yu Chen, Gang Qu</p></summary>
<p>

**Abstract:** Engineering a top-notch deep learning model is an expensive procedure that involves collecting data, hiring human resources with expertise in machine learning, and providing high computational resources. For that reason, deep learning models are considered as valuable Intellectual Properties (IPs) of the model vendors. To ensure reliable commercialization of deep learning models, it is crucial to develop techniques to protect model vendors against IP infringements. One of such techniques that recently has shown great promise is digital watermarking. However, current watermarking approaches can embed very limited amount of information and are vulnerable against watermark removal attacks. In this paper, we present GradSigns, a novel watermarking framework for deep neural networks (DNNs). GradSigns embeds the owner's signature into the gradient of the cross-entropy cost function with respect to inputs to the model. Our approach has a negligible impact on the performance of the protected model and it allows model vendors to remotely verify the watermark through prediction APIs. We evaluate GradSigns on DNNs trained for different image classification tasks using CIFAR-10, SVHN, and YTF datasets. Experimental results show that GradSigns is robust against all known counter-watermark attacks and can embed a large amount of information into DNNs.

</p>
</details>

<details><summary><b>Automatic Exploration Process Adjustment for Safe Reinforcement Learning with Joint Chance Constraint Satisfaction</b>
<a href="https://arxiv.org/abs/2103.03656">arxiv:2103.03656</a>
&#x1F4C8; 3 <br>
<p>Yoshihiro Okawa, Tomotake Sasaki, Hidenao Iwane</p></summary>
<p>

**Abstract:** In reinforcement learning (RL) algorithms, exploratory control inputs are used during learning to acquire knowledge for decision making and control, while the true dynamics of a controlled object is unknown. However, this exploring property sometimes causes undesired situations by violating constraints regarding the state of the controlled object. In this paper, we propose an automatic exploration process adjustment method for safe RL in continuous state and action spaces utilizing a linear nominal model of the controlled object. Specifically, our proposed method automatically selects whether the exploratory input is used or not at each time depending on the state and its predicted value as well as adjusts the variance-covariance matrix used in the Gaussian policy for exploration. We also show that our exploration process adjustment method theoretically guarantees the satisfaction of the constraints with the pre-specified probability, that is, the satisfaction of a joint chance constraint at every time. Finally, we illustrate the validity and the effectiveness of our method through numerical simulation.

</p>
</details>

<details><summary><b>FloMo: Tractable Motion Prediction with Normalizing Flows</b>
<a href="https://arxiv.org/abs/2103.03614">arxiv:2103.03614</a>
&#x1F4C8; 3 <br>
<p>Christoph Schöller, Alois Knoll</p></summary>
<p>

**Abstract:** The future motion of traffic participants is inherently uncertain. To plan safely, therefore, an autonomous agent must take into account multiple possible trajectory outcomes and prioritize them. Recently, this problem has been addressed with generative neural networks. However, most generative models either do not learn the true underlying trajectory distribution reliably, or do not allow predictions to be associated with likelihoods. In our work, we model motion prediction directly as a density estimation problem with a normalizing flow between a noise distribution and the future motion distribution. Our model, named FloMo, allows likelihoods to be computed in a single network pass and can be trained directly with maximum likelihood estimation. Furthermore, we propose a method to stabilize training flows on trajectory datasets and a new data augmentation transformation that improves the performance and generalization of our model. Our method achieves state-of-the-art performance on three popular prediction datasets, with a significant gap to most competing models.

</p>
</details>

<details><summary><b>SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation</b>
<a href="https://arxiv.org/abs/2103.03604">arxiv:2103.03604</a>
&#x1F4C8; 3 <br>
<p>Boxiang Yun, Yan Wang, Jieneng Chen, Huiyu Wang, Wei Shen, Qingli Li</p></summary>
<p>

**Abstract:** Hyperspectral imaging (HSI) unlocks the huge potential to a wide variety of applications relied on high-precision pathology image segmentation, such as computational pathology and precision medicine. Since hyperspectral pathology images benefit from the rich and detailed spectral information even beyond the visible spectrum, the key to achieve high-precision hyperspectral pathology image segmentation is to felicitously model the context along high-dimensional spectral bands. Inspired by the strong context modeling ability of transformers, we hereby, for the first time, formulate the contextual feature learning across spectral bands for hyperspectral pathology image segmentation as a sequence-to-sequence prediction procedure by transformers. To assist spectral context learning procedure, we introduce two important strategies: (1) a sparsity scheme enforces the learned contextual relationship to be sparse, so as to eliminates the distraction from the redundant bands; (2) a spectral normalization, a separate group normalization for each spectral band, mitigates the nuisance caused by heterogeneous underlying distributions of bands. We name our method Spectral Transformer (SpecTr), which enjoys two benefits: (1) it has a strong ability to model long-range dependency among spectral bands, and (2) it jointly explores the spatial-spectral features of HSI. Experiments show that SpecTr outperforms other competing methods in a hyperspectral pathology image segmentation benchmark without the need of pre-training. Code is available at https://github.com/hfut-xc-yun/SpecTr.

</p>
</details>

<details><summary><b>Cycle Self-Training for Domain Adaptation</b>
<a href="https://arxiv.org/abs/2103.03571">arxiv:2103.03571</a>
&#x1F4C8; 3 <br>
<p>Hong Liu, Jianmin Wang, Mingsheng Long</p></summary>
<p>

**Abstract:** Mainstream approaches for unsupervised domain adaptation (UDA) learn domain-invariant representations to narrow the domain shift. Recently, self-training has been gaining momentum in UDA, which exploits unlabeled target data by training with target pseudo-labels. However, as corroborated in this work, under distributional shift in UDA, the pseudo-labels can be unreliable in terms of their large discrepancy from target ground truth. Thereby, we propose Cycle Self-Training (CST), a principled self-training algorithm that explicitly enforces pseudo-labels to generalize across domains. CST cycles between a forward step and a reverse step until convergence. In the forward step, CST generates target pseudo-labels with a source-trained classifier. In the reverse step, CST trains a target classifier using target pseudo-labels, and then updates the shared representations to make the target classifier perform well on the source data. We introduce the Tsallis entropy as a confidence-friendly regularization to improve the quality of target pseudo-labels. We analyze CST theoretically under realistic assumptions, and provide hard cases where CST recovers target ground truth, while both invariant feature learning and vanilla self-training fail. Empirical results indicate that CST significantly improves over the state-of-the-arts on visual recognition and sentiment analysis benchmarks.

</p>
</details>

<details><summary><b>Meta Learning Black-Box Population-Based Optimizers</b>
<a href="https://arxiv.org/abs/2103.03526">arxiv:2103.03526</a>
&#x1F4C8; 3 <br>
<p>Hugo Siqueira Gomes, Benjamin Léger, Christian Gagné</p></summary>
<p>

**Abstract:** The no free lunch theorem states that no model is better suited to every problem. A question that arises from this is how to design methods that propose optimizers tailored to specific problems achieving state-of-the-art performance. This paper addresses this issue by proposing the use of meta-learning to infer population-based black-box optimizers that can automatically adapt to specific classes of problems. We suggest a general modeling of population-based algorithms that result in Learning-to-Optimize POMDP (LTO-POMDP), a meta-learning framework based on a specific partially observable Markov decision process (POMDP). From that framework's formulation, we propose to parameterize the algorithm using deep recurrent neural networks and use a meta-loss function based on stochastic algorithms' performance to train efficient data-driven optimizers over several related optimization tasks. The learned optimizers' performance based on this implementation is assessed on various black-box optimization tasks and hyperparameter tuning of machine learning models. Our results revealed that the meta-loss function encourages a learned algorithm to alter its search behavior so that it can easily fit into a new context. Thus, it allows better generalization and higher sample efficiency than state-of-the-art generic optimization algorithms, such as the Covariance matrix adaptation evolution strategy (CMA-ES).

</p>
</details>

<details><summary><b>Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network</b>
<a href="https://arxiv.org/abs/2103.03518">arxiv:2103.03518</a>
&#x1F4C8; 3 <br>
<p>Julen Balzategui, Luka Eciolaza, Daniel Maestro-Watson</p></summary>
<p>

**Abstract:** Quality inspection applications in industry are required to move towards a zero-defect manufacturing scenario, withnon-destructive inspection and traceability of 100 % of produced parts. Developing robust fault detection and classification modelsfrom the start-up of the lines is challenging due to the difficulty in getting enough representative samples of the faulty patternsand the need to manually label them. This work presents a methodology to develop a robust inspection system, targeting thesepeculiarities, in the context of solar cell manufacturing. The methodology is divided into two phases: In the first phase, an anomalydetection model based on a Generative Adversarial Network (GAN) is employed. This model enables the detection and localizationof anomalous patterns within the solar cells from the beginning, using only non-defective samples for training and without anymanual labeling involved. In a second stage, as defective samples arise, the detected anomalies will be used as automaticallygenerated annotations for the supervised training of a Fully Convolutional Network that is capable of detecting multiple types offaults. The experimental results using 1873 EL images of monocrystalline cells show that (a) the anomaly detection scheme can beused to start detecting features with very little available data, (b) the anomaly detection may serve as automatic labeling in order totrain a supervised model, and (c) segmentation and classification results of supervised models trained with automatic labels arecomparable to the ones obtained from the models trained with manual labels.

</p>
</details>

<details><summary><b>labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds</b>
<a href="https://arxiv.org/abs/2103.04970">arxiv:2103.04970</a>
&#x1F4C8; 2 <br>
<p>Christoph Sager, Patrick Zschech, Niklas Kühl</p></summary>
<p>

**Abstract:** Within the past decade, the rise of applications based on artificial intelligence (AI) in general and machine learning (ML) in specific has led to many significant contributions within different domains. The applications range from robotics over medical diagnoses up to autonomous driving. However, nearly all applications rely on trained data. In case this data consists of 3D images, it is of utmost importance that the labeling is as accurate as possible to ensure high-quality outcomes of the ML models. Labeling in the 3D space is mostly manual work performed by expert workers, where they draw 3D bounding boxes around target objects the ML model should later automatically identify, e.g., pedestrians for autonomous driving or cancer cells within radiography.
  While a small range of recent 3D labeling tools exist, they all share three major shortcomings: (i) they are specified for autonomous driving applications, (ii) they lack convenience and comfort functions, and (iii) they have high dependencies and little flexibility in data format. Therefore, we propose a novel labeling tool for 3D object detection in point clouds to address these shortcomings.

</p>
</details>

<details><summary><b>District Wise Price Forecasting of Wheat in Pakistan using Deep Learning</b>
<a href="https://arxiv.org/abs/2103.04781">arxiv:2103.04781</a>
&#x1F4C8; 2 <br>
<p>Ahmed Rasheed, Muhammad Shahzad Younis, Farooq Ahmad, Junaid Qadir, Muhammad Kashif</p></summary>
<p>

**Abstract:** Wheat is the main agricultural crop of Pakistan and is a staple food requirement of almost every Pakistani household making it the main strategic commodity of the country whose availability and affordability is the government's main priority. Wheat food availability can be vastly affected by multiple factors included but not limited to the production, consumption, financial crisis, inflation, or volatile market. The government ensures food security by particular policy and monitory arrangements, which keeps up purchase parity for the poor. Such arrangements can be made more effective if a dynamic analysis is carried out to estimate the future yield based on certain current factors. Future planning of commodity pricing is achievable by forecasting their future price anticipated by the current circumstances. This paper presents a wheat price forecasting methodology, which uses the price, weather, production, and consumption trends for wheat prices taken over the past few years and analyzes them with the help of advance neural networks architecture Long Short Term Memory (LSTM) networks. The proposed methodology presented significantly improved results versus other conventional machine learning and statistical time series analysis methods.

</p>
</details>

<details><summary><b>Morphological Operation Residual Blocks: Enhancing 3D Morphological Feature Representation in Convolutional Neural Networks for Semantic Segmentation of Medical Images</b>
<a href="https://arxiv.org/abs/2103.04026">arxiv:2103.04026</a>
&#x1F4C8; 2 <br>
<p>Chentian Li, Chi Ma, William W. Lu</p></summary>
<p>

**Abstract:** The shapes and morphology of the organs and tissues are important prior knowledge in medical imaging recognition and segmentation. The morphological operation is a well-known method for morphological feature extraction. As the morphological operation is performed well in hand-crafted image segmentation techniques, it is also promising to design an approach to approximate morphological operation in the convolutional networks. However, using the traditional convolutional neural network as a black-box is usually hard to specify the morphological operation action. Here, we introduced a 3D morphological operation residual block to extract morphological features in end-to-end deep learning models for semantic segmentation. This study proposed a novel network block architecture that embedded the morphological operation as an infinitely strong prior in the convolutional neural network. Several 3D deep learning models with the proposed morphological operation block were built and compared in different medical imaging segmentation tasks. Experimental results showed the proposed network achieved a relatively higher performance in the segmentation tasks comparing with the conventional approach. In conclusion, the novel network block could be easily embedded in traditional networks and efficiently reinforce the deep learning models for medical imaging segmentation.

</p>
</details>

<details><summary><b>Causal Reinforcement Learning: An Instrumental Variable Approach</b>
<a href="https://arxiv.org/abs/2103.04021">arxiv:2103.04021</a>
&#x1F4C8; 2 <br>
<p>Jin Li, Ye Luo, Xiaowei Zhang</p></summary>
<p>

**Abstract:** In the standard data analysis framework, data is first collected (once for all), and then data analysis is carried out. With the advancement of digital technology, decisionmakers constantly analyze past data and generate new data through the decisions they make. In this paper, we model this as a Markov decision process and show that the dynamic interaction between data generation and data analysis leads to a new type of bias -- reinforcement bias -- that exacerbates the endogeneity problem in standard data analysis.
  We propose a class of instrument variable (IV)-based reinforcement learning (RL) algorithms to correct for the bias and establish their asymptotic properties by incorporating them into a two-timescale stochastic approximation framework. A key contribution of the paper is the development of new techniques that allow for the analysis of the algorithms in general settings where noises feature time-dependency.
  We use the techniques to derive sharper results on finite-time trajectory stability bounds: with a polynomial rate, the entire future trajectory of the iterates from the algorithm fall within a ball that is centered at the true parameter and is shrinking at a (different) polynomial rate. We also use the technique to provide formulas for inferences that are rarely done for RL algorithms. These formulas highlight how the strength of the IV and the degree of the noise's time dependency affect the inference.

</p>
</details>

<details><summary><b>Dynamic Resource Management for Providing QoS in Drone Delivery Systems</b>
<a href="https://arxiv.org/abs/2103.04015">arxiv:2103.04015</a>
&#x1F4C8; 2 <br>
<p>Behzad Khamidehi, Majid Raeis, Elvino S. Sousa</p></summary>
<p>

**Abstract:** Drones have been considered as an alternative means of package delivery to reduce the delivery cost and time. Due to the battery limitations, the drones are best suited for last-mile delivery, i.e., the delivery from the package distribution centers (PDCs) to the customers. Since a typical delivery system consists of multiple PDCs, each having random and time-varying demands, the dynamic drone-to-PDC allocation would be of great importance in meeting the demand in an efficient manner. In this paper, we study the dynamic UAV assignment problem for a drone delivery system with the goal of providing measurable Quality of Service (QoS) guarantees. We adopt a queueing theoretic approach to model the customer-service nature of the problem. Furthermore, we take a deep reinforcement learning approach to obtain a dynamic policy for the re-allocation of the UAVs. This policy guarantees a probabilistic upper-bound on the queue length of the packages waiting in each PDC, which is beneficial from both the service provider's and the customers' viewpoints. We evaluate the performance of our proposed algorithm by considering three broad arrival classes, including Bernoulli, Time-Varying Bernoulli, and Markov-Modulated Bernoulli arrivals. Our results show that the proposed method outperforms the baselines, particularly in scenarios with Time-Varying and Markov-Modulated Bernoulli arrivals, which are more representative of real-world demand patterns. Moreover, our algorithm satisfies the QoS constraints in all the studied scenarios while minimizing the average number of UAVs in use.

</p>
</details>

<details><summary><b>Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization</b>
<a href="https://arxiv.org/abs/2103.03995">arxiv:2103.03995</a>
&#x1F4C8; 2 <br>
<p>Wei-Chang Yeh, Yi-Ping Lin, Yun-Chia Liang, Chyh-Ming Lai</p></summary>
<p>

**Abstract:** Convolutional neural networks (CNNs) are widely used in image recognition. Numerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have been proposed by increasing the number of layers, to improve the performance of CNNs. However, performance deteriorates beyond a certain number of layers. Hence, hyperparameter optimisation is a more efficient way to improve CNNs. To validate this concept, a new algorithm based on simplified swarm optimisation is proposed to optimise the hyperparameters of the simplest CNN model, which is LeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and Cifar10 datasets showed that the accuracy of the proposed algorithm is higher than the original LeNet model and PSO-LeNet and that it has a high potential to be extended to more complicated models, such as AlexNet.

</p>
</details>

<details><summary><b>Passing Through Narrow Gaps with Deep Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2103.03991">arxiv:2103.03991</a>
&#x1F4C8; 2 <br>
<p>Brendan Tidd, Akansel Cosgun, Jurgen Leitner, Nicolas Hudson</p></summary>
<p>

**Abstract:** The U.S. Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge requires teams of robots to traverse difficult and diverse underground environments. Traversing small gaps is one of the challenging scenarios that robots encounter. Imperfect sensor information makes it difficult for classical navigation methods, where behaviours require significant manual fine tuning. In this paper we present a deep reinforcement learning method for autonomously navigating through small gaps, where contact between the robot and the gap may be required. We first learn a gap behaviour policy to get through small gaps (only centimeters wider than the robot). We then learn a goal-conditioned behaviour selection policy that determines when to activate the gap behaviour policy. We train our policies in simulation and demonstrate their effectiveness with a large tracked robot in simulation and on the real platform. In simulation experiments, our approach achieves 93\% success rate when the gap behaviour is activated manually by an operator, and 63\% with autonomous activation using the behaviour selection policy. In real robot experiments, our approach achieves a success rate of 73\% with manual activation, and 40\% with autonomous behaviour selection. While we show the feasibility of our approach in simulation, the difference in performance between simulated and real world scenarios highlight the difficulty of direct sim-to-real transfer for deep reinforcement learning policies. In both the simulated and real world environments alternative methods were unable to traverse the gap.

</p>
</details>

<details><summary><b>SCRIB: Set-classifier with Class-specific Risk Bounds for Blackbox Models</b>
<a href="https://arxiv.org/abs/2103.03945">arxiv:2103.03945</a>
&#x1F4C8; 2 <br>
<p>Zhen Lin, Cao Xiao, Lucas Glass, M. Brandon Westover, Jimeng Sun</p></summary>
<p>

**Abstract:** Despite deep learning (DL) success in classification problems, DL classifiers do not provide a sound mechanism to decide when to refrain from predicting. Recent works tried to control the overall prediction risk with classification with rejection options. However, existing works overlook the different significance of different classes. We introduce Set-classifier with Class-specific RIsk Bounds (SCRIB) to tackle this problem, assigning multiple labels to each example. Given the output of a black-box model on the validation set, SCRIB constructs a set-classifier that controls the class-specific prediction risks with a theoretical guarantee. The key idea is to reject when the set classifier returns more than one label. We validated SCRIB on several medical applications, including sleep staging on electroencephalogram (EEG) data, X-ray COVID image classification, and atrial fibrillation detection based on electrocardiogram (ECG) data. SCRIB obtained desirable class-specific risks, which are 35\%-88\% closer to the target risks than baseline methods.

</p>
</details>

<details><summary><b>FedV: Privacy-Preserving Federated Learning over Vertically Partitioned Data</b>
<a href="https://arxiv.org/abs/2103.03918">arxiv:2103.03918</a>
&#x1F4C8; 2 <br>
<p>Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, James Joshi, Heiko Ludwig</p></summary>
<p>

**Abstract:** Federated learning (FL) has been proposed to allow collaborative training of machine learning (ML) models among multiple parties where each party can keep its data private. In this paradigm, only model updates, such as model weights or gradients, are shared. Many existing approaches have focused on horizontal FL, where each party has the entire feature set and labels in the training data set. However, many real scenarios follow a vertically-partitioned FL setup, where a complete feature set is formed only when all the datasets from the parties are combined, and the labels are only available to a single party. Privacy-preserving vertical FL is challenging because complete sets of labels and features are not owned by one entity. Existing approaches for vertical FL require multiple peer-to-peer communications among parties, leading to lengthy training times, and are restricted to (approximated) linear models and just two parties. To close this gap, we propose FedV, a framework for secure gradient computation in vertical settings for several widely used ML models such as linear models, logistic regression, and support vector machines. FedV removes the need for peer-to-peer communication among parties by using functional encryption schemes; this allows FedV to achieve faster training times. It also works for larger and changing sets of parties. We empirically demonstrate the applicability for multiple types of ML models and show a reduction of 10%-70% of training time and 80% to 90% in data transfer with respect to the state-of-the-art approaches.

</p>
</details>

<details><summary><b>Multi-modal anticipation of stochastic trajectories in a dynamic environment with Conditional Variational Autoencoders</b>
<a href="https://arxiv.org/abs/2103.03912">arxiv:2103.03912</a>
&#x1F4C8; 2 <br>
<p>Albert Dulian, John C. Murray</p></summary>
<p>

**Abstract:** Forecasting short-term motion of nearby vehicles presents an inherently challenging issue as the space of their possible future movements is not strictly limited to a set of single trajectories. Recently proposed techniques that demonstrate plausible results concentrate primarily on forecasting a fixed number of deterministic predictions, or on classifying over a wide variety of trajectories that were previously generated using e.g. dynamic model. This paper focuses on addressing the uncertainty associated with the discussed task by utilising the stochastic nature of generative models in order to produce a diverse set of plausible paths with regards to tracked vehicles. More specifically, we propose to account for the multi-modality of the problem with use of Conditional Variational Autoencoder (C-VAE) conditioned on an agent's past motion as well as a rasterised scene context encoded with Capsule Network (CapsNet). In addition, we demonstrate advantages of employing the Minimum over N (MoN) cost function which measures the distance between ground truth and N generated samples and tries to minimise the loss with respect to the closest sample, effectively leading to more diverse predictions. We examine our network on a publicly available dataset against recent state-of-the-art methods and show that our approach outperforms these techniques in numerous scenarios whilst significantly reducing the number of trainable parameters as well as allowing to sample an arbitrary amount of diverse trajectories.

</p>
</details>

<details><summary><b>Decomposable Submodular Function Minimization via Maximum Flow</b>
<a href="https://arxiv.org/abs/2103.03868">arxiv:2103.03868</a>
&#x1F4C8; 2 <br>
<p>Kyriakos Axiotis, Adam Karczmarz, Anish Mukherjee, Piotr Sankowski, Adrian Vladu</p></summary>
<p>

**Abstract:** This paper bridges discrete and continuous optimization approaches for decomposable submodular function minimization, in both the standard and parametric settings.
  We provide improved running times for this problem by reducing it to a number of calls to a maximum flow oracle. When each function in the decomposition acts on $O(1)$ elements of the ground set $V$ and is polynomially bounded, our running time is up to polylogarithmic factors equal to that of solving maximum flow in a sparse graph with $O(\vert V \vert)$ vertices and polynomial integral capacities.
  We achieve this by providing a simple iterative method which can optimize to high precision any convex function defined on the submodular base polytope, provided we can efficiently minimize it on the base polytope corresponding to the cut function of a certain graph that we construct. We solve this minimization problem by lifting the solutions of a parametric cut problem, which we obtain via a new efficient combinatorial reduction to maximum flow. This reduction is of independent interest and implies some previously unknown bounds for the parametric minimum $s,t$-cut problem in multiple settings.

</p>
</details>

<details><summary><b>Multi-Session Visual SLAM for Illumination Invariant Localization in Indoor Environments</b>
<a href="https://arxiv.org/abs/2103.03827">arxiv:2103.03827</a>
&#x1F4C8; 2 <br>
<p>Mathieu Labbé, François Michaud</p></summary>
<p>

**Abstract:** For robots navigating using only a camera, illumination changes in indoor environments can cause localization failures during autonomous navigation. In this paper, we present a multi-session visual SLAM approach to create a map made of multiple variations of the same locations in different illumination conditions. The multi-session map can then be used at any hour of the day for improved localization capability. The approach presented is independent of the visual features used, and this is demonstrated by comparing localization performance between multi-session maps created using the RTAB-Map library with SURF, SIFT, BRIEF, FREAK, BRISK, KAZE, DAISY and SuperPoint visual features. The approach is tested on six mapping and six localization sessions recorded at 30 minutes intervals during sunset using a Google Tango phone in a real apartment.

</p>
</details>

<details><summary><b>Learning Collision-free and Torque-limited Robot Trajectories based on Alternative Safe Behaviors</b>
<a href="https://arxiv.org/abs/2103.03793">arxiv:2103.03793</a>
&#x1F4C8; 2 <br>
<p>Jonas C. Kiemel, Torsten Kröger</p></summary>
<p>

**Abstract:** This paper presents an approach to learn online generation of collision-free and torque-limited robot trajectories. In order to generate future motions, a neural network is periodically invoked. Based on the current kinematic state of the robot and the network prediction, a trajectory for the current time interval can be calculated. The main idea of our paper is to execute the predicted motion only if a collision-free and torque-limited way to continue the trajectory is known. In practice, the motion predicted for the current time interval is extended by a braking trajectory and simulated using a physics engine. If the simulated trajectory complies with all safety constraints, the predicted motion is carried out. Otherwise, the braking trajectory calculated in the previous time interval serves as an alternative safe behavior. Given a task-specific reward function, the neural network is trained using reinforcement learning. The design of the action space used for reinforcement learning ensures that all predicted trajectories comply with kinematic joint limits. For our evaluation, simulated industrial robots and humanoid robots are trained to reach as many randomly placed target points as possible. We show that our method reliably prevents collisions with static obstacles and collisions between the robot arms, while generating motions that respect both torque limits and kinematic joint limits. Experiments with a real robot demonstrate that safe trajectories can be generated in real-time.

</p>
</details>

<details><summary><b>A Convolutional Architecture for 3D Model Embedding</b>
<a href="https://arxiv.org/abs/2103.03764">arxiv:2103.03764</a>
&#x1F4C8; 2 <br>
<p>Arniel Labrada, Benjamin Bustos, Ivan Sipiran</p></summary>
<p>

**Abstract:** During the last years, many advances have been made in tasks like3D model retrieval, 3D model classification, and 3D model segmentation.The typical 3D representations such as point clouds, voxels, and poly-gon meshes are mostly suitable for rendering purposes, while their use forcognitive processes (retrieval, classification, segmentation) is limited dueto their high redundancy and complexity. We propose a deep learningarchitecture to handle 3D models as an input. We combine this architec-ture with other standard architectures like Convolutional Neural Networksand autoencoders for computing 3D model embeddings. Our goal is torepresent a 3D model as a vector with enough information to substitutethe 3D model for high-level tasks. Since this vector is a learned repre-sentation which tries to capture the relevant information of a 3D model,we show that the embedding representation conveys semantic informationthat helps to deal with the similarity assessment of 3D objects. Our ex-periments show the benefit of computing the embeddings of a 3D modeldata set and use them for effective 3D Model Retrieval.

</p>
</details>

<details><summary><b>Liver Fibrosis and NAS scoring from CT images using self-supervised learning and texture encoding</b>
<a href="https://arxiv.org/abs/2103.03761">arxiv:2103.03761</a>
&#x1F4C8; 2 <br>
<p>Ananya Jana, Hui Qu, Carlos D. Minacapelli, Carolyn Catalano, Vinod Rustgi, Dimitris Metaxas</p></summary>
<p>

**Abstract:** Non-alcoholic fatty liver disease (NAFLD) is one of the most common causes of chronic liver diseases (CLD) which can progress to liver cancer. The severity and treatment of NAFLD is determined by NAFLD Activity Scores (NAS)and liver fibrosis stage, which are usually obtained from liver biopsy. However, biopsy is invasive in nature and involves risk of procedural complications. Current methods to predict the fibrosis and NAS scores from noninvasive CT images rely heavily on either a large annotated dataset or transfer learning using pretrained networks. However, the availability of a large annotated dataset cannot be always ensured andthere can be domain shifts when using transfer learning. In this work, we propose a self-supervised learning method to address both problems. As the NAFLD causes changes in the liver texture, we also propose to use texture encoded inputs to improve the performance of the model. Given a relatively small dataset with 30 patients, we employ a self-supervised network which achieves better performance than a network trained via transfer learning. The code is publicly available at https://github.com/ananyajana/fibrosis_code.

</p>
</details>

<details><summary><b>Deeply supervised UNet for semantic segmentation to assist dermatopathological assessment of Basal Cell Carcinoma (BCC)</b>
<a href="https://arxiv.org/abs/2103.03759">arxiv:2103.03759</a>
&#x1F4C8; 2 <br>
<p>Jean Le'Clerc Arrastia, Nick Heilenkötter, Daniel Otero Baguer, Lena Hauberg-Lotte, Tobias Boskamp, Sonja Hetzer, Nicole Duschner, Jörg Schaller, Peter Maaß</p></summary>
<p>

**Abstract:** Accurate and fast assessment of resection margins is an essential part of a dermatopathologist's clinical routine. In this work, we successfully develop a deep learning method to assist the pathologists by marking critical regions that have a high probability of exhibiting pathological features in Whole Slide Images (WSI). We focus on detecting Basal Cell Carcinoma (BCC) through semantic segmentation using several models based on the UNet architecture. The study includes 650 WSI with 3443 tissue sections in total. Two clinical dermatopathologists annotated the data, marking tumor tissues' exact location on 100 WSI. The rest of the data, with ground-truth section-wise labels, is used to further validate and test the models. We analyze two different encoders for the first part of the UNet network and two additional training strategies: a) deep supervision, b) linear combination of decoder outputs, and obtain some interpretations about what the network's decoder does in each case. The best model achieves over 96%, accuracy, sensitivity, and specificity on the test set.

</p>
</details>

<details><summary><b>Parsing Indonesian Sentence into Abstract Meaning Representation using Machine Learning Approach</b>
<a href="https://arxiv.org/abs/2103.03730">arxiv:2103.03730</a>
&#x1F4C8; 2 <br>
<p>Adylan Roaffa Ilmy, Masayu Leylia Khodra</p></summary>
<p>

**Abstract:** Abstract Meaning Representation (AMR) provides many information of a sentence such as semantic relations, coreferences, and named entity relation in one representation. However, research on AMR parsing for Indonesian sentence is fairly limited. In this paper, we develop a system that aims to parse an Indonesian sentence using a machine learning approach. Based on Zhang et al. work, our system consists of three steps: pair prediction, label prediction, and graph construction. Pair prediction uses dependency parsing component to get the edges between the words for the AMR. The result of pair prediction is passed to the label prediction process which used a supervised learning algorithm to predict the label between the edges of the AMR. We used simple sentence dataset that is gathered from articles and news article sentences. Our model achieved the SMATCH score of 0.820 for simple sentence test data.

</p>
</details>

<details><summary><b>NemaNet: A convolutional neural network model for identification of nematodes soybean crop in brazil</b>
<a href="https://arxiv.org/abs/2103.03717">arxiv:2103.03717</a>
&#x1F4C8; 2 <br>
<p>Andre da Silva Abade, Lucas Faria Porto, Paulo Afonso Ferreira, Flavio de Barros Vidal</p></summary>
<p>

**Abstract:** Phytoparasitic nematodes (or phytonematodes) are causing severe damage to crops and generating large-scale economic losses worldwide. In soybean crops, annual losses are estimated at 10.6% of world production. Besides, identifying these species through microscopic analysis by an expert with taxonomy knowledge is often laborious, time-consuming, and susceptible to failure. In this perspective, robust and automatic approaches are necessary for identifying phytonematodes capable of providing correct diagnoses for the classification of species and subsidizing the taking of all control and prevention measures. This work presents a new public data set called NemaDataset containing 3,063 microscopic images from five nematode species with the most significant damage relevance for the soybean crop. Additionally, we propose a new Convolutional Neural Network (CNN) model defined as NemaNet and a comparative assessment with thirteen popular models of CNNs, all of them representing the state of the art classification and recognition. The general average calculated for each model, on a from-scratch training, the NemaNet model reached 96.99% accuracy, while the best evaluation fold reached 98.03%. In training with transfer learning, the average accuracy reached 98.88\%. The best evaluation fold reached 99.34% and achieve an overall accuracy improvement over 6.83% and 4.1%, for from-scratch and transfer learning training, respectively, when compared to other popular models.

</p>
</details>

<details><summary><b>Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Platforms</b>
<a href="https://arxiv.org/abs/2103.03697">arxiv:2103.03697</a>
&#x1F4C8; 2 <br>
<p>Ali Ghadirzadeh, Xi Chen, Petra Poklukar, Chelsea Finn, Mårten Björkman, Danica Kragic</p></summary>
<p>

**Abstract:** Reinforcement learning methods can achieve significant performance but require a large amount of training data collected on the same robotic platform. A policy trained with expensive data is rendered useless after making even a minor change to the robot hardware. In this paper, we address the challenging problem of adapting a policy, trained to perform a task, to a novel robotic hardware platform given only few demonstrations of robot motion trajectories on the target robot. We formulate it as a few-shot meta-learning problem where the goal is to find a meta-model that captures the common structure shared across different robotic platforms such that data-efficient adaptation can be performed. We achieve such adaptation by introducing a learning framework consisting of a probabilistic gradient-based meta-learning algorithm that models the uncertainty arising from the few-shot setting with a low-dimensional latent variable. We experimentally evaluate our framework on a simulated reaching and a real-robot picking task using 400 simulated robots generated by varying the physical parameters of an existing set of robotic platforms. Our results show that the proposed method can successfully adapt a trained policy to different robotic platforms with novel physical parameters and the superiority of our meta-learning algorithm compared to state-of-the-art methods for the introduced few-shot policy adaptation problem.

</p>
</details>

<details><summary><b>ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation</b>
<a href="https://arxiv.org/abs/2103.03664">arxiv:2103.03664</a>
&#x1F4C8; 2 <br>
<p>Raunak Dey, Yi Hong</p></summary>
<p>

**Abstract:** We introduce a neural network framework, utilizing adversarial learning to partition an image into two cuts, with one cut falling into a reference distribution provided by the user. This concept tackles the task of unsupervised anomaly segmentation, which has attracted increasing attention in recent years due to their broad applications in tasks with unlabelled data. This Adversarial-based Selective Cutting network (ASC-Net) bridges the two domains of cluster-based deep learning methods and adversarial-based anomaly/novelty detection algorithms. We evaluate this unsupervised learning model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN family, our model demonstrates tremendous performance gains in unsupervised anomaly segmentation tasks. Although there is still room to further improve performance compared to supervised learning algorithms, the promising experimental results shed light on building an unsupervised learning algorithm using user-defined knowledge.

</p>
</details>

<details><summary><b>MAMBPO: Sample-efficient multi-robot reinforcement learning using learned world models</b>
<a href="https://arxiv.org/abs/2103.03662">arxiv:2103.03662</a>
&#x1F4C8; 2 <br>
<p>Daniël Willemsen, Mario Coppola, Guido C. H. E. de Croon</p></summary>
<p>

**Abstract:** Multi-robot systems can benefit from reinforcement learning (RL) algorithms that learn behaviours in a small number of trials, a property known as sample efficiency. This research thus investigates the use of learned world models to improve sample efficiency. We present a novel multi-agent model-based RL algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the Centralized Learning for Decentralized Execution (CLDE) framework. CLDE algorithms allow a group of agents to act in a fully decentralized manner after training. This is a desirable property for many systems comprising of multiple robots. MAMBPO uses a learned world model to improve sample efficiency compared to model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two simulated multi-robot tasks, where MAMBPO achieves a similar performance to MASAC, but requires far fewer samples to do so. Through this, we take an important step towards making real-life learning for multi-robot systems possible.

</p>
</details>

<details><summary><b>Hierarchical Transformer for Multilingual Machine Translation</b>
<a href="https://arxiv.org/abs/2103.03589">arxiv:2103.03589</a>
&#x1F4C8; 2 <br>
<p>Albina Khusainova, Adil Khan, Adín Ramírez Rivera, Vitaly Romanov</p></summary>
<p>

**Abstract:** The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.

</p>
</details>

<details><summary><b>Transfer Learning based Speech Affect Recognition in Urdu</b>
<a href="https://arxiv.org/abs/2103.03580">arxiv:2103.03580</a>
&#x1F4C8; 2 <br>
<p>Sara Durrani, Muhammad Umair Arshad</p></summary>
<p>

**Abstract:** It has been established that Speech Affect Recognition for low resource languages is a difficult task. Here we present a Transfer learning based Speech Affect Recognition approach in which: we pre-train a model for high resource language affect recognition task and fine tune the parameters for low resource language using Deep Residual Network. Here we use standard four data sets to demonstrate that transfer learning can solve the problem of data scarcity for Affect Recognition task. We demonstrate that our approach is efficient by achieving 74.7 percent UAR on RAVDESS as source and Urdu data set as a target. Through an ablation study, we have identified that pre-trained model adds most of the features information, improvement in results and solves less data issues. Using this knowledge, we have also experimented on SAVEE and EMO-DB data set by setting Urdu as target language where only 400 utterances of data is available. This approach achieves high Unweighted Average Recall (UAR) when compared with existing algorithms.

</p>
</details>

<details><summary><b>Adaptive Gaussian Fuzzy Classifier for Real-Time Emotion Recognition in Computer Games</b>
<a href="https://arxiv.org/abs/2103.03488">arxiv:2103.03488</a>
&#x1F4C8; 2 <br>
<p>Daniel Leite, Volnei Frigeri Jr., Rodrigo Medeiros</p></summary>
<p>

**Abstract:** Human emotion recognition has become a need for more realistic and interactive machines and computer systems. The greatest challenge is the availability of high-performance algorithms to effectively manage individual differences and nonstationarities in physiological data streams, i.e., algorithms that self-customize to a user with no subject-specific calibration data. We describe an evolving Gaussian Fuzzy Classifier (eGFC), which is supported by an online semi-supervised learning algorithm to recognize emotion patterns from electroencephalogram (EEG) data streams. We extract features from the Fourier spectrum of EEG data. The data are provided by 28 individuals playing the games 'Train Sim World', 'Unravel', 'Slender The Arrival', and 'Goat Simulator' - a public dataset. Different emotions prevail, namely, boredom, calmness, horror and joy. We analyze the effect of individual electrodes, time window lengths, and frequency bands on the accuracy of user-independent eGFCs. We conclude that both brain hemispheres may assist classification, especially electrodes on the frontal (Af3-Af4), occipital (O1-O2), and temporal (T7-T8) areas. We observe that patterns may be eventually found in any frequency band; however, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, are the highest correlated with emotion classes. eGFC has shown to be effective for real-time learning of EEG data. It reaches a 72.2% accuracy using a variable rule base, 10-second windows, and 1.8ms/sample processing time in a highly-stochastic time-varying 4-class classification problem.

</p>
</details>

<details><summary><b>Enhancing safety in water transport system based on Internet of Things for developing countries</b>
<a href="https://arxiv.org/abs/2105.09459">arxiv:2105.09459</a>
&#x1F4C8; 1 <br>
<p>Md Mohaimenuzzaman, SM Monzurur Rahman, Musaed Alhussein, Ghulam Muhammad, Khondaker Abdullah Al Mamun</p></summary>
<p>

**Abstract:** Accidents in inland waterways in developing countries are a regular phenomenon throughout the year causing deaths, injuries, monetary loss, and a significant amount of missing people. In consequence, a lot of families are losing their dear ones leading to much misery. The above context demands an intelligent, safe, and reliable water transport system for the developing countries. The concept of Intelligent Transport System (ITS) can be applied to develop such system; however, there are issues with ITS and Internet of Things (IoT) unlocks a new way of developing it. This paper proposes a model to transform the water transport system into an intelligent system based on IoT. IPv6 based machine-to-machine (M2M) protocol, 3G telecommunication technology, and IEEE 802.15.4 network standard play a significant role in this proposed IoT based system.

</p>
</details>

<details><summary><b>Prediction of Hydraulic Blockage at Cross Drainage Structures using Regression Analysis</b>
<a href="https://arxiv.org/abs/2103.10930">arxiv:2103.10930</a>
&#x1F4C8; 1 <br>
<p>Umair Iqbal, Johan Barthelemy, Pascal Perez, Wanqing Li</p></summary>
<p>

**Abstract:** Hydraulic blockage of cross-drainage structures such as culverts is considered one of main contributor in triggering urban flash floods. However, due to lack of during floods data and highly non-linear nature of debris interaction, conventional modelling for hydraulic blockage is not possible. This paper proposes to use machine learning regression analysis for the prediction of hydraulic blockage. Relevant data has been collected by performing a scaled in-lab study and replicating different blockage scenarios. From the regression analysis, Artificial Neural Network (ANN) was reported best in hydraulic blockage prediction with $R^2$ of 0.89. With deployment of hydraulic sensors in smart cities, and availability of Big Data, regression analysis may prove helpful in addressing the blockage detection problem which is difficult to counter using conventional experimental and hydrological approaches.

</p>
</details>

<details><summary><b>Exact and heuristic approaches for multi-objective garbage accumulation points location in real scenarios</b>
<a href="https://arxiv.org/abs/2103.04826">arxiv:2103.04826</a>
&#x1F4C8; 1 <br>
<p>Diego Gabriel Rossit, Jamal Toutouh, Sergio Nesmachnow</p></summary>
<p>

**Abstract:** Municipal solid waste management is a major challenge for nowadays urban societies, because it accounts for a large proportion of public budget and, when mishandled, it can lead to environmental and social problems. This work focuses on the problem of locating waste bins in an urban area, which is considered to have a strong influence in the overall efficiency of the reverse logistic chain. This article contributes with an exact multiobjective approach to solve the waste bin location in which the optimization criteria that are considered are: the accessibility to the system (as quality of service measure), the investment cost, and the required frequency of waste removal from the bins (as a proxy of the posterior routing costs). In this approach, different methods to obtain the objectives ideal and nadir values over the Pareto front are proposed and compared. Then, a family of heuristic methods based on the PageRank algorithm is proposed which aims to optimize the accessibility to the system, the amount of collected waste and the installation cost. The experimental evaluation was performed on real-world scenarios of the cities of Montevideo, Uruguay, and Bahía Blanca, Argentina. The obtained results show the competitiveness of the proposed approaches for constructing a set of candidate solutions that considers the different trade-offs between the optimization criteria.

</p>
</details>

<details><summary><b>Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles</b>
<a href="https://arxiv.org/abs/2103.03975">arxiv:2103.03975</a>
&#x1F4C8; 1 <br>
<p>Nico Lang, Nikolai Kalischek, John Armston, Konrad Schindler, Ralph Dubayah, Jan Dirk Wegner</p></summary>
<p>

**Abstract:** NASA's Global Ecosystem Dynamics Investigation (GEDI) is a key climate mission whose goal is to advance our understanding of the role of forests in the global carbon cycle. While GEDI is the first space-based LIDAR explicitly optimized to measure vertical forest structure predictive of aboveground biomass, the accurate interpretation of this vast amount of waveform data across the broad range of observational and environmental conditions is challenging. Here, we present a novel supervised machine learning approach to interpret GEDI waveforms and regress canopy top height globally. We propose a probabilistic deep learning approach based on an ensemble of deep convolutional neural networks(CNN) to avoid the explicit modelling of unknown effects, such as atmospheric noise. The model learns to extract robust features that generalize to unseen geographical regions and, in addition, yields reliable estimates of predictive uncertainty. Ultimately, the global canopy top height estimates produced by our model have an expected RMSE of 2.7 m with low bias.

</p>
</details>

<details><summary><b>Interpolation of CT Projections by Exploiting Their Self-Similarity and Smoothness</b>
<a href="https://arxiv.org/abs/2103.03968">arxiv:2103.03968</a>
&#x1F4C8; 1 <br>
<p>Davood Karimi, Rabab K. Ward</p></summary>
<p>

**Abstract:** As the medical usage of computed tomography (CT) continues to grow, the radiation dose should remain at a low level to reduce the health risks. Therefore, there is an increasing need for algorithms that can reconstruct high-quality images from low-dose scans. In this regard, most of the recent studies have focused on iterative reconstruction algorithms, and little attention has been paid to restoration of the projection measurements, i.e., the sinogram. In this paper, we propose a novel sinogram interpolation algorithm. The proposed algorithm exploits the self-similarity and smoothness of the sinogram. Sinogram self-similarity is modeled in terms of the similarity of small blocks extracted from stacked projections. The smoothness is modeled via second-order total variation. Experiments with simulated and real CT data show that sinogram interpolation with the proposed algorithm leads to a substantial improvement in the quality of the reconstructed image, especially on low-dose scans. The proposed method can result in a significant reduction in the number of projection measurements. This will reduce the radiation dose and also the amount of data that need to be stored or transmitted, if the reconstruction is to be performed in a remote site.

</p>
</details>

<details><summary><b>Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes of Lung Nodules in CT</b>
<a href="https://arxiv.org/abs/2103.03931">arxiv:2103.03931</a>
&#x1F4C8; 1 <br>
<p>Xiaohang Fu, Lei Bi, Ashnil Kumar, Michael Fulham, Jinman Kim</p></summary>
<p>

**Abstract:** Accurate characterisation of visual attributes such as spiculation, lobulation, and calcification of lung nodules is critical in cancer management. The characterisation of these attributes is often subjective, which may lead to high inter- and intra-observer variability. Furthermore, lung nodules are often heterogeneous in the cross-sectional image slices of a 3D volume. Current state-of-the-art methods that score multiple attributes rely on deep learning-based multi-task learning (MTL) schemes. These methods, however, extract shared visual features across attributes and then examine each attribute without explicitly leveraging their inherent intercorrelations. Furthermore, current methods either treat each slice with equal importance without considering their relevance or heterogeneity, which limits performance. In this study, we address these challenges with a new convolutional neural network (CNN)-based MTL model that incorporates multiple attention-based learning modules to simultaneously score 9 visual attributes of lung nodules in computed tomography (CT) image volumes. Our model processes entire nodule volumes of arbitrary depth and uses a slice attention module to filter out irrelevant slices. We also introduce cross-attribute and attribute specialisation attention modules that learn an optimal amalgamation of meaningful representations to leverage relationships between attributes. We demonstrate that our model outperforms previous state-of-the-art methods at scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary nodules from over 1,000 patients. Our model also performs competitively when repurposed for benign-malignant classification. Our attention modules also provide easy-to-interpret weights that offer insights into the predictions of the model.

</p>
</details>

<details><summary><b>Deep Hedging, Generative Adversarial Networks, and Beyond</b>
<a href="https://arxiv.org/abs/2103.03913">arxiv:2103.03913</a>
&#x1F4C8; 1 <br>
<p>Hyunsu Kim</p></summary>
<p>

**Abstract:** This paper introduces a potential application of deep learning and artificial intelligence in finance, particularly its application in hedging. The major goal encompasses two objectives. First, we present a framework of a direct policy search reinforcement agent replicating a simple vanilla European call option and use the agent for the model-free delta hedging. Through the first part of this paper, we demonstrate how the RNN-based direct policy search RL agents can perform delta hedging better than the classic Black-Scholes model in Q-world based on parametrically generated underlying scenarios, particularly minimizing tail exposures at higher values of the risk aversion parameter. In the second part of this paper, with the non-parametric paths generated by time-series GANs from multi-variate temporal space, we illustrate its delta hedging performance on various values of the risk aversion parameter via the basic RNN-based RL agent introduced in the first part of the paper, showing that we can potentially achieve higher average profits with a rather evident risk-return trade-off. We believe that this RL-based hedging framework is a more efficient way of performing hedging in practice, addressing some of the inherent issues with the classic models, providing promising/intuitive hedging results, and rendering a flexible framework that can be easily paired with other AI-based models for many other purposes.

</p>
</details>

<details><summary><b>Lyapunov-Regularized Reinforcement Learning for Power System Transient Stability</b>
<a href="https://arxiv.org/abs/2103.03869">arxiv:2103.03869</a>
&#x1F4C8; 1 <br>
<p>Wenqi Cui, Baosen Zhang</p></summary>
<p>

**Abstract:** Transient stability of power systems is becoming increasingly important because of the growing integration of renewable resources. These resources lead to a reduction in mechanical inertia but also provide increased flexibility in frequency responses. Namely, their power electronic interfaces can implement almost arbitrary control laws. To design these controllers, reinforcement learning (RL) has emerged as a powerful method in searching for optimal non-linear control policy parameterized by neural networks.
  A key challenge is to enforce that a learned controller must be stabilizing. This paper proposes a Lyapunov regularized RL approach for optimal frequency control for transient stability in lossy networks. Because the lack of an analytical Lyapunov function, we learn a Lyapunov function parameterized by a neural network. The losses are specially designed with respect to the physical power system. The learned neural Lyapunov function is then utilized as a regularization to train the neural network controller by penalizing actions that violate the Lyapunov conditions. Case study shows that introducing the Lyapunov regularization enables the controller to be stabilizing and achieve smaller losses.

</p>
</details>

<details><summary><b>A Learning-Based Approach to Address Complexity-Reliability Tradeoff in OS Decoders</b>
<a href="https://arxiv.org/abs/2103.03860">arxiv:2103.03860</a>
&#x1F4C8; 1 <br>
<p>Baptiste Cavarec, Hasan Basri Celebi, Mats Bengtsson, Mikael Skoglund</p></summary>
<p>

**Abstract:** In this paper, we study the tradeoffs between complexity and reliability for decoding large linear block codes. We show that using artificial neural networks to predict the required order of an ordered statistics based decoder helps in reducing the average complexity and hence the latency of the decoder. We numerically validate the approach through Monte Carlo simulations.

</p>
</details>

<details><summary><b>Model-free two-step design for improving transient learning performance in nonlinear optimal regulator problems</b>
<a href="https://arxiv.org/abs/2103.03808">arxiv:2103.03808</a>
&#x1F4C8; 1 <br>
<p>Yuka Masumoto, Yoshihiro Okawa, Tomotake Sasaki, Yutaka Hori</p></summary>
<p>

**Abstract:** Reinforcement learning (RL) provides a model-free approach to designing an optimal controller for nonlinear dynamical systems. However, the learning process requires a considerable number of trial-and-error experiments using the poorly controlled system, and accumulates wear and tear on the plant. Thus, it is desirable to maintain some degree of control performance during the learning process. In this paper, we propose a model-free two-step design approach to improve the transient learning performance of RL in an optimal regulator design problem for unknown nonlinear systems. Specifically, a linear control law pre-designed in a model-free manner is used in parallel with online RL to ensure a certain level of performance at the early stage of learning. Numerical simulations show that the proposed method improves the transient learning performance and efficiency in hyperparameter tuning of RL.

</p>
</details>

<details><summary><b>Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for Resilient Wireless Signal Classification</b>
<a href="https://arxiv.org/abs/2103.03745">arxiv:2103.03745</a>
&#x1F4C8; 1 <br>
<p>Salvatore D'Oro, Francesco Restuccia, Tommaso Melodia</p></summary>
<p>

**Abstract:** Thanks to its capability of classifying complex phenomena without explicit modeling, deep learning (DL) has been demonstrated to be a key enabler of Wireless Signal Classification (WSC). Although DL can achieve a very high accuracy under certain conditions, recent research has unveiled that the wireless channel can disrupt the features learned by the DL model during training, thus drastically reducing the classification performance in real-world live settings. Since retraining classifiers is cumbersome after deployment, existing work has leveraged the usage of carefully-tailored Finite Impulse Response (FIR) filters that, when applied at the transmitter's side, can restore the features that are lost because of the the channel actions, i.e., waveform synthesis. However, these approaches compute FIRs using offline optimization strategies, which limits their efficacy in highly-dynamic channel settings. In this paper, we improve the state of the art by proposing Chares, a Deep Reinforcement Learning (DRL)-based framework for channel-resilient adaptive waveform synthesis. Chares adapts to new and unseen channel conditions by optimally computing through DRL the FIRs in real-time. Chares is a DRL agent whose architecture is-based upon the Twin Delayed Deep Deterministic Policy Gradients (TD3), which requires minimal feedback from the receiver and explores a continuous action space. Chares has been extensively evaluated on two well-known datasets. We have also evaluated the real-time latency of Chares with an implementation on field-programmable gate array (FPGA). Results show that Chares increases the accuracy up to 4.1x when no waveform synthesis is performed, by 1.9x with respect to existing work, and can compute new actions within 41us.

</p>
</details>

<details><summary><b>Implementing Automated Market Makers with Constant Circle</b>
<a href="https://arxiv.org/abs/2103.03699">arxiv:2103.03699</a>
&#x1F4C8; 1 <br>
<p>Yongge Wang</p></summary>
<p>

**Abstract:** This paper describe the implementation details of constant ellipse based automated market makers (CoinSwap). A CoinSwap prototype has been implemented at http://coinswapapp.io/ and the source codes are available at https://github.com/coinswapapp/

</p>
</details>

<details><summary><b>SDR-based Testbed for Real-time CQI Prediction for URLLC</b>
<a href="https://arxiv.org/abs/2103.03572">arxiv:2103.03572</a>
&#x1F4C8; 1 <br>
<p>Kirill Glinskiy, Evgeny Khorov, Alexey Kureev</p></summary>
<p>

**Abstract:** Ultra-reliable Low-Latency Communication (URLLC) is a key feature of 5G systems. The quality of service (QoS) requirements imposed by URLLC are less than 10ms delay and less than $10^{-5}$ packet loss rate (PLR). To satisfy such strict requirements with minimal channel resource consumption, the devices need to accurately predict the channel quality and select Modulation and Coding Scheme (MCS) for URLLC in a proper way.
  This paper presents a novel real-time channel prediction system based on Software-Defined Radio that uses a neural network. The paper also describes and shares an open channel measurement dataset that can be used to compare various channel prediction approaches in different mobility scenarios in future research on URLLC

</p>
</details>

<details><summary><b>Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis</b>
<a href="https://arxiv.org/abs/2103.03541">arxiv:2103.03541</a>
&#x1F4C8; 1 <br>
<p>Mutian He, Jingzhou Yang, Lei He, Frank K. Soong</p></summary>
<p>

**Abstract:** To scale neural speech synthesis to various real-world languages, we present a multilingual end-to-end framework that maps byte inputs to spectrograms, thus allowing arbitrary input scripts. Besides strong results on 40+ languages, the framework demonstrates capabilities to adapt to new languages under extreme low-resource and even few-shot scenarios of merely 40s transcribed recording, without the need of per-language resources like lexicon, extra corpus, auxiliary models, or linguistic expertise, thus ensuring scalability. While it retains satisfactory intelligibility and naturalness matching rich-resource models. Exhaustive comparative and ablation studies are performed to reveal the potential of the framework for low-resource languages. Furthermore, we propose a novel method to extract language-specific sub-networks in a multilingual model for a better understanding of its mechanism.

</p>
</details>

<details><summary><b>Extend the FFmpeg Framework to Analyze Media Content</b>
<a href="https://arxiv.org/abs/2103.03539">arxiv:2103.03539</a>
&#x1F4C8; 1 <br>
<p>Xintian Wu, Pengfei Qu, Shaofei Wang, Lin Xie, Jie Dong</p></summary>
<p>

**Abstract:** This paper introduces a new set of video analytics plugins developed for the FFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg media features for its comprehensive media encoding, decoding, muxing, and demuxing capabilities can now additionally analyze the video content based on AI models. The plugins are thread optimized for best performance overcoming certain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO Toolkit inference engine as the backend. The analytics workloads are accelerated on different platforms such as CPU, GPU, FPGA or specialized analytics accelerators. With our reference implementation, the feature of OpenVINO as inference backend has been pushed into FFmpeg mainstream repository. We plan to submit more patches later.

</p>
</details>

<details><summary><b>Does chronology matter in JIT defect prediction? A Partial Replication Study</b>
<a href="https://arxiv.org/abs/2103.03506">arxiv:2103.03506</a>
&#x1F4C8; 1 <br>
<p>Hadi Jahanshahi, Dhanya Jothimani, Ayşe Başar, Mucahit Cevik</p></summary>
<p>

**Abstract:** Just-In-Time (JIT) models detect the fix-inducing changes (or defect-inducing changes). These models are designed based on the assumption that past code change properties are similar to future ones. However, as the system evolves, the expertise of developers and/or the complexity of the system also changes.
  In this work, we aim to investigate the effect of code change properties on JIT models over time. We also study the impact of using recent data as well as all available data on the performance of JIT models. Further, we analyze the effect of weighted sampling on the performance of fix-inducing properties of JIT models. For this purpose, we used datasets from Eclipse JDT, Mozilla, Eclipse Platform, and PostgreSQL.
  We used five families of change-code properties such as size, diffusion, history, experience, and purpose. We used Random Forest to train and test the JIT model and Brier Score and the area under the ROC curve for performance measurement.
  Our paper suggests that the predictive power of JIT models does not change over time. Furthermore, we observed that the chronology of data in JIT defect prediction models can be discarded by considering all the available data. On the other hand, the importance score of families of code change properties is found to oscillate over time.
  To mitigate the impact of the evolution of code change properties, it is recommended to use a weighted sampling approach in which more emphasis is placed upon the changes occurring closer to the current time. Moreover, since properties such as "Expertise of the Developer" and "Size" evolve with time, the models obtained from old data may exhibit different characteristics compared to those employing the newer dataset. Hence, practitioners should constantly retrain JIT models to include fresh data.

</p>
</details>

<details><summary><b>Prediction of financial time series using LSTM and data denoising methods</b>
<a href="https://arxiv.org/abs/2103.03505">arxiv:2103.03505</a>
&#x1F4C8; 1 <br>
<p>Qi Tang, Tongmei Fan, Ruchen Shi, Jingyan Huang, Yidan Ma</p></summary>
<p>

**Abstract:** In order to further overcome the difficulties of the existing models in dealing with the non-stationary and nonlinear characteristics of high-frequency financial time series data, especially its weak generalization ability, this paper proposes an ensemble method based on data denoising methods, including the wavelet transform (WT) and singular spectrum analysis (SSA), and long-term short-term memory neural network (LSTM) to build a data prediction model, The financial time series is decomposed and reconstructed by WT and SSA to denoise. Under the condition of denoising, the smooth sequence with effective information is reconstructed. The smoothing sequence is introduced into LSTM and the predicted value is obtained. With the Dow Jones industrial average index (DJIA) as the research object, the closing price of the DJIA every five minutes is divided into short-term (1 hour), medium-term (3 hours) and long-term (6 hours) respectively. . Based on root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE) and absolute percentage error standard deviation (SDAPE), the experimental results show that in the short-term, medium-term and long-term, data denoising can greatly improve the accuracy and stability of the prediction, and can effectively improve the generalization ability of LSTM prediction model. As WT and SSA can extract useful information from the original sequence and avoid overfitting, the hybrid model can better grasp the sequence pattern of the closing price of the DJIA. And the WT-LSTM model is better than the benchmark LSTM model and SSA-LSTM model.

</p>
</details>

<details><summary><b>Moving from Cross-Project Defect Prediction to Heterogeneous Defect Prediction: A Partial Replication Study</b>
<a href="https://arxiv.org/abs/2103.03490">arxiv:2103.03490</a>
&#x1F4C8; 1 <br>
<p>Hadi Jahanshahi, Mucahit Cevik, Ayşe Başar</p></summary>
<p>

**Abstract:** Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.

</p>
</details>

<details><summary><b>Harnessing Geometric Constraints from Emotion Labels to improve Face Verification</b>
<a href="https://arxiv.org/abs/2103.03862">arxiv:2103.03862</a>
&#x1F4C8; 0 <br>
<p>Anand Ramakrishnan, Minh Pham, Jacob Whitehill</p></summary>
<p>

**Abstract:** For the task of face verification, we explore the utility of harnessing auxiliary facial emotion labels to impose explicit geometric constraints on the embedding space when training deep embedding models. We introduce several novel loss functions that, in conjunction with a standard Triplet Loss [43], or ArcFace loss [10], provide geometric constraints on the embedding space; the labels for our loss functions can be provided using either manually annotated or automatically detected auxiliary emotion labels. Our method is implemented purely in terms of the loss function and does not require any changes to the neural network backbone of the embedding function.

</p>
</details>

<details><summary><b>Error-Correction for Sparse Support Recovery Algorithms</b>
<a href="https://arxiv.org/abs/2103.03801">arxiv:2103.03801</a>
&#x1F4C8; 0 <br>
<p>Mohammad Mehrabi, Aslan Tchamkerten</p></summary>
<p>

**Abstract:** Consider the compressed sensing setup where the support $s^*$ of an $m$-sparse $d$-dimensional signal $x$ is to be recovered from $n$ linear measurements with a given algorithm. Suppose that the measurements are such that the algorithm does not guarantee perfect support recovery and that true features may be missed. Can they efficiently be retrieved? This paper addresses this question through a simple error-correction module referred to as LiRE. LiRE takes as input an estimate $s_{in}$ of the true support $s^*$, and outputs a refined support estimate $s_{out}$. In the noiseless measurement setup, sufficient conditions are established under which LiRE is guaranteed to recover the entire support, that is $s_{out}$ contains $s^*$. These conditions imply, for instance, that in the high-dimensional regime LiRE can correct a sublinear in $m$ number of errors made by Orthogonal Matching Pursuit (OMP). The computational complexity of LiRE is $O(mnd)$. Experimental results with random Gaussian design matrices show that LiRE substantially reduces the number of measurements needed for perfect support recovery via Compressive Sampling Matching Pursuit, Basis Pursuit (BP), and OMP. Interestingly, adding LiRE to OMP yields a support recovery procedure that is more accurate and significantly faster than BP. This observation carries over in the noisy measurement setup. Finally, as a standalone support recovery algorithm with a random initialization, experiments show that LiRE's reconstruction performance lies between OMP and BP. These results suggest that LiRE may be used generically, on top of any suboptimal baseline support recovery algorithm, to improve support recovery or to operate with a smaller number of measurements, at the cost of a relatively small computational overhead. Alternatively, LiRE may be used as a standalone support recovery algorithm that is competitive with respect to OMP.

</p>
</details>

<details><summary><b>PRIMA: Precise and General Neural Network Certification via Multi-Neuron Convex Relaxations</b>
<a href="https://arxiv.org/abs/2103.03638">arxiv:2103.03638</a>
&#x1F4C8; 0 <br>
<p>Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, Martin Vechev</p></summary>
<p>

**Abstract:** Formal verification of neural networks is critical for their safe and secure adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex approximations involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging image classifiers from prior work. Our results show that PRIMA is significantly more precise than state-of-the-art, verifying robustness for up to 14%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, precise verification of a realistic neural network for autonomous driving within a few minutes.

</p>
</details>

<details><summary><b>Environmental Sound Classification on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices</b>
<a href="https://arxiv.org/abs/2103.03483">arxiv:2103.03483</a>
&#x1F4C8; 0 <br>
<p>Md Mohaimenuzzaman, Christoph Bergmeir, Ian Thomas West, Bernd Meyer</p></summary>
<p>

**Abstract:** Significant efforts are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small enough for an off-the-shelf microcrocontroller, yet achieves state-of-the-art performance on standard benchmarks. Rather than handcrafting a once-off solution, we present a universal pipeline that converts a large deep convolutional network automatically via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65%) and ESC-50 (87.1%), we describe the compression pipeline and show that it allows us to achieve 97.22% size reduction and 97.28% FLOP reduction while maintaining close to state-of-the-art accuracy (83.65% on ESC-50). We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world data sets.

</p>
</details>

<details><summary><b>Pilot Investigation for a Comprehensive Taxonomy of Autonomous Entities</b>
<a href="https://arxiv.org/abs/2103.03482">arxiv:2103.03482</a>
&#x1F4C8; 0 <br>
<p>William Wagner, Anna Źakowska, Clement Aladi, Joseph Santhosh</p></summary>
<p>

**Abstract:** This paper documents an exploratory pilot study to define the term Autonomous Entity, and any characteristics that are required to identify or classify an Autonomous Entity. Our solution builds on previous work with regard to philosophical and scientific classification methods but focuses on a novel Design Science Research Methodology (DSRM) and model to help identify those characteristics which make any autonomous entity similar or different from others. We have solved the problem of not having an existing term to define our lens by creating a new combinatorial term: "Riskyishness". We present a DSRM and instrument for initial investigation, as well as observational and statistical descriptions of their use in the real world to solicit domain expertise and statistical evidence. Further, we demonstrate a specific application of the methodology by creating a second artifact - a tool to score existing and future technologies based on Riskyishness. The first artifact also provides a technique to disentangle miscellaneous existing technologies or add dimensions to the tools to capture future additions and paradigm shifts.

</p>
</details>


[Next Page]({{ '/2021/03/04/2021.03.04.html' | relative_url }})
