## Summary for 2021-07-30, created on 2021-12-21


<details><summary><b>Deep Natural Language Processing for LinkedIn Search Systems</b>
<a href="https://arxiv.org/abs/2108.08252">arxiv:2108.08252</a>
&#x1F4C8; 172 <br>
<p>Weiwei Guo, Xiaowei Liu, Sida Wang, Michaeel Kazi, Zhoutong Fu, Huiji Gao, Jun Jia, Liang Zhang, Bo Long</p></summary>
<p>

**Abstract:** Many search systems work with large amounts of natural language data, e.g., search queries, user profiles and documents, where deep learning based natural language processing techniques (deep NLP) can be of great help. In this paper, we introduce a comprehensive study of applying deep NLP techniques to five representative tasks in search engines. Through the model design and experiments of the five tasks, readers can find answers to three important questions: (1) When is deep NLP helpful/not helpful in search systems? (2) How to address latency challenges? (3) How to ensure model robustness? This work builds on existing efforts of LinkedIn search, and is tested at scale on a commercial search engine. We believe our experiences can provide useful insights for the industry and research communities.

</p>
</details>

<details><summary><b>Perceiver IO: A General Architecture for Structured Inputs & Outputs</b>
<a href="https://arxiv.org/abs/2107.14795">arxiv:2107.14795</a>
&#x1F4C8; 167 <br>
<p>Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira</p></summary>
<p>

**Abstract:** The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.

</p>
</details>

<details><summary><b>MTVR: Multilingual Moment Retrieval in Videos</b>
<a href="https://arxiv.org/abs/2108.00061">arxiv:2108.00061</a>
&#x1F4C8; 47 <br>
<p>Jie Lei, Tamara L. Berg, Mohit Bansal</p></summary>
<p>

**Abstract:** We introduce mTVR, a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations. We further propose mXML, a multilingual moment retrieval model that learns and operates on data from both languages, via encoder parameter sharing and language neighborhood constraints. We demonstrate the effectiveness of mXML on the newly collected MTVR dataset, where mXML outperforms strong monolingual baselines while using fewer parameters. In addition, we also provide detailed dataset analyses and model ablations. Data and code are publicly available at https://github.com/jayleicn/mTVRetrieval

</p>
</details>

<details><summary><b>Artist Similarity with Graph Neural Networks</b>
<a href="https://arxiv.org/abs/2107.14541">arxiv:2107.14541</a>
&#x1F4C8; 31 <br>
<p>Filip Korzeniowski, Sergio Oramas, Fabien Gouyon</p></summary>
<p>

**Abstract:** Artist similarity plays an important role in organizing, understanding, and subsequently, facilitating discovery in large collections of music. In this paper, we present a hybrid approach to computing similarity between artists using graph neural networks trained with triplet loss. The novelty of using a graph neural network architecture is to combine the topology of a graph of artist connections with content features to embed artists into a vector space that encodes similarity. To evaluate the proposed method, we compile the new OLGA dataset, which contains artist similarities from AllMusic, together with content features from AcousticBrainz. With 17,673 artists, this is the largest academic artist similarity dataset that includes content-based features to date. Moreover, we also showcase the scalability of our approach by experimenting with a much larger proprietary dataset. Results show the superiority of the proposed approach over current state-of-the-art methods for music similarity. Finally, we hope that the OLGA dataset will facilitate research on data-driven models for artist similarity.

</p>
</details>

<details><summary><b>DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models</b>
<a href="https://arxiv.org/abs/2107.14653">arxiv:2107.14653</a>
&#x1F4C8; 20 <br>
<p>Pedro Sarmento, Adarsh Kumar, CJ Carr, Zack Zukowski, Mathieu Barthet, Yi-Hsuan Yang</p></summary>
<p>

**Abstract:** Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument fingerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classification) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, fine-tune models on custom data, create new styles of music, AI-powered songwriting apps, and human-AI improvisation.

</p>
</details>

<details><summary><b>EmailSum: Abstractive Email Thread Summarization</b>
<a href="https://arxiv.org/abs/2107.14691">arxiv:2107.14691</a>
&#x1F4C8; 18 <br>
<p>Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal</p></summary>
<p>

**Abstract:** Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender's intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community. Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum

</p>
</details>

<details><summary><b>Object-aware Contrastive Learning for Debiased Scene Representation</b>
<a href="https://arxiv.org/abs/2108.00049">arxiv:2108.00049</a>
&#x1F4C8; 10 <br>
<p>Sangwoo Mo, Hyunwoo Kang, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin</p></summary>
<p>

**Abstract:** Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.

</p>
</details>

<details><summary><b>Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning</b>
<a href="https://arxiv.org/abs/2108.00045">arxiv:2108.00045</a>
&#x1F4C8; 10 <br>
<p>Faisal Alamri, Anjan Dutta</p></summary>
<p>

**Abstract:** Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are not observed during the training phase. The existing body of works on ZSL mostly relies on pretrained visual features and lacks the explicit attribute localisation mechanism on images. In this work, we propose an attention-based model in the problem settings of ZSL to learn attributes useful for unseen class recognition. Our method uses an attention mechanism adapted from Vision Transformer to capture and learn discriminative attributes by splitting images into small patches. We conduct experiments on three popular ZSL benchmarks (i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results {on all the three datasets}, which illustrate the effectiveness of our proposed method.

</p>
</details>

<details><summary><b>Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2107.14698">arxiv:2107.14698</a>
&#x1F4C8; 10 <br>
<p>Robert Loftin, Aadirupa Saha, Sam Devlin, Katja Hofmann</p></summary>
<p>

**Abstract:** High sample complexity remains a barrier to the application of reinforcement learning (RL), particularly in multi-agent systems. A large body of work has demonstrated that exploration mechanisms based on the principle of optimism under uncertainty can significantly improve the sample efficiency of RL in single agent tasks. This work seeks to understand the role of optimistic exploration in non-cooperative multi-agent settings. We will show that, in zero-sum games, optimistic exploration can cause the learner to waste time sampling parts of the state space that are irrelevant to strategic play, as they can only be reached through cooperation between both players. To address this issue, we introduce a formal notion of strategically efficient exploration in Markov games, and use this to develop two strategically efficient learning algorithms for finite Markov games. We demonstrate that these methods can be significantly more sample efficient than their optimistic counterparts.

</p>
</details>

<details><summary><b>Soft Calibration Objectives for Neural Networks</b>
<a href="https://arxiv.org/abs/2108.00106">arxiv:2108.00106</a>
&#x1F4C8; 9 <br>
<p>Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C. Mozer, Becca Roelofs</p></summary>
<p>

**Abstract:** Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.

</p>
</details>

<details><summary><b>Debiased Explainable Pairwise Ranking from Implicit Feedback</b>
<a href="https://arxiv.org/abs/2107.14768">arxiv:2107.14768</a>
&#x1F4C8; 8 <br>
<p>Khalil Damak, Sami Khenissi, Olfa Nasraoui</p></summary>
<p>

**Abstract:** Recent work in recommender systems has emphasized the importance of fairness, with a particular interest in bias and transparency, in addition to predictive accuracy. In this paper, we focus on the state of the art pairwise ranking model, Bayesian Personalized Ranking (BPR), which has previously been found to outperform pointwise models in predictive accuracy, while also being able to handle implicit feedback. Specifically, we address two limitations of BPR: (1) BPR is a black box model that does not explain its outputs, thus limiting the user's trust in the recommendations, and the analyst's ability to scrutinize a model's outputs; and (2) BPR is vulnerable to exposure bias due to the data being Missing Not At Random (MNAR). This exposure bias usually translates into an unfairness against the least popular items because they risk being under-exposed by the recommender system. In this work, we first propose a novel explainable loss function and a corresponding Matrix Factorization-based model called Explainable Bayesian Personalized Ranking (EBPR) that generates recommendations along with item-based explanations. Then, we theoretically quantify additional exposure bias resulting from the explainability, and use it as a basis to propose an unbiased estimator for the ideal EBPR loss. The result is a ranking model that aptly captures both debiased and explainable user preferences. Finally, we perform an empirical study on three real-world datasets that demonstrate the advantages of our proposed models.

</p>
</details>

<details><summary><b>On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models</b>
<a href="https://arxiv.org/abs/2108.00114">arxiv:2108.00114</a>
&#x1F4C8; 7 <br>
<p>Zeyad Emam, Andrew Kondrich, Sasha Harrison, Felix Lau, Yushi Wang, Aerin Kim, Elliot Branson</p></summary>
<p>

**Abstract:** High-quality labeled datasets play a crucial role in fueling the development of machine learning (ML), and in particular the development of deep learning (DL). However, since the emergence of the ImageNet dataset and the AlexNet model in 2012, the size of new open-source labeled vision datasets has remained roughly constant. Consequently, only a minority of publications in the computer vision community tackle supervised learning on datasets that are orders of magnitude larger than Imagenet. In this paper, we survey computer vision research domains that study the effects of such large datasets on model performance across different vision tasks. We summarize the community's current understanding of those effects, and highlight some open questions related to training with massive datasets. In particular, we tackle: (a) The largest datasets currently used in computer vision research and the interesting takeaways from training on such datasets; (b) The effectiveness of pre-training on large datasets; (c) Recent advancements and hurdles facing synthetic datasets; (d) An overview of double descent and sample non-monotonicity phenomena; and finally, (e) A brief discussion of lifelong/continual learning and how it fares compared to learning from huge labeled datasets in an offline setting. Overall, our findings are that research on optimization for deep learning focuses on perfecting the training routine and thus making DL models less data hungry, while research on synthetic datasets aims to offset the cost of data labeling. However, for the time being, acquiring non-synthetic labeled data remains indispensable to boost performance.

</p>
</details>

<details><summary><b>Unveiling the potential of Graph Neural Networks for robust Intrusion Detection</b>
<a href="https://arxiv.org/abs/2107.14756">arxiv:2107.14756</a>
&#x1F4C8; 7 <br>
<p>David Pujol-Perich, José Suárez-Varela, Albert Cabellos-Aparicio, Pere Barlet-Ros</p></summary>
<p>

**Abstract:** The last few years have seen an increasing wave of attacks with serious economic and privacy damages, which evinces the need for accurate Network Intrusion Detection Systems (NIDS). Recent works propose the use of Machine Learning (ML) techniques for building such systems (e.g., decision trees, neural networks). However, existing ML-based NIDS are barely robust to common adversarial attacks, which limits their applicability to real networks. A fundamental problem of these solutions is that they treat and classify flows independently. In contrast, in this paper we argue the importance of focusing on the structural patterns of attacks, by capturing not only the individual flow features, but also the relations between different flows (e.g., the source/destination hosts they share). To this end, we use a graph representation that keeps flow records and their relationships, and propose a novel Graph Neural Network (GNN) model tailored to process and learn from such graph-structured information. In our evaluation, we first show that the proposed GNN model achieves state-of-the-art results in the well-known CIC-IDS2017 dataset. Moreover, we assess the robustness of our solution under two common adversarial attacks, that intentionally modify the packet size and inter-arrival times to avoid detection. The results show that our model is able to maintain the same level of accuracy as in previous experiments, while state-of-the-art ML techniques degrade up to 50% their accuracy (F1-score) under these attacks. This unprecedented level of robustness is mainly induced by the capability of our GNN model to learn flow patterns of attacks structured as graphs.

</p>
</details>

<details><summary><b>ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback</b>
<a href="https://arxiv.org/abs/2107.14800">arxiv:2107.14800</a>
&#x1F4C8; 6 <br>
<p>Shiyue Zhang, Benjamin Frey, Mohit Bansal</p></summary>
<p>

**Abstract:** We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee-English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts back into the training set and retrain models, equal or slightly better performance is observed, which indicates the potential of human-in-the-loop learning. Our online demo is at https://chren.cs.unc.edu/ , our code is open-sourced at https://github.com/ZhangShiyue/ChrEnTranslate , and our data is available at https://github.com/ZhangShiyue/ChrEn

</p>
</details>

<details><summary><b>When Deep Learners Change Their Mind: Learning Dynamics for Active Learning</b>
<a href="https://arxiv.org/abs/2107.14707">arxiv:2107.14707</a>
&#x1F4C8; 6 <br>
<p>Javad Zolfaghari Bengar, Bogdan Raducanu, Joost van de Weijer</p></summary>
<p>

**Abstract:** Active learning aims to select samples to be annotated that yield the largest performance improvement for the learning algorithm. Many methods approach this problem by measuring the informativeness of samples and do this based on the certainty of the network predictions for samples. However, it is well-known that neural networks are overly confident about their prediction and are therefore an untrustworthy source to assess sample informativeness. In this paper, we propose a new informativeness-based active learning method. Our measure is derived from the learning dynamics of a neural network. More precisely we track the label assignment of the unlabeled data pool during the training of the algorithm. We capture the learning dynamics with a metric called label-dispersion, which is low when the network consistently assigns the same label to the sample during the training of the network and high when the assigned label changes frequently. We show that label-dispersion is a promising predictor of the uncertainty of the network, and show on two benchmark datasets that an active learning algorithm based on label-dispersion obtains excellent results.

</p>
</details>

<details><summary><b>Distributed Representations of Atoms and Materials for Machine Learning</b>
<a href="https://arxiv.org/abs/2107.14664">arxiv:2107.14664</a>
&#x1F4C8; 6 <br>
<p>Luis M. Antunes, Ricardo Grau-Crespo, Keith T. Butler</p></summary>
<p>

**Abstract:** The use of machine learning is becoming increasingly common in computational materials science. To build effective models of the chemistry of materials, useful machine-based representations of atoms and their compounds are required. We derive distributed representations of compounds from their chemical formulas only, via pooling operations of distributed representations of atoms. These compound representations are evaluated on ten different tasks, such as the prediction of formation energy and band gap, and are found to be competitive with existing benchmarks that make use of structure, and even superior in cases where only composition is available. Finally, we introduce a new approach for learning distributed representations of atoms, named SkipAtom, which makes use of the growing information in materials structure databases.

</p>
</details>

<details><summary><b>Can non-specialists provide high quality gold standard labels in challenging modalities?</b>
<a href="https://arxiv.org/abs/2107.14682">arxiv:2107.14682</a>
&#x1F4C8; 5 <br>
<p>Samuel Budd, Thomas Day, John Simpson, Karen Lloyd, Jacqueline Matthew, Emily Skelton, Reza Razavi, Bernhard Kainz</p></summary>
<p>

**Abstract:** Probably yes. -- Supervised Deep Learning dominates performance scores for many computer vision tasks and defines the state-of-the-art. However, medical image analysis lags behind natural image applications. One of the many reasons is the lack of well annotated medical image data available to researchers. One of the first things researchers are told is that we require significant expertise to reliably and accurately interpret and label such data. We see significant inter- and intra-observer variability between expert annotations of medical images. Still, it is a widely held assumption that novice annotators are unable to provide useful annotations for use by clinical Deep Learning models. In this work we challenge this assumption and examine the implications of using a minimally trained novice labelling workforce to acquire annotations for a complex medical image dataset. We study the time and cost implications of using novice annotators, the raw performance of novice annotators compared to gold-standard expert annotators, and the downstream effects on a trained Deep Learning segmentation model's performance for detecting a specific congenital heart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.

</p>
</details>

<details><summary><b>Refining Labelled Systems for Modal and Constructive Logics with Applications</b>
<a href="https://arxiv.org/abs/2107.14487">arxiv:2107.14487</a>
&#x1F4C8; 5 <br>
<p>Tim Lyon</p></summary>
<p>

**Abstract:** This thesis introduces the "method of structural refinement", which serves as a means of transforming the relational semantics of a modal and/or constructive logic into an 'economical' proof system by connecting two proof-theoretic paradigms: labelled and nested sequent calculi. The formalism of labelled sequents has been successful in that cut-free calculi in possession of desirable proof-theoretic properties can be automatically generated for large classes of logics. Despite these qualities, labelled systems make use of a complicated syntax that explicitly incorporates the semantics of the associated logic, and such systems typically violate the subformula property to a high degree. By contrast, nested sequent calculi employ a simpler syntax and adhere to a strict reading of the subformula property, making such systems useful in the design of automated reasoning algorithms. However, the downside of the nested sequent paradigm is that a general theory concerning the automated construction of such calculi (as in the labelled setting) is essentially absent, meaning that the construction of nested systems and the confirmation of their properties is usually done on a case-by-case basis. The refinement method connects both paradigms in a fruitful way, by transforming labelled systems into nested (or, refined labelled) systems with the properties of the former preserved throughout the transformation process.
  To demonstrate the method of refinement and some of its applications, we consider grammar logics, first-order intuitionistic logics, and deontic STIT logics. The introduced refined labelled calculi will be used to provide the first proof-search algorithms for deontic STIT logics. Furthermore, we employ our refined labelled calculi for grammar logics to show that every logic in the class possesses the effective Lyndon interpolation property.

</p>
</details>

<details><summary><b>ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations</b>
<a href="https://arxiv.org/abs/2107.14483">arxiv:2107.14483</a>
&#x1F4C8; 5 <br>
<p>Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, Hao Su</p></summary>
<p>

**Abstract:** Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions, we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced, and a challenge facing interdisciplinary researchers will be held based on the benchmark.

</p>
</details>

<details><summary><b>Trusted-Maximizers Entropy Search for Efficient Bayesian Optimization</b>
<a href="https://arxiv.org/abs/2107.14465">arxiv:2107.14465</a>
&#x1F4C8; 5 <br>
<p>Quoc Phong Nguyen, Zhaoxuan Wu, Bryan Kian Hsiang Low, Patrick Jaillet</p></summary>
<p>

**Abstract:** Information-based Bayesian optimization (BO) algorithms have achieved state-of-the-art performance in optimizing a black-box objective function. However, they usually require several approximations or simplifying assumptions (without clearly understanding their effects on the BO performance) and/or their generalization to batch BO is computationally unwieldy, especially with an increasing batch size. To alleviate these issues, this paper presents a novel trusted-maximizers entropy search (TES) acquisition function: It measures how much an input query contributes to the information gain on the maximizer over a finite set of trusted maximizers, i.e., inputs optimizing functions that are sampled from the Gaussian process posterior belief of the objective function. Evaluating TES requires either only a stochastic approximation with sampling or a deterministic approximation with expectation propagation, both of which are investigated and empirically evaluated using synthetic benchmark objective functions and real-world optimization problems, e.g., hyperparameter tuning of a convolutional neural network and synthesizing 'physically realizable' faces to fool a black-box face recognition system. Though TES can naturally be generalized to a batch variant with either approximation, the latter is amenable to be scaled to a much larger batch size in our experiments.

</p>
</details>

<details><summary><b>Static analysis of ReLU neural networks with tropical polyhedra</b>
<a href="https://arxiv.org/abs/2108.00893">arxiv:2108.00893</a>
&#x1F4C8; 4 <br>
<p>Eric Goubault, Sébastien Palumby, Sylvie Putot, Louis Rustenholz, Sriram Sankaranarayanan</p></summary>
<p>

**Abstract:** This paper studies the problem of range analysis for feedforward neural networks, which is a basic primitive for applications such as robustness of neural networks, compliance to specifications and reachability analysis of neural-network feedback systems. Our approach focuses on ReLU (rectified linear unit) feedforward neural nets that present specific difficulties: approaches that exploit derivatives do not apply in general, the number of patterns of neuron activations can be quite large even for small networks, and convex approximations are generally too coarse. In this paper, we employ set-based methods and abstract interpretation that have been very successful in coping with similar difficulties in classical program verification. We present an approach that abstracts ReLU feedforward neural networks using tropical polyhedra. We show that tropical polyhedra can efficiently abstract ReLU activation function, while being able to control the loss of precision due to linear computations. We show how the connection between ReLU networks and tropical rational functions can provide approaches for range analysis of ReLU neural networks.

</p>
</details>

<details><summary><b>Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks</b>
<a href="https://arxiv.org/abs/2108.00105">arxiv:2108.00105</a>
&#x1F4C8; 4 <br>
<p>Mostafa Parchami, Saif Iftekar Sayed</p></summary>
<p>

**Abstract:** Feature tracking is the building block of many applications such as visual odometry, augmented reality, and target tracking. Unfortunately, the state-of-the-art vision-based tracking algorithms fail in surgical images due to the challenges imposed by the nature of such environments. In this paper, we proposed a novel and unified deep learning-based approach that can learn how to track features reliably as well as learn how to detect such reliable features for tracking purposes. The proposed network dubbed as Deep-PT, consists of a tracker network which is a convolutional neural network simulating cross-correlation in terms of deep learning and two fully connected networks that operate on the output of intermediate layers of the tracker to detect features and predict trackability of the detected points. The ability to detect features based on the capabilities of the tracker distinguishes the proposed method from previous algorithms used in this area and improves the robustness of the algorithms against dynamics of the scene. The network is trained using multiple datasets due to the lack of specialized dataset for feature tracking datasets and extensive comparisons are conducted to compare the accuracy of Deep-PT against recent pixel tracking algorithms. As the experiments suggest, the proposed deep architecture deliberately learns what to track and how to track and outperforms the state-of-the-art methods.

</p>
</details>

<details><summary><b>A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms</b>
<a href="https://arxiv.org/abs/2108.00080">arxiv:2108.00080</a>
&#x1F4C8; 4 <br>
<p>Zhe Huang, Gary Long, Benjamin Wessler, Michael C. Hughes</p></summary>
<p>

**Abstract:** Semi-supervised image classification has shown substantial progress in learning from limited labeled data, but recent advances remain largely untested for clinical applications. Motivated by the urgent need to improve timely diagnosis of life-threatening heart conditions, especially aortic stenosis, we develop a benchmark dataset to assess semi-supervised approaches to two tasks relevant to cardiac ultrasound (echocardiogram) interpretation: view classification and disease severity classification. We find that a state-of-the-art method called MixMatch achieves promising gains in heldout accuracy on both tasks, learning from a large volume of truly unlabeled images as well as a labeled set collected at great expense to achieve better performance than is possible with the labeled set alone. We further pursue patient-level diagnosis prediction, which requires aggregating across hundreds of images of diverse view types, most of which are irrelevant, to make a coherent prediction. The best patient-level performance is achieved by new methods that prioritize diagnosis predictions from images that are predicted to be clinically-relevant views and transfer knowledge from the view task to the diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and evaluation framework inspire further improvements in multi-task semi-supervised learning for clinical applications.

</p>
</details>

<details><summary><b>Distribution free optimality intervals for clustering</b>
<a href="https://arxiv.org/abs/2107.14442">arxiv:2107.14442</a>
&#x1F4C8; 4 <br>
<p>Marina Meilă, Hanyu Zhang</p></summary>
<p>

**Abstract:** We address the problem of validating the ouput of clustering algorithms. Given data $\mathcal{D}$ and a partition $\mathcal{C}$ of these data into $K$ clusters, when can we say that the clusters obtained are correct or meaningful for the data? This paper introduces a paradigm in which a clustering $\mathcal{C}$ is considered meaningful if it is good with respect to a loss function such as the K-means distortion, and stable, i.e. the only good clustering up to small perturbations. Furthermore, we present a generic method to obtain post-inference guarantees of near-optimality and stability for a clustering $\mathcal{C}$. The method can be instantiated for a variety of clustering criteria (also called loss functions) for which convex relaxations exist. Obtaining the guarantees amounts to solving a convex optimization problem. We demonstrate the practical relevance of this method by obtaining guarantees for the K-means and the Normalized Cut clustering criteria on realistic data sets. We also prove that asymptotic instability implies finite sample instability w.h.p., allowing inferences about the population clusterability from a sample. The guarantees do not depend on any distributional assumptions, but they depend on the data set $\mathcal{D}$ admitting a stable clustering.

</p>
</details>

<details><summary><b>Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients</b>
<a href="https://arxiv.org/abs/2108.04358">arxiv:2108.04358</a>
&#x1F4C8; 3 <br>
<p>Ayaan Haque, Ipsita Sutradhar, Mahziba Rahman, Mehedi Hasan, Malabika Sarker</p></summary>
<p>

**Abstract:** Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye illness caused by diabetes, can lead to blindness if it is not identified and treated in its early stages. Unfortunately, diagnosis of DR requires medically trained professionals, but Bangladesh has limited specialists in comparison to its population. Moreover, the screening process is often expensive, prohibiting many from receiving timely and proper diagnosis. To address the problem, we introduce a deep learning algorithm which screens for different stages of DR. We use a state-of-the-art CNN architecture to diagnose patients based on retinal fundus imagery. This paper is an experimental evaluation of the algorithm we developed for DR diagnosis and screening specifically for Bangladeshi patients. We perform this validation study using separate pools of retinal image data of real patients from a hospital and field studies in Bangladesh. Our results show that the algorithm is effective at screening Bangladeshi eyes even when trained on a public dataset which is out of domain, and can accurately determine the stage of DR as well, achieving an overall accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The results confirm the ability of the algorithm to be used in real clinical settings and applications due to its high accuracy and classwise metrics. Our algorithm is implemented in the application Drishti, which is used to screen for DR in patients living in rural areas in Bangladesh, where access to professional screening is limited.

</p>
</details>

<details><summary><b>Efficient Sparse Spherical k-Means for Document Clustering</b>
<a href="https://arxiv.org/abs/2108.00895">arxiv:2108.00895</a>
&#x1F4C8; 3 <br>
<p>Johannes Knittel, Steffen Koch, Thomas Ertl</p></summary>
<p>

**Abstract:** Spherical k-Means is frequently used to cluster document collections because it performs reasonably well in many settings and is computationally efficient. However, the time complexity increases linearly with the number of clusters k, which limits the suitability of the algorithm for larger values of k depending on the size of the collection. Optimizations targeted at the Euclidean k-Means algorithm largely do not apply because the cosine distance is not a metric. We therefore propose an efficient indexing structure to improve the scalability of Spherical k-Means with respect to k. Our approach exploits the sparsity of the input vectors and the convergence behavior of k-Means to reduce the number of comparisons on each iteration significantly.

</p>
</details>

<details><summary><b>Active Learning in Gaussian Process State Space Model</b>
<a href="https://arxiv.org/abs/2108.00819">arxiv:2108.00819</a>
&#x1F4C8; 3 <br>
<p>Hon Sum Alec Yu, Dingling Yao, Christoph Zimmer, Marc Toussaint, Duy Nguyen-Tuong</p></summary>
<p>

**Abstract:** We investigate active learning in Gaussian Process state-space models (GPSSM). Our problem is to actively steer the system through latent states by determining its inputs such that the underlying dynamics can be optimally learned by a GPSSM. In order that the most informative inputs are selected, we employ mutual information as our active learning criterion. In particular, we present two approaches for the approximation of mutual information for the GPSSM given latent states. The proposed approaches are evaluated in several physical systems where we actively learn the underlying non-linear dynamics represented by the state-space model.

</p>
</details>

<details><summary><b>Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields</b>
<a href="https://arxiv.org/abs/2108.00094">arxiv:2108.00094</a>
&#x1F4C8; 3 <br>
<p>Nolan B. Gutierrez, William J. Beksi</p></summary>
<p>

**Abstract:** Thermal images model the long-infrared range of the electromagnetic spectrum and provide meaningful information even when there is no visible illumination. Yet, unlike imagery that represents radiation from the visible continuum, infrared images are inherently low-resolution due to hardware constraints. The restoration of thermal images is critical for applications that involve safety, search and rescue, and military operations. In this paper, we introduce a system to efficiently reconstruct thermal images. Specifically, we explore how to effectively attend to contrasting receptive fields (RFs) where increasing the RFs of a network can be computationally expensive. For this purpose, we introduce a deep attention to varying receptive fields network (AVRFN). We supply a gated convolutional layer with higher-order information extracted from disparate RFs, whereby an RF is parameterized by a dilation rate. In this way, the dilation rate can be tuned to use fewer parameters thus increasing the efficacy of AVRFN. Our experimental results show an improvement over the state of the art when compared against competing thermal image super-resolution methods.

</p>
</details>

<details><summary><b>Tensor-Train Density Estimation</b>
<a href="https://arxiv.org/abs/2108.00089">arxiv:2108.00089</a>
&#x1F4C8; 3 <br>
<p>Georgii S. Novikov, Maxim E. Panov, Ivan V. Oseledets</p></summary>
<p>

**Abstract:** Estimation of probability density function from samples is one of the central problems in statistics and machine learning. Modern neural network-based models can learn high dimensional distributions but have problems with hyperparameter selection and are often prone to instabilities during training and inference. We propose a new efficient tensor train-based model for density estimation (TTDE). Such density parametrization allows exact sampling, calculation of cumulative and marginal density functions, and partition function. It also has very intuitive hyperparameters. We develop an efficient non-adversarial training procedure for TTDE based on the Riemannian optimization. Experimental results demonstrate the competitive performance of the proposed method in density estimation and sampling tasks, while TTDE significantly outperforms competitors in training speed.

</p>
</details>

<details><summary><b>Foundations of data imbalance and solutions for a data democracy</b>
<a href="https://arxiv.org/abs/2108.00071">arxiv:2108.00071</a>
&#x1F4C8; 3 <br>
<p>Ajay Kulkarni, Deri Chong, Feras A. Batarseh</p></summary>
<p>

**Abstract:** Dealing with imbalanced data is a prevalent problem while performing classification on the datasets. Many times, this problem contributes to bias while making decisions or implementing policies. Thus, it is vital to understand the factors which cause imbalance in the data (or class imbalance). Such hidden biases and imbalances can lead to data tyranny and a major challenge to a data democracy. In this chapter, two essential statistical elements are resolved: the degree of class imbalance and the complexity of the concept; solving such issues helps in building the foundations of a data democracy. Furthermore, statistical measures which are appropriate in these scenarios are discussed and implemented on a real-life dataset (car insurance claims). In the end, popular data-level methods such as random oversampling, random undersampling, synthetic minority oversampling technique, Tomek link, and others are implemented in Python, and their performance is compared.

</p>
</details>

<details><summary><b>WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments</b>
<a href="https://arxiv.org/abs/2108.00057">arxiv:2108.00057</a>
&#x1F4C8; 3 <br>
<p>Skye Morgan, Tharindu Ranasinghe, Marcos Zampieri</p></summary>
<p>

**Abstract:** This paper addresses the identification of toxic, engaging, and fact-claiming comments on social media. We used the dataset made available by the organizers of the GermEval-2021 shared task containing over 3,000 manually annotated Facebook comments in German. Considering the relatedness of the three tasks, we approached the problem using large pre-trained transformer models and multitask learning. Our results indicate that multitask learning achieves performance superior to the more common single task learning approach in all three tasks. We submit our best systems to GermEval-2021 under the team name WLV-RIT.

</p>
</details>

<details><summary><b>Toward Robust Autotuning of Noisy Quantum Dot Devices</b>
<a href="https://arxiv.org/abs/2108.00043">arxiv:2108.00043</a>
&#x1F4C8; 3 <br>
<p>Joshua Ziegler, Thomas McJunkin, E. S. Joseph, Sandesh S. Kalantre, Benjamin Harpt, D. E. Savage, M. G. Lagally, M. A. Eriksson, Jacob M. Taylor, Justyna P. Zwolak</p></summary>
<p>

**Abstract:** The current autotuning approaches for quantum dot (QD) devices, while showing some success, lack an assessment of data reliability. This leads to unexpected failures when noisy data is processed by an autonomous system. In this work, we propose a framework for robust autotuning of QD devices that combines a machine learning (ML) state classifier with a data quality control module. The data quality control module acts as a ``gatekeeper'' system, ensuring that only reliable data is processed by the state classifier. Lower data quality results in either device recalibration or termination. To train both ML systems, we enhance the QD simulation by incorporating synthetic noise typical of QD experiments. We confirm that the inclusion of synthetic noise in the training of the state classifier significantly improves the performance, resulting in an accuracy of 95.1(7) % when tested on experimental data. We then validate the functionality of the data quality control module by showing the state classifier performance deteriorates with decreasing data quality, as expected. Our results establish a robust and flexible ML framework for autonomous tuning of noisy QD devices.

</p>
</details>

<details><summary><b>Maximum Entropy Dueling Network Architecture</b>
<a href="https://arxiv.org/abs/2107.14457">arxiv:2107.14457</a>
&#x1F4C8; 3 <br>
<p>Alireza Nadali, Mohammad Mehdi Ebadzadeh</p></summary>
<p>

**Abstract:** In recent years, there have been many deep structures for Reinforcement Learning, mainly for value function estimation and representations. These methods achieved great success in Atari 2600 domain. In this paper, we propose an improved architecture based upon Dueling Networks, in this architecture, there are two separate estimators, one approximate the state value function and the other, state advantage function. This improvement based on Maximum Entropy, shows better policy evaluation compared to the original network and other value-based architectures in Atari domain.

</p>
</details>

<details><summary><b>Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures</b>
<a href="https://arxiv.org/abs/2108.02565">arxiv:2108.02565</a>
&#x1F4C8; 2 <br>
<p>Hans Dermot Doran, Gianluca Ielpo, David Ganz, Michael Zapke</p></summary>
<p>

**Abstract:** With edge-AI finding an increasing number of real-world applications, especially in industry, the question of functionally safe applications using AI has begun to be asked. In this body of work, we explore the issue of achieving dependable operation of neural networks. We discuss the issue of dependability in general implementation terms before examining lockstep solutions. We intuit that it is not necessarily a given that two similar neural networks generate results at precisely the same time and that synchronization between the platforms will be required. We perform some preliminary measurements that may support this intuition and introduce some work in implementing lockstep neural network engines.

</p>
</details>

<details><summary><b>Online unsupervised Learning for domain shift in COVID-19 CT scan datasets</b>
<a href="https://arxiv.org/abs/2108.02002">arxiv:2108.02002</a>
&#x1F4C8; 2 <br>
<p>Nicolas Ewen, Naimul Khan</p></summary>
<p>

**Abstract:** Neural networks often require large amounts of expert annotated data to train. When changes are made in the process of medical imaging, trained networks may not perform as well, and obtaining large amounts of expert annotations for each change in the imaging process can be time consuming and expensive. Online unsupervised learning is a method that has been proposed to deal with situations where there is a domain shift in incoming data, and a lack of annotations. The aim of this study is to see whether online unsupervised learning can help COVID-19 CT scan classification models adjust to slight domain shifts, when there are no annotations available for the new data. A total of six experiments are performed using three test datasets with differing amounts of domain shift. These experiments compare the performance of the online unsupervised learning strategy to a baseline, as well as comparing how the strategy performs on different domain shifts. Code for online unsupervised learning can be found at this link: https://github.com/Mewtwo/online-unsupervised-learning

</p>
</details>

<details><summary><b>Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization</b>
<a href="https://arxiv.org/abs/2108.01441">arxiv:2108.01441</a>
&#x1F4C8; 2 <br>
<p>Quanye Jia, Rui Liu, Jianying Lin</p></summary>
<p>

**Abstract:** Manifold ranking has been successfully applied in query-oriented multi-document summarization. It not only makes use of the relationships among the sentences, but also the relationships between the given query and the sentences. However, the information of original query is often insufficient. So we present a query expansion method, which is combined in the manifold ranking to resolve this problem. Our method not only utilizes the information of the query term itself and the knowledge base WordNet to expand it by synonyms, but also uses the information of the document set itself to expand the query in various ways (mean expansion, variance expansion and TextRank expansion). Compared with the previous query expansion methods, our method combines multiple query expansion methods to better represent query information, and at the same time, it makes a useful attempt on manifold ranking. In addition, we use the degree of word overlap and the proximity between words to calculate the similarity between sentences. We performed experiments on the datasets of DUC 2006 and DUC2007, and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the state-of-the-art systems.

</p>
</details>

<details><summary><b>Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce</b>
<a href="https://arxiv.org/abs/2108.01440">arxiv:2108.01440</a>
&#x1F4C8; 2 <br>
<p>Ding Xiang, Becky West, Jiaqi Wang, Xiquan Cui, Jinzhou Huang</p></summary>
<p>

**Abstract:** E-commerce sites strive to provide users the most timely relevant information in order to reduce shopping frictions and increase customer satisfaction. Multi armed bandit models (MAB) as a type of adaptive optimization algorithms provide possible approaches for such purposes. In this paper, we analyze using three classic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper confidence bound 1 (UCB1) for dynamic content recommendations, and walk through the process of developing these algorithms internally to solve a real world e-commerce use case. First, we analyze the three MAB algorithms using simulated purchasing datasets with non-stationary reward distributions to simulate the possible time-varying customer preferences, where the traffic allocation dynamics and the accumulative rewards of different algorithms are studied. Second, we compare the accumulative rewards of the three MAB algorithms with more than 1,000 trials using actual historical A/B test datasets. We find that the larger difference between the success rates of competing recommendations the more accumulative rewards the MAB algorithms can achieve. In addition, we find that TS shows the highest average accumulative rewards under different testing scenarios. Third, we develop a batch-updated MAB algorithm to overcome the delayed reward issue in e-commerce and enable an online content optimization on our App homepage. For a state-of-the-art comparison, a real A/B test among our batch-updated MAB algorithm, a third-party MAB solution, and the default business logic are conducted. The result shows that our batch-updated MAB algorithm outperforms the counterparts and achieves 6.13% relative click-through rate (CTR) increase and 16.1% relative conversion rate (CVR) increase compared to the default experience, and 2.9% relative CTR increase and 1.4% relative CVR increase compared to the external MAB service.

</p>
</details>

<details><summary><b>Structure Amplification on Multi-layer Stochastic Block Models</b>
<a href="https://arxiv.org/abs/2108.00127">arxiv:2108.00127</a>
&#x1F4C8; 2 <br>
<p>Xiaodong Xin, Kun He, Jialu Bao, Bart Selman, John E. Hopcroft</p></summary>
<p>

**Abstract:** Much of the complexity of social, biological, and engineered systems arises from a network of complex interactions connecting many basic components. Network analysis tools have been successful at uncovering latent structure termed communities in such networks. However, some of the most interesting structure can be difficult to uncover because it is obscured by the more dominant structure. Our previous work proposes a general structure amplification technique called HICODE that uncovers many layers of functional hidden structure in complex networks. HICODE incrementally weakens dominant structure through randomization allowing the hidden functionality to emerge, and uncovers these hidden structure in real-world networks that previous methods rarely uncover. In this work, we conduct a comprehensive and systematic theoretical analysis on the hidden community structure. In what follows, we define multi-layer stochastic block model, and provide theoretical support using the model on why the existence of hidden structure will make the detection of dominant structure harder compared with equivalent random noise. We then provide theoretical proofs that the iterative reducing methods could help promote the uncovering of hidden structure as well as boosting the detection quality of dominant structure.

</p>
</details>

<details><summary><b>Towards Continual Entity Learning in Language Models for Conversational Agents</b>
<a href="https://arxiv.org/abs/2108.00082">arxiv:2108.00082</a>
&#x1F4C8; 2 <br>
<p>Ravi Teja Gadde, Ivan Bulyko</p></summary>
<p>

**Abstract:** Neural language models (LM) trained on diverse corpora are known to work well on previously seen entities, however, updating these models with dynamically changing entities such as place names, song titles and shopping items requires re-training from scratch and collecting full sentences containing these entities. We aim to address this issue, by introducing entity-aware language models (EALM), where we integrate entity models trained on catalogues of entities into the pre-trained LMs. Our combined language model adaptively adds information from the entity models into the pre-trained LM depending on the sentence context. Our entity models can be updated independently of the pre-trained LM, enabling us to influence the distribution of entities output by the final LM, without any further training of the pre-trained LM. We show significant perplexity improvements on task-oriented dialogue datasets, especially on long-tailed utterances, with an ability to continually adapt to new entities (to an extent).

</p>
</details>

<details><summary><b>Synthetic flow-based cryptomining attack generation through Generative Adversarial Networks</b>
<a href="https://arxiv.org/abs/2107.14776">arxiv:2107.14776</a>
&#x1F4C8; 2 <br>
<p>Alberto Mozo, Ángel González-Prieto, Antonio Pastor, Sandra Gómez-Canaval, Edgar Talavera</p></summary>
<p>

**Abstract:** Due to the growing rise of cyber attacks in the Internet, flow-based data sets are crucial to increase the performance of the Machine Learning (ML) components that run in network-based intrusion detection systems (IDS). To overcome the existing network traffic data shortage in attack analysis, recent works propose Generative Adversarial Networks (GANs) for synthetic flow-based network traffic generation. Data privacy is appearing more and more as a strong requirement when processing such network data, which suggests to find solutions where synthetic data can fully replace real data. Because of the ill-convergence of the GAN training, none of the existing solutions can generate high-quality fully synthetic data that can totally substitute real data in the training of IDS ML components. Therefore, they mix real with synthetic data, which acts only as data augmentation components, leading to privacy breaches as real data is used. In sharp contrast, in this work we propose a novel deterministic way to measure the quality of the synthetic data produced by a GAN both with respect to the real data and to its performance when used for ML tasks. As a byproduct, we present a heuristic that uses these metrics for selecting the best performing generator during GAN training, leading to a stopping criterion. An additional heuristic is proposed to select the best performing GANs when different types of synthetic data are to be used in the same ML task. We demonstrate the adequacy of our proposal by generating synthetic cryptomining attack traffic and normal traffic flow-based data using an enhanced version of a Wasserstein GAN. We show that the generated synthetic network traffic can completely replace real data when training a ML-based cryptomining detector, obtaining similar performance and avoiding privacy violations, since real data is not used in the training of the ML-based detector.

</p>
</details>

<details><summary><b>Adaptive Approach Phase Guidance for a Hypersonic Glider via Reinforcement Meta Learning</b>
<a href="https://arxiv.org/abs/2107.14764">arxiv:2107.14764</a>
&#x1F4C8; 2 <br>
<p>Brian Gaudet, Kris Drozd, Ryan Meltzer, Roberto Furfaro</p></summary>
<p>

**Abstract:** We use Reinforcement Meta Learning to optimize an adaptive guidance system suitable for the approach phase of a gliding hypersonic vehicle. Adaptability is achieved by optimizing over a range of off-nominal flight conditions including perturbation of aerodynamic coefficient parameters, actuator failure scenarios, and sensor noise. The system maps observations directly to commanded bank angle and angle of attack rates. These observations include a velocity field tracking error formulated using parallel navigation, but adapted to work over long trajectories where the Earth's curvature must be taken into account. Minimizing the tracking error keeps the curved space line of sight to the target location aligned with the vehicle's velocity vector. The optimized guidance system will then induce trajectories that bring the vehicle to the target location with a high degree of accuracy at the designated terminal speed, while satisfying heating rate, load, and dynamic pressure constraints. We demonstrate the adaptability of the guidance system by testing over flight conditions that were not experienced during optimization. The guidance system's performance is then compared to that of a linear quadratic regulator tracking an optimal trajectory.

</p>
</details>

<details><summary><b>On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals</b>
<a href="https://arxiv.org/abs/2107.14762">arxiv:2107.14762</a>
&#x1F4C8; 2 <br>
<p>Haizhou Shi, Youcai Zhang, Siliang Tang, Wenjie Zhu, Yaqian Li, Yandong Guo, Yueting Zhuang</p></summary>
<p>

**Abstract:** It is a consensus that small models perform quite poorly under the paradigm of self-supervised contrastive learning. Existing methods usually adopt a large off-the-shelf model to transfer knowledge to the small one via distillation. Despite their effectiveness, distillation-based methods may not be suitable for some resource-restricted scenarios due to the huge computational expenses of deploying a large model. In this paper, we study the issue of training self-supervised small models without distillation signals. We first evaluate the representation spaces of the small models and make two non-negligible observations: (i) the small models can complete the pretext task without overfitting despite their limited capacity and (ii) they universally suffer the problem of over clustering. Then we verify multiple assumptions that are considered to alleviate the over-clustering phenomenon. Finally, we combine the validated techniques and improve the baseline performances of five small architectures with considerable margins, which indicates that training small self-supervised contrastive models is feasible even without distillation signals. The code is available at \textit{https://github.com/WOWNICE/ssl-small}.

</p>
</details>

<details><summary><b>Connections between Numerical Algorithms for PDEs and Neural Networks</b>
<a href="https://arxiv.org/abs/2107.14742">arxiv:2107.14742</a>
&#x1F4C8; 2 <br>
<p>Tobias Alt, Karl Schrader, Matthias Augustin, Pascal Peter, Joachim Weickert</p></summary>
<p>

**Abstract:** We investigate numerous structural connections between numerical algorithms for partial differential equations (PDEs) and neural architectures. Our goal is to transfer the rich set of mathematical foundations from the world of PDEs to neural networks. Besides structural insights we provide concrete examples and experimental evaluations of the resulting architectures. Using the example of generalised nonlinear diffusion in 1D, we consider explicit schemes, acceleration strategies thereof, implicit schemes, and multigrid approaches. We connect these concepts to residual networks, recurrent neural networks, and U-net architectures. Our findings inspire a symmetric residual network design with provable stability guarantees and justify the effectiveness of skip connections in neural networks from a numerical perspective. Moreover, we present U-net architectures that implement multigrid techniques for learning efficient solutions of partial differential equation models, and motivate uncommon design choices such as trainable nonmonotone activation functions. Experimental evaluations show that the proposed architectures save half of the trainable parameters and can thus outperform standard ones with the same model complexity. Our considerations serve as a basis for explaining the success of popular neural architectures and provide a blueprint for developing new mathematically well-founded neural building blocks.

</p>
</details>

<details><summary><b>Towards General Function Approximation in Zero-Sum Markov Games</b>
<a href="https://arxiv.org/abs/2107.14702">arxiv:2107.14702</a>
&#x1F4C8; 2 <br>
<p>Baihe Huang, Jason D. Lee, Zhaoran Wang, Zhuoran Yang</p></summary>
<p>

**Abstract:** This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value function or the model is parameterized by general function classes. Provably efficient algorithms for both decoupled and {coordinated} settings are developed. In the {decoupled} setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension -- a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm by a $\sqrt{d}$ factor in the regret when the reward function and transition kernel are parameterized with $d$-dimensional linear features. In the {coordinated} setting where both players are controlled by the agent, we propose a model-based algorithm and a model-free algorithm. In the model-based algorithm, we prove that sample complexity can be bounded by a generalization of Witness rank to Markov games. The model-free algorithm enjoys a $\sqrt{K}$-regret upper bound where $K$ is the number of episodes.

</p>
</details>

<details><summary><b>A data-science-driven short-term analysis of Amazon, Apple, Google, and Microsoft stocks</b>
<a href="https://arxiv.org/abs/2107.14695">arxiv:2107.14695</a>
&#x1F4C8; 2 <br>
<p>Shubham Ekapure, Nuruddin Jiruwala, Sohan Patnaik, Indranil SenGupta</p></summary>
<p>

**Abstract:** In this paper, we implement a combination of technical analysis and machine/deep learning-based analysis to build a trend classification model. The goal of the paper is to apprehend short-term market movement, and incorporate it to improve the underlying stochastic model. Also, the analysis presented in this paper can be implemented in a \emph{model-independent} fashion. We execute a data-science-driven technique that makes short-term forecasts dependent on the price trends of current stock market data. Based on the analysis, three different labels are generated for a data set: $+1$ (buy signal), $0$ (hold signal), or $-1$ (sell signal). We propose a detailed analysis of four major stocks- Amazon, Apple, Google, and Microsoft. We implement various technical indicators to label the data set according to the trend and train various models for trend estimation. Statistical analysis of the outputs and classification results are obtained.

</p>
</details>

<details><summary><b>Task 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices using squeeze-excitation technique and low-complexity constraint</b>
<a href="https://arxiv.org/abs/2107.14658">arxiv:2107.14658</a>
&#x1F4C8; 2 <br>
<p>Javier Naranjo-Alcazar, Sergi Perez-Castanos, Maximo Cobos, Francesc J. Ferri, Pedro Zuccarello</p></summary>
<p>

**Abstract:** Acoustic scene classification (ASC) is one of the most popular problems in the field of machine listening. The objective of this problem is to classify an audio clip into one of the predefined scenes using only the audio data. This problem has considerably progressed over the years in the different editions of DCASE. It usually has several subtasks that allow to tackle this problem with different approaches. The subtask presented in this report corresponds to a ASC problem that is constrained by the complexity of the model as well as having audio recorded from different devices, known as mismatch devices (real and simulated). The work presented in this report follows the research line carried out by the team in previous years. Specifically, a system based on two steps is proposed: a two-dimensional representation of the audio using the Gamamtone filter bank and a convolutional neural network using squeeze-excitation techniques. The presented system outperforms the baseline by about 17 percentage points.

</p>
</details>

<details><summary><b>Practical Attacks on Voice Spoofing Countermeasures</b>
<a href="https://arxiv.org/abs/2107.14642">arxiv:2107.14642</a>
&#x1F4C8; 2 <br>
<p>Andre Kassis, Urs Hengartner</p></summary>
<p>

**Abstract:** Voice authentication has become an integral part in security-critical operations, such as bank transactions and call center conversations. The vulnerability of automatic speaker verification systems (ASVs) to spoofing attacks instigated the development of countermeasures (CMs), whose task is to tell apart bonafide and spoofed speech. Together, ASVs and CMs form today's voice authentication platforms, advertised as an impregnable access control mechanism. We develop the first practical attack on CMs, and show how a malicious actor may efficiently craft audio samples to bypass voice authentication in its strictest form. Previous works have primarily focused on non-proactive attacks or adversarial strategies against ASVs that do not produce speech in the victim's voice. The repercussions of our attacks are far more severe, as the samples we generate sound like the victim, eliminating any chance of plausible deniability. Moreover, the few existing adversarial attacks against CMs mistakenly optimize spoofed speech in the feature space and do not take into account the existence of ASVs, resulting in inferior synthetic audio that fails in realistic settings. We eliminate these obstacles through our key technical contribution: a novel joint loss function that enables mounting advanced adversarial attacks against combined ASV/CM deployments directly in the time domain. Our adversarials achieve concerning black-box success rates against state-of-the-art authentication platforms (up to 93.57\%). Finally, we perform the first targeted, over-telephony-network attack on CMs, bypassing several challenges and enabling various potential threats, given the increased use of voice biometrics in call centers. Our results call into question the security of modern voice authentication systems in light of the real threat of attackers bypassing these measures to gain access to users' most valuable resources.

</p>
</details>

<details><summary><b>DQ-SGD: Dynamic Quantization in SGD for Communication-Efficient Distributed Learning</b>
<a href="https://arxiv.org/abs/2107.14575">arxiv:2107.14575</a>
&#x1F4C8; 2 <br>
<p>Guangfeng Yan, Shao-Lun Huang, Tian Lan, Linqi Song</p></summary>
<p>

**Abstract:** Gradient quantization is an emerging technique in reducing communication costs in distributed learning. Existing gradient quantization algorithms often rely on engineering heuristics or empirical observations, lacking a systematic approach to dynamically quantize gradients. This paper addresses this issue by proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to dynamically adjust the quantization scheme for each gradient descent step by exploring the trade-off between communication cost and convergence error. We derive an upper bound, tight in some cases, of the convergence error for a restricted family of quantization schemes and loss functions. We design our DQ-SGD algorithm via minimizing the communication cost under the convergence error constraints. Finally, through extensive experiments on large-scale natural language processing and computer vision tasks on AG-News, CIFAR-10, and CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better tradeoffs between the communication cost and learning performance than other state-of-the-art gradient quantization methods.

</p>
</details>

<details><summary><b>Neural Network Based Model Predictive Control for an Autonomous Vehicle</b>
<a href="https://arxiv.org/abs/2107.14573">arxiv:2107.14573</a>
&#x1F4C8; 2 <br>
<p>Maria Luiza Costa Vianna, Eric Goubault, Sylvie Putot</p></summary>
<p>

**Abstract:** We study learning based controllers as a replacement for model predictive controllers (MPC) for the control of autonomous vehicles. We concentrate for the experiments on the simple yet representative bicycle model. We compare training by supervised learning and by reinforcement learning. We also discuss the neural net architectures so as to obtain small nets with the best performances. This work aims at producing controllers that can both be embedded on real-time platforms and amenable to verification by formal methods techniques.

</p>
</details>

<details><summary><b>TASK3 DCASE2021 Challenge: Sound event localization and detection using squeeze-excitation residual CNNs</b>
<a href="https://arxiv.org/abs/2107.14561">arxiv:2107.14561</a>
&#x1F4C8; 2 <br>
<p>Javier Naranjo-Alcazar, Sergi Perez-Castanos, Pedro Zuccarello, Francesc J. Ferri, Maximo Cobos</p></summary>
<p>

**Abstract:** Sound event localisation and detection (SELD) is a problem in the field of automatic listening that aims at the temporal detection and localisation (direction of arrival estimation) of sound events within an audio clip, usually of long duration. Due to the amount of data present in the datasets related to this problem, solutions based on deep learning have positioned themselves at the top of the state of the art. Most solutions are based on 2D representations of the audio (different spectrograms) that are processed by a convolutional-recurrent network. The motivation of this submission is to study the squeeze-excitation technique in the convolutional part of the network and how it improves the performance of the system. This study is based on the one carried out by the same team last year. This year, it has been decided to study how this technique improves each of the datasets (last year only the MIC dataset was studied). This modification shows an improvement in the performance of the system compared to the baseline using MIC dataset.

</p>
</details>

<details><summary><b>Evaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH COVID-19 from Audio Challenges</b>
<a href="https://arxiv.org/abs/2107.14549">arxiv:2107.14549</a>
&#x1F4C8; 2 <br>
<p>Alican Akman, Harry Coppock, Alexander Gaskell, Panagiotis Tzirakis, Lyn Jones, Björn W. Schuller</p></summary>
<p>

**Abstract:** We report on cross-running the recent COVID-19 Identification ResNet (CIdeR) on the two Interspeech 2021 COVID-19 diagnosis from cough and speech audio challenges: ComParE and DiCOVA. CIdeR is an end-to-end deep learning neural network originally designed to classify whether an individual is COVID-positive or COVID-negative based on coughing and breathing audio recordings from a published crowdsourced dataset. In the current study, we demonstrate the potential of CIdeR at binary COVID-19 diagnosis from both the COVID-19 Cough and Speech Sub-Challenges of INTERSPEECH 2021, ComParE and DiCOVA. CIdeR achieves significant improvements over several baselines.

</p>
</details>

<details><summary><b>A Framework for Adversarial Streaming via Differential Privacy and Difference Estimators</b>
<a href="https://arxiv.org/abs/2107.14527">arxiv:2107.14527</a>
&#x1F4C8; 2 <br>
<p>Idan Attias, Edith Cohen, Moshe Shechner, Uri Stemmer</p></summary>
<p>

**Abstract:** Streaming algorithms are algorithms for processing large data streams, using only a limited amount of memory. Classical streaming algorithms operate under the assumption that the input stream is fixed in advance. Recently, there is a growing interest in studying streaming algorithms that provide provable guarantees even when the input stream is chosen by an adaptive adversary. Such streaming algorithms are said to be {\em adversarially-robust}. We propose a novel framework for adversarial streaming that hybrids two recently suggested frameworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These recently suggested frameworks rely on very different ideas, each with its own strengths and weaknesses. We combine these two frameworks (in a non-trivial way) into a single hybrid framework that gains from both approaches to obtain superior performances for turnstile streams.

</p>
</details>

<details><summary><b>T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation</b>
<a href="https://arxiv.org/abs/2107.14447">arxiv:2107.14447</a>
&#x1F4C8; 2 <br>
<p>Ruihuang Li, Xu Jia, Jianzhong He, Shuaijun Chen, Qinghua Hu</p></summary>
<p>

**Abstract:** Most existing domain adaptation methods focus on adaptation from only one source domain, however, in practice there are a number of relevant sources that could be leveraged to help improve performance on target domain. We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decomposition (T-SVD) into a neural network's training pipeline. Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at capturing consistent data structure across different domains. Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weighting strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty estimation. Extensive experiments conducted on public benchmarks demonstrate the superiority of our model in addressing the task of MDA compared to state-of-the-art methods.

</p>
</details>

<details><summary><b>Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN</b>
<a href="https://arxiv.org/abs/2107.14444">arxiv:2107.14444</a>
&#x1F4C8; 2 <br>
<p>Xiaohan Ding, Tianxiang Hao, Jungong Han, Yuchen Guo, Guiguang Ding</p></summary>
<p>

**Abstract:** The existence of redundancy in Convolutional Neural Networks (CNNs) enables us to remove some filters/channels with acceptable performance drops. However, the training objective of CNNs usually tends to minimize an accuracy-related loss function without any attention paid to the redundancy, making the redundancy distribute randomly on all the filters, such that removing any of them may trigger information loss and accuracy drop, necessitating a following finetuning step for recovery. In this paper, we propose to manipulate the redundancy during training to facilitate network pruning. To this end, we propose a novel Centripetal SGD (C-SGD) to make some filters identical, resulting in ideal redundancy patterns, as such filters become purely redundant due to their duplicates; hence removing them does not harm the network. As shown on CIFAR and ImageNet, C-SGD delivers better performance because the redundancy is better organized, compared to the existing methods. The efficiency also characterizes C-SGD because it is as fast as regular SGD, requires no finetuning, and can be conducted simultaneously on all the layers even in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by first training a model with the same architecture but wider layers then squeezing it into the original width.

</p>
</details>

<details><summary><b>A SPA-based Manifold Learning Framework for Motor Imagery EEG Data Classification</b>
<a href="https://arxiv.org/abs/2108.00865">arxiv:2108.00865</a>
&#x1F4C8; 1 <br>
<p>Xiangyun Li, Peng Chen, Zhanpeng Bao</p></summary>
<p>

**Abstract:** The electroencephalography (EEG) signal is a non-stationary, stochastic, and highly non-linear bioelectric signal for which achieving high classification accuracy is challenging, especially when the number of subjects is limited. As frequently used solution, classifiers based on multilayer neural networks has to be implemented without large training data sets and careful tuning. This paper proposes a manifold learning framework to classify two types of EEG data from motor imagery (MI) tasks by discovering lower dimensional geometric structures. For feature extraction, it is implemented by Common Spatial Pattern (CSP) from the preprocessed EEG signals. In the neighborhoods of the features for classification, the local approximation to the support of the data is obtained, and then the features are assigned to the classes with the closest support. A spherical approximation (SPA) classifier is created using spherelets for local approximation, and the extracted features are classified with this manifold-based method. The SPA classifier achieves high accuracy in the 2008 BCI competition data, and the analysis shows that this method can significantly improve the decoding accuracy of MI tasks and exhibit strong robustness for small sample datasets. It would be simple and efficient to tune the two-parameters classifier for the online brain-computer interface(BCI)system.

</p>
</details>

<details><summary><b>A Machine-learning Based Initialization for Joint Statistical Iterative Dual-energy CT with Application to Proton Therapy</b>
<a href="https://arxiv.org/abs/2108.00109">arxiv:2108.00109</a>
&#x1F4C8; 1 <br>
<p>Tao Ge, Maria Medrano, Rui Liao, David G. Politte, Jeffrey F. Williamson, Joseph A. O'Sullivan</p></summary>
<p>

**Abstract:** Dual-energy CT (DECT) has been widely investigated to generate more informative and more accurate images in the past decades. For example, Dual-Energy Alternating Minimization (DEAM) algorithm achieves sub-percentage uncertainty in estimating proton stopping-power mappings from experimental 3-mm collimated phantom data. However, elapsed time of iterative DECT algorithms is not clinically acceptable, due to their low convergence rate and the tremendous geometry of modern helical CT scanners. A CNN-based initialization method is introduced to reduce the computational time of iterative DECT algorithms. DEAM is used as an example of iterative DECT algorithms in this work. The simulation results show that our method generates denoised images with greatly improved estimation accuracy for adipose, tonsils, and muscle tissue. Also, it reduces elapsed time by approximately 5-fold for DEAM to reach the same objective function value for both simulated and real data.

</p>
</details>

<details><summary><b>DySMHO: Data-Driven Discovery of Governing Equations for Dynamical Systems via Moving Horizon Optimization</b>
<a href="https://arxiv.org/abs/2108.00069">arxiv:2108.00069</a>
&#x1F4C8; 1 <br>
<p>Fernando Lejarza, Michael Baldea</p></summary>
<p>

**Abstract:** Discovering the governing laws underpinning physical and chemical phenomena is a key step towards understanding and ultimately controlling systems in science and engineering. We introduce Discovery of Dynamical Systems via Moving Horizon Optimization (DySMHO), a scalable machine learning framework for identifying governing laws in the form of differential equations from large-scale noisy experimental data sets. DySMHO consists of a novel moving horizon dynamic optimization strategy that sequentially learns the underlying governing equations from a large dictionary of basis functions. The sequential nature of DySMHO allows leveraging statistical arguments for eliminating irrelevant basis functions, avoiding overfitting to recover accurate and parsimonious forms of the governing equations. Canonical nonlinear dynamical system examples are used to demonstrate that DySMHO can accurately recover the governing laws, is robust to high levels of measurement noise and that it can handle challenges such as multiple time scale dynamics.

</p>
</details>

<details><summary><b>A common variable minimax theorem for graphs</b>
<a href="https://arxiv.org/abs/2107.14747">arxiv:2107.14747</a>
&#x1F4C8; 1 <br>
<p>Ronald R. Coifman, Nicholas F. Marshall, Stefan Steinerberger</p></summary>
<p>

**Abstract:** Let $\mathcal{G} = \{G_1 = (V, E_1), \dots, G_m = (V, E_m)\}$ be a collection of $m$ graphs defined on a common set of vertices $V$ but with different edge sets $E_1, \dots, E_m$. Informally, a function $f :V \rightarrow \mathbb{R}$ is smooth with respect to $G_k = (V,E_k)$ if $f(u) \sim f(v)$ whenever $(u, v) \in E_k$. We study the problem of understanding whether there exists a nonconstant function that is smooth with respect to all graphs in $\mathcal{G}$, simultaneously, and how to find it if it exists.

</p>
</details>

<details><summary><b>Can You Hear It? Backdoor Attacks via Ultrasonic Triggers</b>
<a href="https://arxiv.org/abs/2107.14569">arxiv:2107.14569</a>
&#x1F4C8; 1 <br>
<p>Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek</p></summary>
<p>

**Abstract:** Deep neural networks represent a powerful approach for many real-world applications due to their ability to model even complex data relations. However, such neural networks can also be prohibitively expensive to train, making it common to either outsource the training process to third parties or use pretrained neural networks. Unfortunately, such practices make neural networks vulnerable to various attacks, where one attack is the backdoor attack. In such an attack, the third party training the model may maliciously inject hidden behaviors into the model. Then, if a particular input (called trigger) is fed into a neural network, the network will respond with a wrong result.
  In this work, we explore backdoor attacks for automatic speech recognition systems where we inject inaudible triggers. By doing so, we make the backdoor attack challenging to detect for legitimate users, and thus, potentially more dangerous. We conduct experiments on two versions of a dataset and three neural networks and explore the performance of our attack concerning the duration, position, and type of the trigger. Our results indicate that less than 1% of poisoned data is sufficient to deploy a backdoor attack and reach a 100% attack success rate. Since the trigger is inaudible, it makes it without limitations with respect to the duration of the signal, and we observed that even short, non-continuous triggers result in highly successful attacks. Finally, we conducted our attack in actual hardware and saw that a malicious party could manipulate inference in an Android application by playing the inaudible trigger over the air.

</p>
</details>

<details><summary><b>Topological Similarity Index and Loss Function for Blood Vessel Segmentation</b>
<a href="https://arxiv.org/abs/2107.14531">arxiv:2107.14531</a>
&#x1F4C8; 1 <br>
<p>R. J. Araújo, J. S. Cardoso, H. P. Oliveira</p></summary>
<p>

**Abstract:** Blood vessel segmentation is one of the most studied topics in computer vision, due to its relevance in daily clinical practice. Despite the evolution the field has been facing, especially after the dawn of deep learning, important challenges are still not solved. One of them concerns the consistency of the topological properties of the vascular trees, given that the best performing methodologies do not directly penalize mistakes such as broken segments and end up producing predictions with disconnected trees. This is particularly relevant in graph-like structures, such as blood vessel trees, given that it puts at risk the characterization steps that follow the segmentation task. In this paper, we propose a similarity index which captures the topological consistency of the predicted segmentations having as reference the ground truth. We also design a novel loss function based on the morphological closing operator and show how it allows to learn deep neural network models which produce more topologically coherent masks. Our experiments target well known retinal benchmarks and a coronary angiogram database.

</p>
</details>

<details><summary><b>Single image deep defocus estimation and its applications</b>
<a href="https://arxiv.org/abs/2107.14443">arxiv:2107.14443</a>
&#x1F4C8; 1 <br>
<p>Fernando J. Galetto, Guang Deng</p></summary>
<p>

**Abstract:** Depth information is useful in many image processing applications. However, since taking a picture is a process of projection of a 3D scene onto a 2D imaging sensor, the depth information is embedded in the image. Extracting the depth information from the image is a challenging task. A guiding principle is that the level of blurriness due to defocus is related to the distance between the object and the focal plane. Based on this principle and the widely used assumption that Gaussian blur is a good model for defocus blur, we formulate the problem of estimating the spatially varying defocus blurriness as a Gaussian blur classification problem. We solved the problem by training a deep neural network to classify image patches into one of the 20 levels of blurriness. We have created a dataset of more than 500000 image patches of size $32\times32$ which are used to train and test several well-known network models. We find that MobileNetV2 is suitable for this application due to its low memory requirement and high accuracy. The trained model is used to determine the patch blurriness which is then refined by applying an iterative weighted guided filter. The result is a defocus map that carries the information of the degree of blurriness for each pixel. We compare the proposed method with state-of-the-art techniques and we demonstrate its successful applications in adaptive image enhancement, defocus magnification, and multi-focus image fusion.

</p>
</details>

<details><summary><b>Sensing and Mapping for Better Roads: Initial Plan for Using Federated Learning and Implementing a Digital Twin to Identify the Road Conditions in a Developing Country -- Sri Lanka</b>
<a href="https://arxiv.org/abs/2107.14551">arxiv:2107.14551</a>
&#x1F4C8; 0 <br>
<p>Thilanka Munasinghe, HR Pasindu</p></summary>
<p>

**Abstract:** We propose how a developing country like Sri Lanka can benefit from privacy-enabled machine learning techniques such as Federated Learning to detect road conditions using crowd-sourced data collection and proposed the idea of implementing a Digital Twin for the national road system in Sri Lanka. Developing countries such as Sri Lanka are far behind in implementing smart road systems and smart cities compared to the developed countries. The proposed work discussed in this paper matches the UN Sustainable Development Goal (SDG) 9: "Build Resilient Infrastructure, Promote Inclusive and Sustainable Industrialization and Foster Innovation". Our proposed work discusses how the government and private sector vehicles that conduct routine trips to collect crowd-sourced data using smartphone devices to identify the road conditions and detect where the potholes, surface unevenness (roughness), and other major distresses are located on the roads. We explore Mobile Edge Computing (MEC) techniques that can bring machine learning intelligence closer to the edge devices where produced data is stored and show how the applications of Federated Learning can be made to detect and improve road conditions. During the second phase of this study, we plan to implement a Digital Twin for the road system in Sri Lanka. We intend to use data provided by both Dedicated and Non-Dedicated systems in the proposed Digital Twin for the road system. As of writing this paper, and best to our knowledge, there is no Digital Twin system implemented for roads and other infrastructure systems in Sri Lanka. The proposed Digital Twin will be one of the first implementations of such systems in Sri Lanka. Lessons learned from this pilot project will benefit other developing countries who wish to follow the same path and make data-driven decisions.

</p>
</details>

<details><summary><b>Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images</b>
<a href="https://arxiv.org/abs/2107.14449">arxiv:2107.14449</a>
&#x1F4C8; 0 <br>
<p>Adrià Casamitjana, Matteo Mancini, Juan Eugenio Iglesias</p></summary>
<p>

**Abstract:** Nonlinear inter-modality registration is often challenging due to the lack of objective functions that are good proxies for alignment. Here we propose a synthesis-by-registration method to convert this problem into an easier intra-modality task. We introduce a registration loss for weakly supervised image translation between domains that does not require perfectly aligned training data. This loss capitalises on a registration U-Net with frozen weights, to drive a synthesis CNN towards the desired translation. We complement this loss with a structure preserving constraint based on contrastive learning, which prevents blurring and content shifts due to overfitting. We apply this method to the registration of histological sections to MRI slices, a key step in 3D histology reconstruction. Results on two different public datasets show improvements over registration based on mutual information (13% reduction in landmark error) and synthesis-based algorithms such as CycleGAN (11% reduction), and are comparable to a registration CNN with label supervision. Code and data are publicly available at \url{https://github.com/acasamitjana/SynthByReg}

</p>
</details>


[Next Page](2021/2021-07/2021-07-29.md)
