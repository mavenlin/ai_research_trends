Prev: [2022.03.07]({{ '/2022/03/07/2022.03.07.html' | relative_url }})  Next: [2022.03.09]({{ '/2022/03/09/2022.03.09.html' | relative_url }})
{% raw %}
## Summary for 2022-03-08, created on 2022-03-18


<details><summary><b>Diffusion Models for Medical Anomaly Detection</b>
<a href="https://arxiv.org/abs/2203.04306">arxiv:2203.04306</a>
&#x1F4C8; 36 <br>
<p>Julia Wolleb, Florentin Bieder, Robin Sandk√ºhler, Philippe C. Cattin</p></summary>
<p>

**Abstract:** In medical applications, weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for image-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.

</p>
</details>

<details><summary><b>E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation</b>
<a href="https://arxiv.org/abs/2203.04074">arxiv:2203.04074</a>
&#x1F4C8; 23 <br>
<p>Tao Zhang, Shiqing Wei, Shunping Ji</p></summary>
<p>

**Abstract:** Contour-based instance segmentation methods have developed rapidly recently but feature rough and hand-crafted front-end contour initialization, which restricts the model performance, and an empirical and fixed backend predicted-label vertex pairing, which contributes to the learning difficulty. In this paper, we introduce a novel contour-based method, named E2EC, for high-quality instance segmentation. Firstly, E2EC applies a novel learnable contour initialization architecture instead of hand-crafted contour initialization. This consists of a contour initialization module for constructing more explicit learning goals and a global contour deformation module for taking advantage of all of the vertices' features better. Secondly, we propose a novel label sampling scheme, named multi-direction alignment, to reduce the learning difficulty. Thirdly, to improve the quality of the boundary details, we dynamically match the most appropriate predicted-ground truth vertex pairs and propose the corresponding loss function named dynamic matching loss. The experiments showed that E2EC can achieve a state-of-the-art performance on the KITTI INStance (KINS) dataset, the Semantic Boundaries Dataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for use in real-time applications, with an inference speed of 36 fps for 512*512 images on an NVIDIA A6000 GPU. Code will be released at https://github.com/zhang-tao-whu/e2ec.

</p>
</details>

<details><summary><b>Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification</b>
<a href="https://arxiv.org/abs/2203.04961">arxiv:2203.04961</a>
&#x1F4C8; 21 <br>
<p>Zuzanna Szafranowska, Richard Osuala, Bennet Breier, Kaisar Kushibar, Karim Lekadir, Oliver Diaz</p></summary>
<p>

**Abstract:** Early detection of breast cancer in mammography screening via deep-learning based computer-aided detection systems shows promising potential in improving the curability and mortality rates of breast cancer. However, many clinical centres are restricted in the amount and heterogeneity of available data to train such models to (i) achieve promising performance and to (ii) generalise well across acquisition protocols and domains. As sharing data between centres is restricted due to patient privacy concerns, we propose a potential solution: sharing trained generative models between centres as substitute for real patient data. In this work, we use three well known mammography datasets to simulate three different centres, where one centre receives the trained generator of Generative Adversarial Networks (GANs) from the two remaining centres in order to augment the size and heterogeneity of its training dataset. We evaluate the utility of this approach on mammography patch classification on the test set of the GAN-receiving centre using two different classification models, (a) a convolutional neural network and (b) a transformer neural network. Our experiments demonstrate that shared GANs notably increase the performance of both transformer and convolutional classification models and highlight this approach as a viable alternative to inter-centre data sharing.

</p>
</details>

<details><summary><b>Reproducible Subjective Evaluation</b>
<a href="https://arxiv.org/abs/2203.04444">arxiv:2203.04444</a>
&#x1F4C8; 18 <br>
<p>Max Morrison, Brian Tang, Gefei Tan, Bryan Pardo</p></summary>
<p>

**Abstract:** Human perceptual studies are the gold standard for the evaluation of many research tasks in machine learning, linguistics, and psychology. However, these studies require significant time and cost to perform. As a result, many researchers use objective measures that can correlate poorly with human evaluation. When subjective evaluations are performed, they are often not reported with sufficient detail to ensure reproducibility. We propose Reproducible Subjective Evaluation (ReSEval), an open-source framework for quickly deploying crowdsourced subjective evaluations directly from Python. ReSEval lets researchers launch A/B, ABX, Mean Opinion Score (MOS) and MUltiple Stimuli with Hidden Reference and Anchor (MUSHRA) tests on audio, image, text, or video data from a command-line interface or using one line of Python, making it as easy to run as objective evaluation. With ReSEval, researchers can reproduce each other's subjective evaluations by sharing a configuration file and the audio, image, text, or video files.

</p>
</details>

<details><summary><b>An Uncommon Task: Participatory Design in Legal AI</b>
<a href="https://arxiv.org/abs/2203.06246">arxiv:2203.06246</a>
&#x1F4C8; 10 <br>
<p>Fernando Delgado, Solon Barocas, Karen Levy</p></summary>
<p>

**Abstract:** Despite growing calls for participation in AI design, there are to date few empirical studies of what these processes look like and how they can be structured for meaningful engagement with domain experts. In this paper, we examine a notable yet understudied AI design process in the legal domain that took place over a decade ago, the impact of which still informs legal automation efforts today. Specifically, we examine the design and evaluation activities that took place from 2006 to 2011 within the TeXT Retrieval Conference's (TREC) Legal Track, a computational research venue hosted by the National Institute of Standards and Technologies. The Legal Track of TREC is notable in the history of AI research and practice because it relied on a range of participatory approaches to facilitate the design and evaluation of new computational techniques--in this case, for automating attorney document review for civil litigation matters. Drawing on archival research and interviews with coordinators of the Legal Track of TREC, our analysis reveals how an interactive simulation methodology allowed computer scientists and lawyers to become co-designers and helped bridge the chasm between computational research and real-world, high-stakes litigation practice. In analyzing this case from the recent past, our aim is to empirically ground contemporary critiques of AI development and evaluation and the calls for greater participation as a means to address them.

</p>
</details>

<details><summary><b>Selective-Supervised Contrastive Learning with Noisy Labels</b>
<a href="https://arxiv.org/abs/2203.04181">arxiv:2203.04181</a>
&#x1F4C8; 9 <br>
<p>Shikun Li, Xiaobo Xia, Shiming Ge, Tongliang Liu</p></summary>
<p>

**Abstract:** Deep networks have strong capacities of embedding data into latent representations and finishing following tasks. However, the capacities largely come from high-quality annotated labels, which are expensive to collect. Noisy labels are more affordable, but result in corrupted representations, leading to poor generalization performance. To learn robust representations and handle noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in this paper. Specifically, Sel-CL extend supervised contrastive learning (Sup-CL), which is powerful in representation learning, but is degraded when there are noisy labels. Sel-CL tackles the direct cause of the problem of Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs built by noisy labels mislead representation learning. To alleviate the issue, we select confident pairs out of noisy ones for Sup-CL without knowing noise rates. In the selection process, by measuring the agreement between learned representations and given labels, we first identify confident examples that are exploited to build confident pairs. Then, the representation similarity distribution in the built confident pairs is exploited to identify more confident pairs out of noisy pairs. All obtained confident pairs are finally used for Sup-CL to enhance representations. Experiments on multiple noisy datasets demonstrate the robustness of the learned representations by our method, following the state-of-the-art performance. Source codes are available at https://github.com/ShikunLi/Sel-CL

</p>
</details>

<details><summary><b>On Generalizing Beyond Domains in Cross-Domain Continual Learning</b>
<a href="https://arxiv.org/abs/2203.03970">arxiv:2203.03970</a>
&#x1F4C8; 9 <br>
<p>Christian Simon, Masoud Faraki, Yi-Hsuan Tsai, Xiang Yu, Samuel Schulter, Yumin Suh, Mehrtash Harandi, Manmohan Chandraker</p></summary>
<p>

**Abstract:** Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks often suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome.

</p>
</details>

<details><summary><b>Locate This, Not That: Class-Conditioned Sound Event DOA Estimation</b>
<a href="https://arxiv.org/abs/2203.04197">arxiv:2203.04197</a>
&#x1F4C8; 8 <br>
<p>Olga Slizovskaia, Gordon Wichern, Zhong-Qiu Wang, Jonathan Le Roux</p></summary>
<p>

**Abstract:** Existing systems for sound event localization and detection (SELD) typically operate by estimating a source location for all classes at every time instant. In this paper, we propose an alternative class-conditioned SELD model for situations where we may not be interested in localizing all classes all of the time. This class-conditioned SELD model takes as input the spatial and spectral features from the sound file, and also a one-hot vector indicating the class we are currently interested in localizing. We inject the conditioning information at several points in our model using feature-wise linear modulation (FiLM) layers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the proposed class-conditioned SELD model performs better in terms of common SELD metrics than the baseline model that locates all classes simultaneously, and also outperforms specialist models that are trained to locate only a single class of interest. We also evaluate performance on the DCASE 2021 Task 3 dataset, which includes directional interference (sound events from classes we are not interested in localizing) and notice especially strong improvement from the class-conditioned model.

</p>
</details>

<details><summary><b>Motron: Multimodal Probabilistic Human Motion Forecasting</b>
<a href="https://arxiv.org/abs/2203.04132">arxiv:2203.04132</a>
&#x1F4C8; 8 <br>
<p>Tim Salzmann, Marco Pavone, Markus Ryll</p></summary>
<p>

**Abstract:** Autonomous systems and humans are increasingly sharing the same space. Robots work side by side or even hand in hand with humans to balance each other's limitations. Such cooperative interactions are ever more sophisticated. Thus, the ability to reason not just about a human's center of gravity position, but also its granular motion is an important prerequisite for human-robot interaction. Though, many algorithms ignore the multimodal nature of humans or neglect uncertainty in their motion forecasts. We present Motron, a multimodal, probabilistic, graph-structured model, that captures human's multimodality using probabilistic methods while being able to output deterministic motions and corresponding confidence values for each mode. Our model aims to be tightly integrated with the robotic planning-control-interaction loop; outputting physically feasible human motions and being computationally efficient. We demonstrate the performance of our model on several challenging real-world motion forecasting datasets, outperforming a wide array of generative methods while providing state-of-the-art deterministic motions if required. Both using significantly less computational power than state-of-the art algorithms.

</p>
</details>

<details><summary><b>Occupancy Flow Fields for Motion Forecasting in Autonomous Driving</b>
<a href="https://arxiv.org/abs/2203.03875">arxiv:2203.03875</a>
&#x1F4C8; 8 <br>
<p>Reza Mahjourian, Jinkyu Kim, Yuning Chai, Mingxing Tan, Ben Sapp, Dragomir Anguelov</p></summary>
<p>

**Abstract:** We propose Occupancy Flow Fields, a new representation for motion forecasting of multiple agents, an important task in autonomous driving. Our representation is a spatio-temporal grid with each grid cell containing both the probability of the cell being occupied by any agent, and a two-dimensional flow vector representing the direction and magnitude of the motion in that cell. Our method successfully mitigates shortcomings of the two most commonly-used representations for motion forecasting: trajectory sets and occupancy grids. Although occupancy grids efficiently represent the probabilistic location of many agents jointly, they do not capture agent motion and lose the agent identities. To this end, we propose a deep learning architecture that generates Occupancy Flow Fields with the help of a new flow trace loss that establishes consistency between the occupancy and flow predictions. We demonstrate the effectiveness of our approach using three metrics on occupancy prediction, motion estimation, and agent ID recovery. In addition, we introduce the problem of predicting speculative agents, which are currently-occluded agents that may appear in the future through dis-occlusion or by entering the field of view. We report experimental results on a large in-house autonomous driving dataset and the public INTERACTION dataset, and show that our model outperforms state-of-the-art models.

</p>
</details>

<details><summary><b>Breast cancer detection using artificial intelligence techniques: A systematic literature review</b>
<a href="https://arxiv.org/abs/2203.04308">arxiv:2203.04308</a>
&#x1F4C8; 7 <br>
<p>Ali Bou Nassif, Manar Abu Talib, Qassim Nasir, Yaman Afadar, Omar Elgendy</p></summary>
<p>

**Abstract:** Cancer is one of the most dangerous diseases to humans, and yet no permanent cure has been developed for it. Breast cancer is one of the most common cancer types. According to the National Breast Cancer foundation, in 2020 alone, more than 276,000 new cases of invasive breast cancer and more than 48,000 non-invasive cases were diagnosed in the US. To put these figures in perspective, 64% of these cases are diagnosed early in the disease's cycle, giving patients a 99% chance of survival. Artificial intelligence and machine learning have been used effectively in detection and treatment of several dangerous diseases, helping in early diagnosis and treatment, and thus increasing the patient's chance of survival. Deep learning has been designed to analyze the most important features affecting detection and treatment of serious diseases. For example, breast cancer can be detected using genes or histopathological imaging. Analysis at the genetic level is very expensive, so histopathological imaging is the most common approach used to detect breast cancer. In this research work, we systematically reviewed previous work done on detection and treatment of breast cancer using genetic sequencing or histopathological imaging with the help of deep learning and machine learning. We also provide recommendations to researchers who will work in this field

</p>
</details>

<details><summary><b>UENAS: A Unified Evolution-based NAS Framework</b>
<a href="https://arxiv.org/abs/2203.04300">arxiv:2203.04300</a>
&#x1F4C8; 7 <br>
<p>Zimian Wei, Hengyue Pan, Xin Niu, Peijie Dong, Dongsheng Li</p></summary>
<p>

**Abstract:** Neural architecture search (NAS) has gained significant attention for automatic network design in recent years. Previous NAS methods suffer from limited search spaces, which may lead to sub-optimal results. In this paper, we propose UENAS, an evolution-based NAS framework with a broader search space that supports optimizing network architectures, pruning strategies, and hyperparameters simultaneously. To alleviate the huge search cost caused by the expanded search space, three strategies are adopted: First, an adaptive pruning strategy that iteratively trims the average model size in the population without compromising performance. Second, child networks share weights of overlapping layers with pre-trained parent networks to reduce the training epochs. Third, an online predictor scores the joint representations of architecture, pruning strategy, and hyperparameters to filter out inferior combos. By the proposed three strategies, the search efficiency is significantly improved and more well-performed compact networks with tailored hyper-parameters are derived. In experiments, UENAS achieves error rates of 2.81% on CIFAR-10, 20.24% on CIFAR-100, and 33% on Tiny-ImageNet, which shows the effectiveness of our method.

</p>
</details>

<details><summary><b>Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping</b>
<a href="https://arxiv.org/abs/2203.04299">arxiv:2203.04299</a>
&#x1F4C8; 7 <br>
<p>Yunxiang Li, Ruilong Dan, Shuai Wang, Yifan Cao, Xiangde Luo, Chenghao Tan, Gangyong Jia, Huiyu Zhou, Yaqi Wang, Li Wang</p></summary>
<p>

**Abstract:** Skull stripping is a crucial prerequisite step in the analysis of brain magnetic resonance (MR) images. Although many excellent works or tools have been proposed, they suffer from low generalization capability. For instance, the model trained on a dataset with specific imaging parameters (source domain) cannot be well applied to other datasets with different imaging parameters (target domain). Especially, for the lifespan datasets, the model trained on an adult dataset is not applicable to an infant dataset due to the large domain difference. To address this issue, numerous domain adaptation (DA) methods have been proposed to align the extracted features between the source and target domains, requiring concurrent access to the input images of both domains. Unfortunately, it is problematic to share the images due to privacy. In this paper, we design a source-free domain adaptation framework (SDAF) for multi-site and lifespan skull stripping that can accomplish domain adaptation without access to source domain images. Our method only needs to share the source labels as shape dictionaries and the weights trained on the source data, without disclosing private information from source domain subjects. To deal with the domain shift between multi-site lifespan datasets, we take advantage of the brain shape prior which is invariant to imaging parameters and ages. Experiments demonstrate that our framework can significantly outperform the state-of-the-art methods on multi-site lifespan datasets.

</p>
</details>

<details><summary><b>Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data</b>
<a href="https://arxiv.org/abs/2203.04218">arxiv:2203.04218</a>
&#x1F4C8; 7 <br>
<p>Minori Toyoda, Kanata Suzuki, Yoshihiko Hayashi, Tetsuya Ogata</p></summary>
<p>

**Abstract:** This study achieved bidirectional translation between descriptions and actions using small paired data. The ability to mutually generate descriptions and actions is essential for robots to collaborate with humans in their daily lives. The robot is required to associate real-world objects with linguistic expressions, and large-scale paired data are required for machine learning approaches. However, a paired dataset is expensive to construct and difficult to collect. This study proposes a two-stage training method for bidirectional translation. In the proposed method, we train recurrent autoencoders (RAEs) for descriptions and actions with a large amount of non-paired data. Then, we fine-tune the entire model to bind their intermediate representations using small paired data. Because the data used for pre-training do not require pairing, behavior-only data or a large language corpus can be used. We experimentally evaluated our method using a paired dataset consisting of motion-captured actions and descriptions. The results showed that our method performed well, even when the amount of paired data to train was small. The visualization of the intermediate representations of each RAE showed that similar actions were encoded in a clustered position and the corresponding feature vectors well aligned.

</p>
</details>

<details><summary><b>Robot Learning of Mobile Manipulation with Reachability Behavior Priors</b>
<a href="https://arxiv.org/abs/2203.04051">arxiv:2203.04051</a>
&#x1F4C8; 7 <br>
<p>Snehal Jauhri, Jan Peters, Georgia Chalvatzaki</p></summary>
<p>

**Abstract:** Mobile Manipulation (MM) systems are ideal candidates for taking up the role of a personal assistant in unstructured real-world environments. Among other challenges, MM requires effective coordination of the robot's embodiments for executing tasks that require both mobility and manipulation. Reinforcement Learning (RL) holds the promise of endowing robots with adaptive behaviors, but most methods require prohibitively large amounts of data for learning a useful control policy. In this work, we study the integration of robotic reachability priors in actor-critic RL methods for accelerating the learning of MM for reaching and fetching tasks. Namely, we consider the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target. For this, we devise a novel Hybrid RL method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization. Next, we train a reachability prior using data from the operational robot workspace, inspired by classical methods. Subsequently, we derive Boosted Hybrid RL (BHyRL), a novel algorithm for learning Q-functions by modeling them as a sum of residual approximators. Every time a new task needs to be learned, we can transfer our learned residuals and learn the component of the Q-function that is task-specific, hence, maintaining the task structure from prior behaviors. Moreover, we find that regularizing the target policy with a prior policy yields more expressive behaviors. We evaluate our method in simulation in reaching and fetching tasks of increasing difficulty, and we show the superior performance of BHyRL against baseline methods. Finally, we zero-transfer our learned 6D fetching policy with BHyRL to our MM robot TIAGo++. For more details and code release, please refer to our project site: irosalab.com/rlmmbp

</p>
</details>

<details><summary><b>DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos</b>
<a href="https://arxiv.org/abs/2203.03996">arxiv:2203.03996</a>
&#x1F4C8; 7 <br>
<p>Mathias Parger, Chengcheng Tang, Christopher D. Twigg, Cem Keskin, Robert Wang, Markus Steinberger</p></summary>
<p>

**Abstract:** Convolutional neural network inference on video data requires powerful hardware for real-time processing. Given the inherent coherence across consecutive frames, large parts of a video typically change little. By skipping identical image regions and truncating insignificant pixel updates, computational redundancy can in theory be reduced significantly. However, these theoretical savings have been difficult to translate into practice, as sparse updates hamper computational consistency and memory access coherence; which are key for efficiency on real hardware. With DeltaCNN, we present a sparse convolutional neural network framework that enables sparse frame-by-frame updates to accelerate video inference in practice. We provide sparse implementations for all typical CNN layers and propagate sparse feature updates end-to-end - without accumulating errors over time. DeltaCNN is applicable to all convolutional neural networks without retraining. To the best of our knowledge, we are the first to significantly outperform the dense reference, cuDNN, in practical settings, achieving speedups of up to 7x with only marginal differences in accuracy.

</p>
</details>

<details><summary><b>Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with Pre-Segmentation and IoU Region Merging</b>
<a href="https://arxiv.org/abs/2203.03886">arxiv:2203.03886</a>
&#x1F4C8; 7 <br>
<p>Moritz Zink, Martin Schiele, Pengcheng Fan, Stephan Gasterst√§dt</p></summary>
<p>

**Abstract:** Mask R-CNN has recently achieved great success in the field of instance segmentation. However, weaknesses of the algorithm have been repeatedly pointed out as well, especially in the segmentation of long, sparse objects whose orientation is not exclusively horizontal or vertical. We present here an approach that significantly improves the performance of the algorithm by first pre-segmenting the images with a PSPNet algorithm. To further improve its prediction, we have developed our own cost functions and heuristics in the form of training strategies, which can prevent so-called (early) overfitting and achieve a more targeted convergence. Furthermore, due to the high variance of the images, especially for PSPNet, we aimed to develop strategies for a high robustness and generalization, which are also presented here.

</p>
</details>

<details><summary><b>Part-level Action Parsing via a Pose-guided Coarse-to-Fine Framework</b>
<a href="https://arxiv.org/abs/2203.04476">arxiv:2203.04476</a>
&#x1F4C8; 6 <br>
<p>Xiaodong Chen, Xinchen Liu, Wu Liu, Kun Liu, Dong Wu, Yongdong Zhang, Tao Mei</p></summary>
<p>

**Abstract:** Action recognition from videos, i.e., classifying a video into one of the pre-defined action types, has been a popular topic in the communities of artificial intelligence, multimedia, and signal processing. However, existing methods usually consider an input video as a whole and learn models, e.g., Convolutional Neural Networks (CNNs), with coarse video-level class labels. These methods can only output an action class for the video, but cannot provide fine-grained and explainable cues to answer why the video shows a specific action. Therefore, researchers start to focus on a new task, Part-level Action Parsing (PAP), which aims to not only predict the video-level action but also recognize the frame-level fine-grained actions or interactions of body parts for each person in the video. To this end, we propose a coarse-to-fine framework for this challenging task. In particular, our framework first predicts the video-level class of the input video, then localizes the body parts and predicts the part-level action. Moreover, to balance the accuracy and computation in part-level action parsing, we propose to recognize the part-level actions by segment-level features. Furthermore, to overcome the ambiguity of body parts, we propose a pose-guided positional embedding method to accurately localize body parts. Through comprehensive experiments on a large-scale dataset, i.e., Kinetics-TPS, our framework achieves state-of-the-art performance and outperforms existing methods over a 31.10% ROC score.

</p>
</details>

<details><summary><b>Variational Inference with Locally Enhanced Bounds for Hierarchical Models</b>
<a href="https://arxiv.org/abs/2203.04432">arxiv:2203.04432</a>
&#x1F4C8; 6 <br>
<p>Tomas Geffner, Justin Domke</p></summary>
<p>

**Abstract:** Hierarchical models represent a challenging setting for inference algorithms. MCMC methods struggle to scale to large models with many local variables and observations, and variational inference (VI) may fail to provide accurate approximations due to the use of simple variational families. Some variational methods (e.g. importance weighted VI) integrate Monte Carlo methods to give better accuracy, but these tend to be unsuitable for hierarchical models, as they do not allow for subsampling and their performance tends to degrade for high dimensional models. We propose a new family of variational bounds for hierarchical models, based on the application of tightening methods (e.g. importance weighting) separately for each group of local random variables. We show that our approach naturally allows the use of subsampling to get unbiased gradients, and that it fully leverages the power of methods that build tighter lower bounds by applying them independently in lower dimensional spaces, leading to better results and more accurate posterior approximations than relevant baselines.

</p>
</details>

<details><summary><b>Cluster Head Detection for Hierarchical UAV Swarm With Graph Self-supervised Learning</b>
<a href="https://arxiv.org/abs/2203.04311">arxiv:2203.04311</a>
&#x1F4C8; 6 <br>
<p>Zhiyu Mou, Jun Liu, Xiang Yun, Feifei Gao, Qihui Wu</p></summary>
<p>

**Abstract:** In this paper, we study the cluster head detection problem of a two-level unmanned aerial vehicle (UAV) swarm network (USNET) with multiple UAV clusters, where the inherent follow strategy (IFS) of low-level follower UAVs (FUAVs) with respect to high-level cluster head UAVs (HUAVs) is unknown. We first propose a graph attention self-supervised learning algorithm (GASSL) to detect the HUAVs of a single UAV cluster, where the GASSL can fit the IFS at the same time. Then, to detect the HUAVs in the USNET with multiple UAV clusters, we develop a multi-cluster graph attention self-supervised learning algorithm (MC-GASSL) based on the GASSL. The MC-GASSL clusters the USNET with a gated recurrent unit (GRU)-based metric learning scheme and finds the HUAVs in each cluster with GASSL. Numerical results show that the GASSL can detect the HUAVs in single UAV clusters obeying various kinds of IFSs with over 98% average accuracy. The simulation results also show that the clustering purity of the USNET with MC-GASSL exceeds that with traditional clustering algorithms by at least 10% average. Furthermore, the MC-GASSL can efficiently detect all the HUAVs in USNETs with various IFSs and cluster numbers with low detection redundancies.

</p>
</details>

<details><summary><b>A Gating Model for Bias Calibration in Generalized Zero-shot Learning</b>
<a href="https://arxiv.org/abs/2203.04195">arxiv:2203.04195</a>
&#x1F4C8; 6 <br>
<p>Gukyeong Kwon, Ghassan AlRegib</p></summary>
<p>

**Abstract:** Generalized zero-shot learning (GZSL) aims at training a model that can generalize to unseen class data by only using auxiliary information. One of the main challenges in GZSL is a biased model prediction toward seen classes caused by overfitting on only available seen class data during training. To overcome this issue, we propose a two-stream autoencoder-based gating model for GZSL. Our gating model predicts whether the query data is from seen classes or unseen classes, and utilizes separate seen and unseen experts to predict the class independently from each other. This framework avoids comparing the biased prediction scores for seen classes with the prediction scores for unseen classes. In particular, we measure the distance between visual and attribute representations in the latent space and the cross-reconstruction space of the autoencoder. These distances are utilized as complementary features to characterize unseen classes at different levels of data abstraction. Also, the two-stream autoencoder works as a unified framework for the gating model and the unseen expert, which makes the proposed method computationally efficient. We validate our proposed method in four benchmark image recognition datasets. In comparison with other state-of-the-art methods, we achieve the best harmonic mean accuracy in SUN and AWA2, and the second best in CUB and AWA1. Furthermore, our base model requires at least 20% less number of model parameters than state-of-the-art methods relying on generative models.

</p>
</details>

<details><summary><b>Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation</b>
<a href="https://arxiv.org/abs/2203.03910">arxiv:2203.03910</a>
&#x1F4C8; 6 <br>
<p>Chenze Shao, Yang Feng</p></summary>
<p>

**Abstract:** Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions. This problem is called \textit{catastrophic forgetting}, which is a fundamental challenge in the continual learning of neural networks. In this work, we observe that catastrophic forgetting not only occurs in continual learning but also affects the traditional static training. Neural networks, especially neural machine translation models, suffer from catastrophic forgetting even if they learn from a static training set. To be specific, the final model pays imbalanced attention to training samples, where recently exposed samples attract more attention than earlier samples. The underlying cause is that training samples do not get balanced training in each model update, so we name this problem \textit{imbalanced training}. To alleviate this problem, we propose Complementary Online Knowledge Distillation (COKD), which uses dynamically updated teacher models trained on specific data orders to iteratively provide complementary knowledge to the student model. Experimental results on multiple machine translation tasks show that our method successfully alleviates the problem of imbalanced training and achieves substantial improvements over strong baseline systems.

</p>
</details>

<details><summary><b>Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4</b>
<a href="https://arxiv.org/abs/2203.06649">arxiv:2203.06649</a>
&#x1F4C8; 5 <br>
<p>William Berrios, Arturo Deza</p></summary>
<p>

**Abstract:** Modern high-scoring models of vision in the brain score competition do not stem from Vision Transformers. However, in this short paper, we provide evidence against the unexpected trend of Vision Transformers (ViT) being not perceptually aligned with human visual representations by showing how a dual-stream Transformer, a CrossViT$~\textit{a la}$ Chen et al. (2021), under a joint rotationally-invariant and adversarial optimization procedure yields 2nd place in the aggregate Brain-Score 2022 competition averaged across all visual categories, and currently (March 1st, 2022) holds the 1st place for the highest explainable variance of area V4. In addition, our current Transformer-based model also achieves greater explainable variance for areas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like computation module(Dapello et al.,2020). Our team was also the only entry in the top-5 that shows a positive rank correlation between explained variance per area and depth in the visual hierarchy.
  Against our initial expectations, these results provide tentative support for an $\textit{"All roads lead to Rome"}$ argument enforced via a joint optimization rule even for non biologically-motivated models of vision such as Vision Transformers.

</p>
</details>

<details><summary><b>Predicting conversion of mild cognitive impairment to Alzheimer's disease</b>
<a href="https://arxiv.org/abs/2203.04725">arxiv:2203.04725</a>
&#x1F4C8; 5 <br>
<p>Yiran Wei, Stephen J. Price, Carola-Bibiane Sch√∂nlieb, Chao Li</p></summary>
<p>

**Abstract:** Alzheimer's disease (AD) is the most common age-related dementia. Mild cognitive impairment (MCI) is the early stage of cognitive decline before AD. It is crucial to predict the MCI-to-AD conversion for precise management, which remains challenging due to the diversity of patients. Previous evidence shows that the brain network generated from diffusion MRI promises to classify dementia using deep learning. However, the limited availability of diffusion MRI challenges the model training. In this study, we develop a self-supervised contrastive learning approach to generate structural brain networks from routine anatomical MRI under the guidance of diffusion MRI. The generated brain networks are applied to train a learning framework for predicting the MCI-to-AD conversion. Instead of directly modelling the AD brain networks, we train a graph encoder and a variational autoencoder to model the healthy ageing trajectories from brain networks of healthy controls. To predict the MCI-to-AD conversion, we further design a recurrent neural networks based approach to model the longitudinal deviation of patients' brain networks from the healthy ageing trajectory. Numerical results show that the proposed methods outperform the benchmarks in the prediction task. We also visualize the model interpretation to explain the prediction and identify abnormal changes of white matter tracts.

</p>
</details>

<details><summary><b>Boilerplate Detection via Semantic Classification of TextBlocks</b>
<a href="https://arxiv.org/abs/2203.04467">arxiv:2203.04467</a>
&#x1F4C8; 5 <br>
<p>Hao Zhang, Jie Wang</p></summary>
<p>

**Abstract:** We present a hierarchical neural network model called SemText to detect HTML boilerplate based on a novel semantic representation of HTML tags, class names, and text blocks. We train SemText on three published datasets of news webpages and fine-tune it using a small number of development data in CleanEval and GoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy on these datasets. We then demonstrate the robustness of SemText by showing that it also detects boilerplate effectively on out-of-domain community-based question-answer webpages.

</p>
</details>

<details><summary><b>The Flag Median and FlagIRLS</b>
<a href="https://arxiv.org/abs/2203.04437">arxiv:2203.04437</a>
&#x1F4C8; 5 <br>
<p>Nathan Mankovich, Emily King, Chris Peterson, Michael Kirby</p></summary>
<p>

**Abstract:** Finding prototypes (e.g., mean and median) for a dataset is central to a number of common machine learning algorithms. Subspaces have been shown to provide useful, robust representations for datasets of images, videos and more. Since subspaces correspond to points on a Grassmann manifold, one is led to consider the idea of a subspace prototype for a Grassmann-valued dataset. While a number of different subspace prototypes have been described, the calculation of some of these prototypes has proven to be computationally expensive while other prototypes are affected by outliers and produce highly imperfect clustering on noisy data. This work proposes a new subspace prototype, the flag median, and introduces the FlagIRLS algorithm for its calculation. We provide evidence that the flag median is robust to outliers and can be used effectively in algorithms like Linde-Buzo-Grey (LBG) to produce improved clusterings on Grassmannians. Numerical experiments include a synthetic dataset, the MNIST handwritten digits dataset, the Mind's Eye video dataset and the UCF YouTube action dataset. The flag median is compared the other leading algorithms for computing prototypes on the Grassmannian, namely, the $\ell_2$-median and to the flag mean. We find that using FlagIRLS to compute the flag median converges in $4$ iterations on a synthetic dataset. We also see that Grassmannian LBG with a codebook size of $20$ and using the flag median produces at least a $10\%$ improvement in cluster purity over Grassmannian LBG using the flag mean or $\ell_2$-median on the Mind's Eye dataset.

</p>
</details>

<details><summary><b>MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent</b>
<a href="https://arxiv.org/abs/2203.04317">arxiv:2203.04317</a>
&#x1F4C8; 5 <br>
<p>Soumick Chatterjee, Himanshi Bajaj, Istiyak H. Siddiquee, Nandish Bandi Subbarayappa, Steve Simon, Suraj Bangalore Shashidhar, Oliver Speck, Andreas N√ºrnberge</p></summary>
<p>

**Abstract:** Image registration is the process of bringing different images into a common coordinate system - a technique widely used in various applications of computer vision, such as remote sensing, image retrieval, and most commonly in medical imaging. Deep Learning based techniques have been applied successfully to tackle various complex medical image processing problems, including medical image registration. Over the years, several image registration techniques have been proposed using deep learning. Deformable image registration techniques such as Voxelmorph have been successful in capturing finer changes and providing smoother deformations. However, Voxelmorph, as well as ICNet and FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical view of the supplied image) and therefore can not track large deformations. In order to tackle the aforementioned problems, this paper extends the Voxelmorph approach in three different ways. To improve the performance in case of small as well as large deformations, supervision of the model at different resolutions have been integrated using a multi-scale UNet. To support the network to learn and encode the minute structural co-relations of the given image-pairs, a self-constructing graph network (SCGNet) has been used as the latent of the multi-scale UNet - which can improve the learning process of the model and help the model to generalise better. And finally, to make the deformations inverse-consistent, cycle consistency loss has been employed. On the task of registration of brain MRIs, the proposed method achieved significant improvements over ANTs and VoxelMorph, obtaining a Dice score of 0.8013$\pm$0.0243 for intramodal and 0.6211$\pm$0.0309 for intermodal, while VoxelMorph achieved 0.7747$\pm$0.0260 and 0.6071$\pm$0.0510, respectively.

</p>
</details>

<details><summary><b>Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap</b>
<a href="https://arxiv.org/abs/2203.04275">arxiv:2203.04275</a>
&#x1F4C8; 5 <br>
<p>Tae Ha Park, Simone D'Amico</p></summary>
<p>

**Abstract:** This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target domain images online at deployment. Specifically, ODR performs self-supervised entropy minimization of the predicted satellite foreground, thereby improving the CNN's performance on the target domain images without their pose labels and with minimal computational efforts. The GitHub repository for SPNv2 will be made available in the near future.

</p>
</details>

<details><summary><b>Follow the Water: Finding Water, Snow and Clouds on Terrestrial Exoplanets with Photometry and Machine Learning</b>
<a href="https://arxiv.org/abs/2203.04201">arxiv:2203.04201</a>
&#x1F4C8; 5 <br>
<p>Dang Pham, Lisa Kaltenegger</p></summary>
<p>

**Abstract:** All life on Earth needs water. NASA's quest to follow the water links water to the search for life in the cosmos. Telescopes like JWST and mission concepts like HabEx, LUVOIR and Origins are designed to characterise rocky exoplanets spectroscopically. However, spectroscopy remains time-intensive and therefore, initial characterisation is critical to prioritisation of targets.
  Here, we study machine learning as a tool to assess water's existence through broadband-filter reflected photometric flux on Earth-like exoplanets in three forms: seawater, water-clouds and snow; based on 53,130 spectra of cold, Earth-like planets with 6 major surfaces. XGBoost, a well-known machine learning algorithm, achieves over 90\% balanced accuracy in detecting the existence of snow or clouds for S/N$\gtrsim 20$, and 70\% for liquid seawater for S/N $\gtrsim 30$. Finally, we perform mock Bayesian analysis with Markov-chain Monte Carlo with five filters identified to derive exact surface compositions to test for retrieval feasibility.
  The results show that the use of machine learning to identify water on the surface of exoplanets from broadband-filter photometry provides a promising initial characterisation tool of water in different forms. Planned small and large telescope missions could use this to aid their prioritisation of targets for time-intense follow-up observations.

</p>
</details>

<details><summary><b>Variational methods for simulation-based inference</b>
<a href="https://arxiv.org/abs/2203.04176">arxiv:2203.04176</a>
&#x1F4C8; 5 <br>
<p>Manuel Gl√∂ckler, Michael Deistler, Jakob H. Macke</p></summary>
<p>

**Abstract:** We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.

</p>
</details>

<details><summary><b>Contrastive Conditional Neural Processes</b>
<a href="https://arxiv.org/abs/2203.03978">arxiv:2203.03978</a>
&#x1F4C8; 5 <br>
<p>Zesheng Ye, Lina Yao</p></summary>
<p>

**Abstract:** Conditional Neural Processes~(CNPs) bridge neural networks with probabilistic inference to approximate functions of Stochastic Processes under meta-learning settings. Given a batch of non-{\it i.i.d} function instantiations, CNPs are jointly optimized for in-instantiation observation prediction and cross-instantiation meta-representation adaptation within a generative reconstruction pipeline. There can be a challenge in tying together such two targets when the distribution of function observations scales to high-dimensional and noisy spaces. Instead, noise contrastive estimation might be able to provide more robust representations by learning distributional matching objectives to combat such inherent limitation of generative models. In light of this, we propose to equip CNPs by 1) aligning prediction with encoded ground-truth observation, and 2) decoupling meta-representation adaptation from generative reconstruction. Specifically, two auxiliary contrastive branches are set up hierarchically, namely in-instantiation temporal contrastive learning~({\tt TCL}) and cross-instantiation function contrastive learning~({\tt FCL}), to facilitate local predictive alignment and global function consistency, respectively. We empirically show that {\tt TCL} captures high-level abstraction of observations, whereas {\tt FCL} helps identify underlying functions, which in turn provides more efficient representations. Our model outperforms other CNPs variants when evaluating function distribution reconstruction and parameter identification across 1D, 2D and high-dimensional time-series.

</p>
</details>

<details><summary><b>Multi-Modal Mixup for Robust Fine-tuning</b>
<a href="https://arxiv.org/abs/2203.03897">arxiv:2203.03897</a>
&#x1F4C8; 5 <br>
<p>Junhyuk So, Changdae Oh, Minchul Shin, Kyungwoo Song</p></summary>
<p>

**Abstract:** Pre-trained large-scale models provide a transferable embedding, and they show comparable performance on the diverse downstream task. However, the transferability of multi-modal learning is restricted, and the analysis of learned embedding has not been explored well. This paper provides a perspective to understand the multi-modal embedding in terms of uniformity and alignment. We newly find that the representation learned by multi-modal learning models such as CLIP has a two separated representation space for each heterogeneous dataset with less alignment. Besides, there are unexplored large intermediate areas between two modalities with less uniformity. Less robust embedding might restrict the transferability of the representation for the downstream task. This paper provides a new end-to-end fine-tuning method for robust representation that encourages better uniformity and alignment score. First, we propose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image and text to generate the hard negative samples. Second, we fine-tune the multi-modal model on a hard negative sample as well as normal negative and positive samples with contrastive learning. Our multi-modal Mixup provides a robust representation, and we validate our methods on classification, retrieval, and structure-awareness task.

</p>
</details>

<details><summary><b>Error-based Knockoffs Inference for Controlled Feature Selection</b>
<a href="https://arxiv.org/abs/2203.04483">arxiv:2203.04483</a>
&#x1F4C8; 4 <br>
<p>Xuebin Zhao, Hong Chen, Yingjie Wang, Weifu Li, Tieliang Gong, Yulong Wang, Feng Zheng</p></summary>
<p>

**Abstract:** Recently, the scheme of model-X knockoffs was proposed as a promising solution to address controlled feature selection under high-dimensional finite-sample settings. However, the procedure of model-X knockoffs depends heavily on the coefficient-based feature importance and only concerns the control of false discovery rate (FDR). To further improve its adaptivity and flexibility, in this paper, we propose an error-based knockoff inference method by integrating the knockoff features, the error-based feature importance statistics, and the stepdown procedure together. The proposed inference procedure does not require specifying a regression model and can handle feature selection with theoretical guarantees on controlling false discovery proportion (FDP), FDR, or k-familywise error rate (k-FWER). Empirical evaluations demonstrate the competitive performance of our approach on both simulated and real data.

</p>
</details>

<details><summary><b>The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks</b>
<a href="https://arxiv.org/abs/2203.04466">arxiv:2203.04466</a>
&#x1F4C8; 4 <br>
<p>Xin Yu, Thiago Serra, Srikumar Ramalingam, Shandian Zhe</p></summary>
<p>

**Abstract:** Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.

</p>
</details>

<details><summary><b>Cooperative Trajectory Planning in Uncertain Environments with Monte Carlo Tree Search and Risk Metrics</b>
<a href="https://arxiv.org/abs/2203.04452">arxiv:2203.04452</a>
&#x1F4C8; 4 <br>
<p>Philipp Stegmaier, Karl Kurzer, J. Marius Z√∂llner</p></summary>
<p>

**Abstract:** Automated vehicles require the ability to cooperate with humans for a smooth integration into today's traffic. While the concept of cooperation is well known, the development of a robust and efficient cooperative trajectory planning method is still a challenge. One aspect of this challenge is the uncertainty surrounding the state of the environment due to limited sensor accuracy. This uncertainty can be represented by a Partially Observable Markov Decision Process. Our work addresses this problem by extending an existing cooperative trajectory planning approach based on Monte Carlo Tree Search for continuous action spaces. It does so by explicitly modeling uncertainties in the form of a root belief state, from which start states for trees are sampled. After the trees have been constructed with Monte Carlo Tree Search, their results are aggregated into return distributions using kernel regression. For the final selection, we apply two risk metrics, namely a Lower Confidence Bound and a Conditional Value at Risk. It can be demonstrated that the integration of risk metrics in the final selection policy consistently outperforms a baseline in uncertain environments, generating considerably safer trajectories.

</p>
</details>

<details><summary><b>CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution Detection</b>
<a href="https://arxiv.org/abs/2203.04450">arxiv:2203.04450</a>
&#x1F4C8; 4 <br>
<p>Yifei Ming, Yiyou Sun, Ousmane Dia, Yixuan Li</p></summary>
<p>

**Abstract:** Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to developments in distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf loss functions that suffice for classifying ID samples, but are not optimally designed for OOD detection. In this paper, we propose CIDER, a simple and effective representation learning framework by exploiting hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: (1) a dispersion loss that promotes large angular distances among different class prototypes, and (2) a compactness loss that encourages samples to be close to their class prototypes. We show that CIDER is effective under various settings and establishes state-of-the-art performance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method substantially improves the AUROC by 14.20% compared to the embeddings learned by the cross-entropy loss.

</p>
</details>

<details><summary><b>On generative models as the basis for digital twins</b>
<a href="https://arxiv.org/abs/2203.04384">arxiv:2203.04384</a>
&#x1F4C8; 4 <br>
<p>G. Tsialiamanis, D. J. Wagg, N. Dervilis, K. Worden</p></summary>
<p>

**Abstract:** A framework is proposed for generative models as a basis for digital twins or mirrors of structures. The proposal is based on the premise that deterministic models cannot account for the uncertainty present in most structural modelling applications. Two different types of generative models are considered here. The first is a physics-based model based on the stochastic finite element (SFE) method, which is widely used when modelling structures that have material and loading uncertainties imposed. Such models can be calibrated according to data from the structure and would be expected to outperform any other model if the modelling accurately captures the true underlying physics of the structure. The potential use of SFE models as digital mirrors is illustrated via application to a linear structure with stochastic material properties. For situations where the physical formulation of such models does not suffice, a data-driven framework is proposed, using machine learning and conditional generative adversarial networks (cGANs). The latter algorithm is used to learn the distribution of the quantity of interest in a structure with material nonlinearities and uncertainties. For the examples considered in this work, the data-driven cGANs model outperform the physics-based approach. Finally, an example is shown where the two methods are coupled such that a hybrid model approach is demonstrated.

</p>
</details>

<details><summary><b>Logic-based AI for Interpretable Board Game Winner Prediction with Tsetlin Machine</b>
<a href="https://arxiv.org/abs/2203.04378">arxiv:2203.04378</a>
&#x1F4C8; 4 <br>
<p>Charul Giri, Ole-Christoffer Granmo, Herke van Hoof, Christian D. Blakely</p></summary>
<p>

**Abstract:** Hex is a turn-based two-player connection game with a high branching factor, making the game arbitrarily complex with increasing board sizes. As such, top-performing algorithms for playing Hex rely on accurate evaluation of board positions using neural networks. However, the limited interpretability of neural networks is problematic when the user wants to understand the reasoning behind the predictions made. In this paper, we propose to use propositional logic expressions to describe winning and losing board game positions, facilitating precise visual interpretation. We employ a Tsetlin Machine (TM) to learn these expressions from previously played games, describing where pieces must be located or not located for a board position to be strong. Extensive experiments on $6\times6$ boards compare our TM-based solution with popular machine learning algorithms like XGBoost, InterpretML, decision trees, and neural networks, considering various board configurations with $2$ to $22$ moves played. On average, the TM testing accuracy is $92.1\%$, outperforming all the other evaluated algorithms. We further demonstrate the global interpretation of the logical expressions and map them down to particular board game configurations to investigate local interpretability. We believe the resulting interpretability establishes building blocks for accurate assistive AI and human-AI collaboration, also for more complex prediction tasks.

</p>
</details>

<details><summary><b>PyNET-QxQ: A Distilled PyNET for QxQ Bayer Pattern Demosaicing in CMOS Image Sensor</b>
<a href="https://arxiv.org/abs/2203.04314">arxiv:2203.04314</a>
&#x1F4C8; 4 <br>
<p>Minhyeok Cho, Haechang Lee, Hyunwoo Je, Kijeong Kim, Dongil Ryu, Jinsu Kim, Jonghyun Bae, Albert No</p></summary>
<p>

**Abstract:** The deep learning-based ISP models for mobile cameras produce high-quality images comparable to the professional DSLR camera. However, many of them are computationally expensive, which may not be appropriate for mobile environments. Also, the recent mobile cameras adopt non-Bayer CFAs (e.g., Quad Bayer, Nona Bayer, and QxQ Bayer) to improve image quality; however, most deep learning-based ISP models mainly focus on standard Bayer CFA. In this work, we propose PyNET-QxQ based on PyNET, a light-weighted ISP explicitly designed for the QxQ CFA pattern. The number of parameters of PyNET-QxQ is less than 2.5% of PyNET. We also introduce a novel knowledge distillation technique, progressive distillation, to train the compressed network effectively. Finally, experiments with QxQ images (obtained by an actual QxQ camera sensor, under development) demonstrate the outstanding performance of PyNET-QxQ despite significant parameter reductions.

</p>
</details>

<details><summary><b>A Machine Learning Approach to Digital Contact Tracing: TC4TL Challenge</b>
<a href="https://arxiv.org/abs/2203.04307">arxiv:2203.04307</a>
&#x1F4C8; 4 <br>
<p>Badrinath Singhal, Chris Vorster, Di Meng, Gargi Gupta, Laura Dunne, Mark Germaine</p></summary>
<p>

**Abstract:** Contact tracing is a method used by public health organisations to try prevent the spread of infectious diseases in the community. Traditionally performed by manual contact tracers, more recently the use of apps have been considered utilising phone sensor data to determine the distance between two phones. In this paper, we investigate the development of machine learning approaches to determine the distance between two mobile phone devices using Bluetooth Low Energy, sensory data and meta data. We use TableNet architecture and feature engineering to improve on the existing state of the art (total nDCF 0.21 vs 2.08), significantly outperforming existing models.

</p>
</details>

<details><summary><b>SuperPoint features in endoscopy</b>
<a href="https://arxiv.org/abs/2203.04302">arxiv:2203.04302</a>
&#x1F4C8; 4 <br>
<p>O. L. Barbed, F. Chadebecq, J. Morlana, J. M. Mart√≠nez-Montiel, A. C. Murillo</p></summary>
<p>

**Abstract:** There is often a significant gap between research results and applicability in routine medical practice. This work studies the performance of well-known local features on a medical dataset captured during routine colonoscopy procedures. Local feature extraction and matching is a key step for many computer vision applications, specially regarding 3D modelling. In the medical domain, handcrafted local features such as SIFT, with public pipelines such as COLMAP, are still a predominant tool for this kind of tasks. We explore the potential of the well known self-supervised approach SuperPoint, present an adapted variation for the endoscopic domain and propose a challenging evaluation framework. SuperPoint based models achieve significantly higher matching quality than commonly used local features in this domain. Our adapted model avoids features within specularity regions, a frequent and problematic artifact in endoscopic images, with consequent benefits for matching and reconstruction results.

</p>
</details>

<details><summary><b>Live Laparoscopic Video Retrieval with Compressed Uncertainty</b>
<a href="https://arxiv.org/abs/2203.04301">arxiv:2203.04301</a>
&#x1F4C8; 4 <br>
<p>Tong Yu, Pietro Mascagni, Juan Verde, Jacques Marescaux, Didier Mutter, Nicolas Padoy</p></summary>
<p>

**Abstract:** Searching through large volumes of medical data to retrieve relevant information is a challenging yet crucial task for clinical care. However the primitive and most common approach to retrieval, involving text in the form of keywords, is severely limited when dealing with complex media formats. Content-based retrieval offers a way to overcome this limitation, by using rich media as the query itself. Surgical video-to-video retrieval in particular is a new and largely unexplored research problem with high clinical value, especially in the real-time case: using real-time video hashing, search can be achieved directly inside of the operating room. Indeed, the process of hashing converts large data entries into compact binary arrays or hashes, enabling large-scale search operations at a very fast rate. However, due to fluctuations over the course of a video, not all bits in a given hash are equally reliable. In this work, we propose a method capable of mitigating this uncertainty while maintaining a light computational footprint. We present superior retrieval results (3-4 % top 10 mean average precision) on a multi-task evaluation protocol for surgery, using cholecystectomy phases, bypass phases, and coming from an entirely new dataset introduced here, critical events across six different surgery types. Success on this multi-task benchmark shows the generalizability of our approach for surgical video retrieval.

</p>
</details>

<details><summary><b>R√©nyi State Entropy for Exploration Acceleration in Reinforcement Learning</b>
<a href="https://arxiv.org/abs/2203.04297">arxiv:2203.04297</a>
&#x1F4C8; 4 <br>
<p>Mingqi Yuan, Man-on Pun, Dong Wang</p></summary>
<p>

**Abstract:** One of the most critical challenges in deep reinforcement learning is to maintain the long-term exploration capability of the agent. To tackle this problem, it has been recently proposed to provide intrinsic rewards for the agent to encourage exploration. However, most existing intrinsic reward-based methods proposed in the literature fail to provide sustainable exploration incentives, a problem known as vanishing rewards. In addition, these conventional methods incur complex models and additional memory in their learning procedures, resulting in high computational complexity and low robustness. In this work, a novel intrinsic reward module based on the R√©nyi entropy is proposed to provide high-quality intrinsic rewards. It is shown that the proposed method actually generalizes the existing state entropy maximization methods. In particular, a $k$-nearest neighbor estimator is introduced for entropy estimation while a $k$-value search method is designed to guarantee the estimation accuracy. Extensive simulation results demonstrate that the proposed R√©nyi entropy-based method can achieve higher performance as compared to existing schemes.

</p>
</details>

<details><summary><b>Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP)</b>
<a href="https://arxiv.org/abs/2203.04180">arxiv:2203.04180</a>
&#x1F4C8; 4 <br>
<p>Charles Millard, Mark Chiew, Jared Tanner, Aaron T. Hess, Boris Mailhe</p></summary>
<p>

**Abstract:** Purpose: To develop a tuning-free method for multi-coil compressed sensing MRI that performs competitively with algorithms with an optimally tuned sparse parameter.
  Theory: The Parallel Variable Density Approximate Message Passing (P-VDAMP) algorithm is proposed. For Bernoulli random variable density sampling, P-VDAMP obeys a "state evolution", where the intermediate per-iteration image estimate is distributed according to the ground truth corrupted by a Gaussian vector with approximately known covariance. State evolution is leveraged to automatically tune sparse parameters on-the-fly with Stein's Unbiased Risk Estimate (SURE).
  Methods: P-VDAMP is evaluated on brain, knee and angiogram datasets at acceleration factors 5 and 10 and compared with four variants of the Fast Iterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free variants from the literature.
  Results: The proposed method is found to have a similar reconstruction quality and time to convergence as FISTA with an optimally tuned sparse weighting.
  Conclusions: P-VDAMP is an efficient, robust and principled method for on-the-fly parameter tuning that is competitive with optimally tuned FISTA and offers substantial robustness and reconstruction quality improvements over competing tuning-free methods.

</p>
</details>

<details><summary><b>Robustly-reliable learners under poisoning attacks</b>
<a href="https://arxiv.org/abs/2203.04160">arxiv:2203.04160</a>
&#x1F4C8; 4 <br>
<p>Maria-Florina Balcan, Avrim Blum, Steve Hanneke, Dravyansh Sharma</p></summary>
<p>

**Abstract:** Data poisoning attacks, in which an adversary corrupts a training set with the goal of inducing specific desired mistakes, have raised substantial concern: even just the possibility of such an attack can make a user no longer trust the results of a learning system. In this work, we show how to achieve strong robustness guarantees in the face of such attacks across multiple axes.
  We provide robustly-reliable predictions, in which the predicted label is guaranteed to be correct so long as the adversary has not exceeded a given corruption budget, even in the presence of instance targeted attacks, where the adversary knows the test example in advance and aims to cause a specific failure on that example. Our guarantees are substantially stronger than those in prior approaches, which were only able to provide certificates that the prediction of the learning algorithm does not change, as opposed to certifying that the prediction is correct, as we are able to achieve in our work. Remarkably, we provide a complete characterization of learnability in this setting, in particular, nearly-tight matching upper and lower bounds on the region that can be certified, as well as efficient algorithms for computing this region given an ERM oracle. Moreover, for the case of linear separators over logconcave distributions, we provide efficient truly polynomial time algorithms (i.e., non-oracle algorithms) for such robustly-reliable predictions.
  We also extend these results to the active setting where the algorithm adaptively asks for labels of specific informative examples, and the difficulty is that the adversary might even be adaptive to this interaction, as well as to the agnostic learning setting where there is no perfect classifier even over the uncorrupted data.

</p>
</details>

<details><summary><b>Graph-based Reinforcement Learning meets Mixed Integer Programs: An application to 3D robot assembly discovery</b>
<a href="https://arxiv.org/abs/2203.04120">arxiv:2203.04120</a>
&#x1F4C8; 4 <br>
<p>Niklas Funk, Svenja Menzenbach, Georgia Chalvatzaki, Jan Peters</p></summary>
<p>

**Abstract:** Robot assembly discovery is a challenging problem that lives at the intersection of resource allocation and motion planning. The goal is to combine a predefined set of objects to form something new while considering task execution with the robot-in-the-loop. In this work, we tackle the problem of building arbitrary, predefined target structures entirely from scratch using a set of Tetris-like building blocks and a robotic manipulator. Our novel hierarchical approach aims at efficiently decomposing the overall task into three feasible levels that benefit mutually from each other. On the high level, we run a classical mixed-integer program for global optimization of block-type selection and the blocks' final poses to recreate the desired shape. Its output is then exploited to efficiently guide the exploration of an underlying reinforcement learning (RL) policy. This RL policy draws its generalization properties from a flexible graph-based representation that is learned through Q-learning and can be refined with search. Moreover, it accounts for the necessary conditions of structural stability and robotic feasibility that cannot be effectively reflected in the previous layer. Lastly, a grasp and motion planner transforms the desired assembly commands into robot joint movements. We demonstrate the performance of the proposed method on a set of competitive simulated robot assembly discovery environments and report performance and robustness gains compared to an unstructured end-to-end approach. Videos are available at https://sites.google.com/view/rl-meets-milp .

</p>
</details>

<details><summary><b>COLA: Consistent Learning with Opponent-Learning Awareness</b>
<a href="https://arxiv.org/abs/2203.04098">arxiv:2203.04098</a>
&#x1F4C8; 4 <br>
<p>Timon Willi, Johannes Treutlein, Alistair Letcher, Jakob Foerster</p></summary>
<p>

**Abstract:** Learning in general-sum games can be unstable and often leads to socially undesirable, Pareto-dominated outcomes. To mitigate this, Learning with Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting, by accounting for the agent's influence on the anticipated learning steps of other agents. However, the original LOLA formulation (and follow-up work) is inconsistent because LOLA models other agents as naive learners rather than LOLA agents. In previous work, this inconsistency was suggested as a cause of LOLA's failure to preserve stable fixed points (SFPs). First, we formalize consistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency problem if it converges. Second, we correct a claim made in the literature, by proving that, contrary to Sch√§fer and Anandkumar (2019), Competitive Gradient Descent (CGD) does not recover HOLA as a series expansion. Hence, CGD also does not solve the consistency problem. Third, we propose a new method called Consistent LOLA (COLA), which learns update functions that are consistent under mutual opponent shaping. It requires no more than second-order derivatives and learns consistent update functions even when HOLA fails to converge. However, we also prove that even consistent update functions do not preserve SFPs, contradicting the hypothesis that this shortcoming is caused by LOLA's inconsistency. Finally, in an empirical evaluation on a set of general-sum games, we find that COLA finds prosocial solutions and that it converges under a wider range of learning rates than HOLA and LOLA. We support the latter finding with a theoretical result for a simple game.

</p>
</details>

<details><summary><b>Data augmentation with mixtures of max-entropy transformations for filling-level classification</b>
<a href="https://arxiv.org/abs/2203.04027">arxiv:2203.04027</a>
&#x1F4C8; 4 <br>
<p>Apostolos Modas, Andrea Cavallaro, Pascal Frossard</p></summary>
<p>

**Abstract:** We address the problem of distribution shifts in test-time data with a principled data augmentation scheme for the task of content-level classification. In such a task, properties such as shape or transparency of test-time containers (cup or drinking glass) may differ from those represented in the training data. Dealing with such distribution shifts using standard augmentation schemes is challenging and transforming the training images to cover the properties of the test-time instances requires sophisticated image manipulations. We therefore generate diverse augmentations using a family of max-entropy transformations that create samples with new shapes, colors and spectral characteristics. We show that such a principled augmentation scheme, alone, can replace current approaches that use transfer learning or can be used in combination with transfer learning to improve its performance.

</p>
</details>

<details><summary><b>Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval</b>
<a href="https://arxiv.org/abs/2203.03853">arxiv:2203.03853</a>
&#x1F4C8; 4 <br>
<p>Jun Rao, Fei Wang, Liang Ding, Shuhan Qi, Yibing Zhan, Weifeng Liu, Dacheng Tao</p></summary>
<p>

**Abstract:** This paper seeks to provide the information retrieval community with some reflections on the current improvements of retrieval learning through the analysis of the reproducibility aspects of image-text retrieval models. For the latter part of the past decade, image-text retrieval has gradually become a major research direction in the field of information retrieval because of the growth of multi-modal data. Many researchers use benchmark datasets like MS-COCO and Flickr30k to train and assess the performance of image-text retrieval algorithms. Research in the past has mostly focused on performance, with several state-of-the-art methods being proposed in various ways. According to their claims, these approaches achieve better modal interactions and thus better multimodal representations with greater precision. In contrast to those previous works, we focus on the repeatability of the approaches and the overall examination of the elements that lead to improved performance by pretrained and nonpretrained models in retrieving images and text. To be more specific, we first examine the related reproducibility concerns and why the focus is on image-text retrieval tasks, and then we systematically summarize the current paradigm of image-text retrieval models and the stated contributions of those approaches. Second, we analyze various aspects of the reproduction of pretrained and nonpretrained retrieval models. Based on this, we conducted ablation experiments and obtained some influencing factors that affect retrieval recall more than the improvement claimed in the original paper. Finally, we also present some reflections and issues that should be considered by the retrieval community in the future. Our code is freely available at https://github.com/WangFei-2019/Image-text-Retrieval.

</p>
</details>

<details><summary><b>Multi-Agent Active Search using Detection and Location Uncertainty</b>
<a href="https://arxiv.org/abs/2203.04524">arxiv:2203.04524</a>
&#x1F4C8; 3 <br>
<p>Arundhati Banerjee, Ramina Ghods, Jeff Schneider</p></summary>
<p>

**Abstract:** Active search refers to the task of autonomous robots (agents) detecting objects of interest (targets) in a search space using decision making algorithms that adapt to the history of their observations. It has important applications in search and rescue missions, wildlife patrolling and environment monitoring. Active search algorithms must contend with two types of uncertainty: detection uncertainty and location uncertainty. Prior work has typically focused on one of these while ignoring or engineering away the other. The more common approach in robotics is to focus on location uncertainty and remove detection uncertainty by thresholding the detection probability to zero or one. On the other hand, it is common in the sparse signal processing literature to assume the target location is accurate and focus on the uncertainty of its detection. In this work, we propose an inference method to jointly handle both target detection and location uncertainty. We then build a decision making algorithm on this inference method that uses Thompson sampling to enable efficient active search in both the single agent and multi-agent settings. We perform experiments in simulation over varying number of agents and targets to show that our inference and decision making algorithms outperform competing baselines that only account for either target detection or location uncertainty.

</p>
</details>

<details><summary><b>Harmonicity Plays a Critical Role in DNN Based Versus in Biologically-Inspired Monaural Speech Segregation Systems</b>
<a href="https://arxiv.org/abs/2203.04420">arxiv:2203.04420</a>
&#x1F4C8; 3 <br>
<p>Rahil Parikh, Ilya Kavalerov, Carol Espy-Wilson, Shihab Shamma</p></summary>
<p>

**Abstract:** Recent advancements in deep learning have led to drastic improvements in speech segregation models. Despite their success and growing applicability, few efforts have been made to analyze the underlying principles that these networks learn to perform segregation. Here we analyze the role of harmonicity on two state-of-the-art Deep Neural Networks (DNN)-based models- Conv-TasNet and DPT-Net. We evaluate their performance with mixtures of natural speech versus slightly manipulated inharmonic speech, where harmonics are slightly frequency jittered. We find that performance deteriorates significantly if one source is even slightly harmonically jittered, e.g., an imperceptible 3% harmonic jitter degrades performance of Conv-TasNet from 15.4 dB to 0.70 dB. Training the model on inharmonic speech does not remedy this sensitivity, instead resulting in worse performance on natural speech mixtures, making inharmonicity a powerful adversarial factor in DNN models. Furthermore, additional analyses reveal that DNN algorithms deviate markedly from biologically inspired algorithms that rely primarily on timing cues and not harmonicity to segregate speech.

</p>
</details>

<details><summary><b>Multi-Scale Adaptive Network for Single Image Denoising</b>
<a href="https://arxiv.org/abs/2203.04313">arxiv:2203.04313</a>
&#x1F4C8; 3 <br>
<p>Yuanbiao Gou, Peng Hu, Jiancheng Lv, Xi Peng</p></summary>
<p>

**Abstract:** Multi-scale architectures have shown effectiveness in a variety of tasks including single image denoising, thanks to appealing cross-scale complementarity. However, existing methods treat different scale features equally without considering their scale-specific characteristics, i.e., the within-scale characteristics are ignored. In this paper, we reveal this missing piece for multi-scale architecture design and accordingly propose a novel Multi-Scale Adaptive Network (MSANet) for single image denoising. To be specific, MSANet simultaneously embraces the within-scale characteristics and the cross-scale complementarity thanks to three novel neural blocks, i.e., adaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive fusion block (AFuB). In brief, AFeB is designed to adaptively select details and filter noises, which is highly expected for fine-grained features. AMB could enlarge the receptive field and aggregate the multi-scale information, which is designed to satisfy the demands of both fine- and coarse-grained features. AFuB devotes to adaptively sampling and transferring the features from one scale to another scale, which is used to fuse the features with varying characteristics from coarse to fine. Extensive experiments on both three real and six synthetic noisy image datasets show the superiority of MSANet compared with 12 methods.

</p>
</details>

<details><summary><b>LSTMSPLIT: Effective SPLIT Learning based LSTM on Sequential Time-Series Data</b>
<a href="https://arxiv.org/abs/2203.04305">arxiv:2203.04305</a>
&#x1F4C8; 3 <br>
<p>Lianlian Jiang, Yuexuan Wang, Wenyi Zheng, Chao Jin, Zengxiang Li, Sin G. Teo</p></summary>
<p>

**Abstract:** Federated learning (FL) and split learning (SL) are the two popular distributed machine learning (ML) approaches that provide some data privacy protection mechanisms. In the time-series classification problem, many researchers typically use 1D convolutional neural networks (1DCNNs) based on the SL approach with a single client to reduce the computational overhead at the client-side while still preserving data privacy. Another method, recurrent neural network (RNN), is utilized on sequentially partitioned data where segments of multiple-segment sequential data are distributed across various clients. However, to the best of our knowledge, it is still not much work done in SL with long short-term memory (LSTM) network, even the LSTM network is practically effective in processing time-series data. In this work, we propose a new approach, LSTMSPLIT, that uses SL architecture with an LSTM network to classify time-series data with multiple clients. The differential privacy (DP) is applied to solve the data privacy leakage. The proposed method, LSTMSPLIT, has achieved better or reasonable accuracy compared to the Split-1DCNN method using the electrocardiogram dataset and the human activity recognition dataset. Furthermore, the proposed method, LSTMSPLIT, can also achieve good accuracy after applying differential privacy to preserve the user privacy of the cut layer of the LSTMSPLIT.

</p>
</details>

<details><summary><b>Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models</b>
<a href="https://arxiv.org/abs/2203.04272">arxiv:2203.04272</a>
&#x1F4C8; 3 <br>
<p>Vincent Lim, Ellen Novoseller, Jeffrey Ichnowski, Huang Huang, Ken Goldberg</p></summary>
<p>

**Abstract:** For applications in healthcare, physics, energy, robotics, and many other fields, designing maximally informative experiments is valuable, particularly when experiments are expensive, time-consuming, or pose safety hazards. While existing approaches can sequentially design experiments based on prior observation history, many of these methods do not extend to implicit models, where simulation is possible but computing the likelihood is intractable. Furthermore, they often require either significant online computation during deployment or a differentiable simulation system. We introduce Reinforcement Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based optimal experimental design for non-differentiable implicit models. RL-DAD extends prior work in policy-based Bayesian Optimal Experimental Design (BOED) by reformulating it as a Markov Decision Process with a reward function based on likelihood-free information lower bounds, which is used to learn a policy via deep reinforcement learning. The learned design policy maps prior histories to experiment designs offline and can be quickly deployed during online execution. We evaluate RL-DAD and find that it performs competitively with baselines on three benchmarks.

</p>
</details>

<details><summary><b>Trustable Co-label Learning from Multiple Noisy Annotators</b>
<a href="https://arxiv.org/abs/2203.04199">arxiv:2203.04199</a>
&#x1F4C8; 3 <br>
<p>Shikun Li, Tongliang Liu, Jiyong Tan, Dan Zeng, Shiming Ge</p></summary>
<p>

**Abstract:** Supervised deep learning depends on massive accurately annotated examples, which is usually impractical in many real-world scenarios. A typical alternative is learning from multiple noisy annotators. Numerous earlier works assume that all labels are noisy, while it is usually the case that a few trusted samples with clean labels are available. This raises the following important question: how can we effectively use a small amount of trusted data to facilitate robust classifier learning from multiple annotators? This paper proposes a data-efficient approach, called \emph{Trustable Co-label Learning} (TCL), to learn deep classifiers from multiple noisy annotators when a small set of trusted data is available. This approach follows the coupled-view learning manner, which jointly learns the data classifier and the label aggregator. It effectively uses trusted data as a guide to generate trustable soft labels (termed co-labels). A co-label learning can then be performed by alternately reannotating the pseudo labels and refining the classifiers. In addition, we further improve TCL for a special complete data case, where each instance is labeled by all annotators and the label aggregator is represented by multilayer neural networks to enhance model capacity. Extensive experiments on synthetic and real datasets clearly demonstrate the effectiveness and robustness of the proposed approach. Source code is available at https://github.com/ShikunLi/TCL

</p>
</details>

<details><summary><b>Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection for English and Arabic Using Transformers and Data Augmentation</b>
<a href="https://arxiv.org/abs/2203.04111">arxiv:2203.04111</a>
&#x1F4C8; 3 <br>
<p>Shubham Kumar Nigam, Mosab Shaheen</p></summary>
<p>

**Abstract:** This paper describes our submission to SemEval-2022 Task 6 on sarcasm detection and its five subtasks for English and Arabic. Sarcasm conveys a meaning which contradicts the literal meaning, and it is mainly found on social networks. It has a significant role in understanding the intention of the user. For detecting sarcasm, we used deep learning techniques based on transformers due to its success in the field of Natural Language Processing (NLP) without the need for feature engineering. The datasets were taken from tweets. We created new datasets by augmenting with external data or by using word embeddings and repetition of instances. Experiments were done on the datasets with different types of preprocessing because it is crucial in this task. The rank of our team was consistent across four subtasks (fourth rank in three subtasks and sixth rank in one subtask); whereas other teams might be in the top ranks for some subtasks but rank drastically less in other subtasks. This implies the robustness and stability of the models and the techniques we used.

</p>
</details>

<details><summary><b>VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer</b>
<a href="https://arxiv.org/abs/2203.04099">arxiv:2203.04099</a>
&#x1F4C8; 3 <br>
<p>Juan F. Montesinos, Venkatesh S. Kadandale, Gloria Haro</p></summary>
<p>

**Abstract:** This paper presents an audio-visual approach for voice separation which outperforms state-of-the-art methods at a low latency in two scenarios: speech and singing voice. The model is based on a two-stage network. Motion cues are obtained with a lightweight graph convolutional network that processes face landmarks. Then, both audio and motion features are fed to an audio-visual transformer which produces a fairly good estimation of the isolated target source. In a second stage, the predominant voice is enhanced with an audio-only network. We present different ablation studies and comparison to state-of-the-art methods. Finally, we explore the transferability of models trained for speech separation in the task of singing voice separation. The demos, code, and weights will be made publicly available at https://ipcv.github.io/VoViT/

</p>
</details>

<details><summary><b>Analyzing General-Purpose Deep-Learning Detection and Segmentation Models with Images from a Lidar as a Camera Sensor</b>
<a href="https://arxiv.org/abs/2203.04064">arxiv:2203.04064</a>
&#x1F4C8; 3 <br>
<p>Yu Xianjia, Sahar Salimpour, Jorge Pe√±a Queralta, Tomi Westerlund</p></summary>
<p>

**Abstract:** Over the last decade, robotic perception algorithms have significantly benefited from the rapid advances in deep learning (DL). Indeed, a significant amount of the autonomy stack of different commercial and research platforms relies on DL for situational awareness, especially vision sensors. This work explores the potential of general-purpose DL perception algorithms, specifically detection and segmentation neural networks, for processing image-like outputs of advanced lidar sensors. Rather than processing the three-dimensional point cloud data, this is, to the best of our knowledge, the first work to focus on low-resolution images with 360\textdegree field of view obtained with lidar sensors by encoding either depth, reflectivity, or near-infrared light in the image pixels. We show that with adequate preprocessing, general-purpose DL models can process these images, opening the door to their usage in environmental conditions where vision sensors present inherent limitations. We provide both a qualitative and quantitative analysis of the performance of a variety of neural network architectures. We believe that using DL models built for visual cameras offers significant advantages due to the much wider availability and maturity compared to point cloud-based perception.

</p>
</details>

<details><summary><b>An Online Semantic Mapping System for Extending and Enhancing Visual SLAM</b>
<a href="https://arxiv.org/abs/2203.03944">arxiv:2203.03944</a>
&#x1F4C8; 3 <br>
<p>Thorsten Hempel, Ayoub Al-Hamadi</p></summary>
<p>

**Abstract:** We present a real-time semantic mapping approach for mobile vision systems with a 2D to 3D object detection pipeline and rapid data association for generated landmarks. Besides the semantic map enrichment the associated detections are further introduced as semantic constraints into a simultaneous localization and mapping (SLAM) system for pose correction purposes. This way, we are able generate additional meaningful information that allows to achieve higher-level tasks, while simultaneously leveraging the view-invariance of object detections to improve the accuracy and the robustness of the odometry estimation. We propose tracklets of locally associated object observations to handle ambiguous and false predictions and an uncertainty-based greedy association scheme for an accelerated processing time. Our system reaches real-time capabilities with an average iteration duration of 65~ms and is able to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a public dataset. Additionally, we implemented our approach as a modular ROS package that makes it straightforward for integration in arbitrary graph-based SLAM methods.

</p>
</details>

<details><summary><b>Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks</b>
<a href="https://arxiv.org/abs/2203.03929">arxiv:2203.03929</a>
&#x1F4C8; 3 <br>
<p>Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, Reza Shokri</p></summary>
<p>

**Abstract:** The wide adoption and application of Masked language models~(MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities -- to what extent do MLMs leak information about their training data? Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying the potential robustness of MLMs to privacy attacks. In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM's model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are extremely susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level, with a significant improvement in the low-error region: at 1% false positive rate, our attack is 51X more powerful than prior work.

</p>
</details>

<details><summary><b>High-order Order Proximity-Incorporated, Symmetry and Graph-Regularized Nonnegative Matrix Factorization for Community Detection</b>
<a href="https://arxiv.org/abs/2203.03876">arxiv:2203.03876</a>
&#x1F4C8; 3 <br>
<p>Zhigang Liu, Xin Luo</p></summary>
<p>

**Abstract:** Community describes the functional mechanism of a network, making community detection serve as a fundamental graph tool for various real applications like discovery of social circle. To date, a Symmetric and Non-negative Matrix Factorization (SNMF) model has been frequently adopted to address this issue owing to its high interpretability and scalability. However, most existing SNMF-based community detection methods neglect the high-order connection patterns in a network. Motivated by this discovery, in this paper, we propose a High-Order Proximity (HOP)-incorporated, Symmetry and Graph-regularized NMF (HSGN) model that adopts the following three-fold ideas: a) adopting a weighted pointwise mutual information (PMI)-based approach to measure the HOP indices among nodes in a network; b) leveraging an iterative reconstruction scheme to encode the captured HOP into the network; and c) introducing a symmetry and graph-regularized NMF algorithm to detect communities accurately. Extensive empirical studies on eight real-world networks demonstrate that an HSGN-based community detector significantly outperforms both benchmark and state-of-the-art community detectors in providing highly-accurate community detection results.

</p>
</details>

<details><summary><b>SuperCone: Modeling Heterogeneous Experts with Concept Meta-learning for Unified Predictive Segments System</b>
<a href="https://arxiv.org/abs/2203.07029">arxiv:2203.07029</a>
&#x1F4C8; 2 <br>
<p>Keqian Li, Yifan Hu</p></summary>
<p>

**Abstract:** Understanding users through predicative segments play an essential role for modern enterprises for more efficient and efficient information exchange. For example, by predicting whether a user has particular interest in a particular area of sports or entertainment, we can better serve the user with more relevant and tailored content. However, there exists a large number of long tail prediction tasks that are hard to capture by off the shelf model architectures due to data scarcity and task heterogeneity. In this work, we present SuperCone, our unified predicative segments system that addresses the above challenges. It builds on top of a flat concept representation that summarizes each user's heterogeneous digital footprints, and uniformly models each of the prediction task using an approach called "super learning ", that is, combining prediction models with diverse architectures or learning method that are not compatible with each other or even completely unknown. Following this, we provide end to end deep learning architecture design that flexibly learns to attend to best suited heterogeneous experts while at the same time learns deep representations of the input concepts that augments the above experts by capturing unique signal. Experiments show that SuperCone can outperform state-of-the-art recommendation and ranking algorithms on a wide range of predicative segment tasks, as well as several public structured data learning benchmarks.

</p>
</details>

<details><summary><b>Structure and Distribution Metric for Quantifying the Quality of Uncertainty: Assessing Gaussian Processes, Deep Neural Nets, and Deep Neural Operators for Regression</b>
<a href="https://arxiv.org/abs/2203.04515">arxiv:2203.04515</a>
&#x1F4C8; 2 <br>
<p>Ethan Pickering, Themistoklis P. Sapsis</p></summary>
<p>

**Abstract:** We propose two bounded comparison metrics that may be implemented to arbitrary dimensions in regression tasks. One quantifies the structure of uncertainty and the other quantifies the distribution of uncertainty. The structure metric assesses the similarity in shape and location of uncertainty with the true error, while the distribution metric quantifies the supported magnitudes between the two. We apply these metrics to Gaussian Processes (GPs), Ensemble Deep Neural Nets (DNNs), and Ensemble Deep Neural Operators (DNOs) on high-dimensional and nonlinear test cases. We find that comparing a model's uncertainty estimates with the model's squared error provides a compelling ground truth assessment. We also observe that both DNNs and DNOs, especially when compared to GPs, provide encouraging metric values in high dimensions with either sparse or plentiful data.

</p>
</details>

<details><summary><b>Multi-Agent Policy Transfer via Task Relationship Modeling</b>
<a href="https://arxiv.org/abs/2203.04482">arxiv:2203.04482</a>
&#x1F4C8; 2 <br>
<p>Rongjun Qin, Feng Chen, Tonghan Wang, Lei Yuan, Xiaoran Wu, Zongzhang Zhang, Chongjie Zhang, Yang Yu</p></summary>
<p>

**Abstract:** Team adaptation to new cooperative tasks is a hallmark of human intelligence, which has yet to be fully realized in learning agents. Previous work on multi-agent transfer learning accommodate teams of different sizes, heavily relying on the generalization ability of neural networks for adapting to unseen tasks. We believe that the relationship among tasks provides the key information for policy adaptation. In this paper, we try to discover and exploit common structures among tasks for more efficient transfer, and propose to learn effect-based task representations as a common space of tasks, using an alternatively fixed training scheme. We demonstrate that the task representation can capture the relationship among tasks, and can generalize to unseen tasks. As a result, the proposed method can help transfer learned cooperation knowledge to new tasks after training on a few source tasks. We also find that fine-tuning the transferred policies help solve tasks that are hard to learn from scratch.

</p>
</details>

<details><summary><b>Contextual Networks and Unsupervised Ranking of Sentences</b>
<a href="https://arxiv.org/abs/2203.04459">arxiv:2203.04459</a>
&#x1F4C8; 2 <br>
<p>Hao Zhang, You Zhou, Jie Wang</p></summary>
<p>

**Abstract:** We construct a contextual network to represent a document with syntactic and semantic relations between word-sentence pairs, based on which we devise an unsupervised algorithm called CNATAR (Contextual Network And Text Analysis Rank) to score sentences, and rank them through a bi-objective 0-1 knapsack maximization problem over topic analysis and sentence scores. We show that CNATAR outperforms the combined ranking of the three human judges provided on the SummBank dataset under both ROUGE and BLEU metrics, which in term significantly outperforms each individual judge's ranking. Moreover, CNATAR produces so far the highest ROUGE scores over DUC-02, and outperforms previous supervised algorithms on the CNN/DailyMail and NYT datasets. We also compare the performance of CNATAR and the latest supervised neural-network summarization models and compute oracle results.

</p>
</details>

<details><summary><b>Structural Learning of Simple Staged Trees</b>
<a href="https://arxiv.org/abs/2203.04390">arxiv:2203.04390</a>
&#x1F4C8; 2 <br>
<p>Manuele Leonelli, Gherardo Varando</p></summary>
<p>

**Abstract:** Bayesian networks faithfully represent the symmetric conditional independences existing between the components of a random vector. Staged trees are an extension of Bayesian networks for categorical random vectors whose graph represents non-symmetric conditional independences via vertex coloring. However, since they are based on a tree representation of the sample space, the underlying graph becomes cluttered and difficult to visualize as the number of variables increases. Here we introduce the first structural learning algorithms for the class of simple staged trees, entertaining a compact coalescence of the underlying tree from which non-symmetric independences can be easily read. We show that data-learned simple staged trees often outperform Bayesian networks in model fit and illustrate how the coalesced graph is used to identify non-symmetric conditional independences.

</p>
</details>

<details><summary><b>Model-free feature selection to facilitate automatic discovery of divergent subgroups in tabular data</b>
<a href="https://arxiv.org/abs/2203.04386">arxiv:2203.04386</a>
&#x1F4C8; 2 <br>
<p>Girmaw Abebe Tadesse, William Ogallo, Celia Cintas, Skyler Speakman</p></summary>
<p>

**Abstract:** Data-centric AI encourages the need of cleaning and understanding of data in order to achieve trustworthy AI. Existing technologies, such as AutoML, make it easier to design and train models automatically, but there is a lack of a similar level of capabilities to extract data-centric insights. Manual stratification of tabular data per a feature (e.g., gender) is limited to scale up for higher feature dimension, which could be addressed using automatic discovery of divergent subgroups. Nonetheless, these automatic discovery techniques often search across potentially exponential combinations of features that could be simplified using a preceding feature selection step. Existing feature selection techniques for tabular data often involve fitting a particular model in order to select important features. However, such model-based selection is prone to model-bias and spurious correlations in addition to requiring extra resource to design, fine-tune and train a model. In this paper, we propose a model-free and sparsity-based automatic feature selection (SAFS) framework to facilitate automatic discovery of divergent subgroups. Different from filter-based selection techniques, we exploit the sparsity of objective measures among feature values to rank and select features. We validated SAFS across two publicly available datasets (MIMIC-III and Allstate Claims) and compared it with six existing feature selection methods. SAFS achieves a reduction of feature selection time by a factor of 81x and 104x, averaged cross the existing methods in the MIMIC-III and Claims datasets respectively. SAFS-selected features are also shown to achieve competitive detection performance, e.g., 18.3% of features selected by SAFS in the Claims dataset detected divergent samples similar to those detected by using the whole features with a Jaccard similarity of 0.95 but with a 16x reduction in detection time.

</p>
</details>

<details><summary><b>Regularized Training of Intermediate Layers for Generative Models for Inverse Problems</b>
<a href="https://arxiv.org/abs/2203.04382">arxiv:2203.04382</a>
&#x1F4C8; 2 <br>
<p>Sean Gunn, Jorio Cocola, Paul Hand</p></summary>
<p>

**Abstract:** Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particular signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion algorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate this principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generative model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems.

</p>
</details>

<details><summary><b>Beam Search for Feature Selection</b>
<a href="https://arxiv.org/abs/2203.04350">arxiv:2203.04350</a>
&#x1F4C8; 2 <br>
<p>Nicolas Fraiman, Zichao Li</p></summary>
<p>

**Abstract:** In this paper, we present and prove some consistency results about the performance of classification models using a subset of features. In addition, we propose to use beam search to perform feature selection, which can be viewed as a generalization of forward selection. We apply beam search to both simulated and real-world data, by evaluating and comparing the performance of different classification models using different sets of features. The results demonstrate that beam search could outperform forward selection, especially when the features are correlated so that they have more discriminative power when considered jointly than individually. Moreover, in some cases classification models could obtain comparable performance using only ten features selected by beam search instead of hundreds of original features.

</p>
</details>

<details><summary><b>Multi-Agent Broad Reinforcement Learning for Intelligent Traffic Light Control</b>
<a href="https://arxiv.org/abs/2203.04310">arxiv:2203.04310</a>
&#x1F4C8; 2 <br>
<p>Ruijie Zhu, Lulu Li, Shuning Wu, Pei Lv, Yafai Li, Mingliang Xu</p></summary>
<p>

**Abstract:** Intelligent Traffic Light Control System (ITLCS) is a typical Multi-Agent System (MAS), which comprises multiple roads and traffic lights.Constructing a model of MAS for ITLCS is the basis to alleviate traffic congestion. Existing approaches of MAS are largely based on Multi-Agent Deep Reinforcement Learning (MADRL). Although the Deep Neural Network (DNN) of MABRL is effective, the training time is long, and the parameters are difficult to trace. Recently, Broad Learning Systems (BLS) provided a selective way for learning in the deep neural networks by a flat network. Moreover, Broad Reinforcement Learning (BRL) extends BLS in Single Agent Deep Reinforcement Learning (SADRL) problem with promising results. However, BRL does not focus on the intricate structures and interaction of agents. Motivated by the feature of MADRL and the issue of BRL, we propose a Multi-Agent Broad Reinforcement Learning (MABRL) framework to explore the function of BLS in MAS. Firstly, unlike most existing MADRL approaches, which use a series of deep neural networks structures, we model each agent with broad networks. Then, we introduce a dynamic self-cycling interaction mechanism to confirm the "3W" information: When to interact, Which agents need to consider, What information to transmit. Finally, we do the experiments based on the intelligent traffic light control scenario. We compare the MABRL approach with six different approaches, and experimental results on three datasets verify the effectiveness of MABRL.

</p>
</details>

<details><summary><b>CaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classification</b>
<a href="https://arxiv.org/abs/2203.04298">arxiv:2203.04298</a>
&#x1F4C8; 2 <br>
<p>Yijiang Chen, Xiangdong Zhou, Zhen Xing, Zhidan Liu, Minyang Xu</p></summary>
<p>

**Abstract:** Self-supervised representation learning of Multivariate Time Series (MTS) is a challenging task and attracts increasing research interests in recent years. Many previous works focus on the pretext task of self-supervised learning and usually neglect the complex problem of MTS encoding, leading to unpromising results. In this paper, we tackle this challenge from two aspects: encoder and pretext task, and propose a unified channel-aware self-supervised learning framework CaSS. Specifically, we first design a new Transformer-based encoder Channel-aware Transformer (CaT) to capture the complex relationships between different time channels of MTS. Second, we combine two novel pretext tasks Next Trend Prediction (NTP) and Contextual Similarity (CS) for the self-supervised representation learning with our proposed encoder. Extensive experiments are conducted on several commonly used benchmark datasets. The experimental results show that our framework achieves new state-of-the-art comparing with previous self-supervised MTS representation learning methods (up to +7.70\% improvement on LSST dataset) and can be well applied to the downstream MTS classification.

</p>
</details>

<details><summary><b>A Sharp Characterization of Linear Estimators for Offline Policy Evaluation</b>
<a href="https://arxiv.org/abs/2203.04236">arxiv:2203.04236</a>
&#x1F4C8; 2 <br>
<p>Juan C. Perdomo, Akshay Krishnamurthy, Peter Bartlett, Sham Kakade</p></summary>
<p>

**Abstract:** Offline policy evaluation is a fundamental statistical problem in reinforcement learning that involves estimating the value function of some decision-making policy given data collected by a potentially different policy. In order to tackle problems with complex, high-dimensional observations, there has been significant interest from theoreticians and practitioners alike in understanding the possibility of function approximation in reinforcement learning. Despite significant study, a sharp characterization of when we might expect offline policy evaluation to be tractable, even in the simplest setting of linear function approximation, has so far remained elusive, with a surprising number of strong negative results recently appearing in the literature.
  In this work, we identify simple control-theoretic and linear-algebraic conditions that are necessary and sufficient for classical methods, in particular Fitted Q-iteration (FQI) and least squares temporal difference learning (LSTD), to succeed at offline policy evaluation. Using this characterization, we establish a precise hierarchy of regimes under which these estimators succeed. We prove that LSTD works under strictly weaker conditions than FQI. Furthermore, we establish that if a problem is not solvable via LSTD, then it cannot be solved by a broad class of linear estimators, even in the limit of infinite data. Taken together, our results provide a complete picture of the behavior of linear estimators for offline policy evaluation (OPE), unify previously disparate analyses of canonical algorithms, and provide significantly sharper notions of the underlying statistical complexity of OPE.

</p>
</details>

<details><summary><b>YouTube-GDD: A challenging gun detection dataset with rich contextual information</b>
<a href="https://arxiv.org/abs/2203.04129">arxiv:2203.04129</a>
&#x1F4C8; 2 <br>
<p>Yongxiang Gu, Xingbin Liao, Xiaolin Qin</p></summary>
<p>

**Abstract:** An automatic gun detection system can detect potential gun-related violence at an early stage that is of paramount importance for citizens security. In the whole system, object detection algorithm is the key to perceive the environment so that the system can detect dangerous objects such as pistols and rifles. However, mainstream deep learning-based object detection algorithms depend heavily on large-scale high-quality annotated samples, and the existing gun datasets are characterized by low resolution, little contextual information and little data volume. To promote the development of security, this work presents a new challenging dataset called YouTube Gun Detection Dataset (YouTube-GDD). Our dataset is collected from 343 high-definition YouTube videos and contains 5000 well-chosen images, in which 16064 instances of gun and 9046 instances of person are annotated. Compared to other datasets, YouTube-GDD is "dynamic", containing rich contextual information and recording shape changes of the gun during shooting. To build a baseline for gun detection, we evaluate YOLOv5 on YouTube-GDD and analyze the influence of additional related annotated information on gun detection. YouTube-GDD and subsequent updates will be released at https://github.com/UCAS-GYX/YouTube-GDD.

</p>
</details>

<details><summary><b>Contrastive Enhancement Using Latent Prototype for Few-Shot Segmentation</b>
<a href="https://arxiv.org/abs/2203.04095">arxiv:2203.04095</a>
&#x1F4C8; 2 <br>
<p>Xiaoyu Zhao, Xiaoqian Chen, Zhiqiang Gong, Wen Yao, Yunyang Zhang, Xiaohu Zheng</p></summary>
<p>

**Abstract:** Few-shot segmentation enables the model to recognize unseen classes with few annotated examples. Most existing methods adopt prototype learning architecture, where support prototype vectors are expanded and concatenated with query features to perform conditional segmentation. However, such framework potentially focuses more on query features while may neglect the similarity between support and query features. This paper proposes a contrastive enhancement approach using latent prototypes to leverage latent classes and raise the utilization of similarity information between prototype and query features. Specifically, a latent prototype sampling module is proposed to generate pseudo-mask and novel prototypes based on features similarity. The module conveniently conducts end-to-end learning and has no strong dependence on clustering numbers like cluster-based method. Besides, a contrastive enhancement module is developed to drive models to provide different predictions with the same query features. Our method can be used as an auxiliary module to flexibly integrate into other baselines for a better segmentation performance. Extensive experiments show our approach remarkably improves the performance of state-of-the-art methods for 1-shot and 5-shot segmentation, especially outperforming baseline by 5.9% and 7.3% for 5-shot task on Pascal-5^i and COCO-20^i. Source code is available at https://github.com/zhaoxiaoyu1995/CELP-Pytorch

</p>
</details>

<details><summary><b>Obstacle Aware Sampling for Path Planning</b>
<a href="https://arxiv.org/abs/2203.04075">arxiv:2203.04075</a>
&#x1F4C8; 2 <br>
<p>Murad Tukan, Alaa Maalouf, Dan Feldman, Roi Poranne</p></summary>
<p>

**Abstract:** Many path planning algorithms are based on sampling the state space. While this approach is very simple, it can become costly when the obstacles are unknown, since samples hitting these obstacles are wasted. The goal of this paper is to efficiently identify obstacles in a map and remove them from the sampling space. To this end, we propose a pre-processing algorithm for space exploration that enables more efficient sampling. We show that it can boost the performance of other space sampling methods and path planners.
  Our approach is based on the fact that a convex obstacle can be approximated provably well by its minimum volume enclosing ellipsoid (MVEE), and a non-convex obstacle may be partitioned into convex shapes. Our main contribution is an algorithm that strategically finds a small sample, called the \emph{active-coreset}, that adaptively samples the space via membership-oracle such that the MVEE of the coreset approximates the MVEE of the obstacle. Experimental results confirm the effectiveness of our approach across multiple planners based on Rapidly-exploring random trees, showing significant improvement in terms of time and path length.

</p>
</details>

<details><summary><b>Lane Detection with Versatile AtrousFormer and Local Semantic Guidance</b>
<a href="https://arxiv.org/abs/2203.04067">arxiv:2203.04067</a>
&#x1F4C8; 2 <br>
<p>Jiaxing Yang, Lihe Zhang, Huchuan Lu</p></summary>
<p>

**Abstract:** Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer \cite{transformer}. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately, in which the predicted Gaussian map of the starting point of each lane serves to guide the process. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.

</p>
</details>

<details><summary><b>DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set Feature Extraction</b>
<a href="https://arxiv.org/abs/2203.04007">arxiv:2203.04007</a>
&#x1F4C8; 2 <br>
<p>Jiajun Fei, Ziyu Zhu, Wenlei Liu, Zhidong Deng, Mingyang Li, Huanjun Deng, Shuo Zhang</p></summary>
<p>

**Abstract:** Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one. Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer. It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters. In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets. We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold. We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions. The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets. The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets. On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively. The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin.

</p>
</details>

<details><summary><b>Semi-Random Sparse Recovery in Nearly-Linear Time</b>
<a href="https://arxiv.org/abs/2203.04002">arxiv:2203.04002</a>
&#x1F4C8; 2 <br>
<p>Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</p></summary>
<p>

**Abstract:** Sparse recovery is one of the most fundamental and well-studied inverse problems. Standard statistical formulations of the problem are provably solved by general convex programming techniques and more practical, fast (nearly-linear time) iterative methods. However, these latter "fast algorithms" have previously been observed to be brittle in various real-world settings.
  We investigate the brittleness of fast sparse recovery algorithms to generative model changes through the lens of studying their robustness to a "helpful" semi-random adversary, a framework which tests whether an algorithm overfits to input assumptions. We consider the following basic model: let $\mathbf{A} \in \mathbb{R}^{n \times d}$ be a measurement matrix which contains an unknown subset of rows $\mathbf{G} \in \mathbb{R}^{m \times d}$ which are bounded and satisfy the restricted isometry property (RIP), but is otherwise arbitrary. Letting $x^\star \in \mathbb{R}^d$ be $s$-sparse, and given either exact measurements $b = \mathbf{A} x^\star$ or noisy measurements $b = \mathbf{A} x^\star + Œæ$, we design algorithms recovering $x^\star$ information-theoretically optimally in nearly-linear time. We extend our algorithm to hold for weaker generative models relaxing our planted RIP assumption to a natural weighted variant, and show that our method's guarantees naturally interpolate the quality of the measurement matrix to, in some parameter regimes, run in sublinear time.
  Our approach differs from prior fast iterative methods with provable guarantees under semi-random generative models: natural conditions on a submatrix which make sparse recovery tractable are NP-hard to verify. We design a new iterative method tailored to the geometry of sparse recovery which is provably robust to our semi-random model. We hope our approach opens the door to new robust, efficient algorithms for natural statistical inverse problems.

</p>
</details>

<details><summary><b>End-to-end Multiple Instance Learning with Gradient Accumulation</b>
<a href="https://arxiv.org/abs/2203.03981">arxiv:2203.03981</a>
&#x1F4C8; 2 <br>
<p>Axel Andersson, Nadezhda Koriakina, Nata≈°a Sladoje, Joakim Lindblad</p></summary>
<p>

**Abstract:** Being able to learn on weakly labeled data, and provide interpretability, are two of the main reasons why attention-based deep multiple instance learning (ABMIL) methods have become particularly popular for classification of histopathological images. Such image data usually come in the form of gigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches (instances). However, the sheer size of the data makes training of ABMIL models challenging. All the instances from one WSI cannot be processed at once by conventional GPUs. Existing solutions compromise training by relying on pre-trained models, strategic sampling or selection of instances, or self-supervised learning. We propose a training strategy based on gradient accumulation that enables direct end-to-end training of ABMIL models without being limited by GPU memory. We conduct experiments on both QMNIST and Imagenette to investigate the performance and training time, and compare with the conventional memory-expensive baseline and a recent sampled-based approach. This memory-efficient approach, although slower, reaches performance indistinguishable from the memory-expensive baseline.

</p>
</details>

<details><summary><b>Comparing lifetime learning methods for morphologically evolving robots</b>
<a href="https://arxiv.org/abs/2203.03967">arxiv:2203.03967</a>
&#x1F4C8; 2 <br>
<p>Fuda van Diggelen, Eliseo Ferrante, A. E. Eiben</p></summary>
<p>

**Abstract:** Evolving morphologies and controllers of robots simultaneously leads to a problem: Even if the parents have well-matching bodies and brains, the stochastic recombination can break this match and cause a body-brain mismatch in their offspring. We argue that this can be mitigated by having newborn robots perform a learning process that optimizes their inherited brain quickly after birth. We compare three different algorithms for doing this. To this end, we consider three algorithmic properties, efficiency, efficacy, and the sensitivity to differences in the morphologies of the robots that run the learning process.

</p>
</details>

<details><summary><b>Digital Speech Algorithms for Speaker De-Identification</b>
<a href="https://arxiv.org/abs/2203.03932">arxiv:2203.03932</a>
&#x1F4C8; 2 <br>
<p>Stefano Marinozzi, Marcos Faundez-Zanuy</p></summary>
<p>

**Abstract:** The present work is based on the COST Action IC1206 for De-identification in multimedia content. It was performed to test four algorithms of voice modifications on a speech gender recognizer to find the degree of modification of pitch when the speech recognizer have the probability of success equal to the probability of failure. The purpose of this analysis is to assess the intensity of the speech tone modification, the quality, the reversibility and not-reversibility of the changes made.

</p>
</details>

<details><summary><b>Visual anomaly detection in video by variational autoencoder</b>
<a href="https://arxiv.org/abs/2203.03872">arxiv:2203.03872</a>
&#x1F4C8; 2 <br>
<p>Faraz Waseem, Rafael Perez Martinez, Chris Wu</p></summary>
<p>

**Abstract:** Video anomalies detection is the intersection of anomaly detection and visual intelligence. It has commercial applications in surveillance, security, self-driving cars and crop monitoring. Videos can capture a variety of anomalies. Due to efforts needed to label training data, unsupervised approaches to train anomaly detection models for videos is more practical An autoencoder is a neural network that is trained to recreate its input using latent representation of input also called a bottleneck layer. Variational autoencoder uses distribution (mean and variance) as compared to latent vector as bottleneck layer and can have better regularization effect. In this paper we have demonstrated comparison between performance of convolutional LSTM versus a variation convolutional LSTM autoencoder

</p>
</details>

<details><summary><b>Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective</b>
<a href="https://arxiv.org/abs/2203.03871">arxiv:2203.03871</a>
&#x1F4C8; 2 <br>
<p>Quan Cui, Bingchen Zhao, Zhao-Min Chen, Borui Zhao, Renjie Song, Jiajun Liang, Boyan Zhou, Osamu Yoshie</p></summary>
<p>

**Abstract:** This work simultaneously considers the discriminability and transferability properties of deep representations in the typical supervised learning task, i.e., image classification. By a comprehensive temporal analysis, we observe a trade-off between these two properties. The discriminability keeps increasing with the training progressing while the transferability intensely diminishes in the later training period.
  From the perspective of information-bottleneck theory, we reveal that the incompatibility between discriminability and transferability is attributed to the over-compression of input information. More importantly, we investigate why and how the InfoNCE loss can alleviate the over-compression, and further present a learning framework, named contrastive temporal coding~(CTC), to counteract the over-compression and alleviate the incompatibility. Extensive experiments validate that CTC successfully mitigates the incompatibility, yielding discriminative and transferable representations. Noticeable improvements are achieved on the image classification task and challenging transfer learning tasks. We hope that this work will raise the significance of the transferability property in the conventional supervised learning setting. Code will be publicly available.

</p>
</details>

<details><summary><b>Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science</b>
<a href="https://arxiv.org/abs/2203.07031">arxiv:2203.07031</a>
&#x1F4C8; 1 <br>
<p>Scott Allen Cambo, Darren Gergle</p></summary>
<p>

**Abstract:** Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model's development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.

</p>
</details>

<details><summary><b>Structural & Granger CAUSALITY for IoT Digital Twin</b>
<a href="https://arxiv.org/abs/2203.04876">arxiv:2203.04876</a>
&#x1F4C8; 1 <br>
<p>PG Madhavan</p></summary>
<p>

**Abstract:** In this foundational expository article on the application of Causality Analysis in IoT, we establish the basic theory and algorithms for estimating Structural and Granger causality factors from measured multichannel sensor data (vector timeseries). Vector timeseries is modeled as a Structural Vector Autoregressive (SVAR) model; utilizing Kalman Filter and Independent Component Analysis (ICA) methods, Structural and generalized Granger causality factors are estimated. The estimated causal factors are presented as a Fence graph which we call Causal Digital Twin. Practical applications of Causal Digital Twin are demonstrated on NASA Prognostic Data Repository Bearing data collection. Use of Causal Digital Twin for counterfactual experiments are indicated.
  Causal Digital Twin is a horizontal solution that applies to diverse use cases in multiple industries such as Industrial, Manufacturing, Automotive, Consumer, Building and Smart City.

</p>
</details>

<details><summary><b>Second-life Lithium-ion batteries: A chemistry-agnostic and scalable health estimation algorithm</b>
<a href="https://arxiv.org/abs/2203.04249">arxiv:2203.04249</a>
&#x1F4C8; 1 <br>
<p>Aki Takahashi, Anirudh Allam, Simona Onori</p></summary>
<p>

**Abstract:** Battery state of health is an essential metric for diagnosing battery degradation during testing and operation. While many unique measurements are possible in the design phase, for practical applications often only temperature, voltage and current sensing are accessible. This paper presents a novel combination of machine learning techniques to produce accurate predictions significantly faster than standard Gaussian processes. The data-driven approach uses feature generation with simple mathematics, feature filtering, and bagging, which is validated with publicly available aging datasets of more than 200 cells with slow and fast charging, across different cathode chemistries, and for various operating conditions. Based on multiple training-test partitions, average and median state of health prediction root mean square error (RMSE) is found to be less than 1.48% and 1.27%, respectively, with a limited amount of input data, showing the capability of the approach even when input data and time are limiting factors. The process developed in this paper has direct applicability to today's incumbent open challenge of assessing retired batteries on the basis of their residual health, and therefore nominal remaining useful life, to allow fast classification for second-life reutilization.

</p>
</details>

<details><summary><b>Adaptative Perturbation Patterns: Realistic Adversarial Learning for Robust NIDS</b>
<a href="https://arxiv.org/abs/2203.04234">arxiv:2203.04234</a>
&#x1F4C8; 1 <br>
<p>Jo√£o Vitorino, Nuno Oliveira, Isabel Pra√ßa</p></summary>
<p>

**Abstract:** Adversarial attacks pose a major threat to machine learning and to the systems that rely on it. Nonetheless, adversarial examples cannot be freely generated for domains with tabular data, such as cybersecurity. This work establishes the fundamental constraint levels required to achieve realism and introduces the Adaptative Perturbation Pattern Method (A2PM) to fulfill these constraints in a gray-box setting. A2PM relies on pattern sequences that are independently adapted to the characteristics of each class to create valid and coherent data perturbations. The developed method was evaluated in a cybersecurity case study with two scenarios: Enterprise and Internet of Things (IoT) networks. Multilayer Perceptron (MLP) and Random Forest (RF) classifiers were created with regular and adversarial training, using the CIC-IDS2017 and IoT-23 datasets. In each scenario, targeted and untargeted attacks were performed against the classifiers, and the generated examples were compared with the original network traffic flows to assess their realism. The obtained results demonstrate that A2PM provides a time efficient generation of realistic adversarial examples, which can be advantageous for both adversarial training and attacks.

</p>
</details>

<details><summary><b>Learning based Age of Information Minimization in UAV-relayed IoT Networks</b>
<a href="https://arxiv.org/abs/2203.04227">arxiv:2203.04227</a>
&#x1F4C8; 1 <br>
<p>Biplav Choudhury, Prasenjit Karmakar, Vijay K. Shah, Jeffrey H. Reed</p></summary>
<p>

**Abstract:** Unmanned Aerial Vehicles (UAVs) are used as aerial base-stations to relay time-sensitive packets from IoT devices to the nearby terrestrial base-station (TBS). Scheduling of packets in such UAV-relayed IoT-networks to ensure fresh (or up-to-date) IoT devices' packets at the TBS is a challenging problem as it involves two simultaneous steps of (i) sampling of packets generated at IoT devices by the UAVs [hop-1] and (ii) updating of sampled packets from UAVs to the TBS [hop-2]. To address this, we propose Age-of-Information (AoI) scheduling algorithms for two-hop UAV-relayed IoT-networks. First, we propose a low-complexity AoI scheduler, termed, MAF-MAD that employs Maximum AoI First (MAF) policy for sampling of IoT devices at UAV (hop-1) and Maximum AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS (hop-2). We prove that MAF-MAD is the optimal AoI scheduler under ideal conditions (lossless wireless channels and generate-at-will traffic-generation at IoT devices). On the contrary, for general conditions (lossy channel conditions and varying periodic traffic-generation at IoT devices), a deep reinforcement learning algorithm, namely, Proximal Policy Optimization (PPO)-based scheduler is proposed. Simulation results show that the proposed PPO-based scheduler outperforms other schedulers like MAF-MAD, MAF, and round-robin in all considered general scenarios.

</p>
</details>

<details><summary><b>Neural Contextual Bandits via Reward-Biased Maximum Likelihood Estimation</b>
<a href="https://arxiv.org/abs/2203.04192">arxiv:2203.04192</a>
&#x1F4C8; 1 <br>
<p>Yu-Heng Hung, Ping-Chun Hsieh</p></summary>
<p>

**Abstract:** Reward-biased maximum likelihood estimation (RBMLE) is a classic principle in the adaptive control literature for tackling explore-exploit trade-offs. This paper studies the stochastic contextual bandit problem with general bounded reward functions and proposes NeuralRBMLE, which adapts the RBMLE principle by adding a bias term to the log-likelihood to enforce exploration. NeuralRBMLE leverages the representation power of neural networks and directly encodes exploratory behavior in the parameter space, without constructing confidence intervals of the estimated rewards. We propose two variants of NeuralRBMLE algorithms: The first variant directly obtains the RBMLE estimator by gradient ascent, and the second variant simplifies RBMLE to a simple index policy through an approximation. We show that both algorithms achieve $\widetilde{\mathcal{O}}(\sqrt{T})$ regret. Through extensive experiments, we demonstrate that the NeuralRBMLE algorithms achieve comparable or better empirical regrets than the state-of-the-art methods on real-world datasets with non-linear reward functions.

</p>
</details>

<details><summary><b>Understanding person identification via gait</b>
<a href="https://arxiv.org/abs/2203.04179">arxiv:2203.04179</a>
&#x1F4C8; 1 <br>
<p>Simon Hanisch, Evelyn Muschter, Adamantini Chatzipanagioti, Shu-Chen Li, Thorsten Strufe</p></summary>
<p>

**Abstract:** Gait recognition is the process of identifying humans from their bipedal locomotion such as walking or running. As such gait data is privacy sensitive information and should be anonymized. With the rise of more and higher quality gait recording techniques, such as depth cameras or motion capture suits, an increasing amount of high-quality gait data becomes available which requires anonymization. As a first step towards developing anonymization techniques for high-quality gait data, we study different aspects of movement data to quantify their contribution to the gait recognition process. We first extract categories of features from the literature on human gait perception and then design computational experiments for each of the categories which we run against a gait recognition system. Our results show that gait anonymization is a challenging process as the data is highly redundant and interdependent.

</p>
</details>

<details><summary><b>An Efficient Polyp Segmentation Network</b>
<a href="https://arxiv.org/abs/2203.04118">arxiv:2203.04118</a>
&#x1F4C8; 1 <br>
<p>Tugberk Erol, Duygu Sarikaya</p></summary>
<p>

**Abstract:** Cancer is a disease that occurs as a result of uncontrolled division and proliferation of cells. The number of cancer cases has been on the rise over the recent years.. Colon cancer is one of the most common types of cancer in the world. Polyps that can be seen in the large intestine can cause cancer if not removed with early intervention. Deep learning and image segmentation techniques are used to minimize the number of polyps that goes unnoticed by the experts during the diagnosis. Although these techniques give good results, they require too many parameters. We propose a new model to solve this problem. Our proposed model includes less parameters as well as outperforming the success of the state of the art models. In the proposed model, a partial decoder is used to reduce the number of parameters while maintaning success. EfficientNetB0, which gives successfull results as well as requiring few parameters, is used in the encoder part. Since polyps have variable aspect and aspect ratios, an asymetric convolution block was used instead of using classic convolution block. Kvasir and CVC-ClinicDB datasets were seperated as training, validation and testing, and CVC-ColonDB, ETIS and Endoscene datasets were used for testing. According to the dice metric, our model had the best results with %71.8 in the ColonDB test dataset, %89.3 in the EndoScene test dataset and %74.8 in the ETIS test dataset. Our model requires a total of 2.626.337 parameters. When we compare it in the literature, according to similar studies, the model that requires the least parameters is U-Net++ with 9.042.177 parameters.

</p>
</details>

<details><summary><b>Learning to Erase the Bayer-Filter to See in the Dark</b>
<a href="https://arxiv.org/abs/2203.04042">arxiv:2203.04042</a>
&#x1F4C8; 1 <br>
<p>Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng Jin Teoh, Jiajun Shen</p></summary>
<p>

**Abstract:** Low-light image enhancement - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the low-light image enhancement by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.

</p>
</details>

<details><summary><b>A Compilation Flow for the Generation of CNN Inference Accelerators on FPGAs</b>
<a href="https://arxiv.org/abs/2203.04015">arxiv:2203.04015</a>
&#x1F4C8; 1 <br>
<p>Seung-Hun Chung, Tarek S. Abdelrahman</p></summary>
<p>

**Abstract:** We present a compilation flow for the generation of CNN inference accelerators on FPGAs. The flow translates a frozen model into OpenCL kernels with the TVM compiler and uses the Intel OpenCL SDK to compile to an FPGA bitstream. We improve the quality of the generated hardware with optimizations applied to the base OpenCL kernels generated by TVM. These optimizations increase parallelism, reduce memory access latency, increase concurrency and save on-chip resources. We automate these optimizations in TVM and evaluate them by generating accelerators for LeNet-5, MobileNetV1 and ResNet-34 on an Intel Stratix~10SX. We show that the optimizations improve the performance of the generated accelerators by up to 846X over the base accelerators. The performance of the optimized accelerators is up to 4.57X better than TensorFlow on CPU, 3.83X better than single-threaded TVM and is only 0.34X compared to TVM with 56 threads. Our optimized kernels also outperform ones generated by a similar approach (that also uses high-level synthesis) while providing more functionality and flexibility. However, it underperforms an approach that utilizes hand-optimized designs. Thus, we view our approach as useful in pre-production environments that benefit from increased performance and fast prototyping, realizing the benefits of FPGAs without hardware design expertise.

</p>
</details>

<details><summary><b>Mutual Contrastive Learning to Disentangle Whole Slide Image Representations for Glioma Grading</b>
<a href="https://arxiv.org/abs/2203.04013">arxiv:2203.04013</a>
&#x1F4C8; 1 <br>
<p>Lipei Zhang, Yiran Wei, Ying Fu, Stephen Price, Carola-Bibiane Sch√∂nlieb, Chao Li</p></summary>
<p>

**Abstract:** Whole slide images (WSI) provide valuable phenotypic information for histological assessment and malignancy grading of tumors. The WSI-based computational pathology promises to provide rapid diagnostic support and facilitate digital health. The most commonly used WSI are derived from formalin-fixed paraffin-embedded (FFPE) and frozen sections. Currently, the majority of automatic tumor grading models are developed based on FFPE sections, which could be affected by the artifacts introduced by tissue processing. Here we propose a mutual contrastive learning scheme to integrate FFPE and frozen sections and disentangle cross-modality representations for glioma grading. We first design a mutual learning scheme to jointly optimize the model training based on FFPE and frozen sections. Further, we develop a multi-modality domain alignment mechanism to ensure semantic consistency in the backbone model training. We finally design a sphere normalized temperature-scaled cross-entropy loss (NT-Xent), which could promote cross-modality representation disentangling of FFPE and frozen sections. Our experiments show that the proposed scheme achieves better performance than the model trained based on each single modality or mixed modalities. The sphere NT-Xent loss outperforms other typical metrics loss functions.

</p>
</details>

<details><summary><b>Evolutionary Neural Cascade Search across Supernetworks</b>
<a href="https://arxiv.org/abs/2203.04011">arxiv:2203.04011</a>
&#x1F4C8; 1 <br>
<p>Alexander Chebykin, Tanja Alderliesten, Peter A. N. Bosman</p></summary>
<p>

**Abstract:** To achieve excellent performance with modern neural networks, having the right network architecture is important. Neural Architecture Search (NAS) concerns the automatic discovery of task-specific network architectures. Modern NAS approaches leverage supernetworks whose subnetworks encode candidate neural network architectures. These subnetworks can be trained simultaneously, removing the need to train each network from scratch, thereby increasing the efficiency of NAS. A recent method called Neural Architecture Transfer (NAT) further improves the efficiency of NAS for computer vision tasks by using a multi-objective evolutionary algorithm to find high-quality subnetworks of a supernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS - Evolutionary Neural Cascade Search. ENCAS can be used to search over multiple pretrained supernetworks to achieve a trade-off front of cascades of different neural network architectures, maximizing accuracy while minimizing FLOPS count. We test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100, ImageNet) and achieve Pareto dominance over previous state-of-the-art NAS models up to 1.5 GFLOPS. Additionally, applying ENCAS to a pool of 518 publicly available ImageNet classifiers leads to Pareto dominance in all computation regimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied by an 18\% decrease in computation effort from 362 to 296 GFLOPS. Our code is available at https://github.com/AwesomeLemon/ENCAS

</p>
</details>

<details><summary><b>Online Weak-form Sparse Identification of Partial Differential Equations</b>
<a href="https://arxiv.org/abs/2203.03979">arxiv:2203.03979</a>
&#x1F4C8; 1 <br>
<p>Daniel A. Messenger, Emiliano Dall'Anese, David M. Bortz</p></summary>
<p>

**Abstract:** This paper presents an online algorithm for identification of partial differential equations (PDEs) based on the weak-form sparse identification of nonlinear dynamics algorithm (WSINDy). The algorithm is online in a sense that if performs the identification task by processing solution snapshots that arrive sequentially. The core of the method combines a weak-form discretization of candidate PDEs with an online proximal gradient descent approach to the sparse regression problem. In particular, we do not regularize the $\ell_0$-pseudo-norm, instead finding that directly applying its proximal operator (which corresponds to a hard thresholding) leads to efficient online system identification from noisy data. We demonstrate the success of the method on the Kuramoto-Sivashinsky equation, the nonlinear wave equation with time-varying wavespeed, and the linear wave equation, in one, two, and three spatial dimensions, respectively. In particular, our examples show that the method is capable of identifying and tracking systems with coefficients that vary abruptly in time, and offers a streaming alternative to problems in higher dimensions.

</p>
</details>

<details><summary><b>A Preliminary Study on Aging Examining Online Handwriting</b>
<a href="https://arxiv.org/abs/2203.03933">arxiv:2203.03933</a>
&#x1F4C8; 1 <br>
<p>Marcos Faundez-Zanuy, Enric Sesa-Nogueras, Josep Roure-Alcob√©, Anna Esposito, Jiri Mekyska, Karmele L√≥pez-de-Ipi√±a</p></summary>
<p>

**Abstract:** In order to develop infocommunications devices so that the capabilities of the human brain may interact with the capabilities of any artificially cognitive system a deeper knowledge of aging is necessary. Especially if society does not want to exclude elder people and wants to develop automatic systems able to help and improve the quality of life of this group of population, healthy individuals as well as those with cognitive decline or other pathologies. This paper tries to establish the variations in handwriting tasks with the goal to obtain a better knowledge about aging. We present the correlation results between several parameters extracted from online handwriting and the age of the writers. It is based on BIOSECURID database, which consists of 400 people that provided several biometric traits, including online handwriting. The main idea is to identify those parameters that are more stable and those more age dependent. One challenging topic for disease diagnose is the differentiation between healthy and pathological aging. For this purpose, it is necessary to be aware of handwriting parameters that are, in general, not affected by aging and those who experiment changes, increase or decrease their values, because of it. This paper contributes to this research line analyzing a selected set of online handwriting parameters provided by a healthy group of population aged from 18 to 70 years. Preliminary results show that these parameters are not affected by aging and therefore, changes in their values can only be attributed to motor or cognitive disorders.

</p>
</details>

<details><summary><b>Estimating the average causal effect of intervention in continuous variables using machine learning</b>
<a href="https://arxiv.org/abs/2203.03916">arxiv:2203.03916</a>
&#x1F4C8; 1 <br>
<p>Yoshiaki Kitazawa</p></summary>
<p>

**Abstract:** The most widely discussed methods for estimating the Average Causal Effect / Average Treatment Effect are those for intervention in discrete binary variables whose value represents the intervention / non-intervention groups. On the other hand, methods for intervening in continuous variables independent of the data generating model has not been developed. In this study, we give a method for estimating the average causal effect for intervention in continuous variables that can be applied to data of any generating model as long as the causal effect is identifiable. The proposing method is independent of machine learning algorithms and preserves the identifiability of the data.

</p>
</details>

<details><summary><b>Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate</b>
<a href="https://arxiv.org/abs/2203.03899">arxiv:2203.03899</a>
&#x1F4C8; 1 <br>
<p>Ziye Ma, Somayeh Sojoudi</p></summary>
<p>

**Abstract:** This paper is concerned with low-rank matrix optimization, which has found a wide range of applications in machine learning. This problem in the special case of matrix sense has been studied extensively through the notion of Restricted Isometry Property (RIP), leading to a wealth of results on the geometric landscape of the problem and the convergence rate of common algorithms. However, the existing results are not able to handle the problem with a general objective function subject to noisy data. In this paper, we address this problem by developing a mathematical framework that can deal with random corruptions to general objective functions, where the noise model is arbitrary. We prove that as long as the RIP constant of the noiseless objective is less than $1/3$, any spurious local solution of the noisy optimization problem must be close to the ground truth solution. By working through the strict saddle property, we also show that an approximate solution can be found in polynomial time. We characterize the geometry of the spurious local minima of the problem in a local region around the ground truth in the case when the RIP constant is greater than $1/3$. This paper offers the first set of results on the global and local optimization landscapes of general low-rank optimization problems under arbitrary random corruptions.

</p>
</details>

<details><summary><b>Adapt$\mathcal{O}$r: Objective-Centric Adaptation Framework for Language Models</b>
<a href="https://arxiv.org/abs/2203.03989">arxiv:2203.03989</a>
&#x1F4C8; 0 <br>
<p>Michal ≈†tef√°nik, V√≠t Novotn√Ω, Nikola Groverov√°, Petr Sojka</p></summary>
<p>

**Abstract:** Progress in natural language processing research is catalyzed by the possibilities given by the widespread software frameworks. This paper introduces Adaptor library that transposes the traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives. We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation. Adaptor aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios.

</p>
</details>


{% endraw %}
Prev: [2022.03.07]({{ '/2022/03/07/2022.03.07.html' | relative_url }})  Next: [2022.03.09]({{ '/2022/03/09/2022.03.09.html' | relative_url }})